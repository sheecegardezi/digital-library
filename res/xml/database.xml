<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<add>
<doc>
<field name="id">1</field>
<field name="author">Tracy Hall</field>
<field name="author">Sarah Beecham</field>
<field name="author">June Verner</field>
<field name="author">David Wilson</field>
<field name="title">The impact of staff turnover on software projects: the importance of understanding what makes software practitioners tick</field>
<field name="keyword">Motivation</field>
<field name="keyword"> project success</field>
<field name="keyword"> staff turnover.</field>
<field name="abstract">In this paper we investigate the impact of staff turnover on software projects. In particular we investigate whether high staff turnover damages project success. We analyse data from an empirical study of 89 software practitioners to show that projects with high staff turnover are less successful. Furthermore our empirical data suggests a relationship between high staff turnover on projects and low staff motivation levels. We discuss factors which have been previously found to improve motivation levels and conclude that improving motivation levels can reduce staff turnover, which in turn increases project success.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">2</field>
<field name="author">June Verner</field>
<field name="author">Jennifer Sampson</field>
<field name="author">Narciso Cerpa</field>
<field name="title">What factors lead to software project failure?</field>
<field name="keyword">software project failure</field>
<field name="keyword"> software project management</field>
<field name="keyword"> failure factors</field>
<field name="keyword"> project risk</field>
<field name="abstract">It has been suggested that there is more than one reason for a software development project to fail. However, most of the literature discussing project failure tends to be rather general, supplying us with lists of risk and failure factors, and focusing on the negative business effects of the failure. Very little research has attempted an in-depth investigation of a number of failed projects to identify exactly what are the factors behind the failure. In this research we analyze data from 70 failed projects. This data provides us with practitioners perspectives on 57 development and management factors for projects they considered were failures. Our results show that all projects investigated suffered from numerous failure factors. For a single project the number of such factors ranged from 5 to 47. While there does not appear to be any overarching set of failure factors we discovered that all of the projects suffered from poor project management. Most projects additionally suffered from organizational factors outside the project manager s control. We conclude with suggestions for minimizing the four most common failure factors.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">3</field>
<field name="author">Litifa Noor</field>
<field name="author">Alagan Anpalagan</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="title">SNR and BER Derivation and Analysis of Downlink OFDM Systems with Noisy Fading Doppler Channels</field>
<field name="keyword">BER</field>
<field name="keyword"> OFDM</field>
<field name="keyword"> Fading</field>
<field name="keyword"> Doppler</field>
<field name="abstract">In this paper, we investigate the SNR and BER performance of an OFDM system under imperfect synchro-

nization which is caused due to noise, Doppler shift and frequency selective fades in the channel. An analytical expression for BER is derived assuming BPSK modulation and numerical results are presented to show the e ect and extent of the above channel impairments in OFDM downlinks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">4</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="author">Sam Reisenfeld</field>
<field name="title">Performance Analysis of a Logarithmic based Phase Detector for Tan-Locked Loops</field>
<field name="keyword">Digital Tanlocked Loops</field>
<field name="keyword"> DPLL</field>
<field name="keyword"> Logarithmic phase detector</field>
<field name="abstract">Nonlinear system design is a common practice in communication

engineering for improved performances in advanced receivers.

Here we look into a logarithmic based nonlinear loop

for a second order arctan based digital phase locked loop (DPLL)

for frequency synchronisation. The steady state and the

acquisition performances of the loop are analyzed. The logarithmic

nonlinearity is intentionally introduced to have improved

phase noise performances during the steady state tracking

mode. We present a close form expression for the open loop

statistical distribution of the phase noise process and compare it

with the linear PLL model on its performances. We also study

the acquisition process of the loop by looking at the phase plane

trajectories.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">5</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Jyrki Katajainen</field>
<field name="author">Damian Merrick</field>
<field name="author">Cahya Ong</field>
<field name="author">Thomas Wolle</field>
<field name="title">Compressing spatio-temporal trajectories</field>
<field name="abstract">Trajectory data is becoming increasingly available and the size of the trajectories is getting larger. In this paper we study the problem of compressing spatio-temporal trajectories such that the most common queries can still be answered approximately after the compression step has taken place. In the process we develop an O(n log^k n)-time implementation of the Douglas-Peucker algorithm, where k = 2 or k = 3 depending on the type of approximation, in the case when the polygonal path of n vertices given as input is allowed to self-intersect.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">6</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="author">Omar Hashmi</field>
<field name="author">Qian Zheng</field>
<field name="title">A Complex-Envelope Based Digital Phase Locked Loop with an arctan Phase Detector Implemented on FPGA and Performance Analysis</field>
<field name="keyword">PLL implementation</field>
<field name="keyword"> FPGA</field>
<field name="abstract">In this paper we present a Digital Phase Locked Loop (D-PLL), based on an arctan phase detector for complex signals, implemented on a Field Programmable Gate Array (FPGA) together with its performance analysis. The work is motivated by the requirement of signal synchronisation in Software Defined Radios (SDR) for high speed communication receivers. The theoretical model for the D-PLL and its corresponding implementation methodology on a Xillinx Virtex-IV FPGA are given in this paper. We also provide some test results and simulations results on the implemented system and verify them using the theoretical models</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">7</field>
<field name="author">Marc Benkert</field>
<field name="author">Bojan Djordjevic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Thomas Wolle</field>
<field name="title">Finding Popular Places</field>
<field name="abstract">Widespread availability of location aware devices (such as GPS receivers) promotes capture of detailed movement trajectories of people, animals, vehicles and other moving objects, opening new options for a better understanding of the processes involved. We

investigate spatio-temporal movement patterns in large tracking data sets. Specifically we study so-called popular places , that is, regions that are visited by many entities. We present upper and lower bounds.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">8</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="author">Bijan Rohani</field>
<field name="author">Manora Caldera</field>
<field name="title">Design and Implementation of a 20Mbps High-Speed Base-Band Processing unit with Synchronisation: A Communications Test-Bed</field>
<field name="abstract">The high demand in providing broadband mobile services for 3G+ systems motivates researchers and engineers to design, implement and test highly efficient and scalable high speed communication systems. With similar motivations, in this paper, we present a high speed base-band processing unit implemented in software targeting high speed hardware devices for advanced communication receiver design. The hardware platform, implementation methodologies, the reprogrammable software, and the theory behind the implemented communication system are presented here. The synchronisation sub-systems of the base-band processing unit are also presented here.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">9</field>
<field name="author">Otfried Cheong</field>
<field name="author">Hazel Everett</field>
<field name="author">Marc Glisse</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Samuel Hornus</field>
<field name="author">Sylvain Lazard</field>
<field name="author">Mira Lee</field>
<field name="author">Hyeon-Suk Na</field>
<field name="title">Farthest-polygon Voronoi diagrams</field>
<field name="abstract">Given a family of k disjoint connected polygonal sites of total complexity n, we consider the farthest-site Voronoi diagram of these sites, where the distance to a site is the distance to a closest point on it. We show that the complexity of this diagram is O(n), and give an O(n log3 n) time algorithm to compute it. We also prove a number of structural properties of this diagram. In particular, a Voronoi region may consist of k _ 1 connected components, but if one component is bounded, then it is equal to the entire region.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">10</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="title">NEURO-WIRELESS TECHNOLOGY FOR NEURAL PROSTHESES:BY-PASSING DAMAGED OR MALFUNCTIONING NERVES BY MEANS OF WIRELESS COMMUNICATIONS</field>
<field name="keyword">Neuro Wireless Technology</field>
<field name="keyword"> wireless communication</field>
<field name="keyword"> neural system</field>
<field name="abstract">The nervous system traditionally communicates using guided

channels similar to the wired communication system. Here we

propose to study the issues related to in-body wireless communications

for neural prostheses. The need for such technology

is eminent in the event of a nervous system breakdown or a

disorder due to infectious disease, or some physical damages

caused to the nerves. The very low powered neural signals

need to be transmitted by means of wireless with a transmitter

and a receiver unit connecting a (large or small) portion

of a damaged or malfunctioning nerve. Though the technology

is predominantly driven by hardware and sensor technology,

it is interesting to identify the related issues in terms of

wireless transmission, for example; body area channel modelling

and prediction, signal design and synchronization, interferences

and noise within the body, and many more, which we

present in this paper.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">11</field>
<field name="author">Mohammad Farshi</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Experimental study of geometric t-spanners: a running time comparison</field>
<field name="abstract">The construction of t-spanners of a given point set has received a lot of attention, especially from a theoretical perspective. We experimentally study the performance of the most common construction algorithm for points in the Euclidean plane. In a previous paper we considered the properties of the produced networks from five common algorithms. We consider several additional algorithms and mainly focus on the running times. This is the first time an extensive comparison has been made between the running times of construction algorithms of t-spanners. It has been shown that the greedy algorithm produces t-spanners of very high quality. However, the greedy algorithm has a cubic running time while many other construction

algorithms have a running time of O(n log n). Our main contribution is the implementation of faster variants of the greedy algorithm.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">12</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="title">Steady State Distribution of a Hyperbolic Digital TanLock Loop with Extended Pull-in Range for Frequency Synchronization in High Doppler Environment</field>
<field name="keyword">Frequency Synchronization</field>
<field name="keyword"> Digital Tanlock Loop</field>
<field name="keyword"> Digital Phase Locked Loop</field>
<field name="keyword"> DPLL</field>
<field name="keyword"> PLL</field>
<field name="keyword"> arctan</field>
<field name="keyword"> hyperbolic loop</field>
<field name="abstract">A hyperbolic arctan based Digital Tanlock Loop (D-TLL) operating with complex signals at baseband

or intermediate frequencies in high Doppler environments is treated here. The arctand D-PLL, known

as the tanlock loop (TLL), is used in software defined radio architectures for frequency acquisition and

tracking. The hyperbolic nonlinearity intentionally introduced within the phase detector extends the pull-in

range of the frequency for a given loop, compared to the normal D-TLL, allowing a wider frequency

acquisition range which is suitable for high Doppler communications environment. In this paper we study

the steady state phase noise performances of such a feedback loop for additive Gaussian noise using

stochastic analysis. The stochastic model of a first-order hyperbolic loop and the theoretical analysis for

the corresponding statistical distribution of the closed loop steady state phase noise are presented. The

theoretical results are also verified by simulations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">13</field>
<field name="author">Mattias Andersson</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Patrick Laube</field>
<field name="author">Thomas Wolle</field>
<field name="title">Reporting Leadership Patterns Among Trajectories</field>
<field name="abstract">Widespread availability of location aware devices (such as GPS receivers) promotes capture of detailed movement trajectories of people, animals, vehicles and other moving objects, opening new options for a better understanding of the processes involved. In this paper we investigate spatio-temporal movement patterns in large tracking data sets. We present a natural definition of the pattern one object is leading others , and discuss how such leadership patterns can be computed from a group of moving entities. The proposed definition is based on behavioural patterns discussed in the behavioural ecology literature. We also present several algorithms for computing the pattern, and they are analysed both theoretically and experimentally.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">14</field>
<field name="author">Ghazi Al-Naymat</field>
<field name="author">Sanjay Chawla</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Dimensionality reduction for long duration and complex spatio-temporal queries</field>
<field name="abstract">From tracking of moose in Sweden, to movement of traffic in a large metropolis, spatio-temporal data is continuously being collected and made available in the public

domain. This provides an opportunity to mine and query spatio-temporal data with the purpose of finding substantial patterns and understand the underlying data generating

process. An important class of queries is based on the flock pattern. A flock is a large subset of objects moving along paths close to each other for a certain pre-defined time. The standard approach to process a flock query is to map spatio-temporal data into a high dimensional space and reduce the query into a sequence of standard range queries which can be presented using a spatial indexing structure. However, as it is well known, the performance of spatial indexing structures drastically deteriorates in high dimensional space. In this paper we propose a preprocessing strategy which consists of using a random projection to reduce the dimensionality of the transformed space. We prove a probabilistic approximation which results from the projection and present experimental results which show, for the first time, the possibility of breaking the curse of dimensionality in a spatio-temporal setting.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">15</field>
<field name="author">Mohammad Ali Abam</field>
<field name="author">Mark de Berg</field>
<field name="author">Mohammad Farshi</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Region-Fault Tolerant Geometric Spanners</field>
<field name="abstract">We introduce the concept of region-fault tolerant spanners for planar point sets, and prove the existence of region-fault tolerant spanners of small size. For a geometric graph G on a point set P and a region F, we define G F to be what remains of G after the vertices and edges of G intersecting F have been removed. A C-fault tolerant t-spanner is a geometric graph G on P such that for any convex region F, the graph G F is a t-spanner for Gc(P) F, where Gc(P) is the complete geometric graph on P. We prove that any set P of n points admits a C-fault tolerant (1 + eps)-spanner of size O(n log n), for any constant eps &gt; 0; if adding Steiner points is allowed then the size of the spanner reduces to O(n), and for several special cases we show how to obtain region-fault tolerant spanners of O(n) size without using Steiner points. We also consider fault-tolerant geodesic t-spanners: this is a variant where, for any disk D, the distance in G D between any two points u; is at most t times the geodesic distance between u and v. We prove that for any P we can add O(n) Steiner points to obtain a fault-tolerant

geodesic (1 + eps)-spanner of size O(n).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">16</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Giri Narasimhan</field>
<field name="author">Michiel Smid</field>
<field name="title">Distance-preserving approximations of polygonal paths</field>
<field name="abstract">Given a polygonal path P with vertices p1, ..., pn in R^d and a real number t&gt;1, a path Q = (pi1, pi2, ... , pik) is a t-distance preserving approximation of P if 1 = i1 &lt; i2 &lt; ... &lt; ik = n and each straight-line edge (pij ; pij+1) of Q approximates the distance between pij and pij+1 along the path P within a factor of t. We present exact and approximation algorithms that compute such a path Q that minimizes k (when given t) or t (when given k). We also present some experimental results.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">17</field>
<field name="author">Mattias Andersson</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Christos Levcopoulos</field>
<field name="title">Approximate distance oracles for graphs with dense clusters</field>
<field name="abstract">Let H1 = (V,E1) be a collection of N pairwise vertex disjoint O(1)-spanners where the weight of an edge is equal to the Euclidean distance between its endpoints. Let H2 = (V,E2) be the graph on V with M edges of non-negative weight. The union of the two graphs is denoted G = (V, E1 U E2). We present a data structure of size

O(M2 + n log n) that answers (1 + \eps)-approximate shortest path queries in G in constant time, where \eps &gt; 0 is constant.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">18</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Marc van Kreveld</field>
<field name="author">Bettina Speckmann</field>
<field name="title">Efficient Detection of Motion Patterns in Spatio-Temporal Data Sets</field>
<field name="abstract">Moving point object data can be analyzed through the discovery of patterns. We consider the computational e ciency of detecting four such spatio-temporal patterns, namely flock, leadership,

convergence, and encounter, as defined by Laube et al., 2004. These patterns are large enough subgroups of the moving point objects that exhibit similar movement in the sense of direction, heading for the same location, and/or proximity. By the use of techniques from computational geometry, including approximation algorithms, we improve the running time bounds of existing algorithms to detect these patterns.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">19</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Christos Levcopoulos</field>
<field name="title">Minimum weight pseudo-triangulations</field>
<field name="abstract">We consider the problem of computing a minimum weight pseudo-triangulation of a set S of n points in the plane. We first present an O(n log n)-time algorithm that produces a pseudo-triangulation of weight O(log n * wt(M(S))) which is shown to be asymptotically worst-case optimal, i.e., there exists a point set S for which every pseudo-triangulation has weight _(log n * wt(M(S))), where wt(M(S)) is the weight of a minimum weight spanning tree of S. We also present a constant factor approximation algorithm running in cubic time. In the process we give an algorithm that produces a minimum weight pseudo-triangulation of a simple polygon.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">20</field>
<field name="author">Bathiya Senanayake</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Timing Acquisition for Multi-User IDMA</field>
<field name="keyword">IDMA</field>
<field name="keyword"> Receiver</field>
<field name="abstract">Acquisition of the timing in an IDMA system must take place before signal detection and decoding are performed. Acquisition in the presence of severe multiple-access interference with time varying codes makes the task even more difficult. Inefficient designs cause a large number of false alarms and/or missed detections. This paper applies a powerful acquisition technique to IDMA systems in the uplink. Under high multiple access interference conditions conventional acquisition techniques used for DS-CDMA systems simply

fail. The proposed method utilises soft data from the iterative IDMA receiver to effectively cancel interference from the co-channel users. Analytical performance in terms of the number of users, spreading gain, performance of the IDMA receiver, and noise variance is developed, together with simulation results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">21</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Eranga Perera</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Multicasting with selective delivery: A SafetyNet for vertical handoffs</field>
<field name="abstract">In future, mobility support will require handling roaming in heterogeneous access networks. In order to enable seamless roaming it is necessary to minimize the impact of the vertical handoffs. Localized mobility management schemes such as FMIPv6 and HMIPv6 do not provide sufficient handoff performance, since they have been designed for horizontal handoffs. In this paper, we propose the SafetyNet protocol, which allows a Mobile Node to perform seamless vertical handoffs. Further, we propose a handoff timing algorithm which allows a Mobile Node to delay or even completely avoid upward vertical handoffs. We implement the SafetyNet protocol and compare its performance with the Fast Handovers for Mobile IPv6 protocol in our wireless test bed and analyze the results. The experimental results indicate that the proposed SafetyNet protocol can provide an improvement of up to 95% for TCP performance in vertical handoffs, when compared with FMIPv6 and an improvement of 64% over

FMIPv6 with bicasting. We use numerical analysis of the protocol to show that its signaling and data transmission overhead is comparable to Fast Mobile IPv6 and significantly smaller than that of FMIPv6 with bicasting.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">22</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="title">A Network Mobility Management Architecture for a Heterogeneous Network Environment</field>
<field name="keyword">Mobility management</field>
<field name="keyword"> Network mobility</field>
<field name="keyword"> Mobile IP</field>
<field name="abstract">Network mobility management enables mobility of personal area networks and

 vehicular networks across heterogeneous access networks using a Mobile

 Router. This dissertation presents a network mobility management

 architecture for minimizing the impact of handoffs on the communications of

 nodes in the mobile network. The architecture addresses mobility in legacy

 networks without infrastructure support, but can also exploit infrastructure

 support for improved handoff performance. Further, the proposed architecture

 increases the efficiency of communications of nodes in the mobile network

 with counter parts in the fixed network through the use of caching and route

 optimization. The performance and costs of the proposed architecture are

 evaluated through empirical and numerical analysis. The analysis shows the

 feasibility of the architecture in the networks of today and in those of the

 near future.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">23</field>
<field name="author">Mattias Andersson</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Patrick Laube</field>
<field name="author">Thomas Wolle</field>
<field name="title">Reporting Leaders and Followers Among Trajectories of Moving Point Objects</field>
<field name="keyword">moving point objects </field>
<field name="keyword"> trajectories </field>
<field name="keyword"> movement patterns </field>
<field name="keyword"> leadership </field>
<field name="keyword"> spatio</field>
<field name="keyword">temporal data structures </field>
<field name="keyword"> computational geometry</field>
<field name="abstract">Widespread availability of location aware devices (such as GPS receivers) promotes

capture of detailed movement trajectories of people, animals, vehicles and other moving

objects, opening new options for a better understanding of the processes involved. In

this paper we investigate spatio-temporal movement patterns in large tracking data sets.

We present a natural definition of the pattern one object is leading others , which is based

on behavioural patterns discussed in the behavioural ecology literature. Such leadership

patterns can be characterised by a minimum time length for which they have to exist and

by a minimum number of entities involved in the pattern. Furthermore, we distinguish two

models (discrete and continuous) of the time axis for which patterns can start and end. For

all variants of these leadership patterns, we describe algorithms for their detection, given

the trajectories of a group of moving entities. A theoretical analysis as well as experiments

show that these algorithms efficiently report leadership patterns.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">24</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Eranga Perera</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Yuri Ismailov</field>
<field name="title">An Experimental Evaluation of Mobile Node based versus Infrastructure based Handoff Schemes</field>
<field name="keyword">Mobile IPv6</field>
<field name="keyword"> Make-Before-Break Handoffs</field>
<field name="keyword"> Fast Handovers for Mobile IPv6</field>
<field name="abstract">The rate at which the Internet is becoming mobile is unprecedented. This has increased the demand for continuous connectivity even while moving from one network to another at very high speeds. Moving from one network to another gives rise to a handoff process which often incurs packet losses and severe end to end transport protocol performance degradations for the Mobile Node. Most research on IP mobility has focused on minimizing the delays of the handoff process with network infrastructure based approaches. A different way of minimizing the impact of the handoff is to enable the Mobile Node to connect to multiple access networks simultaneously, allowing it to perform Make-Before-Break handoffs. In this paper, we compare the performance of these two alternatives, focusing on the use of Fast Handovers for Mobile IPv6 framework on the infrastructure side and on the other hand Make-Before-Break handoffs using two network interfaces. Both of these schemes require proactive handoffs for optimal performance. The results show that the use of two interfaces for Make-Before-Break handoffs provides increased handoff performance over Fast Handovers for Mobile IPv6. Index Terms Mobile IPv6, Make-Before-Break Handoffs, Fast Handovers for Mobile IPv6</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">25</field>
<field name="author">Marc Benkert</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Florian H _bner</field>
<field name="author">Thomas Wolle</field>
<field name="title">Reporting Flock Patterns</field>
<field name="keyword">moving point objects</field>
<field name="keyword"> trajectories</field>
<field name="keyword"> spatio-temporal data</field>
<field name="keyword"> computational geometry</field>
<field name="abstract">Data representing moving objects is rapidly getting more available, especially

in the area of wildlife GPS tracking. It is a central belief that information is hidden in large

data sets in the form of interesting patterns, where a pattern can be any configuration of

some moving objects in a certain area and/or during a certain time period. One of the most

common spatio-temporal patterns sought after is flocks. A flock is a large enough subset

of objects moving along paths close to each other for a certain pre-defined time. We give

a new definition that we argue is more realistic than the previous ones, and by the use of

techniques from computational geometry we present fast algorithms to detect and report

flocks. The algorithms are analysed both theoretically and experimentally.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">26</field>
<field name="author">Laurent Dairaine</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Sebastien Ardon</field>
<field name="title">IREEL: Remote Experimentation with Real Protocols and Applications over Emulated Network</field>
<field name="abstract">This paper presents a novel e-learning platform called IREEL. IREEL is a virtual laboratory allowing students to drive experiments with real Internet applications and end-to-end protocols in the context of networking courses. This platform consists in a remote network emulator offering a set of pre-defined applications and protocol mechanisms. Experimenters configure and control the emulation and the end-systems behavior in order to perform tests, measurements and observations on protocols or applications operating under controlled specific networking conditions. A set of end-to-end mechanisms, mainly focusing on transport and application-level protocols, are currently available. IREEL is scalable and easy to use thanks to an ergonomic web interface.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">27</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Laurent Dairaine</field>
<field name="title">Optimization of TFRC Loss History Initialization</field>
<field name="keyword">Optimization</field>
<field name="keyword"> TFRC</field>
<field name="keyword"> transport.</field>
<field name="abstract">This letter deals with the initialization of the loss history structure in the TFRC (TCP-Friendly Rate Control) mechanism. This initialization occurs after the detection of the first loss event after every slowstart phase. The loss history is crucial for the algorithm since it returns the packet loss rate estimation. This estimation is used in the TFRC equation to compute the sending rate. In this letter, we propose a new method to compute the packet loss rate which is more computationally efficient and remains as accurate as the classical commonly used method. The motivation of this work is to reduce the computation time and formulate a unified computation scheme. This method is based on the Newton s algorithm issued from numerical analysis of the TCP throughput equation. This proposal is evaluated analytically and the results show a significant improvement in terms of the computation time.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">28</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Patrick Senac</field>
<field name="title">Towards sender-based TFRC</field>
<field name="abstract">Pervasive communications are increasingly sent over mobile devices and personal digital assistants. This trend has been observed during the last football world cup where cellular phones service providers have measured a significant increase in multimedia traffic. To better carry multimedia traffic, the IETF standardized a new TCP Friendly Rate Control (TFRC) protocol. However, the current receiver-based TFRC design is not well suited to resource limited end systems. We propose a scheme to shift resource allocation and computation to the sender. This sender based approach led us to develop a new algorithm for loss notification and loss rate computation. We demonstrate the gain obtained in terms of memory requirements and CPU processing compared to the current design. Moreover this shifting solves security issues raised by classical TFRC implementations. We have implemented this new sender-based TFRC, named TFRClight, and conducted measurements under real world conditions.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">29</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Laurent Dairaine</field>
<field name="title">Study and enhancement of DCCP over DiffServ Assured Forwarding class</field>
<field name="abstract">The Datagram Congestion Control Protocol (DCCP) has been proposed as a transport protocol which supports real-time traffic. In this paper, we focus on the use of DCCP/CCID3 (Congestion Control ID 3) over a DiffServ/AF class. This class of service is used to build services that provide only a minimum throughput guarantee without any delay or jitter restrictions. This minimum throughput guarantee is called the target rate. In this context, the throughput obtained by DCCP/CCID3 mainly depends on RTT and loss probability. As a result, the application does not always get the negotiated target rate. To cope with this problem, we propose to evaluate a simple adaptation of the CCID3 congestion control mechanism, allowing the application to reach its target rate whatever the RTT value of the application s flow is. As this adaptation can be seen as an extension to the DCCP with CCID3 congestion control, we call it gDCCP for guaranteed DCCP. Results from simulations are presented to illustrate the improvements of the proposed modification in various situations. Finally, we investigate the deployment of this proposal in terms of security.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">30</field>
<field name="author">Martin Wehrle</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Planning as satisfiability with relaxed -step plans</field>
<field name="abstract">Planning as satisfiability is a powerful approach to solving domain independent planning problems. In this paper, we consider a relaxed semantics for plans with parallel operator application based on -step semantics. Operators can be applied in parallel if there is at least one ordering in which they can be sequentially executed. Under certain conditions, we allow them to be executed simultaneously in a state s even if not all of them are applicable in s. In this case, we guarantee that they are enabled by other operators that are applied at the same time point. We formalize the semantics of parallel plans in this setting, and propose an effective translation for STRIPS problems into the propositional logic. We finally show that this relaxed semantics yields an approach to classical planning that is sometimes much more efficient than the existing SAT-based planners.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">31</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Complexity of Concurrent Temporal Planning</field>
<field name="abstract">We consider the problem of temporal planning in which a given goal is reached by taking a number of actions which may temporally overlap and interfere, and the interference may be essential for reaching the goals.



We formalize a general temporal planning problem, show that its plan existence problem is EXPSPACE-complete, and give conditions under which it is reducible to classical planning and is therefore only PSPACE-complete. Our results are the first to show that temporal planning can be computationally more complex than classical planning. They also show how and why a very large and important fragment of temporal PDDL is reducible to classical planning.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">32</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Diagnosers and diagnosability of succinct transition systems</field>
<field name="abstract">Reasoning about the knowledge of an agent is an important problem in many areas of AI. For example in diagnosis a basic question about a system is whether it is possible to diagnose it, that is, whether it is always possible to know whether a faulty behavior has occurred. In this paper we investigate the complexity of this diagnosability problem and the size of automata that perform diagnosis.



There are algorithms for testing diagnosability in polynomial time in the number of states in the system. For succinct system representations, which may be exponentially smaller than the state space of the system, the diagnosability problem is consequently in EXPTIME. We show that this upper bound is not tight and that the decision problem is in fact PSPACE-complete.



On-line diagnosis can be carried out by diagnosers which are automata that recognize faulty behavior. We show that diagnosers in the worst case have a size that is exponential in the number of states, both for explicit and succinct system representations. This is a consequence of the diagnoser having to maintain beliefs about the state of the system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">33</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Asymptotically optimal encodings of conformant planning in QBF</field>
<field name="abstract">The world is unpredictable, and acting intelligently requires anticipating possible consequences of actions that are taken. Assuming that the actions and the world are deterministic, planning can be represented in the classical propositional logic. Introducing nondeterminism (but not probabilities) or several initial states increases the complexity of the planning problem and requires the use of quantified Boolean formulae (QBF).



The currently leading logic-based approaches to conditional planning use explicitly or implicitly a QBF with the prefix EAE. We present formalizations of the planning problem as QBF which have an asymptotically optimal linear size and the optimal number of quantifier alternations in the prefix:

EA and AE. This is in accordance with the fact that the planning problem (under the restriction to polynomial size plans) is on the second level of the polynomial hierarchy, not on the third.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">34</field>
<field name="author">Sarah Hickmott</field>
<field name="author">Jussi Rintanen</field>
<field name="author">Sylvie Thiebaux</field>
<field name="author">Lang White</field>
<field name="title">Planning via Petri net unfolding</field>
<field name="abstract">The factored state representation and concurrency semantics of Petri nets are closely related to those of concurrent planning domains, yet planning and Petri net analysis have developed independently, with minimal and usually unconvincing attempts at cross-fertilisation. In this paper, we investigate and exploit the relationship between the two areas, focusing on Petri net unfolding, which is an attractive reachability analysis method as it naturally enables the recognition and separate resolution of independent subproblems. On the one hand, based on unfolding, we develop a new forward search method for cost-optimal partial-order planning which can be exponentially more efficient than state space search. On the other hand, inspired by well-known planning heuristics, we investigate the automatic generation of heuristics to guide unfolding, resulting in a more efficient, directed reachability analysis tool for Petri nets.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">35</field>
<field name="author">Robert Mattm _ller</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Planning for temporally extended goals as propositional satisfiability</field>
<field name="abstract">Planning for temporally extended goals (TEGs) expressed as formulae of Linear-time Temporal Logic (LTL) is a proper generalization of classical planning, not only allowing to specify properties of a goal state but of the whole plan execution. Additionally, LTL formulae can be used to represent domain-specific control knowledge to speed up planning. In this paper we extend SAT-based planning for LTL goals (akin to bounded LTL model-checking in verification) to partially ordered plans, thus significantly increasing planning efficiency compared to purely sequential SAT planning. We consider a very relaxed notion of partial ordering and show how planning for LTL goals (without the next-time operator) can be translated into a SAT problem and solved very efficiently. The results extend the practical applicability of SAT-based planning to a wider class of planning problems. In addition, they could be applied to solving problems in bounded LTL model-checking more efficiently.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">36</field>
<field name="author">Alban Grastien</field>
<field name="author">Anbulagan</field>
<field name="author">Jussi Rintanen</field>
<field name="author">Elena Kelareva</field>
<field name="title">Diagnosis of discrete-event systems using satisfiability algorithms</field>
<field name="abstract">The diagnosis of a discrete-event system is the problem of computing possible behaviors of the system given observations of the actual behavior, and testing whether the behaviors are normal or faulty. We show how the diagnosis problems can be translated into the propositional satisfiability problem (SAT) and solved by algorithms for SAT. Our experiments demonstrate that current SAT algorithms can solve much bigger diagnosis problems than traditional diagnosis algorithms do.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">37</field>
<field name="author">Jussi Rintanen</field>
<field name="author">Alban Grastien</field>
<field name="title">Diagnosability testing with satisfiability algorithms</field>
<field name="abstract">We show how testing whether a system is diagnosable can be reduced to the satisfiability problem and how satisfiability algorithms yield a very efficient approach to testing diagnosability.



Diagnosability is the question whether it is always possible to know whether a given system has exhibited a failure behavior. This is a basic question that underlies diagnosis, and it is also closely related to more general questions about the possibility to know given facts about system behavior.



The work combines the twin plant construct of Jiang et al., which is the basis of diagnosability testing of systems with an enumerative representation, and SAT-based techniques to AI planning which form a very promising approach to finding paths in very large transition graphs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">38</field>
<field name="author">Feiselia Tan</field>
<field name="author">Youmna Borghol</field>
<field name="author">Sebastien Ardon</field>
<field name="title">EMO: A Statistical Encounter-based Mobility MOdel for Simulating Delay Tolerant Networks</field>
<field name="keyword">Delay Tolerant Networks</field>
<field name="keyword"> DTN</field>
<field name="keyword"> mobility models</field>
<field name="abstract">We propose EMO, a model to evaluate Delay Tolerant Networks (DTN) and opportunistic systems, which focuses on simulating encounter events between mobile radios, rather than node locations as done in existing models and simulators. Our approach introduces a more accurate simulation of DTNs on the main system timescale (the encounter timescale), while trading off some accuracy at the bit-level, through an abstraction of radio propagation simulation. To design EMO, we extract and characterize the necessary parameters from experimental

data and propose a method to generate synthetic node encounter traces based on this characterization. The output of the model is validated using hold-out cross validation method. Our validation results indicates that EMO is able to maintain the statistical properties of experimental data over a wide range of time (simulation duration) and space (number of nodes) scales, with mean square errors of less than 2% for the main system parameters.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">39</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Jing Chen</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Emmanuel Lochin</field>
<field name="title">Video TFRC</field>
<field name="keyword">complexity measure</field>
<field name="keyword"> congestion control</field>
<field name="keyword"> crosslayer design</field>
<field name="keyword"> H.264/AVC</field>
<field name="keyword"> rate control</field>
<field name="keyword"> TCP-friendly</field>
<field name="keyword"> multimedia streaming</field>
<field name="abstract">Abstract TCP-friendly rate control (TFRC) is a congestion control technique that trade-offs responsiveness to the network conditions for a smoother throughput variation. We take advantage of this trade-off by calculating the rate gap between the theoretical TCP throughput and the smoothed TFRC throughput. Any rate gain from this rate gap is then opportunistically used for video coding. We define a frame complexity measure to determine the additional rate to be used from the rate gap and then perform a rate negotiation to determine the target rate for the encoder and the final sending rate. Results show that although this method has a more aggressive sending rate compared to TFRC, it is still TCPfriendly, does not contribute too much to network congestion and achieves a reasonable video quality gain over the conventional method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">40</field>
<field name="author">Feiselia Tan</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Robert Hsieh</field>
<field name="title">The Impact of User Mobility Patterns on Opportunistic Content Distribution Network</field>
<field name="keyword">Content distribution</field>
<field name="keyword"> mobile communication</field>
<field name="keyword"> opportunistic network</field>
<field name="abstract">Understanding the impact of mobility on opportunistic network is a challenging problem. This paper focuses on analyzing the impact of specific type of mobility characteristic, namely user mobility patterns. We base our analysis on Opportunistic temporal-Pairing Access Network (OPAN), an opportunistic content distribution ramework that utilizes both pairings between nodes and infrastructure-based wireless network. Focusing our study to explore the impact of peak hour traffic (i.e. due to human mobility patterns involving routines and schedules) and hence node density (clustering) on opportunistic content distribution paradigm, we introduce two models based on a rail public transportation model, namely Random Train Model and Peak hour Train Model. Our simulation results show that peak hour traffic increases the downlink traffic (i.e. uses more downlink capacity) in OPAN however provides faster diffusion time for nodes to download content. Our simulation results further suggests that mobility patterns with high node clustering is more beneficial for content distribution in mobile opportunistic networks due to higher chance of forming direct pairing between nodes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">41</field>
<field name="author">Jun Yang</field>
<field name="author">Jian Zhang</field>
<field name="title">Offline Swimmer Cap Tracking Using Trajectory Interpolation</field>
<field name="keyword">offline tracking</field>
<field name="keyword"> trajectory interpolation</field>
<field name="abstract">In this paper, we present a preliminary attempt to solve the difficult problem of tracking swimmer cap in swimming videos to facilitate swimmer performance assessment. Due to the great challenges posed by moving camera and severe figure-background occlusions, an offline approach based on trajectory interpolation is adopted. Firstly, each frame is searched for hypothesized positions of the target cap using mean shift mode seeking. Secondly, most outliers due to ambiguities and noise are eliminated using lane constraints, and the hypothesis in the space-time volume are clustered into trajectory segments based on a spatial and temporal closeness criteria. Finally, cubic spline trajectory interpolation is used to infer the target cap position in occluded frames. Experiments show that satisfying tracking results are achieved by our approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">42</field>
<field name="author">An Zhao</field>
<field name="title">Robust Histogram-Based Object Tracking in Image Sequences</field>
<field name="keyword">object tracking</field>
<field name="keyword"> histogram model</field>
<field name="keyword"> feature space</field>
<field name="keyword"> Bhattacharyya Coefficient</field>
<field name="abstract">Object tracking is an important aspect of computer vision which has attracted considerable attention recently. In this paper, we first introduce our framework for object tracking. Then, a new mathematical model for tracking performance analysis is presented. In addition, five matrix filters are designed and adopted to realize robust and efficient object tracking. Finally, an approach for combining histograms with different feature spaces into one histogram is given. According to our extensive experimental results, we found that our approaches greatly improve the visual tracking results.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">43</field>
<field name="author">James Young</field>
<field name="author">Gregor Mcewan</field>
<field name="author">Saul Greenberg</field>
<field name="author">Ehud Sharlin</field>
<field name="title">Moving a Media Space into the Real World through Group-Robot Interaction</field>
<field name="abstract">New generation media spaces let group members see each other 

and share information, but are often static and separated from the 

physical world. To solve this problem, we propose the AIBO 

Surrogate a robotic interface for a media space group, allowing 

members to extend their group interactions into the physical, real 

world. Distributed group members see a first-person view of what 

the robot sees and can control its walking direction, gaze and 

actions. For members physically collocated with the robot the 

AIBO Surrogate provides physical presence and awareness: a tele- 

embodiment of the distributed group.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">44</field>
<field name="author">Carl Gutwin</field>
<field name="author">Saul Greenberg</field>
<field name="author">Roger Blum</field>
<field name="author">Jeff Dyck</field>
<field name="author">Kimberly Tee</field>
<field name="author">Gregor Mcewan</field>
<field name="title">Supporting Informal Collaboration in Shared-Workspace Groupware.</field>
<field name="keyword">Awareness</field>
<field name="keyword"> community-based groupware</field>
<field name="keyword"> real-time interaction</field>
<field name="keyword"> groupware.</field>
<field name="abstract">Shared-workspace groupware has not become common in the workplace, despite 

many positive results from research labs. One reason for this lack of success is that most shared 

workspace systems are designed around the idea of planned, formal collaboration sessions yet 

much of the collaboration that occurs in a co-located work group is informal and opportunistic. 

To support informal collaboration, groupware must be designed and built differently. We 

introduce the idea of community-based groupware (CBG), in which groupware is organized 

around groups of people working independently, rather than shared applications, documents, or 

virtual places. Community-based groupware provides support for three things that are 

fundamental to informal collaboration: awareness of others and their individual work, 

lightweight means for initiating interactions, and the ability to move into closely-coupled 

collaboration when necessary. We demonstrate three prototypes that illustrate the ideas behind 

CBG, and argue that this way of organizing groupware supports informal collaboration better 

than other existing approaches.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">45</field>
<field name="author">Sue Zhou</field>
<field name="author">Glynn Rogers</field>
<field name="author">Mike Hogan</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Tim Hu</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">An Incentive Based Routing Algorithm for Improving Message Forwarding in Structured Peer-to-Peer Networks</field>
<field name="keyword">peer-to-peer</field>
<field name="abstract">Much of existing research work in peer-to-peer networks is focused on architecture and protocols. All make assumptions that nodes in the network will operate for the common good without any selfinterests. However, most nodes in peer-to-peer networks act selfishly in the reality, which makes the doing-the-common-good assumption invalid. One of the major challenges in peer-to-peer networks is how to manage self-interest to encourage nodes to follow the designed protocols. In this paper, we propose an incentive based routing algorithm for improving

message forwarding in structured peer-to-peer networks. Using a parameter, called Participation Willingness (PW), we are able to introduce incentives for all network nodes. Furthermore, we apply the PW value to the routing algorithm of Chord and perform a simulation study. Simulation results showed that the proposed algorithm provides performance improvement over the original Chord routing algorithm.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">46</field>
<field name="author">Natalia Romero</field>
<field name="author">Gregor Mcewan</field>
<field name="author">Saul Greenberg</field>
<field name="title">A Field Study of Community Bar: (Mis)-matches between Theory and Practice.</field>
<field name="keyword">Locales</field>
<field name="keyword"> casual interaction</field>
<field name="keyword"> distributed groupware</field>
<field name="abstract">Community Bar (CB) is groupware supporting informal 

awareness and casual interaction. CB s design was derived from 

three sources: prior empirical research findings concerning 

informal awareness and casual interaction, a comprehensive 

sociological theory called the Locales Framework, and the 

Focus/Nimbus model of awareness. We conducted a field study of 

a group s on-going CB use. We use its results to reflect upon the 

matches and mis-matches that occurred between the theoretical 

and actual usage behaviors anticipated by our design principles 

vs. those observed in our deployment. As a critique, this reflection 

is an important iterative step in recognizing flaws not just as 

usability problems, but as an incorrect translation of theory into 

design that can be re-analyzed from a theoretical perspective. keep individuals informed about each other in social and 

professional contexts. They reinforce social bonds, and they make 

the transition to tightly-coupled collaboration easier. However, 

the same studies also found that casual interactions severely drop 

off when people are physically separated by even small distances 

[9,14]. Thus distributed communities of co-workers miss out on 

these valuable interaction opportunities. In response, developers 

have designed groupware that displays informal awareness 

information leading to casual interaction between distributed 

group members, e.g., Instant Messengers (IM) [12], chat rooms / 

MUDS [4], and video-based media spaces [1].</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">47</field>
<field name="author">Laurent Dairaine</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Sebastien Ardon</field>
<field name="title">IREEL: Remote Experimentation with Real Protocols and Applications over Emulated Network</field>
<field name="keyword">Network</field>
<field name="keyword"> Protocols</field>
<field name="keyword"> Emulation</field>
<field name="keyword"> Quality of service</field>
<field name="keyword"> TCP/IP</field>
<field name="abstract">This paper presents a novel e-learning platform called IREEL. IREEL is a virtual laboratory allowing students to drive experiments with real Internet applications and end-to-end protocols in the context of networking courses. This platform consists in a remote network emulator offering a set of predefined applications and protocol mechanisms. Experimenters configure and control the emulation and the end-systems behavior in order to perform tests, measurements, and observations on protocols or applications operating under controlled specific networking conditions. A set of end-to-end mechanisms, mainly focusing on transport and application-level protocols, are currently available. IREEL is scalable and easy to use thanks to an ergonomic web interface.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">48</field>
<field name="author">Feiselia Tan</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Max Ott</field>
<field name="title">UbiStore: Ubiquitous and Opportunistic Backup Architecture</field>
<field name="keyword">backup</field>
<field name="keyword"> opportunistic systems</field>
<field name="abstract">This paper presents UbiStore, a novel distributed and opportunistic backup architecture, where mobile

devices backup their data over short-range, ad-hoc wireless links to other devices encountered as a result of user mobility. We base our design on the assumption that typical user mobility patterns will incur some repetitive encounters in the course of daily life (public transports, home/office) which can facilitate the recovery of data in case of a device failure. This paper presents the UbiStore design, an evaluation framework, and early results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">49</field>
<field name="author">Knut Hueper</field>
<field name="author">Fatima Silva Leite</field>
<field name="title">On the Geometry of Rolling and Interpolation Curves on S^n , SO_n , and Grassmann Manifolds</field>
<field name="keyword">Rolling mapping</field>
<field name="keyword"> interpolation</field>
<field name="keyword"> sphere</field>
<field name="keyword"> orthogonal group</field>
<field name="keyword"> Grassmann manifold</field>
<field name="keyword"> parallel transport</field>
<field name="keyword"> geodesics</field>
<field name="keyword"> geometric splines</field>
<field name="keyword"> constrained variational problems</field>
<field name="keyword"> kinematic equation.</field>
<field name="abstract">We present a procedure to generate smooth interpolating curves on submanifolds, which are given in closed form in terms of the coordinates of the embedding space. In contrast to other existing methods, this approach makes the corresponding algorithm easy to implement. The idea is to project the prescribed data on the manifold onto the affine tangent space at a particular point, solve the interpolation problem on this affine subspace, and then project the resulting curve back on the manifold. One of the novelties of this approach is the use of rolling mappings. The manifold is required to roll on the affine subspace like a rigid body, so that the motion is described by the action of the Euclidean group on the embedding space. The interpolation problem requires a combination of a pullback/push forward with rolling and unrolling. The rolling procedure by itself highlights interesting properties and gives rise to a new, but simple, concept of geometric polynomial curves on manifolds. This paper is an extension of our previous work, where mainly the 2-sphere case was studied in detail. The present paper includes results for the n-sphere, orthogonal group SO_n, and real Grassmann manifolds. In particular, we present the kinematic equations for rolling these manifolds along curves without slip or twist, and derive from them formulas for the parallel transport of vectors along curves on the manifold.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">50</field>
<field name="author">Renato Iannella</field>
<field name="title">The Policy-Aware Web meets Virtual Goods</field>
<field name="keyword">policy</field>
<field name="keyword">aware web</field>
<field name="abstract">The Policy-Aware Web is the promise for supporting policy management at the Web infrastructure level and is now becoming a focus of new research for the Web. A policy is any set of rules or statements that capture and express the requirements of individuals and organisations from a corporate, legal, best practices, and/or social perspective. Currently, policy languages exist that cover and broadly address privacy, access control, and obligation management areas. However, what is missing is an overall framework and architecture allowing a combination of constraints and policy languages to interoperate and provide an accountable, enforceable, flexible and trusted experience for the web community. This talk will discuss how to identify the road forward towards the policy-aware web architecture and the role and impact on the virtual goods communities.



In particular, we look for answers and directions to the following debatable points of view:



 * Will the web community accept or need a policy-aware web?

 * What is the range of policies that should be supported?

 * Will the Semantic Web really help or hinder?

 * What standards need to be developed?

 * How to gracefully introduce the policy-aware web to the existing web?

 * How will users trust the policy-aware web?</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">51</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Bjorn Landfeldt</field>
<field name="title">Monitoring assisted robust routing for wireless mesh networks</field>
<field name="keyword">Anomaly detection</field>
<field name="keyword"> wireless mesh networks</field>
<field name="keyword"> robust routing</field>
<field name="keyword"> monitoring</field>
<field name="abstract">In this paper, we present a monitoring assisted routing scheme for wireless mesh networks which exploits the broadcast nature of wireless transmissions at special routers with added monitoring functionalities. These routers passively listen to the transmissions in neighborhood and compare the routing behavior against the routing state collectively maintained by them. If any inconsistency is found, as a result of software/hardware

malfunction, these routers try to determine the node causing it and recover from it in a timely fashion. The scheme is developed for wireless mesh networks where the communication overhead is a critical issue. The performance evaluation of our scheme shows considerable improvement in reliability (i.e., delivery ratio achieved by standard routing protocols) with minimal overhead under situations of malfunctions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">52</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Bjorn Landfeldt</field>
<field name="title">Monitoring assisted robust routing in wireless mesh networks</field>
<field name="keyword">Monitoring</field>
<field name="keyword"> robust routing</field>
<field name="keyword"> wireless mesh networks</field>
<field name="abstract">In this paper, we present a monitoring assisted robust routing scheme for wireless mesh networks which exploits the broadcast nature of wireless transmissions at special routers with added monitoring functionalities. These routers passively listen to the transmissions in their neighborhood and compare the routing behavior against the routing state collectively maintained by them. If any inconsistency is found, as a result of software/hardware malfunction, these routers try to determine the node causing it and recover from it in a timely fashion. The scheme is developed for wireless mesh networks where the communication overhead is a critical issue. The performance evaluation of our scheme shows considerable improvement in reliability (i.e., delivery ratio achieved by standard routing protocols) with minimal overhead under situations of malfunctions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">53</field>
<field name="author">Sara Hakami</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Bjorn Landfeldt</field>
<field name="author">Tim Moors</field>
<field name="title">Detection and identification of anomalies in wireless mesh networks using Principal Component Analysis (PCA)</field>
<field name="keyword">Principal component analysis</field>
<field name="keyword"> Anomaly detection</field>
<field name="keyword"> Wireless mesh networks</field>
<field name="abstract">Anomaly detection is becoming a powerful and necessary component as wireless networks gain popularity. In this paper, we evaluate the efficacy of PCA based anomaly detection for wireless mesh networks. PCA was originally developed

for wired networks. Our experiments show that it is possible to detect different types of anomalies in an interference

prone wireless environment. However, the sensitivity of PCA to small changes in flows prompted us to develop an

anomaly identification scheme which automatically identifies the flow(s) causing the detected anomaly and their contributions in terms of number of packets. Our results show that the identification scheme is able to differentiate false alarms from real anomalies and pinpoint the culprit(s) in case of a real fault or threat. The experiments were performed over an 8 node mesh testbed deployed in an urban street layout in Sydney, under different realistic traffic scenarios. Our identification scheme facilitates the use of PCA based method for real-time anomaly detection in wireless networks as it can filter the false alarms locally at the monitoring nodes without excessive computational overhead.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">54</field>
<field name="author">Sai Sun</field>
<field name="author">Renato Iannella</field>
<field name="title">Modeling and Integration of Disaster Situational Reports</field>
<field name="keyword">Information Modeling</field>
<field name="keyword"> Information Integration</field>
<field name="keyword"> crisis situational reports</field>
<field name="abstract">During disasters and emergencies, situational reports are generated to provide critical information on the current state, desired responses, and future needs of the incident. These reports are usually summarised and communicated to higher-levels for coordination and management (e.g. from local, to district, to state, to commonwealth levels). Correctly and effectively generating and processing these crisis reports plays a vital role in emergency services management. Currently, situational reports are primarily unstructured textual documents but usually follow a common pattern to their presentation and layout. The generation and processing of these reports is almost wholly manual, which inevitably causes the omission of information, low efficiency of manipulation, and hence, difficulty in decision support functions. Based on a case study from a disaster exercise, this paper proposes the information model of situational reports and further discusses the information integration and coordination of these reports, thus improving the performance of the disaster management system.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">55</field>
<field name="author">Ronnie Taib</field>
<field name="author">Natalie Ruiz</field>
<field name="title">Wizard of Oz for Multimodal Interfaces Design: Deployment Considerations</field>
<field name="keyword">Wizard of Oz</field>
<field name="keyword"> Multimodal user interface</field>
<field name="keyword"> Speech and gesture</field>
<field name="keyword"> User-centred design</field>
<field name="abstract">The use of Wizard of Oz (WOz) techniques for the acquisition of

multimodal interaction patterns is common, but often relies on highly or fully

simulated functionality. This paper suggests that a more operational WOz can

benefit multimodal interaction research. The use of a hybrid system containing

both fully-functional components and WOz-enabled components is an effective

approach, especially for highly multi-modal systems, and collaterally, for

cognitively loaded applications. The description of the requirements and

resulting WOz set-up created for a user study in a traffic incident management

application design is presented. We also discuss the impact of the ratio of

simulated and operational parts of the system dictated by these requirements, in

particular those related to multimodal interaction analysis.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">56</field>
<field name="author">Ronnie Taib</field>
<field name="author">Natalie Ruiz</field>
<field name="title">Integrating Semantics into Multimodal Interaction Patterns</field>
<field name="abstract">A user experiment on multimodal interaction (speech, hand position and

hand shapes) to study two major relationships: between

the level of cognitive load experienced by users and the resulting

multimodal interaction patterns; and how the semantics of the

information being conveyed affected those patterns. We found that as

cognitive load increases, users' multimodal productions tend to become

semantically more complementary and less redundant across

modalities. This validates cognitive load theory as a theoretical

background for understanding the occurrence of particular kinds of

multimodal productions. Moreover, results indicate a significant

relationship between the temporal multimodal integration pattern (7

patterns in this experiment) and the semantics of the command being

issued by the user (4 types of commands), shedding new light on

previous research findings that assign a unique temporal integration

pattern to any given subject regardless of the communication taking

place.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">57</field>
<field name="author">Knut Hueper</field>
<field name="author">Martin Kleinsteuber</field>
<field name="author">Fatima Silva Leite</field>
<field name="title">Rolling Stiefel Manifolds</field>
<field name="abstract">In this paper rolling maps for real Stiefel manifolds are studied. Real Stiefel manifolds being the set of all orthonormal k-frames of an n-dimensional real Euclidean space are compact manifolds. They are considered here as rigid bodies embedded in a suitable Euclidean space such that the corresponding Euclidean group acts on the rigid body by rotations and translations in the usual way. We derive the kinematic equations describing this rolling motion.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">58</field>
<field name="author">Gregor Mcewan</field>
<field name="author">Markus Rittenbruch</field>
<field name="author">Tim Mansfield</field>
<field name="title">Understanding Awareness in Mixed Presence Collaboration.</field>
<field name="keyword">Awareness</field>
<field name="keyword"> tabletop</field>
<field name="keyword"> mixed presence</field>
<field name="keyword"> presence disparity</field>
<field name="keyword"> display disparity.</field>
<field name="abstract">Mixed presence collaboration combines distributed and 

collocated collaboration there are multiple distributed sites, 

each with a collocated group. While collocated collaboration 

and purely distributed collaboration are each the subject of rich 

bodies of research, the combination is less well explored. In this 

paper we present our initial concepts of awareness support in 

mixed presence collaboration. We present this as a first version 

model of awareness. The selected literature we have used to 

inform the model is drawn from collocated research and 

distributed research as well as the small body of work 

addressing mixed presence collaboration directly. In this paper 

we present a discussion of this relevant literature and use it to 

explain our model. We also offer a sample of applying the 

model through the use of a scenario.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">59</field>
<field name="author">Hao Shen</field>
<field name="author">Knut Hueper</field>
<field name="author">Martin Kleinsteuber</field>
<field name="title">Local Convergence Analysis of FastICA and Related Algorithms</field>
<field name="keyword">Independent component analysis</field>
<field name="keyword"> FastICA</field>
<field name="keyword"> approximate Newton method</field>
<field name="keyword"> convergence analysis</field>
<field name="keyword"> principal fibre bundle.</field>
<field name="abstract">The FastICA algorithm is one of the most prominent methods to solve the problem of linear Independent Component Analysis (ICA). Although there have been several attempts to prove local convergence properties of FastICA, rigorous analysis is still missing in the community. The major difficulty of analysis is due to the well known sign flipping phenomenon of FastICA, which causes the discontinuity of the corresponding FastICA map on the unit sphere. In this paper, by using the concept of principal fibre bundles, FastICA is proven to be locally quadratically convergent to a correct separation. Higher order local convergence properties of FastICA are also investigated in the framework of a scalar shift strategy. Moreover, as a parallelised version of FastICA, the so-called QR FastICA algorithm, which employs the QR decomposition instead of the polar decomposition, is shown to share similar local convergence properties with the original FastICA.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">60</field>
<field name="author">Adeel Baig</field>
<field name="author">Lavy Libman</field>
<field name="author">Mahbub Hassan</field>
<field name="title">Fairness Control by Mobile Routers in On-Board Communication Networks</field>
<field name="abstract">Communication solutions for passengers on public transport vehicles, in the form of on-board networks connected to the Internet via a mobile router (MR) and a wireless link, are increasingly offered by public transport providers in many countries. An important challenge in such networks is to guarantee a fair access to the scarce wireless bandwidth by all on-board users, particularly in networks with multiple wireless interfaces. Accordingly, we consider an active fairness control scheme, where

the MR transparently overrides the TCP advertised receiver window size to limit the transmission rates of on-board TCP connections to their fair share. We focus on the overhead that this scheme places on the mobile router and the wireless interface, due to the need for the MR to estimate the round-trip time (RTT) of each on-board connection. We investigate the performance of three strategies that differ in the timing of RTT estimations, and show that a fairness target-based strategy, which continuously monitors the ongoing fairness index and invokes an RTT reestimation whenever it drops below a predetermined value, consistently achieves a significant improvement in fairness for only a mild overhead cost.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">61</field>
<field name="author">Ahmad Yar Khan Malik</field>
<field name="author">Lavy Libman</field>
<field name="author">Salil Kanhere</field>
<field name="author">Mahbub Hassan</field>
<field name="title">Analysis of Resource Reservation Aggregation in On-Board Networks</field>
<field name="abstract">The concept of providing mobile Internet connectivity for passengers in public transport vehicles, where users connect to a local network that attaches to the Internet via a mobile router and a wireless link, has become increasingly popular in recent years, as evidenced by the growing amount of commercially available systems and associated research and standardization activities. The challenge of providing wireless connectivity to networks in motion is compounded by the highly dynamic nature of the user population and the strict Quality-of-Service (QoS) requirements of many applications typical of such environments. As a result, several protocols extending Internet QoS support approaches to on-board mobile networks have been proposed in the past. In this paper, we focus on modeling and performance evaluation of periodical aggregation of resource reservation messages, which forms the basis of the On-Board RSVP protocol. We present a model consisting of a discrete-time, multiple-server and finite-capacity queueing system with bulk arrivals and departures, conduct a detailed analysis of the model, and use it to evaluate the performance of the resource reseration aggregation scheme in a practical scenario. The validity of our model is also backed by extensive simulation results.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">62</field>
<field name="author">Kun-chan Lan</field>
<field name="author">Zhe Wang</field>
<field name="author">Rod Berriman</field>
<field name="author">Tim Moors</field>
<field name="author">Mahbub Hassan</field>
<field name="author">Lavy Libman</field>
<field name="author">Max Ott</field>
<field name="author">Bjorn Landfeldt</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Douglas Quail</field>
<field name="title">Implementation of a Wireless Mesh Network Testbed for Traffic Control</field>
<field name="abstract">Wireless mesh networks (WMN) have attracted considerable interest in recent years as a convenient, flexible and low-cost alternative to wired communication infrastructures in many contexts. However, the great majority of research on metropolitan-scale WMN has been centered around maximization of available bandwidth, suitable for non-real-time applications such as Internet access for the general public. On the other hand, the suitability of WMN for mission-critical infrastructure applications remains by and large unknown, as protocols typically employed in WMN are, for the most part, not designed for real-time communications. In this paper, we describe the Smart Transport and Roads Communications (STaRComm) project at National ICT Australia (NICTA), which sets a goal of designing a wireless mesh network architecture to solve the communication needs of the traffic control system in Sydney, Australia. This system, known as SCATS (Sydney Coordinated Adaptive Traffic System) and used in over 100 cities around the world, connects a hierarchy of several thousand devices from individual traffic light controllers to regional computers and the central Traffic Management Centre (TMC) and places stringent requirements on the reliability and latency of the data exchanges. We discuss our experience in the deployment of an initial testbed consisting of 7 mesh nodes placed at intersections with traffic lights, and share the results and insights learned from our measurements and initial trials in the process.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">63</field>
<field name="author">Lixiang Xiong</field>
<field name="author">Lavy Libman</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Optimal Strategies for Cooperative MAC-Layer Retransmission in Wireless Networks</field>
<field name="abstract">The concept of cooperative retransmission in wireless networks has attracted considerable research attention. The basic idea is that when a receiver cannot decode a frame, the retransmission is handled not by its original source but rather by a neighbour that overheard the transmission successfully, and may have a better channel to the destination. However, the majority of existing literature tackles the issue from the physical layer perspective, with either a single cooperating neighbour, or a multipleneighbour setting where the receiver is capable of combining and decoding the signal from several simultaneous retransmissions. In this paper, we consider the case of multiple cooperating neighbours from a MAC-layer perspective. Thus, we assume a receiver that can only decode one transmission at a time, while multiple simultaneous retransmissions (by several neighbours that had overheard the frame successfully) will cause a collision. As a result, each neighbour with a successfully overheard copy of the frame faces a tradeoff between helping with a cooperative retransmission and possibly causing a collision. Accordingly, we pose the optimization problem of finding a distributed randomized strategy for the cooperating neighbours, which assigns a certain retransmission probability to every neighbour in each time slot, so as to minimize the expected latency until successful reception. We analyse the performance achieved by two approaches: one where the original source is silent while the neighbours conduct their cooperative retransmissions, and another where both the

source and the neighbours may have a nonzero retransmission probability simultaneously. We show that the latter approach offers a significant performance improvement over the former one, as well as either traditional retransmission or two-hop routing to the destination.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">64</field>
<field name="author">Lixiang Xiong</field>
<field name="author">Lavy Libman</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Distributed strategies for Minimum-Latency Cooperative Retransmission in Wireless Networks</field>
<field name="abstract">Despite the considerable research literature on cooperative diversity in the physical layer in recent years, related concepts at higher layers have not received nearly as much attention. This paper focuses on cooperative retransmission at the MAC layer, the idea being that when a frame fails to reach the receiver, its retransmission is handled not by the original source but rather by common neighbours that overheard the transmission successfully, and may have a better channel to the destination. Unlike the physical layer, where taking advantage of cooperative diversity requires the receiver to be capable of combining and decoding the signals from several sources, the MAC-layer context assumes a plain receiver that can only decode one transmission at a time, whereas simultaneous cooperative retransmissions by multiple neighbours will cause a collision. Accordingly, most of the existing literature in this context considers mechanisms for choosing and coordinating the cooperation of a single best neighbour. In contrast, we consider a distributed randomized framework, where each neighbour with a successfully overheard copy of the frame independently faces a tradeoff between helping with a cooperative retransmission and possibly causing a collision. We pose the optimization problem of finding a distributed strategy, assigning a retransmission probability to every neighbour in each time slot, so as to minimize the expected latency until successful reception. We then propose a heuristic solution that is based on greedy maximization of the success probability in each transmission attempt, combined with tracking the probability distribution of the system state , and provide a detailed analysis based on a Markov fading channel model. We evalute our approach in a wide variety of scenarios, and demonstrate that, in general, it achieves a significant performance improvement over either traditional retransmission or two-hop routing to the destination.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">65</field>
<field name="author">Alfandika Nyandoro</field>
<field name="author">Lavy Libman</field>
<field name="author">Mahbub Hassan</field>
<field name="title">Service Differentiation Using the Capture Effect in 802.11 Wireless LANs</field>
<field name="keyword">Wireless LAN</field>
<field name="keyword"> IEEE 802.11</field>
<field name="keyword"> Quality-of-Service (QoS)</field>
<field name="keyword"> QoS differentiation</field>
<field name="keyword"> transmission power control</field>
<field name="keyword"> capture effect.</field>
<field name="abstract">We investigate the effects of using dual transmission power to achieve quality-of-service (QoS) differentiation in IEEE 802.11 wireless LANs. Specifically, we assume all stations employ the standard IEEE 802.11 distributed coordination function (DCF), with the exception that some stations transmit at a higher power than others. Consequently, in the event of a collision involving frames from both power levels, a high-power frame is more likely to be received correctly as a result of the so-called capture effect (i.e. received at a power sufficiently higher than that of the interference, allowing it to be decoded correctly). This effectively leads to QoS differentiation between two classes, corresponding to the high-power and low-power stations. We develop a Markov model for the IEEE 802.11 DCF in a network with dual transmission power over a Rayleigh-fading channel, and use it to evaluate the resulting performance, in terms of the key metrics of throughput and delay. We explore how the performance of the service classes depends on the proportion of stations in each class and the transmission power ratio, focusing in particular on the bounds achieved in the limit case of ideal differentiation , characterised by a perfect capture probability of high-power frames. We find that significant performance differentiation between high-power and low-power stations is achieved even with transmission power ratios that are not very high, leading us to conclude that employing multiple transmission power levels is a viable and efficient approach for service quality differentiation in wireless LANs.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">66</field>
<field name="author">Sarfraz Nawaz</field>
<field name="author">Sanjay Jha</field>
<field name="title">A graph drawing approach to sensor network localization</field>
<field name="keyword">Localization</field>
<field name="keyword"> Location</field>
<field name="keyword"> Sensor Networks</field>
<field name="keyword"> Graph Drawing</field>
<field name="abstract">In this paper, we propose an anchor free localization mechanism for wireless sensor networks. Our

algorithm is based on a graph drawing approach and uses inter-node distances to localize sensor

nodes in a local coordinate system up to a global translation, rotation and reflection without any absolute reference positions such as GPS or other anchor nodes. We show that it is possible to avoid folds and flips in the localized network layout by introducing long range constraints among non adjacent nodes which can be derived from inter-node distance measurements between adjacent nodes. We evaluate the effect of different parameters like network shape, scale, average neighbors and ranging noise on our algorithm and compare it with an anchor based and a number of anchor free schemes. We also present experimental results from an actual sensor network showing the accuracy of our approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">67</field>
<field name="author">Sarfraz Nawaz</field>
<field name="author">Sanjay Jha</field>
<field name="title">Collaborative Localization for Wireless Sensor Networks</field>
<field name="keyword">Localization</field>
<field name="keyword"> Location</field>
<field name="keyword"> Sensor Networks</field>
<field name="abstract">In this paper, we present a self sufficient and distributed localization algorithm for wireless sensor networks that does not require any external infrastructure support to determine sensor node coordinates. Our algorithm uses measured inter-node distances and localizes the entire network in a local coordinate system upto a global translation, rotation and reflection. Using TOSSIM simulations, we show that our approach consumes only $0.1\%$ of battery energy and exhibits smaller localization errors as compared to traditional approaches. The main contribution of this work is a multi-phase approach that allows the resource constrained sensor nodes to collaborate with each other and solve the localization problem for the entire network.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">68</field>
<field name="author">Anastasia Bezerianos</field>
<field name="title">Using Alternative Views for Layout, Comparison and Context Switching Tasks in Wall Displays</field>
<field name="keyword">wall displays</field>
<field name="keyword"> interaction</field>
<field name="keyword"> alternative views</field>
<field name="abstract">In this paper we first present a set of tasks that are relevant to wall display interaction. Among these, layout management, context switching and comparison tasks could benefit from the use of interactive shortcut views of remote areas of a wall display, presented close to the user. Such a shortcut view technique, the ScaleView portals, is evaluated against using a simple magnification lens and walking when performing these tasks. We observed that 

for a layout and comparison task with frequent context switching, users preferred ScaleView portals. But for simpler tasks, such as searching, regular magnification lenses and walking were preferred. General observations on how the display was used as a peripheral reference by 

different participants highlighted one of the benefits of using wall sized displays: users may visually refer to the large, spread out content on the wall display, even if they prefer to interact with it close to their location.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">69</field>
<field name="author">Nathalie Henry</field>
<field name="author">Anastasia Bezerianos</field>
<field name="author">Jean-Daniel Fekete</field>
<field name="title">Using Node Duplications to Improve the Readability of Clustered Social Networks</field>
<field name="keyword">Clustering</field>
<field name="keyword"> Graph Visualization</field>
<field name="keyword"> Node Duplications</field>
<field name="keyword"> Social Networks</field>
<field name="abstract">Exploring communities is an important task in social network analysis. Such communities are currently identified using clustering methods to group actors. This approach leads to actors belonging to one and only one cluster, whereas in real life an actor can belong to several communities. This article discusses issues raised by duplicating actors in a social

network. It presents several visual designs for presenting duplicated actors and a controlled experiment comparing network visualization with and without duplication, performed on 6 tasks that are important for graph readability and visual interpretation of social networks. We show that in our experiment, duplications significantly improve community-related tasks but sometimes interfere with other important tasks. Finally, we propose a set of guidelines for choosing candidate actors for duplication and discuss alternative way to render them in social network representations.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">70</field>
<field name="author">Anastasia Bezerianos</field>
<field name="author">Gregor Mcewan</field>
<field name="title">Presence Disparity in Mixed Presence Collaboration</field>
<field name="keyword">Tabletop</field>
<field name="keyword"> mixed presence</field>
<field name="keyword"> collaboration</field>
<field name="keyword"> presence disparity</field>
<field name="abstract">We present the design of an experiment investigating presence disparity in mixed presence collaboration using digital tabletops. In an attempt to verify previous work and relate their results, we examined different presence representations of remote collaborators: audio, video, telepointers and video arms. Our early results show some interesting trends that we are currently investigating in more detail through further analysis of our data.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">71</field>
<field name="author">Anastasia Bezerianos</field>
<field name="author">Gregor Mcewan</field>
<field name="title">Presence Disparity in Mixed Presence Collaboration</field>
<field name="keyword">Tabletop</field>
<field name="keyword"> mixed presence</field>
<field name="keyword"> collaboration</field>
<field name="keyword"> presence disparity</field>
<field name="abstract">We present the design of an experiment investigating 

presence disparity in mixed presence collaboration 

using digital tabletops. In an attempt to verify previous 

work and relate their results, we examined different 

presence representations of remote collaborators: 

audio, video, telepointers and video arms. Our early 

results show some interesting trends that we are 

currently investigating in more detail through further 

analysis of our data.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">72</field>
<field name="author">Ross Jeffery</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Colin Connaughton</field>
<field name="title">Misleading metrics and unsound analyses</field>
<field name="abstract">Using corporate project data, the authors show how an international standard's recommendations for analyzing productivity are inappropriate and, when incorporated with statistical process-control techniques, can generate misleading analyses.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">73</field>
<field name="author">Michael Maher</field>
<field name="author">Ge Huang</field>
<field name="title">On Computing Constraint Abduction Answers</field>
<field name="abstract">We address the problem of computing and representing answers

of constraint abduction problems over the Herbrand domain.

This problem is of interest when performing type inference

involving generalized algebraic data types (GADTs).

We show that simply recognizing a maximally general answer 

or fully maximal answer is co-NP complete.

However we present an algorithm that computes the

(finite) set of fully maximal answers of an abduction problem.

The maximally general answers are generally infinite in number

but we show how to generate a finite representation of them

when only unary function symbols are present.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">74</field>
<field name="author">Michael Maher</field>
<field name="title">Local Consistency for Extended CSPs</field>
<field name="abstract">We extend the framework of Constraint Satisfaction Problems to make it more suitable for/applicable to modern constraint programming languages where both constraint satisfaction and constraint solving have a role. Some rough principles for local consistency conditions are developed, appropriate notions of local consistency are formulated, and relationships betwen the various consistency conditions are established.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">75</field>
<field name="author">Rafal Kolanski</field>
<field name="title">A Logic for Virtual Memory</field>
<field name="keyword"/>
<field name="abstract">We present an extension to classical

separation logic which allows reasoning about virtual memory. Our logic is

formalised in the Isabelle/HOL theorem prover in a manner allowing

classical separation logic notation to be used at an abstract

level. We demonstrate that in the common cases, such as user

applications, our logic reduces to classical separation logic. At the

same time we can express properties about page tables, direct physical

memory access, virtual memory access, and shared memory in detail.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">76</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">A Comparisonal Study of the Multi-Layer Kohonen Self-Organizing Feature Maps for Spoken Language Identification</field>
<field name="keyword">Language identification</field>
<field name="keyword"> hierarchical multi-layer Kohonen self-organizing feature map</field>
<field name="keyword"> modified group delay function</field>
<field name="abstract">Our previous research indicates that the multi-layer Kohonen self-organizing feature map (MLKSFM) gives a promising performance for spoken language identification (LID). In this paper, we enhance this approach in two distinct ways. Firstly, by considering the phase information, we propose a new type of feature vector which combines the modified group delay function (MODGDF) and the traditional MFCC. Secondly, we propose a hierarchical structure of the MLKSFM, in which the pre-classification is performed at the lower level MLKSFM and the final language identification is performed at the top level MLKSFM. For the OGI-TS speech corpus, the best LID rate is achieved at 87.3% for the 45-sec test speech utterances by using the hierarchical MLKSFM with 4 classes pre-classified at the lower level MLKSFM. For the 10-sec test speech utterances, the best LID rated is achieved at 60.0% by using the non-hierarchical MLKSFM LID system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">77</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">Automatic Language Recognition with Tonal and Non-tonal Language Pre-classification</field>
<field name="keyword">Language identification</field>
<field name="keyword"> PPRLM</field>
<field name="keyword"> Witten-Bell discounting</field>
<field name="abstract">Previous research indicates that the Parallel Phoneme Recognition followed by Language Modelling (PPRLM) system provides state of the art language identification performance on conversational telephone speech. In this paper, in order to improve the recognition accuracy and save the amount of CPU run time while handling large number of languages, an innovative method for tonal and non-tonal language pre-classification by using prosodic information is reported. Also, by incorporating different confidence measures into the traditional PPRLM framework, an optimized language recognition system that can be applied in an open-set language recognition task is proposed. For the task of 12 target languages and 4 non-target languages, our results show that with the optimized preclassification, Universal Background Phone Model confidence measuring and Witten-Bell discounting, the system can achieve recognition accuracy rates of 77.9% for 30-sec speech segments and 49.2% for 10-sec speech segments.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">78</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">Multi-Layer Kohonen s Self-Organizing Feature Map for Language Identification</field>
<field name="keyword">language identification</field>
<field name="keyword"> multi-layer Kohonen s self-organizing feature map</field>
<field name="keyword"> histogram equalization</field>
<field name="abstract">In this paper we describe a novel use of multi-layer Kohonen s self-organizing feature map (MLKSFM) for spoken language identification (LID). A normalized, segment based input feature vector is used in order to maintain the temporal information of speech signal. The LID is performed by using different system configurations of the MLKSFM. Compared with a baseline PPRLM system, our novel system is capable of achieving a similar identification rate, but requires less training time and no labeled training data. The MLKSFM with the sheet shaped map and the hexagonal lattice neighborhoods relationship is found to give the best performance for the LID task, and this system is able to achieve a LID rate of 76.4% and 62.4% for the 45-sec and 10-sec OGI speech utterances, respectively.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">79</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">A Novel Method for Automatic Tonal and Non-tonal Language Classification</field>
<field name="keyword">tonal language classification</field>
<field name="keyword"> language identification</field>
<field name="abstract">This paper describes a novel method for tonal and non-tonal language classification using prosodic information. Normalized feature parameters that measure the speed and level of pitch change are used to perform the classification task. To demonstrate the effectiveness of the proposed method, the classification rates of different system configurations are compared. Evaluating a 16-language classification task using a GMM classifier, the novel system can achieve a classification rate of 87.1% for 45-sec speech segments and 81.9% for 10-sec speech segments. Possible applications of the new method to perform pre-classification in language identification are also discussed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">80</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">Language Identification Using Modified MLKSFM for Pre-classification with Novel Front-end Features</field>
<field name="keyword">Language identification</field>
<field name="keyword"> Hilbert-Huang transform</field>
<field name="keyword"> pre-classification</field>
<field name="keyword"> modified MLKSFM</field>
<field name="keyword"> modified group delay function</field>
<field name="abstract">This work presents two contributions to automatic language identification. The first contribution is the use of the modified multi-layer Kohonen self-organizing feature map (MLKSFM) as a pre-classification for language identification (LID). As a second contribution, we discuss the novel application of the Hilbert-Huang transform (HHT) to generate features for the LID pre-classification task. The instantaneous frequency (IF) and instantaneous amplitude (IA) of a speech signal are examined in the pre-classification. The experiment results on a 16-language speech database indicates that, the HHT by itself cannot perform well in the LID pre-classification task, however it helps improve the pre-classification rate when concatenated with other cepstral features. The final LID rate is also increased when the pre-classification is applied. We achieve the best LID rates of 85.2% and 62.3% for 45-sec and 10-sec test utterances, respectively.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">81</field>
<field name="author">Fang Chen</field>
<field name="author">Ronnie Taib</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Yu Shi</field>
<field name="author">David Yee</field>
<field name="title">User Interface Design for Traffic Incident Management Systems</field>
<field name="keyword">Traffic incident handling</field>
<field name="keyword"> computerised incident form</field>
<field name="keyword"> traffic control room</field>
<field name="abstract">Effective and efficient road traffic incident handling is vital to the overall operations of transport management. This task requires operators having fast and accurate means of capturing various incident conditions. In this paper, we describe the project activities of a collaboration that sought to design, develop, and evaluate a new user interface of a computerised incident input form, which will be deployed in the control room of the New South Wales Roads and Traffic Authority Transport Management Centre. A series of user studies has revealed that the new input form design is well ranked by the operators.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">82</field>
<field name="author">Fang Chen</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Ning Wang</field>
<field name="title">Exploiting Speech-Gesture Correlation in Multimodal Interaction</field>
<field name="keyword">speech analysis</field>
<field name="keyword"> prosodic features</field>
<field name="keyword"> deictic gesture</field>
<field name="keyword"> multimodal user interaction</field>
<field name="abstract">This paper introduces a study about deriving a set of quantitative relationships between speech and co-verbal gestures for improving multimodal input fusion. The initial phase of this study explores the prosodic features of two human communication modalities, speech and gestures, and investigates the nature of their temporal relationships. We have studied a corpus of natural monologues with respect to frequent deictic hand gesture strokes, and their concurrent speech prosody. The prosodic features from the speech signal have been co-analyzed with the visual signal to learn the correlation of the prominent spoken semantic units with the corresponding deictic gesture strokes. Subsequently, the extracted relationships can be used for disambiguating hand movements, correcting speech recognition errors, and improving input fusion for multimodal user interactions with computers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">83</field>
<field name="author">Xiwei (Sherry) Xu</field>
<field name="author">Liming Zhu</field>
<field name="author">Jenny Liu</field>
<field name="author">Mark Staples</field>
<field name="title">Resource-Oriented Business Process Modeling for Ultra-Large-Scale Systems</field>
<field name="abstract">REpresentational State Transfer (REST) [3] and the resource-oriented viewpoint are considered to be the guiding principles behind the WWW ULS ecosystem. RESTful principles are responsible for many of the desirable ULS quality attributes achieved, such as loose-coupling, reliability, data visibility and interoperability. However, many exiting Web-based or service-oriented applications (WSDL/SOAP-based) only use WWW/HTTP as a tunneling protocol or abuse URL and POX (Plain Old XML) by encoding method semantics in them. These applications are designed as fine-grained distributed Remote Procedure Calls (RPC), breaking many of the REST principles, and are subsequently harmful to the overall ULS system health. The debate on REST versus SOAP-based Big Web services has been raging in the industry. We observe that the main problems lie in two areas: 1) conceptually modeling process-centric business applications using a resource-oriented viewpoint promoted by the REST principles; and 2) decentralizing a workflow-based business process (e.g. BPEL) into distributed and dynamic process fragments. In this paper, we propose a solution to these two problems. Our approach aligns process-intensive applications with the basic Web principles and promotes dynamic and distributed process coordination.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">84</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Matthias Fruth</field>
<field name="author">Annabelle McIver</field>
<field name="title">Graphical modelling for simulation and formalanalysis of wireless network protocols</field>
<field name="keyword">model checking</field>
<field name="keyword"> PRISM</field>
<field name="keyword"> wireless networks</field>
<field name="abstract">It is well-known that the performance of wireless protocols depends on the quality of the wireless links, which in turn is affected by the network topology. The aim of this paper is to investigate the use of probabilistic model checking in the analysis of performance of wireless protocols, using a probabilistic abstraction of wireless unreliability. Our main contributions are first, to show how to formalise wireless link unreliability via probabilistic behaviour derived from the current best analytic models , and second, to show how such formal models can be generated automatically from a graphical representation of the network, and analysed with the PRISM model checker.



We also introduce CaVi, a graphical specification tool, which reduces the specification task to the design of the network layout, and provides a uniform design interface linking model checking with simulation. We illustrate our techniques with a randomised gossiping protocol.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">85</field>
<field name="author">Jia Meng</field>
<field name="author">Lawrence C. Paulson</field>
<field name="author">Gerwin Klein</field>
<field name="title">A Termination Checker for Isabelle Hoare Logic</field>
<field name="abstract">Hoare logic is widely used for software specification and verification. Frequen tly we need to prove the total correctness of a program: to prove that the progr am not only satisfies its pre- and post-conditions but also terminates. We have implemented a termination checker for Isabelle's Hoare logic. The tool can be used as an oracle, where Isabelle accepts its claim of termination. The tool can also be used as an Isabelle method for proving the entire total correctness spe cification. For many loop structures, verifying the tool's termination claim wit hin Isabelle is essentially automatic.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">86</field>
<field name="author">Harvey Tuch</field>
<field name="author">Gerwin Klein</field>
<field name="author">Michael Norrish</field>
<field name="title">Types, Bytes, and Separation Logic</field>
<field name="keyword">Separation Logic</field>
<field name="keyword"> C</field>
<field name="keyword"> Interactive Theorem Proving</field>
<field name="keyword"> Isabelle</field>
<field name="abstract">We present a formal model of memory that both captures the low-level features of C's pointers and memory, and that forms the basis for an expressive implementation of separation logic. At the low level, we do not commit common oversimplifications, but correctly deal with C's model of programming language values and the heap. At the level of separation logic, we are still able to reason abstractly and efficiently. We implement this framework in the theorem prover Isabelle/HOL and demonstrate it on two case studies. We show that the divide between detailed and abstract does not impose undue verification overhead, and that simple programs remain easy to verify. We also show that the framework is applicable to real, security- and safety-critical code by formally verifying the memory allocator of the L4 microkernel.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">87</field>
<field name="author">Luping Zhou</field>
<field name="author">Richard Hartley</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="author">Kaarin Anstey</field>
<field name="author">Nicolas Cherbuin</field>
<field name="author">Perminder Sachdev</field>
<field name="title">A study of hippocampal shape difference between genders 

by efficient hypothesis test and discriminative deformation</field>
<field name="abstract">Hypothesis testing is an important way to detect the statistical

difference between two populations. In this paper, we use the Fisher

permutation and bootstrap tests to differentiate hippocampal shape between

genders. These methods are preferred to traditional hypothesis

tests which impose assumptions on the distribution of the samples. An

efficient algorithm is adopted to rapidly perform the exact tests. We extend

this algorithm to multivariate data by projecting the original data

onto an informative direction to generate a scalar test statistic. This

 informative direction is found to preserve the original discriminative

information. This direction is further used in this paper to isolate the

discriminative shape difference between classes from the individual variability,

achieving a visualization of shape discrepancy.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">88</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Comparative Colorimetric Simulation and Evaluation of Digital Cameras Using Spectroscopy Data</field>
<field name="abstract">In this paper, we present a comparative simulation and evaluation of digital cameras using spectroscopy, colorimetry and photogrammetry. Here, we compare the output of commercially available cameras to that yielded using the CIE-1931 colour standard and the colour matching functions proposed by Stiles and Burch [14]. We present a method to estimate the colour yielded by trichromatic cameras based upon their spectral response. We then present the results on three experimental vehicles and three commercially available cameras. We perform simulation using the spectrum of light emitted by black body radiators, the spectra of sample colours widely used for colour correction and present renderings using the spectra of known materials under controlled illumination conditions.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">89</field>
<field name="author">Niklas Pettersson</field>
<field name="author">Lars Petersson</field>
<field name="author">Lars Andersson</field>
<field name="title">The Histogram Feature -- A Resource-Efficient Weak Classifier</field>
<field name="keyword">Object detection</field>
<field name="keyword"> computer vision</field>
<field name="keyword"> boosted classifiers</field>
<field name="keyword"> HOG</field>
<field name="keyword"> Histograms of oriented gradients</field>
<field name="abstract">This paper presents a Weak Classifier that is extremely fast to compute,

yet highly discriminant. This Weak Classifier may be used in, for example,

a boosting framework and is the result of a novel way of organizing and 

evaluating Histograms of Oriented Gradients. The method requires only 

one access to main memory to evaluate each feature, in comparison with 

the more well-known Haar features which require somewhere between six

and nine memory accesses to evaluate each feature. This low memory

bandwidth makes the Weak Classifier especially ideal for use in small

systems with little or no memory cache available.



The presented Weak Classifier has been extensively tested in a boosted

framework on data sets consisting of pedestrians and various road signs.

The classifier yields detection results that are far superior than the

results obtained from Haar features when tested on road signs and similar

structures, whereas the detection results are comparable to those of 

Haar features when tested on pedestrians. In addition, the computational

resources necessary for these results have been shown to be

considerably smaller for the new weak classifier.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">90</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Matthew Hennessy</field>
<field name="author">Carroll Morgan</field>
<field name="author">Cuicui Zhang</field>
<field name="title">Remarks on Testing Probabilistic Processes</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> CSP</field>
<field name="keyword"> transition systems</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> simulation</field>
<field name="keyword"> complete axiomatisations</field>
<field name="keyword"> structural operational semantics.</field>
<field name="abstract">We develop a general testing scenario for probabilistic processes, giving rise to two theories: probabilistic may testing and probabilistic must testing. These are applied to a simple probabilistic version of the process calculus CSP. We examine the algebraic theory of probabilistic testing, and show that many of the axioms of standard testing are no longer valid in our probabilistic setting; even for non-probabilistic CSP processes, the distinguishing power of probabilistic tests is much greater than that of standard tests. We develop a method for deriving inequations valid in probabilistic may testing based on a probabilistic extension of the notion of simulation. Using this, we obtain a complete axiomatisation for non-probabilistic processes subject to probabilistic may testing.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">91</field>
<field name="author">Luping Luping.zhou</field>
<field name="author">Nick Barnes</field>
<field name="author">Richard Hartley</field>
<field name="title">A Study of Hippocampal Shape Difference Between Genders by Efficient Hypothesis Test and Discriminative Deformation</field>
<field name="abstract">Hypothesis testing is an important way to detect the statistical difference between two populations. In this paper, we use the Fisher permutation and bootstrap tests to differentiate hippocampal shape between genders. These methods are preferred to traditional hypothesis tests which impose assumptions on the distribution of the samples. An efficient algorithm is adopted to rapidly perform the exact tests. We extend this algorithm to multivariate data by projecting the original data onto an informative direction to generate a scalar test statistic. This informative direction is found to preserve the original discriminative information. This direction is further used in this paper to isolate the discriminative shape difference between classes from the individual variability, achieving a visualization of shape discrepancy.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">92</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Carroll Morgan</field>
<field name="author">Chenyi Zhang</field>
<field name="title">Scalar Outcomes Suffice for Finitary Probabilistic Testing</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> probabilistic automata</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> reward testing</field>
<field name="keyword"> hyperplanes</field>
<field name="keyword"> compactness</field>
<field name="keyword"> Markov Decision Processes.</field>
<field name="abstract">The question of equivalence has long vexed research in concurrency, leading to many different denotational- and bisimulation-based approaches; a breakthrough occurred with the insight that tests expressed within the concurrent framework itself, based on a special ``success action'', yield equivalences that make only inarguable distinctions.



When probability was added, however, it seemed necessary to extend the testing framework beyond a direct probabilistic generalisation in order to remain useful. An attractive possibility was the extension to multiple success actions that yielded vectors of real-valued outcomes.



Here we prove that such vectors are unnecessary when processes are finitary, that is finitely branching and finite-state: single scalar outcomes are just as powerful. Thus for finitary processes we can retain the original, simpler testing approach and its direct connections to other naturally scalar-valued phenomena.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">93</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Matthew Hennessy</field>
<field name="author">Carroll Morgan</field>
<field name="author">Cuicui Zhang</field>
<field name="title">Characterising Testing Preorders for Finite Probabilistic Processes</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> CSP</field>
<field name="keyword"> transition systems</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> simulation</field>
<field name="keyword"> failure simulation</field>
<field name="keyword"> modal characterisations</field>
<field name="keyword"> complete axiomatisations.</field>
<field name="abstract">In 1992 Wang &amp; Larsen extended the may- and must preorders of De Nicola and Hennessy to processes featuring probabilistic as well as nondeterministic choice. They concluded with two problems that have remained open throughout the years, namely to find complete axiomatisations and alternative characterisations for these preorders. This paper solves both problems for finite processes with silent moves. It characterises the may preorder in terms of simulation, and the must preorder in terms of failure simulation. It also gives a characterisation of both preorders using a modal logic. Finally it axiomatises both preorders over a probabilistic version of CSP.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">94</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Matthew Hennessy</field>
<field name="author">Carroll Morgan</field>
<field name="title">Characterising Testing Preorders for Finite Probabilistic Processes</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> CSP</field>
<field name="keyword"> transition systems</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> simulation</field>
<field name="keyword"> failure simulation</field>
<field name="keyword"> modal characterisations</field>
<field name="keyword"> complete axiomatisations.</field>
<field name="abstract">In 1992 Wang &amp; Larsen extended the may- and must preorders of De Nicola and Hennessy to processes featuring probabilistic as well as nondeterministic choice. They concluded with two problems that have remained open throughout the years, namely to find complete axiomatisations and alternative characterisations for these preorders. This paper solves both problems for finite processes with silent moves. It characterises the may preorder in terms of simulation, and the must preorder in terms of failure simulation. It also gives a characterisation of both preorders using a modal logic. Finally it axiomatises both preorders over a probabilistic version of CSP.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">95</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Bas Luttik</field>
<field name="author">Nikola Tr ka</field>
<field name="title">Computation Tree Logic with Deadlock Detection</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> temporal logic</field>
<field name="keyword"> Kripke structures</field>
<field name="keyword"> labelled transition systems</field>
<field name="keyword"> stuttering equivalence</field>
<field name="keyword"> branching bisimulation</field>
<field name="keyword"> divergence</field>
<field name="keyword"> deadlock.</field>
<field name="abstract">We study the equivalence relation on states of labelled transition systems of satisfying the same formulas in Computation Tree Logic without the next state modality (CTL-X). This relation is obtained by De Nicola &amp; Vaandrager by translating labelled transition systems to Kripke structures, while lifting the totality restriction on the latter. They characterised it as divergence sensitive branching bisimulation equivalence.



We find that this equivalence fails to be a congruence for interleaving parallel composition. The reason is that the proposed application of CTL-X to non-total Kripke structures lacks the expressiveness to cope with deadlock properties that are important in the context of parallel composition. We propose an extension of CTL-X, or an alternative treatment of non-totality, that fills this hiatus. The equivalence induced by our extension is characterised as branching bisimulation equivalence with explicit divergence, which is, moreover, shown to be the coarsest congruence contained in divergence sensitive branching bisimulation equivalence.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">96</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Bas Ploeger</field>
<field name="title">Correcting a Space-Efficient Simulation Algorithm</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> verification</field>
<field name="keyword"> algorithms</field>
<field name="keyword"> time and space complexity</field>
<field name="keyword"> simulation preorder</field>
<field name="keyword"> Kripke structures</field>
<field name="keyword"> generalised coarsest partition problem.</field>
<field name="abstract">Although there are many efficient algorithms for calculating the simulation preorder on finite Kripke structures, only two have been proposed of which the space complexity is of the same order as the size of the output of the algorithm. Of these, the one with the best time complexity exploits the representation of the simulation problem as a generalised coarsest partition problem. It is based on a fixed-point operator for obtaining a generalised coarsest partition as the limit of a sequence of partition pairs. We show that this fixed-point theory is flawed, and that the algorithm is incorrect. Although we do not see how the fixed-point operator can be repaired, we correct the algorithm without affecting its space and time complexity.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">97</field>
<field name="author">Taolue Chen</field>
<field name="author">Wan Fokkink</field>
<field name="author">Robert van Glabbeek</field>
<field name="title">On Finite Bases for Weak Semantics: Failures versus Impossible Futures</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> Process Algebra</field>
<field name="keyword"> BCCS</field>
<field name="keyword"> labeled transition systems</field>
<field name="keyword"> complete axiomatizations</field>
<field name="keyword"> failures semantics</field>
<field name="keyword"> impossible futures semantics.</field>
<field name="abstract">We provide a finite basis for the (in)equational theory of the process algebra BCCS modulo the weak failures preorder and equivalence. We also give positive and negative results regarding the axiomatizability of BCCS modulo weak impossible futures semantics.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">98</field>
<field name="author">Laurene Vaughan</field>
<field name="author">Markus Rittenbruch</field>
<field name="author">Stephen Viller</field>
<field name="author">Jeremy Yuille</field>
<field name="author">Ian MacColl</field>
<field name="title">Spontaneous scenarious: an approach to user engagement</field>
<field name="keyword">Virtual communities</field>
<field name="keyword"> social software</field>
<field name="keyword"> deployment methods</field>
<field name="abstract">In this paper we present work on a scenario and persona based approach to exploring social software solutions for a globally distributed network of researchers, designers and artists. We discuss issues identified with scenario based approaches and a potential participatory solution adopted in this project.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">99</field>
<field name="author">Charles Gretton</field>
<field name="title">Gradient-Based Relational Reinforcement-Learning of Temporally Extended Policies</field>
<field name="abstract">We consider the problem of computing general policies for

decision-theoretic planning problems with temporally extended

rewards. We describe a gradient-based approach to relational

reinforcement-learning (RRL) of policies for that setting. In

particular, the learner optimises its behaviour by acting in a set of

problems drawn from a target domain. Our approach is similar to {\em

inductive policy selection} because the policies learnt are given in

terms of relational control-rules. These rules are generated either (1) by

reasoning from a first-order specification of the domain, or (2) more

or less arbitrarily according to a taxonomic concept language. To this

end the paper contributes a domain definition language for problems

with temporally extended rewards, and a taxonomic concept language in

which concepts and relations can be temporal. We evaluate our approach

in versions of the miconic, logistics and blocks-world planning

benchmarks and find that it is able to learn good 

policies. Our experiments show there is a significant advantage

in making temporal concepts available in RRL for planning, whether

rewards are temporally extended or not.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">100</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">John Thornton</field>
<field name="author">Charles Gretton</field>
<field name="author">Abdul Sattar</field>
<field name="title">Advances in Local Search for Satisfiability</field>
<field name="keyword">satisfiability</field>
<field name="keyword"> local search</field>
<field name="keyword"> hybrid</field>
<field name="abstract">In this paper we describe a stochastic local search (SLS) procedure for finding satisfying models of satisfiable propositional formulae. This new algorithm, gNovelty$^+$, draws on the features of two other WalkSAT family algorithms: R+AdaptNovelty$^+$ and G$^2$WSAT, while also successfully employing a dynamic local search (DLS) clause weighting heuristic to further improve performance.



gNovelty$^+$ was a Gold Medal winner in the random category of the 2007 SAT competition. In this paper we present a detailed description of the algorithm and extend the SAT competition results via an empirical study of the effects of problem structure and parameter tuning on the performance of gNovelty$^+$. The study also compares gNovelty$^+$ with two of the most representative WalkSAT-based solvers: G$^2$WSAT, AdaptNovelty$^+,$ and two of the most representative DLS solvers: RSAPS and PAWS. Our new results augment the SAT competition results and show that gNovelty$^+$ is also highly competitive in the domain of solving {\em structured} satisfiability problems in comparison with other SLS techniques.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">101</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">John Thornton</field>
<field name="author">Charles Gretton</field>
<field name="author">Abdul Sattar</field>
<field name="title">Combining Adaptive and Dynamic Local Search for Satisfiability</field>
<field name="keyword">satisfiability</field>
<field name="keyword"> local search</field>
<field name="keyword"> hybrid</field>
<field name="abstract">In this paper we describe a stochastic local search (SLS) procedure for finding \Omit{satisfying }models of satisfiable propositional formulae. This new algorithm, gNovelty$^+$, draws on the features of two other WalkSAT family algorithms: AdaptNovelty$^+$ and G$^2$WSAT, while also successfully employing a hybrid clause weighting heuristic based on the features of two dynamic local search (DLS) algorithms: PAWS and (R)SAPS.



gNovelty$^+$ was a Gold Medal winner in the random category of the 2007 SAT competition. In this paper we present a detailed description of the algorithm and extend the SAT competition results via an empirical study of the effects of problem structure, parameter tuning and resolution preprocessors on the performance of gNovelty$^+$. The study compares gNovelty$^+$ with three of the most representative WalkSAT-based solvers: AdaptG$^2$WSAT0, G$^2$WSAT and AdaptNovelty$^+,$ and two of the most representative DLS solvers: RSAPS and PAWS. Our new results augment the SAT competition results and show that gNovelty$^+$ is also highly competitive in the domain of solving {\em structured} satisfiability problems in comparison with other SLS techniques.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">102</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">John Thornton</field>
<field name="author">Abdul Sattar</field>
<field name="title">Efficiently Exploiting Dependencies in Local Search for SAT</field>
<field name="keyword">satisfiability</field>
<field name="keyword"> local search</field>
<field name="abstract">Despite significant improvements over the last two decades, local search techniques still struggle to compete with the best systematic methods when solving highly structured real-world satisfiability (SAT) problems. Recent work has successfully employed a dependency lattice structure to represent dependent variables within a local search architecture, producing the first local search technique able to consistently solve the well-known 32-bit parity problem within 24 hours. However, these results are still many times slower than those obtained by the best systematic solvers.



In this paper we propose a new local search hybrid platform that splits a conjunctive normal form (CNF) formula into three sub-components: i) a minimal dependency lattice (representing the core connections between logic gates), ii) a conjunction of equivalence clauses, and iii) the remaining clauses. In addition, we adopt a new hierarchical cost function that focuses on solving the core components of the problem first. In an experimental study using hard industrial benchmarks, we show that our hybrid platform significantly outperforms the original dependency lattice approach and delivers performance that is now competitive with the best modern systematic solvers.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">103</field>
<field name="author">Guillem Godoy</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Sophie Tison</field>
<field name="title">Classes of Tree Homomorphisms with Decidable Preservation of Regularity</field>
<field name="keyword">tree automata</field>
<field name="keyword"> theory</field>
<field name="abstract">Decidability of regularity preservation by a homomorphism

is a well known open problem for regular tree languages.

Two interesting subclasses of this problem are considered:

first, it is proved that regularity preservation is decidable in 

polynomial time when the domain language is constructed over a monadic 

signature, i.e., over a signature where all symbols have arity 0 or 1. 

Second, decidability is proved for the case 

where non-linearity of the homomorphism is restricted

to the root node (or nodes of bounded depth) of any input term. 

The latter result is obtained by proving 

decidability of this problem:

Given a set of terms with regular constraints on

the variables, is its set of ground instances regular?

This extends previous results where

regular constraints where not considered.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">104</field>
<field name="author">Abdul Babar</field>
<field name="author">Didar Zowghi</field>
<field name="author">Karl Cox</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Steven Bleistein</field>
<field name="author">June Verner</field>
<field name="title">Problem Frames and Business Strategy Modelling</field>
<field name="keyword">business strategy</field>
<field name="keyword"> requirements engineering</field>
<field name="keyword"> problem frames</field>
<field name="abstract">Requirements engineering describes how an IT implementation will support business functions. Most IT systems are operational in nature and typical requirements engineering techniques and methods are usually adequate in capturing and documenting the requirements for such systems. Jackson s problem frames is a technique that is suitable for straightforward IT systems descriptions. However, when an organization deploys an IT system that must deliver upon its competitive business strategy, then an appropriate technique or method needs to be used to capture that business strategy in order to connect it to business-critical IT requirements. Here we provide an overview of the use and adaptation of problem frames to connect requirements to business strategy. We found that a simplification of the problem frames notation was necessary and that to be really effective, we had to integrate problem fames with goal modelling and Map, a method that helps describe the evolution of IT over time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">105</field>
<field name="author">Konstanty Bialkowski</field>
<field name="author">Adam Postula</field>
<field name="author">A. Abbosh</field>
<field name="author">M.E. Bialkowski</field>
<field name="title">2 2 MIMO Testbed for Dual 2.4GHz/5GHz Band</field>
<field name="abstract">The paper describes a Multiple Input Multiple Output communication testbed for operation in a dual 2.4GHz/5GHz band, which is under development at the University of Queensland. This advanced testing equipment makes use of Field Programmable Gate Array technology for fast processing of the baseband signals. Its RF modules include commercially available RF transceiver chips and in-house developed antennas. Using a 100Mbit Ethernet network connection and a web browser, the estimated channel matrices and received and decoded signal constellations can be visualized in quasi real-time on a personal computer.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">106</field>
<field name="author">G. N. Mercer</field>
<field name="author">S. I. Barry</field>
<field name="author">D. O. Marlow</field>
<field name="author">Philip Kilby</field>
<field name="title">Investigating the Effect of Detection and Classification Range and Aircraft Dynamics on a Simplified Maritime Surveillance Scenario</field>
<field name="keyword">Dynamic Routing</field>
<field name="keyword"> Maritime Surveillance</field>
<field name="abstract">Airborne maritime surveillance operations are part of Australia's national security. The determination of an efficient route to approach each ship detected to within the classification range is a difficult variation on the classical Travelling Salesman Problem because, for example, the ships are moving. In this article, variations in the detection and classification ranges are investigated, with greater classification ranges resulting in route length reductions of up to 20%. Including a finite turning circle radius for an aircraft can cause in increase in route length of up to 20% for a turning circle radius of 10 nautical miles.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">107</field>
<field name="author">Sven Koenig</field>
<field name="author">Richard Borie</field>
<field name="author">Vangelis Markakis</field>
<field name="author">Philip Kilby</field>
<field name="author">Craig Tovey</field>
<field name="author">Keskinocak Pinar</field>
<field name="title">Agent Coordination with Regret Clearing</field>
<field name="abstract">Sequential single-item auctions can be used for the distributed

allocation of tasks to cooperating agents. We study

how to improve the team performance of sequential single item

auctions while still controlling the agents in real time.

Our idea is to assign that task to agents during the current

round whose regret is large, where the regret of a task is defined

as the difference of the second-smallest and smallest

team costs resulting from assigning the task to the second best

and best agent, respectively. Our experimental results

show that sequential single-item auctions with regret clearing

indeed result in smaller team costs than standard sequential

single-item auctions for three out of four combinations

of two different team objectives and two different capacity

constraints (including no capacity constraints).</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">108</field>
<field name="author">Dhammika Elkaduwe</field>
<field name="author">Gerwin Klein</field>
<field name="author">Kevin Elphinstone</field>
<field name="title">Verified Protection Model of the seL4 Microkernel</field>
<field name="abstract">This paper presents a machine-checked high-level security

analysis of seL4 an evolution of the L4 kernel series targeted to secure,

embedded devices. We provide an abstract specification of the seL4 access

control system in terms of a classical take-grant model together with a

formal proof of how confined subsystems can be enforced. All proofs and

specifications in this paper are machine-checked and developed in the

interactive theorem prover Isabelle/HOL.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">109</field>
<field name="author">Jinbo Huang</field>
<field name="title">A Case for Simple SAT Solvers</field>
<field name="abstract">As SAT becomes more popular due to its ability to handle large real-world problems, progress in efficiency appears to have slowed down over the past few years. On the other hand, we now have access to many sophisticated implementations of SAT solvers, sometimes boasting large amounts of code. Although low-level optimizations can help, we argue that the SAT algorithm itself offers opportunities for more significant improvements. Specifically, we start with a no-frills solver implemented in less than 550 lines of code, and show that by focusing on the central aspects of the solver, higher performance can be achieved over some best existing solvers on a large set of benchmarks. This provides motivation for further research into these more important aspects of SAT algorithms, which we hope will lead to future significant advances in SAT.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">110</field>
<field name="author">Yang Wang</field>
<field name="title">Foreground and Shadow Detection Based on Conditional Random Field</field>
<field name="abstract">This paper presents a conditional random field (CRF) approach to integrate spatial and temporal constraints for moving object detection and cast shadow removal in image sequences. Interactions among both detection (foreground/background/shadow) labels and observed data are unified by a probabilistic framework based on the conditional random field, where the interaction strength can be adaptively adjusted in terms of data similarity of neighboring sites. Experimental results show that the proposed approach effectively fuses contextual dependencies in video sequences and significantly improves the accuracy of object detection.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">111</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Jing Chen</field>
<field name="title">Towards a Stringent Bit-Rate Conformance for Frame-Layer Rate Control in H.264/AVC</field>
<field name="keyword">bit allocation</field>
<field name="keyword"> complexity measure</field>
<field name="keyword"> frame-layer</field>
<field name="keyword"> H.264/AVC</field>
<field name="keyword"> linear R-Q model</field>
<field name="keyword"> rate control</field>
<field name="abstract">This paper presents a novel frame-layer rate control technique that adaptively determines the frame complexity for bit allocation in order to satisfy the target bit-rate constraints without degrading the decoded video significantly. To do this, we first obtain the edge energy of each frame to measure the frame complexity as well as to determine the weighting of a frame for bit allocation. We then present a new bit-rate traffic model for bit allocation to achieve a better conformance to the target bit-rate. Finally, we integrate the edge energy

complexity measure into the rate-quantization (R-Q) model. Our results shows robust improvements over the current rate control methods adopted in H.264/AVC in terms of meeting the target bit-rate as well as determining the quality of the decoded video.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">112</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Jing Chen</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Emmanuel Lochin</field>
<field name="title">Video TFRC</field>
<field name="keyword">complexity measure</field>
<field name="keyword"> congestion control</field>
<field name="keyword"> cross-layer design</field>
<field name="keyword"> H.264/AVC</field>
<field name="keyword"> rate control</field>
<field name="keyword"> TCP-friendly</field>
<field name="keyword"> multimedia streaming</field>
<field name="abstract">TCP-friendly rate control (TFRC) is a congestion control technique that trade-offs responsiveness to the network

conditions for a smoother throughput variation. We take advantage of this trade-off by calculating the rate gap between the theoretical TCP throughput and the smoothed TFRC throughput. Any rate gain from this rate gap is then opportunistically used for video coding. We define a frame complexity measure to determine the additional rate to be used from the rate gap and then perform a rate negotiation to determine the target rate for the encoder and the final sending rate. Results show that although this method has a more aggressive sending rate compared to TFRC, it is still TCP-friendly, does not contribute too much to network congestion and achieves a reasonable video quality gain over the conventional method.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">113</field>
<field name="author">Peizhao Hu</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Ricky Robinson</field>
<field name="title">An Autonomic Context Management System for Pervasive Computing</field>
<field name="keyword">context management system</field>
<field name="keyword"> context-aware</field>
<field name="keyword"> pervasive computing</field>
<field name="keyword"> reconfigurable</field>
<field name="abstract">Context-aware applications adapt to changing computing environments or changing user circumstances/tasks. Context information that supports such adaptations is provided by the underlying infrastructure, which gathers, pre-processes and provisions context information from a variety of context information sources. Such an infrastructure is prone to failures and disconnections that negatively impact on the ability of context-aware applications to adapt (and therefore dramatically impact on their usability). This paper describes a model-based autonomic context management system (ACoMS) that can dynamically con gure and recon gure its context information gathering and pre-processing functionality in order to provide fault tolerant provisioning of context information. The approach uses standards based descriptions of context information sources to increase openness, interoperability and scalability of context-aware systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">114</field>
<field name="author">Peizhao Hu</field>
<field name="author">Ricky Robinson</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Sensor Standards: Overview and Experiences</field>
<field name="keyword">Sensor Standards</field>
<field name="keyword"> IEEE 1451</field>
<field name="keyword"> SensorML</field>
<field name="abstract">Sensors are swiftly nding their way into real-world applications, from farming to factory oor monitoring. In parallel with this deployment, several standards are being developed to extend greater interoperability between sensor systems. In this paper, we explore a subset of the various sensor standards, and relate our experiences in attempting to integrate some of these standards for the purposes of developing an autonomic context manager.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">115</field>
<field name="author">Peizhao Hu</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Ricky Robinson</field>
<field name="title">Reconfigurable middleware for sensor based applications</field>
<field name="keyword">reconfigurable middleware</field>
<field name="keyword"> context-aware</field>
<field name="abstract">The pervasive computing paradigm introduced context-aware applications that adapt themselves to their surrounding environment based on context information. Context information may be of di erent types including sensed context information gathered from a variety of sensors. In pervasive computing user intervention/interactions with their application should be minimised. On the other hand, pervasive computing environments are very dynamic environments context information that supports adaptation decisions may 

not always be available due to user mobility and potential network and/or sensor failures. As context information supports autonomous adaptation of applications, the provision of context information itself has to be managed by an autonomic system able to both recognise and recover from context delivery failures. This paper presents the architecture of a context management system that is recon gurable with regard to sensor and/or network failure and can support dynamic discovery and replacement of sensors. One of the components of this middleware architecture, the model and management of sensor descriptions, is described in more detail as it plays an important role in sensor discovery and replacement.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">116</field>
<field name="author">Peizhao Hu</field>
<field name="author">Ricky Robinson</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Context-Aware Routing in Wireless Mesh Networks</field>
<field name="keyword">context-aware routing</field>
<field name="keyword"> wireless mesh networks</field>
<field name="abstract">Wireless mesh networks promise to deliver resilient connectivity among network nodes, allowing data to be relayed seamlessly between mobile clients and infrastructure. Routing is the vital process in archiving self-con guration, self-healing and, to some degree, self-optimization. However, the heterogeneity of network nodes and highly dynamic network topologies create new challenges for developing e cient and adaptive routing solutions. The increasing amount and complexity of information that routing solutions have to consider, in order to cope with the changing network situation and/or user requirements, is a key challenge. We propose adopting a recon gurable context management system to simplify the task of accessing a variety of information required by adaptive routing protocols and to hide the low-level complexities of information sources management. In addition, we show how our middleware supports fault-tolerance of various information failures, freeing protocol developers to concentrate on improving the routing mechanism and the metric information model of routing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">117</field>
<field name="author">Peizhao Hu</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">Experimental evaluation of AODV in a hybrid wireless mesh network</field>
<field name="keyword">AODV</field>
<field name="keyword"> Mesh network</field>
<field name="abstract">Wireless Mesh Networks (WMNs) are a variant of 

ad-hoc networks that have recently received increasing attention. 

They are a promising technology for a wide range of applications 

for the reason of their cost-effectiveness, self-con guring and 

self-healing nature. Routing protocols are a key component of 

WMNs, as they are responsible for discovering and establishing 

end-to-end, often multi-hop, paths between nodes in dynamic 

environments. In this paper, we evaluate the performance of 

the Ad-hoc On-demand Distance Vector (AODV) protocol for 

hybrid WMNs, which are a special type of wireless mesh network 

that are comprised of both static infrastructure nodes as well 

as mobile client devices. The results obtained from our test- 

bed network indicate that AODV performs effectively in hybrid 

WMNs and is able to handle a high volume of traf c with minimal 

latency. While existing works on performance evaluation of ad- 

hoc and WMN routing protocols are typically performed either 

via simulations or via experiments on test-beds. Comparisons of 

results from both methods are rarely provided. One of the key 

contributions of this paper is the validation of our simulation 

results by performing experiments on a real WMN test-bed 

using the same scenario and mobility pattern. Our results show 

a reasonably good correlation between the simulation results 

and our test-bed measurements and we are able to explain the 

observed variations. Our work serves as a base line result for 

the exploration of larger and more complex network topologies.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">118</field>
<field name="author">Ryan Wishart</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">A Light-Weight Client Mobility Approach for Infrastructure Mesh Networks</field>
<field name="keyword">mesh networks</field>
<field name="keyword"> client support</field>
<field name="keyword"> mobility</field>
<field name="keyword"> light-weight</field>
<field name="abstract">Infrastructure mesh networks offer a high-capacity

wireless backhaul network through which clients, such as PDAs,

can connect to one another or with external networks. To use the

mesh network, a client must route its outbound traffic via one

of the mesh routers in the infrastructure mesh. As the clients

are mobile, they may move out of range of the mesh router they

were using and need to associate with another. Client handoff

mechanisms enable this change in mesh routers to occur in a

manner that limits disruption to any transport or application

layer sessions the client may be running. In this paper we present

an extremely light-weight handoff approach for clients that relies

on gratuitous ARP messages broadcast at regular intervals from

mesh routers within the infrastructure mesh. An evaluation of our

approach using a 5 node testbed has shown that client handoffs

can be conducted quickly, and with minimal loss of packets for

both TCP and UDP traffic.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">119</field>
<field name="author">Huidong Jin</field>
<field name="title">Practical Issues on Privacy-Preserving Health Data Mining</field>
<field name="abstract">Privacy-preserving data mining techniques could encourage

health data custodians to provide accurate information for mining by

ensuring that the data mining procedures and results cannot, with any

reasonable degree of certainty, violate data privacy. We outline privacypreserving

data mining techniques/systems in the literature and in

industry. They range from privacy-preserving data publishing, privacypreserving

(distributed) computation to privacy-preserving data mining

result release. We discuss their strength and weaknesses respectively, and

indicate there is no perfect technical solution yet. We also provide and

discuss a possible development framework for privacy-preserving health

data mining systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">120</field>
<field name="author">Ke Zhang</field>
<field name="author">Huidong Jin</field>
<field name="author">Nianjun Liu</field>
<field name="author">Zhouyu Fu</field>
<field name="title">Optimal Learning High-Order Markov Random Fields Priors of Colour Image</field>
<field name="keyword">Markov random fields</field>
<field name="keyword"> image prior</field>
<field name="keyword"> colour image denoising</field>
<field name="abstract">In this paper, we present an optimised learning algorithm

for learning the parametric prior models for high-order Markov random

fields (MRF) of colour images. Compared to the priors used by conventional

low-order MRFs, the learned priors have richer expressive power

and can capture the statistics of natural scenes. Our proposed optimal

learning algorithm is achieved by simplifying the estimation of partition

function without compromising the accuracy of the learned model.

The parameters in MRF colour image priors are learned alternatively

and iteratively in an EM-like fashion by maximising their likelihood. We

demonstrate the capability of the proposed learning algorithm of highorder

MRF colour image priors with the application of colour image

denoising. Experimental results show the superior performance of our

algorithm compared to the state of the art of colour image priors in [1],

although we use a much smaller training image set.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">121</field>
<field name="author">Ryan Wishart</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">A Light-Weight Gateway Discovery Scheme for Infrastructure Mesh Networks</field>
<field name="keyword">Internet</field>
<field name="keyword"> gateway</field>
<field name="keyword"> client device</field>
<field name="keyword"> mesh network</field>
<field name="keyword"> access router</field>
<field name="abstract">Infrastructure mesh networks offer a high-capacity

wireless backhaul network through which client devices, such

as PDAs, can connect to one another or with the Internet. In

dynamically deployed mesh networks the routers within the mesh

network may be unaware of existing Internet gateways and need

to discover them on demand. In this paper we present a lightweight

gateway discovery and traffic forwarding approach for

discovering the presence of these Internet gateways and managing

communication with them. A comparison of our work with

existing approaches shows that our method is superior in terms of latency and per packet overhead.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">122</field>
<field name="author">Lan Du</field>
<field name="author">Huidong Jin</field>
<field name="author">Olivier de Vel</field>
<field name="author">Nianjun Liu</field>
<field name="title">A Latent Semantic Indexing and WordNet based Information Retrieval Model for Digital Forensics</field>
<field name="keyword">Information retrieval</field>
<field name="keyword"> ontology</field>
<field name="keyword"> Latent semantic index</field>
<field name="keyword"> WordNet</field>
<field name="keyword"> Digital Forensics</field>
<field name="abstract">It is well known that either domain specific or domain

independent knowledge has been adopted in Information Retrieval (IR)

to improve the retrieval performance. In this paper, we propose a

novel IR model for digital forensics by using Latent Semantic

Indexing (LSI) and WordNet as an underlying reference ontology to

retrieve suspicious emails according to the semantic meaning of an

investigator's query. Our model incorporates corpus independent

knowledge from WordNet and corpus dependent knowledge from LSI into

query expansion and reduction; and LSI is also adopted to simulate

human meaning-based judgement of relatedness between investigator's

queries and emails. We compare the performance of the resulting

LSI And WordNet based

Information Retrieval System

(LAWIRS) with other three systems we implement, i.e. the LSI system,

the Lucene system and the Lucene system with query expansion.

Experimental results on several email datasets demonstrate that for

short Boolean queries, LAWIRS can successfully capture their meaning

and yield substantial improvements in the overall retrieval

performance.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">123</field>
<field name="author">Timothy Bourke</field>
<field name="author">Arcot Sowmya</field>
<field name="title">Tool support for verifying trace inclusion with Uppaal</field>
<field name="keyword">timed automata</field>
<field name="keyword"> trace inclusion</field>
<field name="keyword"> uppaal</field>
<field name="abstract">Trace inclusion against a deterministic Timed Automata can be verified with

the Uppaal model checking tool by constructing a test automaton that traps

illegal synchronisation possibilities. Constructing the automaton manually

is tedious and error prone. This paper presents a tool that does it

automatically for a subset of Uppaal models.



Certain features of Uppaal, namely selection bindings and channel arrays,

complicate the construction. We first formalise these features, and then

show how to incorporate them directly in the testing construction. To do so

we limit the forms of subscript that can be used to specify

synchronisations; striving for a balance between practicability and program

complexity. Unfortunately, some combinations of selection bindings and

universal quantifiers cannot be effectively manipulated. The tool does not

yet validate the determinism requirements, nor handle committed states or

broadcast channels.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">124</field>
<field name="author">Phan Son</field>
<field name="author">Lan Du</field>
<field name="author">Huidong Jin</field>
<field name="author">Olivier de Vel</field>
<field name="author">Nianjun Liu</field>
<field name="author">Terry Caelli</field>
<field name="title">A Simple WordNet-Ontology based Email Retrieval System for Digital Forensics</field>
<field name="keyword">Information retrieval</field>
<field name="keyword"> WordNet</field>
<field name="keyword"> Extended Boolean Model</field>
<field name="abstract">Because of the high impact of high-tech digital crime upon

our society, it is necessary to develop effective Information Retrieval (IR)

tools to support digital forensic investigations. In this paper, we propose

an IR system for digital forensics that targets emails. Our system

incorporates WordNet (i.e. a domain independent ontology for the vocabulary)

into an Extended Boolean Model (EBM) by applying query

expansion techniques. Structured Boolean queries in Backus-Naur Form

(BNF) are utilized to assist investigators in effectively expressing their

information requirements.We compare the performance of our system on

several email datasets with a traditional Boolean IR system built upon

the Lucene keyword-only model. Experimental results show that our system

yields a promising improvement in retrieval performance without the

requirement of very accurate query keywords to retrieve the most relevant

emails.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">125</field>
<field name="author">Huidong Jin</field>
<field name="title">Privacy-Preserving Publishing Frequent Sequential Patterns</field>
<field name="keyword">Privacy-preserving data mining</field>
<field name="keyword"> inference channel</field>
<field name="keyword"> k-anonymous sequential patterns</field>
<field name="keyword"> secure computation</field>
<field name="abstract">Releasing frequent sequential patterns can compromise individual privacy of underlying sequences.

We propose two concrete objectives as a potential standard for privacy-preserving publishing sequential

patterns: k-anonymity and alpha-dissociation. The first one, extended from k-anonymity model for data,

addresses the problem of inferring patterns with very low support, say, in [1; k) where k is an anonymity

threshold. These inferred patterns can act as quasi-identifiers in linking attacks. We show theoretically

that, for all but one definition of support, it is impossible to reliably infer support values for patterns

with two or more negative items (items do not occur in a pattern) based solely on frequent sequential

patterns. We formulate possible privacy inference channels for the remaining definition of support. The second objective is proposed to address the problem of high certainty of inferring sensitive information.

We establish an algorithm PICS (Privacy Inference Channels Sanitisation) to detect and remove privacy

inference channels related to the two objectives by incrementing support values of sequential patterns.

A series of experimental results show, with small extra computation overhead, PICS efficiently reduces

privacy disclosure risk for publishing frequent sequential patterns.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">126</field>
<field name="author">Asad Pirzada</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Multi-Linked AODV Routing Protocol for Wireless Mesh Networks</field>
<field name="keyword">Multi-linked</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wierless</field>
<field name="keyword"> network</field>
<field name="keyword"> routing</field>
<field name="abstract">Nodes in multi-hop wireless networks, and specifically

in ad-hoc and mesh networks, are being increasingly

equipped with multiple wireless network interfaces (radios)

operating on orthogonal channels to achieve better utilisation

of the frequency spectrum. In addition to reducing interference

via increased channel diversity, these additional interfaces can be

used to create multiple concurrent links between adjacent nodes,

i.e. nodes within single-hop range of each other. Information

about the availability of multiple links between nodes provides the

opportunity to increase the overall performance of the network

by optimally balancing traffic between the set of available internode

links. In this paper, we present extensions to the well known

Ad-hoc On-demand Distance Vector (AODV) routing protocol

with the aim to discover and exploit multiple links in Wireless

Mesh Networks. As demonstrated via extensive simulations,

Multi-Link AODV (AODV-ML) achieves a more than 100%

improvement over standard multi-radio AODV in terms of key

performance metrics such as packet delivery ratio, latency and

routing overhead.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">127</field>
<field name="author">Shaojie Qiao</field>
<field name="author">Changjie Tang</field>
<field name="author">Huidong Jin</field>
<field name="title">Constrained K-Closest Pairs Query Processing Based on Growing Window in Crime Databases</field>
<field name="keyword">patial analysis; crime databases; constrained closest pairs; query processing; R</field>
<field name="keyword">tree</field>
<field name="abstract">Spatial analysis in crime databases has recently

been an active research topic. To solve the problem of finding the

closest pairs of objects within a given spatial region, as required

in crime geo-data applications, this paper proposes an efficient

constrained k-closest pairs query processing algorithm based on

growing window. It expands the window progressively instead of

searching the whole workspace for multiple types of spatial

objects. It employs a density-based range estimation method to

calculate the query range and an optimized R-tree to store the

index entities. In addition, a distance threshold T for the closest

pair of objects is introduced to prune the tree nodes.

Experiments evaluate the effect of three important factors, i.e.,

the portion of overlapping between the workspaces of two data

sets, the value of k, and the size of buffer. The results show that

the new algorithm outperforms the heap-based approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">128</field>
<field name="author">Jonathan Guerin</field>
<field name="author">Marius Portmann</field>
<field name="author">Asad Pirzada</field>
<field name="title">Routing Metrics for Multi-Radio Wireless Mesh Networks</field>
<field name="keyword">routing metrics; multi</field>
<field name="keyword">radio; wireless mesh networks</field>
<field name="abstract">Multi-radio wireless mesh networks provide a great 

improvement over traditional mobile ad-hoc networks and are 

able to provide increased capacity and scalability. Routing 

metrics for multi-radio wireless mesh networks must cater to the 

specific characteristics that these networks exhibit and attempt to 

exploit these to maximize performance. Many routing metrics 

were designed for single-radio ad-hoc networks and hence do not 

perform well in multi-radio mesh networks. In this paper, we 

provide a survey of recently proposed routing metrics for multi- 

radio mesh networks. We also provide an overview of the most 

relevant single-radio metrics, since they typically form the basis 

of multi-radio metrics. For our survey, we identify the key 

components from which multi-radio metrics are constructed. We 

also provide a list of key criteria, which forms the basis of our 

qualitative comparison of routing metrics for multi-radio mesh 

networks. 

Hybrid Mesh Networks blend features from Client Mesh 

and Infrastructure Mesh Networks. Mesh Routers in Hybrid 

Mesh configurations still form the backbone of the topology 

and may provide backhaul access to external networks. 

However, in order to increase the reach of the network, client 

devices can be involved in routing. For example, if a client is 

not within communication range of a Mesh Router, another 

client device can act as a relay to the nearest router. 

Recently, a lot of research effort has been focused on multi- 

radio wireless mesh networks. Due to the relatively low cost of 

commodity wireless hardware such as radio interfaces based on 

IEEE 802.11 standards, it is now feasible to include multiple 

radios on a single node. By operating these interfaces on 

orthogonal channels, the capacity of a Mesh Router can be</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">129</field>
<field name="author">Tamir Yedidya</field>
<field name="author">Richard Hartley</field>
<field name="author">Jean-Pierre Guillon</field>
<field name="author">Yogesan Kanagasingam</field>
<field name="title">Automatic Dry Eye Detection</field>
<field name="keyword">Dry Eye Syndrome</field>
<field name="keyword"> Retina</field>
<field name="keyword"> Automatic</field>
<field name="abstract">Dry Eye Syndrome is a common disease in the western world, with effects from uncomfortable itchiness to permanent damage to the ocular surface. Nevertheless, there is still no objective test that provides reliable results. We have developed a new method for the automated detection of dry areas in videos taken after instilling fluorescein in the tear film. The method consists of a multi-step algorithm to first locate the iris in each image, then align the images and finally analyze the aligned sequence in order to find the regions of interest. Since the fluorescein spreads on the ocular surface of the eye the edges of the iris are fuzzy making the detection of the iris challenging. We use RANSAC to first detect the upper and lower eyelids and then the iris. Then we align the images by finding differences in intensities at different scales and using a least squares optimization method (Levenberg-Marquardt), to overcome the movement of the iris and the camera. The method has been tested on videos taken from different patients. It is demonstrated to find the dry areas accurately and to provide a measure of the extent of the disease.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">130</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Ronnie Taib</field>
<field name="author">Yu Shi</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Fang Chen</field>
<field name="title">Using Pen Input Features as Indices of Cognitive Load</field>
<field name="keyword">Pen gesture</field>
<field name="keyword"> speech</field>
<field name="keyword"> multimodal</field>
<field name="keyword"> cognitive load</field>
<field name="abstract">Multimodal interfaces are known to be useful in map-based applications, and in complex, time-pressure based tasks. Cognitive load variations in such tasks have been found to impact multimodal behaviour. For example, users become more multimodal and tend towards semantic complementarity as cognitive load increases. The richness of multimodal data means that systems could monitor particular input features to detect experienced load variations. In this paper, we present our attempt to induce controlled levels of load and solicit natural speech and pen-gesture inputs. In particular, we analyse for these features in the pen gesture modality. Our experimental design relies on a map-based Wizard of Oz, using a tablet PC. This paper details analysis of pen-gesture interaction across subjects, and presents suggestive trends of increases in the degree of degeneration of pen-gestures in some subjects, and possible trends in gesture kinematics, when cognitive load increases.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">131</field>
<field name="author">David Cock</field>
<field name="author">Gerwin Klein</field>
<field name="author">Thomas Sewell</field>
<field name="title">Secure Microkernels, State Monads and Scalable Refinement</field>
<field name="keyword">State Monad</field>
<field name="keyword"> Refinement</field>
<field name="keyword"> Microkernel</field>
<field name="keyword"> Isabelle/HOL</field>
<field name="keyword"> seL4</field>
<field name="abstract">We present a scalable, practical Hoare Logic and refinement calculus

for the nondeterministic state monad with exceptions and failure in

Isabelle/HOL. The emphasis of this formalisation is on large-scale

verification of imperative-style functional programs, rather than

expressing monad calculi in full generality. We achieve scalability in

two dimensions. The method scales to multiple team members working

productively and largely independently on a single proof and also

to large programs with large and complex properties.



We report on our experience in applying the techniques in an extensive

(100K lines of proof) case study---the formal verification of an

executable model of the seL4 operating system microkernel.</field>
<field name="date">2014</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">132</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="title">Situational Method Quality</field>
<field name="abstract">Some overall method characteristics, such as agility and scalability, have become increasingly important. These characteristics are different from existing method requirements which focus on the functional purposes of individual method chunks and overall methods. Characteristics like agility and scalability are often not embodied in the function of a single method chunk but are instead reflected in constraints over one or more method chunks, connections between method chunks and cross-cutting aspects of the overall method. We propose the concept of method tactics, which are techniques for achieving certain method quality attributes. We identify a list of method tactics focusing on agility and scalability by considering factors that affect these quality attributes. We validate the feasibility of using method tactics by applying them to traditional software development method chunks and deriving practices for agile development. We examine the effectiveness of the tactics by comparing our derived practices with existing practices for agile development. The comparison results show that most of the derived practices are found in existing agile methods. We also identify new practices that may have potential for use in agile methods. The results demonstrate initial support for our proposal for the use of method tactics, and for the extraction or invention of further cross-cutting primitive method tactics for more flexible situational method engineering.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">133</field>
<field name="author">Makoto Nonaka</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="title">Impacts of Architecture and Quality Investment in Software Product Line Development</field>
<field name="abstract">Investment on architecture and quality improvement for a software product line can increase reuse, enhance product reliability, and shorten time-to-market. Such investments should be carefully chosen to be effective, to avoid over-investment, and to return benefits within the desired time. In this paper, we show how a stochastic simulation model can be used to explore the impacts of such investments. The model is validated by comparison to COPLIMO, a COCOMO II based effort estimation model for product line development, and by inspecting effort distributions of the generated unplanned work. For the illustrative model and scenarios in this paper, we show that the degree of architecture reuse has the largest impact. Preventing degraded architectural dependencies itself does not have a meaningful impact, but if such degradation is also associated with adverse effects on defect injection and detection, it can be significant. Process improvement has a meaningful impact, but over-investment is possible.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">134</field>
<field name="author">Liming Zhu</field>
<field name="author">Leon J. Osterweil</field>
<field name="author">Mark Staples</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Challenges Observed in the Definition of Reference Business Processes</field>
<field name="abstract">In many modern enterprises, explicit business process definitions facilitate the pursuit of business goals in such ways as best practice reuse, process analysis, process efficiency improvement, and automation. Most real-world business processes are large and complex. Successfully capturing, analysing, and automating these processes requires process definition languages that capture a variety of process aspects with a wealth of details. Most current process modeling languages, such as Business Process Modeling Notation (BPMN), focus on structural control flows among activities while providing inadequate support for other process definition needs. In this paper, we first illustrate these inadequacies through our experiences with a collection of real-world reference business processes from the Australian lending industry. We observe that the most significant inadequacies include lack of resource management, exception handling, process variation, and data flow integration. These identified shortcomings led us to consider the Little-JIL language as a vehicle for defining business processes. Little-JIL addresses the afore-mentioned inadequacies with a number of innovative features. Our investigation concludes that these innovative features are effective in addressing a number of key reference business process definition needs.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">135</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Scott Brandt</field>
<field name="title">Proceedings of the 2007 Workshop on Operating System Platforms for Embedded Real-Time Applications</field>
<field name="abstract">Proceedings of the 2007 Workshop on Operating System Platforms for Embedded Real-Time Applications Technical Report , NICTA, July, 2007



Editors: Scott Brandt and Kevin Elphinstone</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">136</field>
<field name="author">Xi Chen</field>
<field name="author">Mark Staples</field>
<field name="title">Using Practice Outcome Areas to Understand Perceived Value of CMMI Specific Practices for SMEs</field>
<field name="keyword">SME</field>
<field name="keyword"> Software Process Improvement</field>
<field name="keyword"> CMMI</field>
<field name="keyword"> Specific Practice</field>
<field name="abstract">In this article, we present a categorization of CMMI Specific Practices, and use this to reanalyze prior work describing the perceived value of those practices for Small-to-Medium-sized Enterprises (SMEs), in order to better understand the software engineering practice needs of SMEs. Our categorization is based not on process areas, but on outcome areas (covering organizational, process, project, and product outcomes) and on the nature of activities leading to outcomes in those areas (covering planning, doing, checking, and improvement activities). Our reanalysis of the perceived value of Specific Practices for the CMMI Level 2 Process Areas shows that SMEs most value practices for working on project-related outcomes, and for planning and doing work on product-related outcomes. Our categorization of practices will serve as a framework for further study about CMMI and other SPI approaches.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">137</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Najmi G. Haider</field>
<field name="title">Segmentation of Sindhi Speech Using Formants</field>
<field name="keyword">Speech processing</field>
<field name="keyword"> Segmentation</field>
<field name="keyword"> Sindhi</field>
<field name="keyword"> formant frequencies</field>
<field name="keyword"> rate of change dynamics</field>
<field name="abstract">A speech segmentation method using formant frequencies is presented. The method uses speech samples of a major language of Indian sub-continent, Sindhi. It performs VCP (vowel-consonant-pause) segmentation and generates VCP strings for speech signals. The VCP strings and their formation may enable a recognizer to identify the speech on-the-fly, hence minimizing the system training and making the recognizer very efficient. The method applies velocity and acceleration parameters of rate-of-change dynamics on formants of speech to segment it into vowel, consonant, and pause parts. A test-bed software, to implement the proposed method and conduct all experiments, is also presented. Results show that the method is speaker as well as gender independent. Its segmentation performance is almost over 90% in most conditions and over 60% under some worst conditions. Long-term goal is to develop an efficient speaker-independent speech recognizer based on proposed method. A model of such a recognizer is also presented.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">138</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="title">Cognitive Load Measurement using User s Speech Features</field>
<field name="abstract">Multimodal systems are a clear trend for future HCI systems. Initially, these systems will support, but eventually will replace, the GUIs of many of today s computer and mobile applications. These systems will greatly enhance the accessibility of computing to diverse types of users, improve the performance stability of systems, and give users a more powerful interface for accessing and manipulating information.&lt;p&gt;



This study investigates the speech features that can be used to measure users level of cognitive load and aims to develop a model/ implementation that can be integrated with a multimodal system to enhance its overall performance of presenting natural and dynamic behavior. Besides speech features, the study also proposes the use of biosensors to get users skin conductance data. This is to further verify the presence of potential speech features for cognitive load measurement by matching users speech with biosensor data. It is well-known that skin conductance is directly proportional to stress and memory load.&lt;p&gt;



To achieve the objectives of the study, experimental methodology has been employed where several speech samples were collected from native English speakers and analysed for potential speech features. The subjects were required to read out loud some stories of different difficulty levels and then answer few questions afterwards. Couple of stories involved a dual task too where subjects were to remember the numbers they would hear while reading and answering. The objective of the experiment was to analyze how speech features like response latency, disfluencies and pauses change for different difficulty levels during reading the stories and answering the questions, and how background dual-task affects the user s cognitive load.&lt;p&gt; 



Preliminary results have shown that the user s cognitive load increases with the increasing difficulty of the story. The cognitive load further increases when user is asked to perform a dual-task along with the primary task of reading and answering. A further analysis of users speech provides an indication that response latency and intersentential pauses could be the most potential speech features for cognitive load measurement.&lt;p&gt;</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">139</field>
<field name="author">Mark Staples</field>
<field name="author">Mahmood Niazi</field>
<field name="title">Systematic review of organizational motivations for adopting CMM-based SPI</field>
<field name="keyword">CMM</field>
<field name="keyword"> CMMI</field>
<field name="keyword"> Capability Maturity Model</field>
<field name="keyword"> Software Process Improvement</field>
<field name="abstract">Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that complementary research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">140</field>
<field name="author">Ihor Kuz</field>
<field name="author">Jenny Liu</field>
<field name="title">Extending the capabilities of component models for embedded systems</field>
<field name="keyword">component</field>
<field name="keyword"> OS</field>
<field name="keyword"> embedded</field>
<field name="keyword"> scenario</field>
<field name="keyword"> CAmkES</field>
<field name="abstract">Component-based development helps to improve the modularity and reusability of embedded systems. Component models devised for embedded systems are typically restricted due to the limited computing, storage and power resources of the target systems. Most existing component models for embedded systems therefore only support a static component architecture and provide a simple and lightweight core. With the increasing demand for more feature-rich embedded systems these component architectures must be extended. In order to remain useful for the development of resource-restricted embedded systems, however, the extensions must be optional. Creating such extensions requires a cost-effective development process that can produce reusable, rather than application-specific, extensions. This necessitates a systematic approach to seamlessly integrate application specific requirements of the extension, the existing component model and the constraints of the computing environment. In this paper we propose a scenario-based architectural approach to extending the capabilities of the CAmkES component model. This approach is used to distil application specific requirements and computing constraints, summarise generic scenarios, drive the extension to the core CAmkES architecture. We illustrate our approach with a case study involving the addition of dynamic capabilities to CAmkES.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">141</field>
<field name="author">Leonid Ryzhyk</field>
<field name="author">Ihor Kuz</field>
<field name="author">Gernot Heiser</field>
<field name="title">Formalising device driver interfaces</field>
<field name="keyword">OS</field>
<field name="keyword"> device driver</field>
<field name="keyword"> formalisation</field>
<field name="abstract">The lack of well-defined protocols for interaction with the

operating system is a common source of defects in device drivers.

In this paper we investigate the use of a formal language to

define these protocols unambiguously. We present a language that

allows us to convey all important requirements for driver

behaviour in a compact specification and that can be readily

understood by software engineers. It is intended to close the

communication gap between OS and driver developers and enable

more reliable device drivers.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">142</field>
<field name="author">Jenny Liu</field>
<field name="author">Liming Zhu</field>
<field name="author">Len Bass</field>
<field name="author">Ian Gorton</field>
<field name="author">Mark Staples</field>
<field name="title">Non-Functional Property Driven Service Governance: Performance Implications</field>
<field name="abstract">Service governance is a set of businesses processes, policies and technical solutions that support enterprises in their implementation and management of their SOA. The decisions of service governance, especially concerning service boundaries at the enterprise level, influence the deployment topology of business services across or within business organizations. Deployment topologies are realized by integration technologies such as Enterprise Service Bus (ESB). Service governance and technical solutions interact in a subtle way including through communication patterns and protocols between services and ESBs, as well as the deployment and configuration of ESB. These factors have a strong influence on the Non-Functional Properties (NFP) of a SOA solution. A systematic approach is essential to understand alternative technical solutions for a specific service governance decision. This paper proposes a modeling approach to evaluate the performance-related NFP impacts when mapping service governance to technical solutions using an ESB. This approach is illustrated by the quantitative performance analysis of a real world example, service governance from an Australian lending organization.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">143</field>
<field name="author">Liming Zhu</field>
<field name="author">Leon J. Osterweil</field>
<field name="author">Mark Staples</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">Borislava I. Simidchieva</field>
<field name="title">Desiderata for Languages to be Used in the Definition of Reference Business Processes</field>
<field name="keyword">business process; process definition language; BPMN; Little</field>
<field name="keyword">JIL; work</field>
<field name="keyword">flow</field>
<field name="abstract">In many modern enterprises, explicit business process definitions facilitate the pursuit of business goals in such ways as best practice reuse, process analysis, process

efficiency improvement, and automation. Most real-world business processes are large and complex. Successfully capturing, analysing, and automating these processes requires process definition languages that capture a variety of process aspects with a wealth of details. Most current process modelling languages, such as Business Process Modelling Notation (BPMN), focus on structural control flows among activities while providing inadequate support for other process definition needs. In this paper, we first illustrate these inadequacies through our experiences with a collection of real-world reference business processes from the Australian lending industry. We observe that the most significant inadequacies include lack of resource management, exception handling, process variation, and data flow integration. These identified shortcomings led us to consider the Little-JIL language as a vehicle for defining business processes. Little-JIL addresses the afore-mentioned inadequacies with a number of innovative features. Our investigation concludes that these innovative features are effective in addressing a number of key reference business process definition needs.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">144</field>
<field name="author">Louise Leenen</field>
<field name="author">A. Anbulagan</field>
<field name="author">Thomas Meyer</field>
<field name="author">Aditya Ghose</field>
<field name="title">Modeling and Solving Semiring Constraint Satisfaction Problems by Transformation to Weighted Semiring Max-SAT</field>
<field name="keyword">Constraint satisfaction</field>
<field name="keyword"> constraint programming</field>
<field name="keyword"> MAX-SAT</field>
<field name="abstract">We present a variant of the Weighted Maximum Satisfiability Problem (Weighted Max-SAT), 

which is a modeling of the Semiring Constraint Satisfaction framework. We show how to 

encode a Semiring Constraint Satisfaction Problem (SCSP) into an instance of a propositional 

Weighted Max-SAT, and call the encoding Weighted Semiring Max-SAT (WS-Max-SAT). 

The clauses in our encoding are highly structured and we exploit this feature to develop 

two algorithms for solving WS-Max-SAT: an incomplete algorithm

based on the well-known GSAT algorithm for Max-SAT, and a branch-and-bound algorithm which 

is complete. Our preliminary experiments show that the translation of SCSP 

into WS-Max-SAT is feasible, and that our branch-and-bound algorithm performs surprisingly well. 

We aim in future to combine the natural flexible representation of the SCSP framework with 

the inherent efficiencies of SAT solvers by adjusting existing SAT solvers to solve WS-Max-SAT.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">145</field>
<field name="author">Matt Ruan</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Turbo Equalization using Particle Filtering with Grouping</field>
<field name="abstract">This paper presents an efficient particle filtering algorithm for Turbo equalization. It approximates the maximum a posteriori (MAP) equalizer performance with low computational complexity. By grouping the identical trajectories of particles, we improve the efficiency of conventional particle filters and manage to work with large number of trajectories without a significant increase in complexity. EXIT chart analysis and Monte Carlo simulation confirm the excellent performance of the proposed method for channels even with severe inter-symbol interference.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">146</field>
<field name="author">Xiangyun Zhou</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Iterative Channel Estimation for IDMA Systems in Time-Varying Channels</field>
<field name="abstract">In this paper, we develop low-complexity iterative channel estimation techniques for emerging IDMA systems. The channel estimators make use of pilots as well as soft decoded data information. We derive a lower bound for channel estimation error that reflects the reliability of soft decoded data. We show that the estimators perform close to a minimum variance unbiased estimator as the mean square error (MSE) approaches the lower bound. Numerical results on the MSE and BER performance also show that the proposed channel estimators are able to track the time-varying channel states.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">147</field>
<field name="author">Ming Zhao</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">An Iterative Receiver with Channel Estimation for MIMO-OFDM over a Time and Frequency Dispersive Fading Channel</field>
<field name="abstract">A joint iterative channel estimation and data detection receiver is proposed in this paper for multiple-inputmultiple-output orthogonal frequency deivision multiplexing (MIMO-OFDM) system, which employs two mandatory MIMO profiles as in IEEE 802.16e MIMO Mobile WiMAX system namely Alamouti space time coding (STC) and spatial multiplexing (SM), over time and frequency dispersive fading channel. A novel low complexity channel estimator with time-domain

and frequency-domain combining of channel estimates from preamble/pilot/soft decoded data information is proposed and integrated with maximum ratio combining (MRC) data detector

and co-antenna interference canceler (IC) for STC system and SM system respectively. Numerical results show that the proposed receiver has 2dB gain compared to conventional non-iterative

receivers in pedestrian low mobility condition and more than 6dB gain in vehicular high Doppler environment.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">148</field>
<field name="author">David P Shepherd</field>
<field name="author">Zhenning Shi</field>
<field name="author">Michael Anderson</field>
<field name="author">Mark Reed</field>
<field name="title">EXIT Chart Analysis of an Iterative Receiver with Channel Estimation</field>
<field name="abstract">In this paper we derive a novel extrinsic information transfer (EXIT) function for a channel estimator that uses both pilot symbols and soft decoding information. We consider a CDMA system with forward error correction that employs iterative multiuser detection based on soft interference cancellation and single user decoding. The J function is useful in convergence analysis as it provides a mapping between mutual information and variance. J functions are derived for both the flat-fading Rayleigh channel and channel estimate, allowing the a priori inputs to the channel estimator, interference canceller and decoder to be modeled. Algorithms for obtaining the EXIT functions for the three receiver blocks are proposed. We then

combine the EXIT functions to create an EXIT chart that can be used to predict the behaviour of the overall receiver. Simulations are run for a 3GPP-like receiver in a mobile environment with a vehicle speed of 100km/h. It is found that under these conditions, the detector trajectory closely matches our proposed EXIT chart.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">149</field>
<field name="author">Mark Reed</field>
<field name="title">The Business Case for Advanced WiMAX Receivers: From Initial Deployment to Mature Systems</field>
<field name="abstract">Optimisation of a WiMAX system can occur at various places in the system providing different benefits to the overall system performance. Optimisation is critical to maximise the efficiency of the wireless network. During initial deployment coverage is optimised to minimise infrastructure costs and cross the profitability point earlier, while later as the network matures capacity needs to be optimised. A number of methods can be used to optimise a WiMAX, One area that has not been studied with any depth is the system performance improvements that can be achieved by the utilization of advanced receivers. In this presentation we investigate the gains possible from advanced receivers and illustrate the system benefits in terms of cell range improvement and average data throughput for a user. Surprisingly for a very small improvement in receiver sensitivity of only 2dB, cell area improvement can be as much as 50% while data rate throughput can increase by 11%. As Multiple-Input Multiple-Output (MIMO) is deployed and modem chip developers can differentiate themselves more through their products then even more significant gains can be achieved.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">150</field>
<field name="author">Mark Reed</field>
<field name="title">The Business Case for Advanced WiMAX Receivers: From Initial Deployment to Mature Systems</field>
<field name="abstract">Optimisation of a WiMAX system can occur at various places in the system providing different benefits to the overall system performance. Optimisation is critical to maximise the efficiency of the wireless network. During initial deployment coverage is optimised to minimise infrastructure costs and cross the profitability point earlier, while later as the network matures capacity needs to be optimised. A number of methods can be used to optimise a WiMAX, One area that has not been studied with any depth is the system performance improvements that can be achieved by the utilization of advanced receivers. In this presentation we investigate the gains possible from advanced receivers and illustrate the system benefits in terms of cell range improvement and average data throughput for a user. Surprisingly for a very small improvement in receiver sensitivity of only 2dB, cell area improvement can be as much as 50% while data rate throughput can increase by 11%. As Multiple-Input Multiple-Output (MIMO) is deployed and modem chip developers can differentiate themselves more through their products then even more significant gains can be achieved.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">151</field>
<field name="author">David Shepherd</field>
<field name="author">Fredrik Br _nnstr _m</field>
<field name="author">Frank Schreckenbach</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Adaptive Optimization of an Iterative Multiuser Detector for Turbo-coded CDMA</field>
<field name="keyword">CDMA</field>
<field name="keyword"> turbo</field>
<field name="keyword"> iterative</field>
<field name="keyword"> EXIT chart</field>
<field name="keyword"> schedule</field>
<field name="keyword"> power</field>
<field name="keyword"> optimization</field>
<field name="abstract">Extrinsic information transfer (EXIT) charts are utilized to optimize the iterative multiuser detector receiver in a multiuser turbo-coded CDMA system. The (receive) power levels are optimized for the system load using a constrained nonlinear optimization approach. The optimal decoding schedule is derived dynamically using the power optimized EXIT chart and a Viterbi search algorithm. Dynamic scheduling is shown to be a more flexible approach which results in a more stable QoS for a typical system configuration than one-shot scheduling, and

large complexity savings over a receiver without scheduling.



We verify through simulations that complexity savings of over 50% and power savings of over 8dB can be achieved. We show that the optimized power levels combined with adaptive scheduling allows for efficient utilization of receiver resources for heavily loaded systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">152</field>
<field name="author">Brian Anderson</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a Consensus in a Dynamically Changing Environment: A Graphical Approach</field>
<field name="abstract">This paper presents new graph-theoretic results appropriate for the analysis of a variety of consensus problems cast in dynamically changing environments. The concepts of rooted, strongly rooted, and neighbor-shared are defined, and conditions are derived for compositions of sequences of directed graphs to be of these types. The graph of a stochastic matrix is defined, and it is shown that under certain conditions the graph of a Sarymsakov matrix and a rooted graph are one and the same. As an illustration of the use of the concepts developed in this paper, graph-theoretic conditions are obtained which address the convergence question for the leaderless version of the widely studied Vicsek consensus problem.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">153</field>
<field name="author">Brian Anderson</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a Consensus in a Dynamically Changing Environment- convergence rates, measurement delays and asynchronous events</field>
<field name="keyword">cooperative control</field>
<field name="keyword"> graph theory</field>
<field name="keyword"> switched systems</field>
<field name="keyword"> convergence rates</field>
<field name="keyword"> delays</field>
<field name="keyword"> asynchronism</field>
<field name="abstract">This paper uses recently established properties of compositions of directed graphs together with results from the theory of nonhomogeneous Markov chains to derive worst case convergence rates for the headings of a group of mobile autonomous agents which arise in connection with the widely studied Vicsek consensus problem. The paper also uses graph-theoretic constructions to solve modified versions of the Vicsek problem in which there are measurement delays, asynchronous events, or a group leader. In all three cases the conditions under which consensus is achieved prove to be almost the same as the conditions under which consensus is achieved in the synchronous, delay-free, leaderless case.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">154</field>
<field name="author">Adi Botea</field>
<field name="author">Martin Mueller</field>
<field name="author">Jonathan Schaeffer</field>
<field name="title">Fast Planning with Iterative Macros</field>
<field name="abstract">Research on macro-operators has a long history in planning and other search applications.

There has been a revival of interest in this topic, leading to systems

that successfully combine macro-operators with current state-of-the-art

planning approaches based on heuristic search.

However, research is still necessary to make macros become a standard,

widely-used enhancement of search algorithms.

This article introduces sequences of macro-actions, called iterative macros.

Iterative macros exhibit both the potential advantages

(e.g., travel fast towards goal)

and the potential limitations (e.g., utility problem) of classical macros,

only on a much larger scale.

A family of techniques are introduced to balance

this trade-off in favor of faster planning.

Experiments on a collection of planning benchmarks show that, when

compared to low-level search and even to search with classical macro-operators,

iterative macros can achieve an impressive speed-up in search.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">155</field>
<field name="author">John-Paul Kelly</field>
<field name="author">Adi Botea</field>
<field name="author">Sven Koenig</field>
<field name="title">Planning with Hierarchical Task Networks in Video Games</field>
<field name="abstract">Artificial intelligence (AI) technology can have a dramatic

impact on the quality of a video game. AI planning methods

are useful in a wide range of game components, including

modules to control the behaviour of fully autonomous units.

However, planning is computationally expensive and the CPU

and memory resources available at runtime to a game AI

module are scarce. Offline planning can be a good strategy

to avoid a runtime performance bottleneck.

In this work we apply planning with hierarchical task networks

(HTNs) to video games. HTNs can speed up planning

dramatically, since search is guided with human-encoded

knowledge. We describe an architecture that computes plans

offline. This can be seen as a form of generating scripts automatically,

replacing the traditional approach of composing

them by hand. The results are very encouraging. Scripts are

automatically generated at a level of complexity that would

require a great human effort to create.</field>
<field name="date">2014</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">156</field>
<field name="author">Adi Botea</field>
<field name="title">Crossword Grid Composition with a Hierarchical CSP Encoding</field>
<field name="abstract">Automatic composition of crossword grids is challenging for many

interesting puzzles. This paper introduces a solving approach that uses a hierarchical

CSP encoding. At the high level, word slots are variables and all dictionary

words that fit into that slot are possible values. The low level uses each grid cell

as a variable that has all alphabet characters as possible values. Searching for a

solution explores the high-level space, instantiating entire words rather than individual

cells. Channelling constraints between the two levels reduce the high-level

search space. The benefits of the new model are demonstrated with experiments

on large puzzles.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">157</field>
<field name="author">Andre Cire</field>
<field name="author">Adi Botea</field>
<field name="title">Learning in Planning with Temporally Extended Goals and Uncontrollable Events</field>
<field name="abstract">Recent contributions to advancing planning from the

classical model to more realistic problems include using temporal

logic such as LTL to express desired properties of a solution plan.

This paper introduces a planning model that combines temporally

extended goals and uncontrollable events. The planning task is to

reach a state such that all event sequences generated from that state

satisfy the problem temporally extended goal. A real-life application

that motivates this work is to use planning to configure a system in

such a way that its subsequent, non-deterministic internal evolution

(nominal behavior) is guaranteed to satisfy a condition expressed in

temporal logic.

A solving architecture is presented that combines planning, model

checking and learning. An online learning process incrementally discovers

information about the problem instance at hand. The learned

information is useful both to guide the search in planning and to

safely avoid unnecessary calls to the model checking module. A detailed

experimental analysis of the approach presented in this paper

is included. The new method for online learning is shown to greatly

improve the system performance.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">158</field>
<field name="author">Sherif Sakr</field>
<field name="title">An Experimental Investigation of XML Compression Tools</field>
<field name="keyword">XML </field>
<field name="keyword"> Compression</field>
<field name="abstract">This paper presents an extensive experimental study of the

state-of-the-art of XML compression tools. The study reports the

behavior of nine XML compressors using a large corpus of XML

documents which covers the different natures and scales of XML

documents. In addition to assessing and comparing the performance

characteristics of the evaluated XML compression tools, the study

tries to assess the effectiveness and practicality of using these

tools in the real world. Finally, we provide some guidelines and

recommendations which are useful for helping developers and users

for making an effective decision for selecting the most suitable

XML compression tool for their needs.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">159</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="title">Congestion Aware Routing in Hybrid Wireless Mesh Networks</field>
<field name="keyword">Multi-radio</field>
<field name="keyword"> routing</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="abstract">Multi-radio Wireless Mesh Networks (WMN) are gaining lot of popularity owing to their increased application to community and public safety networks. WMNs form a static wireless backhaul to provide connectivity to mobile clients. The wireless medium, being shared and contended for, creates a number of hurdles including congestion, interference and noise. Multi-radio nodes can take advantage of the wider frequency spectrum. However, current mesh technologies employ a simplistic approach by assigning one channel for client servicing and another for the backhaul network. The improper reuse of the same channel across multiple hops causes extensive co-channel interference leading to lower bandwidth. The problem is aggravated in a hybrid WMN where the mobile clients act as routers for other clients. In this paper, we propose a congestion aware routing protocol, which can successfully establish channel diverse routes through least congested areas of a hybrid WMN. The prime advantage of the protocol is its ability to discover optimal routes in a distributed manner without the requirement of an omniscient network entity. Simulation results show that the congestion aware routing protocol can successfully achieve a high packet delivery ratio with lower routing overhead and latency in a hybrid WMN.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">160</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">Establishing High Capacity Routes in Wireless Mesh Networks</field>
<field name="keyword">Wireless</field>
<field name="keyword"> mesh</field>
<field name="keyword"> network</field>
<field name="keyword"> routing</field>
<field name="keyword"> channel</field>
<field name="abstract">A Wireless Network consists of two types of nodes, Mesh Routers and Mesh Clients. Mesh Routers are static multi-radio nodes that form the wireless back-haul. Mesh Clients are mobile single-radio nodes that communicate using the wireless back-haul. Routing protocols act as the binding force in these networks by forming a ubiquitous network environment. However, routing across heterogeneous devices encompasses a number of hurdles. One common problem is the lack of capability-aware routing in a highly contented wireless medium. In this paper, we present a variant of the Ad-hoc On-demand Distance Vector (AODV) routing protocol with the aim to exploit node heterogeneity in wireless mesh networks. The protocol endeavours to discover and establish multiple parallel links between adjacent Mesh Routers. These links are then bundled to create a high capacity virtual link. As demonstrated via extensive simulations, our protocol achieves a more than 100% improvement over standard AODV in terms of key performance metrics such as packet delivery ratio, routing overhead and latency.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">161</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">High Performance AODV Routing Protocol for Hybrid Wireless Mesh Networks</field>
<field name="keyword">Hybrid</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="keyword"> routing</field>
<field name="abstract">Hybrid Wireless Mesh Networks mimic the three main capabilities of any autonomic network i.e. self-configuring, self-optimising and self-healing. These networks are multi-hop wireless networks that can rapidly form a communication cloud in an incident area. A Hybrid Wireless Network consists of two types of nodes, Mesh Routers and Mesh Clients. Mesh Routers are more static and less resource constrained than mobile Mesh Clients, and form the wireless back-haul of the network. Routing protocols act as the binding force in these networks by forming a ubiquitous network environment. However, routing across heterogeneous devices in a ubiquitous manner, where both types of nodes participate in the routing and forwarding of packets, encompasses a number of hurdles. One common problem is the lack of capability-aware routing in a highly contented wireless medium. In this paper, we present a variant of the Ad-hoc On-demand Distance Vector (AODV) routing protocol with the aim to exploit the node heterogeneity in ubiquitous networks. As demonstrated via extensive simulations, our protocol achieves a more than 100% improvement over standard AODV in terms of key performance metrics such as packet loss, goodput and delay.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">162</field>
<field name="author">Lin Luo</field>
<field name="author">Andrew Zhang</field>
<field name="author">Zhenning Shi</field>
<field name="title">Performance of Time-Division OFDMA Systems with FDE over Frequency-Selective Fading Channels</field>
<field name="keyword">OFDMA</field>
<field name="keyword"> TD-OFDMA</field>
<field name="keyword"> FDE</field>
<field name="keyword"> Performance</field>
<field name="abstract">For alleviating the fixed and high complexity, high peak to average power ratio (PAPR), and sensitivity to carrier frequency offset problems caused by the large size of Fast Fourier Transform (FFT) in general OFDMA systems, the time-division OFDMA (TD-OFDMA) systems, based on an innovative concept of the Layered FFT structure, was recently proposed. In this paper, the theoretical analysis is presented to compare the BER

performance of OFDMA and TD-OFDMA systems in frequency selective fading channels. Two frequency domain equalization (FDE) techniques, i.e., the zero forcing (ZF) and minimum mean square error (MMSE) criteria, are considered. Theoretical analysis, verified by simulations, shows that the TD-OFDMA systems can achieve similar and even better performance than conventional OFDMA systems when a MMSE equalizer is applied.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">163</field>
<field name="author">Andrew Zhang</field>
<field name="author">Lin Luo</field>
<field name="author">Zhenning Shi</field>
<field name="title">Quadrature OFDMA Systems</field>
<field name="abstract">In current OFDMA systems, three major problems arise due to the large number of subcarriers, including high peak

to average power ratio (PAPR), sensitivity to carrier frequency offset (CFO), and high complexity in user terminal. In this paper, based on an innovative concept of Layered FFT structure, we propose novel Quadrature OFDMA system which can overcome these problems. In particular, the proposed system can achieve same guard-interval overhead and same bandwidth occupation to conventional OFDMA schemes, while with significantly reduced complexity and power consumption in user terminals. Simulation results show that the proposed Quadrature OFDMA system can achieve similar and even better performance than conventional OFDMA systems, particularly when a Minimum Mean Square Error (MMSE) equalizer is applied.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">164</field>
<field name="author">Lin Luo</field>
<field name="author">Andrew Zhang</field>
<field name="author">Zhenning Shi</field>
<field name="title">Novel Block-Interleaved Multi-code CDMA System for UWB Communications</field>
<field name="keyword">Multi-code</field>
<field name="keyword"> Block-Interleaved</field>
<field name="keyword"> CDMA</field>
<field name="keyword">ZCZ/LCZ sequences</field>
<field name="keyword"> Ultra wideband.</field>
<field name="abstract">In this paper, for the purpose of better performance with lower complexity in UWB communications, we propose a

novel Block-Interleaved Multi-code (BIM)-CDMA system with zero/low correlation zone (ZCZ/LCZ) spreading sequences acting as PN codes. Based on ZCZ/LCZ sequences, this system can effectively deal with dense multipath channels by combining the energy of multipath signals and avoiding inter-chip interference (ICI) and intersymbol interference (ISI). Also a new ZCZ sequence is developed for the system to obtain better performance. Simulation results show that the performance of the BIM-CDMA system is considerably improved over that of the direct sequence (DS)-CDMA systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">165</field>
<field name="author">Stefan Petters</field>
<field name="author">Patryk Zadarnowski</field>
<field name="author">Gernot Heiser</field>
<field name="title">Measurements or static analysis or both?</field>
<field name="keyword">real</field>
<field name="keyword">time wcet</field>
<field name="abstract">To date, measurement-based WCET analysis and static analysis have largely been seen as being at odds with each other. We argue that they should be considered complementary, and that the combination of both represents a promising approach. In this paper we discuss in some detail how we aim to improve on our probabilistic measurement-based

technique by adding static cache analysis. Specifically we are planning to make use of recent advances in the functional languages research community. The objective of this paper is not to present finished or almost finished work. Instead we hope to trigger discussion and solicit feedback from the community in order to avoid pitfalls experienced by others and help focus our research.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">166</field>
<field name="author">Dave Snowdon</field>
<field name="author">Godfrey van der Linden</field>
<field name="author">Stefan Petters</field>
<field name="author">Gernot Heiser</field>
<field name="title">Accurate Run-Time Prediction of Performance Degradation under Frequency Scaling</field>
<field name="abstract">Dynamic voltage and frequency scaling is employed to minimise energy consumption in mobile devices. The energy required to execute a piece of software is highly depedent on its execution time, and devices are typically subject to timeliness or quality-ofservice

constraints. For both these reasons, the performance at a proposed frequency setpoint must be accurately estimated. The frequently-made assumption that performance scales linearly with core frequency has shown to be incorrect, and better performance models are required which take into account the effects, and frequency setting, of the memory architecture. This paper presents a methodology, based on off-line hardware characterisation and runtime workload characterisation, for the generation of an execution time model. Its evaluation shows that it provides a highly accurate (to within 2% on average) prediction of performance at arbitrary frequency settings and that the models can be used to implement operating-system level dynamic voltage and frequency scaling schemes for embedded systems.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">167</field>
<field name="author">Gernot Heiser</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Ihor Kuz</field>
<field name="author">Gerwin Klein</field>
<field name="author">Stefan Petters</field>
<field name="title">Towards Trustworthy Computing Systems: Taking Microkernels to the Next Level</field>
<field name="keyword">security safety dependability real</field>
<field name="keyword">time</field>
<field name="abstract">As computer systems become increasingly mission-critical, used in life-critical situations, and relied upon to protect intellectual property, operating-system reliability is becoming an ever growing concern. In the past, mission- and life-critical embedded systems consisted of simple microcontrollers running a small amount of software

that could be validated using traditional and informal techniques. However, with the growth of software complexity, traditional techniques for ensuring software reliability have not been able to keep up, leading to an overall degradation of reliability. This paper argues that microkernels are the best approach for delivering truly trustworthy computer systems in the foreseeable future. It presents the NICTA operating-systems research vision, centred around the L4 microkernel and based on four core projects. The

seL4 project is designing an improved API for a secure microkernel, L4.verified will produce a full formal verification of the microkernel, Potoroo combines execution-time measurements with static analysis to determine the worst case execution profiles of the kernel, and CAmkES provides a component architecture for building systems that use the microkernel. Through close collaboration with Open Kernel Labs (a NICTA spinoff) the research output of these projects will make its way into products over the next few years.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">168</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">David Greenaway</field>
<field name="author">Sergio Ruocco</field>
<field name="title">Lazy queueing and direct process switch merit or myths?</field>
<field name="abstract">The L4 microkernel, like many first and second generation microkernels, was designed to maximise best-effort performance. One component of its functionality critical to overall

system performance is its interprocess communication primitive. L4 uses two techniques to minimise communication costs: direct process switching and lazy queue management.

These techniques improve performance at the expense of real-time predictability of the scheduler. Now that L4 is being adopted in the embedded space, which features real-time

requirements, we must determine if there is continued merit in using the optimisations. In this paper we quantitatively analyse the two optimisations using different kernel implementations and measure the performance improvements of the optimisations directly, and indirectly using the Re-aim benchmark suite. We find that the system-level performance improvements are marginal for this Unix-like workload.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">169</field>
<field name="author">Dave Snowdon</field>
<field name="author">Stefan Petters</field>
<field name="author">Gernot Heiser</field>
<field name="title">Accurate On-line Prediction of Processor and Memory Energy Usage Under Voltage Scaling</field>
<field name="keyword">Energy</field>
<field name="keyword"> Power</field>
<field name="keyword"> DVS</field>
<field name="keyword"> DVFS</field>
<field name="keyword"> PMC</field>
<field name="keyword"> Performance counter</field>
<field name="abstract">Minimising energy use is an important factor in the operation of many classes of embedded systems in particular, batterypowered devices. Dynamic voltage and frequency scaling (DVFS) provides some control over a processor s performance and energy consumption. In order to employ DVFS for managing a system s energy use, it is necessary to predict the effect this scaling has on the system s total energy consumption. Simple (yet widely-used) energy models lead to dramatically incorrect results for important classes of application programs. Predicting the energy used under scaling requires (i) a prediction of the dependency of program performance (and hence execution time) on the frequencies and (ii) a prediction of the power drawn

by the execution as a function of the frequencies and voltages. As both of these characteristics are workload-specific our approach builds a model that, given a workload execution at one frequency setpoint, will predict the run-time and power at any other frequency setpoint. We assume temporal locality (which is valid for

the vast majority of applications) so predicting the characteristics of one time slice, frame, or other instance of a task, will imply the characteristics of subsequent time slices, frames or instances (e.g. MPEG video decoding). We present a systematic approach to building these models for a hardware platform, determining the best performance counters and weights. This characterisation, done once for a particular platform, produces platform-specific but workload-independent performance

and power models. We implemented the model on a real system and evaluated it

under a comprehensive benchmark suite against measurements of the actual energy consumption. The results show that the model can accurately predict the energy use of a wide class of applications and is highly responsive to changes in the application behaviour.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">170</field>
<field name="author">Gernot Heiser</field>
<field name="title">Your system is secure? Prove it!</field>
<field name="abstract">Computer security is an old problem which has lost none of its relevance as is evidenced by the annual Security issue of ;login:. The systems research community has increased its attention to security issues in recent years, as can be seen by an increasing number of security-related papers published in the mainstream systems conferences SOSP, OSDI, and USENIX. However, the focus is primarily on desktop and server systems. 



I have argued two years ago in this place that security of embedded systems, whether mobile phones, smart cards, or automobiles, is a looming problem of even bigger proportions, yet there does not seem to be a great sense of urgency about it. Although there are embedded operating-system (OS) vendors working on certifying their offerings

to some of the highest security standards, those systems do not seem to be aimed at, or even suitable for, mobile wireless devices.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">171</field>
<field name="author">Yong Sun</field>
<field name="author">Helmut Prendinger</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">LET TECHNOLOGY ADAPT TO PRACTICE: A MULTIMODAL INPUT FUSION APPROACH FITTING CLOSER TO THE PECULIARITIES OF MULTIMODAL INPUTS</field>
<field name="abstract">By adopting the Multiset Combinatory Category Grammar in the multimodal input fusion, the proposed approach achieves efficiency while capturing the permutation in multimodal inputs. An evaluation of the approach is also presented in this paper.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">172</field>
<field name="author">Yong Sun</field>
<field name="author">Helmut Prendinger</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="author">Mitsuru Ishizuka</field>
<field name="title">THE HINGE between Input and Output: Understanding the Multimodal Input Fusion Results In an Agent-Based Multimodal Presentation System</field>
<field name="keyword">Multimodal interfaces</field>
<field name="keyword"> multimodal input fusion</field>
<field name="keyword"> input understanding</field>
<field name="keyword"> discourse representation</field>
<field name="abstract">A multimodal interface provides multiple modalities for input and output, such as speech, eye gaze and facial expression. With the recent progresses in multimodal interfaces, various approaches about multimodal input fusion and output generation have been proposed. However, less attention has been paid to how to integrate them together in a multimodal input and output system. This paper proposes an approach, termed as THE HINGE, in providing agent-based

multimodal presentations in accordance with multimodal input fusion results. The analysis of experiment result shows the proposed approach enhances the flexibility of the system while maintains its stability.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">173</field>
<field name="author">Yong Sun</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">An Efficient Unification-based Multimodal Language Processor in Multimodal Input Fusion</field>
<field name="keyword">Fusion techniques &amp; hybrid architectures</field>
<field name="keyword"> Processing of language and action patterns.</field>
<field name="abstract">A Multimodal User Interface (MMUI) allows a user to interact with a computer in a way similar to human-to-human communication, for example, through speech and gesture. Being an essential component in MMUIs, Multimodal Input Fusion should be able to find the semantic interpretation of a user s intention from recognized multimodal symbols which are semantically complementary. We enhanced our efficient

unification-based multimodal parsing processor, which has the potential to achieve low polynomial computational complexity while parsing versatile multimodal inputs within a speech and

gesture based MMUI, to handle multimodal inputs from more than two modalities. Its ability to disambiguate speech recognition results with gesture recognition results was verified with an experiment. The analysis of experiment results demonstrates that the improvement is significant after applying this technique.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">174</field>
<field name="author">Yong Sun</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">An Efficient Multimodal Language Processor for Parallel Input Strings in Multimodal Input Fusion</field>
<field name="abstract">Multimodal User Interaction technology aims at

building more natural and intuitive interfaces allowing

a user to interact with a computer in a way similar to

human-to-human communication, for example, through

speech and gesture. As a critical component in

Multimodal User Interaction, Multimodal Input Fusion

explores the ways to effectively interpret the combined

semantic interpretation of user inputs through multiple

modalities. This paper proposes a new efficient

unification-based multimodal language processor

which can handle parallel input strings for Multimodal

Input Fusion. With a structure sharing technology, it

has the potential to achieve a low polynomial

computational complexity while parsing multimodal

inputs in versatile styles. The applicability of the

proposed processor has been validated through an

experiment with multimodal commands collected from

traffic incident management scenarios. The description

of the proposed multimodal language processor and

preliminary experiment results are presented.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">175</field>
<field name="author">Yong Sun</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">Extend Chart-Parsing to Support Integration of Deictic Gesture in Natural Language Interface</field>
<field name="keyword">Natural languages</field>
<field name="keyword"> Human-machine interface</field>
<field name="keyword"> Algorithms.</field>
<field name="abstract">Natural language interface enables an efficient and effective interaction by allowing a user to submit a single phrase in natural language to the system. If deictic hand gestures are used to specify referents in speech expression, users will gain more expressive power and speech commands can be more concise. How to integrate referential expressions with deictic gestures become a critical issue in an interface using gesture input to assist speech input in expressing user s intension. This paper presents a novel approach to extend chart parsing used in natural language processing to integrate multimodal input based on speech and manual deictic gesture. It can be used to design Human-Machine Systems. It has been tested in a transport management application. The preliminary experiment of the proposed algorithm shows encouraging results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">176</field>
<field name="author">Yong Sun</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">An Input-Parsing Algorithm Supporting Integration of Deictic Gesture in Natural Language Interface</field>
<field name="keyword">Multimodal chart parsing</field>
<field name="keyword"> Multimodal Fusion</field>
<field name="keyword"> Deictic Gesture</field>
<field name="keyword"> Deictic Terms.</field>
<field name="abstract">Natural language interface (NLI) enables an efficient and effective interaction by allowing a user to submit a single phrase in natural language to the system. Free hand gestures can be added to an NLI to specify the referents for deictic terms in speech. By combining NLI with other modalities to a multimodal user interface, speech utterance length can be reduced, and users need not clearly specify the referent verbally. Integrating deictic terms with deictic gestures is a critical function in multimodal user interface. This paper presents a novel approach to extend chart parsing used in natural language processing (NLP) to integrate multimodal input based on speech and manual deictic gesture. The effectiveness of the technique has been validated through experiments, using a traffic incident management scenario where an operator interacts with a map on large display at a distance and issues multimodal commands through speech and manual gestures. The preliminary experiment of the proposed algorithm shows encouraging results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">177</field>
<field name="author">Gernot Heiser</field>
<field name="title">The Role of Virtualization in Embedded Systems</field>
<field name="keyword">virtual machines</field>
<field name="keyword"> embedded systems</field>
<field name="keyword"> operating systems</field>
<field name="keyword"> microkernels</field>
<field name="abstract">System virtualization, which enjoys immense popularity in the enterprise and personal computing

spaces, is recently gaining significant interest in the embedded domain. Starting from a comparison of key characteristics of enterprise systems and embedded systems, we will examine the differences in motivation for the use of system virtual machines, and the resulting differences in the requirements on the technology. We find that these differences

are quite substantial, and that virtualization is unable to special requirements of embedded systems.



Instead, more general operating-systems technologies are required, which support virtualization as a special case.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">178</field>
<field name="author">Gernot Heiser</field>
<field name="title">Do Microkernels Suck?</field>
<field name="keyword">operating systems</field>
<field name="keyword"> microkernels</field>
<field name="keyword"> virtual machines</field>
<field name="abstract">At the 2007 OLS, Christoph Lameter presented a talk titled Extreme High Performance Computing or Why Microkernels Suck . Having more than a passing interest in microkernels, and having deployed one in a few (dozen? hundred?) million mobile phones, I found this somewhat intriguing, and started to analyse the argumentation.



I would like to share the result with the LCA community, to help people make up their own mind. In a nutshell, I think Christoph's work is a great contribution to Linux scalability, but knowing Linux doesn't mean you know understand microkernels. In fact, I will show that the paper is, as far as microkernels are concerned, essentially based on folklore rather than fact, and fails to provide any solid evidence for its assertions. Rather than starting a flame war, I will present and examine the facts, and discuss what conclusions can be drawn from them.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">179</field>
<field name="author">Kee Siong Ng</field>
<field name="author">John Lloyd</field>
<field name="title">Probabilistic Reasoning in a Classical Logic</field>
<field name="keyword">probabilistic reasoning</field>
<field name="keyword"> classical logic</field>
<field name="keyword"> higher-order logic</field>
<field name="abstract">We offer a view on how probability is related to logic. Specifically, we argue against the widely held belief that standard classical logics have no direct way of modelling the certainty of assumptions in theories and no direct way of stating the certainty of theorems proved from these (uncertain) assumptions. The argument rests on the observation that probability densities, being functions, can be represented and reasoned with naturally and directly in (classical) higher-order logic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">180</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Evaluation of BPMN tools</field>
<field name="keyword">Tool evaluation</field>
<field name="keyword"> BPMN tools</field>
<field name="keyword"> business process modelling</field>
<field name="abstract">This paper reports our experiences with evaluating BPMN tools in the context of a project on developing an Australian process standard for the lending industry. We present the framework, the methodology and some of the major results of the evaluation. This can provide guidance for other researchers and practitioners with similar goals and constraints, in both the private and the public sector. We reveal a number of mismatches between the current BPMN tool market and the needs of potential users. Vendors may use these insights to enhance their tools in terms of performance and accessibility to testing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">181</field>
<field name="author">John Lloyd</field>
<field name="author">Kee Siong Ng</field>
<field name="title">Reflections on Agent Beliefs</field>
<field name="abstract">Some issues concerning beliefs of agents are discussed. These issues are the general syntactic form of beliefs, the logic underlying beliefs, acquiring beliefs, and reasoning with beliefs. The logical setting is more expressive and aspects of the reasoning and acquisition processes are more general than are usually considered.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">182</field>
<field name="author">John Lloyd</field>
<field name="author">Kee Siong Ng</field>
<field name="title">Probabilistic and Logical Beliefs</field>
<field name="abstract">This paper proposes a method of integrating two different concepts of belief in artificial intelligence: belief as a probability distribution and belief as a logical formula. The setting for the integration is a highly expressive logic.

The integration is explained in detail, as its comparison to other approaches to integrating logic and probability.

An illustrative example is given to motivate the usefulness of the ideas in agent applications.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">183</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Locating creativity in a framework of designing for innovation</field>
<field name="abstract">This paper focuses on creativity in the process of designing as the foundation of potential innovations resulting from that process. Using an ontological framework that defines distinct stages in designing, it identifies the locations for creativity independently of their embodiment in human designers or computational tools. The paper shows that innovation, a consequence of creativity, can arise from a large variety of processes in designing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">184</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">An ontological model of emergent design in software engineering</field>
<field name="keyword">Emergent Design</field>
<field name="keyword"> Software Engineering</field>
<field name="keyword"> Ontology</field>
<field name="abstract">This paper proposes an ontological account of a recent model of software engineering: emergent design. This model augments traditional software design approaches by accounting for incremental and unexpected modifications of the design state space. We show that the principles of emergent design are consistent with a model of designing as a reflective conversation. We use the situated function-behaviour-structure (FBS) framework to represent the activities driving emergent design. This is a basis for increasing understanding and acceptance of emergent design.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">185</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Creative designing: An ontological view</field>
<field name="abstract">This paper presents an ontological account of creative designing. It identifies processes and activities in designing that can produce novel design concepts to change the state space of possible designs. Activities that foster creativity are integrated in an ontological framework of design, the situated function-behaviour-structure (FBS) framework. This provides a foundation for locating these activities across the design steps. An important outcome of this approach is that most steps of designing can be shown to contain opportunities for creativity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">186</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">An ontology of situated design teams</field>
<field name="keyword">Design Teams</field>
<field name="keyword"> Function Behavior Structure Framework</field>
<field name="keyword"> Situatedness</field>
<field name="keyword"> Team Interaction</field>
<field name="abstract">This paper presents an ontological framework for situated design teams in which the team is both the subject and the object of designing. Team designing is modelled using the set of processes provided by the situated function-behaviour-structure (FBS) framework. This is a formal basis for understanding the drivers for change in the product to be designed and in the design team. We specifically focus on changes in a team s structure that emerge from interactions among individual team members and sub-teams.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">187</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">A function-behavior-structure ontology of processes</field>
<field name="keyword">Function Behavior Structure Framework</field>
<field name="keyword"> Process Ontology</field>
<field name="keyword"> Situatedness</field>
<field name="abstract">This paper presents how the function-behaviour-structure (FBS) ontology can be used to represent processes despite its original focus on representing objects. The FBS ontology provides a uniform framework for classifying processes and includes semantics in their representation. We show that this ontology supports a situated view of processes based on a model of three interacting worlds. A framework the situated FBS framework is then used to describe the situated design of processes.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">188</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">John S Gero</field>
<field name="title">An ontology of computer-aided design</field>
<field name="abstract">This chapter develops an ontology of computer-aided design, based on the function-behaviour-structure (FBS) ontology. It proposes two complementary views of the process of design. The object-centred view applies the FBS ontology to the artefact being designed. Integrating an ontology of three design worlds , this view establishes a framework of designing as a set of transformations between the function, behaviour and structure of the design object, driven by interactions between the three design worlds. Building on this framework, the process-centred view applies the FBS ontology to the activities defined by the object-centred view. This increases the level of detail and provides a more well-defined set of representations of these activities. Our ontological framework can be used to provide a better understanding of the functionalities required of existing and future computer-aided design support.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">189</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">Liming Zhu</field>
<field name="title">An ontologically-based evaluation of software design methods</field>
<field name="abstract">This paper develops an ontological basis for evaluating software design methods, based on the situated function-behaviour-structure (FBS) framework. This framework accounts for the situatedness of designing, viewing it as a dynamic activity driven by the interactions between designers and the artefacts being designed. Based on this framework, we derive a general evaluation schema that we apply to five software design methods. The ideas presented in this work contribute to a better understanding of design methods, and uncover opportunities for method integration and development.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">190</field>
<field name="author">Felix Werner</field>
<field name="author">Joaquin Sitte</field>
<field name="author">Frederic Maire</field>
<field name="title">Automatic Place Determination using Colour Histograms and Self-Organising Maps</field>
<field name="keyword">Mobile Robotics</field>
<field name="keyword"> Self-Organising Maps</field>
<field name="keyword"> Colour Histogram</field>
<field name="keyword"> Topological Localisation</field>
<field name="abstract">In this paper we propose a model-free appearance-based method to automatically determine places as landmarks for topological navigation. Most of the current approaches for automatically selecting landmarks are based on template models or complex feature detectors. We use modified colour histograms, more precisely an entropy-constrained 3D~colour clustering as appearance-based image features which adapt to the colour distribution of the environment. An unsupervised neural network learning strategy is used to automatically determine places by clustering the modified histograms. Results from experiments in an indoor environment with a robot equipped with a panoramic camera show that the places, which were clustered in histogram space refer to physically close positions in the world domain in a large degree and can be used as landmarks for navigation purposes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">191</field>
<field name="author">Felix Werner</field>
<field name="author">Joaquin Sitte</field>
<field name="author">Frederic Maire</field>
<field name="title">On the Induction of Topological Maps from Sequences of Colour Histograms</field>
<field name="abstract">This paper presents an appearance--based method to automatically determine places from vision data for topological mapping. The approach exploits the continuity of the visual appearance of consecutive images when a robot traverses the environment. Places are determined by clustering colour histograms, and a probabilistic filtering strategy eliminates spurious places with weak evidence. Further, we discuss steps towards the induction of the topology of an environment from a sequence of visited places. Particularly, our system faces the problem of physically different places which appear identical in perception space. We present results from experiments on two data sets, one consist of panoramic images and another one includes images from a standard camera.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">192</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Comparative Colorimetric Simulation and Evaluation of Digital Cameras Using Spectroscopy Data</field>
<field name="keyword">colorimetry</field>
<field name="keyword"> photogrammetry</field>
<field name="keyword"> hyperspectral imaging</field>
<field name="keyword"> reflectance modelling</field>
<field name="abstract">In this paper, we present a comparative simulation and

evaluation of digital cameras using spectroscopy, colorimetry

and photogrammetry. Here, we compare the output of

commercially available cameras to that yielded using the

CIE-1931 colour standard and the colour matching functions

proposed by Stiles and Burch [14]. We present a

method to estimate the colour yielded by trichromatic cameras

based upon their spectral response. We then present

the results on three experimental vehicles and three commercially

available cameras. We perform simulation using

the spectrum of light emitted by black body radiators, the

spectra of sample colours widely used for colour correction

and present renderings using the spectra of known materials

under controlled illumination conditions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">193</field>
<field name="author">Michael Norrish</field>
<field name="author">Ren Vestergaard</field>
<field name="title">Proof Pearl: de Bruijn Terms Really Do Work</field>
<field name="keyword">lambda</field>
<field name="keyword">calculus theorem</field>
<field name="keyword">proving</field>
<field name="abstract">Placing our result in a web of related mechanised results, we give a direct proof that the de Bruijn -calculus ( la Huet, Nipkow and Shankar) is isomorphic to an -quotiented -calculus. In order to establish the link, we introduce an index-carrying abstraction mechanism over de Bruijn terms, and consider it alongside a simplified substitution mechanism. Relating the new notions to those of the -quotiented and the proper de Bruijn formalisms draws on techniques from the theory of nominal sets.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">194</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dhammika Jayalath</field>
<field name="author">Ying Chen</field>
<field name="title">Asymmetric OFDM Systems Based on Layered FFT Structure</field>
<field name="keyword">ofdm</field>
<field name="abstract">In this letter, we first extend the convolution theory of discrete Fourier transform (DFT) and introduce a structure of layered fast Fourier transform (FFT). Based on this framework, we propose novel asymmetric orthogonal frequency-division multiplexing (OFDM) systems that bridge general OFDM and single carrier systems. Adaptive to the capability of the transceiver, asymmetric OFDM systems provide significant flexibility in system design and operation. We show how effects of noise enhancement and frequency diversity counteract each other in asymmetric OFDM systems. Performance comparison with general OFDM and single carrier systems is also given.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">195</field>
<field name="author">Jie Xu</field>
<field name="author">getian.ye@ Getian</field>
<field name="author">jian.zhang@ Jian</field>
<field name="title">Long-term Trajectory Extraction for Moving Vehicles</field>
<field name="keyword">trajectory</field>
<field name="keyword"> SIFT</field>
<field name="abstract">In recent years, trajectory analysis of moving vehicles in video-based traffic monitoring systems has drawn the attention of many researchers. Trajectory extraction is a fundamental step that is required prior to trajectory analysis. Lots of previous work have focused on trajectory extraction via tracking. However, they often fail to achieve long-term consistent trajectories. In this paper, we propose a robust approach for extracting long-term trajectories of moving vehicles in traffic monitoring using SIFT-descriptor. Experimental results show that the proposed method outperforms tracking-based techniques.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">196</field>
<field name="author">Imtiaz Husain</field>
<field name="author">Jinhong Yuan</field>
<field name="author">Andrew Zhang</field>
<field name="title">Modified channel shortening receiver based on MSSNR algorithm for UWB channels</field>
<field name="abstract">A modified version of the maximum shortening signal-to-noise ratio (MSSNR) algorithm for channel shortening in ultra-wideband (UWB) communication systems is presented. The proposed algorithm introduces two additional UWB channel related parameters in the optimisation problem along with the conventional energy criterion. This modification significantly improves the performance of the conventional MSSNR algorithm and enables it to handle the extreme nature of channel shortening needed in UWB systems. The proposed algorithm outperforms the conventional MSSNR algorithm in terms of different comparative parameters</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">197</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Ralf Huuck</field>
<field name="author">Sean Seefried</field>
<field name="author">Michael Tapp</field>
<field name="title">Fade to Grey: Tuning Static Program Analysis</field>
<field name="keyword">static analysis</field>
<field name="keyword"> goanna</field>
<field name="keyword"> software verification</field>
<field name="keyword"> case study</field>
<field name="abstract">Static program analysis complements traditional dynamic testing by discovering generic patterns and relations in source code, which indicate software deficiencies such as memory corruption, unexpected program behavior and memory leaks. Since static program analysis builds on approximations of a program's concrete behavior there is often a trade-off between reporting potential bugs that might be the result of an over-approximation and silently suppressing those defects in that grey area. While this trade-off is less important for small files it has severe implications when facing large software packages, i.e., 1,000,000 LoC and more. In this work we report on experiences with using our static C/C++ analyzer Goanna on such large software systems, motivate why a flexible property specification language is vital, and present a number of decisions that had to be made to select the right checks as well as a sensible reporting strategy. We illustrate our findings by empirical data obtained from regularly analyzing the Firefox source code.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">198</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Ralf Huuck</field>
<field name="author">Sean Seefried</field>
<field name="title">Incremental False Path Elimination for Static Software Analysis</field>
<field name="keyword">static analysis</field>
<field name="keyword"> counterexample guided abstraction refinement</field>
<field name="keyword"> interval solving</field>
<field name="keyword"> software verification</field>
<field name="keyword"> goanna</field>
<field name="abstract">In this work we introduce a novel approach for removing false positives in static program analysis. We present an incremental algorithm that investigates paths to failure locations with respect to feasibility. The feasibility test it done by interval constraint solving over a semantic abstraction of program paths. Sets of infeasible paths can be ruled out by enriching the analysis incrementally with observers. Much like counterexample guided abstraction refinement for software verification our approach enables to start static program analysis with a coarse syntactic abstraction and use richer semantic information to rule out false positives when necessary and possible. Moreover, we present our implementation in the Goanna static analyzer and compare it to other tools for C/C++ program analysis.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">199</field>
<field name="author">Patrik Haslum</field>
<field name="author">Malte Helmert</field>
<field name="author">Blai Bonet</field>
<field name="author">Adi Botea</field>
<field name="author">Sven Koenig</field>
<field name="title">Domain-Independent Construction of Pattern Database Heuristics for Cost-Optimal Planning</field>
<field name="abstract">Heuristic search is a leading approach to domain-independent planning.

For cost-optimal planning, however, existing admissible heuristics are

generally too weak to effectively guide the search.

Pattern database heuristics (PDBs), which are based on abstractions of the

search space, are currently one of the most promising approaches to

developing better admissible heuristics.

The informedness of PDB heuristics depends crucially on the selection of

appropriate abstractions (patterns).

Although PDBs have been applied to many search problems, including planning,

there are not many insights into how to select good patterns, even manually.

What constitutes a good pattern depends on the problem domain, making the

task even more difficult for domain-independent planning, where the process

needs to be completely automatic and general.

We present a novel way of constructing good patterns automatically from the

specification of planning problem instances. We demonstrate that this allows

a domain-independent planner to solve planning problems optimally in some

very challenging domains, including a STRIPS formulation of the Sokoban

puzzle.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">200</field>
<field name="author">Malte Helmert</field>
<field name="author">Patrik Haslum</field>
<field name="author">Jorg Hoffmann</field>
<field name="title">Flexible Abstraction Heuristics for Optimal Sequential Planning</field>
<field name="abstract">We describe an approach to deriving consistent heuristics for automated planning, based on explicit search in abstract state spaces. The key to managing complexity is interleaving composition of abstractions over different sets of state variables with abstraction of the partial composites.



The approach is very general and can be instantiated in many different ways by following different \emph{abstraction strategies}. In particular, the technique subsumes \emph{planning with pattern databases} as a special case. Moreover, with suitable abstraction strategies it is possible to derive perfect heuristics in a number of classical benchmark domains, thus allowing their optimal solution in polynomial time.



To evaluate the practical usefulness of the approach, we perform empirical experiments with one particular abstraction strategy. Our results show that the approach is competitive with the state of the art.</field>
<field name="date">2014</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">201</field>
<field name="author">Blai Bonet</field>
<field name="author">Patrik Haslum</field>
<field name="author">Sarah Hickmott</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">Directed Unfolding of Petri Nets</field>
<field name="abstract">The key to efficient on-the-fly reachability analysis based on

unfolding is to focus the expansion of the finite prefix towards the

desired marking. However, current unfolding strategies typically

equate to blind (breadth-first) search. They do not exploit the

knowledge of the marking that is sought, merely entertaining the hope

that the road to it will be short. This paper investigates

\emph{directed unfolding}, which exploits problem-specific information

in the form of a heuristic function to guide decisions to the desired

marking. In the unfolding context, heuristic values are estimates of

the distance between configurations. We show that suitable heuristics

can be automatically extracted from the original net. We prove that

unfolding can rely on heuristic search strategies while preserving the

finiteness and completeness of the generated prefix, and in some

cases, the optimality of the firing sequence produced. We also

establish that the size of the prefix obtained with a useful

class of heuristics is never worse than that obtained by blind

unfolding. Experimental results demonstrate that directed unfolding

scales up to problems that were previously out of reach of the

unfolding technique.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">202</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="author">luc Knockaert</field>
<field name="title">Maximum a posteriori maximum entropy signal denoising</field>
<field name="keyword">wavelet denoising</field>
<field name="keyword"> bayesian estimation</field>
<field name="keyword"> maximum entropy</field>
<field name="abstract">When fitting wavelet based models, shrinkage of the empirical

wavelet coefficients is an effective tool for signal denoising.

Based on different approaches, different shrinkage functions have

been proposed in the literature. The shrinkage functions derived

using Bayesian estimation theory depend on the prior used on the

wavelet coefficients. However, no simple and direct method exists

for the choice of the prior. In this paper a new method based on

maximum entropy considerations is proposed for the construction of

the prior on the wavelet coefficients. The new shrinkage function

is obtained by coupling this prior to maximum a posteriori

arguments. A comparison with classical shrinkage functions is

given in a simulation example of image denoising in order to

illustrate the effectiveness of the proposed thresholding method.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">203</field>
<field name="author">Francesca Rossi</field>
<field name="author">Peter van Beek</field>
<field name="author">Toby Walsh</field>
<field name="title">Constraint Programming</field>
<field name="abstract">An overview of the field of constraint programming.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">204</field>
<field name="author">Saad Khan</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">Performance Comparison of Reactive Routing Protocols for Hybrid Wireless Mesh Networks</field>
<field name="keyword">Multi-radio</field>
<field name="keyword"> routing</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="abstract">Wireless mesh networks have recently gained a lot of popularity due to their rapid deployment and instant communication capabilities. These networks comprise of somewhat static multi-radio Mesh Routers, which essentially provide connectivity between the mobile single-radio Mesh Clients. Special routing protocols are employed, which facilitate routing between the Mesh Routers as well as between the Mesh Routers and the Mobile Clients. AODV is a well known routing protocol that can discover routes on-the-fly in a mobile environment. The protocol is highly scalable and can support thousands of nodes making it an ideal protocol for wireless mesh networks. However, as the protocol was actually developed for single-radio nodes, it frequently lacks the ability to exploit the potential offered by the Mesh Routers and, hence, sub-optimal routing takes place in a mesh environment. This paper gives an overview of four variants of the AODV routing protocol that can potentially be used for mesh formation. In order to determine their aptness for application to hybrid wireless mesh networks, this paper presents their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">205</field>
<field name="author">Saad Khan</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">Robust Communications using Wireless Mesh Networks for Safeguarding Water Infrastructures</field>
<field name="keyword">Security</field>
<field name="keyword"> Wireless Mesh Networks</field>
<field name="keyword"> Communication</field>
<field name="abstract">Since ancient times, water has been considered a vital resource for the survivability of nations. It is a resource that calls for impregnable security and incessant protection. Water infrastructures generally consist of dams and associated water-supply systems. These infrastructures are vulnerable to a number of natural and man-made disasters. The threat of natural calamities such as cyclones, earthquakes and tsunamis and man-made disasters like terrorist attacks are now more imminent than ever. In spite of these facts, water infrastructures are still being provided with meagre communication support such as land-lines and mobile phones for use in normal and emergency scenarios. These communication resources are in fact vulnerable to the very same catastrophes as those threatening the water infrastructures themselves. In this paper we present a communication infrastructure based on wireless mesh networks, which can satisfy the two main communication requirements of any water infrastructure, i.e. surveillance and crisis communications. This paper has been mainly written with Australian water infrastructures in mind. However, emphasise is made on the application of the proposed communication network being applicable to any water infrastructure. We argue that wireless mesh networks can provide the requisite dual-role communication functionality for water infrastructures, even in the advent of severe calamities or disasters.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">206</field>
<field name="author">June Verner</field>
<field name="title">Quality Software Development: What do we need to improve in the software development process?</field>
<field name="keyword">failure factors</field>
<field name="keyword"> improvement of the development process</field>
<field name="keyword"> software development quality</field>
<field name="keyword"> software failure</field>
<field name="abstract">EXTENDED ABSTRACT

There are a number of authors who have documented problems with our software development processes. Unrealistic schedules and budgets, with continuing steams of requirements changes lead to projects with a high risk of failure. There is too often only a vague idea of the required solution at the outset of a project and this makes it difficult for estimators and developers. It is commonly accepted that there will be requirements changes throughout the project. This means that managing changes becomes a challenge. 



Most IT experts agree that failures occur far more often than they should. Most developments are expensive, with a difficult process affected by a series of problems including poor project management, cost and schedule overruns, poor quality software and under-motivated developers. Unfortunately few project post-mortems are conducted, and little understanding is gained from the results of past projects Most project failures are predictable and avoidable, but many project managers cannot identify what success and failure factors are important, early enough to take action. 



 Many factors are described in the literature including project pressures with project stakeholders impacting the cost and quality of the project, organizational structure, communication with customer/users, and amongst customers, developers scheduling. project budget, and inaccurate estimates of resources, poor project management, poor reporting of project status, unmanaged risks, inability to handle project complexity, the project management process and tracking tools, sloppy development practices, software development methodologies, use of immature technology, product quality, business processes and resources.

 

 



There are generally more than 1 or 2 reasons for a project failure which may be caused by a combination of technical, project management and business decisions. Most organizations do not see preventing failure as an urgent matter. Unfortunately, the literature on project failure is based on handful of failed project case studies. 

 

Our research is concerned with getting quantitative evidence on aspects contributing to failure. If we know what factors are responsible for failure we can focus on these factors and improve the quality of our development processes for future projects. We also should recognize effects of project failure on development staff. Late projects usually cause long hours of unpaid overtime, loss of motivation, and stress. This then leads to high staff turnover and its associated costs. This research provides an update on prior studies and tests the validity of previously reported anecdotal evidence about project failure factors. We build on previously reported research but our concern is with everyday projects, not the high profile projects normally reported in the newspapers.



We collected a set of project data and teased it out to find the main failure factors for a whole group of projects. We first developed a questionnaire targeting software developers perceptions of practices affecting project outcomes. Our questionnaire was based on extensive discussions with 90+ software developers as well as the literature. The survey included 88 questions organized into sections covering sponsor/senior management, customer/user, requirements, estimation and scheduling, the development process, the project manager and project management, and the development team. We also asked if the project was a success or a failure. The questionnaire was distributed to practitioners from a large U.S. financial institution where each responded by answering it twice. It was then distributed to various companies in North Eastern U.S.A., a number of different Australian companies; and a number of different Chilean organizations; each respondent in the latter groups answered the questionnaire once. 



We collected data on 304 projects, 70 were failures. Of the failed projects, 49 were in-house and 21 outsourced, 65 were development projects and 5 maintenance projects. .The largest project that failed had 180 developers. We examined the data using frequency analyses and as well investigated relationships between the most important factors</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">207</field>
<field name="author">Bee Bee Chua</field>
<field name="author">June Verner</field>
<field name="title">Estimation of rework effort based on requirements categorization</field>
<field name="keyword">cost estimation</field>
<field name="keyword"> rework effort</field>
<field name="keyword"> requirements changes</field>
<field name="abstract">Poor estimates of effort and time are frequently responsible for software project failure. Therefore, there is a need to investigate and better understand the impact of requirement changes on software development projects in terms of effort.



Requirement changes are unavoidable and unpredictable during the development of most software projects. Regardless of the software development team size or the size of the project, every requirements change must be dealt carefully to ensure that every team member understands what the requirement change is, how the change is to be implemented and the amount of time needed to correct or to modify the software.



To estimate accurately the effort required during rework for any requirements change is a difficult task for many experienced cost and schedule estimators. This is because there is normally not enough data available on which to base a model, and there is no useful categorization of the various requirements types for estimating the amount of effort. 



IT change control forms designed by industrial practitioners are not based on any theoretical framework that categorizes requirements changes in a logical, economical and structured manner. Practitioners often design change control forms based on the types of requirements outlined in project without a real understanding of the requirement s actual characteristics and attributes. Until now, there has been no standard method for categorizing requirements changes able to provide an understanding (both analytical and theoretical) that will enable IT practitioners to categorize requirements changes at both the project level and the requirements level. We provide such a categorization and use this as the basis of a rework effort estimation model.



This research presents an empirical model that categorizes requirements changes based on change control forms and uses the categorization as input into a cost estimation model. This empirical model will benefit IT practitioners by providing them with a method for better effort estimation. The added contribution is that it can help mitigate project risks early.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">208</field>
<field name="author">Lili Marziana Abdullah</field>
<field name="author">June Verner</field>
<field name="title">Risk framework for outsourced strategic IT system development from the client perspective</field>
<field name="keyword">outsourcing</field>
<field name="keyword"> strategic IT</field>
<field name="keyword"> project success</field>
<field name="keyword"> project risk</field>
<field name="abstract">While a series of risks connected with IT development and IT outsourcing are identified in the literature research on risk in the context of strategic IT system development outsourcing is almost non-existent. This study investigates risks that are most likely to cause failure in a strategic IT system development outsourcing project, from the client perspective. This study examines strategic IT systems from a business strategy perspective and presents the characteristics of a strategic IT system by synthesizing material from the management literature and the IT literature. Three different streams of literature, namely IT development, strategic IT, and IT outsourcing, and reported examples of strategic IT system development outsourcing project failure are examined. Based on the risks in the literature and using IT outsourcing stages as a basis, a risk framework is developed and tested in a preliminary case study. The case study investigates the development of a strategic system jointly developed by an in-house IT team and outsourcing vendors in Australia. The risk framework developed and tested here will assist clients to focus their attention on risks that can lead to failure of the outsourced strategic IT system developments.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">209</field>
<field name="author">Shuichiro Yamamoto</field>
<field name="author">Komon Ibe</field>
<field name="author">June Verner</field>
<field name="author">Karl Cox</field>
<field name="author">Steven Bleistein</field>
<field name="title">Actor Relationship Analysis for the i* Framework</field>
<field name="abstract">The i* framework is a goal-oriented approach that addresses organizational IT requirements engineering concerns, and is considered an effective technique for analyzing dependencies between actors. However, the effectiveness and limitations of i* are unclear. When we modelled an industrial case with a large number of actors using i*, we discovered difficulties in (1) validating the completeness of the model, and (2) managing change. To solve these problems, we propose an actor relationship matrix analysis method (ARM) as a precursor to i* modeling, which we found aided in addressing the above two issues. This paper defines our method and demonstrates it with a case study. ARM enables requirements engineers to better ensure completeness of requirements in a repeatable and systematic manner that does not currently exist in the i* framework.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">210</field>
<field name="author">Abdul Babar</field>
<field name="author">Karl Cox</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Steven Bleistein</field>
<field name="author">June Verner</field>
<field name="title">Integrating B-SCP and MAP to Manage the Evolution of Strategic IT Requirements</field>
<field name="keyword">Requirements engineering</field>
<field name="keyword"> evolution</field>
<field name="keyword"> business strategy</field>
<field name="keyword"> strategic alignment</field>
<field name="keyword"> B-SCP</field>
<field name="keyword"> MAP</field>
<field name="abstract">This paper presents the first steps in a research project that integrates two requirements engineering methodologies, B-SCP and MAP, in order to manage the evolution of strategic IT. Our integration approach presents a mechanism to validate and verify MAP requirements against B-SCP requirements and vice versa. MAP has an inbuilt Gap Analysis process which saves the overhead of inventing a new approach to deal with requirements evolution. In addition, MAP extends B-SCP's capability by the addition of non-deterministic process modelling. Our solution is evaluated on an exemplar case study which looks at a system built for Seven Eleven Japan.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">211</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Basem Suleiman</field>
<field name="author">Hanan Lutfiyya</field>
<field name="title">UML Profiles for WS-Policy4MASC as Support for Business Value Driven Engineering and Management of Web Services and Their Compositions</field>
<field name="abstract">WS-Policy4MASC is an XML language for specifica-tion of policies for run-time Web service management (monitoring and control) activities executed by the Man-ageable and Adaptable Services Compositions (MASC) middleware. Among its original contributions are specifi-cation of diverse business values (benefits or costs, tangi-ble or intangible, agreed or possible, absolute or relative) and specification of various control strategies maximizing different business values (e.g., only agreed intangible benefits). To facilitate development of Web service sys-tems that can be managed with WS-Policy4MASC and the MASC middleware and to improve alignment between run-time management activities and design-time models, we developed novel UML profiles for WS-Policy4MASC. Their original contributions are in improved support for: a) specification of run-time management activities and business values within design-time models, b) automatic creation of run-time management policies from design-time models, c) feedback of run-time management infor-mation values into analysis of design-time models. We validated our solutions on detailed examples.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">212</field>
<field name="author">Basem Suleiman</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Eldar Aliev</field>
<field name="title">Time-Quality Metric Model for Quality Measurement of Web-Based Systems</field>
<field name="abstract">The dramatic growth of e-commerce and the reliance of organizations success on quality of their Web systems have required the need for building high quality Web-based applications with minimum time and effort. Therefore, Web application s quality measurements and metrics are becoming increasingly important. Although many quality metric models for web-based applications have been proposed but they introduced a set of ambiguous guidelines and rules and they were not validated theoretically and empirically. We propose a Time-Quality metric model for Web-based systems quality measurement that (1) is based on set of web quality characteristics defined by ISO 9126 standard and (2) uses the percentage contribution of each characteristic (i.e. the importance of each characteristic) to determine the web quality and (3) considers the impact of development time (i.e. using WebMo model as measurement for web system development time) on quality of web-based system. The model consists of two measurement phases; first the percentage contribution of each quality characteristic and the development time for a Web system is initially estimated using predefined formulas, then same measurements are carried out but after building the system. The quality of the developed system is then adjusted by development time factor. We argue that Web-based quality measurement model should consider the development time as a key impact on the quality of Web systems beside a set of essential quality characteristics. The model is used to evaluate a real-world Web application example and to illustrate its applicability. The example shows how the total quality of the Web system is affected. Although the model quantifies set of quality characteristics and introduced mathematical formulas for measuring quality of Web applications but it suffers from subjectivity used in evaluating quality characteristics which would lead to imprecision in measurement. Further, it needs to consider other measurement quality factors such as different resources that a Web application could interact with during run-time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">213</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Basem Suleiman</field>
<field name="author">Abdul Babar</field>
<field name="title">SPECIFICATION OF BUSINESS VALUE WITH AND IN SOFTWARE PATTERNS</field>
<field name="abstract">Business value is a crucial, but frequently neglected, aspect of software quality. Specification of business value with and in software patterns promises improved software quality. Here, with refers to patterns describing best practices in modeling, monitoring, and control of business value of software. Contrary, in refers to annotations of existing patterns (for various problems) with business value information. We illustrate our discussion on the WS-Policy4MASC language and the UML profiles for it. WS-Policy4MASC is an XML language for specification of policies for run-time Web service monitoring and control activities. It supports specification of diverse business values and various business strategies for maximizing business values. The UML profiles for WS-Policy4MASC can be used for specification of business value (and other quality aspects) both with and in software patterns.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">214</field>
<field name="author">Basem Suleiman</field>
<field name="title">Commercial-Off-The-Shelf Software Development Framework</field>
<field name="abstract">Budget and schedule savings are the driving factors for the adoption of Commercial-Off-The-Shelf (COTS) software components by software development organizations. The reliance on COTS components has lead to Component-Based Development (CBD) software systems and introduced changes to the software development process and hence software project management responsibilities and roles. This paper introduces a general

framework discussion of essential management aspects for CBD, focusing on COTS. Particularly,stakeholders, requirements, component selection and architecture management issues are discussed from different angles. Some CBD management guidelines and best practices for these aspects are outlined in the conclusion. In addition, CBD management challenges are drawn along with some suggestions in the conclusion section.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">215</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">The Weighted CFG Constraint</field>
<field name="abstract">We introduce the weighted CFG constraint and propose a propaga-

tion algorithm that enforces domain consistency in O(n^3 |G|) time. We show that

this algorithm can be decomposed into a set of primitive arithmetic constraints

without hindering propagation.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">216</field>
<field name="author">Nicola Stokes</field>
<field name="author">Yi Li</field>
<field name="author">Lawrence Cavedon</field>
<field name="author">Justin Zobel</field>
<field name="title">Exploring Abbreviation Expansion for Genomic Information Retrieval</field>
<field name="keyword">genomic information retrieval</field>
<field name="keyword"> query expansion</field>
<field name="abstract">Abbreviations are commonly found instances of synonymy in Biomedical journal papers. Information retrieval systems that index paragraphs rather than full-text articles are more susceptible to term variation of this kind, since abbreviations are typically only defined once at the beginning of the text. One solution to this problem is to expand the user query automatically with all possible abbreviation instances for each query term. In this paper, we compare the effectiveness of two abbreviation expansion techniques on the TREC 2006 Genomics Track queries and collection. Our results show that for highly ambiguous abbreviations the query collocation effect isn t strong enough to deter the retrieval of erroneous passages. We conclude that full-text abbreviation resolution prior to passage indexing is the most appropriate approach to this problem.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">217</field>
<field name="author">Benjamin Goudey</field>
<field name="author">Nicola Stokes</field>
<field name="author">David Martinez</field>
<field name="title">Exploring extensions to machine learning-based gene normalisation</field>
<field name="keyword">biomedical text mining</field>
<field name="keyword"> genomic entity-recognition</field>
<field name="abstract">One of the foundational text-mining tasks in the biomedical domain is the identification of genes and protein names in journal papers. However, the ambiguous nature of gene names means that the performance of information management tasks such as query-based retrieval will suffer if gene name mentions are not explicitly mapped back to a unique identifier in order to resolve issues relating to synonymy (i.e. many different lexical forms representing the same gene) and ambiguity (i.e. many distinct genes sharing the same lexical form). This task is called gene name normalisation, and was recently investigated at the BioCreative Challenge (Hirschman et al., 2004b), a text-mining evaluation forum focusing on core biomedical text processing tasks. In this work, we present a machine learning approach to gene normalisation based on work by Crim et al. (2005). We compare this system with a number of simple dictionary lookup-based methods. We also investigate a number of novel features not used by Crim et al. (2005). Our results show that it is difficult to improve upon the original set of features used by Crim et al. We also show that for some organisims gene name normalisation can be successfully performed using simple dictionary lookup techniques.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">218</field>
<field name="author">Abdul Babar</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Aligning the Map Requirements Modelling with the B-method for Formal Software Development</field>
<field name="abstract">We present a software development approach that aligns a requirements elicitation technique with a formal

method of software specification abstraction. The goal/strategy modeling technique Map augmented with

Jackson s context diagrams (representing environment) is used to elicit requirements and the B-method

is used to translate Map requirements into formal specifications. Comprehensive tool support allows the

B-method to refine and implement the specification correctly. Our approach brings improvement to an

approach that uses generic requirements for rigorous software development. The resulting specification

model bridges the gap between software requirements and formal specifications and supports automatic refinement of strategic requirements into software code. To illustrate how our approach bridges this gap, we

discuss the Point of Sale (PoS) requirements model of Seven Eleven Japan (SEJ).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">219</field>
<field name="author">Silvia Richter</field>
<field name="author">Malte Helmert</field>
<field name="author">Charles Gretton</field>
<field name="title">A Stochastic Local Search Approach to Vertex Cover</field>
<field name="abstract">We introduce a novel stochastic local search algorithm for the 

vertex cover problem. Compared to current exhaustive search

techniques, our algorithm achieves excellent performance on a suite of

problems drawn from the field of biology. We also evaluate our

performance on the commonly used DIMACS benchmarks for the

related clique problem, finding that

our approach is competitive with the

current best stochastic local search algorithm for finding cliques.

On three very large problem instances, our algorithm establishes new

records in solution quality.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">220</field>
<field name="author">Abdul Babar</field>
<field name="author">Karl Cox</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Steven Bleistein</field>
<field name="author">June.Verner</field>
<field name="title">Identifying Domain Context for the Intentional Modelling Technique MAP</field>
<field name="abstract">Context identification is an important feature of goal modelling techniques. It helps to understand

wider organisational systems during requirements engineering. The goal of this paper is to identify

domain context for the process driven requirements modelling technique MAP. We present our

preliminary research on adding domain context to MAP by using Jackson s context diagrams. The

resulting model shows a clear picture of domain entities involved in the MAP processes. We validate

our approach on a case dealing with the Point of Sale system of Seven Eleven Japan (SEJ).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">221</field>
<field name="author">Abdelkarim Erradi</field>
<field name="author">Piyush Maheshwari</field>
<field name="author">Vladimir Tosic</field>
<field name="title">WS-Policy Based Monitoring of Composite Web Services</field>
<field name="abstract">MASC (Manageable and Adaptive Service Compositions) is a policy-based middleware for

monitoring and control of composite Web services execution. The monitorable requirements are specified in

the WS-Policy4MASC language that extends WS-Policy by defining new types of monitoring and control policy

assertions. This paper focuses on MASC monitoring capabilities to detect business exceptions and runtime

faults. Our solutions are complementary to the existing approaches and provide: synchronous and asynchronous

monitoring both at the SOAP messaging layer and the process orchestration layer, greater diversity of

monitoring and control constructs, as well as the externalization of monitoring and adaptation actions

from definitions of business processes. We implemented a MASC proof-of-concept prototype and evaluated it on

monitoring and adaptation scenarios from a stock trading case study. Our performance studies indicate that MASC overhead and scalability are acceptable.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">222</field>
<field name="author">Abdelkarim Erradi</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Piyush Maheshwari</field>
<field name="title">MASC - .NET-Based Middleware for Adaptive Composite Web Services</field>
<field name="abstract">MASC (Manageable and Adaptive Service Compositions) is a policy-based middleware for monitoring of Web service compositions and their dynamic adaptation to various runtime changes. MASC policies are described in our new WS-Policy extension called WS-Policy4MASC. Compared with recent related works, MASC has several distinctive characteristics, such as coordination of adaptation on the SOAP messaging layer and the business process orchestration layer, use of both technical and business metrics for adaptation decisions, and extending the power and flexibility of the new Microsoft .NET 3.0 platform. In this paper, we focus on MASC support for adaptation to address business exceptions and manage runtime faults. For example, a sub-process (or an activity) can be added, deleted, replaced, skipped, or retried. We have been implementing a MASC proof-of-concept prototype and evaluating it on adaptation scenarios from a stock trading case study. Our performance studies of the prototype indicate that overheads introduced by MASC are acceptable.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">223</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Abdelkarim Erradi</field>
<field name="author">Piyush Maheshwari</field>
<field name="title">WS-Policy4MASC - A WS-Policy Extension Used in the MASC Middleware</field>
<field name="abstract">WS-Policy4MASC is a new XML language that we developed for specification of monitoring and control particularly,

adaptation) policies in the Manageable and Adaptable Services Compositions (MASC) middleware. It extends the

Web Services Policy Framework (WS-Policy) by defining new types of policy assertions. Goal policy assertions specify requirements and guarantees to be met in desired normal operation. Action policy assertions specify actions to be taken if certain conditions are met or not met. Utility policy assertions specify monetary values assigned to particular situations. Meta-policy assertions are used to specify which action policy assertions are alternatives and which business value-driven conflict resolution strategy should be used. WSPolicy4MASC also enables detailed specification of additional information necessary for run-time policy-driven management. We evaluated feasibility of the WSPolicy4MASC solutions by implementing a policy repository and other modules in MASC. We examined their usefulness on a set of realistic scenarios.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">224</field>
<field name="author">Karl Goeschka</field>
<field name="author">Schahram Dustdar</field>
<field name="author">Frank Leymann</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Proceedings of Middleware for Service Oriented Computing (MW4SOC 2007)</field>
<field name="keyword">middleware</field>
<field name="keyword"> service-oriented architecture</field>
<field name="keyword"> Web services</field>
<field name="abstract">Service Oriented Computing (SOC) is a computing paradigm broadly pushed by vendors, utilizing services to support the rapid development of distributed applications in heterogeneous environments. The visionary promise of SOC is a world of cooperating services being loosely coupled to flexibly create dynamic business processes and agile applications that may span organizations and computing platforms and can nevertheless adapt quickly and autonomously to changes of requirements or context.

Consequently, the subject of Service Oriented Computing is vast and enormously complex, spanning any concepts and technologies that find their origins in diverse disciplines like Workflow Management Systems, Component Based Computing, "classical" Web applications, and Enterprise Application Integration (EAI) including Message Oriented Middleware. In addition, there is a strong need to merge technology with an understanding of business processes and organizational structures, a combination of recognizing an enterprise's pain points and the potential solutions that can be applied to correct them.

Middleware, on the other hand, is defined by the ObjectWeb consortium as the software layer in a distributed computing system that lies between the operating system and the applications on each site of the system. Middleware is the enabling technology of system and enterprise application integration (EAI) and therefore it clearly and evidently plays a key role for SOC. While the immediate need of middleware support for Service Oriented Architectures (SOA) is evident, current approaches and solutions mostly fall short by primarily providing support for the EAI aspect of SOC only and do not sufficiently address composition support, service management and monitoring. 

Moreover, quality properties (in particular dependability and security) need to be addressed not only by interfacing and communication standards, but also in terms of integrated middleware support. But what makes these issues so different in a SOA setting? Why - for instance - is traditional middleware support for transaction processing different to transaction processing in SOA, reflecting different types of atomicity needs? One answer lies in the administrative heterogeneity, the loose coupling between coarsegrained operations and long-running interactions, high dynamicity, and the required flexibility during run-time. Recently, massive-scale and mobility were added to the challenges for Middleware for SOC.

However, loose coupling is not always the best approach to solve a particular problem. In order to temporarily allow for a stronger (traditional) form of coupling (like group membership agreement or atomic transaction), the middleware also has to provide explicit and configurable means to change between different strengths of coupling and various communication paradigms. This will enable servicebased applications to take the best from both worlds by dynamically adjusting the interactions between the composed services. The highly dynamic modularity and need for flexible integration of services (e.g. Web service implementations) may require new middleware architectures, protocols, and services. These considerations also lead to the question to what extent service-orientation at the middleware layer itself is beneficial (or not). Recently emerging "Middleware as service" offerings, from providers like Amazon or from the open source community, support this trend towards "infrastructure services" that can be purchased and consumed over the Internet. However, this model may not be suitable for all kinds of middleware functions, including those addressing dependability (availability, reliability, integrity, safety, maintainability). Providing end-to-end properties and addressing cross-cutting concerns in a crossorganizational SOA is a particular challenge and the limits and benefits thereof have still to be investigated.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">225</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Karl Goeschka</field>
<field name="author">Aad Moorsel</field>
<field name="author">Raymond Wong</field>
<field name="title">Proceedings of Middleware for Web Services (MWS) 2007</field>
<field name="keyword">middleware</field>
<field name="keyword"> Web services</field>
<field name="keyword"> service oriented computing</field>
<field name="abstract">This is an introduction to the proceedings of the MWS 2007 workshop held at EDOC 2007. It first explains

the motivation for and background of the workshop. Then, it contains a short description of the keynote,

each long and short peer-reviewed paper, and the discussion session Impact of Various Execution Environments on Middleware for Web Services . After the closing statements, MWS 2007 Program Committee members are listed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">226</field>
<field name="author">Basem Suleiman</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Eldar Aliev</field>
<field name="title">Non-Functional Property Specifications for WRIGHT ADL</field>
<field name="abstract">Many architecture description languages (ADLs)were proposed for describing structure of software systems in terms of components and connectors. However,specification of non-functional properties is not addressed well in these ADLs, although it is crucial for building quality software architectures. We propose new extensions of WRIGHT ADL with capabilities to specify non-functional properties at the architectural level. They are based on ontological definitions of nonfunctional properties. To demonstrate their usefulness, we applied them to an e-health example.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">227</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Ulrich Furbach</field>
<field name="author">Bjoern Pelzer</field>
<field name="title">Hyper Tableaux with Equality</field>
<field name="abstract">In most theorem proving applications, a proper treatment of equational theories or equality is mandatory. In this paper we show how to integrate a modern treatment of equality in the hyper tableau calculus. It is based on splitting of positive clauses and an adapted version of the superposition inference rule, where equations used for paramodulation are drawn (only) from a set of positive unit clauses, the candidate model. The calculus also features a generic, semantically justified simplification rule which covers many redundancy elimination techniques known from superposition-style theorem proving. Our main theoretical result is the soundness and completeness of the calculus. The calculus is implemented, and we also report on practical experiments.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">228</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Alexander Fuchs</field>
<field name="author">Hans de Nivelle</field>
<field name="author">Cesare Tinelli</field>
<field name="title">Computing Finite Models by Reduction to Function-Free Clause Logic</field>
<field name="keyword">Automated Theorem Proving</field>
<field name="keyword"> Model Generation</field>
<field name="abstract">Recent years have seen considerable interest in procedures for computing finite models of first-order logic specifications. One of the major paradigms, MACE-style model building, is based on reducing model search to a sequence of propositional satisfiability problems and applying (efficient) SAT solvers to them. A problem with this method is that it does not scale well because the propositional formulas to be considered may become very large.



We propose instead to reduce model search to a sequence of satisfiability problems consisting of function-free first-order clause sets, and to apply (efficient) theorem provers capable of deciding such problems. The main appeal of this method is that first-order clause sets grow more slowly than their propositional counterparts, thus allowing for more space efficient reasoning.



In this paper we describe our proposed reduction in detail and discuss how it is integrated into the Darwin prover, our implementation of the Model Evolution calculus. The results are general, however, as our approach can be used in principle with any system that decides the satisfiability of function-free first-order clause sets.



To demonstrate its practical feasibility, we tested our approach on all satisfiable problems from the TPTP library. Our methods can solve a significant subset of these problems, which overlaps but is not included in the subset of problems solvable by state-of-the-art finite model builders such as Paradox and Mace4.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">229</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Cesare Tinelli</field>
<field name="title">The Model Evolution Calculus as a First-Order DPLL Method</field>
<field name="keyword">Automated Theorem Proving</field>
<field name="keyword"> Instance-Based Methods</field>
<field name="abstract">The DPLL procedure is the basis of some of the most successful propositional satisfiability solvers to date. Although originally devised as a proof-procedure for first-order logic, it has been used almost exclusively for propositional logic so far because of its highly inefficient treatment of quantifiers, based on instantiation into ground formulas. The FDPLL calculus by Baumgartner was the first successful attempt to lift the procedure to the first-order level without resorting to ground instantiations. FDPLL lifts to the first-order case the core of the DPLL procedure, the splitting rule, but ignores other aspects of the procedure that, although not necessary for completeness, are crucial for its effectiveness in practice.



 In this paper, we present a new calculus loosely based on FDPLL that lifts these aspects as well. In addition to being a more faithful litfing of the DPLL procedure, the new calculus contains a more systematic treatment of universal literals, which are crucial to achieve efficiency in practice. The new calculus has been implemented successfully in the Darwin system, described elsewhere. The main results of this paper are theoretical, showing the soundness and completeness of the new calculus. In addition, the paper provides a high-level description of a proof procedure for the calculus, as well as a comparison with other calculi.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">230</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Ulrich Furbach</field>
<field name="author">Bjoern Pelzer</field>
<field name="title">The Hyper Tableaux Calculus with Equality and an Application to Finite Model Computation</field>
<field name="keyword">Automated Theorem Proving</field>
<field name="keyword"> Equality</field>
<field name="abstract">In most theorem proving applications, a proper treatment of equational theories or equality is mandatory. In this paper we show how to integrate a modern treat- ment of equality in the hyper tableau calculus. It is based on splitting of positive clauses and an adapted version of the superposition inference rule, where equations used for superposition are drawn (only) from a set of positive unit clauses, and superposition inferences into positive literals is restricted into (positive) unit clauses only. The calculus also features a generic, semantically justified simplification rule which covers many redundancy elimination techniques known from superposition theorem proving. Our main results are soundness and completeness of the calculus, but we also show how to apply the calculus for finite model computation, and we briefly describe the implementation.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">231</field>
<field name="author">Yu Shi</field>
<field name="author">Timothy Tsui</field>
<field name="title">An FPGA-based Smart Camera for Gesture Analysis for Healthcare Applications</field>
<field name="abstract">Affective computing aims at analyzing users affective states. Its applications include edutainment, HCI, and healthcare. Affective states may be conveyed through voice, facial expression, gesturing and physiological representations. This paper presents our attempt to develop a smart camera which can recognize simple head and hand gestures, for healthcare applications.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">232</field>
<field name="author">Yu Shi</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Ronnie Taib</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Fang Chen</field>
<field name="title">Multimodal Human-Machine Interface and User Cognitive Load Measurement</field>
<field name="keyword">Human-machine interface</field>
<field name="keyword"> Cognitive science</field>
<field name="keyword"> Human-centred design</field>
<field name="keyword"> Human factors</field>
<field name="keyword"> Transport</field>
<field name="keyword"> Traffic control.</field>
<field name="abstract">Multimodal user interface (MMUI) is an emerging technology that aims at providing a more intuitive and natural way for people to operate and control a computer or a machine. MMUI allows users to control a computer using various input modalities, including speech, touch, gestures and hand-writing. It has potential to minimise user s cognitive load when performing complex tasks. In this paper we present our work in building an MMUI research platform for intelligent transport system applications, and our attempt to evaluate a user s cognitive load based on analysis of his or her multimodal behaviours and physiological measurement.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">233</field>
<field name="author">Yu Shi</field>
<field name="author">Timothy Tsui</field>
<field name="title">An FPGA-based Smart Camera for Gesture Recognition in HCI Applications</field>
<field name="keyword">Intelligent Systems; Human</field>
<field name="keyword">Computer Interaction; Embedded Systems; Computer Vision; Pattern Recognition.</field>
<field name="abstract">Smart camera is a camera that can not only see but also think and act. A smart camera is an embedded vision system which captures and processes image to extract application-specific information in real time. The brain of a smart camera is a special processing module that performs application specific information processing. The design of a smart camera as an embedded system is challenging because video processing has insatiable demand for performance and power, but at the same time embedded systems place considerable constraints on the design. We present our work to develop GestureCam, an FPGA-based smart camera built from scratch that can recognize simple hand gestures. The first completed version of GestureCam has shown promising real-time performance and is being tested in several desktop HCI (Human Computer Interface) applications.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">234</field>
<field name="author">Renato Iannella</field>
<field name="title">Towards the Policy-Aware Web: The Real Web 3.0?</field>
<field name="keyword">Policy-Aware Web</field>
<field name="keyword"> Privacy</field>
<field name="keyword"> Rights</field>
<field name="keyword"> Identity.</field>
<field name="abstract">The sharing and control of information on the Web needs to be 

balanced to provide the web community with the best experience 

and outcomes. The Policy-Aware Web is an emerging direction 

that may hold the key in getting this balance right by allowing 

future policy languages to maintain this harmonization. This panel 

will discuss, highlight, and debate the challenges in moving 

towards the Policy-Aware Web and the paths it could lead. The 

target audience includes all web stakeholders as most policies, 

such as privacy and rights, are intrinsic to all web users, 

information, services and content.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">235</field>
<field name="author">Jim Steel</field>
<field name="author">Renato Iannella</field>
<field name="author">Brian Lam</field>
<field name="title">Using Ontologies for Decision Support in Resource Messaging</field>
<field name="abstract">Emergency management is by its nature, and in some jurisdictions by its definition, an activity that requires a concerted effort by a number of governmental and non-governmental agencies. There is a growing appreciation that collaboration between these parties is best served through the use of interoperable standards for message formats for purposes such as alerting and resource exchange. However, it is also important to realize that, although much advantage can be drawn from standardizing certain aspects of communication, such as the structure of messages, different agencies will use different vocabularies. In this paper we discuss how ontologies can be used with standard messaging formats for resource messaging to enable intelligent decision support mechanisms in the presence of differing vocabularies across organizational boundaries. We also present a survey of the opportunities for using ontologies in emergency management, and the issues that must be addressed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">236</field>
<field name="author">Tom Ridge</field>
<field name="author">Michael Norrish</field>
<field name="author">Peter Sewell</field>
<field name="title">A rigorous approach to networking: TCP, from implementation to protocol to service</field>
<field name="keyword">TCP theorem</field>
<field name="keyword">proving validation</field>
<field name="abstract">Despite more then 30 years of research on protocol specification, the major protocols deployed in the Internet, such as TCP, are described only in informal prose RFCs and executable code. In part this is because the scale and complexity of these protocols makes them challenging targets for formalization. 



In this paper we show how these difficulties can be addressed. We develop a high-level specification for TCP and the Sockets API, expressed in the HOL proof assistant, describing the byte-stream service that TCP provides to users. This complements our previous low-level specification of the protocol internals, and makes it possible for the first time to state what it means for TCP to be correct: that the protocol implements the service. We define a precise abstraction function between the models and validate it by testing, using verified testing infrastructure within HOL. This is a pragmatic alternative to full proof, providing reasonable confidence at a relatively low entry cost. 



Together with our previous validation of the low-level model, this shows how one can rigorously tie together concrete implementations, low-level protocol models, and specifications of the services they claim to provide, dealing with the complexity of real-world protocols throughout.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">237</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Li Cheng</field>
<field name="author">Quoc Le</field>
<field name="author">Alex Smola</field>
<field name="title">Learning Graph Matching</field>
<field name="keyword">Graph Matching</field>
<field name="keyword"> Structured Estimation</field>
<field name="keyword"> Machine Learning</field>
<field name="abstract">As a fundamental problem in pattern recognition, graph matching has found a variety of applications in the field of computer vision. In graph matching, patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs. There are many ways in which the problem has been formulated, but most can be cast in general as a quadratic assignment problem, where a linear term in the objective function encodes node compatibility functions and a quadratic term encodes edge compatibility functions. 

The main research focus in this theme is about designing efficient algorithms for solving approximately the quadratic assignment problem, since it is NP-hard.



In this paper, we turn our attention to the complementary problem: how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide. We present a method for learning graph matching: the training examples are pairs of graphs and the ``labels'' are matchings between pairs of graphs. We present experimental results with real image data which give evidence that learning can improve the performance of standard graph matching algorithms. 

In particular, it turns out that linear assignment with such a learning scheme may improve over state-of-the-art quadratic assignment relaxations. 

This finding suggests that for a range of problems where quadratic assignment was thought to be essential for securing good results, linear assignment, which is far more efficient, could be just sufficient if learning is performed. This enables speed-ups of graph matching by up to 4 orders of magnitude while retaining state-of-the-art accuracy.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">238</field>
<field name="author">Desmond Chik</field>
<field name="author">Jochen Trumpf</field>
<field name="author">Nicol N. Schraudolph</field>
<field name="title">3D Hand Tracking in a Stochastic Approximation Setting</field>
<field name="keyword">stochastic approximation</field>
<field name="keyword"> 3D hand tracking</field>
<field name="keyword"> Pose estimation</field>
<field name="abstract">This paper introduces a hand tracking system with a theoretical proof



 of convergence. The tracking system follows a model-based approach and



 uses image-based cues, namely silhouettes and colour constancy. We show



 that, with the exception of a small set of parameter configurations,



 the cost function of our tracker has a well-behaved unique minimum.



 The convergence proof for the tracker relies on the convergence theory



 in stochastic approximation. We demonstrate that our tracker meets the



 sufficient conditions for stochastic approximation to hold locally.



 Experimental results on synthetic images generated from real hand 



 motions show the feasibility of this approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">239</field>
<field name="author">Julian McAuley</field>
<field name="author">Luciano Costa</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Rich-club phenomenon across complex network hierarchies</field>
<field name="keyword">Rich-club phenomenon</field>
<field name="keyword"> complex networks</field>
<field name="abstract">The rich-club phenomenon in complex networks is characterized when nodes of higher degree are 

more interconnected than nodes with lower degree. The presence of this phenomenon may indicate 

several interesting high-level network properties, such as tolerance to hub failures. Here, the authors 

investigate the existence of this phenomenon across the hierarchies of several real-world networks. 

Their simulations reveal that the presence or absence of this phenomenon in a network does not 

imply its presence or absence in the network s successive hierarchies, and that this behavior is even 

nonmonotonic in some cases.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">240</field>
<field name="author">Golam Sarwar</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Roksana Boreli</field>
<field name="title">Experimental performance of DCCP over live satellite and long range wireless links</field>
<field name="abstract">We present experimental results for the performance over satellite and long range wireless (WiMax) links of the new TCP-Friendly Rate Control (TFRC) congestion control mechanism from the Datagram Congestion Control Protocol (DCCP) proposed for use with real-time traffic. We evaluate the performance of the standard DCCP/CCID3 algorithm and identify two problem areas: the measured DCCP/CCID3 rate is inferior to the rate achievable with standard TCP and a significant rate oscillation continuously occurs making the resulting rate variable even in the short term. We analyse the links and identify the potential causes, i.e. long and variable delay and link errors. As a second contribution, we propose a change in the DCCP/CCID3 algorithm in which the number of feedback messages is increased from the currently standard of at least one per return trip time. Although it is recognised that the increase in control traffic may decrease the overall efficiency, we demonstrate that the change results in higher data rates which are closer to what is achievable with TCP on those networks and that the overhead introduced remains acceptable.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">241</field>
<field name="author">Jochen Eisl</field>
<field name="author">Michael Georgiades</field>
<field name="author">Tony Jokikyyny</field>
<field name="author">Roksana Boreli</field>
<field name="author">Eranga Perera</field>
<field name="author">Kostas Pentikousis</field>
<field name="title">Management of multiple mobility protocols and tools in dynamically configurable networks</field>
<field name="keyword"/>
<field name="abstract">Solutions for mobility management in wireless networks have been investigated and proposed in various research projects and standardization bodies. With the continuing deployment of different access networks, the wider range of applications tailored for a mobile environment, and a larger diversity of wireless end systems, it emerged that a single mobility protocol (such as Mobile IP) is not sufficient to handle the different requirements adequately. Thus a solution is needed to manage multiple mobility protocols in end systems and network nodes, to detect and select the required protocols, versions and optional features, and enable control on running daemons. For this purpose a mobility toolbox has been developed as part of the EU funded Ambient Networks project. This paper describes this modular management approach and illustrates the additional benefits a mobility protocol can gain by using state transfer as an example.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">242</field>
<field name="author">Eranga Perera</field>
<field name="author">Roksana Boreli</field>
<field name="title">A mobility toolbox architecture for All-IP networks: An Ambient Networks approach</field>
<field name="keyword">mobility management multi</field>
<field name="keyword">protocol</field>
<field name="abstract">The future Internet will need to cater for an increasing number of mobile devices and mobile networks, roaming across different access networks and trust domains. In addition, various limitations imposed by the end user, service provider, or network operator agreements and preferences will need to be considered. A plethora of mobility management protocols have been proposed to handle different and mostly limited sets of these mobility requirements. In this article we make the case for coexistence of mobility protocols in order to support the large range of mobility scenarios possible in future all-IP networks. This coexistence takes the form of a mobility toolbox that enables mobility handling mechanisms to be selected according to the context. We then present a design for the mobility toolbox as a component of the Ambient Networks architecture including a simplified mobility tool interface towards protocol modules, and show how it meets the requirements of future all-IP networks. We further demonstrate the feasibility and the performance gains of the mobility toolbox architecture with a prototype implementation based on network mobility.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">243</field>
<field name="author">Jenny Liu</field>
<field name="author">Ian Gorton</field>
<field name="author">Vinh Kah Lee</field>
<field name="title">The Architecture of Event Correlation Service in Adaptive Middleware-based Applications</field>
<field name="keyword">Event correlation</field>
<field name="keyword"> middleware</field>
<field name="keyword"> common based event</field>
<field name="keyword"> Web services</field>
<field name="abstract">Loosely coupled component communication driven by events is a key mechanism for building middleware-based applications that must achieve reliable qualities of service in an adaptive manner. In such a system, events that encapsulate state snapshots of a running system are generated by monitoring components. Hence, an event correlation service is necessary for correlating monitored events from multiple sources. The requirements for the event correlation raise two challenges: to seamlessly integrate event correlation services with other services and applications; and to provide reliable event management with minimal delay. This paper describes our experience in the design and implementation of an event correlation service. The design encompasses an event correlator and an event proxy that are integrated with an architecture for adaptive middleware components. The implementation utilizes the Common Based Event (CBE) specification and stateful Web service technologies to support the deployment of the event correlation service in a distributed architecture. We evaluate the performance of the overall solution in a test bed and present the results in terms of the trade-off between the flexibility and the performance overhead of the architecture.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">244</field>
<field name="author">Jenny Liu</field>
<field name="author">Ian Gorton</field>
<field name="author">Nihar Trivedi</field>
<field name="title">A Lightweight and Extensible Architecture for Adaptive Server Applications, Software: Practice and Experience</field>
<field name="keyword">adaptive</field>
<field name="keyword"> server applications</field>
<field name="keyword"> autonomic computing</field>
<field name="keyword"> components</field>
<field name="keyword"> performance</field>
<field name="abstract">Server applications augmented with behavioral adaptation logic can react to environmental changes, creating self-managing server applications with improved quality of service at runtime. However, developing adaptive server applications is challenging due to the complexity of the underlying server technologies and highly dynamic application environments. This paper presents an architecture framework, the Adaptive Server Framework (ASF), to facilitate the development of adaptive behavior for legacy server applications. ASF provides a clear separation between the implementation of adaptive behavior and the business logic of the server application. This means a server application can be extended with programmable adaptive features through the definition and implementation of control components defined in ASF. Furthermore, ASF is a lightweight architecture in that it incurs low CPU overhead and memory usage. We demonstrate the effectiveness of ASF through a case study, in which a server application dynamically determines the resolution and quality to scale an image based on the load of the server and network connection speed. The experimental evaluation demonstrates the performance gains possible by adaptive behavior and the low overhead introduced by ASF.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">245</field>
<field name="author">Jenny Liu</field>
<field name="author">Liming Zhu</field>
<field name="author">Ian Gorton</field>
<field name="title">Performance Assessment for e-Government Services: An Experience Report</field>
<field name="keyword">performance</field>
<field name="keyword"> e-Government</field>
<field name="keyword"> services</field>
<field name="keyword"> and J2EE</field>
<field name="abstract">The transformation and integration of government services, enabled

by the use of new technologies such as application servers and Web services, is

fundamental to reduce the cost of government and improving service outcomes

to citizens. Many core Government information systems comprise applications

running on legacy mainframes, databases and transaction processing monitors.

As Governments worldwide provide direct access over the Internet to these

legacy applications from the general public, they may be exposed to workloads

well above the origin design parameters of these back-end systems. This creates

a significant risk of high profile failures for Government agencies whose newly

integrated systems become overloaded. In this paper we describe how we

conducted a performance assessment of a business-critical, Internet-facing Web

services that integrated new and legacy systems from two Australian

Government agencies. We leveraged prototype tools from our own research

along with known techniques in performance modeling. We were able to clearly

demonstrate that the existing hardware and software would be adequate to

handle the predicted workload for the next financial year. We were also able to

do what-if analysis and predict how the system can perform with alternative

strategies to scale the system. We conclude by summarizing the lessons learnt,

including the importance of architecture visibility, benchmarking data quality,

and measurement feasibility due to issues of outsourcing, privacy legislation

and cross-agency involvement.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">246</field>
<field name="author">Jenny Liu</field>
<field name="author">Ian Gorton</field>
<field name="author">Liming Zhu</field>
<field name="title">Performance Prediction of Service-Oriented Applications based on an Enterprise Service Bus</field>
<field name="keyword">performance analysis</field>
<field name="keyword"> Web Services</field>
<field name="keyword"> Enterprise Service Bus</field>
<field name="abstract">An Enterprise Service Bus (ESB) is a standards-based

integration platform that combines messaging, web

services, data transformation, and intelligent routing in a

highly distributed environment. The ESB has been

adopted as a key component of SOA infrastructures. For

SOA implementations with large number of users,

services, or traffic, maintaining the necessary

performance levels of applications integrated using an

ESB presents a substantial challenge, both to the

architects who design the infrastructure as well as to IT

professionals who are responsible for administration. In

this paper, we develop a performance model for analyzing

and predicting the runtime performance of service

applications composed on a COTS ESB platform. Our

approach utilizes benchmarking techniques to measure

primitive performance overheads of service routing

activities in the ESB. The performance characteristics of

the ESB and services running on the ESB are modeled in

a queuing network, which facilitates the performance

prediction of service oriented applications. This model is

validated by an example ESB based service application

modeled from real world loan broking business

application.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">247</field>
<field name="author">Jenny Liu</field>
<field name="author">Ted Wong</field>
<field name="title">Component Architecture and Modeling for Microkernel-based Embedded System Development</field>
<field name="keyword">model driven development</field>
<field name="keyword"> embedded systems</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> code generation</field>
<field name="abstract">Microkernel-based approach provides operating

system support for developing embedded systems with

high performance and safety through memory protection.

This allows us to introduce architectural mechanisms that

enable good separation of concerns, while still satisfy the

performance, security and reliability needs of embedded

systems. Organizing embedded software as interacting

components with well-defined interfaces is compatible

with modeling methods. The challenge issue is to

seamlessly transform models and integrate tools at

different levels of design, implementation and deployment.

In this paper we present our solution to this issue. Our

contribution is twofold: first, we device a unified model

driven and component based development approach. We

adopt a layered architecture to construct a tool chain,

which allows flexible extension at different layers.

Second, we develop a software tool suite in order to

support and demonstrate our solution, which includes a

UML-based modeling environment and a set of

component development tools on top of a microkernel

operating system.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">248</field>
<field name="author">Alfredo Gabaldon</field>
<field name="author">Gerhard Lakemeyer</field>
<field name="title">ESP: A Logic of Only-Knowing, Noisy Sensing and Acting</field>
<field name="abstract">When reasoning about actions and sensors in realistic domains,

the ability to cope with uncertainty often plays an essential

role. Among the approaches dealing with uncertainty,

the one by Bacchus, Halpern and Levesque, which uses the

situation calculus, is perhaps the most expressive. However,

there are still some open issues. For example, it remains unclear

what an agent s knowledge base would actually look

like. The formalism also requires second-order logic to represent

uncertain beliefs, yet a first-order representation clearly

seems preferable. In this paper we show how these issues can

be addressed by incorporating noisy sensors and actions into

an existing logic of only-knowing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">249</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Jenny Liu</field>
<field name="author">Liming Zhu</field>
<field name="title">Towards Business Driven Autonomic Service-Oriented Computing</field>
<field name="abstract">WS-Policy4MASC is our XML language for specification of policies for run-time Web service management. Among its original contributions are specification of diverse business values (benefits/costs, tangible/intangible, 

agreed/possible, absolute/relative) and specification of various control strategies maximizing different business values (e.g., only agreed intangible benefits). It was originally developed for the MASC (Manageable and Adaptable Service Compositions) middleware. However,we envision that it can be used in a broader context of

autonomic computing, because its rich policy specification is very valuable for developing policy-driven autonomic systems, especially for service-oriented computing. We discuss our research on combining WS-Policy4MASC with the Adaptive Server Framework (ASF), which supports composing adaptive applications on Java and .Net platforms. WS-Policy4MASC enriches ASF with policy definition semantics and enables complex policy management for autonomic computing. The combination of WS-Policy4MASC and ASF forms a step towards business-

driven IT management with autonomic features.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">250</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Business Value Driven Engineering and Management of Web Services and Their Compositions</field>
<field name="keyword">Web services</field>
<field name="keyword"> Web service management</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> WS-Policy</field>
<field name="keyword"> middleware</field>
<field name="keyword"> UML profile</field>
<field name="abstract">An Extensible Markup Language (XML) Web service is a distributed software component that has a unique Uniform Resource Identifier (URI) and can be accessed over the Internet by means of XML-based description, discovery, and messaging technologies. Management (monitoring and control) of Web services and their compositions is needed to ensure regular operation, attain or surpass the guaranteed quality of service (QoS), accommodate change, and keep track of the consumed resources. 

 WS-Policy4MASC is a new XML notation for specification of monitoring and control (particularly, adaptation) policies in the Manageable and Adaptable Services Compositions (MASC) middleware. It extends the Web Services Policy Framework (WS-Policy) by defining new types of policy assertions (goal, action, utility, and meta-policy assertions). Among its original contributions are specification of diverse business values (benefits or costs, tangible or intangible, agreed or possible, absolute or relative) and specification of various control strategies maximizing different business values (e.g., only agreed intangible benefits). We evaluated feasibility of the WS-Policy4MASC solutions by implementing a policy repository and other modules in the MASC middleware and examined their usefulness on a set of realistic scenarios. 

 Further, to facilitate development of Web service systems that can be managed with WS-Policy4MASC and the MASC middleware and to improve alignment between run-time management activities and design-time models, we developed novel Unified Modeling Language (UML) profiles for WS-Policy4MASC. Their original contributions are in improved support for: a) specification of run-time management activities and business values within design-time models, b) automatic creation of run-time management policies from design-time models, and to some extent c) feedback of run-time management information values into analysis of design-time models. We validated our initial solutions on detailed examples, but continue research in this important direction.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">251</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Impact of Various Execution Environments on Middleware for Web Services</field>
<field name="abstract">The workshop will also contain the discussion session "Impact of Various Execution Environments on Middleware for Web Services". The use of Web service requesters and providers executing in diverse executing environments is increasing rapidly. On one side of the spectrum are mobile and/or embedded environments, somewhere in the middle are the traditional business server environments, while on the other end of the spectrum are Grid computing systems. Embedded environments and many mobile environments are characterized with limited resources, such as processing power, memory, communication bandwidth, electrical energy. Mobility brings a number of issues and opportunities, such as location updates and contextawareness. Among the issues with increased importance for Web services in executing Grid systems are virtualization and instance lifecycle management. Due to these and other issues, middleware for Web services executing in one type of execution environment might require significant modifications to accommodate other execution environments. While many MWS 2007 workshop participants already gained significant experience about these issues, their impact, and possible solutions (as well as non-solutions), there are still many unknowns. Thus, the goal of this discussion session is to facilitate exchange of knowledge, experiences, ideas,and opinions in this important area.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">252</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Web Services and LIXI: A Primer</field>
<field name="keyword">Web services</field>
<field name="keyword"> Web service development</field>
<field name="keyword"> LIXI Valuations</field>
<field name="abstract">Dr Vladimir Tosic (NICTA) will provide a primer workshop on web services standards for technologists and managers with little or no prior experience in this field. Participants will also learn about implementing web services, using LIXI Valuations as examples.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">253</field>
<field name="author">Bernhard Hengst</field>
<field name="title">Safe State Abstraction and Reusable Continuing Subtasks in Hierarchical Reinforcement Learning</field>
<field name="keyword"/>
<field name="abstract">Hierarchical reinforcement learning methods have not been

able to simultaneously abstract and reuse subtasks with discounted value

functions. The contribution of this paper is to introduce two completion

functions that jointly decompose the value function hierarchically

to solve this problem. The significance of this result is that the benefits

of hierarchical reinforcement learning can be extended to discounted

value functions and to continuing (infinite horizon) reinforcement learning

problems. This paper demonstrates the method with the an algorithm

that discovers subtasks automatically. An example is given where the optimum

policy requires a subtask never to terminate.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">254</field>
<field name="author">Raymond Sheh</field>
<field name="author">Waleed M Kadous</field>
<field name="author">Claude Sammut</field>
<field name="author">Bernhard Hengst</field>
<field name="title">Extracting Terrain Features from Range Images for Autonomous Random Stepfield Traversal</field>
<field name="abstract">One of the challenges of rescue robotics is to create

robots that can autonomously traverse rough, unstructured terrain.

Although mechanical engineering can produce very capable robots,

mechanical engineering alone will not drive them. In this paper, we

present a terrain feature extractor that can be taught to find significant

features in range images of terrain around a robot. A terrain model is

generated from the many points in the range sensor data. Techniques

from the field of knowledge acquisition are then used to find patterns

in the terrain model. A knowledge acquisition system can then be

taught to drive a robot in unstructured terrain based on these features.

We evaluate the performance of the initial stages of the feature

extractor on a real robot, traversing NIST specification red stepfields.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">255</field>
<field name="author">Christian Chong-White</field>
<field name="author">Bernhard Hengst</field>
<field name="title">NICTA &amp; RTA PARTNERSHIP FOR TRAFFIC CONTROL INNOVATION</field>
<field name="abstract">NICTA and the New South Wales Roads and Traffic Authority (RTA) have formed a partnership to increase the effectiveness of existing traffic and transport infrastructure using smart technology. NICTA is an Australian national research institute in the ICT sector with expertise in constraint programming, machine learning and optimisation. The aim is to research and develop new generation traffic control systems to tackle congestion that is estimated to currently cost Australia alone about $10 billion annually. This paper outlines the technical, methodological and collaborative nature of the relationship and shows how the parties complement each others experience and expertise. The research benefits from two live RTA contributed test-beds; a traffic signal controlled intersection in Sydney, and a highway roundabout in the Illawarra region. We also discuss the design of advanced real-time traffic control systems using video sensors, traffic models and optimisation techniques for flexible signal group control.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">256</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Subsuming the BPM life cycle in an ontological framework of designing</field>
<field name="keyword">BPM</field>
<field name="keyword"> BPM life cycle; FBS ontology</field>
<field name="abstract">This paper proposes a framework to represent life-cycle activities performed in business process management (BPM). It is based on the function-behaviour-structure (FBS) ontology that represents all design entities uniformly, independently of the specific stages in their life cycle. The framework specifies a set of distinct activities that operate on the function, behaviour and structure of a business process, subsuming the different life-cycle stages within a single framework. This provides an explicit description of a number of issues in BPM, including design-time analysis, flexibility of tasks and sub-processes, interaction between life-cycle stages, and the use of experience.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">257</field>
<field name="author">Rafal Kolanski</field>
<field name="author">Gerwin Klein</field>
<field name="title">Mapped Separation Logic</field>
<field name="keyword">Isabelle</field>
<field name="keyword"> HOL</field>
<field name="keyword"> Virtual Memory</field>
<field name="keyword"> Separation Logic</field>
<field name="abstract">We present Mapped Separation Logic, an instance of Separation Logic

for reasoning about virtual memory. Our logic is formalised in the

Isabelle/HOL theorem and it allows reasoning on properties about page 

tables, direct physical memory access, virtual memory access, and 

shared memory. Mapped Separation Logic fully supports all rules of 

abstract Separation Logic, including the frame rule.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">258</field>
<field name="author">Sherif Sakr</field>
<field name="title">XSelMark: A Micro-Benchmark for Selectivity Estimation Approaches of XML Queries</field>
<field name="keyword">XML </field>
<field name="keyword"> Query Optimization </field>
<field name="keyword"> Selectivity Estimation </field>
<field name="keyword"> Benchmark</field>
<field name="abstract">Estimating the sizes of query results and intermediate results is a crucial part of any effective query optimization process. Due to several reasons, the selectivity estimation problem in the XML domain is more complicated than that in the relational domain. Several research efforts have proposed selectivity estimation approaches in the XML domain. Lacking of a suitable benchmark was one of the main reasons which prevented a real assessment and comparison between the approaches to be conducted. This paper is a first step towards a comprehensive assessment of the available selectivity estimation approaches of XML queries along with their strengths and weaknesses. We propose a selectivity estimation benchmark for XML queries, XSelMark. It consists of a set of 25 queries organized into seven groups and covers the main aspects of selectivity estimation of XML queries. These queries have been designed with respect to an XML document instance of a popular benchmark for XML data management, XMark. In addition, we suggest some criteria of assessing the capability and quality of XML queries selectivity estimation approaches. Finally, we use the proposed benchmark to assess the capabilities of the-state-of-the-art of the selectivity estimation approaches.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">259</field>
<field name="author">Harvey Tuch</field>
<field name="title">Structured Types and Separation Logic</field>
<field name="keyword">Separation Logic</field>
<field name="keyword"> C</field>
<field name="keyword"> Interactive Theorem Proving</field>
<field name="abstract">Structured types, such as C's arrays and structs, present additional challenges in pointer program verification. The conventional proof abstractions, multiple independent typed heaps and separation logic, which in previous work have been built on a low-level memory model for C and shown to be sound, are not directly applicable in verifications. This is due to the non-monotonic nature of pointer and lvalue validity in the presence of the unary &amp;-operator. For example, type-safe updates through pointers to fields of a struct break the independence of updates across typed heaps or *-conjuncts. In this paper we present a generalisation of our earlier formal memory model that captured the low-level features of C's pointers and memory and formed the basis for an expressive implementation of separation logic, with new features providing explicit support for C's structured types. We implement this framework in the theorem prover Isabelle/HOL and all proofs are machine checked.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">260</field>
<field name="author">Joshua Ho</field>
<field name="author">Maurizio Stefani</field>
<field name="author">Cristobal dos Remedios</field>
<field name="author">Michael Charleston</field>
<field name="title">Differential variability analysis of gene expression and its application to human diseases</field>
<field name="keyword">microarray analysis</field>
<field name="keyword"> bioinformatics</field>
<field name="keyword"> differential variability</field>
<field name="keyword"> human diseases</field>
<field name="abstract">Motivation: Current microarray analyses focus on identifying sets

of genes that are differentially expressed (DE) or differentially

coexpressed (DC) in different biological states (e.g., diseased vs.

non-diseased). We observed that in many human diseases, some

genes have a significant increase or decrease in expression variability

(variance). As these observed changes in expression variability may

be caused by alteration of the underlying expression dynamics, such

differential variability (DV) patterns are also biologically interesting.

Results: Here we propose a novel analysis for changes in gene

expression variability between groups of samples, which we call

differential variability analysis. We introduce the concept of differential

variability (DV), and present a simple procedure for identifying

DV genes from microarray data. Our procedure is evaluated

with simulated and real microarray datasets. The effect of data

preprocessing methods on identification of DV gene is investigated.

The biological significance of DV analysis is demonstrated with four

human disease datasets. The relationships among DV, DE and

DC genes are investigated. The results suggest that changes in

expression variability are associated with changes in coexpression

pattern, which imply that DV is not merely stochastic noise, but

informative signal.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">261</field>
<field name="author">Jennifer Sampson</field>
<field name="title">Facilitating Interoperability in Semantic Web Applications Using Ontologies</field>
<field name="abstract">Semantic Web applications often rely on the reuse of underlying data from legacy information systems. The motivation for our research is how to make this data accessible for use in such applications. We propose the use of ontologies and conceptual modelling in an overall approach for data interoperability. The approach incorporates a process for ontology alignment using our tool for visual ontology alignment. First results from our tool evaluation were promising and show that visualization is useful for locating additional candidate alignments by an end user.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">262</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Aad van Moorsel</field>
<field name="author">Raymond Wong</field>
<field name="title">Special Issue on Middleware for Web Services (MWS)</field>
<field name="keyword">Web service</field>
<field name="keyword"> middleware</field>
<field name="keyword"> performance management</field>
<field name="keyword"> dynamic adaptation.</field>
<field name="abstract">Web services are distributed computing application components implementing the Service-Oriented Architecture (SOA). They are used for Business-to-Business (B2B)integration, Enterprise Application Integration (EAI), e-business process integration and management services. Reusable Web services technologies are implemented in middleware, so powerful middleware for web services is therefore a prerequisite for advanced e-business

process integration and management. This special issue contains revised best papers from the 2005 Middleware for Web Services workshop (MWS 2005) held at the EDOC 2005 conference, as well as selected papers from the open call for papers. This issue also contains two regular papers, outside the special issue topic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">263</field>
<field name="author">Monika Lanzenberger</field>
<field name="author">Jennifer Sampson</field>
<field name="author">Marcus Rester</field>
<field name="title">Visual ontology alignment for semantic interoperability</field>
<field name="abstract">Purpose Ontology alignment plays an important role in the context of semantic interoperability . Usually

ontology alignment tools generate results that are difficult to understand or assess. In order to enable

users to check and improve alignment results and to understand their consequences we use information

visualization techniques. Moreover, we discuss the relevant quality aspects in ontology alignment as well

as current activities and available tools.

Methodology/Approach Based on literature study we identify quality measures for ontology alignment

and define requirements for visual ontology alignment. As a proof of concepts we developed a prototype

called AlViz.

Findings Information visualization offers appropriate methods for the assessment of ontology alignment

results. Different levels of detail and overview help the user to navigate and understand the alignments.

Originality/value Along a comprehensive framework we identify alignment assessment tasks and we

introduce and apply a visualization tool which aims at making ontology alignment results manageable

and comprehensible.

Keywords</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">264</field>
<field name="author">Monika Lanzenberger</field>
<field name="author">Jennifer Sampson</field>
<field name="title">Human-mediated Visual Ontology Alignment</field>
<field name="abstract">We develop a multiple-view tool called AlViz, which aims at supporting the ontology alignment process visually. Combing views on several levels of abstraction, the tool tries to make the relatedness between entities accessible. Based on a literature study we identified relevant phases emerging in ontology alignment. We extended a general alignment framework in order to reflect the adoption of visualization techniques. This framework builds the background for our user study. We evaluate visual ontology alignment with AlViz in three stages: (1) Participative software development, (2) usability evaluation, and (3) utility study. The evaluation methods proved viable even though our study design is challenging.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">265</field>
<field name="author">Mark Staples</field>
<field name="author">Mahmood Niazi</field>
<field name="title">Experiences Using Systematic Review Guidelines</field>
<field name="abstract">Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that complementary research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">266</field>
<field name="author">Peter Carr</field>
<field name="author">Richard Hartley</field>
<field name="title">Portable Multi-Megapixel Panoramic Camera with Real-Time Recording and Playback</field>
<field name="keyword">mosaic video distributed resolution real</field>
<field name="keyword">time</field>
<field name="abstract">We are interested in the problem of automatically tracking football players, subject to the constraint that only one vantage point is available. Tracking algorithms benefit from seeing the entire playing field, as one does not have to worry about objects entering and leaving the field of view. However, the image of the entire field must be of sufficient resolution to allow each of the players to be identified automatically.



To achieve this desired video data, several high definition video cameras are used to record a football match from a single vantage point. The cameras are oriented to cover the entire playing field, and their images combined to create a single high-resolution video feed. The user is able to pan and zoom in real-time within the unified video stream while it is playing. The system is achieved by distributing tasks across a network of computers and only processing data that will be visible to the user.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">267</field>
<field name="author">Getian Ye</field>
<field name="author">Jian Zhang</field>
<field name="title">High-resolution image reconstruction under illumination change</field>
<field name="keyword">high-resolution</field>
<field name="keyword"> reconstruction</field>
<field name="keyword"> illumination change</field>
<field name="keyword"> image registration</field>
<field name="abstract">In this paper, we propose an approach to high-resolution image reconstruction under illumination change. It is based on the maximum a posteriori framework for performing joint image registration and high-resolution reconstruction. In this approach, an efficient multi-image registration is proposed to estimate the global motion and illumination change between the high-resolution image and low-resolution images. Considering different degradation from frame to frame and registration error, we then present a multichannel regularized HR reconstruction technique. Experimental results demonstrate the efficacy of the proposed approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">268</field>
<field name="author">Getian Ye</field>
<field name="title">High-resolution multi-sprite generation for background sprite coding</field>
<field name="keyword">high-resolution</field>
<field name="keyword"> sprite generation</field>
<field name="keyword"> mosaicing</field>
<field name="keyword"> multi-sprite coding</field>
<field name="abstract">In this paper, we consider high-resolution multi-sprite generation and its application to background sprite coding. Firstly, we propose an approach to partitioning a video sequence into multiple background sprites and selecting an optimal reference frame for each sprite range. This approach groups images that cover a similar scene into the same sprite range. We then propose an iterative regularized technique for constructing a high-resolution sprite in each sprite range. This technique determines the regularization parameter automatically and produces sprite images with high visual quality. Due to the advantages of high-resolution multi-sprites, a high-resolution sprite coding method is also presented. It achieves high coding efficiency especially at a low bit-rate.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">269</field>
<field name="author">Jie Xu</field>
<field name="author">Getian Ye</field>
<field name="author">Jian Zhang</field>
<field name="title">Long-term Trajectory Extraction for Moving Vehicles</field>
<field name="keyword">trajectory extraction</field>
<field name="keyword"> traffic monitoring</field>
<field name="keyword"> SIFT descriptor</field>
<field name="abstract">In recent years, trajectory analysis of moving vehicles in video-based traffic monitoring systems has drawn the attention of many researchers. Trajectory extraction is a fundamental step that is required prior to trajectory analysis. Lots of previous work have focused on trajectory extraction via tracking. However, they often fail to achieve long-term consistent trajectories. In this paper, we propose a robust approach for extracting long-term trajectories of moving vehicles in traffic monitoring using SIFT-descriptor. Experimental results show that the proposed method outperforms tracking-based techniques.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">270</field>
<field name="author">Chern Har Yew</field>
<field name="author">Hanan Lutfiyya</field>
<field name="author">Vladimir Tosic</field>
<field name="title">On Integrating Trust into Business-Driven Management of Web Services and Their Compositions</field>
<field name="keyword">Web service</field>
<field name="keyword"> business-driven IT management</field>
<field name="keyword"> trust</field>
<field name="abstract">This paper discusses why trust is an essential aspect for business-driven IT management, as well as how it impacts management of web services and their compositions. We summarize two ongoing projects which will enable the use of trust for business-driven management of web services.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">271</field>
<field name="author">Yazhe Tang</field>
<field name="author">Hanan Lutfiyya</field>
<field name="author">Vladimir Tosic</field>
<field name="title">An Analysis of Web Service SLA Management Infrastructures based on the C-MAPE Model</field>
<field name="keyword">Contract</field>
<field name="keyword"> SLA</field>
<field name="keyword"> Web service management</field>
<field name="keyword"> QoS</field>
<field name="keyword"> Analysis</field>
<field name="abstract">Quality of service (QoS) management is becoming an integral part of current web services. Contracts are playing a very important role in MAPE-based (monitor, analyse, plan and execute) web service QoS anagement. Therefore, we propose a general model, contract-MAPE (C-MAPE) model, and based on this model, review five major projects in this area. The analysis results show that these major research projects are quite much following C-MAPE model, whereas they give different support on different aspects. We also analyse possible ways to achieve integrated QoS management, for web services and their compositions. This paper contributes to the understanding of automatic processing of formal contracts, as well as of the relationship between contracts, QoS management systems, and web service provisioning systems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">272</field>
<field name="author">Fawad Nazir</field>
<field name="author">Hideaki Takeda</field>
<field name="title">Extraction and Analysis of Tripartite Relationships from Wikipedia</field>
<field name="keyword">Social Network Analaysis</field>
<field name="keyword"> Social Networking</field>
<field name="abstract">Social aspects are critical in the decision making process

for social actors (human beings). Social aspects can be

categorized into social interaction, social communities,

social groups or any kind of behavior that emerges from

interlinking, overlapping or similarities between interests

of a society. These social aspects are dynamic and

emergent. Therefore, interlinking them in a social

structure, based on bipartite affiliation network, may

result in isolated graphs. The major reason is that as

these correspondences are dynamic and emergent,

therefore they should be coupled with more than single

affiliation in order to sustain the interconnections during

interest evolutions. In this paper we propose to interlink

actors using multiple tripartite graphs rather than a

bipartite graph which was the focus of most of the

previous social network building techniques. The utmost

benefit of using tripartite graph is that we can have

multiple and hierarchical links between social actors.

Therefore in this paper we discuss the extraction, plotting

and analysis methods of tripartite relations between

authors, articles and categories from Wikipedia.

Furthermore, we also discuss the advantages of tripartite

relationships over bipartite relationships. As a conclusion

of this study we argue based on our results that, to build

useful, robust and dynamic social networks, actors should

be interlinked in one or more tripartite networks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">273</field>
<field name="author">Fawad Nazir</field>
<field name="author">Hideaki Takeda</field>
<field name="title">Comparison of Community Identification Techniques for Two-Mode Affiliation Networks Using Wikipedia Data</field>
<field name="keyword">Social Networks</field>
<field name="keyword"> Community Identification</field>
<field name="keyword"> Cohesive subgroups</field>
<field name="keyword"> Edge Betweenness</field>
<field name="keyword"> Hierarchical Clustering</field>
<field name="keyword"> Dendrograms.</field>
<field name="abstract">One of the most important questions in social networks is the identification of cohesive subgroups (a.k.a. community identification). These cohesive subgroups are loosely defined as collection of individuals who interact frequently. Once the communities are identified they often reveal interesting properties of the social network members, such as common hobbies, interests, social bindings, occupations etc. Several types of algorithms exist for analysis and identification of cohesive subgroups in one-mode networks that focus on pair-wise ties. However, less attention has been given to identification of cohesive subgroups in two-mode affiliation networks. Two mode affiliation networks focus on ties existing among actors through joint affiliations. Therefore, in this paper we evaluate two cohesive subgroups identification methods i.e. edge betweenness and hierarchical clustering, for two-mode affiliation network using the Wikipedia data. We conclude from our results that edge betweenness technique, when applied to two-more affiliation network, is a better techniques in terms of the modularity value that means it can generate more strong social communities in terms of social ties. On the other hand this technique is less time efficient as compared to hierarchical clustering.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">274</field>
<field name="author">Fawad Nazir</field>
<field name="author">Tallat Tarar</field>
<field name="author">Faran Javed</field>
<field name="author">Hiroki Suguri</field>
<field name="author">Farooq Ahmad</field>
<field name="author">Arshad Ali</field>
<field name="title">Constella: A Complete IP Network Topology Discovery Solution</field>
<field name="keyword">Network Monitoring and Management</field>
<field name="keyword"> OSI Layers Stack</field>
<field name="keyword"> Simple Network Management Protocol (SNMP)</field>
<field name="keyword"> Internet Control Message Protocol (ICMP)</field>
<field name="keyword"> Management Information Base (MIB)</field>
<field name="keyword"> Java Programming Language</field>
<field name="keyword"> Service Oriented Architectures (SOA).</field>
<field name="abstract">Network topology discovery for the large IP networks is a very well

studied area of research. Most of the previous work focus on improving the

efficiency in terms of time and completeness of network topology discovery

algorithms and less attention has been given to the deployment scenarios and

user centric view of network topology discovery. In this paper we propose a

novel network topology discovery algorithm and a flexible architecture. The

silent features of our work are loosely coupled architecture, network boundary

aware architecture, Integratability of network topology discovery solutions

using MIB/SOA, discovering the transparency of dumb/incorporative elements,

flexible network Visualization, and intelligent algorithm for quick response to

user discovery request. To the best of our knowledge no existing solution has

focused on the above mentioned requirements. After several years of research

experience in developing a complete, flexible and scalable solution for network

topology discovery we propose to divide it into three loosely coupled

components: topology discovery algorithm, topology object generation and

persistence and topology visualization. In this paper we will present our

proposed integrated complete network topology discovery solution, discuss the

motivation of our proposed architecture, the efficiency and user-friendliness of

our work. Our results show that the average accuracy of our algorithm is 92.4%

and the time it takes for discovery is approximately 100 hosts a second

depending the network load and number of alive machines. The more alive

entities in the network the faster will be the discovery and higher the crosstraffic

load on the network the slower will be the discovery.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">275</field>
<field name="author">David Rajaratnam</field>
<field name="author">Maurice Pagnucco</field>
<field name="title">Prime Implicates for Approximate Reasoning</field>
<field name="abstract">Techniques for improving the computational efficiency of inference

have held a long fascination in computer science.

Two popular methods include "approximate logics" and

"knowledge compilation".

In this paper we apply the idea of "approximate compilation"

to develop a notion of prime implicates for the family of classically

sound, but incomplete, approximate logics S-3. These logics allow

for differing levels of approximation by varying membership of a set

of propositional atoms.

We present a method for computing the prime S-3-implicates of a

clausal knowledge base and empirical results on the behaviour of

prime S-3-implicates over randomly generated 3-SAT problems.

A very important property of S-3-implicates and our algorithm for

computing them is that decreasing the level of approximation can

be achieved in an incremental manner without re-computing

from scratch (Theorem 7).</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">276</field>
<field name="author">Fawad Nazir</field>
<field name="author">Roksana Boreli</field>
<field name="author">Stephen Herborn</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">A Case Study for Mobility Management Protocol Co-existence</field>
<field name="keyword">Terms Mobility</field>
<field name="keyword"> Mobility Management</field>
<field name="keyword"> OSI Protocol Stack</field>
<field name="keyword"> Session Mobility</field>
<field name="abstract">Most existing mobility management protocols can

be broadly categorized into three major categories: higher level

protocols (e.g. SIP, SLM, MSOCKS, TCP Migrate, FreezeTCP,

L7-Mobility etc), lower level protocols (e.g. LWAPP, IAPP, MIP,

CIP, HMIP, LIN6 etc) and middle layer protocols (e.g. HIP,

MAST, MOBIKE etc). In general, lower layer protocols handle

device mobility without affecting the higher layers. The higher

layer protocols manage the mobility at the socket level and may

also support session mobility. The middle layer protocols are

targeted to provide loose coupling between higher and lower

layer protocols, so that changes in lower layers are invisible

to the higher layers and vice versa. All these protocols have

their strengths and weaknesses and most try to give a complete

mobility solution without considering services provided by other

mobility management protocols. Therefore, in this paper we

introduce a notion of Mobility Enabled Protocol Stack based

on the concept of mobility management protocol coexistence. We

argue that mobility management is a requirement and every

layer in the OSI stack has its own responsibility to fulfill this

requirement. In order to support our argument in this paper we

present a case study for co-existence of Host Identity Protocol

(HIP) and Session Layer Mobility (SLM) to provide a complete

mobility solution.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">277</field>
<field name="author">Fawad Nazir</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Towards Mobility enabled Protocol Stack for Future Wireless Networks</field>
<field name="keyword">Mobility</field>
<field name="keyword"> Mobility Management</field>
<field name="keyword"> OSI communication stack</field>
<field name="keyword"> mobile networking</field>
<field name="keyword"> wireless network architecture</field>
<field name="keyword"> heterogeneity.</field>
<field name="abstract">Future wireless networks have two widely accepted characteristics. Firstly, they

will be based on all-IP based network architecture and secondly they will integrate

heterogeneous wireless access technologies. As a result, there exist today a

multitude of solutions aimed at managing these imminent challenges. These

solutions are at varying stages of deployment, from purely analytical research, to

experimentally validated proposals, right through to fully standardised and

commercially available systems. In this paper we discuss the meaning,

requirements, responsibilities and solutions for mobility management on all seven

layers of the OSI communication stack. We identify internet mobility requirements

and perform valuable three dimensional analyses between internet mobility

requirements, mobility management protocols and layers of OSI communication

stack. We also quantify types of mobilites possible in the future wireless networks

and associate them with the responsible layers. In the end we conclude that no

single layer in the OSI stack is responsible to completely address all the internet

mobility requirements and support all the mobility types. We strongly believe that

every layer has its own responsibilities in order to support mobility. Therefore in

order to deal with mobility challenge we should have a Mobility Enabled

Protocol Stack instead of mobility management solution on a specific layer. We

argue that the best approach to build a complete mobility enabled protocol stack

for future wireless networks is based on the concept of Co-Existence of mobility

management protocols proposed on different layers, in a way that we get best out

of each. In the end, in order to support our arguments, we propose a novel mobility

enabled protocol stack, naming mechanism and wireless network architecture for

the future wireless networks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">278</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Patrick Senac</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Michel Diaz</field>
<field name="title">SPAD: a Distributed Middleware Architecture for QoS Enhanced Alternate Path Discovery</field>
<field name="keyword">Quality of Service</field>
<field name="keyword"> Overlay Networks</field>
<field name="keyword"> Peer-to-Peer</field>
<field name="keyword"> Service-oriented Networks</field>
<field name="keyword"> Middleware</field>
<field name="abstract">In the next generation Internet, the network will evolve from a plain communication medium into one that provides endless services to the users. These services will be composed of multiple cooperative distributed application elements. We name these services overlay applications. The cooperative application elements within an overlay application will build a dynamic communication mesh, namely an overlay association. The Quality of Service (QoS) perceived by the users of an overlay application greatly depends on the QoS experienced on the communication paths of the corresponding overlay association. In this paper, we present SPAD (Super-Peer Alternate path Discovery), a distributed middleware architecture that aims at providing enhanced QoS between end-points within an overlay association. To achieve this goal, SPAD provides a complete scheme to discover and utilize composite alternate end-to-end paths with better QoS than the path given by the default IP routing mechanisms.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">279</field>
<field name="author">Patrick Laube</field>
<field name="author">Matt Duckham</field>
<field name="author">Thomas Wolle</field>
<field name="title">Decentralized Movement Pattern Detection amongst Mobile Geosensor Nodes</field>
<field name="abstract">Movement patterns, like flocking and converging, leading and following, are examples of high-level process knowledge derived from low-level trajectory data. Conventional techniques for the detection of movement patterns rely on centralized ``omniscient'' computing systems that have global access to the trajectories of mobile entities. However, in decentralized spatial information processing systems, exemplified by wireless sensor networks, individual processing units may only have access to local information about other individuals in their immediate spatial vicinity. Where the individuals in such decentralized systems are mobile, there is a need to be able to detect movement patterns using collaboration between individuals, each of which possess only partial knowledge of the global system state. This paper presents an algorithm for decentralized detection of the movement pattern flock, with applications to mobile wireless sensor networks. The algorithm's reliability is evaluated through testing on simulated trajectories emerging from unconstrained random movement and correlated random walk.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">280</field>
<field name="author">Xi Chen</field>
<field name="author">Mark Staples</field>
<field name="author">Paul Bannerman</field>
<field name="title">Analysis of CMMI Specific Practice Dependencies</field>
<field name="keyword">SPI</field>
<field name="keyword"> CMMI</field>
<field name="keyword"> Specific Practice</field>
<field name="keyword"> Work Products</field>
<field name="keyword"> Dependency</field>
<field name="abstract">CMMI contains a collection of Process Areas (PAs), and each PA contains many Specific Practices (SPs). However, the CMMI specification does not provide any explicit recommendation about which individual SPs can or should be implemented before other SPs. In this paper we identify dependencies between CMMI SPs in PAs in maturity level 2, and also between those PAs. We analyzed the text of the CMMI specification to identify every Work Product (WP) produced and used by every SP in maturity level 2. Our analysis was validated by independent researchers and comparison with an existing dependency analysis shown in CMMI training materials. Our results have significance as a reference model of SP and PA dependencies for both SPI researchers and practitioners. For researchers we have provided an explicit representation of SP and PA dependencies that were previously only implicit in the CMMI specification. For practitioners, our results may provide guidance on the order of implementation of SPs and PAs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">281</field>
<field name="author">Luping Zhou</field>
<field name="author">Richard Hartley</field>
<field name="author">Lei Wang</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="title">Regularized Discriminative Direction for Shape Difference Analysis</field>
<field name="abstract">The discriminative direction has been proven useful to re-

veal the subtle difference of two anatomical shape classes. When a shape

moves along this direction, its deformation will best manifest the class

difference detected by a kernel classifier. However, we observe that such a

direction cannot maintain a shape s anatomical correctness, introduc-

ing spurious difference. To remove this drawback, we develop a regular-

ized discriminative direction by requiring a shape to conform to its pop-

ulation distribution when it deforms along the discriminative direction.

Instead of using iterative optimization, an analytic solution is provided

to work out this direction effortlessly. Experimental study shows its su-

perior performance in pursuing the authentic difference of hippocampal

shapes. Moreover, the obtained result is supported by other independent

research from a different perspective.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">282</field>
<field name="author">Jens Teubner</field>
<field name="author">Torsten Grust</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Sherif Sakr</field>
<field name="title">Dependable Cardinality Forecasts for XQuery</field>
<field name="abstract">Though inevitable for effective cost-based query rewriting, the

derivation of meaningful cardinality estimates has remained a

notoriously hard problem in the context of XQuery. By basing the

estimation on a relational representation of the XQuery syntax, we show

how existing cardinality estimation techniques for XPath and proven

relational estimation machinery can play together to yield dependable

forecasts for arbitrary XQuery (sub)expressions.

Our approach benefits from a light-weight form of data flow analysis.

\emph{Abstract domain identifiers} guide our query analyzer through the

estimation process and allow for informed decisions even in case of

deeply nested XQuery expressions. A variant of \emph{projection paths}

\cite{projecting-xml} provides a versatile interface into which existing

techniques for XPath cardinality estimation can be plugged in

seamlessly. We demonstrate an implementation of this interface based on

\emph{data guides}.

Experiments show how our approach can equally cope with both, structure-

and value-based queries. It is robust with respect to

intermediate estimation errors, from which we typically found our

implementation to recover gracefully.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">283</field>
<field name="author">Kazuhiro Inaba</field>
<field name="author">Haruo Hosoya</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Multi-Return Macro Tree Transducers</field>
<field name="abstract">This paper introduces an extension of macro tree transducers

with the capability of states to return multiple trees at the same time.

The new model is studied under call-by-value semantics, and is shown to be

strictly more expressive than call-by-value macro tree transducers,

and moreover, to have better closure properties under composition.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">284</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Keisuke Nakano</field>
<field name="title">XML Type Checking for Macro Tree Transducers with Holes</field>
<field name="abstract">Macro forest transducers (mfts) extend macro tree

transducers (mtts) from ranked to unranked trees.

Mfts are more powerful than mtts (operating on binary tree encodings)

because they support sequence concatenation of output trees as build-in

operation. Surprisingly, inverse type inference for mfts, for a fixed

output type, can be done within the same complexity as for mtts.

Inverse type inference is used in algorithms for

exact type checking of XML transformations.

The macro tree transducer with holes (hmtt) is a new concept that is

introduced in this paper. It generalizes sequence concatenation of mfts to

arbitrary tree concatenation. Hmtts are strictly more powerful than

mfts, in a similar way as mfts are more powerful than mtts.

Again, it comes as a surprise that inverse type

inference remains within the same complexity bound as for mfts.

Hmtts are a natural and robust extension of mtts:

any hmtt can be simulated by an mtt, followed by a

so called ``YIELD-mapping'', and,

conversely, any composition of an mtt with a YIELD-mapping

can be simulated by an hmtt.

This characterization implies that inverse type inference

for two-fold compositions of total deterministic mtts

can be done in 2-exponential time (a tower of exponents of height 2),

while the previously best known algorithm takes

3-exponential time.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">285</field>
<field name="author">Cheng Luo</field>
<field name="author">Xiongcai Cai</field>
<field name="author">Jian Zhang</field>
<field name="title">GATE: A Novel Resampler in Particle Filters for Object Tracking</field>
<field name="keyword">Tracking</field>
<field name="keyword"> Object Recognition</field>
<field name="keyword"> Segmentation</field>
<field name="abstract">This paper presents a novel resampling algorithm for particle filters based on image segmentation and optimization techniques employed in active contour models and level set methods. The proposed Geodesic Active conTour rEsampling, namely GATE, enables particle filters to track object of interest in complex environments using merely a simple feature. GATE creates a spatial prior in the state space using shape information of the tracked object. The created spatial prior is then used to filter particles in the state space in order to reshape and refine the posterior distribution of the particle filtering. This leads to significant improvement of the resampling in the particle filtering, so the significantly overall improvement of the particle filtering. The promising performance of our method on real sequences are demonstrated.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">286</field>
<field name="author">A. Anbulagan</field>
<field name="author">Alban Grastien</field>
<field name="title">Importance de la semantique dans le codage CNF de contraintes de cardinalite : application au diagnostic de SED</field>
<field name="keyword">Modeling Cardinality Constraint</field>
<field name="keyword"> Satisfiability</field>
<field name="keyword"> Diagnosis of DES</field>
<field name="abstract">Le codage d un probl`eme a un impact enorme sur

le temps de calcul en SAT : un solveur SAT peut r e-

soudre tr`es facilement un probl`eme cod e d une certaine

mani`ere, et eprouver des difficult es pour le m eme pro-

bl`eme cod e d une autre mani`ere. Pire, un codage peut

favoriser le temps de calcul d un solveur, et se montrer

au contraire inadapt e pour un autre solveur. Dans cet

article, nous nous concentrons sur l expression en CNF

de contraintes de cardinalit e : etant donn e un ensemble

V de variables propositionelles, etant donn ee une valeur

enti`ere k, exactement k variables de V doivent etre eva-

lu ees `a vrai. Nous proposons de nouvelles mani`eres de

mod eliser ces contraintes en cherchant `a grouper effica-

cement les variables de s emantique proche. Nous etu-

dions ces codages sur des probl`emes de diagnostic de

syst`eme `a ev enements discrets (SED). Des probl`emes

jusqu ici insolubles peuvent etre `a pr esent r esolus `a la

fois par des proc edures syst ematiques ou stochastiques.

Les r esultats mettent egalement en lumi`ere l existence

d un codage qui convienne aux deux types d algorithme.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">287</field>
<field name="author">Hai Pham</field>
<field name="author">Dimosthenis Pediaditakis</field>
<field name="author">Thanassis Boulis</field>
<field name="title">From Simulation to Real Deployments in WSN and Back</field>
<field name="abstract">The paper presents our efforts to validate some high-level aspects of the WSN simulator we have built as well as the operational functionality of our multi-parameter MAC protocol. In order to do so, we resort to real deployments involving TelosB motes. The simulator, named Castalia, boasts the most accurate wireless channel and radio models for WSN found in current literature. These models are capturing some essential experimental findings. This does not guaranty though that the simulator will behave similarly with a real deployment at the high level (i.e., the protocol or application level). We investigate how our multi-parameter MAC protocol behaves in a real deployment so as to take a first step towards validating and possibly tuning Castalia. The investigation starts by determining the connectivity map for the real deployment and then trying to reproduce it in the simulator. We then proceed with the protocol testing and comparing. We report the difficulties faced and our findings from this process.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">288</field>
<field name="author">Thanassis Boulis</field>
<field name="title">Demo Abstract: Castalia: Revealing Pitfalls in Designing Distributed Algorithms in WSN</field>
<field name="abstract">We present Castalia, a simulator for WSN that models many aspects of the WSN system and uses advanced models especially in terms of the channel and radio behaviour. We show the effects of these features in distributed algorithms that work fine with simpler simulators but fail under Castalia. The demo will present the differences, explain the failures and show how to redesign the algorithms to make them work under more realistic conditions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">289</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">ALARM: An Adaptive Load-Aware Routing Metric for Hybrid</field>
<field name="keyword">Hybrid</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="keyword"> routing</field>
<field name="abstract">Hybrid Wireless Mesh Networks (WMN) are generally employed to establish communication during disaster recovery operations. The Hybrid WMN network is formed in a spontaneous manner when wireless nodes belonging to different agencies are operated in the disaster area. These wireless nodes generally have heterogeneous configurations in terms of number of transceivers, computational power, battery resources and mobility pattern. Internet Protocol (IP) acts as the common platform for integrating these heterogeneous devices. Routing protocols are engaged to determine paths between sets of wireless nodes. A number of routing metrics have been developed to achieve performance gains in multi-radio, multi-hop WMNs. However, most of these metrics need access to external information like link quality statistics, channel numbers, etc on a regular basis. This frequent information exchange coupled with the incessant variation in mobility and traffic load conditions causes degraded performance of these metrics in Hybrid WMNs. In this paper, we present a routing metric named ALARM specifically designed for Hybrid WMNs. ALARM is computed using the number of packets queued per wireless interface. This computed value offers an accurate representation of the traffic load, link quality, interference and noise levels. With the help of extensive simulations, we show that our routing metric outperforms well-know routing metrics like ETT and WCETT under varying mobility and traffic load conditions in Hybrid WMNs. We further show the practicality of the metric through a prototype implementation and provide performance results obtained from a small-scale testbed deployment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">290</field>
<field name="author">Lixiang Xiong</field>
<field name="author">Lavy Libman</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Distributed strategies for Minimum-Latency Cooperative Retransmission in Wireless Networks</field>
<field name="keyword">cooperative retransmission</field>
<field name="keyword"> opportunistic routing</field>
<field name="keyword"> delay-critical applications</field>
<field name="abstract">We consider cooperative retransmission strategies in wireless networks, where the retransmission of a failed frame is handled not by the original source but rather by common neighbors overhearing the transmission. The majority of existing literature in this space focuses on opportunistic mechanisms for choosing a single "best" neighbor, with a goal of minimizing the number of required (re-)transmissions. However, the coordination overhead of such mechanisms renders them unsuitable in general for scenarios involving delay-critical control or sensing applications, where the delivery latency, rather than number of retransmissions, is the dominant performance criterion. Accordingly, we study a distributed uncoordinated setting, where each neighbor that successfully overhears a frame decides independently whether to retransmit it in subsequent time slots, considering that multiple simultaneous such retransmissions will cause a collision. We employ a Bayesian approach to analyze the evolution of the system state view from the perspective of each cooperative neighbor, and derive a strategy of finding a sequence of retransmission probabilities for every neighbor in each time slot to minimize the expected delivery latency. We demonstrate for a wide variety of scenarios that this strategy achieves a significantly lower expected latency than either traditional retransmission or two-hop routing to the destination.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">291</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Max Ott</field>
<field name="title">Predicting Network Availability Using User Context</field>
<field name="keyword">Network Availability Prediction</field>
<field name="keyword"> Context Prediction</field>
<field name="keyword"> Semi</field>
<field name="abstract">Mobile devices can connect to different access networks with

different characteristics at different times in a heterogeneous

network environment. Majority of solutions for utilizing multiple

access possibilities are reactive in nature. For example, when

networks are encountered, the device performs a vertical handover

to the network that offers the highest bandwidth. But the cost of

handover may not be justified if that high bandwidth network

appears only for a short time. Knowledge of future network

availability would help to proactively handle the handover process

intelligently. Network availability prediction is addressed in

literature as user path predictions with network coverage maps,

using domain specific models designed under assumed conditions.

We model this as a more robust context prediction problem

without using any domain specific knowledge that can use any of

the available context variables like GSM cell ID, WLAN AP,

whether the power cable plugged, number of people around etc. It

is also important to learn most contributing context variables to

predictions so that the device can avoid non contributing variables

saving power and processing. In this paper, we propose a Semi-

Markovian context prediction model to predict WLAN

availability and present a method to rank each context variable

according to their importance in predictions. We also use the same

method for optimizing model parameters. Real user data collected

in our experiments show that when WLAN status is static,

predictions are nearly equal and even in changing environments,

difference between actual and predicted probabilities can go up

only to 26% on average and the context variable ranking is

realistic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">292</field>
<field name="author">Steve Glass</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">A Study of the TKIP Cryptographic DoS Attack</field>
<field name="keyword">security</field>
<field name="keyword"> DoS</field>
<field name="keyword"> defences</field>
<field name="abstract">Wireless networks, especially those based on 802.11, have found widespread use in domestic, commercial, educational, military and public-safety environments. The security of these wireless networks is assuming an increasing importance as users come to rely on the availability and correct functioning of wireless network services.



This paper investigates the cryptographic denial-of-service (DoS) attack agaist the 802.11i TKIP security protocol. We have conducted a laboratory study and show that it takes very little effort to bring TKIP-protected network traffic to a complete halt. This attack maybe used not just to compromise availability but is also an effective means of conducting a security-level rollback to the insecure WEP protocol. We use a testbed network to evaluate a remedial measure that eliminates the vulnerability on which the attack is based.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">293</field>
<field name="author">Steve Glass</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">Securing Multi-Hop Wireless Networks Against Impersonation Attacks</field>
<field name="keyword">security</field>
<field name="keyword"> defences</field>
<field name="keyword"> intrusion detection</field>
<field name="abstract">Wireless sensor networks originated with military surveillance applications but are increasingly being used to monitor industrial processes, the health of medical patients and environmental hazards such as geological events and bushfires. In many applications failures of the sensor net can place millions of dollars, and sometimes human lives, at threat so security has assumed an increasing importance.



Securing wireless sensor networks is a difficult problem because of the severely constrained environment provided by typical sensor motes. Conventional network security measures cannot simply be retro-fitted into wireless sensor nets because the computational and communications costs are often too high. The problem is further compounded because the physical integrity of sensors themselves cannot be guaranteed. The sensor network environment requires that new, innovative and efficient approaches are adopted.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">294</field>
<field name="author">Steve Glass</field>
<field name="author">Marius Portmann</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">Securing Wireless Mesh Networks</field>
<field name="keyword">security</field>
<field name="keyword"> wireless mesh</field>
<field name="abstract">Wireless networks are becoming ubiquitous and can be found in domestic, commercial, industrial, military and health care applications. Wireless Mesh Networks (WMNs) combine the robustness and performance of conventional infrastructure networks with the large service area, self-organizing and selfhealing properties of mobile ad hoc networks (MANETs). In this article we consider the problem of ensuring security in WMNs and discuss some of the problems of ensuring the security at the network and data-link layers.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">295</field>
<field name="author">Golam Sarwar</field>
<field name="author">Roksana Boreli</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="title">Improvements in DCCP congestion control for satellite links</field>
<field name="keyword">DCCP</field>
<field name="keyword"> TFRC</field>
<field name="keyword"> transport protocol</field>
<field name="keyword"> wireless</field>
<field name="keyword"> satellite</field>
<field name="abstract">We propose modifications in the TCP-Friendly Rate Control (TFRC) congestion control mechanism from the Datagram Congestion Control Protocol (DCCP) intended for use with real-time traffic, which are aimed at improving its performance for long delay (primarily satellite) links. Firstly, we propose an algorithm to optimise the number of feedback messages per return trip time (RTT) rather than use the currently standard of at least one per RTT, based on the combination of the observed data rate, link delay and average packet size. We analyse the improvements achievable with both modifications in different phases of congestion control and present results from simulations with modified ns-2 DCCP and live experiments using the modified DCCP Linux kernel implementation. We demonstrate that the changes results in improved slow start performance and a reduced data loss compared to standard DCCP, while the increased overhead introduced remains acceptable.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">296</field>
<field name="author">Jack Tsai</field>
<field name="author">Tim Moors</field>
<field name="title">Minimum Interference Multipath Routing Using Multiple Gateways in Mesh Networks</field>
<field name="keyword">multipath</field>
<field name="keyword"> routing</field>
<field name="keyword"> mesh</field>
<field name="keyword"> gateway</field>
<field name="abstract">In this paper we investigate the reliability of multipath routing using multiple gateways in a wireless 

mesh network. We propose a greedy gateway placement algorithm that tries to minimise the number of gateways needed when a maximum distance between each node and a predefine number of gateways is guaranteed. Simulations results show that our intelligent gateways placement can be employed to complement multipath routing to further improve the reliability of packet delivery.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">297</field>
<field name="author">Andrew Zhang</field>
<field name="author">Rodney Kennedy</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Reduced-Rank Shift-Invariant Technique and Its Application for Synchronization and Channel Identification in UWB Systems</field>
<field name="keyword">Ultra Wideband</field>
<field name="keyword"> Shift invariant techniques</field>
<field name="keyword"> Synchronization</field>
<field name="keyword"> Channel estimation</field>
<field name="abstract">In this paper, we investigate reduced rank shift invariant

techniques in the application of synchronization and channel

identification for UWB systems. General correlator based

algorithms confront many limitations in UWB systems. The shift

invariant techniques, such as ESPRIT and the matrix pencil method,

have high resolution ability, but the associated high complexity

makes them less attractive in real time implementations. Aiming at

reducing the complexity, we developed novel Reduced-rank

Identification of Principal Components (RIPC) algorithms. These

RIPC algorithms can automatically track the principal components

and reduce the computational complexity significantly by

transforming the generalized eigen-problem in an original high

dimensional space to a lower dimensional space depending on the

number of desired principal signals. Technical details of RIPC

algorithms in the application of joint UWB synchronization and

channel estimation, including the operations of sampling, fast

Fourier transform (FFT) and the capture of synchronization delay,

are given. Experiments show the performance is only slightly

inferior to the general full rank algorithms.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">298</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Nikolay Mihaylov</field>
<field name="author">Sherif Sakr</field>
<field name="title">XML Tree Structure Compression</field>
<field name="keyword">XML</field>
<field name="keyword"> trees</field>
<field name="keyword"> data compression</field>
<field name="abstract">In an XML document a considerable fraction consists of markup,

that is, begin and end-element tags describing the document's

tree structure.

XML compression tools such as XMill separate the tree structure

from the data content and compress each separately.

The main focus in these compression tools is how to group similar

data content together prior to performing standard

data compression such as gzip, bzip2, or ppm.

or natural language based dictionary compression.

In contrast, the focus of this paper is on compressing the tree

structure part of an XML document.

We use a known algorithm to derive a grammar representation of

the tree structure which factors out the repetition of

tree patterns. We then investigate several succinct binary

encodings of these grammars.

Our experiments show that we can be consistently smaller than

the tree structure compression carried out by XMill, using the same

backend compressors as XMill on our encodings. However, the most

surprising result is that our own Huffman-like encoding of the

grammars (without any backend compressor whatsoever)

consistently outperforms XMill with gzip backend.

This is of particular interest because our Huffmann-like encoding

can be queried without prior decompression. To the best of our

knowledge this offers the smallest queriable XML tree structure

representation currently available.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">299</field>
<field name="author">renato iannella</field>
<field name="title">Digital Rights Management Technology</field>
<field name="abstract">Digital rights management (DRM) technologies covers the broad area of intellectual property management by providing secure and trusted services to control the use and distribution of content. DRM technologies consists of a mix of business models, social issues, legal conformance, and technical capabilities. This implies that DRM technologies are an integral part of the entire end-to-end content management lifecycle, not just a single service that exists in isolation. Hence the key to successful DRM technologies is that it is not be seen as a separate DRM system, but as part of the overall content management and consumption framework.

This chapter covers digital rights management (DRM) technologies, the broad area of the management of intellectual property rights over content utilizing digital technologies. Explored in this chapter are the fundamental aspects and framework behind DRM, its evolution to date, and its future. The key aspects of DRM architectures, licensing, and information models are discussed in detail, including the current state of DRM standards.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">300</field>
<field name="author">Karen Henricksen</field>
<field name="author">Renato Iannella</field>
<field name="title">Towards Standards-Based Resource Management Systems for Emergency Management</field>
<field name="abstract">A key challenge in emergency management is the efficient management of resources 

both human (e.g., response teams) and material (e.g., tents and food supplies). A large- 

scale event such as a cyclone/hurricane, earthquake or tsunami can potentially involve 

tens of thousands (or more) of resource requests and offers. Sophisticated information 

systems are required for managing the necessary information exchanges between 

resource requesters, owners, coordinating agencies and other parties, and for tracking the 

status of deployed resources. These systems must be scalable and support cross- 

organisational cooperation. To meet these requirements, they should ideally be based on 

open standards that allow interoperation between different Resource Management System 

(RMS) implementations, as well as interoperation and integration with other types of 

emergency management, and business-as-usual, software.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">301</field>
<field name="author">Renato Iannella</field>
<field name="title">Rights Management and Licensing Multimedia Services</field>
<field name="abstract">Digital Rights Management (DRM) covers the broad area of intellectual property management and enforcement by providing secure and trusted services to control the use and distribution of content. More specifically, Rights Management is now becoming the mechanism to address a more holistic view of DRM in the content communities. In particular, with various technologies used for licensing content with rights expression languages. These technologies and standards will be covered in this chapter and how they are applied to multimedia content services</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">302</field>
<field name="author">Kevin Buchin</field>
<field name="author">Sergio Cabello</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Maarten L _ffler</field>
<field name="author">Jun Luo</field>
<field name="author">G _nter Rote</field>
<field name="author">Rodrigo Silveira</field>
<field name="author">Bettina Speckmann</field>
<field name="author">Thomas Wolle</field>
<field name="title">Detecting Hotspots in Geographic Networks</field>
<field name="abstract">We study a point pattern detection problem on networks, motivated by geographical analysis tasks, such as crime hotspot detection. Given a network $N$ (for example, a street, train, or highway network) together with a set of sites which are located on the network. (for example, accident locations or crime scenes), we want to find a connected subnetwork $F$ of $N$ of small total length that contains many sites. That is, we are searching for a subnetwork $F$ that spans a cluster of sites which are close with respect to the network distance.



We consider different variants of this problem where $N$ is either a general graph or restricted to a tree, and the subnetwork $F$ that we are looking for is either a simple path, a path with self-intersections at vertices, or a tree. Many of these variants are NP-hard, that is, polynomial-time solutions are very unlikely to exist. Hence we focus on exact algorithms for special cases and efficient algorithms for the general case under realistic input assumptions.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">303</field>
<field name="author">Bojan Djordjevic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Anh Pham</field>
<field name="author">Thomas Wolle</field>
<field name="title">Detecting Regular Visit Patterns</field>
<field name="abstract">We are given a trajectory $\T$ and an area $\A$.

$\T$ might intersect $\A$ several times,

and our aim is to detect whether $\T$ visits $\A$ with some regularity,

e.g.~what is the longest time span that a GPS-GSM equipped elephant visited a specific lake on a daily (weekly or yearly) basis,

where the elephant has to visit the lake {\em most} of the days (weeks or years), but not necessarily on {\em every} day (week or year).



During the modelling of such applications, we encountered an elementary problem on bitstrings, that we call {\sc LDS (LongestDenseSubstring)}.

The bits of the bitstring correspond to a sequence of regular time points,

in which a bit is set to $1$ iff the trajectory $\T$ intersects the area $\A$ at the corresponding time point.

For the LDS problem, we are given a string $s$ as input and want to output a longest substring of $s$,

such that the ratio of $1$'s in the substring is at least a certain threshold.



In our model, LDS is a core problem for many applications that aim at detecting regularity of~$\T$ intersecting $\A$.

We propose an optimal algorithm to solve LDS,

and also for related problems that are closer to applications,

we provide efficient algorithms for detecting regularity.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">304</field>
<field name="author">Jacky Keung</field>
<field name="author">Barbara Kitchenham</field>
<field name="title">Experiments with Analogy-X for Software Cost Estimation</field>
<field name="abstract">We developed a novel method called Analogy-X to provide statistical inference for analogy-based software effort estimation. Analogy-X is a method to statistically evaluate the relationship between useful project features and target features such as effort to be estimated, which ensures the dataset used is relevant to the prediction problem, and project features are selected based on their statistical contribution to the target variables. We hypothesize that this method can be (1) easily applied to a much larger dataset, and (2) also it can be used for incorporating joint effort and duration estimation into analogy, which was not previously possible with conventional analogy estimation. To test these two hypotheses, we conducted two experiments using different datasets. Our results show that Analogy-X is able to deal with ultra large datasets effectively and provides useful statistics to evaluate the quality of the dataset. In addition, our results show that feature selection for duration estimation differ from feature selection for joint-effort duration estimation. We conclude Analogy-X allows users to assess the best procedure for estimating duration given their specific requirements and dataset.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">305</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Jacky Keung</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Ross Jeffery</field>
<field name="title">Semi-Quantitative Modeling for Managing Software Development Processes</field>
<field name="abstract">Software process modeling has become one of the necessary techniques for managing software development processes. However, purely quantitative process modeling requires a very specific understanding and accurate measurement of the software process, which relies on reliable and precise history data. This paper presents a semi-quantitative modeling approach that the authors have developed and propose to manage software development processes. It allows the uncertainty and contingency existing during the software development, and facilitates manager s qualitative and quantitative estimates and assessments of process progress. We demonstrate its value and flexibility by developing semi-quantitative models of the test-and-fix process of incremental software development. This modeling approach can support process or project management activities, including estimating, planning, tracking and decision making.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">306</field>
<field name="author">Taolue Chen</field>
<field name="author">Wan Fokkink</field>
<field name="author">Robert van Glabbeek</field>
<field name="title">Ready to Preorder: The Case of Weak Process Semantics</field>
<field name="keyword">process algebra</field>
<field name="keyword"> semantic equivalences and preorders</field>
<field name="keyword"> equational axiomatisations</field>
<field name="keyword"> _-completeness; failures equivalence</field>
<field name="abstract">Recently, Aceto, Fokkink lfsd ttir proposed an

algorithm to turn any sound and complete axiomatisation of any

preorder listed in the linear time branching time spectrum at

least as coarse as the ready simulation preorder, into a sound and

complete axiomatisation of the corresponding equivalence its

kernel. Moreover, if the former axiomatisation is

 -complete, so is the latter. Subsequently, de Frutos

Escrig, Gregorio Rodr guez &amp; Palomino generalised this

result, so that the algorithm is applicable to any preorder at

least as coarse as the ready simulation preorder, provided it is

initials preserving. The current paper shows that the same

algorithm applies equally well to weak semantics: the proviso of

initials preserving can be replaced by other conditions, such as

weak initials preserving and satisfying the second -law.

This makes it applicable to all 87 preorders surveyed in "the

linear time branching time spectrum II" that are at least as

coarse as the ready simulation preorder. We also extend the scope

of the algorithm to infinite processes, by adding recursion

constants. As an application of both extensions, we provide a

complete axiomatisation of the CSP failures equivalence for BCCS

processes with divergence.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">307</field>
<field name="author">Elena Kelareva</field>
<field name="author">Olivier Buffet</field>
<field name="author">Jinbo Huang</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">Factored Planning Using Decomposition Trees</field>
<field name="keyword">planning</field>
<field name="keyword"> scheduling</field>
<field name="keyword"> artificial intelligence</field>
<field name="abstract">Improving AI planning algorithms relies on the ability to exploit the structure of the problem at hand. A promising direction is that of factored planning, where the domain is partitioned into subdomains with as little interaction as possible. Recent work in this field has led to an detailed theoretical analysis of such approaches and to a couple of high-level planning algorithms, but with no practical implementations or with limited experimentations. This paper presents dTreePlan, a new generic factored planning algorithm which uses a

decomposition tree to efficiently partition the domain. We discuss some of its aspects, progressively describing a specific implementation before presenting experimental results. This prototype algorithm is a promising contribution with major possible improvements and helps enrich the picture of factored planning approaches.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">308</field>
<field name="author">Anika Schumann</field>
<field name="author">Yannick Pencole</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">A Spectrum of On-Line Symbolic Diagnosis Approaches</field>
<field name="keyword">Model-Based Diagnosis</field>
<field name="keyword"> Artificial Intelligence</field>
<field name="abstract">This paper deals with the monitoring and diagnosis of large

discrete-event systems. The problem is to determine, on-line, all

faults and states that explain the flow of observations. Model-based

diagnosis approaches that first compile the diagnosis information

off-line suffer from space explosion, and those that operate on-line

without any prior compilation have poor time performance. Our

contribution is a broader spectrum of approaches that suits

applications with diverse time and space requirements. Approaches on

this spectrum differ in the amount of reasoning and compilation

performed off-line and therefore in the way they resolve the tradeoff

between the space occupied by the compiled information and the time

taken to produce a diagnosis. We tackle the space and time complexity

of diagnosis by encoding all approaches in a symbolic framework based

on binary decision diagrams. This allows for the compact

representation of the compiled diagnosis information, and for its

handling across many states at once rather than for each state

individually. Our experiments demonstrate the diversity and

scalability of our symbolic methods spectrum, as well as its

superiority over the corresponding enumerative implementations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">309</field>
<field name="author">Paul Brebner</field>
<field name="author">Liam O'Brien</field>
<field name="author">Jon Gray</field>
<field name="title">Performance Modeling for e-Government Service Oriented Architectures (SOAs)</field>
<field name="keyword">e</field>
<field name="keyword">Government SOA Performance modelling</field>
<field name="abstract">In Australia, there are examples of large and complex e-Government systems both under development and currently deployed. Some systems provide services directly to citizens and business, whereas others provide mission-critical services to other federal, state or local government agencies. Increasingly, these systems are being designed as Service Oriented Architectures (SOAs), and are implemented as workflows and composite service applications. Because of their critical role in the delivery of services to citizens and business, it is vital to understand the performance and scalability limits of these SOA systems well in advance of switch-on.

 Since 2006 our software engineering group has been developing a range of related technologies for early lifecycle performance assessment of large, heterogeneous service architectures. The suite of related technology artifacts includes modeling methodologies, techniques and tools, and architecture and technology assessments. We have undertaken a series of collaborative research engagements with Australian government agencies to validate the effectiveness of this approach to performance assessment. With each field trial, the performance assessment methods are becoming more robust, repeatable, and mature. This experience report illustrates the most recent application of the technology to a whole-of-government service architecture.

The technology enabled rapid development of performance models for SOAs, modeling complex deployment technologies such as server virtualization, use and reuse of available performance measurements to parameterize the model, computation of critical performance metrics for SOAs, model validation and refinement, modeling of architectural and resource alternatives, composable performance modeling of service compositions, and reasoning about Service Level Agreements (SLAs).

We also describe our experiences with performance modeling tools, and report on our own tool which was designed to directly support interactive SOA performance modeling. We conclude with an assessment of the technology's effectiveness and the business impact upon the participating organization.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">310</field>
<field name="author">Paul Brebner</field>
<field name="author">Liam O'Brien</field>
<field name="author">Jon Gray</field>
<field name="title">Performance Modeling for Service Oriented Architectures</field>
<field name="keyword">Service Oriented Architecture</field>
<field name="keyword"> SOA</field>
<field name="keyword"> performance modeling.</field>
<field name="abstract">We present a tool for performance modeling of Service Oriented Architectures (SOAs). As mission-critical use of whole-of-government SOAs become pervasive, the capability to model and predict the run-time performance of interdependent composite applications is critical. The tool can be used by architects early in the software engineering lifecycle to predict performance and scalability, to evaluate architectural alternatives, to provide guidance for capacity planning and the negotiation of Service Level Agreements (SLAs). It directly models and produces metrics for SOA applications in terms that are familiar to architects (services, workflows, and compositions of services). The tool enables the performance model to be generated from available architectural artifacts and performance data, making it easy to use. It is highly dynamic to facilitate interactive evaluation of alternative architectural choices. The tool can model complex deployment scenarios such as server virtualisation. Development and evaluation of the tool was carried out in the context of architectural modeling for large-scale SOA-based Australian e-Government systems. The tool radically simplified the construction and execution of SOA performance models, and contributed critical insights for the architecting of these systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">311</field>
<field name="author">Gia Hieu Dinh</field>
<field name="author">Jenny Liu</field>
<field name="title">Enabling Mobile Access to Adaptive Services and Portal</field>
<field name="abstract">Currently, enhancing the interoperability of Web 2.0 Technologies over mobile

devices remains a challenge to software engineering. Resource limitation of mobile

devices and non-adaptability of web server application has restricted the interaction

between the two aforementioned technologies causing clients difficulties in handling

the response in an appropriate manner.

This project proposes an Adaptive Service Middleware Architecture liable to enhance

interoperability of Web 2.0 Technologies over mobile devices through minimizing the

barrier from web server application non-adaptability. The architecture makes a

twofold contribution to enable the mobile access to web services and interactive

portals. As for web services, a proof-of-concept implementation have been developed

to demonstrate the adaptiveness of the architecture while invoking a third-party

Amazon Web Services. A number of performance measurements have also been

conducted based on this implementation to evaluate the performance overheads of the

adaptive components. The results have proved that the overheads that adaptive

components incurred are insignificant to the non-adaptive approach. In addition, the

architecture can also be extended to an interactive portal which is accessible from

mobile devices. This portal can adaptively render other third-party portlets that follow

the Java Portlet Specification (JSR-168) standard and Web Services for Remote

Portlets (WSRP) standard.

Once the architecture is implemented, it can act as a transparent middleware that

enhances the adaptiveness of the servers to minimize the resources required from

mobile devices.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">312</field>
<field name="author">Gunawan Herman</field>
<field name="author">Getian Ye</field>
<field name="author">Jie Xu</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">Region-Based Image Categorization with Reduced Feature Set</field>
<field name="keyword">image categorization</field>
<field name="keyword"> multiple instance learning</field>
<field name="keyword"> hyperclique pattern</field>
<field name="keyword"> reduce feature set</field>
<field name="abstract">In this paper, we propose a new approach to region-based image categorization that is formulated as Multiple-Instance Learning (MIL) problems in general. The advantages of the proposed algorithm over existing algorithms are three-folds: (i) unlike the existing methods which use learning methods that are specifically designed for MIL or for certain datasets, the proposed method uses a general-purpose standard supervised learning method, (ii) it uses a significantly small set of discriminative features which are empirically more discriminative than the PCA features (i.e. principal components), and (iii) it is simple and efficient and it achieves comparable performance to the state-of-the-art algorithms. The features used in the proposed algorithm are the hyperclique patterns which are ``condensed'' into a small set of discriminative features. Each hyperclique pattern consists of multiple strongly-correlated instances (i.e., features). As a result, hyperclique patterns are able to capture the information that cannot be captured by individual features.

The efficiency and the good performance of the proposed approach make it a practical solution to the MIL problems in general. In this paper, we apply the proposed approach to image categorization and drug activity prediction and the experimental results show promising results.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">313</field>
<field name="author">Phu Ngoc Le</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">An Improved Soft Threshold Method for DCT Speech Enhancement</field>
<field name="keyword">speech enhancement</field>
<field name="keyword"> DCT</field>
<field name="keyword"> soft threshold</field>
<field name="abstract">An improved soft threshold method for speech enhancement in the Discrete Cosine Transform (DCT) domain is proposed in this paper. Rather than apply a threshold only to noise-dominant frames, as per traditional DCT-based approaches, our proposed approach also applies the threshold process appropriately in signal-dominant frames. Experimental results show a quality improvement with our proposed method compared to traditional soft threshold methods.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">314</field>
<field name="author">Jie Xu</field>
<field name="author">Getian Ye</field>
<field name="author">Gunawan Herman</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">Detecting and Recognizing Moving Pedestrians in Video</field>
<field name="keyword">pedestrian detection</field>
<field name="keyword"> pedestrian recognition</field>
<field name="keyword"> surveillance</field>
<field name="abstract">Detecting and recognizing pedestrians in video footages are two essential and significant tasks in many automatic video understanding systems. In this paper, we propose an efficient approach to moving pedestrian detection and recognition in video. The testing process of this approach involves two main steps: moving edge detection and hypotheses generation. Moving edges are firstly extracted by comparing the edges identified in adjacent frames. Shape context descriptors are then produced for the edge points sampled from the moving edges and matched against the instances of a codebook that is learned from a set of training samples to generate initial hypotheses. Final hypotheses are formed by pruning initial hypotheses with large overlaps. Experiments with a publicly available dataset show that the proposed approach can reliably detect and recognize moving pedestrians in real scenes that contain either different viewing angles or different degrees of occlusions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">315</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="title">Characterising Probabilistic Processes Logically (Extended Abstract)</field>
<field name="keyword">Concurreny</field>
<field name="keyword"> Probabilistic Processes</field>
<field name="keyword"> Modal mu-calculus</field>
<field name="keyword"> simulation</field>
<field name="keyword"> bisimulation</field>
<field name="keyword"> characteristic formulae</field>
<field name="abstract">In this paper we work on (bi)simulation semantics of processes that

exhibit both nondeterministic and probabilistic behaviour.

We propose a probabilistic extension of the modal mu-calculus and show

how to derive characteristic formulae for various simulation-like

preorders over finite-state processes without divergence.

In addition, we show that even without the fixpoint operators

this probabilistic mu-calculus can be used to characterise these

behavioural relations in the sense that two states are equivalent

if and only if they satisfy the same set of formulae.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">316</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">An ontological basis for design methods</field>
<field name="keyword">Design Methods; Function</field>
<field name="keyword">Behaviour</field>
<field name="keyword">Structure (FBS) Ontology; Prescriptive Design Knowledge</field>
<field name="abstract">This paper presents a view of design methods as process artefacts that can be represented using the function-behaviour-structure (FBS) ontology. This view allows identifying five fundamental approaches to methods: black-box, procedural, artefact-centric, formal and managerial approaches. They all describe method structure but emphasise different aspects of it. Capturing these differences addresses common terminological confusions relating to methods. The paper provides an overview of the use of the fundamental method approaches for different purposes in designing. In addition, the FBS ontology is used for developing a notion of prescriptiveness of design methods as an aggregate construct defined along four dimensions: certainty, granularity, flexibility and authority. The work presented in this paper provides an ontological basis for describing, understanding and managing design methods throughout their life cycle.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">317</field>
<field name="author">Bo Yin</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Fang Chen</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="title">Exploring Classification Techniques in Speech based Cognitive Load Monitoring</field>
<field name="keyword">cognitive load</field>
<field name="keyword"> pattern recognition</field>
<field name="keyword"> Gaussian mixture model</field>
<field name="keyword"> support vector machine</field>
<field name="keyword"> fusion</field>
<field name="abstract">The ability to monitor cognitive load level in real time is extremely useful for preventing fatal operating errors or improving the efficiency of task execution. In top of the success of our previously proposed speech based cognitive load monitoring system, we explored alternative classification techniques in this paper, including simple linear kernel Support Vector Machine (SVM), hybrid SVM-GMM which accepts the likelihood scores from GMM as inputs for SVM, and a fusion approach which integrates GMM, SVM and SVM-GMM systems together. All systems are evaluated on the data collected from two different tasks a reading comprehension and a Stroop test based task. SVM-GMM based system achieves the highest performance on both tasks and improves the accuracy of three cognitive load levels classification from 71.1% to 75.6% and 77.5% to 88.2%, respectively.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">318</field>
<field name="author">Bo Yin</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Fang Chen</field>
<field name="title">Introducing FM based Feature to Hierarchical Language Identification</field>
<field name="keyword">frequency modulation</field>
<field name="keyword"> language identification</field>
<field name="keyword"> hierarchical classification</field>
<field name="keyword"> fusion</field>
<field name="abstract">Although relatively neglected in auditory analysis, phase information plays an important role in human auditory intelligibility. This paper investigates a Frequency Modulation (FM) based feature and its contribution to a Language Identification (LID) system, using Hierarchical LID framework. FM components represent the phase information of a given signal in an AM-FM model. In this paper, we extract a FM-based feature using a technique which produces consistent and continuous FM components, and build a LID system on this feature with GMM based modeling. The performance is improved by combining this system with existing MFCC, Prosody based systems and a PRLM system. Comparing to the baseline system without integrating a FM-based system, the proposed Hierarchical LID system shows improvements. Additionally, the proposed system outperforms the GMM fusion-based system integrating the same four primary systems, showing that Hierarchical LID framework is more effective in integrating additional features.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">319</field>
<field name="author">Jianlong Zhou</field>
<field name="author">Masa Takatsuka</field>
<field name="title">Structural Relationship Preservation in Volume Rendering</field>
<field name="keyword">structural relationship</field>
<field name="keyword"> relationship depiction</field>
<field name="keyword"> volume rendering</field>
<field name="abstract">This paper presents structural relationship preservation as a technique for improving efficiency of volume rendering-based 3D data analysis. In the presented approach, a mapping between data and renderings is defined and volumetric object appearances are determined by structural relationships of interest in the data space. Two typical relationships of inclusion and neighboring are defined, extracted, represented, and revealed in volume rendering in the pipeline. The structural relationship representation, which is controlled by a contour tree, allows users to perceive relationships, and to control how and what structural relationships are revealed in the pipeline. The experimental results show that structural relationship preservation in volume rendering provides more intuitive and physically meaningful renderings.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">320</field>
<field name="author">Sherif Sakr</field>
<field name="title">Algebra-based XQuery Cardinality Estimation</field>
<field name="keyword">XML </field>
<field name="keyword"> XQuer </field>
<field name="keyword"> Cardinality Estimation</field>
<field name="abstract">Estimating the sizes of query results and intermediate results is crucial to many aspects of query processing. All database systems rely

on the the use of selectivity estimates to choose the cheapest execution plan. In principle, the problem of cardinality estimation is more complicated

in the XML domain than the relational domain. In this article we presents a novel framework for estimating the cardinality of XQuery expressions

as well as its sub-expressions. As a major innovation, we exploit the relational algebraic infrastructure to provide accurate estimation in the context of XML and XQuery domains.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">321</field>
<field name="author">Paul Bannerman</field>
<field name="title">Software Project Risk in the Public Sector</field>
<field name="keyword">Software projects</field>
<field name="keyword"> risk management</field>
<field name="keyword"> project management</field>
<field name="keyword"> system development process</field>
<field name="keyword"> methodology.</field>
<field name="abstract">Controlling risk in software projects is a major contributor to successful outcomes, in both the private and public sectors. Few studies investigate risk management in the public sector. This paper reports a study of risk practices in government agencies in an Australian State. It finds that risk management practices lag behind project management practices and that there is scope for improvement in both. Ten major risk factors are identified. Also, some conventional conceptions of project and risk management are challenged. The paper concludes by discussing implications of the findings for research and practice in the public sector.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">322</field>
<field name="author">Paul Bannerman</field>
<field name="title">Toward An Integrated Framework of Software Project Threats</field>
<field name="keyword">Threat management</field>
<field name="keyword"> risk management</field>
<field name="keyword"> issue management</field>
<field name="keyword"> crisis management</field>
<field name="keyword"> software projects.</field>
<field name="abstract">This paper proposes an integrated threat management framework to improve outcomes of software projects. Current best practice prescribes risk management and issue management to control threats. However, these processes cover only part of the spectrum of uncertainty. A broader framework of threat management is proposed that integrates issue management, risk management and crisis management. Case examples and steps to transition beyond risk management are also provided. Implications for research and practice are discussed. The framework provides a basis to extend efforts in research and practice to improve the capability of organizations to manage uncertainty and improve project results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">323</field>
<field name="author">Paul Bannerman</field>
<field name="title">Macro-Processes Informing Micro-Processes: The Case of Software Project Performance</field>
<field name="keyword">Software process</field>
<field name="keyword"> macro-process</field>
<field name="keyword"> capabilities</field>
<field name="keyword"> project performance</field>
<field name="abstract">This paper explores the operational context of software processes and how it can inform the micro-process level environment. It examines the case of software project performance, describing a novel explanation. Drawing on the management literature, project performance is modeled as the contested outcome of learning as a driver of success and certain barrier conditions as drivers of failure, through their effects on organizational capabilities. A case study illustrates application of the theory. Implications of this macro-process case for micro level software process improvement are discussed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">324</field>
<field name="author">Paul Bannerman</field>
<field name="title">Capturing Business Benefits from Process Improvement: Four Fallacies</field>
<field name="abstract">A basic assumption underlying any process improvement initiative is that it will have a positive impact on the organization. Therefore, it can become easy to assume that process change will in fact deliver benefits to business. This paper takes a practice-based look at some fundamental assumptions about process improvement that can be as fallacious as they can be true. The argument is supported by case scenarios. Also, some ways are suggested to manage around these fallacies to achieve net benefits rather than no impact or negative impacts on the business. Four basic fallacies are considered: that process improvement leads to business improvement; that process change equates to process improvement; that software processes are non-lethal; and the vision of the enterprise as an automated process. The paper concludes that the future success of process improvement as a management strategy is dependent upon the capability of organizations to capture material gains.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">325</field>
<field name="author">Paul Bannerman</field>
<field name="title">Smoothing Innovation Discontinuities</field>
<field name="keyword">Management</field>
<field name="keyword"> technological innovation</field>
<field name="keyword"> organizational capabilities</field>
<field name="keyword"> liability of newness.</field>
<field name="abstract">This conceptual paper examines the impacts of technological discontinuities from a competence perspective. It proposes a composite model of positive and negative effects on innovation competencies, illustrated with examples of information technology innovation. It concludes that management strategies may be enacted to position a firm to mitigate many effects of disruptive innovation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">326</field>
<field name="author">Paul Bannerman</field>
<field name="title">Risk and Risk Management in Software Projects: A Reassessment</field>
<field name="keyword">Software projects</field>
<field name="keyword"> risk management</field>
<field name="keyword"> project management</field>
<field name="keyword"> threat management</field>
<field name="abstract">Controlling risk in software projects is considered to be a major contributor to project success. This paper reconsiders the status of risk and risk management in the literature and practice. The analysis is supported by a study of risk practices in government agencies in an Australian State, contributing to a gap in research in the public sector. It is found that risk is narrowly conceived in research, and risk management is under-performed in practice. The findings challenge some conventional conceptions of risk management and project management. For example, it was found that software projects do not conform to a uniform structure, as assumed in much of the literature. This introduces variations in the risk and project management challenges they face. Findings also suggest that formal project management is neither necessary nor sufficient for project success. It is concluded that risk management research lags the needs of practice, and risk management as practiced lags the prescriptions of research. Implications and directions for future research and practice are discussed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">327</field>
<field name="author">Paul Bannerman</field>
<field name="title">Risk Factors in NSW Government Agency</field>
<field name="abstract">Executive Summary



NSW Government agencies have a proud tradition in effective ICT adoption, which is supported by the projects in this study. However, while we are getting better at managing projects, outcomes can be improved through better risk management. The identification and control of risks has a critical contribution to make in delivering successful projects.



Three major findings emerged from risk management practices in the projects studied:

1.	there is a need to improve our understanding and management of software project risk

2.	we tend to build risks into the design of our software projects

3.	we need to develop risk management capabilities as well as better engineering solutions



The research study examined 17 software projects and 17 agencies. While project management tended to be driven by a formal methodology, risk management was not. Furthermore, a review of research and practice literature on risk management indicated that our conception of software project risk has largely stalled.



The study found ten groups of risk factors that are especially relevant to agency software projects. These risk factors point to the importance of:

 	having an effective project governance framework

 	matching methods to the needs of each project during set up

 	engaging third-party partners in a way that optimises their contributions

 	having strong business proprietorship

 	building individual and organisational capabilities in project management

 	concurrently managing the organisational impacts of ICT-enabled change

 	viewing software projects as a management as well as engineering activity

 	recognising and treating agency- and project-specific red flags 

 	developing real-time risk management capabilities as well as processes

 	seeking and capturing benefits from software projects



In response to these findings, five management guidelines and ten process guidelines are outlined to improve software project risk management and performance in agencies. A simple scorecard is also suggested for getting a quick assessment of the overall health of a software project at any time throughout its life cycle.



Finally, opportunities created by the study for further collaboration to improve risk management and project performance are outlined.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">328</field>
<field name="author">Paul Bannerman</field>
<field name="title">Defining Project Success: A Multi-level Framework</field>
<field name="abstract">This paper proposes a multi-level framework for defining project success that had wide application in practice. There has been much discussion in the literature on the definition of project success but no consensus has emerged. A key problem to be overcome is a multiplicity of expectations of projects and perceptions of their performance. A reference framework for project success would support development of the discipline by providing a common language for communication and comparison as well as facilitating focus on what stakeholders believe is important. The framework is developed from a synthesis of the literature and related stakeholder interests. The framework comprises five levels of success criteria: process success, project management success, product success, business success and strategic success. Project success is defined as the highest level achieved at any point of assessment regardless of performance at lower levels. Application of the framework is illustrated with case examples of information systems projects but the framework is applicable to any discipline.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">329</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">A note on image restoration using Cp and MSE</field>
<field name="keyword">Image restoration</field>
<field name="keyword"> regularization</field>
<field name="keyword"> mean square error</field>
<field name="keyword"> Cp</field>
<field name="abstract">Image restoration necessitates the choice of a regularization

parameter that controls the trade-off between fidelity to the

blurred noisy observed image and the smoothness of the restored

image. The choice of this parameter for which several estimators

have been proposed is crucial for the quality of the restored

image. In this letter, two estimators for choosing the

regularization parameter are proposed. One is a simple closed form

approximation to the minimum of the Cp selection criterion

and the other is an approximation to the minimum of a mean squared

error MSE based criterion.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">330</field>
<field name="author">Martin</field>
<field name="author">Abd-Krim Kleinsteuber</field>
<field name="author">Seghouane</field>
<field name="title">On the Deterministic CRB for DOA Estimation in Unknown Noise fields Using Sparse Sensor Arrays</field>
<field name="keyword">Cramer-Rao bound</field>
<field name="keyword"> DOA estimation</field>
<field name="keyword"> maximum likelihood</field>
<field name="keyword"> differential geometry</field>
<field name="abstract">The Cramer-Rao bound CRB plays an important role in DOA

estimation because it is always used as a benchmark for comparison

of the different proposed estimation algorithms. In this paper,

using well-known techniques of global analysis and differential

geometry, four necessary conditions for the maximum of the

log-likelihood function are derived, two of which seem to be new.

The CRB is derived for the general class of sensor arrays composed

of multiple arbitrary widely separated subarrays in a concise way

via a coordinate free form of the Fisher Information. The result

derived in \cite{Vorobyov} is confirmed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">331</field>
<field name="author">Stephen Herborn</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Max Ott</field>
<field name="title">Predictive Context Aware Mobility Handling</field>
<field name="keyword">Mobility</field>
<field name="keyword"> Multi-homing</field>
<field name="keyword"> Context aware</field>
<field name="abstract">The handling of device multi-homing and mobility,

such as deciding which network interface to use or when to

perform vertical handoff between network interfaces, can be

greatly enhanced by considering recent context information.

We describe a system for context aware multi-homing and

mobility handling which enacts network interface allocation

and handoff decisions based on the predicted characteristics

of transport layer sockets and network interfaces. Predictions

are made using a statistical machine learning technique which

can utilise simple context data such as time-of-day and GPS

co-ordinates, as well as more complex contextual information

such as nearby Bluetooth beacons and internal system state. We

present a prototype implementation of the described system and

show via experimentation that it enables more timely mobility

handling without requiring changes to either applications or to

the underlying operating system.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">332</field>
<field name="author">Tomasz Kowalski</field>
<field name="author">John Slaney</field>
<field name="title">A Finite Fragment of S3</field>
<field name="abstract">It is shown that the pure (strict) implication fragment of the modal logic S3 has finitely many non-equivalent formulae in one variable. The exact number of such formulae is not known. We show that this finiteness result is the best possible, since the analogous fragment of S4, and therefore of S3, in two variables has infinitely many non-equivalent formulae.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">333</field>
<field name="author">Asif Ali</field>
<field name="author">John Slaney</field>
<field name="title">Counting Loops with the Inverse Property</field>
<field name="keyword">Loops</field>
<field name="keyword"> Inverse property</field>
<field name="abstract">The numbers of isomorphism classes of IP loops of order up to 13 have been obtained by exhaustive enumeration, and are presented here along with some basic observations concerning IP loops.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">334</field>
<field name="author">Silvia Richter</field>
<field name="author">Malte Helmert</field>
<field name="author">Matthias Westphal</field>
<field name="title">Landmarks Revisited</field>
<field name="abstract">Landmarks for propositional planning tasks are variable assignments

 that must occur at some point in every solution plan.

 We propose a novel approach for using landmarks in planning by

 deriving a pseudo-heuristic and combining it with other

 heuristics in a search framework. The incorporation of

 landmark information is shown to improve success rates and solution

 qualities of a heuristic planner. We furthermore show how additional

 landmarks and orderings can be found using the information present

 in multi-valued state variable representations of planning tasks.

 Compared to previously published approaches, our landmark

 extraction algorithm provides stronger guarantees of correctness for

 the generated landmark orderings, and our novel use of landmarks

 during search solves more planning tasks and delivers

 considerably better solutions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">335</field>
<field name="author">B. A. Kitchenham</field>
<field name="author">Hiyam Al Khilidar</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">Mike Berry</field>
<field name="author">Karl Cox</field>
<field name="author">Jacky Keung</field>
<field name="author">Felicia Kurniawati</field>
<field name="author">Mark Staples</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Liming Zhu</field>
<field name="title">Evaluating Guidelines for Empirical Software Engineering Studies</field>
<field name="abstract">Background Several researchers have criticized the standards of performing and reporting empirical studies in software engineering. In order to address this problem, Jedlitschka and Pfahl have produced reporting guidelines for controlled experiments in software engineering. They pointed out that their guidelines needed evaluation. We agree that guidelines need to be evaluated before they can be widely adopted.

Aim The aim of this paper is to present the method we used to evaluate the guidelines and report the results of our evaluation exercise. We suggest our evaluation process may be of more general use if reporting guidelines for other types of empirical study are developed.

Method We used a reading method inspired by perspective-based and checklist-based reviews to perform a theoretical evaluation of the guidelines. The perspectives used were: Researcher, Practitioner/Consultant, Meta-analyst, Replicator, Reviewer and Author. Apart from the Author perspective, the reviews were based on a set of questions derived by brainstorming. A separate review was performed for each perspective. The review using the Author perspective considered each section of the guidelines sequentially.

Results The reviews detected 44 issues where the guidelines would benefit from amendment or clarification and 8 defects.

Conclusions Reporting guidelines need to specify what information goes into what section and avoid excessive duplication. The current guidelines need to be revised and then subjected to further theoretical and empirical validation. Perspective-based checklists are a useful validation method but the practitioner/consultant perspective presents difficulties.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">336</field>
<field name="author">Timothy Bourke</field>
<field name="author">Arcot Sowmya</field>
<field name="title">Automatically transforming and relating Uppaal models of embedded systems</field>
<field name="keyword">Timed trace inclusion</field>
<field name="keyword"> Uppaal</field>
<field name="keyword"> Model transformation</field>
<field name="abstract">Relations between models are important for effective automatic validation,

for comparing implementations with specifications, and for increased

understanding of embedded systems designs. Timed automata may be used to

model a system at multiple levels of abstraction, and timed trace inclusion

is one way to relate the models.



&lt;p&gt;

It is known that a deterministic and tau-free timed automaton can be

transformed such that reachability analysis can decide timed trace inclusion

with another timed automaton. Performing the transformation manually is

tedious and error-prone. We have developed a tool that does it automatically

for a large subset of Uppaal models.



&lt;p&gt;

Certain features of the Uppaal modeling language, namely selection bindings

and channel arrays, complicate the transformation. We formalize these

features and extend the validation technique to incorporate them. We find it

impracticable to manipulate some forms of channel array subscripts, and some

combinations of selection bindings and universal quantifiers; doing so

either requires premature parameter instantiation or produces models that

Uppaal rejects.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">337</field>
<field name="author">Marius Portmann</field>
<field name="author">Asad Amir Pirzada</field>
<field name="title">Wireless Mesh Networks for Public Safety and Crisis Management Applications</field>
<field name="abstract">Wireless mesh networks (WMNs) are multihop wireless networks with self-healing and self-configuring capabilities. These features, plus the ability to provide wireless broadband connectivity at a comparatively low cost, make WMNs a promising technology for a wide range of applications. While discussing the suitability of WMN technology for public safety and crisis management communication, this article highlights its strengths and limitations and points to current and future research in this context.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">338</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Performance analysis of multi-radio AODV in hybrid wireless mesh networks</field>
<field name="keyword">Multi-radio</field>
<field name="keyword"> routing</field>
<field name="keyword"> wireless mesh network</field>
<field name="abstract">Wireless Mesh Networks (WMNs), based on commodity hardware, present a promising technology for a wide range of applications due to their self-configuring and self-healing capabilities, as well as their low equipment and deployment costs. One of the key challenges that WMN technology faces is the limited capacity and scalability due to co-channel interference, which is typical for multi-hop wireless networks. A simple and relatively low-cost approach to address this problem is the use of multiple wireless network interfaces (radios) per node. Operating the radios on distinct orthogonal channels permits effective use of the frequency spectrum, thereby, reducing interference and contention. In this paper, we evaluate the performance of the multi-radio Ad-hoc On-demand Distance Vector (AODV) routing protocol with a specific focus on hybrid WMNs. Our simulation results show that under high mobility and traffic load conditions, multi-radio AODV offers superior performance as compared to its single-radio counterpart. We believe that multi-radio AODV is a promising candidate for WMNs, which need to service a large number of mobile clients with low latency and high bandwidth requirements.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">339</field>
<field name="author">Stephan Mir</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">HOVER: Hybrid On-demand Distance Vector Routing for Wireless Mesh Networks</field>
<field name="keyword">Multi-radio</field>
<field name="keyword"> routing</field>
<field name="keyword"> mesh</field>
<field name="abstract">Hybrid Wireless Mesh Networks are a combination of mobile ad hoc networks and infrastructure wireless mesh networks, consisting of two types of nodes: mobile Mesh Clients and static Mesh Routers. Mesh Routers, which are typically equipped with multiple radios, provide a wireless multi-hop backhaul. The resource constrained Mesh Clients also participate in the routing and forwarding of packets to extend the reach of the network. Current ad-hoc routing protocols have been designed for relatively homogeneous networks and do not perform well in Hybrid Wireless Mesh Networks. In this paper, we present HOVER (Hybrid On-demand Distance Vector Routing), a modified version of the AODV routing protocol, that achieves significant performance improvements in terms of packet delivery and latency in Hybrid Wireless Mesh Networks. Our modifications include a link quality estimation technique based on HELLO packets, a new routing metric that differentiates between node types, and a channel selection scheme that minimises interference in multi-radio mesh networks. We present an evaluation of our improvements via extensive simulations. We further show the practicality of the protocol through prototype implementation and provide measurement results obtained from our test-bed.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">340</field>
<field name="author">Mark Reed</field>
<field name="author">Leif Hanlen</field>
<field name="author">Giovanni Corazza</field>
<field name="title">Return Link Code Acquisition for 1-D and 2-D DS-CDMA for High Capacity Multiuser Systems</field>
<field name="keyword">Synchronization</field>
<field name="keyword"> Code division multiple access</field>
<field name="keyword"> Cochannel Interference</field>
<field name="keyword"> Multiuser Channels</field>
<field name="keyword"> Antenna Arrays</field>
<field name="abstract">Acquisition of the code timing in a direct-sequence code-division multiple-access system at the base station must take place before signal detection and decoding is possible. Code acquisition under severe multiple access interference conditions with time varying codes makes the task even more difficult. Inefficient designs lead to large number of false alarms and/or missed detections. This requirement is needed for conventional single antenna (one dimensional) designs and also for multi-element antenna (two dimensional) designs. This paper details a powerful code acquisition technique for the uplink of direct-sequence code-division multiple-access systems under high loaded situations for both 1-D and 2-D schemes, where the number of users is greater than the processing gain. Under this high multiple access interference condition the DS-CDMA acquisition problem becomes very difficult and conventional search methods simply fail. The method discussed utilises soft data from a multiuser detector to reduce the interference received by the acquisition unit. Analytical performance is compared to simulation results in terms of the number of users, processing gain, interferer signal power, cancellation factor, antenna configuration, and noise variance. Numerical results validate performance under realistic conditions with amplitude, phase, and frequency impairments.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">341</field>
<field name="author">Oliver Nagy</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Performance analysis of a generic system model for uncoded IDMA using serial and parallel interference cancellation</field>
<field name="keyword">IDMA</field>
<field name="keyword"> Receiver</field>
<field name="abstract">This paper shows howto accurately describe a fully synchronised interleave division multiple access (IDMA) scheme without channel coding by a matrix model. This model allows the derivation of the optimal detector and provides additional insights into the IDMA principle, and we show that the matrices are structured and sparse.We use BER and EXIT charts to study the performance of parallel and serial interference cancellation schemes and demonstrate that the latter converges faster and is independent of the scrambling code. Any bit interleaved DS-CDMA system can be viewed as a special case of IDMA, and an IDMA receiver can therefore be used to detect DS-CDMA signals.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">342</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Bas Ploeger</field>
<field name="title">Five determinisation algorithms</field>
<field name="keyword">Finite automata</field>
<field name="keyword"> determinisation algorithms</field>
<field name="keyword"> memory usage</field>
<field name="keyword"> cellular automaton 110</field>
<field name="abstract">Determinisation of nondeterministic finite automata is a

well-studied problem that plays an important role in compiler theory

and system verification. In the latter field, one often encounters

automata consisting of millions or even billions of states. On such

input, the memory usage of analysis tools becomes the major

bottleneck. In this paper we present several determinisation

algorithms, all variants of the well-known subset construction, that

aim to reduce memory usage and produce smaller output automata. One

of them produces automata that are already minimal. We apply our

algorithms to determinise automata that describe the possible

sequences appearing after a fixed-length run of cellular automaton

110, and obtain a significant improvement in both memory and time

efficiency.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">343</field>
<field name="author">Oliver Nagy</field>
<field name="author">Leif Hanlen</field>
<field name="title">Capacity Bounds for LTI Channels</field>
<field name="abstract">Linear time invariant (LTI) channels are channels 

with memory and therefore introduce inter symbol interference 

(ISI), in general. The problem reduces to a single user AWGN 

channel, if the spectrum after the channel satisfies the Nyquist 

theorem and allows for ISI free communication. In the frequency 

domain, the relationship between source signal spectrum and 

received spectrum is straightforward, and it is but difficult to 

design a pulse shape with it. This approach, unlike other ones 

to bound the capacity of channels with memory, neither neglects 

the transients nor depends on data preambles (eg. OFDM).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">344</field>
<field name="author">Roy Timo</field>
<field name="author">Kim Blackmore</field>
<field name="author">Leif Hanlen</field>
<field name="title">On the Entropy Rate of Word-Valued Sources</field>
<field name="abstract">A word-valued source $\Y$ is a discrete finite alphabet random process which is created by encoding a discrete random process $\X$ with a symbol-to-word function $f$. In Information Theory (in particular source coding), it is of interest to know which word valued sources possess an entropy rate $\overline{H}(\Y)$. Nishiara and Morita showed that if $\X$ is independent and identically distributed and $f$ is prefix free, then $\overline{H}(\Y)$ exists and is equal to $\overline{H}(\X)$ divided the expected codeword length. This ``conservation of entropy" result was latter extended by Goto, Matsushima and Hirasawa to include stationary and ergodic $\X$. In this paper, we extend these results to ergodic and Asymptotically Mean Stationary (AMS) $\X$: If $\X$ is ergodic, then $\overline{H}(\Y)$ is equal to $\overline{H}(\X)$ divided by the expected codeword length; however, if $\X$ is Asymptotically Mean Stationary (AMS), then $\overline{H}(\Y)$ is equal to the expectation of the entropy rate of each stationary ergodic sub-source of $\X$ divided by the expected codeword length of that sub-source.



The second result in this paper solves an open problem concerning the existence of $\overline{H}(\Y)$ when $f$ is not prefix free. We show: If $\X$ is ergodic and $f$ is not prefix free, then $\overline{H}(\Y)$ exists and is upper bound by $\overline{H}(\X)$ divided by the expected codeword length. Or more generally, if $\X$ is Asymptotically Mean Stationary (AMS) and $f$ is not prefix free, then $\overline{H}(\Y)$ exists and is upper bound by the expectation of the entropy rate of each stationary ergodic sub-source of $\X$ divided by the expected codeword length of that sub-source.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">345</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Helmut Seidl</field>
<field name="title">Deciding Equivalence of Top-Down XML Transformations in Polynomial Time</field>
<field name="abstract">Many useful XML transformations can be formulated by

deterministic top-down tree transducers. A

canonical form for such transducers is presented which allows

to decide equivalence of their induced transformations

in polynomial time.

If the transducer is total, the canonical form

can be obtained in polynomial time as well.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">346</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Thomas Perst</field>
<field name="author">Helmut Seidl</field>
<field name="title">Exact XML Type Checking in Polynomial Time</field>
<field name="abstract">Stay macro tree transducers (smtts) are an expressive formalism for

reasoning about XSLT-like document transformations. Here, we consider

the exact type checking problem for smtts. While the problem is

decidable, the involved technique of inverse type inference is known

to have exponential worst-case complexity (already for top-down

transformations without parameters). We present a new \emph{adaptive}

type checking algorithm based on forward type inference through exact

characterizations of output languages. The new algorithm correctly

type-checks all call-by-value smtts. Given that the output type is

specified by a deterministic automaton, the algorithm is

\emph{polynomial-time} whenever the transducer uses only few parameters

and visits every input node only constantly often. Our new approach can

also be generalized from smtts to stay macro forest transducers which

additionally support concatenation as built-in output operation.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">347</field>
<field name="author">Damien Fisher</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Structural Selectivity Estimation for XML Documents</field>
<field name="abstract">Estimating the selectivity of queries is a crucial problem in

database systems. Virtually all database systems rely on the use of

selectivity estimates to choose amongst the many possible

execution plans for a particular query.

In terms of XML databases, the problem of selectivity estimation

of queries presents new challenges:

many evaluation operators are possible, such as

simple navigation, structural joins, or twig joins, and

many different indexes are possible.

%ranging from

%traditional B-trees to complicated XML-specific graph indexes.

A new synopsis for XML documents is introduced which can be effectively

used to estimate the selectivity of complex path queries.

The synopsis is based on a lossy compression of the document tree

that underlies the XML document, and can be computed in one pass from

the document. It has several advantages over existing

approaches: (1) it allows one to estimate the selectivity of queries

containing all XPath axes, including the order-sensitive ones,

(2) the estimator returns a range within which the actual

selectivity is guaranteed to lie, with the size of this range implicitly

providing a confidence measure of the estimate, and

(3) the synopsis can be incrementally updated to reflect changes

in the XML database.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">348</field>
<field name="author">Irini Fundulaki</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Formalizing XML access control for update operations</field>
<field name="abstract">Several languages have been proposed over the past years

which support the specification of access control on XML data.

Most of these languages consider read-access restrictions only and do

not deal with access rights for updates (such as add, delete, or modify

operations). Fine-grain XML update operations are subject to

current research. This paper proposes XUAC, a language for

specifying access control on XML data in the presence of update

operations. The update operations used in XUAC are based on

the W3C XQuery Update Facility working draft. A

formal access control model is defined which allows to study

properties of XUAC access policies. One essential

property is consistency: the policy should not allow the execution

of a sequence of updates which has the same total effect as an

update forbidden by the policy. Since XUAC is a rich

language with inherent ambiguities, checking consistency of a set of

XUAC rules is difficult, and undecidable in general.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">349</field>
<field name="author">Brian Anderson</field>
<field name="author">Ming Cao</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Sequential Localization of Sensor Networks</field>
<field name="abstract">The sensor network localization problem with distance information is to determine the positions of

all sensors in a network, given the positions of some of the sensors and the distances between some pairs

of sensors. A definition is given of what is meant for a sensor network in the plane to be sequentially

localizable. It is shown that the graph of a sequentially localizable network must have a bilateration

ordering and a polynomial time algorithm is given for deciding whether or not a network s graph has

such an ordering. A provably correct algorithm is given which consists of solving a sequence of quadratic

equations. It is shown that the algorithm can localize any localizable network in the plane whose graph

has a bilateration ordering.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">350</field>
<field name="author">Brian Anderson</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Agreeing Asynchronously</field>
<field name="keyword">Cooperative control</field>
<field name="keyword"> multiagent systems</field>
<field name="keyword"> asynchronous</field>
<field name="abstract">This paper formulates and solves a version of the

widely studied Vicsek consensus problem in which each member

of a group of n &gt; 1 agents independently updates its heading

at times determined by its own clock. It is not assumed that

the agents clocks are synchronized or that the event times

between which any one agent updates its heading are evenly

spaced. Nor is it assumed that heading updates must occur

instantaneously. Using the concept of analytic synchronization 

together with several key results concerned with properties of

 compositions of directed graphs, it is shown that the conditions

under which a consensus is achieved are essentially the same as

those applicable in the synchronous case provided the notion of an

agent s neighbor between its event times is appropriately defined.

However, in sharp contrast with the synchronous case where for

analysis an n dimensional state space model is adequate, for

the asynchronous version of the problem a 2n-dimensional state

space model is required. It is explained how to analyze this model

despite the fact that, unlike the synchronous case, the stochastic

matrices involved do not have all positive diagonal entries.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">351</field>
<field name="author">MIng Cao</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a Consensus in a Dynamically Changing Environment--A Graphical Approach</field>
<field name="keyword">cooperative control</field>
<field name="keyword"> graph theory</field>
<field name="keyword"> switched systems</field>
<field name="keyword"> multiagent systems</field>
<field name="abstract">This paper presents new graph-theoretic results appropriate for the analysis of a

variety of consensus problems cast in dynamically changing environments. The concepts of rooted,

strongly rooted, and neighbor-shared are defined, and conditions are derived for compositions of

sequences of directed graphs to be of these types. The graph of a stochastic matrix is defined, and

it is shown that under certain conditions the graph of a Sarymsakov matrix and a rooted graph

are one and the same. As an illustration of the use of the concepts developed in this paper, graphtheoretic

conditions are obtained which address the convergence question for the leaderless version

of the widely studied Vicsek consensus problem.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">352</field>
<field name="author">Ming Cao</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a consensus in a dynamically changing environment: convergence rates, measurement delays, and asynchronous events</field>
<field name="keyword">cooperative control</field>
<field name="keyword"> graph theory</field>
<field name="keyword"> switched systems</field>
<field name="keyword"> convergence rates</field>
<field name="keyword"> delays</field>
<field name="abstract">This paper uses recently established properties of compositions of directed graphs

together with results from the theory of nonhomogeneous Markov chains to derive worst case convergence

rates for the headings of a group of mobile autonomous agents which arise in connection with

the widely studied Vicsek consensus problem. The paper also uses graph-theoretic constructions to

solve modified versions of the Vicsek problem in which there are measurement delays, asynchronous

events, or a group leader. In all three cases the conditions under which consensus is achieved prove

to be almost the same as the conditions under which consensus is achieved in the synchronous,

delay-free, leaderless case.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">353</field>
<field name="author">Giorgio Busatto</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Efficient memory representation of XML document trees</field>
<field name="keyword">Tree grammar</field>
<field name="keyword"> compression</field>
<field name="keyword"> in-memory XML representation</field>
<field name="abstract">Implementations that load XML documents and give access to them via, e.g., the DOM, suffer from huge memory demands: the space needed to load an XML document is usually many times larger than the size of the document. A considerable amount of memory is needed to store the tree structure of the XML document. In this paper, a technique is presented that allows to represent the tree structure of an XML document in an efficient way. The representation exploits the high regularity in XML documents by compressing their tree structure; the latter means to detect and remove repetitions of tree patterns. Formally, context-free tree grammars that generate only a single tree are used for tree compression. The functionality of basic tree operations, like traversal along edges, is preserved under this compressed representation. This allows to directly execute queries (and in particular, bulk operations) without prior decompression. The complexity of certain computational problems like validation against XML types or testing equality is investigated for compressed input trees.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">354</field>
<field name="author">Scott Sanner</field>
<field name="author">Craig Boutilier</field>
<field name="title">Approximate Solution Techniques for Factored First-order MDPs</field>
<field name="keyword">Automated Planning</field>
<field name="keyword"> First-order Logic</field>
<field name="abstract">Most traditional approaches to probabilistic planning in relationally specified MDPs rely on grounding the problem w.r.t. specific domain instantiations, thereby incurring

a combinatorial blowup in the representation. An alternative approach is to lift a relational MDP to a first-order MDP (FOMDP) specification and develop solution

approaches that avoid grounding. Unfortunately, state-of-the-art FOMDPs are inadequate for specifying factored transition models or additive rewards that scale with the domain size structure that is very natural in probabilistic planning problems. To remedy these

deficiencies, we propose an extension of the FOMDP formalism known as a factored FOMDP and present generalizations of symbolic dynamic programming and linear-value approximation solutions to exploit its structure. Along the way, we also make contributions to

the field of first-order probabilistic inference (FOPI) by demonstrating novel first-order structures that can be exploited without domain grounding. We present empirical results to demonstrate that we can obtain solutions whose complexity scales polynomially in the logarithm of the domain size results that are impossible to obtain with any previously proposed solution method.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">355</field>
<field name="author">Joost Engelfriet</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Helmut Seidl</field>
<field name="title">Deciding Equivalence of Top-Down XML Transformations in Polynomial Time</field>
<field name="keyword">XML</field>
<field name="keyword"> top-down tree transducer</field>
<field name="keyword"> equivalence</field>
<field name="keyword"> minimization</field>
<field name="abstract">Many useful XML transformations can be expressed by

deterministic top-down tree transducers. A normal form

is presented for such transducers (extended with the facility

to inspect their input trees). A transducer in normal form

has a unique canonical form which can be obtained by

a minimization procedure, in polynomial time. Thus,

equivalence of transducers in normal form can be decided

in polynomial time.

If the transducer is total, the normal form

can be obtained in polynomial time as well.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">356</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="title">The Complexity of Tree Automata and XPath on Grammar-Compressed Trees</field>
<field name="keyword">tree automata</field>
<field name="keyword"> XPath</field>
<field name="keyword"> tree compression</field>
<field name="abstract">The complexity of various membership problems for tree automata

on compressed trees is analyzed.

Two compressed representations are considered: dags,

which allow to share identical subtrees in a tree, and straight-line

context-free tree grammars, which moreover

allow to share identical intermediate parts in a tree. Several

completeness results for the classes NL, P, and PSPACE are obtained.

Finally, the complexity of the evaluation problem for (structural)

XPath queries on trees that are compressed via straight-line

context-free tree grammars is investigated.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">357</field>
<field name="author">Javier Pereira</field>
<field name="author">Narciso Cerpa</field>
<field name="author">June Verner</field>
<field name="author">Mario Rivas</field>
<field name="author">J Drew Procaccino</field>
<field name="title">What do software practitioners really think about project success: A cross-cultural comparison</field>
<field name="abstract">Due to the increasing globalization of software development we are interested to discover if there exist significant cultural differences in practitioners definition of a successful project. This study presents the results of a survey in which Chilean software practitioners perceptions of software project success are compared with previous research with US practitioners. Responses from both groups of practitioners indicate that there is a relationship between team-work and success; our results also indicate similar perceptions related to the importance of job satisfaction and project success. However, Chilean responses suggest that if a practitioner is allowed too much freedom within the work environment, job stress results; this in turn is reflected in increasing demands for both job satisfaction and good environmental conditions. This may indicate the potential for the attribution of failure to conditions outside the team, thus preventing a search for problematic team issues and technical problems. In contrast, the data suggests peer control inside the US teams indicating a less stressful environment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">358</field>
<field name="author">Jenny Liu</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">Ian Gorton</field>
<field name="title">Middleware Architecture Evaluation for Dependable Self-managing Systems</field>
<field name="keyword">middleware</field>
<field name="keyword"> software architecture</field>
<field name="abstract">Middleware provides infrastructure support for creating dependable software systems. A specific middleware implementation plays a critical role in determining the quality attributes that satisfy a system s dependability requirements. Evaluating a middleware architecture at an early development stage can help to pinpoint critical architectural challenges and optimize design decisions. In this paper, we present a method and its application to evaluate middleware architectures, driven by emerging architecture patterns for developing self-managing systems. Our approach focuses on two key attributes of dependability, reliability and maintainability by means of fault tolerance and fault prevention. We identify the architectural design patterns necessary to build an adaptive self-managing architecture that is capable of preventing or recovering from failures. These architectural patterns and their impacts on quality attributes create the context for middleware evaluation. Our approach is demonstrated by an example application -- failover control of a financial application on an enterprise service bus.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">359</field>
<field name="author">Mahmoud Niazi</field>
<field name="author">Karl Cox</field>
<field name="author">June Verner</field>
<field name="title">A measurement framework for assessing the maturity of requirements engineering process</field>
<field name="keyword">Process maturity </field>
<field name="keyword"> Process improvement </field>
<field name="keyword"> Requirements engineering</field>
<field name="abstract">Because requirements engineering (RE) problems are widely acknowledged as having a major impact on the effectiveness of the software development process, Sommerville et al. have developed a requirements maturity model. However, research has shown that the measurement process within Sommerville s model is ambiguous, and implementation of his requirements maturity model leads to confusion. Hence, the objective of our research is to propose a new RE maturity measurement framework (REMMF) based on Sommerville s model and to provide initial validation of REMMF. The main purpose of proposing REMMF is to allow us to more effectively measure the maturity of the RE processes being used within organisations and to assist practitioners in measuring the maturity of their RE processes. In order to evaluate REMMF, two organisations implemented the measurement framework within their IT divisions, provided us with an assessment of their requirements process and gave feedback on the REMMF measurement process. The results show that our measurement framework is clear, easy to use and provides an entry point through which the practitioners can effectively judge the strengths and weakness of their RE processes. When an organisation knows where it is, it can more effectively plan for improvement.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">360</field>
<field name="author">Martin P. Lawitzky</field>
<field name="author">Dave Snowdon</field>
<field name="author">Stefan M. Petters</field>
<field name="title">Integrating Real Time and Power Management in a Real System</field>
<field name="keyword">real time</field>
<field name="keyword"> DVFS</field>
<field name="keyword"> power management</field>
<field name="keyword"> experience report</field>
<field name="keyword"> RBED</field>
<field name="abstract">Deploying dynamic voltage and frequency scaling (DVFS) techniques in a real-time context has generated some interest in recent years. However, most of this work is based on highly simplifying assumptions regarding the cost and benefit of frequency scaling. We have integrated a measurement-based DVFS technique with an EDF based scheduling framework. This enables the use of the dynamic slack caused by the variability of execution time, to reduce energy consumption and thus extend battery life or reduce thermal load. We have tested the approach using hardware instrumentation on a real system. This paper describes not only the theoretical basis for the work, but also our experiences with DVFS when confronted with physical reality.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">361</field>
<field name="author">Getian Ye</field>
<field name="author">Wang Yang</field>
<field name="author">Jie Xu</field>
<field name="author">Gunawan Herman</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">A Practical Approach to Multiple High-resolution Sprite Generation</field>
<field name="keyword">high resolution</field>
<field name="keyword"> mosaic</field>
<field name="keyword"> sprite coding</field>
<field name="keyword"> globale motion estimation</field>
<field name="abstract">The MPEG-4 video coding standard introduces a novel concept of sprite or mosaic that is a large image composed of pixels belonging to a video object visible throughout a video segment. The sprite captures spatio-temporal information in a very compact way and makes it possible for efficient object-based video compression. In this paper, we propose a practical approach to generating multiple super-resolution sprites for sprite coding. In order to construct super-resolution sprites and reduce coding cost, we firstly partition a video sequence into multiple independent sprites and group the images covering a similar scene into the same sprite. We then propose efficient and practical algorithms for cumulative global motion estimation and super-resolution sprite construction. Experiments with real video sequences show that the proposed approach outperforms the previous single sprite and multiple sprite techniques.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">362</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Ursula Goltz</field>
<field name="author">Jens-Wolfhard Schicke</field>
<field name="title">Symmetric and Asymmetric Asynchronous Interaction</field>
<field name="keyword">reactive systems</field>
<field name="keyword"> Petri nets</field>
<field name="keyword"> distributed systems</field>
<field name="keyword"> asynchronous interaction</field>
<field name="keyword"> equivalence notions</field>
<field name="abstract">We investigate classes of systems based on different interaction patterns

 with the aim of achieving distributability. As our system model we use Petri

 nets. In Petri nets, an inherent concept of simultaneity is built in, since

 when a transition has more than one preplace, it can be crucial that tokens

 are removed instantaneously. When modelling a system which is intended to be

 implemented in a distributed way by a Petri net, this built-in concept of

 synchronous interaction may be problematic. To investigate the problem we

 assume that removing tokens from places can no longer be considered as

 instantaneous. We model this by inserting silent (unobservable) transitions

 between transitions and their preplaces. We investigate three different

 patterns for modelling this type of asynchronous interaction. &lt;I&gt;Fully

 symmetric asynchrony&lt;/I&gt; assumes that every removal of a token from a place is

 time consuming. For &lt;I&gt;symmetric asynchrony&lt;/I&gt;, tokens are only removed

 slowly in case of backward branched transitions, hence where the concept of

 simultaneous removal actually occurs. Finally we consider a more intricate

 pattern by allowing to remove tokens from preplaces of backward branched

 transitions asynchronously in sequence (&lt;I&gt;asymmetric asynchrony&lt;/I&gt;). 



 We investigate the effect of these different transformations of instantaneous

 interaction into asynchronous interaction patterns by comparing the

 behaviours of nets before and after insertion of the silent transitions. We

 exhibit for which classes of Petri nets we obtain equivalent behaviour with

 respect to failures equivalence. 

 

 It turns out that the resulting hierarchy of Petri net classes can be

 described by semi-structural properties. In case of fully symmetric

 asynchrony and symmetric asynchrony, we obtain precise characterisations;

 for asymmetric asynchrony we obtain lower and upper bounds. 

 

 We briefly comment on possible applications of our results to Message

 Sequence Charts.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">363</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="author">Vladimir Tosic</field>
<field name="title">On Creating Industry-Wide Reference Architectures</field>
<field name="abstract">Many industries have been developing e-business stan-dards to improve business-to-business interoperability on a mass scale. Most e-business standards have been composed of business data models with some message exchange pat-terns. Such data-only standards leave a very large interpre-tation space for the implementation stage at each individ-ual organization. Thus, true industry-wide interoperability is still hard to achieve with such standards. In this industry report, we describe our experiences in creating and evalu-ating reference architectures for the Australian lending industry. Our reference architectures are deliberately non-structural, in order to achieve the right level of prescrip-tiveness. Instead, they are based on a set of quality-centric architectural rules. We devised new methods for analyzing interoperability and evaluating such industry-level refer-ence architectures. The first reference architecture has now been adopted and achieved positive effects. We also sum-marize several lessons learnt, related to the need for going beyond data format standardization, alignment of reference architectures with industry structures, and new enterprise computing techniques for ultra-large-scale systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">364</field>
<field name="author">Liming Zhu</field>
<field name="author">Thong Nguyen</field>
<field name="author">Tan Doan</field>
<field name="author">Jenny Liu</field>
<field name="title">State of the Art: Model Driven Development for Technology Evaluation</field>
<field name="abstract">Model Driven Development (MDD) refers to the methodical use of models in various software engineering activities. MDD has been mainly used to guide development through modelling and code generation. However, features of MDD can help system evaluation and acquisition as well. DSTO, which is responsible for technically evaluating ADF acquisitions, is considering integrating MDD into their evaluation activities. This report critically reviews the state-of-the-art of MDD in the context of evaluating quality (non-functional properties) of systems and advising on risk assessment and mitigation. We propose some improvements over the existing MDD approaches for acquisition evaluation. We provide some recommendations on tooling. In addition to the benefits to DSTO, this work also makes important scientific advances in the area of MDD.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">365</field>
<field name="author">Dhammika Elkaduwe</field>
<field name="author">Philip Derrin</field>
<field name="author">Kevin Elphinstone</field>
<field name="title">Kernel design for isolation and assurance of physical memory</field>
<field name="keyword">isolation</field>
<field name="keyword"> seL4</field>
<field name="keyword"> memory management</field>
<field name="keyword"> embedded systems</field>
<field name="keyword"> microkernels</field>
<field name="abstract">Embedded systems are evolving into increasingly complex software systems. One approach to managing this software complexity is to divide the system into smaller, tractable components and provide strong isolation guarantees between them. This paper focuses on one aspect of the system s behaviour that is critical to any such guarantee: management of physical memory resources.



We present the design of a kernel that has formally demonstrated the ability to make strong isolation guarantees of physical memory. We also present the macro-level performance characteristics of a kernel implementing the proposed design.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">366</field>
<field name="author">Leif Hanlen</field>
<field name="author">Alex Grant</field>
<field name="title">Capacity Analysis of Correlated MIMO Channels</field>
<field name="abstract">The capacity of correlated finite-dimensions MIMO channels, where the channel gains have a generalized Wishart distribution is found. Asymptotic expressions are given where one dimension is much larger than the other. For many transmitters, the asymptotic capacity can be divided into two components: one arising from the dominant eigenvalues of the correlation matrix, and the other from the remaining eigenvalues.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">367</field>
<field name="author">Leif Hanlen</field>
<field name="author">Alex Grant</field>
<field name="author">rod kennedy</field>
<field name="title">On the Capacity of Operator Channels</field>
<field name="abstract">We examine linear, bounded, fixed operators as channels between continuous input functions and continuous output functions. The time-frequency waveform channel and continuous space channel are both related to an abstract operator viewpoint of communication channels. The properties of the operator required in order to allow valid capacity estimation are examined, and the modelling of noise in a continuous setting is discussed. We outline the procedure for calculating the information theoretic capacity for this restricted class of operator 

channels and provide an example of operator channels in the form of a power constrained input signal.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">368</field>
<field name="author">Leif Hanlen</field>
<field name="author">Alex Grant</field>
<field name="author">Rod Kennedy</field>
<field name="title">On Capacity for Single-Frequency Spatial Channels</field>
<field name="abstract">We present information theoretic capacity results for information transfer in space, under the general assumption that the receiver applies a spatial filter to receive signals. Our work presents novel applications of linear, bounded, invariant operators to communication channels. We outline the procedure for calculating the information theoretic capacity for this restricted class of operator channels and apply this result to wireless information transfer.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">369</field>
<field name="author">Leif Hanlen</field>
<field name="author">Minyue Fu</field>
<field name="title">On point-wise models for MIMO systems</field>
<field name="abstract">We consider the standard (point-wise) linear channel model for MIMO wireless systems in terms of a continuous operator channel. We show analytically that the pointwise representation over-estimates the (true) modal connection strengths and produces artificially distorted channel singular values. Analytic results are compared with simulations for simple channel models and the convergence of the point-wise model toward the continuous model is shown.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">370</field>
<field name="author">Glenn Dickins</field>
<field name="author">Leif Hanlen</field>
<field name="title">Fast Calculation of Singular Values for MIMO Wireless Systems</field>
<field name="abstract">The standard (point-wise) linear channel model for MIMO wireless systems provides a simplistic mapping from antenna elements to continuous (operator) view point of wireless channels. Low-rank, high-dimension sampling matrices generated by Ray-Tracing may be used to estimate (with error) the true operator channel. In order to achieve reasonable estimation error bounds, intractably large dimension matrices must be used for Ray Tracing. We consider an algorithm for estimating the singular values of the large dimensional matrix via application of Power Factorisation. We show there is no preferential basis choice for preconditioning and provide a simple algorithm for high-speed evaluation of the dominant singular values.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">371</field>
<field name="author">Glenn Dickins</field>
<field name="author">Micheal Williams</field>
<field name="author">Leif Hanlen</field>
<field name="title">On the Dimensionality of Spatial Fields with Restricted Angle of Arrival</field>
<field name="abstract">Wireless communication occurs through continuous fields, over regions of space. Although the communication channel may be modeled in terms of infinite dimensional vector spaces, it is paramount to develop finite dimensional approximations for this channel. We extend previous work which examined the fundamental finite dimensionality of fields over a finite region of 

space to incorporate restricted angles of signal arrival. By considering a subspace of fields where direction of arrival of the field components, or source spatial distribution, is restricted we show the received field dimensionality is linearly related to the product of the radius of the region of interest and the angular restriction. This result provides a rigourous foundation for analysing the performance and capacity of MIMO systems in non isotropic environments. The proof is presented for the two dimensional case. It is apparent that this work can be extended to the case of three dimensions for a similar result.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">372</field>
<field name="author">Roy Timo</field>
<field name="author">Kim Blackmore</field>
<field name="author">Leif Hanlen</field>
<field name="title">On entropy measures for dynamic network topologies: Limits to MANET</field>
<field name="abstract">What are the fundamental limits on the communications potential of wireless networks? We contend that quantifying topological dynamics resulting from node movement enables one 

to: find the minimum overhead required by the network to maintain connectivity and, calculate the communication potential of the network. A mobility metric is proposed for the unbiased comparison of networks. This is an entropy measure based on the uncertainty of change in the topology of the network, and is referred to as topological uncertainty. Topological uncertainty determines the minimum overhead required by the network to correctly identify the topology and hence, provide node connectivity. We use topological uncertainty to derive fundamental bounds on the maximum bit rate available within a Mobile Ad-Hoc Networking environment. Our work demonstrates the potential of entropy measures to describe the complexities of node connectivity within wireless networks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">373</field>
<field name="author">Lei Qiu</field>
<field name="author">Danchi Jiang</field>
<field name="author">Leif Hanlen</field>
<field name="title">Neural Network Prediction of Radio Propagation</field>
<field name="abstract">Preliminary work for predicting signal distributions in a local area, using a feature-based neural network is presented. The neural network is trained by radio signal measurements at known positions. After appropriately setting the parameters of nodes of the neural network, a corresponding virtual propagation environment is built, which reasonably represents the actual 

environment. Radio signal strength distribution is predicted by the virtual environment. A new method for radio signal measurement is introduced which mitigates the effect of small scale fading when determining the fingerprint of a position.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">374</field>
<field name="author">Bishwarup Mondal</field>
<field name="author">Robert Jr Heath</field>
<field name="author">Leif Hanlen</field>
<field name="title">Quantization on the Grassmann Manifold: Applications to Precoded MIMO Wireless Systems</field>
<field name="abstract">This paper studies the problem of quantization of a source that lives on the complex Grassmann manifold. The special structure of the Grassmann manifold and the distortion measures that are 

de _ned on it differentiates this problem from the traditional problem of vector quantization in Euclidean spaces. Assuming a uniform source distribution along with a distortion based on chordal distance, codebook design algorithms are mentioned and rate distortion tradeoffs are studied. The expected distortion for such a quantizer is approximately characterized. These results are then applied to the performance analysis of a multiple antenna wireless 

communication system.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">375</field>
<field name="author">Leif Hanlen</field>
<field name="author">Alex Grant</field>
<field name="title">Optimal Transmit Covariance for MIMO Channels with Statistical Transmitter side Information</field>
<field name="abstract">We give an optimality condition for the input covariance for arbitrary ergodic Gaussian vector channels under the condition that the channel gains are independent of the transmit signal, the transmitter has knowledge of the channel gain probability law and the receiver has knowledge of each channel realization. Using this optimality condition, we find an iterative algorithm for numerical computation of optimal input covariance matrices.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">376</field>
<field name="author">Leif Hanlen</field>
<field name="author">Alex Grant</field>
<field name="title">On Capacity of Ergodic Multiple-Input Multiple-Output Channels</field>
<field name="abstract">Capacity results for Gaussian matrix channels are investigated where the receiver has knowledge of the channel realization and the transmitter has knowledge only of the channel statistics. We extend beamforming results and examine arbitrary ergodic random vector channels, under the condition that the transmit covariance is independent of the channel.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">377</field>
<field name="author">Glenn Dickins</field>
<field name="author">Leif Hanlen</field>
<field name="title">On Finite Dimensional Approximation in MIMO</field>
<field name="abstract">We consider measures of richness in multipath wireless channels. We examine bounds on the dimensionality of spatially constrained wireless signals. We show that the continuous system-model bounds on dimensionality rely on an implicit model for noise. We consider dimensionality bounds where the noise model is made an explicit parameter. We show that the dimensionality of a spatially constrained field may be considered as a parameter for a virtual antenna model. We propose a simple antenna model which incorporates spatial dimensionality and relate this to the current point-like model used in MIMO channel models.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">378</field>
<field name="author">Leif Hanlen</field>
<field name="author">Minyue Fu</field>
<field name="title">Wireless Communications with Spatial Diversity: A Volumetric Model</field>
<field name="abstract">his paper presents a novel physical-modeling approach to wireless systems with multiple antennas. The fundamental problem of modeling the communication channel is studied, where the channel consists of a finite spatial volume for transmitting, a finite spatial volume for reception, and an arbitrary set of reflective-scattering bodies. The number of communication 

modes (or degrees of freedom) for such a system is calculated, using the procedure developed. We present a simple model for multipath channels, which allows insight into the development 

of a correlated multiple-input multiple-output (MIMO) channel model. In particular, the model is independent of transmitter and receiver elements and relies on the physical parameters of the channel involved. Our work explains which physical parameters determine the channel model and its channel capacity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">379</field>
<field name="author">Ram Somaraju</field>
<field name="author">Leif Hanlen</field>
<field name="title">Uncertainty Principles for Signal Concentrations</field>
<field name="abstract">Uncertainty Principles for concentration of signals into truncated subspaces are considered. The classic Uncertainty Principle is explored as a special case of a more general 

operator framework. The time-bandwidth concentration problem is shown as a similar special case. A spatial concentration of radio signals example is provided, and it is shown that an Uncertainty Principle exists for concentration of single-frequency signals for regions in space. We show that the uncertainty is related to the volumes of the spatial regions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">380</field>
<field name="author">Leif Hanlen</field>
<field name="author">Roy Timo</field>
<field name="author">Rasika Perera</field>
<field name="title">On Dimensionality for Sparse Multipath</field>
<field name="abstract">We give a 2WT style result for the degrees of freedom of multipath signals that pass through spatially limited (sparse) scattering environments. The dimensionality scales with the circumference of the scattering region, and the total communications path length. We provide a direct comparison to the time-frequency case, where space replaces time. This is a rigorous wavefield examination of previous heuristic geometric arguments.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">381</field>
<field name="author">Glenn Dickins</field>
<field name="author">Terrence Betlehem</field>
<field name="author">Leif Hanlen</field>
<field name="title">A stochastic MIMO model utilising spatial dimensionality and modes</field>
<field name="abstract">This paper presents an efficiently parametrised model for second-order-statistics dominated MIMO channels. Recently, new MIMO models have been developed to emulate the statistics of real measurements: (1) analytic models which parametrise the statistics of the channel gains, and (2) geometric models which interpret the channel as separate multi-paths. Unfortunately analytic models are tied to the measurement array geometry, while geometric models significantly increase model complexity. We present a new stochastic framework, based on 

a modal decomposition of the MIMO channel, which allows channel models for arbitrary array geometries from a single set of measured data. Such a framework yields simple MIMO 

models that efficiently parametrise the channel, with adjustable accuracy. Results show that the new models match the capacity of real and simulated data as well as similar models.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">382</field>
<field name="author">Roy Timo</field>
<field name="author">Leif Hanlen</field>
<field name="author">Kim Blackmore</field>
<field name="title">MANETs: Routing Overhead and Reliability</field>
<field name="abstract">Node mobility and physical channel effects cause the quality of links in wireless networks to fluctuate randomly. At the network layer, these changes are accommodated by the control information (overhead) of routing protocols. We provide lower bounds on the minimum average control overhead of deterministic routing protocols. When routing devices use noisy, out-of-date, or guess topology information, routing errors will occur. The number of routing 

errors experienced by well known routing schemes grows non-trivially with increased estimation noise. Route error growth is a novel characterization of protocol robustness for switched packet networks. This work motivates an information theoretic view of routing protocols.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">383</field>
<field name="author">Matt Ruan</field>
<field name="author">Leif Hanlen</field>
<field name="author">Mark Reed</field>
<field name="title">Spatially Interpolated Beamforming Using Discrete Prolate Spheroidal Sequences</field>
<field name="abstract">This paper presents a novel design method for a spatially interpolated beamformer. By separating the design of the physical antenna array and the digital signal processors, we re- 

move the restriction on the half-wavelength spacing requirement of a uniform linear array to achieve higher efficiency. To be more realistic, however, we fix the element positions 

regardless of the steering angles. The maximum energy concentration properties of discrete prolate spheroidal sequences are exploited to optimize the weighting coefficients and filtering window. Two examples are given to demonstrate the high flexibility, low computational complexity, and non compromised performance of the proposed beamformer.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">384</field>
<field name="author">Alex Grant</field>
<field name="author">Leif Hanlen</field>
<field name="title">Sub-optimal Power Allocation for MIMO Channels</field>
<field name="abstract">We consider t-input r-output Rayleigh fading channels with transmit-sided correlation, where the receiver knows the channel realizations, and the transmitter only knows the channel statistics. Using Lagrange duality, we develop an easily computable, tight upper bound on the loss in information rate due to the use of any given input covariance for this channel. 

This bound is applied to two simple transmission strategies. The first strategy is a reduced-rank uniform allocation, in which independent, equal power Gaussian symbols are transmitted on the t strongest eigenvectors of the transmit covariance matrix, where 0 __ __ 1 is chosen to optimize the resulting information rate. The second strategy is water-filling on the eigenvalues of the transmit covariance matrix. The upper bound on loss shows these strategies are nearly optimal for a wide range of signal to noise ratios and correlation scenarios.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">385</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="title">Novel Unitary Space-Time Signal Design</field>
<field name="abstract">This paper presents a novel unitary space-time signal design for two to six transmit antennas which is applicable to both coherent and differential space-time coding. In the context of differential modulation, this new space-time signal design demonstrates good bit-error performance when compared with previously designed codes both theoretically and by simulation. The design for two antennas is shown to subsume previous differential unitary space-time signal constellation with the property that the number of optimum codes scales with the size of the constellation. As part of analysis of this novel design an exact expression for the union bound on error probability is given for differential unitary space-time modulation. The calculation of the union bound on bit-error probability can also shown to be considerably simplified for these codes allowing for the simplified specification of larger constellations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">386</field>
<field name="author">David Smith</field>
<field name="title">Super-Orthogonal Co-ordinate Interleaved Orthogonal Designs</field>
<field name="abstract">A super-orthogonal co-ordinate interleaved orthogonal design (CIOD)

space-time block code is introduced for three and four transmit

antennas. Using unitary matrix transformations the constituent CIOD

is expanded, with a performance improvement in terms of $E_b/N_0$

and an increased code rate in MIMO channels, and only a moderate

increase in decoding complexity. Good performance is shown in

comparison to other codes of comparable decoding complexity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">387</field>
<field name="author">Roy Timo</field>
<field name="author">Leif Hanlen</field>
<field name="author">Kim Blackmore</field>
<field name="title">A Lower Bound on Network Layer Control Information</field>
<field name="abstract">Any asymptotic mean stationary mobility model generates, at the network layer, a route process that satisfies the Asymptotic Equipartition Partition Theorem of Information Theory. This permits an information theoretic lower bound for network layer control information. 

By recasting the mobility problem as a Dynamical System, we provide a unique and rigorous examination of mobility models and routing protocols. In particular, new results on stationary and ergodic properties of common mobility models via two useful generalizations are provided. The important concept perfect simulation for reliable, and repeatable, results is formalized. A fixed to variable length encoding lemma of independent interest for asymptotically mean stationary sources is developed. Finally, a lower bound on network layer control information is presented.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">388</field>
<field name="author">Roy Timo</field>
<field name="author">Leif Hanlen</field>
<field name="title">Intrinsic Capacity of Random Scattered Spatial Communication</field>
<field name="abstract">We consider information transfer under spatially constrained wireless communication through a scattered random environment. We show that the capacity of the channel is dominated by the free-space separation of the transmit and receive regions from the scattering environment. We show that the free-space separation produces a compact random channel operator whose nformation rate may be estimated via a water-pouring result. We apply our results to the dense-scattered spatial channel, to provide a novel upper-bound on the capacity of information transfer in a scattered environment.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">389</field>
<field name="author">Min Zhang</field>
<field name="author">Thushara D. Abhayapala</field>
<field name="author">Dhammika Jayalath</field>
<field name="author">David Smith</field>
<field name="author">Chandra Athaudage</field>
<field name="title">Multirate Space-Time-Frequency Linear Block Coding</field>
<field name="abstract">This paper presents a multirate space-time-frequency linear block coding scheme (STFBC) with full transmit diversity for a variety of transmission rates. The proposed multirate STFBC can achieve relatively smooth balance between the performance and the transmission rate for a given constellation size. Design of a space-time linear block coding (STBC) scheme is presented as a special case of the proposed multirate STFBC, which perform better than some of the existing STBCs. Moreover, optimized multirate STFBCs have also been compared with some of the existing STFBCs. Simulations results show that the design parameter has sufficient flexibility to achieve improved performance with reduced computational complexity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">390</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="title">New Group Product Differential Unitary Space-Time Codes with Simplified Design and Detection</field>
<field name="abstract">A new differential unitary space-time code is presented that is a

product of a group diagonal matrices of different orders and a

unitary Vandermonde matrix. Design is simplified through

properties of the union bound on block error probability for the

presented code. Detection is simplified via properties of the

diagonal matrices which form a cyclic group. Superior performance

to best known differential space-time codes is shown, particularly

for 3, 5 and 6 transmit antennas..</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">391</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="title">Differential Unitary Space-Time Signal Designs with Improved Performance using Parametric Equivalence</field>
<field name="abstract">We present two unitary space-time signal designs for two or more

transmit antennas. The first design is a reparametrization of

rotations applied to diagonal matrices to form unitary space-time

codes. The rotations are applied as matrices similar to

Alamouti-style block codes. The second design subsumes rotations

applied to diagonal matrices for two antennas. These designs are

applied in a non-coherent/differential framework. Optimization of

constellations is performed using the union bound on

block-error-probability, for which a new exact closed-form

expression is given. Both designs have superior block-error

performance over previous designs, and have similar performance to

each other.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">392</field>
<field name="author">Jussi Rintanen</field>
<field name="title">A new approach to planning in networks</field>
<field name="abstract">Control of networks like those for transportation, water distribution, power distribution, communication to name a few, provides challenges to planning and scheduling. Many problems can be defined in terms of a basic state space model, but more general problems require an expressive language for talking about the topology and connectivity of the system, which are outside the scope of standard planning languages.



In this work we introduce a general framework for defining planning languages for networked systems, with capability to express properties of connectivity and topology of such systems. Even the most restricted languages we consider are not practically reducible to network-unaware languages such as STRIPS. However, we discuss conditions under which the planning problem is still solvable by using the same resource bounds as standard planning problems and methods very closely related to those for classical planning, thus providing an efficient and expressive approach to tackling networked planning problems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">393</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Regression for Classical and Nondeterministic Planning</field>
<field name="abstract">Many forms of reasoning about actions and planning can be reduced to regression, the computation of the weakest precondition a state has to satisfy to guarantee the satisfaction of another condition in the successor state. In this work we formalize a general syntactic regression operation for ground PDDL operators, show its correctness, and define a composition operation based on regression. As applications we present the computation of macro-actions, a powerful algorithm for computing invariants, and a generalization of the h^n heuristic of Haslum and Geffner.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">394</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Quasi-random Sampling Approach to Image Retrieval</field>
<field name="keyword">image retrieval</field>
<field name="keyword"> quasi-random sampling</field>
<field name="keyword"> EM-clustering</field>
<field name="abstract">In this paper, we present a novel approach to contents-based image retrieval. The method hinges in the use

of quasi-random sampling to retrieve those images in a database which are related to a query image provided by

the user. Departing from random sampling theory, we make use of the EM algorithm so as to organize the images in the database into compact clusters that can then be used for stratified random sampling. For the purposes of retrieval, we use the similarity between the query and the clustered images to govern the sampling process within clusters. In this way, the sampling can be viewed as a stratified sampling one which is random at the cluster level and takes into account the intra-cluster structure of the dataset. This approach leads to a measure of statistical confidence that relates to the theoretical hard-limit of the retrieval performance.

We show results on the Oxford Flowers dataset.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">395</field>
<field name="author">Jun Zhou</field>
<field name="author">Li Cheng</field>
<field name="author">Walter F. Bischof</field>
<field name="title">Prediction and Change Detection In Sequential Data for Interactive Applications</field>
<field name="keyword">human-computer interaction</field>
<field name="keyword"> machine learning</field>
<field name="keyword"> change detection</field>
<field name="keyword"> road tracking</field>
<field name="abstract">We consider the problems of sequential prediction and change detection that arise often in interactive applications: A semi-automatic predictor is applied to a time-series and is expected to make proper predictions and request new human input when change points are detected. Motivated by the Transductive Support Vector Machines (Vapnik 1998), we propose an online framework that naturally addresses these problems in a unified manner. Our empirical study with a synthetic dataset and a road tracking dataset demonstrate the efficacy

of the proposed approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">396</field>
<field name="author">Patrik Haslum</field>
<field name="title">A New Approach To Tractable Planning</field>
<field name="abstract">We describe a restricted class of planning problems and polynomial time

membership and plan existence decision algorithms for this class. The

definition of the problem class is based on a graph representation of

planning problems, similar to Petri nets, and the use of a graph grammar

to characterise a subset of such graphs. Thus, testing membership in the

class is a graph parsing problem. The plan generation algorithm also

exploits this connection, making use of the parse tree. We show that the

new problem class is incomparable with, \textit{i.e.}, neither a subset

nor a superset of, previously known classes of tractable planning problems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">397</field>
<field name="author">Christian Bessiere</field>
<field name="author">Konstantinos Stergiou</field>
<field name="author">Toby Walsh</field>
<field name="title">Domain filtering consistencies for non-binary constraints</field>
<field name="keyword">Constraint satisfaction</field>
<field name="keyword"> constraint propagation</field>
<field name="keyword"> consistency techniques</field>
<field name="abstract">In non-binary CSPs the study of local consistencies that only

prune values from domains has so far been almost exclusively

limited to generalized arc consistency and its weaker versions.

This is in contrast with binary CSPs where numerous such domain

filtering consistencies have been proposed. In this paper we

present a detailed theoretical, algorithmic and empirical study of

domain filtering consistencies for non-binary problems. We study

three inverse consistencies that are inspired by corresponding

variable based inverse consistencies for binary problems. These

consistencies are stronger than generalized arc consistency, but

weaker than pairwise consistency, which is a strong consistency

that removes tuples from constraint relations. Among other

theoretical results, and contrary to expectations, we prove that

the inverse consistencies we study do not reduce to the variable

based definitions of their counterparts on binary constraints. We

propose a number of algorithms to achieve the three consistencies.

These algorithms have a time complexity better than the previous generic

algorithm for inverse consistencies. In fact, one of our

algorithms has a time complexity comparable to that for

generalized arc consistency despite performing more pruning. Our

experiments demonstrate that inverse consistencies are promising

as they can be more efficient than generalized arc consistency on

certain non-binary problems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">398</field>
<field name="author">Bee Bee Chua</field>
<field name="author">Danilo Valeros Bernardo</field>
<field name="author">June Verner</field>
<field name="title">Criteria for estimating for requirements change effort</field>
<field name="keyword">Requirements Changes</field>
<field name="keyword"> Change Request</field>
<field name="keyword"> Rework Effort</field>
<field name="abstract">IT practitioners realize that poor scheduling can cause project failure. Because schedule overruns may be caused by the effort involved in making requirement changes, a software process improvement challenge is to better estimate the cost and effort of requirements changes. Difficulties with such effort estimation is partially caused by a lack of data for analysis with little information about the data types involved in the requirements changes. This research is an exploratory study, based on change request forms, in requirements change categorization. This categorization can be used to develop an empirical model for requirements change effort as input into a cost estimation model. An empirically based estimation model will provide IT practitioners with a basis for better effort estimation of effort needed for requirements changes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">399</field>
<field name="author">Roy Timo</field>
<field name="author">Alex Grant</field>
<field name="author">Leif Hanlen</field>
<field name="title">Source Coding for a Noiseless Broadcast Channel with Partial Receiver Side-Information</field>
<field name="abstract">A transmitter communicates the outputs X, Y from a finite discrete memoryless source to two receivers via a noiseless broadcast channel. Each message is required at only one receiver, the receivers cannot cooperate, and one receiver has side information U . We show the achievable communication rates for this channel are R _ H (Y ) + H (X |Y , U ). Achievability is proved via a rate-split version of random binning. Cut-set outer bounds are not tight, so an alternate converse is presented. This problem is an example of a broadcast network, with m messages and m independent receivers each having different side-information. This broadcast network is motivated by a control information problem from mobile networking, and it generalizes the Wyner-Ziv, Heegard-Berger, and Sgarro problems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">400</field>
<field name="author">Leif Hanlen</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Bounds on Space-Time-Frequency Dimensionality</field>
<field name="abstract">We bound the number of electromagnetic signals which may be observed over a frequency range 2W for a time T within a region of space enclosed by a radius R. Our result implies that broadband fields in space cannot be arbitrarily complex: there is a finite amount of information which may be extracted from a region of space via electromagnetic radiation. Three-dimensional space allows a trade-off between large carrier frequency and bandwidth. We demonstrate applications in super-resolution and broadband communication.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">401</field>
<field name="author">Leif Hanlen</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Space-Time-Frequency Degrees of Freedom: Fundamental Limits for Spatial Information</field>
<field name="abstract">We bound the number of electromagnetic signals which may be observed over a frequency range [F 

 _ W, F + W ] a time interval [0, T ] within a sphere of radius R. We show that the such constrained signals may be represented by a series expansion whose terms are bounded exponentially to zero beyond a threshold. Our result implies there is a finite amount of information which may be extracted from a region of space via electromagnetic radiation.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">402</field>
<field name="author">Akramus Salehin</field>
<field name="author">Leif Hanlen</field>
<field name="title">Dimensionality for Two Ring Scattering Model</field>
<field name="abstract">We consider a two ring scattering environment as a continuous wireless channel. In our model, there is finite spatial volume for transmitting, receiving as well as finite spatial volumes of scatterers. The number of multipath channels for a two ring scattering model is developed as a numerical solution. An approximate analytic solution is given. The dimensionality of the channel is developed as a function of the physical parameters of the channel such as volume of transmitting or receiving region, together with the inner and outer radius of the scattering regions in a two ring model. The method introduced in this paper can be applied for different scenarios consisting of multiple scattering environments.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">403</field>
<field name="author">Rodney Kennedy</field>
<field name="author">Parastoo Sadeghi</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Haley Jones</field>
<field name="title">Intrinsic Limits of Dimensionality and Richness in Random Multipath Fields</field>
<field name="abstract">We study the dimensions or degrees of freedom of farfield multipath that is observed in a limited, source-free region of space. The multipath fields are studied as solutions to the wave equation in an infinite-dimensional vector space. We prove two universal upper bounds on the truncation error of fixed and random multipath fields. A direct consequence of the derived bounds is that both fixed and random multipath fields have an effective finite dimension. For a circular spatial 

region, we show that this finite dimension is proportional to the radius. We use the Karhunen- 

Loeve (KL) expansion of random multipath fields to quantify the notion of multipath richness. 

The multipath richness is defined as the number of significant eigenvalues in the KL expansion 

that achieves 99% of the total multipath energy. We prove a lower bound on the largest eigenvalue 

and use this bound to show that reducing the angular spread of multipath angular power spectrum 

reduces multipath richness. We also provide a numerical algorithm to find multipath eigenvalues, 

which unlike the Fredholm equation method, does not require selecting quadrature points.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">404</field>
<field name="author">Lei Qiu</field>
<field name="author">Rodney Kennedy</field>
<field name="title">Spatial Correlation based Degrees of Freedom of Multipath</field>
<field name="abstract">This paper derives a general upper bound to 

the degrees of freedom of multipath field based only on angular 

power spectrum limited measurements. The numerical results 

demonstrate that the low-order modal concentration of a wave 

field based on statistical measurement is proportional to richness 

of multipath of the wave field. This indicates that we can have 

a more compact dimensionality of a receiver region when the 

information is limited to the angular power spectrum with richer 

multipath.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">405</field>
<field name="author">Parastoo Sadeghi</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Rodney kennedy</field>
<field name="title">Intrinsic Finite Dimensionality of Random Multipath Fields</field>
<field name="abstract">We study the dimensions or degrees of freedom of random multipath 

 _elds in wireless communications. Random multipath _elds are pre- 

sented as solutions to the wave equation in an in _nite-dimensional 

vector space. We prove a universal bound for the dimension of ran- 

dom multipath _eld in the mean square error sense. The derived 

maximum dimension is directly proportional to the radius of the 

two-dimensional spatial region where the _eld is coupled to. Us- 

ing the Karhunen-Loeve expansion of multipath _elds, we prove 

that, among all random multipath _elds, isotropic random multipath 

achieves the maximum dimension bound. These results mathemat- 

ically quantify the imprecise notion of rich scattering that is often 

used in multiple-antenna communication theory and show that even 

the richest scatterer (isotropic) has a finite intrinsic dimension.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">406</field>
<field name="author">Parastoo Sadeghi</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Rodney Kennedy</field>
<field name="title">Directional Random Scattering MIMO Channels: Entropy Analysis and Capacity Optimization</field>
<field name="abstract">In this paper, we study the effect of directional ran- 

dom scattering on the capacity of multiple-input multiple-output 

(MIMO) systems. First, we use the spatial decomposition of the 

MIMO channel matrix to analyze the randomness (entropy) 

of directional scattering. The analysis shows that directional 

scatterers (with at least a null in the angular power spectrum) 

will no longer be random when the receiver observation radius 

is sufficiently large. Therefore, directional scattering limits the 

expected linear increase of MIMO capacity with increasing the 

number of antennas. Second, we consider the effect of receiver 

antenna arrangement (positions) on the capacity of MIMO 

systems. For any random scatterer with a given angular power 

spectrum, we show that it is possible to choose the receiver 

antenna arrangement with the optimum whitening of the MIMO 

channel matrix that, in turn, maximizes MIMO channel capacity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">407</field>
<field name="author">Matthew McKay</field>
<field name="author">Alex Grant</field>
<field name="author">Iain Collings</field>
<field name="title">Largest eigenvalue statistics of double-correlated complex Wishart matrices and MIMO-MRC</field>
<field name="abstract">This paper considers multiple-input multiple-output (MIMO) antenna systems employing transmit beamforming (BF) with maximum ratio combining (MRC) receivers. Rayleigh fading environments are considered, with both transmit and receive spatial correlation. Exact expressions are presented for the probability density function (p.d.f.) of the output signal-to-noise ratio (SNR), as well 

as the system outage probability. The results are based on efficient closed-form expressions which we derive for the p.d.f. and c.d.f. of the maximum eigenvalue of double-correlated complex Wishart matrices. The results are validated through comparison with Monte-Carlo simulations, and used to examine the effect of spatial correlation on the SNR p.d.f. and the outage probability.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">408</field>
<field name="author">Rasika Perera</field>
<field name="author">Kien Nguyen</field>
<field name="author">Tony Pollock</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Capacity of non-coherent Rayleigh fading MIMO channels</field>
<field name="abstract">This paper investigates the capacity of discrete time uncorrelated Rayleigh 

fading multiple input multiple output (MIMO) channels without channel state 

information (CSI) at neither the transmitter or the receiver. We prove that 

to achieve the capacity, the amplitude of the multiple input needs to have a 

discrete distribution with a finite number of mass points with one of them 

located at the origin. We show how to compute the capacity numerically in 

multi antenna configuration at any signal to noise ratio (SNR) with the discrete 

input using the Kuhn-Tucker condition for optimality. Furthermore, we show 

that at low SNR, the capacity with two mass points is optimal. As the number 

of receiver antennas increases, the maximum SNR at which two mass points 

are optimal decreases. Using this result we argue that on-o</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">409</field>
<field name="author">Rasika Perera</field>
<field name="author">Tony Pollock</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Non-coherent Rayleigh fading MIMO channels: Capacity and optimal input</field>
<field name="abstract">Information transfer over a discrete time uncorre- 

lated Rayleigh fading multiple input multiple output (MIMO) 

channel is considered, where neither the transmitter nor the 

receiver has the knowledge of the channel state information (CSI) 

except the fading statistics. We derive a capacity supremum 

with the receive antenna number at any signal to noise ratio 

(SNR) using Lagrange optimisation. We show that the asymptotic 

capacity is double logarithmic when the input power is large. We 

prove that to achieve the capacity, the amplitude of the multiple 

input needs to have a discrete distribution with a finite number 

of mass points, one of them necessarily located at the origin. We 

show how to compute the capacity numerically in multi-antenna 

configuration at any SNR with the discrete input using the Kuhn- 

Tucker condition for optimality. Furthermore, we show that the 

capacity with two mass points is optimal at low SNR signifying 

on-off keying. As the number of receive antennas increases, the 

maximum SNR at which two mass points are optimal decreases.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">410</field>
<field name="author">Rasika Perera</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Tony Pollock</field>
<field name="title">Gaussian inputs: Performance limits over non-coherent SISO and MIMO channels</field>
<field name="abstract">Performance limits of information transfer over a discrete time memoryless Rayleigh fading channel with 

neither the receiver nor the transmitter knowing the fading coefficients except its statistics is an important 

problem in information theory. We derive closed form expressions for the mutual information of single input 

single output (SISO) and multiple input multiple output (MIMO) Rayleigh fading channels for any antenna 

number at any signal to noise ratio (SNR). Using these expressions, we show that the maximum mutual 

information of non-coherent Rayleigh fading MIMO channels is achieved with a single transmitter and 

multiple receivers when the input distribution is Gaussian. We show that the addition of transmit antennas 

for a fixed number of receivers result in a reduction of mutual information. Furthermore, we argue that the 

mutual information is bounded by the SNR in both SISO and MIMO systems showing the sub-optimality 

of Gaussian signalling in non-coherent Rayleigh fading channels.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">411</field>
<field name="author">Lei Qiu</field>
<field name="author">Rodney Kennedy</field>
<field name="author">Terrence Betlehem</field>
<field name="title">Spatial degrees of freedom of correlated multipath</field>
<field name="abstract">In this paper we consider the spatial degrees of freedom in the context of a multi-antenna wireless communication system. We investigate how the degrees of freedom depend on important system parameters such as the spatial extent of a region containing the antennas and, more importantly, the angular correlation of multipath. These results naturally augment known results which show how the degrees of freedom are affected by multipath from a restricted range of angles. We clarify the distinction between the spatial degrees of freedom with respect to an orthonormal basis and the concept of richness of multipath which is related to the Karhenen Loeve expansions for a random multipath field.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">412</field>
<field name="author">Lei Qiu</field>
<field name="author">Rodney Kennedy</field>
<field name="title">Radio location using pattern matching techniques in fixed wireless communication networks</field>
<field name="abstract">In this paper we propose a practical radio location scheme for fixed wireless networks, relying on only one base station. We exploit the already available channel impulse response (CIR) of fixed wireless terminals as a location fingerprint in wireless localisation. We show and interpret why the localisation accuracy achieved by the support vector machine (SVM) has no significant improvement compared to general instance-based learning algorithms.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">413</field>
<field name="author">Roy Timo</field>
<field name="author">Kim Blackmore</field>
<field name="author">John Papandriopoulos</field>
<field name="title">Strong Stochastic Stability for Dynamic Source Routing</field>
<field name="abstract">A word-valued source Y is a discrete finite alphabet 

random process which is created by encoding a discrete random 

process X with a symbol-to-word function f . In Information The- 

ory (in particular source coding), it is of interest to know which 

word valued sources possess an entropy rate H (Y). Nishiara 

and Morita showed that if X is independent and identically 

distributed and f is prefix free, then H (Y) exists and is equal to 

H (X) divided the expected codeword length. This conservation 

of entropy result was latter extended by Goto, Matsushima and 

Hirasawa to include stationary and ergodic X. In this paper, 

we extend these results to ergodic and Asymptotically Mean 

Stationary (AMS) X: If X is ergodic, then H (Y) is equal to 

H (X) divided by the expected codeword length; however, if X 

is Asymptotically Mean Stationary (AMS), then H (Y) is equal 

to the expectation of the entropy rate of each stationary ergodic 

sub-source of X divided by the expected codeword length of that 

sub-source. 

The second result in this paper solves an open problem 

concerning the existence of H (Y) when f is not prefix free. 

We show: If X is ergodic and f is not prefix free, then H (Y) 

exists and is upper bound by H (X) divided by the expected 

codeword length. Or more generally, if X is Asymptotically Mean 

Stationary (AMS) and f is not prefix free, then H (Y) exists 

and is upper bound by the expectation of the entropy rate of 

each stationary ergodic sub-source of X divided by the expected codeword length of that sub-source.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">414</field>
<field name="author">Kevin Buchin</field>
<field name="author">Maike Buchin</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Jun Luo</field>
<field name="author">Martin Loffler</field>
<field name="title">Detecting Commuting Patterns by Clustering Subtrajectories</field>
<field name="abstract">In this paper we consider the problem of detecting commuting patterns in a trajectory. For this we search for similar subtrajectories. To measure spatial similarity we choose the Frechet distance and the discrete Frechet distance between subtrajectories, which are invariant under differences in speed.



We give several approximation algorithms, and also show that the problem of finding the `longest' subtrajectory cluster is as hard as MaxClique to compute and approximate.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">415</field>
<field name="author">Robby Goetschalckx</field>
<field name="author">Scott Sanner</field>
<field name="author">Kurt Driessens</field>
<field name="title">Reinforcement Learning with the Use of Costly Features</field>
<field name="keyword">Reinforcement Learning</field>
<field name="keyword"> Cost-sensitive Learning</field>
<field name="abstract">In many practical reinforcement learning problems, the state space is

too large to permit an exact representation of the value function,

much less the time required to compute it. In such cases, a common

solution approach is to compute an approximation of the value function

in terms of state features. However, relatively little attention has

been paid to the cost of computing these state features. For example,

search-based features may be useful for value prediction, but their

computational cost must be traded off with their impact on value

accuracy. To this end, we introduce a new cost-sensitive sparse

linear regression paradigm for value function approximation in

reinforcement learning where the learner is able to select only those

costly features that are sufficiently informative to justify their

computation. We illustrate the learning behavior of our approach

using a simple experimental domain that allows us to explore the

effects of a range of costs on the cost-performance trade-off.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">416</field>
<field name="author">Scott Sanner</field>
<field name="author">Craig Boutilier</field>
<field name="title">Practical Solution Techniques for First-order MDPs</field>
<field name="abstract">Most traditional solution approaches to relationally-specified decision-theoretic planning problems (e.g., those stated in PPDDL) ground the specification w.r.t. a specific instantiation of domain objects and apply a solution approach directly to the resulting ground Markov decision process (MDP). Unfortunately, the space and time complexity of these solution approaches scale linearly with the domain size in the best case and exponentially in the worst case. An alternate approach to grounding a relational planning problem is to lift it to a first-order MDP (FOMDP) specification. This FOMDP can then be solved directly, resulting in a domain-independent solution. However, such generality does not come without its own set of challenges and the purpose of this article is to explore practical solution techniques for solving FOMDPs. To demonstrate the efficacy of our proposed techniques, we present results of our first-order approximate linear programming (FOALP) planner on problems from the probabilistic track of the 5th International Planning Competition.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">417</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Improving Business Processes in the Australian Lending Industry</field>
<field name="keyword">business process modelling</field>
<field name="keyword"> service-oriented computing</field>
<field name="keyword"> software architecture</field>
<field name="abstract">The Lending Industry XML Initiative (LIXI) is a standardization body in the Australian lending industry. Its members are diverse companies participating in the complex business processes related to consumer lending for housing property, ranging from multinational banks through mortgage brokers, insurers, and regulatory authorities to 1-person property valuation companies. LIXI was formed to improve business process automation within the participating companies and interoperability within the industry by standardizing the vocabulary of lending in XML (Extensible Markup Language) formats. In spite of successes in the adoption of standards within the industry, the goal of complete and open interoperation has not been fully achieved. LIXI approached NICTA, an Australian research institute and centre of excellence in information and communications technology (ICT), for help in addressing this challenge, and the resulting work is the topic of this presentation. 

The initial goal of the project was to define reference business process models in the lending industry. We (the project members) have critically analysed applicability of the existing business process languages (e.g., the Business Process Modeling Notation BPMN) and tools for the needs of LIXI members. Further, we have developed a number of standard lending and valuation business processes (e.g., modelled in BPMN). Based on the success of this collaboration, the goals of the project were extended to develop additional technical solutions that support LIXI and its members in the quest for improvement of lending business processes and the interoperability of participants in the industry. Thus, the insights from LIXI business process modelling were used as a basis for identifying gaps in current process modelling languages and determining the desiderata for business process modelling beyond workflows. We have also explored and evaluated the applicability of advanced enterprise computing technologies such as service-oriented architectures (SOAs), REpresentational State Transfer (RESTful) services, Web service composition technologies (e.g., the Web Services Business Process Execution Language WS-BPEL and the message-driven Soya), and solutions for ultra-large scale systems (ULS) to LIXI business processes. This has resulted in development of reference architectures for interoperable implementation of business processes in property valuation and lending product information dissemination. These reference architectures are accompanied by corresponding reference implementations available to LIXI members. 

The feedback from LIXI and its members on these results has been very positive and some of the solutions developed in this project have been adopted in practice. The range and depth of the collaboration between LIXI and NICTA continues to grow. The goal of this project was to improve business processes in the Australian lending industry by developing better approaches for business process modelling and inventing new techniques for business process interoperation on a very large (industry-wide) scale. Many of the insights and results obtained within this project are transferable to other vertical domains. For example, we have started to engage with Australian government agencies on improvement of e-government business processes. The research results are the foundation of our ongoing work on advanced solutions for improved business process modelling, analysis, and execution.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">418</field>
<field name="author">Ko-Hsin Cindy Wang</field>
<field name="author">Adi Botea</field>
<field name="title">Fast and Memory-Efficient Multi-Agent Pathfinding</field>
<field name="keyword">multi-agent</field>
<field name="keyword"> path planning</field>
<field name="abstract">Multi-agent path planning has been shown to be a PSPACE-hard problem. Running a complete search such as A* at the global level is often intractable in practice, since both the number of states and the branching factor grow fast as the number of mobile units increases. In addition to the inherent difficulty of the problem, in many real-life applications such as computer games, solutions have to be computed in real time, using limited CPU and memory resources.



In this paper we introduce FAR, a method for multi-agent path planning on grid maps. When building a search graph from a grid map, FAR implements a flow restriction idea inspired from a real-life two-way road. The movement along a given row (or column) is restricted to only one direction, avoiding head-to-head collisions. The movement direction is flipped from one row (or column) to the next. Additional rules ensure that two locations reachable from each other on the original map remain connected (in both directions) in the graph. After building the search graph, an A* search is independently run for each mobile unit. During plan execution, deadlocks are detected as cycles of units that wait for each other to move. A heuristic procedure for deadlock breaking attempts to repair plans locally, instead of running a larger scale, more expensive replanning step.



Experiments are run on a collection of maps extracted from Baldur's Gate, a popular commercial computer game. We compare FAR with WHCA*, a recent successful algorithm for multi-agent path planning on grid maps. FAR is shown to run significantly faster, use much less memory, and scale up to problems with more mobile units.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">419</field>
<field name="author">A. Anbulagan</field>
<field name="author">Adi Botea</field>
<field name="title">Crossword Puzzles as a Constraint Problem</field>
<field name="keyword">Crossword puzzles</field>
<field name="keyword"> phase transition</field>
<field name="abstract">Despite being occasionally used as a benchmark in CSP research, no thorough study of crossword puzzles as a CP problem has been available in the past. In this paper we provide a detailed analysis of the crossword domain. We present new results in crossword composition, showing that our system significantly outperforms the previous successful techniques in the literature. We emphasize phase transition phenomena, identify classes of hard problems, and provide a way to compute the expected number of solutions. Phase transition is shown to occur when

varying problem parameters such as the dictionary size and the number of blocked cells on a grid. Unlike previous CSP contributions on phase transition, which always consider randomly generated problems, in our

work we experiment with large-size realistic problems. This study provides a better understanding of the domain properties and of the current state-of-the-art solving techniques, which makes crosswords a more attractive benchmark for testing new CSP algorithms.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">420</field>
<field name="author">John-Paul Kelly</field>
<field name="author">Adi Botea</field>
<field name="author">Sven Koenig</field>
<field name="title">Offline Planning with Hierarchical Task Networks in Video Games</field>
<field name="abstract">Artificial intelligence (AI) technology can have a dramatic impact on the quality of a video game.

AI planning methods are useful in a wide range of game components, including modules to control the behaviour of fully autonomous units. However, planning is computationally expensive and the CPU and memory resources available at runtime to a game AI module are scarce. Offline planning can be a good strategy to avoid a runtime performance bottleneck. In this work we apply planning with hierarchical task networks (HTNs) to video games. HTNs can speed up planning dramatically, since search is guided with human-encoded knowledge. We describe an architecture that computes plans offline. Plans are represented as game scripts. This can be seen as a form of generating scripts automatically, replacing the traditional approach of composing them by hand. We apply our ideas to The Elder Scrolls IV: Oblivion, a commercial game by Bethesda Softworks. The results are very encouraging. Scripts are automatically generated at a level of complexity that would require a great human effort to create.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">421</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Challenges and Recent Advances in QoS Provisioning in Wireless Mesh networks: A survey</field>
<field name="abstract">Wireless mesh networks (WMNs) comprising of mobile

and static nodes connected wirelessly are emerging as a

key technology for future generation of wireless

networks. WMNs self-organize, self-configure and selfheal

themselves and can increase the coverage of

conventional infrastructure-based wireless LANs and

MANs without significant additional infrastructure

deployments. Due to these unique features, WMNs are

being used in many applications ranging from emergency

response situations to wireless metropolitan area

networks. Quality of Service (QoS) provisioning in

WMNs is of utmost importance in order to support realtime

audio and video communications. However, QoS

provisioning in highly mobile wireless networks such as

WMNs is a very challenging problem compared to

provisioning of QoS in wired IP networks. The main

reasons for this are unpredictable node mobility, wireless

multi-hop communication, contention for wireless

channel access, limited battery power and wireless range

of mobile devices, as well as the absence of a central

coordination authority in WMNs. This paper describes

the challenges and the state of the art in provisioning of

QoS over WMNs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">422</field>
<field name="author">Peizhao Hu</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="title">Experimental Evaluation of AODV in a Hybrid Wireless Mesh Network</field>
<field name="keyword">Hybrid</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="keyword"> routing</field>
<field name="abstract">Wireless Mesh Networks (WMNs) are a variant of

ad-hoc networks that have recently received increasing attention.

They are a promising technology for a wide range of applications

for the reason of their cost-effectiveness, self-configuring and

self-healing nature. Routing protocols are a key component of

WMNs, as they are responsible for discovering and establishing

end-to-end, often multi-hop, paths between nodes in dynamic

environments. In this paper, we evaluate the performance of

the Ad-hoc On-demand Distance Vector (AODV) protocol for

hybrid WMNs, which are a special type of wireless mesh network

that are comprised of both static infrastructure nodes as well

as mobile client devices. The results obtained from our testbed

network indicate that AODV performs effectively in hybrid

WMNs and is able to handle a high volume of traffic with minimal

latency. While existing works on performance evaluation of adhoc

and WMN routing protocols are typically performed either

via simulations or via experiments on test-beds. Comparisons of

results from both methods are rarely provided. One of the key

contributions of this paper is the validation of our simulation

results by performing experiments on a real WMN test-bed

using the same scenario and mobility pattern. Our results show

a reasonably good correlation between the simulation results

and our test-bed measurements and we are able to explain the

observed variations. Our work serves as a base line result for

the exploration of larger and more complex network topologies.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">423</field>
<field name="author">Selwyn Russell</field>
<field name="title">Issues in Merging Internet Autonomous Systems for Emergency Communications</field>
<field name="abstract">The Internet has certain properties which make it the first choice for communi-

cations during an emergency. Experiences during the Kobe earthquake in 1995

indicated that the Internet was more resilient than other networks [1]. Fault

tolerance and fault recovery were basic DARPA requirements [2]. The lack of

a central controller or base station (as used in the two way radio systems of-

ten used by emergency handling teams) eliminates a potential critical point of

failure.

In a large developed area, there will be numerous TCP/IP networks installed:

fiber, copper, wireless, and cable. Internet connectivity is typically provided

through communications carriers which operate for profit. Over the past few

decades, governments have encouraged or caused, e.g. through privatisation of a government utility, increasing numbers of suppliers of communications services.



In a region, the larger utilities and enterprises provide their own networks, each individually registered with the Internet authorities as an Autonomous System (AS). These competing networks normally operate independently, with common gateway points where traffic is passed from one carrier to another. In a time of a large scale emergency in the area, these independent networks provide a basis for a fault tolerant network with good capacity and compatibility with many other networks and user devices. Even though a single network may not be fault tolerant, the diversity and duplication of links and nodes of the combined individual infrastructures are the correct components to form an adaptive network suitable for the situation. The difficulty of course is in merging the independent competing networks into a single collaborative network.



During a large scale emergency traffic behaves quite differently from normal

times. The emergency recovery team needs a communication network with ca-

pabilities and configurations which have a very different profile from everyday

communications networks, to avoid network overload from enquiry bursts and

to favour emergency management traffic. Ideally, this temporary network would

involve a merger of all commercial networks in the area which at other times

are competitive. Merging networks involves reconguration of signicant nodes

throughout the individual ASes. For large networks, care is needed to ensure inefficiencies and manual errors are not introduced, resulting in a single temporary AS which is inferior to the original arrangement.



To function as a single composite network, the routing tables of at least the

gateway routers need to be modified so the networks co-operate and share the

loads, and give priority to traffic to/from emergency services devices. 



Current router software is based on the assumption of a stable network with only small and incremental changes. During an emergency, speed and accuracy are vital, but at those times there is more chance of human error. To simplify communications management throughout a region, it is desirable to be able to describe the ideal network behaviour at that time in a high level policy style specification, and to

have the relevant network components automatically reconfigured in accordance

with the regional specification.



To accomplish this goal, management must be able to simply specify a complete and unambiguous high level policy for the communications network, and

accomplish this act in times of stress and time pressure with a minimum of delay.



An RFC on policies for routing first appeared in 1989 and languages for policy

speci cation were used in RIPE-81 in 1993 [3]. Further work has been done, e.g. [4], relating to the control of traffic through transit networks, but we conclude they are not the type of policies or policy description languages needed during an emergency.



As well as the technical di culties involved in merging networks, there are

numerous non-technical barriers, such as commercial secrets, privacy, legal liability, and policies. If the the ASes are highly competitive, commercially sensitive information on operational details might be difficult to obtain, and there is the risk of incompatible equipment, formats, databases or support systems.



In future work we will investigate in more detail requirements for solutions

and ways to meet them [5]</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">424</field>
<field name="author">Selwyn Russell</field>
<field name="author">Peter Croll</field>
<field name="title">A Project for the Synthesis of Composite TCP/IP Networks</field>
<field name="abstract">This paper describes a project being undertaken by National ICT Australia with a Safeguarding

Australia theme.

History shows that the pattern of use of communications during a large scale emergency to be quite

different from those of normal times. The emergency recovery team needs a communication network

with capabilities and configurations which have a very different profile from everyday communications

networks, to avoid network overload from enquiry bursts and to favour emergency management

traffic. Ideally, this temporary network would involve a unification of all commercial networks which

at other times are competitive. In transforming the network in a region, crucial decisions are which

components of the network to modify and how. With the move towards TCP/IP based networks,

the Internet offers a widely used and deployed platform, and the configuration of routing tables

becomes the main focus in the quest for rapid modification of network properties to synthesize

a composite network. Current router software is not easily modified simultaneously across many

routers throughout a region in a way which suits emergency management.

This project seeks to provide a user-friendly transmission policy language for rapid definition of

the required TCP/IP network behaviour, and the means to convert the policy to a list of target

routers and their corresponding reconfigurations for automatic processing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">425</field>
<field name="author">Ryan Wishart</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Context-Enhanced Authentication for Infrastructureless Network Environments</field>
<field name="abstract">Infrastructureless networks are becoming more popular with

the increased prevalence of wireless networking technology. A significant

challenge faced by these infrastructureless networks is that of providing

security. In this paper we examine the issue of authentication, a fundamental

component of most security approaches, and show how it can

be performed despite an absence of trusted infrastructure and limited

or no existing trust relationship between network nodes. Our approach

enables nodes to authenticate using a combination of contextual information,

harvested from the environment, and traditional authentication

factors (such as public key cryptography). Underlying our solution is a

generic threshold signature scheme that enables distributed generation

of digital certificates.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">426</field>
<field name="author">Marius Portmann</field>
<field name="author">Asad Pirzada</field>
<field name="title">Wireless Mesh Networks for Public Safety and Disaster Recovery Communications</field>
<field name="abstract">Recent events have revealed significant shortcomings in Public Safety and Disaster Recovery (PSDR) communications. A lot of currently deployed PSDR communication technology suffers from lack of interoperability and reliability in disaster scenarios. Furthermore, even the most advanced PSDR communication systems do not provide true broadband capabilities to support bandwidth intensive applications such as high quality real-time video.

Wireless Mesh Networks (WMNs) are a promising alternative technology for PSDR

communication, providing features such as broadband support, excellent resilience to failures, self-configuration capability, interoperability and low cost. This paper provides a background on WMN technology and discusses its ability to meet the specific requirements of PSDR applications. We specifically address current limitations of WMNs and provide an overview of current research activities that are underway to overcome these shortcomings.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">427</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Evaluation of Multi-Radio Extensions to AODV for Wireless Mesh Networks</field>
<field name="keyword">Network</field>
<field name="keyword"> routing protocols</field>
<field name="keyword"> experimentation</field>
<field name="keyword"> performance</field>
<field name="abstract">Due to their self-configuring and self-healing capabilities,

as well as their low equipment and deployment cost, Wireless

Mesh Networks (WMNs) based on commodity hardware

present a promising technology for a wide range of applications.

Currently, one of key challenges that WMN technology

faces is the limited capacity and scalability due to high

levels of interference, which is typical for multi-hop wireless

networks. A simple and relatively low-cost approach

to address this problem that has recently been proposed is

the use of multiple wireless network interfaces (radios) per

node. Operating the radios on each node on different, nonoverlapping

channels allows making more efficient use of the

radio spectrum and thereby reducing interference and contention.

In this paper, we evaluate the performance of the

Ad-hoc On-demand Distance Vector (AODV) routing protocol

in a Multi-Radio Wireless Mesh Network. Our simulation

results show that under high traffic load conditions,

Multi-Radio AODV (AODV-MR) is able to efficiently utilize

the increased spectrum, and proves to be far superior to

single radio AODV.We therefore believe that AODV-MR is

a promising candidate for multi-radio WMNs.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">428</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Performance Comparison of Multi-Path AODV and DSR Protocols in Hybrid Mesh Networks</field>
<field name="keyword">Multi-path</field>
<field name="keyword"> routing</field>
<field name="keyword"> mesh</field>
<field name="keyword"> wireless</field>
<field name="keyword"> network</field>
<field name="abstract">Wireless Mesh Networks (WMNs) have gained a

lot of attention recently. Features such as self-configuration, selfhealing

and the low cost of equipment and deployment make

WMN technology a promising platform for a wide range of

applications. Traditional ad-hoc routing protocols are typically

used to deal with the dynamic nature of these networks, which

is mainly due to mobility. However, these protocols typically

suffer from a number of shortcomings, such as high routing

overhead and limited scalability. More recently, it has been

shown that multi-path ad-hoc routing protocols have a number of

advantages over their single-path counterparts, including reduced

overhead and increased reliability. This motivates the work

presented in this paper, which provides a comparison of AOMDV

and DSR-MP, the multi-path variants of AODV and DSR. We

specifically study the performance of these protocols in a hybrid

wireless mesh network, where static mesh routers and mobile

clients collaborate to implement network functionality such as

routing and packet forwarding. Based on extensive simulations,

we present a comparative analysis covering performance metrics

such as packet loss, latency and path optimality.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">429</field>
<field name="author">Ricky Robinson</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Adaptive and Resilient Systems for Emergency Response</field>
<field name="abstract">Events of recent years have brought to light the difficulties involved in responding

to major emergencies, particularly with respect to the on-site coordination of the many

responding agencies. This paper presents an overview of the work being conducted by a team

within National ICT Australia (NICTA) on rapidly deployable, resilient and adaptive networks and applications for emergency response. Our work is focused on creating reconfigurable, self-healing hybrid wireless mesh networks and enabling context-aware applications for emergency response teams that can be deployed over such networks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">430</field>
<field name="author">Joshua Ho</field>
<field name="author">Rajeev Koundinya</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Cristobal G. dos Remedios</field>
<field name="author">Michael A. Charleston</field>
<field name="title">Inferring differential leukocyte activity from antibody microarrays using a latent variable model: Application to cardiovascular research</field>
<field name="keyword">antibody microarray</field>
<field name="keyword"> leukocyte activity</field>
<field name="keyword"> latent variable model</field>
<field name="keyword"> Bayesian network</field>
<field name="keyword"> EM algorithm</field>
<field name="keyword"> cardiovascular diseases</field>
<field name="abstract">Recent development of cluster of differentiation (CD) antibody array has enabled im-

munophenotyping of different complex human diseases. Standard DNA microarray anal-

ysis methods have been used to analyze these antibody array datasets. However, a CD

antigen is commonly expressed by a number of leukocyte subtypes. Common method

failed to take this cell-antigen relationships into account during the analysis. As a re-

sult, DE antigens are usually subjected to manual interpretation to identify differentially

active leukocyte activity in these diseases. Here we present a model-based approach to

incorporate the cell-antigen relationships within the DE analysis. The idea is to model

each cell type as a latent variable, and represent the entire system as a Bayesian network.

Once the parameters of the latent variable model (LVM) are learned from the data us-

ing the expectation-maximization (EM) approach, differentially active leukocytes can be

easily otained from the model. Here we describe the model fomulation and assumptions

which leads to an efficient EM algorithm. We applied our LVM to analysis of two car-

diovascular disease datasets. The results generated by our approach is more consistent

with the literature than other methods.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">431</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Karen Henricksen</field>
<field name="author">Peizhao Hu</field>
<field name="title">Towards a Standards-Based Autonomic Context Management System</field>
<field name="abstract">Pervasive computing applications must be sufficiently au-

tonomous to adapt their behaviour to changes in computing resources

and user requirements. This capability is known as context-awareness.

In some cases, context-aware applications must be implemented as au-

tonomic systems which are capable of dynamically discovering and re-

placing context sources (sensors) at run-time. Unlike other types of appli-

cation autonomy, this kind of dynamic reconfiguration has not been suffi-

ciently investigated yet by the research community. However, application-

level context models are becoming common, in order to ease program-

ming of context-aware applications and support evolution by decoupling

applications from context sources. We can leverage these context models

to develop general (i.e., application-independent) solutions for dynamic,

run-time discovery of context sources (i.e., context management). This

paper presents a model and architecture for a reconfigurable context

management system that supports interoperability by building on emerg-

ing standards for sensor description and classification.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">432</field>
<field name="author">Karen Henricksen</field>
<field name="author">Ricky Robinson</field>
<field name="title">A Survey of Middleware for Sensor Networks: State-of-The-Art and Future Directions</field>
<field name="abstract">In future computing environments, networked sensors will

play an increasingly important role in mediating between

the physical and virtual worlds. However, programming

sensor networks, and the applications that depend on the

data they produce, is extremely challenging. The need for

suitable middleware to address this problem is evident. In

the last few years, various middleware solutions for sensor

networks have emerged. These differ in terms of their models

for querying and data aggregation, and their assumptions

about the topology and other characteristics of the network.

Naturally, the assumptions made for each particular middleware

limit its potential applicability. Most of the current

solutions provide relatively simple query abstractions, and

therefore are not suitable for applications that have sophisticated

requirements for processing of sensor data in the

network. This paper presents a survey and analysis of the

current state-of-the art in the field, highlighting the open research

challenges. It also draws on the authors experience

with developing middleware for context-aware systems - that

is, systems that rely on sensor-derived data to intelligently

adapt their behaviour - to propose some future directions

for the development of middleware for sensor networks.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">433</field>
<field name="author">Myilone Anandarajah</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Ricky Robinson</field>
<field name="title">Caching Context Information in Pervasive Systems</field>
<field name="keyword">context-awareness</field>
<field name="keyword"> pervasive computing</field>
<field name="keyword"> caching</field>
<field name="abstract">Context aware systems are systems that use context information

to adapt their behaviour or the content they provide.

This paper addresses the problem of disconnections between

nodes in these systems. Disconnections in a context aware

system may occur because of node mobility, network failures

or node failures. A research opportunity lies in improving

the robustness of such systems to disconnections. While traditional

distributed systems methods of improving robustness

in the face of disconnections can be applied to context

aware systems, the additional metadata available to them

may be leveraged to provide smarter caching algorithms.

This paper presents ideas for context information caching

algorithms that utilise such metadata.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">434</field>
<field name="author">Asad Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Hybrid Mesh Ad-hoc On-demand Distance Vector Routing Protocol</field>
<field name="keyword">wireless mesh networks</field>
<field name="keyword"> routing</field>
<field name="keyword"> hybrid mesh networks</field>
<field name="keyword"> channel diversity</field>
<field name="abstract">Wireless Mesh Networks (WMNs) have recently

gained increasing attention and have emerged as a

technology with great potential for a wide range of

applications. WMNs can be considered as a super-

set of traditional mobile ad-hoc networks (MANETs),

where the network is comprised of mobile client de-

vices (MESH CLIENTs). In addition to MESH CLIENTs, a

WMN can also contain relatively static devices called

mesh routers (MESH ROUTERs). Such hybrid WMNs

are characterized by a high level of heterogeneity,

since static MESH ROUTERs are typically much less re-

source constrained than mobile MESH CLIENTs, and

are also often equipped with multiple radio inter-

faces. Traditional ad-hoc routing protocols do not

differentiate between these types of nodes and there-

fore cannot achieve optimal performance in hybrid

WMNs. In this paper, we propose simple extensions

to the Ad-hoc On-demand Distance Vector (AODV)

routing protocol, which aim to take advantage of

the heterogeneity in hybrid WMNs by preferentially

routing packets via paths consisting of high capac-

ity MESH ROUTERs. In addition, we implement a sim-

ple channel selection scheme that reduces interfer-

ence and maximizes channel diversity in multi-radio

WMNs. Our simulation results show that in hybrid

WMNs, our extensions result in signi cant perfor-

mance gains over the standard AODV protocol.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">435</field>
<field name="author">Myilone Anandarajah</field>
<field name="author">Ricky Robinson</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Hoarding Context Information with Context Clusters</field>
<field name="abstract">The components of a context-aware system can often become

disconnected because of the dynamic environments

within which they are deployed. Hoarding context information

on the client application side can improve the probability

that the application will continue to behave correctly in

the event of disconnection. While traditional approaches to

caching and hoarding can be used to combat this problem,

we contend that a cache management solution that uses the

extra information captured by context modelling techniques

will provide more robust operation. Specifically, we focus

on the metadata provided by the Context Modelling Language

(CML), which may enable smarter decisions to be

made by a cache management system for context information.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">436</field>
<field name="author">Ricky Robinson</field>
<field name="author">Karen Henricksen</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">XCML: A Runtime Representation for the Context Modelling Language</field>
<field name="abstract">The Context Modelling Language (CML), derived from

Object Role Modeling (ORM), is a powerful approach for

capturing the pertinent object types and relationships between

those types in context-aware applications. Its support

for data quality metrics, context histories and fact type classifications

make it an ideal design tool for context-aware

systems. However, CML currently lacks a suitable representation

for exchanging context models and instances

in distributed systems. A runtime representation can be

used by context-aware applications and supporting infrastructure

to exchange context information and models between

distributed components, and it can be used as the

storage representation when relational database facilities

are not present. This paper shows the benefits of using

CML for modelling context as compared to commonly used

RDF/OWL-based context models, shows that translations

of CML to RDF or OWL are lossy, discusses existing techniques

for serialising ORM models, and presents an alternative

XML-based representation for CML called XCML.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">437</field>
<field name="author">Charles Gretton</field>
<field name="title">Gradient-Based Relational Reinforcement-Learning of Temporally Extended Policies</field>
<field name="keyword">stochastic planning</field>
<field name="keyword"> relational reinforcement learning</field>
<field name="abstract">\footnote{This paper is a short summary of my ICAPS-07 paper with the

 same title.} We consider the problem of computing general policies

for decision-theoretic planning problems with temporally extended

rewards. We consider a gradient-based approach to relational

reinforcement-learning (RRL) of policies for that setting. In

particular, the learner optimises its behaviour by acting in a set of

problems drawn from a target domain. Our approach is similar to {\em

 inductive policy selection} because the policies learnt are given in

terms of relational control-rules. These rules are generated either

(1) by reasoning from a first-order domain description, or (2) more or

less arbitrarily according to a taxonomic concept language.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">438</field>
<field name="author">Ricky Robinson</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Ted McFadden</field>
<field name="title">Resource Discovery in Modern Computing Environments</field>
<field name="keyword">pervasive computing</field>
<field name="keyword"> resource discovery</field>
<field name="keyword"> service discovery</field>
<field name="abstract">Resource discovery is an integral part of many kinds

of computing environments, including emerging heterogeneous

environments for pervasive computing. This paper characterises

a number of current and future computing environments and

summarises their resource discovery requirements. It then analyses,

with respect to the requirements of each environment,

several established service discovery protocols and some newer

protocols that are still in the research domain. In addition, the

key features of a new resource discovery protocol that has been

developed to operate with heterogeneous computing environments

are described.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">439</field>
<field name="author">Steve Glass</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">A Study of the TKIP DoS Attack</field>
<field name="abstract">Wireless networks, especially those based on 802.11,

have found widespread use in domestic, commercial, educational,

military and public-safety environments. The security of these

wireless networks is assuming an increasing importance as users

come to rely on the availability and correct functioning of wireless

network services.

This paper investigates the cryptographic denial-of-service

(DoS) attack agaist the 802.11i TKIP security protocol. We have

conducted a laboratory study and show that it takes very little

effort to bring TKIP-protected network traffic to a complete halt.

This attack maybe used not just to compromise availability but is

also an effective means of conducting a security-level rollback to

the insecure WEP protocol. We use a testbed network to evaluate

a remedial measure that eliminates the vulnerability on which

the attack is based.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">440</field>
<field name="author">Steve Glass</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="author">Marius Portmann</field>
<field name="title">Securing Wireless Mesh Networks</field>
<field name="abstract">Now found in domestic, commercial, industrial, military, and healthcare

applications, wireless networks are becoming ubiquitous. Wireless mesh

networks (WMNs) combine the robustness and performance of conventional

infrastructure networks with the large service area and self-organizing and

self-healing properties of mobile ad hoc networks. In this article, the authors

consider the problem of ensuring security in WMNs, introduce the IEEE

802.11s draft standard, and discuss the open security threats faced at the

network and data-link layers.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">441</field>
<field name="author">Steve Glass</field>
<field name="title">Securing Multi-Hop Wireless Networks Against Impersonation Attacks</field>
<field name="abstract">Multi-hop wireless networks as used in sensor nets are particularly

vulnerable to impersonation attacks. In this class of

attacks a hostile adversary controls one or mode nodes which

misrepresent their identity and selectively forward traffic to t

legitimate recipients. Such attacks can be used to undermine

network integrity, availability and reputation-based

trust schemes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">442</field>
<field name="author">Gerwin Klein</field>
<field name="title">Operating System Verification - An Overview</field>
<field name="keyword">Formal Software Verification</field>
<field name="keyword"> Operating Systems</field>
<field name="keyword"> Theorem Proving</field>
<field name="abstract">This paper gives a high-level introduction to the topic of formal,

interactive, machine-checked software verification in general, and the

verification of operating systems code in particular. We survey the

state of the art, the advantages and limitations of machine-checked

code proofs, and describe one specific ongoing larger-scale

verification project in more detail.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">443</field>
<field name="author">Jacky Keung</field>
<field name="title">Empirical Evaluation of Analogy-X for Software Cost Estimation</field>
<field name="keyword">Analogy</field>
<field name="keyword"> software cost estimation</field>
<field name="keyword"> software engineering</field>
<field name="abstract">This paper reports on the empirical evaluation of a novel approach called Analogy-X, which is an extension to the classical analogy-based software cost estimation. The Analogy-X approach is a set of procedures that utilize the principles of Mantel randomization test to provide inferential statistics to Analogy. Our previous studies, have clearly demonstrated the novelty and effectiveness of this technique. This paper is to perform further empirical evaluation of Analogy-X using different kinds of datasets. The results were compared with results produced by ANGEL, showing different prediction accuracy measured by MMRE, further investigation shows the prediction accuracy produced by the two approaches were actually similar, but Analogy-X has the advantage of being able to use Mantel statistics to select project features and detect abnormal data points, which provides a sound statistical basis for analogy- based systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">444</field>
<field name="author">Yasutaka Kamei</field>
<field name="author">Jacky Keung</field>
<field name="author">Akito Monden</field>
<field name="author">Ken-ichi Matsumoto</field>
<field name="title">An Over-sampling Method for Analogy-based Software Effort Estimation</field>
<field name="keyword">Analogy</field>
<field name="keyword"> software cost estimation</field>
<field name="keyword"> software engineering</field>
<field name="abstract">This paper proposes a novel method to generate synthetic project cases and add them to a fit dataset for the purpose of improving the performance of analogy-based software effort estimation. The proposed method extends conventional over-sampling method, which is a preprocessing procedure for n-group classification problems, which makes it suitable for any imbalanced dataset to be used in analogy-based system. We experimentally evaluated the effect of the over-sampling method to improve the performance of the analogy-based software effort estimation by using the Desharnais dataset. Results show significant improvement to the estimation accuracy by using our approach.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">445</field>
<field name="author">Sylvie Thiebaux</field>
<field name="author">Olivier Buffet</field>
<field name="title">Planification d'operations</field>
<field name="keyword">AI</field>
<field name="keyword"> planning</field>
<field name="keyword"> MDPs</field>
<field name="abstract">Ce chapitre aborde l application des MDP aux probl mes de planification d op rations

tels qu on les trouve dans des domaines aussi vari s que l exploration spatiale

(rovers, satellites, t lescopes), les op rations militaires et la gestion de projets. La

planification automatis e [GHA 04, REG 04] est une branche de l intelligence artificielle

; elle vise construire des syst mes g n riques capables de choisir et d organiser

les op rations entreprendre, de mani re atteindre des objectifs donn s moindre

co t. Ici, nous examinons des probl mes de planification complexes qui requi rent non

seulement l ex cution parall le d op rations, la prise en compte explicite du temps

(dur e des op rations, instants auxquels les op rations affectent l tat de l environnement),

mais aussi la gestion de l incertitude li e aux effets des op rations, aux instants

auxquels ils se produisent et la dur e des op rations. Il s agit de probl mes de planification

temporelle probabiliste [ABE 06, ABE 07b, LIT 05, MAU 05, MAU 07]. Nous

d finissons ces probl mes de fa _on intuitive puis formelle avant de montrer comment

ils se mod lisent par des MDP et se r solvent en adaptant des algorithmes discut s au

cours des chapitres pr c dents.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">446</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Smith</field>
<field name="author">Daniel Lewis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Dynamic narrowband channel measurements around 2.4 GHz for body area networks</field>
<field name="keyword"/>
<field name="abstract">This document presents preliminary real-time measurements of the dynamic nature of 10 MHz bandwidth radio channels around the human body at 2.4 GHz.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">447</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">Changing Legal Systems: Abrogation and Annulment. Part II: Temporalised Defeasible Logic</field>
<field name="keyword">belief revision</field>
<field name="keyword"> norm changes</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">In this paper we propose a temporal extension of Defeasible Logic to

 model legal modifications, such as abrogation and annulment. Hence,

 this framework overcomes the difficulty, discussed elsewhere, of capturing these modification types using

 belief and base revision.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">448</field>
<field name="author">Conrad Sanderson</field>
<field name="title">Biometric Person Recognition: Face, Speech and Fusion</field>
<field name="keyword">biometrics</field>
<field name="keyword"> face processing</field>
<field name="keyword"> speech processing</field>
<field name="keyword"> data fusion</field>
<field name="abstract">Over the last decade, interest in biometric based identification and verification systems has increased considerably. One application is the use of speech signals, face images or fingerprints in order to supplement security systems based on passwords. Biometric recognition can also be applied to other areas, such as passport control (immigration checkpoints), forensic work (to determine whether a biometric sample belongs to a suspect)

and law enforcement applications (e.g. surveillance). While biometric systems based on face images and/or speech signals can be effective, their performance can degrade in the presence of challenging conditions. In face based systems this can be in the form of a change in the illumination direction and/or face pose variations. Multi-modal systems use more than one biometric at the same time. This is done for two main reasons -- to achieve better robustness and to increase discrimination power. This book can serve as a useful primer for face and speech processing, as well as information fusion. It reviews relevant backgrounds and reports research aimed at increasing the robustness of single- and multi-modal biometric identity verification systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">449</field>
<field name="author">Daniel Lewis</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="title">Some lead Body Area Network (BAN) user requirements and constraints: A summary and analysis of interviews with lead users</field>
<field name="keyword">technology applications</field>
<field name="keyword"> end users</field>
<field name="keyword"> use inspiration</field>
<field name="abstract">A summary and analysis of interviews with some lead Body Area Network (BAN) lead users, to promote discussion within the BAN study group on the user requirements and constraints for BAN</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">450</field>
<field name="author">Duy Hoang Pham</field>
<field name="author">Guido Governatori</field>
<field name="author">Simon Raboczi</field>
<field name="title">Agents adapt to majority behaviours</field>
<field name="keyword">Multiagent systems</field>
<field name="keyword"> Non-monotonic reasoning</field>
<field name="abstract">Agents within a group can have different perceptions of their working environment and autonomously fulfil their goals. However, they can be aware of beliefs and goals of the group as well as other members so that they can adjust their behaviours accordingly.

To model these agents, we explicitly include knowledge commonly shared by the group and that obtained from other agents. By avoiding actions which violate ``mental attitudes'' shared by the majority of the group, agents demonstrate their social commitment to the group.

Defeasible logic is chosen as our representation formalism for its computational efficiency, and for its ability to handle incomplete and conflicting information.

Hence, our agents can enjoy the low computational cost while performing ``reasoning about others''. Finally, we present the implementation of our multi-agent system.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">451</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">BIO Logical Agents: Norms, Beliefs, Intentions in Defeasible Logic</field>
<field name="keyword">agents</field>
<field name="keyword"> norms</field>
<field name="abstract">In this paper we follow the BOID (Belief,

 Obligation, Intention, Desire) architecture to

 describe agents and agent types in Defeasible

 Logic. We argue, in particular, that the

 introduction of obligations can provide a new

 reading of the concepts of intention and

 intentionality. Then we examine the notion of social

 agent (i.e., an agent where obligations prevail over

 intentions) and discuss some computational and

 philosophical issues related to it. We show that

 the notion of social agent either requires more

 complex computations or has some philosophical

 drawbacks</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">452</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">A Computational Framework for Institutional Agency</field>
<field name="keyword">agents</field>
<field name="keyword"> institutional agents</field>
<field name="keyword"> counts as</field>
<field name="abstract">This paper provides a computational framework, based

 on Defeasible Logic, to capture some aspects of

 institutional agency. Our background is

 Kanger-Lindahl-P\"orn account of organised

 interaction, which describes this interaction within

 a multi-modal logical setting. This work focuses in

 particular on the notions of counts-as link and on

 those of attempt and of personal and direct action

 to realise states of affairs. We show how standard

 Defeasible Logic can be extended to represent these

 concepts: the resulting system preserves some basic

 properties commonly attributed to them. In addition,

 the framework enjoys nice computational properties,

 as it turns out that the extension of any theory can

 be computed in time linear to the size of the theory

 itself.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">453</field>
<field name="author">Insu Song</field>
<field name="author">Guido Governatori</field>
<field name="title">Designing Agent Chips</field>
<field name="abstract">We outline meta-encoding schemas for compiling nonmonotonic logic theories into Verilog HDL (Hardware Description Language) descriptions. These descriptions can be synthesized into gate level specifications for direct fabrication of silicon chips. The method is applied for designing agent chips incorporating similar features found in the BDI (Belief, Desire, and Intention) and Brooks' subsumption architectures.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">454</field>
<field name="author">Mehdi Dastani</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Insu Song</field>
<field name="author">Leendert van der Torre</field>
<field name="title">Contextual Deliberation of Cognitive Agents in Defeasible Logic</field>
<field name="keyword">context</field>
<field name="keyword"> agents</field>
<field name="abstract">This article extends Defeasible Logic to deal with the contextual deliberation process of cognitive agents. First, we introduce meta-rules to reason with rules. Meta-rules are rules that have as a consequent rules for motivational components, such as obligations, intentions and desires. In other words, they include nested rules. Second, we introduce explicit preferences among rules. They deal with complex structures where nested rules can be involved.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">455</field>
<field name="author">Subhasis Thakur</field>
<field name="author">Guido Governatori</field>
<field name="author">Vineet Padmanabhan</field>
<field name="author">Jenny Eriksson Lundstr{\"o}m</field>
<field name="title">Dialogue Games in Defeasible Logic</field>
<field name="abstract">In this paper we show how to capture dialogue games

 in Defeasible Logic. We argue that Defeasible Logic

 is a natural candidate and general representation

 formalism to capture dialogue games even with

 requirements more complex than existing formalisms

 for this kind of games. We parse the dialogue into

 defeasible rules with time of the dialogue as time

 of the rule. As the dialogue evolves we allow an

 agent to upgrade the strength of unchallenged

 rules. The proof procedures of \cite{tocl} are used

 to determine the winner of a dialogue game.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">456</field>
<field name="author">Stefan Petters</field>
<field name="author">Martin Lawitzky</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Ryan Heffernan</field>
<field name="title">Fitting an EDF based Scheduling Approach to Componentised Real(-Time) Systems</field>
<field name="keyword">Real-Time</field>
<field name="keyword"> Scheduling</field>
<field name="keyword"> Operating System</field>
<field name="keyword"> Real-Time Analysis</field>
<field name="abstract">Componentised systems, in particular those with fault confinement through address spaces, are currently emerging as a hot topic in systems research. This paper extends the unified rate-based scheduling framework RBED in several dimensions to fit the requirements of such systems. First, we have removed the requirement of the deadline of a task being equal to its period. Second, we introduce inter-process communication and end-to-end deadlines, reflecting the need to communicate and avoid fragmentation of the system through deadline partitioning. Additionally we also discuss server tasks, general I/O management, budget replenishment and low level details to deal with the physical reality of real systems work.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">457</field>
<field name="author">Matthew Asquith</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Damian Merrick</field>
<field name="title">An ILP for the line ordering problem</field>
<field name="abstract">In this paper we consider a problem that occurs when drawing public transportation networks. Given an embedded graph G = (V,E) (e.g. the railroad network) and a set H of paths in G (e.g. the train lines), we want to draw the paths along the edges of G such that they cross each other as few times as possible. For aesthetic reasons we insist that the relative order of the paths that traverse a vertex does not change within the area occupied by the vertex. We prove that the problem, which is known to be NP-hard, can be rewritten as an integer linear program that finds the optimal solution for the problem. In the case when the order of the endpoints of the paths is fixed we prove that the problem can be solved in polynomial time. This improves a recent result by Bekos et al. (2007).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">458</field>
<field name="author">Jiuyong Li</field>
<field name="author">Wang Hua</field>
<field name="author">Huidong Jin</field>
<field name="author">Jianming Yong</field>
<field name="title">Current Developments of k-anonymous data releasing</field>
<field name="keyword">Privacy preserving</field>
<field name="keyword"> data releasing</field>
<field name="keyword"> k-anonymity</field>
<field name="abstract">Disclosure-control is a traditional statistical methodology for protecting privacy when data is

released for analysis. Disclosure-control methods have enjoyed a revival in the data mining community,

especially after the introduction of the k-anonymity model by Samarati and Sweeney.

Algorithmic advances on k-anonymisation provide simple and effective approaches to protect private

information of individuals via only releasing k-anonymous views of a data set. Thus, the kanonymity

model has gained increasing popularity. Recent research identifies some drawbacks of

the k-anonymity model and presents enhanced k-anonymity models. This paper reviews problems of

the k-anonymity model and its enhanced variants, and different methods for implementing k-anonymity.

It compares the k-anonymity model with the secure multiparty computation-based privacypreserving

techniques in the data mining literature. The paper also discusses further development

directions of the k-anonymous data releasing.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">459</field>
<field name="author">Glenn Geers</field>
<field name="title">Some Research Questions for Computational Transportation Science</field>
<field name="abstract">In this paper some research questions which potentially lie within

the purview of the new discipline of Computational Transportation

Science are presented.Some of these questions are no doubt old and

jaded and some may rightly reside in a different research field

(or be seen as engineering) but viable solutions to all of the 

issues presented are of vital importance to the efficacy of future 

transportation systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">460</field>
<field name="author">Gernot Heiser</field>
<field name="title">Trusted &lt;= Trustworthy &lt;= Proof Position Paper</field>
<field name="keyword">operating systems</field>
<field name="keyword"> security</field>
<field name="keyword"> common criteria</field>
<field name="keyword"> implementation correctness</field>
<field name="keyword"> proof</field>
<field name="abstract">Trusted computing is important, but we argue that it remains an illusion as long as the underlying trusted computing base (TCB) is not trustworthy.

We observe that present approaches to trusted computing do not really address this issue, but are

trusting TCBs which have not been shown to deserve this trust. We argue that only mathematical

proof can ensure the trustworthiness of the TCB. In short: trust requires trustworthiness, which in turn requires proof. We also show that this is achievable.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">461</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Jenny Liu</field>
<field name="author">Liming Zhu</field>
<field name="title">On Combining WS-Policy4MASC and ASF to Support Business-Driven Autonomic Service-Oriented Computing</field>
<field name="keyword">Web services</field>
<field name="keyword"> quality of service</field>
<field name="keyword"> business-driven IT management</field>
<field name="keyword"> autonomic computing</field>
<field name="abstract">WS-Policy4MASC is an XML language for specification of policies for run-time Web service management. Among its original contributions is specification of diverse business values and various control strategies maximiz-ing different business values. While it was originally developed for the MASC (Manageable and Adaptable Service Compositions) middleware, it can also be used in a broader context of autonomic service-oriented computing. We discuss our research on using WS-Policy4MASC in the Adaptive Server Framework (ASF), which supports composing adaptive applications on Java and .NET plat-forms. WS-Policy4MASC enriches ASF with policy definition semantics and enables complex policy management. The combination of WS-Policy4MASC and ASF forms a step towards business-driven autonomic management of ser-vice-oriented systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">462</field>
<field name="author">Peter Stuckey</field>
<field name="author">Christian Schulte</field>
<field name="title">Dynamic Analysis of Bounds versus Domain Propagation</field>
<field name="keyword">constraint propagation</field>
<field name="keyword"> bounds propagation</field>
<field name="keyword"> domain propagation</field>
<field name="abstract">Constraint propagation solvers interleave propagation (removing impossible

values from variables domains) with search.

 Schulte and Stuckey introduced the use of static analysis to

determine where in a constraint program domain

propagators can be replaced by more efficient bounds propagators and still ensure that the

same search space is traversed. This paper introduces a dynamic

yet considerably simpler approach to uncover the same

information. The information is obtained by a linear time

traversal of an analysis graph that straightforwardly reflects

the properties of propagators implementing constraints and is easily

obtained from the propagator (constraint) graph.



Experiments conducted in Gecode confirm that the simple dynamic method is efficient

and that it is beneficial to optimize 

in the search, making use of the simplification of the constraint graph

that arises from search to make the method more applicable.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">463</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="title">A Network Mobility Management Architecture for a Heterogeneous Network Environment</field>
<field name="keyword">Mobility management</field>
<field name="keyword"> Network mobility</field>
<field name="keyword"> Mobile IP</field>
<field name="abstract">With the almost ubiquitous availability of wireless

 communication networks and the increasing communication capabilities

 of electronic devices, the prediction that most devices are

 constantly connected to the Internet is fast becoming a

 reality. Furthermore, the speed of the wireless networks is

 increasing at a fast rate. In a vehicular environment, passengers

 and vehicular sensors can access services in the Internet by

 utilizing a mobile network architecture which connects the vehicular

 network to the Internet through heterogeneous wireless networks.



 Deployment of a mobile network architecture using mobile

 routers, e.g. the NEMO architecture, allows the devices within the

 vehicle to connect to a vehicular wired or wireless local area

 network. The mobile router can then forward data between the local

 area network and the Internet using multiple access networks,

 external antennas outside the vehicle, and higher transmission

 power. These characteristics of the NEMO architecture increase the

 number of connectivity options to the devices connected to the

 vehicular network, thus increasing the available bandwidth and

 decreasing the cost of communications. However, the long latency of

 handoffs, and the overheads of the protocols involved in NEMO may

 limit the benefits and applicability of a NEMO based architecture.



 This chapter presents a mobile network architecture which

 addresses the handoff delays and protocol overheads of NEMO. The

 feasibility of the architecture is evaluated by comparing its

 performance with state of the art proposals using both analytical

 and empirical data.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">464</field>
<field name="author">Jing Chen</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Zhidong Li</field>
<field name="title">A Machine Learning Framework for Real-Time Traffic Density Detection</field>
<field name="keyword">Machine learning</field>
<field name="keyword"> pattern recognition</field>
<field name="keyword"> feature extraction</field>
<field name="keyword"> unsupervised clustering</field>
<field name="keyword"> and HMM.</field>
<field name="abstract">Traffic flow information can be used by an intelligent transportation system to detect and manage traffic congestion. One of the key elements in determining the traffic flow information is traffic density estimation. The goal of traffic density estimation is to determine the density of vehicles on a given road from loop detectors, traffic radars, or surveillance cameras. However, due to the inflexibility of deploying loop detectors and traffic radars, there is a growing trend of using a video content understanding approach to determine the traffic flow from a surveillance camera. But difficulties arise when attempting to do this in real-time under different illumination and weather conditions as well as during heavy traffic congestions. In this paper, we attempt to address the problem of real-time traffic density estimation by using a stochastic model called Hidden Markov Models (HMM) to probabilistically determine the traffic density state. Choosing a good set of model parameters for HMMs has a direct impact on the accuracy of traffic density estimation. Thus, we propose a novel feature extraction scheme for representing traffic density, and a novel approach for initializing and structuring the HMMs by using an unsupervised clustering technique called AutoClass. We show through extensive experiments that our proposed real-time algorithm achieves an average traffic density estimation accuracy of 96.6% over various different illumination and weather conditions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">465</field>
<field name="author">Sijun Lu</field>
<field name="author">Jian Zhang</field>
<field name="author">David Feng</field>
<field name="title">Detecting Ghost And Left Objects In Surveillance Video</field>
<field name="keyword">ghost</field>
<field name="keyword"> left object</field>
<field name="keyword"> inpainting</field>
<field name="keyword"> background modeling</field>
<field name="keyword"> motion detection</field>
<field name="keyword"> object tracking</field>
<field name="abstract">This paper proposes an efficient method for detecting ghost and left objects in surveillance video, which, if not identified, may lead to errors or wasted computational power in background modeling and object tracking in video surveillance systems. This method contains two main steps: the first one is to detect stationary objects, which narrows down the evaluation targets to a very small number of regions in the input image; the second step is to discriminate the candidates between ghost and left objects. For the first step, we introduce a novel stationary object detection method based on continuous object tracking and shape matching. For the second step, we propose a fast and robust inpainting method to differentiate between ghost and left objects by reconstructing the real background using the candidate s corresponding regions in the current input and background image. The effectiveness of our method has been validated by experiments over a variety of video sequences and comparisons with existing state-of-art methods.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">466</field>
<field name="author">Gunawan Herman</field>
<field name="author">Getian Ye</field>
<field name="author">Jie Xu</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">Improving Object Detection by Removing Noisy Samples from Training Sets</field>
<field name="keyword">Object detection</field>
<field name="keyword"> data pruning</field>
<field name="keyword"> noisy samples</field>
<field name="abstract">Object detection is often formulated as a binary classification task with supervised learning that involves training datasets. Noisy samples, including mislabeled samples and "hard-to-learn" samples, are usually found in training datasets. Such samples have a detrimental effect on the generalization performance of trained classifiers and are required to be pruned. In this paper, we propose a novel data pruning algorithm that 

is based on recursive Bayes approach and AdaBoost. Recursive Bayes approach increases the confidence of predictions in every iteration, while AdaBoost minimizes the number of predictions that have low confidence. Extensive experiments on real datasets show the effectiveness of the proposed algorithm in identifying and pruning noisy samples from training datasets and concurrently improving the performance of classification and object detection.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">467</field>
<field name="author">Shaokang Chen</field>
<field name="author">Erik Berglund</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Experimental Analysis of Face Recognition on Still and CCTV images</field>
<field name="keyword">face recognition</field>
<field name="keyword"> CCTV</field>
<field name="keyword"> image quality</field>
<field name="abstract">Although automatic identity inference based on faces has

shown success when using high quality images, for CCTV

based images it is hard to attain similar levels of performance.

Furthermore, compared to recognition based on

static images, relatively few studies have been done for

video based face recognition. In this paper, we present an

empirical analysis and comparison of face recognition using

high quality and CCTV images in several important aspects:

image quality (including resolution, noise, blurring

and interlacing) as well as geometric transformations (such

as translations, rotations and scale changes). The results

show that holistic face recognition can be tolerant to image

quality degradation but can also be highly influenced by geometric

transformations. In addition, we show that camera

intrinsics have much influence when using different cameras

for collecting gallery and probe images the recognition

rate is considerably reduced. We also show that the

classification performance can be considerably improved

by straightforward averaging of consecutive face images

from a CCTV video sequence.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">468</field>
<field name="author">Jie Xu</field>
<field name="author">Getian Ye</field>
<field name="author">Gunawan Herman</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">An Efficient Approach to Detecting Pedestrians in Video</field>
<field name="keyword">Pedestrian detection</field>
<field name="keyword"> Shape context</field>
<field name="keyword"> Visual codebook</field>
<field name="abstract">Detecting and recognizing pedestrians in video footages are essential and significant tasks in many automatic video understanding systems. In this paper, we propose an efficient approach to moving pedestrian detection and recognition. This approach consists of two main steps: moving edge detection and hypotheses generation. Moving edges are firstly detected by comparing the edges identified in adjacent frames. Based on detected moving edges, initial hypotheses are then generated by using a codebook of shape context descriptors learned from a set of training samples. In order to form the final hypotheses, initial hypotheses with large overlaps are pruned. The proposed approach reduces the size of the feature space for recognition and hence has the following advantages by comparison with the existing techniques: (1) fewer training samples, (2) lower false positive rate, and (3) lower computational cost. Experiments with a publicly available dataset show that the proposed approach can reliably detect and recognize moving pedestrians in real scenes that contain either different viewing angles or different degrees of occlusions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">469</field>
<field name="author">John Slaney</field>
<field name="author">Asif Ali</field>
<field name="title">Generating Loops with the Inverse Property</field>
<field name="keyword">Finite algebra</field>
<field name="keyword"> IP loops</field>
<field name="keyword"> Constraint satisfaction</field>
<field name="abstract">This is an investigation in the tradition of Fujita et al (IJCAI 1993), Zhang et al (JSC 1996) Dubois and Dequen (CP 2001) in which CP or SAT techniques are used to answer existence questions concerning small algebras. In this paper, we open the attack on IP loops, an interesting and under-investigated variety intermediate between loops and groups.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">470</field>
<field name="author">John Slaney</field>
<field name="title">A Logic for Vagueness</field>
<field name="keyword">Logic</field>
<field name="keyword"> Vagueness</field>
<field name="abstract">This paper presents an approach to the problem of accommodating vague discourse in the framework of a system of logic. The proposed logic is related to standard fuzzy logics such as the Lukasiewicz systems, but has better proof-theoretic properties. Completeness theorems relative to a class of Routley-Meyer frames, a cut elimination theorem and decidability for the propositional fragment are proved and discussed. The philosophical problems surrounding vagueness remain unsettled, but the present paper advances the discussion by promoting a nonclassical logical approach which avoids some of the unsatisfactory properties of those already in the literature.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">471</field>
<field name="author">Liming Zhu</field>
<field name="author">Thong Nguyen</field>
<field name="author">Tan Dao</field>
<field name="author">Jenny Liu</field>
<field name="title">Model Driven Development for Technology Evaluation - Recommendations</field>
<field name="abstract">Model Driven Development (MDD) refers to the methodical use of models in various software engineering activities. MDD has been mainly used to guide development through modelling and code generation. However, features of MDD can help system evaluation and acquisition as well. DSTO, which is responsible for technically evaluating ADF acquisitions, is considering integrating MDD into their evaluation activities. In a separate report, we critically reviewed the state-of-the-art of MDD in the context of evaluating quality (non-functional properties) of systems and advising on risk assessment and mitigation. In this report, we propose some improvements over the existing MDD approaches for acquisition evaluation. We provide some recommendations on tooling. In addition to the benefits to DSTO, this work also makes important scientific advances in the area of MDD.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">472</field>
<field name="author">Pranam Janney</field>
<field name="author">Jack Zhenghua Yu</field>
<field name="title">Invariant Features of Local Textures a rotation invariant local texture</field>
<field name="keyword">illumination invariance</field>
<field name="keyword"> texture classification</field>
<field name="keyword"> rotation invariant</field>
<field name="keyword"> local textures</field>
<field name="keyword"> features</field>
<field name="keyword"> histograms</field>
<field name="keyword">wavelets</field>
<field name="abstract">In this paper, we present a new rotation-invariant texture descriptor algorithm called Invariant Features of Local Textures (IFLT). The proposed algorithm extracts rotation invariant features from a small neighbourhood of pixels around a centre pixel or a texture patch. Intensity vector which is derived from a texture patch is normalized and Haar wavelet filtered to derive rotation-invariant features. Texture classification experiments on the Brodatz album and Outex databases have shown that the proposed algorithm has a high rate of correct classification.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">473</field>
<field name="author">Saul Greenberg</field>
<field name="author">Gregor Mcewan</field>
<field name="author">Michael Rounding</field>
<field name="title">Reflecting on Several Metaphors of MUD-based Media Spaces</field>
<field name="abstract">Over the last decade, we designed and used three media spaces: Teamrooms, Notification Collage, and Community Bar. All were oriented towards creating a shared environment supporting a small community of people: about two to around twenty members were expected to inhabit the media space. All provided others with a sense of presence through portrait images and/or snapshot-based video of its members, and all emphasised creation and sharing of real-time groupware artefacts. They differed in that each was designed around a different metaphor: multiple rooms for Teamrooms, a shared live bulletin board for the Notification Collage, and an expandable sidebar that contained multiple places for Community Bar. This chapter briefly reflects on how the systems and their metaphors served as a communal place. We saw that many factors both large and small profoundly affected how these media spaces were adopted by the community. We also saw that there was a tension between the explicit structures offered by media space design (rooms, places, bulletin boards and so on) vs. the very light weight and often implicit ways that people form and reform into groups and how they attend to information in the real world.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">474</field>
<field name="author">Joshua LeVasseur</field>
<field name="author">Volkmar Uhlig</field>
<field name="author">Yaowei Yang</field>
<field name="author">Matthew Chapman</field>
<field name="author">Peter Chubb</field>
<field name="author">Ben Leslie</field>
<field name="author">Gernot Heiser</field>
<field name="title">Pre-virtualization: soft layering for virtual machines</field>
<field name="abstract">Despite its current popularity, para-virtualization has an

enormous cost. Its deviation from the platform architecture

abandons many of the benefits of traditional virtualization:

stable and well-defined platform interfaces, hypervisor neu-

trality, operating system neutrality, and upgrade neutral-

ity in sum, modularity. Additionally, para-virtualization

has a significant engineering cost. These limitations are

accepted as inevitable for significantly better performance,

and for the ability to provide virtualization-like behavior on

non-virtualizable hardware such as x86.



Virtualization and its modularity solve many systems

problems, and when combined with the performance of

para-virtualization become even more compelling. We show

how to achieve both together. We still modify the guest

operating system, but according to a set of design princi-

ples that avoids lock-in, which we call soft layering. Ad-

ditionally, our approach is highly automated and thus re-

duces the implementation and maintenance burden of para-

virtualization, which is especially useful for enabling obso-

leted operating systems. We demonstrate soft layering on

x86 and Itanium: we can load a single Linux binary on a

variety of hypervisors (and thus substitute virtual machine

environments and their enhancements), while achieving es-

sentially the same performance as para-virtualization with

less effort.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">475</field>
<field name="author">Paul Brebner</field>
<field name="author">Liam O'Brien</field>
<field name="author">Jon Gray</field>
<field name="title">Business transformation to SOA: aspects of the migration and performance and QoS issues</field>
<field name="abstract">Organizations face challenges to be more adaptable and transform to meet new customer demands with fewer resources and streamlining of its business activities. There is a growing move to introduce SOAs with their promise of cost-efficiency, agility, adaptability and legacy leverage. However there are many aspects of transforming an organization to use SOA and many obstacles and issues that the organization has to address when introducing SOAs. In this paper we outline some of the major aspects of SOA introduction and focus on some of the open issues that still need to be tackled. In the discussion on the various aspects of SOA introduction we focus in on performance and QoS which are major pieces to get right if the SOA implementation is to be successful. We outline some of the work that we are doing in this area and some problem areas where further research is needed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">476</field>
<field name="author">Jacky Keung</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Ross Jeffery</field>
<field name="title">Analogy-X: Providing Statistical Inference to Analogy Based Software Cost Estimation</field>
<field name="keyword">Cost estimation</field>
<field name="keyword"> Management</field>
<field name="keyword"> Statistical methods</field>
<field name="keyword"> Software Engineering</field>
<field name="abstract">Abstract Data-intensive analogy has been proposed as a means of software cost estimation as an alternative to other data intensive methods such as linear regression. Unfortunately, there are drawbacks to the method. There is no mechanism to assess its appropriateness for a specific dataset. In addition, heuristic algorithms are necessary to select the best set of variables and identify abnormal project cases. We introduce a solution to these problems based upon the use of the Mantel correlation randomization test called Analogy-X. We use the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. The method is demonstrated using the Desharnais dataset and two random datasets, showing (1) the use of Mantel s correlation to identify whether analogy is appropriate, (2) a stepwise procedure for feature selection, as well as (3) the use of a leverage statistic for sensitivity analysis that detects abnormal data points. Analogy-X, thus, provides a sound statistical basis for analogy, removes the need for heuristic search and greatly improves its algorithmic performance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">477</field>
<field name="author">Nicolas Van Wambeke</field>
<field name="author">Ernesto Exposito</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="title">Enhanced Transport Protocols</field>
<field name="abstract">The deployment of QoS network services and the entitled set of services offered by existing transport protocols have motivated the design of new transport protocols. In this chapter, we present a set of standardised transport protocols and advanced transport protocol mechanisms to support Quality of Service and satisfy new application requirements.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">478</field>
<field name="author">Abdul Babar</field>
<field name="author">Karl Cox</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Evaluating Three Map and B-SCP Integration Methods for Requirements Evolution Situation</field>
<field name="keyword">requirements engineering</field>
<field name="keyword"> integration</field>
<field name="keyword"> business value</field>
<field name="abstract">Integration of requirements engineering techniques has been common and proven beneficial. Map is a strategy-driven modeling technique that elicits requirements in terms of intentions and strategies. B-SCP is a framework to address alignment between requirements and business strategy. To address the problem of strategic requirements evolution in B-SCP, we have previously proposed the possibility of integrating Map with B-SCP. In this paper, we present three integration approaches for Map and B-SCP and evaluate their usefulness for two case studies: Seven Eleven Japan (SEJ) and CommSec Australia. SEJ presents an enterprise problem domain, while Commsec Australia presents a simple application domain. We find that each integration approach has advantages and disadvantages. We also conclude that the usefulness of each integration approach varies depending on the complexity of application domain and the nature of modelled requirements.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">479</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Patrick Senac</field>
<field name="title">Design, implementation and evaluation of a QoS-aware transport protocol</field>
<field name="keyword">Transport protocol</field>
<field name="keyword"> TFRC</field>
<field name="keyword"> SACK</field>
<field name="keyword"> QoS networks</field>
<field name="abstract">In the context of a reconfigurable transport protocol framework, we propose a QoS-aware Transport Protocol (QSTP), specifically designed to operate over QoS-enabled networks with bandwidth guarantee. QSTP combines QoS-aware TFRC congestion control mechanism, which takes into account the network-level bandwidth reservations, with a Selective ACKnowledgment (SACK) mechanism in order to provide a QoS-aware transport service that fill the gap between QoS enabled network services and QoS constraint applications. We have developed a prototype of this protocol in the user-space and conducted a large range of measurements to evaluate this proposal under various network conditions. Our results show that QSTP allows applications to reach their negotiated QoS over bandwidth guaranteed networks, such as DiffServ/AF network, where TCP fails. This protocol appears to be the first reliable protocol especially designed for QoS network architectures with bandwidth guarantee.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">480</field>
<field name="author">David Cock</field>
<field name="title">Bitfields and Tagged Unions in C Verification through Automatic Generation</field>
<field name="keyword">bitfields</field>
<field name="keyword"> Isabelle/HOL</field>
<field name="keyword"> refinement</field>
<field name="abstract">We present a tool for automatic generation of packed bitfields and tagged

unions for systems-level C, along with automatic, machine checked refinement

proofs in Isabelle/HOL. Our approach provides greater predictability than

compiler-specific bitfield implementations, and also provides a basis for

formal reasoning about these typically non-type-safe operations. The tool is

used in the implementation of the seL4 microkernel, and hence also in the

lowest-level refinement step of the L4.verified project which aims to prove the

functional correctness of seL4. In the seL4 implementation, it has eliminated

the need for unions entirely.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">481</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Ursula Goltz</field>
<field name="author">Jens-Wolfhard Schicke</field>
<field name="title">On Synchronous and Asynchronous Interaction in Distributed Systems</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> Petri nets</field>
<field name="keyword"> distributed systems</field>
<field name="keyword"> reactive systems</field>
<field name="keyword"> asynchronous interaction</field>
<field name="keyword"> semantic equivalences.</field>
<field name="abstract">When considering distributed systems, it is a central issue how to deal with interactions between components. In this paper, we investigate the paradigms of synchronous and asynchronous interaction in the context of distributed systems. We investigate to what extent or under which conditions synchronous interaction is a valid concept for specification and implementation of such systems. We choose Petri nets a our system model and consider different notions of distribution by associating locations to elements of nets. First, we investigate the concept of simultaneity which is inherent in the semantics of Petri nets when transitions have multiple input places. We assume that tokens may only be taken instantaneously by transitions on the same location. We exhibit a hierarchy of `asynchronous' Petri net classes by different assumptions on possible distributions. Alternatively, we assume that the synchronisations specified in a Petri net are crucial system properties. Hence transitions and their preplaces may no longer placed on separate locations. We then answer the question which systems may be implemented in a distributed way without restricting concurrency, assuming that locations are inherently sequential. It turns out that in both settings we find semi-structural properties of Petri nets describing exactly the problematic situations for interactions in distributed systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">482</field>
<field name="author">Andrew Zhang</field>
<field name="title">Reduced Rank Equalization for Precoded OFDM Systems</field>
<field name="abstract">In precoded OFDM systems, zero-forcing (ZF)

equalizers suffer from noise enhancement and error spreading

problems. A minimum mean square error (MMSE) equalizer

can mitigate the problems and improve system performance,

however, it has relatively higher complexity and is sensitive to the

estimation accuracy of noise variance. In this paper, based on the

principle of reduced-rank signal processing, we propose reducedrank

equalizers for precoded OFDM systems. These equalizers

can achieve performance close to MMSE equalizers while with

complexity similar to ZF equalizers.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">483</field>
<field name="author">Andrew Zhang</field>
<field name="author">Lin Luo</field>
<field name="author">Zhenning Shi</field>
<field name="title">Quadrature OFDMA Systems Based on Layered FFT STRUCTURE</field>
<field name="keyword">OFDMA</field>
<field name="keyword"> FFT</field>
<field name="keyword"> Multi-user Access</field>
<field name="abstract">Abstract In current OFDMA systems three major problems

arise due to the large number of subcarriers, including high peak

to average power ratio (PAPR), sensitivity to carrier frequency

offset (CFO), and high complexity in users terminals. In this

paper, based on an innovative concept of layered FFT structure,

we propose novel Quadrature OFDMA (Q-OFDMA) systems

which can overcome these problems. In particular, the proposed

systems can achieve the same guard-interval overhead and same

bandwidth occupation to conventional OFDMA systems, while

with reduced PAPR and improved CFO robustness and frequency

diversity. Q-OFDMA systems also promise low complexity in

downlink receivers. Parameter configuration is investigated for

both predefined and adaptive users data rates. Theoretical

comparison of bit error rate (BER) performance between QOFDMA

and OFDMA is conducted, and validated by simulation

results. It is shown that Q-OFDMA systems could achieve

better performance than OFDMA when signal to noise ratio

(SNR) is above a threshold depending on the channel condition,

and advanced equalizers, such as minimum mean square error

equalizer, can significantly decrease this threshold due to the

large frequency diversity in Q-OFDMA systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">484</field>
<field name="author">Ying Chen</field>
<field name="author">Andrew Zhang</field>
<field name="author"/>
<field name="title">New Training Sequence Structure for Zero-Padded SC-FDE System in Presence of Carrier Frequency Offset</field>
<field name="abstract">Frequency Domain Equalization (FDE) is an attractive solution for

wireless broadband transmission because of its strong capability

in handling multipath environment. However, frequency domain

channel estimation suffers from the inter-carrier

interference(ICI) caused by carrier frequency offset (CFO). In

this paper, we proposed new training sequence structure for

channel estimation to reduce the estimation errors caused by

residual CFO. The proposed new training sequence structure can be

easily obtained from any existing channel training sequences

without introducing significant changes to their original

property. Simulation results show that, this scheme can

efficiently reduce the channel estimation errors in FDE system,

and improve the system performance.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">485</field>
<field name="author">Lin Luo</field>
<field name="author">Andrew Zhang</field>
<field name="author">Zhenning Shi</field>
<field name="title">Iterative (Turbo) Joint Channel Estimation and Signal Detection for Quadrature OFDMA Systems</field>
<field name="keyword">Iterative</field>
<field name="keyword"> equation</field>
<field name="keyword"> channel estimation</field>
<field name="keyword"> Q-OFDMA</field>
<field name="abstract">Quadrature OFDMA (Q-OFDMA) systems have been recently proposed to reduce the complexity and peak-toaverage power ratio (PAPR), and improve carrier frequency offset (CFO) robustness for OFDMA systems. However, QOFDMA receiver obtains frequency diversity at the cost of noise enhancement. This paper proposes an iterative (turbo) equalization in conjunction with channel estimation for Q-OFDMA systems to mitigate the noise enhancement effect and improve the BER performance. In the proposed scheme, the channel estimation technique makes use of both training symbols and soft coded data information to suppress the inter-symbol interference (ISI) caused by channel estimation errors in Q-OFDMA systems. Simulation results show that performance improvement can be achieved with the proposed algorithms.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">486</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Sijun Lu</field>
<field name="author">Jian Zhang</field>
<field name="title">Camera Calibration using Moving Vehicles for Traffic Surveillance</field>
<field name="keyword">Camera Calibration</field>
<field name="keyword"> Vanishing Point</field>
<field name="keyword"> Vehicle Tracking</field>
<field name="keyword"> Image Straightening</field>
<field name="abstract">A statistical and computer vision approach using tracked moving vehicle shapes for auto-calibrating traffic surveillance cameras is presented. Various methods have been designed to estimate scene vanishing point and invertible transformation between real world and image coordinates. Results are validated against traditional methods in different traffic locations yield accurate results with much more flexibility and reliability.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">487</field>
<field name="author">Bernhard Hengst</field>
<field name="title">Partial Order Hierarchical Reinforcement Learning</field>
<field name="keyword">Machine Learning</field>
<field name="keyword"> Hierarchical reinforcement learning</field>
<field name="keyword"> automatic problem decomposition</field>
<field name="abstract">In this paper the notion of a partial-order plan is extended to

task-hierarchies. We introduce the concept of a partial-order

task-hierarchy that decomposes a problem using multi-tasking

actions. We go further and show how a problem can be automatically

decomposed into a partial-order task-hierarchy, and solved using

hierarchical reinforcement learning. The problem structure

determines the reduction in memory requirements and learning time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">488</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Semantic Information Retrieval in a Distributed Environment</field>
<field name="keyword">semantic</field>
<field name="keyword"> SPARQL</field>
<field name="keyword"> aggregation</field>
<field name="abstract">Efficient Information retrieval is the key goal of Semantic Web. This technology ameliorates the documents with metadata in a machine readable markup. Well known semantic search engines such as swoogle1 or SHOE2 use pre-fetched static indexing for efficiently retrieving the requested runtime documents. This paper presents a novel application dependent approach for improving the efficiency and retrieving the content documents based on an ASK type of SPARQL query in a distributed environment. We developed an initial prototype of the system and the performance curve for this approach is discussed in this paper. Observation on the results clearly shows that our approach receives favourable preliminary results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">489</field>
<field name="author">Jacky Keung</field>
<field name="title">Theoretical Maximum Prediction Accuracy for Analogy-based Software Cost Estimation</field>
<field name="keyword">Software Metrics and Measurement</field>
<field name="keyword"> Software Cost Estimation</field>
<field name="keyword"> Analogy</field>
<field name="keyword"> MMRE</field>
<field name="abstract">Software cost estimation is an important area of research in software engineering. Various cost estimation model evaluation criteria (such as MMRE, MdMRE etc.) have been developed for comparing prediction accuracy among cost estimation models. All of these metrics capture the residual difference between the predicted value and the actual value in the dataset, but ignoring the importance of the dataset quality itself. What is more, they are implicitly assuming the prediction model is able to predict with up to 100% accuracy at its maximum for a given dataset. Given that these prediction models barely provide an estimate based on observed historical data, absolute accuracy cannot be possibly achieved. 

It is therefore important to realize the theoretical maximum prediction accuracy for the given model with a given dataset. In this paper, we discuss the practical importance of this notion, and propose a novel method for its identification in the application of analogy-based software cost estimation. Specifically, we determine the theoretical maximum prediction accuracy of analogy using a unique dynamic K-NN approach to simulate and optimize the prediction system. The results of an empirical experiment show that our method is practical and important for researchers seeking to develop improved prediction models, because it offers an alternative for practical comparison between different prediction models</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">490</field>
<field name="author">Marc Benkert</field>
<field name="author">Bojan Djordjevic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Thomas Wolle</field>
<field name="title">Finding Popular Places</field>
<field name="keyword">Moving point objects; spatio</field>
<field name="keyword">temporal data; trajectories; movement pattern</field>
<field name="abstract">Widespread availability of location aware devices (such as GPS receivers) promotes capture of detailed movement trajectories of people, animals, vehicles and other moving objects. We investigate spatio-temporal movement patterns in large tracking data sets, i.e.~in large sets of polygonal paths. Specifically, we study so-called `popular places', that is, regions that are visited by many entities.



Given a set of polygonal paths with a total of $\bar{n}$ vertices, we look at the problem of computing such popular places in two different settings. For the discrete model, where only the vertices of the polygonal paths are considered, we propose an $O(\bar{n} \log \bar{n})$ algorithm; and for the continuous model, where also the straight line segments between the vertices of a polygonal path are considered, we develop an $O(\bar{n}^2)$ algorithm. We also present lower bounds and hardness results.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">491</field>
<field name="author">Basem Suleiman</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Integration of UML Modeling and Policy-Driven Management of Web Service Systems</field>
<field name="keyword">model-driven architecture</field>
<field name="keyword"> UML profile</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> business-driven IT management</field>
<field name="keyword"> XSLT</field>
<field name="abstract">We address the problems that Web service management

policies are not defined during Web service

design (they are usually defined later, during deployment

or run-time) and that management information

collected during run-time is not directly annotated on

designs to guide their improvements. We present novel

mechanisms for: (1) generation of WS-Policy4MASC

policies (for run-time Web service management) from

the corresponding UML profiles, (2) feedback of information

monitored during run-time (by MASC middleware)

into another set of UML diagram annotations.

The latter annotations show which design elements

have not performed well (in technical or financial

metrics) and service designers can use this information

for design analysis and re-engineering decisions.

We validated our mechanisms on prototypes using

Eclipse UML tools and XSLT processing, applied

to a stock trading Web service composition example.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">492</field>
<field name="author">Jenny Liu</field>
<field name="author">Simon Thuong</field>
<field name="author">Shiping Chen</field>
<field name="author">Liming Zhu</field>
<field name="title">Composing Adaptive Web Services on COTS Middleware</field>
<field name="keyword">software architecture</field>
<field name="keyword"> adaptation</field>
<field name="keyword"> middleware</field>
<field name="keyword"> QoS</field>
<field name="abstract">Composing adaptive and self-managing Web services

needs plug-and-play architecture so that the deployment of

control components does not require changes made to the

Web services and the host middleware platforms. This is

especially challenging for Web services running on COTS

middleware platforms, such as Microsoft .Net. In this paper,

we propose an architectural solution that introduces a management

proxy between adaptive control components and

Web services. The management proxy can be customized

and seamlessly integrated with a COTS middleware platform

by leveraging the existing middleware mechanisms.

This solution enables dynamically composing adaptive Web

services on COTS middleware without stopping its services.

We demonstrate this architecture by a realistic Web service

application built on .Net Windows Communication Foundation

(WCF). The performance overhead incurred by this

architecture is measured, and the results validate that our

solution is efficient in terms of performance and flexibility.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">493</field>
<field name="author">Xiwei (Sherry) Xu</field>
<field name="author">Liming Zhu</field>
<field name="author">Jenny Liu</field>
<field name="author">Mark Staples</field>
<field name="title">Resource-Oriented Architecture for Business Processes</field>
<field name="abstract">REpresentational State Transfer(REST) [3] is the set of design principles behind the World Wide Web (WWW). REST treats all entities in the world as link-connected resources, and supports a Resource-Oriented Architecture (ROA) for the design of applications. REST and ROA are responsible for many of the desirable quality attributes achieved in the WWW, such as loose-coupling (better adaptability) and interoperability. However, many exiting Web-based or service-oriented applications (WSDL/SOAP-based) only use WWW/HTTP as a tunneling protocol or abuse URL and POX (Plain Old XML) by encoding method semantics in them. These applications use fine-grained Remote Procedure Calls (RPC), breaking REST/ROA principles. We observe two kinds of challenges: 1) conceptually modelling process-intensive applications using a ROA promoted by the REST principles; and 2) practically decomposing a workflow-based business process into distributed, dynamic and RESTful process fragments. In this paper, we propose a ROA for business processes following the RESTful principles. We evaluate our approach by comparing it with current SOAP/WSDL/BPEL-driven approaches in terms of feasibility, process visibility, interoperability, and adaptability. We demonstrate that our approach better aligns process-intensive applications with the basic Web principles and promotes these desired quality attributes through dynamic and distributed process coordination.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">494</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Eranga Perera</field>
<field name="title">Context Aware Mobility Management</field>
<field name="keyword"/>
<field name="abstract">Context is any information that can enhance a computing system s relevance, timeliness and usefulness to the user. Recent research has been devoted to the use of context in a mobile environment, particularly in handling the mobility itself. This book chapter will start with defining what context is, how it is represented, and present a generalized system architecture. We then look at the problem of mobility in general and discuss existing solutions. Next we show how context can be leveraged to achieve more intelligent mobility management decisions. We highlight some of the research issues particular to context-aware mobility management and survey existing solutions. We argue that these solutions have not truly addressed these issues and present our own architecture for handling mobility.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">495</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">A Proposed Architecture for Context-Aware Mobility Management</field>
<field name="keyword"/>
<field name="abstract">Mobility management is one area where context awareness can be of great use. However, most context management systems have been developed for smart spaces and are not designed for highly mobile devices. We propose an architecture for context-aware mobility management that is both distributed and hierarchical. The architecture consists of a global context server, a number of local proxy and adaptation servers, and the mobile nodes. Our architecture is unique in that both the local servers and the mobile nodes make mobility decisions, instead of having all the intelligence reside on network side components or central servers only, as is the case in most context management systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">496</field>
<field name="author">Jacky Keung</field>
<field name="author">Ross Jeffery</field>
<field name="title">Automated Support for Software Cost Estimation using Web-CoBRA</field>
<field name="keyword">Software Metrics and Measurement</field>
<field name="keyword"> Software Cost Estimation</field>
<field name="abstract">Software cost estimation is a crucial yet very difficult task for a project manager at the very beginning of a new project. Since software projects are always different in nature, past projects may not necessarily cover all aspects of a new project when used as a basis for cost estimation. The CoBRA hybrid cost estimation technique uses expert knowledge to build a causal model of context-specific cost factors and past project data to predict costs in terms of effort as well as to assess the risks of a project. Further practical advantages of CoBRA are its high level of interpretability and its transparency. While our previous studies have shown that a modified CoBRA called Web-CoBRA produces higher prediction accuracy, the method was not fully adopted by our industry partner because of its complex application steps when it is manually performed. 

 In this paper, we report on our experiences with further automating Web-CoBRA based software cost estimation for a software company. It supports group decision-making processes by utilizing a wideband Delphi technique. We identify a range of problems when applying Web-CoBRA in the context of a software company and describe the approaches we used to solve these problems in our new tool called EffortWatch. 

 Furthermore, we report on the evaluation and the effectiveness of EffortWatch using a technology acceptance model (TAM) questionnaire. The result is then compared with a previous study, showing EffortWatch drastically improves the use of the Web-CoBRA technique.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">497</field>
<field name="author">Felix Werner</field>
<field name="author">Charles Gretton</field>
<field name="author">Frederic Maire</field>
<field name="author">Joaquin Sitte</field>
<field name="title">Induction of Topological Environment Maps from Sequences of Visited Places</field>
<field name="keyword">Environment mapping</field>
<field name="keyword"> stochastic local search</field>
<field name="keyword"> mobile robots</field>
<field name="abstract">In this paper we address the problem of topologically mapping environments which contain inherent perceptual aliasing caused by repeated environment structures. We propose an approach that does not use motion or odometric information but only a sequence of deterministic measurements observed by traversing an environment. Our algorithm implements a stochastic local search to build a small map which is consistent with local adjacency information extracted from a sequence of observations. Moreover, local adjacency information is incorporated to disambiguate places which are physically different but appear identical to the robots senses. Experiments show that the proposed method is capable of mapping environments with a high degree of perceptual aliasing, and that it infers a small map quickly.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">498</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Connectivity Properties of Wireless Multi-hop Networks</field>
<field name="abstract">Given a multi-hop network in which a total of n nodes are randomly and independently distributed in a unit square following a uniform distribution and each node has a uniform transmission range r(n), and two distinct nodes can directly communicate with each other if and only if their Euclidean distance is at most r(n), this paper investigates the characteristics of the minimum transmission range rc(n), at which the network is connected with a high probability. We show that for small values of n, r2

c (n) grows approximately linearly with 1/n ; and as n goes to infinity, r2 c (n) scales with logn/n . Simulations are performed to verify the theoretical analysis. The results of this paper are very useful in the design and dimensioning of wireless sensor networks and wireless ad hoc networks.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">499</field>
<field name="author">Anushiya Kannan</field>
<field name="author">Baris Fidan</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Robust Distributed Sensor Network Localization Based on Analysis of Flip Ambiguities</field>
<field name="keyword">flip ambiguity</field>
<field name="keyword"> sensor network localization</field>
<field name="keyword"> Localization performance</field>
<field name="abstract">A major problem in wireless sensor network localization is erroneous local geometric realizations in some parts of the network due to the sensitivity to certain distance measurement errors, which may in turn affect the reliability of the localization of the whole or a major portion of the sensor network. This phenomenon is well-described using the notion of flip ambiguity in rigid graph theory. In a recent study by the coauthors, an initial formal geometric analysis of flip ambiguity problems has been provided. The ultimate aim of that study was to quantify the likelihood of flip ambiguities in arbitrary sensor neighborhood geometries. In this paper we propose a more general robustness criterion to detect flip ambiguities in arbitrary sensor neighborhood geometries. This criterion enhances the recent study by the coauthors by removing the assumptions of accurately knowing some inter-sensor distances. The established robustness criterion is found to be useful in two aspects: (a) Analyzing the effects of flip ambiguity and (b) Enhancing the reliability of the location estimates of the prevailing localization algorithms by incorporating this robustness criterion to eliminate neighborhoods with flip ambiguity from being included in the localization.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">500</field>
<field name="author">renato iannella</field>
<field name="title">Emergency 2.0 - From Standards to Social Networking</field>
<field name="abstract">Information management strategies for Emergency Response are facing new challenges as the new emerging technologies of Web 2.0 and Social Networks provide innovative ways to access and share critical information.



Renato will also talk about the work and progress of the W3C Emergency Information Interoperability Framework Incubator Group which he co-chairs. This group is setting the information road map for web standards in response to new challenges with the emerging technologies of Web 2.0 and Social Networks that are providing innovative ways to access and share critical information during an emergency.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">501</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Alexander Fuchs</field>
<field name="author">Cesare Tinelli</field>
<field name="title">ME(LIA) - Model Evolution With Linear Integer Arithmetic Constraints</field>
<field name="abstract">Many applications of automated deduction require reasoning modulo

some form of integer arithmetic. Unfortunately, theory reasoning

support for the integers in current theorem provers is sometimes

too weak for practical purposes. In this paper we propose a novel

calculus for a large fragment of first-order logic modulo Linear

Integer Arithmetic (LIA) that overcomes several limitations of

existing theory reasoning approaches. The new calculus --- based

on the \emph{Model Evolution} calculus, a first-order logic

version of the propositional DPLL procedure --- supports

restricted quantifiers, requires only a decision procedure for

LIA-validity instead of a complete LIA-unification procedure, and

is amenable to strong redundancy criteria. We present a basic

version of the calculus and prove it sound and (refutationally)

complete.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">502</field>
<field name="author">Franz Baader</field>
<field name="author">Andreas Bauer</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Anne Cregan</field>
<field name="author">Alfredo Gabaldon</field>
<field name="author">Krystian Ji</field>
<field name="author">Kevin Lee</field>
<field name="author">David Rajaratnam</field>
<field name="author">Rolf Schwitter</field>
<field name="title">A Novel Architecture for Situation Awareness Systems</field>
<field name="keyword">Situation Awareness</field>
<field name="keyword"> Description Logics</field>
<field name="keyword"> Theorem proving</field>
<field name="abstract">Situation Awareness (SA) is the problem of comprehending elements

of an environment within a volume of time and space. It is a

crucial factor in decision-making in dynamic environments. The

current research challenge is to build systems that support

*higher-level* information fusion, viz., to integrate domain

specific knowledge and automatically draw conclusions that would

otherwise remain hidden or would have to be drawn by a human

operator. To address this challenge, we have developed a novel

system architecture and system implementation as part of the

*Situation Awareness by Inference and Logic* (SAIL) project. It

differs from other approaches by emphasizing the role of formal

logics and automated theorem provers in all its main

components. This supports a declarative approach to building SA

systems across different domains. The paper describes a

particular SAIL system and its architecture.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">503</field>
<field name="author">Duy Hoang Pham</field>
<field name="title">Efficient Representation and Effective Reasoning for Multi-Agent Systems</field>
<field name="abstract">In multi-agent systems, interactions between agents are often related to

cooperation or competition in such a fashion that they can fulfil their

tasks. Successful interactions often require agents to share common and unified

knowledge about their working environment. However, autonomous

agents observe and judge their surroundings by their

own view. Consequently, agents possibly have partial and sometimes

conflicting descriptions of the world. In scenarios where

they have to coordinate, they are required to identify the shared knowledge in the group and to be able to reason with

available information. This problem requires more sophisticated

modelling and reasoning methods, which is beyond the classical logics and monotonic reasoning. 



We introduce a formal framework based on Defeasible Logic (DL) to describe the knowledge commonly shared by agents, and that obtained from other agents. This enables an agent to efficiently reason about the environment and intentions of other agents given available information. We propose to extend the reasoning mechanism of DL with the \textit{superior knowledge}. This mechanism allows an agent to integrate its mental attitude with a more trustworthy source of information such as the knowledge shared by the majority of other agents.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">504</field>
<field name="author">Cahya Ong</field>
<field name="author">Sijun Lu</field>
<field name="author">Jian Zhang</field>
<field name="title">An Approach for Enhancing the Results of Detecting Foreground Objects and Their Moving Shadows in Surveillance Video</field>
<field name="keyword">shadow removal</field>
<field name="keyword"> foreground analysis</field>
<field name="keyword"> background modelling</field>
<field name="keyword"> holes removal</field>
<field name="abstract">Automated surveillance system is becoming increasingly important especially in the fields of computer vision and video processing. This paper describes a novel approach for improving the results of detecting foreground objects and their shadows in indoor image sequences. Several previous techniques have been developed in the literature that deal with moving shadows. However, a comparative evaluation of the existing approaches shows that most of the methods are unable to extract and preserve the shape of the moving objects completely. Since accurately detecting the moving objects from the background scene is a crucial step in such system, we then design a method to improve the detection results by appropriately filling the regions that are erroneously removed from foreground regions. We propose to combine the classification result of the moving objects, object representation based on the cardboard and head detection technique for performing the improvement task.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">505</field>
<field name="author">John Lim</field>
<field name="author">Nick Barnes</field>
<field name="title">Directions of Egomotion from Antipodal Points</field>
<field name="abstract">We present a novel geometrical constraint on the egomotion

of a single, moving camera. Using a camera with

a large field-of-view (FOV), the optical flow measured at

a single pair of antipodal points on the image sphere constrains

the set of all possible camera motion directions to a

subset region. By considering the flow at many such antipodal

point pairs, it is shown that the intersection of all subset

regions arising from each pair yields an estimate on the

directions of motion. These antipodal point constraints rely

on the geometrical properties of using a spherical representation

of the image as well as the larger information content

available from a large FOV. An algorithm using these constraints

was implemented and tested on both simulated and

real images. Results show comparable performance to the

state of the art in the presence of noise and outliers whilst

processing in constant time.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">506</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Dietmar Pfahl</field>
<field name="title">Software Process Simulation over Decade: Trends Discovery from A Systematic Review</field>
<field name="keyword">Software process simulation</field>
<field name="keyword"> systematic literature review</field>
<field name="abstract">Software Process Simulation (SPS) research has increased since 1998 when the &#12;rst ProSim Workshop was held. This paper aims to discover how SPS has evolved during the past 10 years based on the preliminary results from the systematic literature review of SPS publications from 1998 to 2007. Trends over the period showed that interest in continuous modelling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">507</field>
<field name="author">Yang Wang</field>
<field name="title">Joint Random Field Model for All-Weather Moving Vehicle Detection</field>
<field name="keyword">Contextual constraint</field>
<field name="keyword"> random field</field>
<field name="keyword"> vehicle detection.</field>
<field name="abstract">This paper proposes a joint random field (JRF) model for moving vehicle detection in video sequences. The JRF model extends the conditional random field (CRF) by introducing auxiliary latent variables to characterize the structure and evolution of visual scene. Hence detection labels (e.g. vehicle/roadway) and hidden variables (e.g. pixel intensity under shadow) are jointly estimated to enhance vehicle segmentation in video sequences. Data-dependent contextual constraints among both detection labels and latent variables are integrated during the detection process. The proposed method handles both moving cast shadows/lights and various weather/illumination conditions. Computationally efficient algorithm has been developed for real-time vehicle detection in video streams. Experimental results show that the approach effectively deals with various illumination conditions and robustly detects moving vehicles even in grayscale video.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">508</field>
<field name="author">Michael Maher</field>
<field name="author">Nina Narodytska</field>
<field name="author">Claude-Guy Quimper</field>
<field name="author">Toby Walsh</field>
<field name="title">Flow-Based Propagators for the SEQUENCE and Related Global Constraints</field>
<field name="abstract">We propose new filtering algorithms for the SEQUENCE constraint

and some extensions of the SEQUENCE constraint

based on

network flows. We enforce domain consistency

on the SEQUENCE constraint in O(n^2)

time down a branch of the search tree. This improves upon the best existing

domain consistency algorithm by a factor of O(log n).

The flows used in these algorithms are derived from a 

linear program. Some of them differ from the flows used to propagate global

constraints like GCC since the domains

of the variables are encoded as costs on 

the edges rather than capacities. 

Such flows

are efficient for maintaining bounds consistency over large domains

and may be useful for other global constraints.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">509</field>
<field name="author">Yang Wang</field>
<field name="author">Getian Ye</field>
<field name="title">Joint Random Fields for Moving Vehicle Detection</field>
<field name="abstract">This paper proposes a joint random field (JRF) model for moving vehicle detection in video sequences. The JRF model extends the conditional random field (CRF) by introducing auxiliary latent variables to characterize the structure and evolution of visual scene. Hence detection labels and hidden variables are jointly estimated to enhance vehicle segmentation in video sequences. Data-dependent contextual constraints among both detection labels and latent variables are integrated during the detection process. The proposed method handles both moving cast shadows/lights and background illumination variations. Computationally efficient algorithm has been developed for real-time vehicle detection in video streams. Experimental results show that the approach effectively deals with various illumination conditions and robustly detects moving vehicles even in grayscale video.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">510</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A NURBS-Based Spectral Reflectance Descriptor with Applications in Computer Vision and Pattern Recognition</field>
<field name="keyword">spectral reflectance</field>
<field name="keyword"> NURBS</field>
<field name="keyword"> descriptor</field>
<field name="keyword"> skin recognition</field>
<field name="keyword"> material classification</field>
<field name="abstract">In this paper, we present a surface reflectance descriptor

based on the control points resulting from the interpolation

of Non-Uniform Rational B-Spline (NURBS) curves to multispectral

reflectance data. The interpolation is based upon a knot 

removal scheme in the parameter domain. Thus, we exploit 

the local support of NURBS so as to recover a compact 

descriptor robust to noise and local perturbation of the spectra. 

We demonstrate the utility of our NURBS-based descriptor 

for material identification. To this end, we perform skin spectra 

recognition making use of a Support Vector Machine classifier. 

We also provide results on hyperspectral imagery and 

elaborate on the preprocessing step for skin segmentation. 

We compare our results with those obtained using an alternative 

descriptor.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">511</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Fast Pedestrian Detection Using a Cascade of Boosted Covariance Features</field>
<field name="abstract">Efficiently and accurately detecting pedestrians plays a very

 important role in many computer vision applications such as video

 surveillance and smart cars. In order to find the {\em right}

 feature for this task, we first present a comprehensive

 experimental study on pedestrian detection using state-of-the-art

 locally extracted features (local receptive fields, histogram of

 oriented gradients and region covariance).





 Building upon the findings of our experiments, we propose a new,

 simpler pedestrian detector using the covariance features. Unlike

 the work in \cite{Tuzel2007Human}, where the feature selection and

 weak classifier training are performed on the Riemannian manifold,

 we select features and train weak classifiers in the Euclidean

 space for faster computation. To this end, AdaBoost with weighted

 Fisher linear discriminant analysis based weak classifiers are

 designed. A cascaded classifier structure is constructed for

 efficiency in the detection phase. Experiments on different

 datasets prove that the new pedestrian detector is not only

 comparable to the state-of-the-art pedestrian detectors but it also

 performs at a faster speed.

 

 To further accelerate the detection, we adopt a faster

 strategy---multiple layer boosting with heterogeneous features---to

 exploit the efficiency of the Haar feature and the

 discriminative power of the covariance feature. Experiments show

 that by combining the Haar and covariance features, we speed up

 the original covariance feature detector \cite{Tuzel2007Human}

 by up to an order of magnitude in detection time 

 with a slight drop in detection performance.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">512</field>
<field name="author">Brian Lam</field>
<field name="author">Wai-Mei Lau</field>
<field name="author">Kam-Chung Leung</field>
<field name="title">vChina: A project on Learning in Video-Conferencing Environment in Distance Education</field>
<field name="keyword">Distance Education</field>
<field name="keyword"> Video-Conferencing Learning Environment</field>
<field name="keyword"> ICT in Education</field>
<field name="abstract">The use of information technology forms an integral part of primary and secondary education in many different parts of the world. In the past decade, as the price of software and hardware has become more affordable, there is an increasing interest in applying various communication technologies to support collaborative learning. The introduction of Video-Conferencing Learning Environment (VCLE) is a good example of applying communication technologies to support interactive distance education. This article reports on the experience of developing video-conferencing based education between some schools in Hong Kong and some schools in China. It also discusses some of its technical and pedagogical implications in implementing video-conferencing based education.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">513</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Combining Symmetry Breaking and Global Constraints</field>
<field name="keyword">constraint programming</field>
<field name="keyword"> global constraints</field>
<field name="keyword"> symmetry breaking</field>
<field name="abstract">We propose a new family of constraints which combine together lexicographical

ordering constraints for symmetry breaking with other common global

constraints. We give a general purpose propagator for this family of constraints,

and show how to improve its complexity by exploiting properties of the included

global constraints.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">514</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Constraint and Variable Ordering Heuristics for Compiling Configuration Problems</field>
<field name="keyword">constraint programming</field>
<field name="keyword"> binary decision diagram</field>
<field name="abstract">To facilitate interactive design, the solutions to configuration problems can be compiled into a decision

diagram. We develop three heuristics for reducing the time and space required to do this. These heuristics are based on the distinctive clustered and hierarchical structure of the constraint graphs of configuration problems. The first heuristic attempts to limit the growth in the size of the decision diagram

by providing an order in which constraints are added to the decision diagram. The second heuristic

provides an initial order for the variables within the decision diagram. Finally, the third heuristic

groups variables together so that they can be reordered by a dynamic variable reordering procedure

used during the construction of the decision diagram. These heuristics provide one to two orders

magnitude improvement in the time to compile a wide range of configuration.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">515</field>
<field name="author">Michael Maher</field>
<field name="author">Nina Narodytska</field>
<field name="author">Claude-Guy Quimper</field>
<field name="author">Toby Walsh</field>
<field name="title">Flow-Based Propagators for the SEQUENCE and Related Global Constraints</field>
<field name="keyword">constraint programming</field>
<field name="keyword"> global constraints</field>
<field name="abstract">We propose new filtering algorithms for the SEQUENCE

constraint and several extensions which are based on network flows.

Our propagator for the SEQUENCE constraint enforces domain consistency

in O(n^2) time down a branch of the search tree. This improves

upon the best existing domain consistency algorithm by a factor

of O(log n). The flows used in these algorithms are derived from

a linear program. Some of them differ from the flows used to propagate

global constraints like GCC since the domains of the variables

are encoded as costs on the edges rather than capacities. Such flows

are efficient for maintaining bounds consistency over large domains

and may be useful for other global arithmetic constraints.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">516</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Smith</field>
<field name="author">Daniel Lewis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Narrowband channel characterization for Body Area Networks</field>
<field name="keyword"/>
<field name="abstract">This document presents the results of narrowband on-body wireless channel measurements around the 900 and 2400 MHz ISM bands. The focus is on characterizing the effects of movement on the BAN channel.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">517</field>
<field name="author">Kee Siong Ng</field>
<field name="author">John Lloyd</field>
<field name="author">William Uther</field>
<field name="title">Probabilistic Modelling, Inference and Learning using Logical Theories</field>
<field name="abstract">This paper provides a study of probabilistic modelling, inference and learning in a logic-based setting.

We show how probability densities, being functions, can be represented and reasoned with naturally 

and directly in higher-order logic, an expressive formalism not unlike the (informal) everyday language 

of mathematics. We give efficient inference algorithms and illustrate the general approach with a diverse 

collection of applications. The problem of acquiring probabilistic theories in the context of agent systems 

is also considered.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">518</field>
<field name="author">Kazuhiro Inaba</field>
<field name="author">Sebastian Maneth</field>
<field name="title">The Complexity of Tree Transducer Output Languages</field>
<field name="keyword">Macro Tree Transducer</field>
<field name="keyword"> Formal Language</field>
<field name="keyword"> Context-sensitive</field>
<field name="keyword"> Automaton</field>
<field name="keyword"> Complexity</field>
<field name="abstract">Two complexity results are shown for the output languages

generated by compositions of macro tree transducers.

They are in the complexity class NSPACE(n)

and hence are context-sensitive,

and the class is NP-complete.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">519</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Gerwin Klein</field>
<field name="author">Philip Derrin</field>
<field name="author">Timothy Roscoe</field>
<field name="author">Gernot Heiser</field>
<field name="title">Towards a Practical, Verified Kernel</field>
<field name="keyword">seL4</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> verification</field>
<field name="abstract">In the paper we examine one of the issues in designing, specifying, implementing and formally verifying a small operating system kernel -- how to provide a productive and iterative development methodology for both operating system developers and formal methods practitioners.



We espouse the use of functional programming languages as a medium for prototyping that is readily amenable to formalisation with a low barrier to entry for kernel developers, and report early experience in the process of designing and building seL4: a new, practical, and formally verified microkernel.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">520</field>
<field name="author">Yu (David) Shi</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Ronnie Taib</field>
<field name="author">Fang Chen</field>
<field name="title">Designing Cognition-Adaptive Human Computer Interface for Mission-Critical Systems</field>
<field name="abstract">With applications of new information and communication technologies, computer-based information systems are becoming more and more sophisticated and com-plex. This is particularly true in large incident and emergency management sys-tems. The increasing complexity creates significant challenges to the design of user interfaces (UIs). One of the fundamental goals of UI design is to provide us-ers with intuitive and effective interaction channels to/from the computer system so that tasks are completed more efficiently and user s cognitive work load or stress is minimized. To achieve this goal, UI and information system designers should understand human cognitive process and its implications, and incorporate this knowledge into task design and interface design. In this paper we present the design of CAMI, a Cognition-Adaptive Multimodal Interface, for a large metro-politan traffic incident and emergency management system. The novelty of our design resides in combining complementary concepts and tools from Cognitive System Engineering and from Cognitive Load Theory. The key component of CAMI is a module that can detect and assess user s cognitive work load in real-time, which facilitates the delivery of dynamic and cognitively-adaptive user inter-face. We also present our work in building multimodal interfaces which hold promises for more intuitive and expressive interaction, and our use of Galvanic Skin Responses as a way to objectively assess cognitive load and evaluate cogni-tive efficiency of new interfaces.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">521</field>
<field name="author">Daniel Lewis</field>
<field name="title">IEEE 802.15.6 Call for Applications Summary</field>
<field name="abstract">Abstract



This document seeks to summarize significant applications (those with more then a single slide of detail) presented to the IEEE 802.16.6 TG / IEEE 802.15-BAN SG / IEEE 802.15 IG in the context of the categorization scheme developed by the group and represented in the applications matrix.



Purpose

(1) To collate identified 802.15.6 applications into a single design document which will be distributed with the call for proposals.

(2) To identify omitted or erroneous application information requiring updating before distribution with a call for proposals.

(3) To provide a single point of reference (and application reference numbers) which can be used as a common point of reference when evaluating and comparing call for proposal responses.

(4) Refine the applications matrix categorization scheme through use. (We have attempted to locate each significant presented application in the categorization scheme. Applications which appear out of place likely indicate a shortcoming in the categorization scheme which should be addressed through iteration and refinement of the document)</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">522</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Kostia Robert</field>
<field name="author">Sijun Lu</field>
<field name="author">Jian Zhang</field>
<field name="title">Vehicle classification at nighttime using Eigenspaces and Support Vector Machine</field>
<field name="keyword">vehicle classification</field>
<field name="keyword"> support vector machines</field>
<field name="keyword"> eigenspace</field>
<field name="abstract">A robust framework to classify vehicles in nighttime traffic

using vehicle eigenspaces and support vector machine is

presented. In this paper, a systematic approach has been

proposed and implemented to classify vehicles from roadside

camera video sequences. Collections of vehicle images

are analyzed to obtain their representative eigenspaces.

The model Support Vector Machine (SVM) built from those

vehicle spaces will then become a reliable classifier for any

unknown vehicle images. This approach has been implemented

and proven to be robust in both speed and accuracy

for vehicle classification at night.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">523</field>
<field name="author">Zhidong Li</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Thanes Wassantachat</field>
<field name="author">Jing Chen</field>
<field name="title">On Traffic Density Estimation with a Boosted SVM Classifier</field>
<field name="abstract">Traffic density and flow are important inputs to an intelligent transport system (ITS) to better manage traffic congestion. Presently, this is obtained through loop detectors (LD), traffic radars and surveillance cameras. However, installing loop detectors and traffic radars tends to be difficult and costly. Currently, a more popular way of circumventing this is to develop a sort of virtual loop detector (VLD) by using video content understanding technology to simulate the behavior of a loop detector to further estimate the traffic flow from a surveillance camera. But difficulties arise when attempting to obtain a reliable and real-time VLD under changing illumination and weather conditions. In this paper, we study the efficiency of using some informative features and the different combinations of the features to describe the traffic density and develop a real-time VLD by using a boosted SVM classifier to probabilistically determine the traffic density state. We show through extensive experiments that our proposed real-time VLD achieves an average accuracy at around 95% under various different illumination and weather conditions in daytime.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">524</field>
<field name="author">John Slaney</field>
<field name="title">Constraint Modelling: A Challenge for First Order Automated Reasoning</field>
<field name="keyword">Constraints</field>
<field name="keyword"> Modelling</field>
<field name="keyword"> G12</field>
<field name="keyword"> Deduction</field>
<field name="abstract">The process of modelling a problem in a form suitable for solution by constraint satisfaction or operations research techniques, as opposed to the process of solving it once formulated, requires a significant amount of reasoning. Contemporary modelling languages separate the first order description or "model'' from its grounding instantiation or "data''. Properties of the model independent of the data may thus be established by first order reasoning. In this talk, I survey the opportunities arising from this new application direction for automated deduction, and note some of the formidable obstacles in the way of a practically useful implementation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">525</field>
<field name="author">Sylvie Thiebaux</field>
<field name="author">Charles Gretton</field>
<field name="author">John Slaney</field>
<field name="author">David Price</field>
<field name="author">Froduald Kabanza</field>
<field name="title">Decision-Theoretic Planning with Non-Markovian Rewards</field>
<field name="keyword">Planning</field>
<field name="keyword"> Decision theory</field>
<field name="keyword"> Non-markovian</field>
<field name="abstract">A decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-Markovian rewards (NMRDP). In decision-theoretic planning, where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states, NMRDPs form a more

natural model than the commonly adopted fully Markovian decision process (MDP) model. While the more tractable solution methods developed for MDPs do not directly apply in the presence of non-Markovian rewards, a number of solution methods for NMRDPs have been proposed in the literature. These all exploit a compact specification of the non-Markovian reward function in temporal logic, to automatically translate the NMRDP into an equivalent MDP which is solved using efficient MDP solution methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process Planner), a software platform for the development and experimentation of methods for decision-theoretic planning with non-Markovian rewards. The current version of NMRDPP implements, under a single interface, a family of methods based on existing as well as new approaches which we describe in detail. These include dynamic programming, heuristic search, and structured methods. Using nmrdpp, we compare the methods and identify certain problem features that affect their performance. nmrdpp s treatment of non-Markovian rewards is inspired by the treatment of domain-specific search control knowledge in the TLPlan planner, which it incorporates as a special case. In the First International Probabilistic Planning Competition, NMRDPP was able to compete and perform well in both the domain-independent and hand-coded tracks, using search control knowledge in the latter.</field>
<field name="date">2014</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">526</field>
<field name="author">John Slaney</field>
<field name="title">Semipositive LTL with an Uninterpreted Past Operator</field>
<field name="keyword">Linear temporal logic</field>
<field name="keyword"> Decision processes</field>
<field name="abstract">$LTL is a version of linear temporal logic in which eventualities are not expressible, but in which there is a sentential constant $ intended to be true just at the end of some behaviour of interest - that is, to mark the end of the accepted (finite) words of some language. There is an effectively recognisable class of $LTL formulae which express behaviours, but in a sense different from the standard one of temporal logics like LTL or CTL. This representation is useful for solving a class of decision processes with temporally extended goals, which in turn are useful for representing an important class of AI planning problems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">527</field>
<field name="author">John Slaney</field>
<field name="title">Relevant Logic and Paraconsistency</field>
<field name="keyword">Paraconsistency</field>
<field name="keyword"> Relevant logic</field>
<field name="abstract">This is an account of the approach to paraconsistency associated with relevant logic. The logic fde of first degree entailments is shown to arise naturally out of the deeper concerns of relevant logic. The relationship between relevant logic and resolution, and especially the disjunctive syllogism, is then examined. The relevant refusal to validate these inferences is defended, and finally it is suggested that more needs to be done towards a satisfactory theory of when they may nonetheless safely be used.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">528</field>
<field name="author">John Slaney</field>
<field name="author">Arnold Binas</field>
<field name="author">David Price</field>
<field name="title">Guiding a Theorem Prover with Soft Constraints</field>
<field name="keyword"/>
<field name="abstract">Attempts to use finite models to guide the search for proofs by resolution and the like in first order logic all suffer from the need to trade off the expense of generating and maintaining models against the improvement in quality of guidance as investment in the semantic aspect of the reasoning is increased. Previous attempts to resolve this tradeoff have resulted either in poor selection of models, or in fragility as the search becomes over-sensitive to the order of clauses, or in extreme slowness. Here we present a fresh approach, whereby most of the clauses for which a model is sought are treated as soft constraints. The result is a partial model of all the clauses rather than an exact model of only a subset of them. This allows our system to combine the speed of maintaining just a single model with the robustness previously requiring multiple models. We present experimental evidence of benefits over a range of first order problem domains.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">529</field>
<field name="author">Vladimir Tosic</field>
<field name="title">On Modeling and Maximizing Business Value for Autonomic Service-Oriented Systems</field>
<field name="keyword">Web service</field>
<field name="keyword"> business value</field>
<field name="keyword"> Web service management</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> business-driven IT management</field>
<field name="keyword"> autonomic computing</field>
<field name="abstract">The existing Web service management solutions are almost exclusively focused on optimization of technical QoS metrics (e.g., availability). However, financial (e.g., profit) and other business value metrics (e.g., customer satisfaction) are more important than technical QoS metrics. We discuss the challenges of business value modeling and maximization in autonomic (e.g., self-healing) Web service management systems. In particular, we point out the need to model business strategies (e.g., maximizing customer satisfaction) and diverse types of business values (not only financial ones). To illustrate how some of these challenges can be addressed, we present the modeling of diverse business values and business strategies in our language WS-Policy4MASC and the corresponding algorithms for business value maximization in our middleware MASC (Manageable and Adaptable Service Compositions).</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">530</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Patrick Senac</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Michel Diaz</field>
<field name="title">A Peer-to-Peer Scheme to Discover and Select QoS Enhanced Alternate Paths</field>
<field name="abstract">Increasing number of communication softwares are built on distributed architectures based on the Peer-to-Peer (P2P) model, such as Skype, and PeerCast. This model provides several significant benefits, such as deployment scalability, resilience to single points of failure, communication cost sharing, and anonymity. In a former contribution, we proposed an unstructured Super-Peer architecture (SPAD) that further uses the P2P paradigm to provide enhanced Quality of Service (QoS) between two users of these communication softwares. SPAD allows the discovery and use of composite alternate end-to-end paths that experience better delay than the path given by the default IP routing mechanisms. This paper extends SPAD's capabilities to multi-QoS constrained alternate-paths. It presents two extensions: i) the capability to discover alternate paths with better packet-loss and/or delay, ii) multiple schemes to select among the discovered paths the ones that best meets the requirements of the users.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">531</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="title">Distributed Discovery and Management of Alternate Internet Paths with Enhanced Quality of Service</field>
<field name="abstract">The convergence of recent technology advances opens the way to new ubiquitous environments, where network-enabled devices collectively form invisible pervasive computing and networking environments around the users. These users increasingly require extensive applications and capabilities from these devices. Recent approaches propose that cooperating service providers, at the edge of the network, offer these required capabilities (i.e service), instead of having them directly provided by the devices. Thus, the network evolves from a plain communication medium into an endless source of services. Such a service, namely an overlay application, is composed of multiple distributed application elements, which cooperate via a dynamic communication mesh, namely an overlay association . The Quality of Service (QoS) perceived by the users of an overlay application greatly depends on the QoS on the communication paths of the corresponding overlay association. 



This thesis asserts and shows that it is possible to provide QoS to an overlay application by using alternate Internet paths resulting from the compositions of independent consecutive paths. Moreover, this thesis also demonstrates that it is possible to discover, select and compose these independent paths in a distributed manner within an community comprising a limited large number of autonomous cooperating peers, such as the fore-mentioned service providers. Thus, the main contributions of this thesis are i) a comprehensive description and QoS characteristic analysis of these composite alternate paths, and ii) an original architecture, termed SPAD (Super-Peer based Alternate path Discovery), which allows the discovery and selection of these alternate paths in a distributed manner. SPAD is a fully distributed system with no single point of failure, which can be easily and incrementally deployed on the current Internet. It empowers the end-users at the edge of the network, allowing them to directly discover and utilize alternate paths.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">532</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Claudio Bartolini</field>
<field name="author">Patrick C.K. Hung</field>
<field name="title">Management of Service-Oriented Implementations of Business Processes: From Quality of Service to Business Value</field>
<field name="keyword">Business process management</field>
<field name="keyword"> Web service management</field>
<field name="keyword"> monitoring</field>
<field name="keyword"> control</field>
<field name="keyword"> Quality of service (QoS)</field>
<field name="keyword"> business value</field>
<field name="keyword"> Web service description</field>
<field name="keyword"> contract</field>
<field name="keyword"> policy</field>
<field name="keyword"> service-oriented architecture (SOA)</field>
<field name="keyword"> tools for Web services</field>
<field name="abstract">Management (monitoring and control) of business processes is needed to ensure regular operation, attain or surpass the guaranteed quality of service (QoS), accommodate change, keep track of the consumed resources, and perform billing. Monitoring is used to measure QoS and/or business value attributes, while control is used to reactively/proactively ensure that the measured quantities are within desired (guaranteed) boundaries. To successfully perform management activities, a comprehensive specification of management goals is necessary. 

 Management of business processes can be viewed from several aspects and at several layers of granularity. In this tutorial, we will discuss monitoring and control of service-oriented implementations of business processes, with particular emphasis on QoS management and maximization of business value. That is, we will assume that services implementing business process activities are using Web service technologies such as SOAP and the Web Services Description Language (WSDL) and that they are composed using technologies such as the Web Services Business Process Execution Language (WSBPEL). By QoS we will mean technical metrics such as response time, throughput, and availability, while by business value we will mean both financial metrics such as prices, profit, and return on investment (ROI) and non-financial business metrics such as number of customers, market share, and customer satisfaction. 

 The tutorial will first clarify the importance of these topics and why the widely used basic Web service technologies are not enough. Then, it will explain theoretical principles for specification, monitoring, and control of QoS and business value attributes. Examples of these principles are contracts (including service level agreements SLAs), policies, intermediaries, probes, and multiple request queues. Next, it will provide a critical analysis of several important specification languages, research infrastructures, industrial products, and standardization efforts in this area. Currently there are many more results on management maximizing QoS than on management maximizing business value, but the latter promises better alignment between business and information technology (IT). Therefore, this tutorial will also present a brief introduction into business-driven IT management (BDIM) and will discuss possible approaches to extend QoS driven management solutions into business value driven management solutions. At the end, a number of open topics and resources for further study will be identified.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">533</field>
<field name="author">Dimosthenis Pediaditakis</field>
<field name="author">Sayed Hani Mohajerani</field>
<field name="author">Thanassis Boulis</field>
<field name="title">Poster Abstract: Castalia: the Difference of Accurate Simulation in WSN</field>
<field name="abstract">We present Castalia, a simulator for WSN that models many aspects of the WSN system and uses advanced models especially in terms of the channel and radio behaviour. We discuss the effects of these features in a nominally robust algorithm designed with a simpler simulator to show the importance of accurate modeling.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">534</field>
<field name="author">Iain Little</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">Probabilistic Planning Vs Replanning</field>
<field name="keyword">probabilistic planning</field>
<field name="keyword"> replanning</field>
<field name="keyword"> probabilistic planning competition</field>
<field name="abstract">A theoretical comparison of probabilistic planning and replanning techniques, in the context of the planning competition. Our main contribution is a baseline test for probabilistic interestingness, along with some examples of its application. We also attempt an analysis of the latest probabilistic competition problems, and suggest some improvements that could be made for future instances of the competition.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">535</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">The Power Supply Restoration Benchmark for ICKEPS-07</field>
<field name="keyword">planning</field>
<field name="keyword"> knowledge engineering</field>
<field name="keyword"> reconfiguration</field>
<field name="keyword"> power systems</field>
<field name="abstract">This document provides the material necessary to deal with the power supply restoration (PSR) benchmark (Thiebaux

&amp; Cordier 2001), as featured in the International Competition on Knowledge engineering for Planning &amp; Scheduling (ICKEPS-07).</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">536</field>
<field name="author">Blai Bonet</field>
<field name="author">Patrik Haslum</field>
<field name="author">Sarah Hickmott</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">Directed Unfolding of Petri Nets</field>
<field name="keyword">Petri nets</field>
<field name="keyword"> reachability analysis</field>
<field name="keyword"> unfolding</field>
<field name="keyword"> search</field>
<field name="keyword"> heuristics</field>
<field name="keyword"> planning</field>
<field name="abstract">The key to efficient on-the-fly reachability analysis based on unfolding is to focus the expansion of the finite prefix towards the desired marking. However, current unfolding strategies typically equate to blind (breadth-first) search. They do not exploit the knowledge of the marking that is sought, merely entertaining the hope that the road to it will be short. This paper investigates directed unfolding, which exploits problem-specific information in the form of a heuristic function to guide decisions to the desired marking. In the unfolding context, heuristic values are estimates of the distance between configurations. We show that suitable heuristics can be automatically extracted from the original net. We prove that unfolding can rely on heuristic search strategies while preserving the finiteness and completeness of the generated prefix, and in some cases, the optimality of the firing sequence produced. Experimental results demonstrate that directed unfolding scales up to problems that were previously out of reach of the unfolding technique.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">537</field>
<field name="author">Michael Norrish</field>
<field name="title">A Formal Semantics for C++</field>
<field name="abstract">This document describes a formal operational model of the dynamic semantics of much of the C++ language (as specified in the ISO Standard). The formal model was developed in the HOL theorem-prover, providing additional guarantees as to its good sense. This report presents and explains extracts from the mechanised source-code that was fed to HOL. 



This work was performed under funding from QinetiQ s Systems Assurance Group under the UK MOD Output 3a research project entitled Robust Languages.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">538</field>
<field name="author">Julien Hendrickx</field>
<field name="author">Brad Yu</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Rigidity and persistence for ensuring shape maintenance of multiagent meta formations</field>
<field name="abstract">This paper treats the problem of the merging

of formations, where the underlying model of a formation is

graphical. We first analyze the rigidity and persistence of metaformations,

which are formations obtained by connecting several

rigid or persistent formations. Persistence is a generalization

to directed graphs of the undirected notion of rigidity. In the

context of moving autonomous agent formations, persistence

characterizes the efficacy of a directed structure of unilateral

distance constraints seeking to preserve a formation shape. We

derive then, for agents evolving in a two- or three-dimensional

space, the conditions under which a set of persistent formations

can be merged into a persistent meta-formation, and give the

minimal number of interconnections needed for such a merging.

We also give conditions for a meta-formation obtained by merging

several persistent formations to be persistent.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">539</field>
<field name="author">Julien Hendrickx</field>
<field name="author">Baris Fidan</field>
<field name="author">Brad Yu</field>
<field name="author">Brian Anderson</field>
<field name="author">Vincent Blondel</field>
<field name="title">Formation re-organization by primitive operations on directed graphs</field>
<field name="abstract">In this paper, we study the construction and transformation

of 2-D persistent graphs. Persistence is a generalization

to directed graphs of the undirected notion of rigidity. Both notions

are currently being used in various studies on coordination

and control of autonomous multiagent formations. In the context

of mobile autonomous agent formations, persistence characterizes

the efficacy of a directed formation structure with unilateral distance

constraints seeking to preserve the shape of the formation.

Analogously to the powerful results about Henneberg sequences

in minimal rigidity theory, we propose different types of directed

graph operations allowing one to sequentially build any minimally

persistent graph (i.e., persistent graph with a minimal number of

edges for a given number of vertices), each intermediate graph being

also minimally persistent. We also consider the more generic

problem of obtaining oneminimally persistent graph fromanother,

which corresponds to the online reorganization of the sensing and

control architecture of an autonomous agent formation. We prove

that we can obtain any minimally persistent formation from any

other one by a sequence of elementary local operations such that

minimal persistence is preserved throughout the reorganization

process. Finally, we briefly explore how such transformations can

be performed in a decentralized way.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">540</field>
<field name="author">Adrian Bishop</field>
<field name="author">Brian Anderson</field>
<field name="author">Baris Fidan</field>
<field name="author">Pubudu Pathirana</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Bearing-only localization using geometrically constrained optimization</field>
<field name="abstract">In this paper we examine the problem of optimal

bearing-only localization of a single target using synchronous

measurements from multiple sensors. We approach the problem

by forming geometric relationships between the measured parameters

and their corresponding errors in the relevant emitter

localization scenarios. Specifically, we derive a geometric constraint

equation on the measurement errors in such a scenario.

Using this constraint, we formulate the localization task as a

constrained optimization problem that can be performed on

the measurements in order to provide the optimal values such

that the solution is consistent with the underlying geometry.

We illustrate and confirm the advantages of our approach

through simulation, offering detailed comparison with traditional

Maximum Likelihood estimation.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">541</field>
<field name="author">Adrian Bishop</field>
<field name="author">Baris Fidan</field>
<field name="author">Kutluyil Dogancay</field>
<field name="author">Brian Anderson</field>
<field name="author">Pubudu Pathirana</field>
<field name="title">Exploiting geometry for improved hybrid AOA/TDOA-based localization</field>
<field name="abstract">In this paper we examine the geometrically constrained optimization approach to localization with hybrid bearing (angle

of arrival, AOA) and time difference of arrival (TDOA) sensors. In particular, we formulate a constraint on the

measurement errors which is then used along with constraint-based optimization tools in order to estimate the maximum

likelihood values of the errors given an appropriate cost function. In particular we focus on deriving a localization

algorithm for stationary target localization in the so-called adverse localization geometries where the relative positioning of

the sensors and the target do not readily permit accurate or convergent localization using traditional approaches. We

illustrate this point via simulation and we compare our approach to a number of different techniques that are discussed in

the literature.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">542</field>
<field name="author">Baris Fidan</field>
<field name="author">Soura Dasgupta</field>
<field name="author">Brian Anderson</field>
<field name="title">Guaranteeing practical convergence in algorithms for sensor and source localization</field>
<field name="abstract">This paper considers localization of a source or

a sensor from distance measurements. We argue that linear

algorithms proposed for this purpose are susceptible to poor

noise performance. Instead given a set of sensors/anchors of

known positions and measured distances of the source/sensor

to be localized from them we propose a potentially non-convex

weighted cost function whose global minimum estimates the

location of the source/sensor one seeks. The contribution of

this paper is to provide nontrivial ellipsoidal and polytopic

regions surrounding these sensors/anchors of known positions,

such that if the object to be localized is in this region, localization

occurs by globally exponentially convergent gradient descent

in the noise free case. Exponential convergence in the noise

free case represents practical convergence as it ensures graceful

performance degradation in the presence of noise. These results

guide the deployment of sensors/anchors so that small subsets

can be made responsible for practical localization in geographical

areas determined by our approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">543</field>
<field name="author">Adrian Bishop</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="author">Kutluyil Dogancay</field>
<field name="author">Pubudu Pathirana</field>
<field name="title">Optimal range-difference based localization considering geometrical constraints</field>
<field name="abstract">This paper proposes a new type of algorithm aimed at finding the traditional maximum likelihood estimate of

the position of a target given time difference of arrival information, contaminated by noise. The novelty lies in the

fact that a performance index, akin to but not identical with that in maximum likelihood, is minimized subject to a

number of constraints, which flow from geometric constraints inherent in the underlying problem. The minimization

is in a higher dimensional space than for traditional maximum likelihood, and has the advantage that the algorithm

can be very straightforwardly and systematically initialized. Simulation evidence shows that failure to converge to a

solution of the localization problem in the vicinity of the true value is less likely to occur with this new algorithm

than with traditional maximum likelihood. This makes it attractive to use in adverse geometric situations.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">544</field>
<field name="author">Lachlan Lachlan.Blackhall</field>
<field name="author">Priscilla Priscilla.Kanjohn</field>
<field name="title">Model-Based Diagnosis of Hybrid Dynamical Networks for Fault Tolerant Control</field>
<field name="abstract">A method for determining (diagnosing) the operational mode of a

hybrid dynamical network using model based diagnosis is presented.

This methodology unifies discrete diagnosis capabilities with the

continuous time dynamics of hybrid dynamical networks. This method

is novel in that the diagnosis is not performed on the output

trajectory of the network but rather on the fundamental governing

dynamics of the network. This novelty allows for the diagnosis of

arbitrary switching events in the network. The chosen methodology

can be partially decentralised and provides optimal diagnoses even

when only a portion of the network can be observed. This offers

performance and implementation benefits for large distributed

networks. Computational results are presented for the case where the

mode changes are purely structural. That is, where only the

interconnection structure of the network is changing. The results

indicate that this methodology has some benefits for the

fault-tolerant control of hybrid dynamical networks.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">545</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="author">Hatem Hmam</field>
<field name="title">Self-localization of mobile agents in the plane</field>
<field name="abstract">This paper considers initially the problem of localizing

three agents moving in the plane when the inter-agent

distances are known, and in addition, the angle subtended at each

agent by lines drawn from two landmarks at known positions is

also known. In addition, it is shown that there are in general a

finite number greater than one of possible sets of positions for

the three agents. Later, generalization of the result for more than

three agents is presented.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">546</field>
<field name="author">Dirk van der Walle</field>
<field name="author">Baris Fidan</field>
<field name="author">Andrew Sutton</field>
<field name="author">Brad Yu</field>
<field name="author">Brian Anderson</field>
<field name="title">Non-hierarchical UAV formation control for surveillance tasks</field>
<field name="abstract">In this paper, we consider motion and formation

control of a team of three unmanned aerial vehicles (UAVs)

for a particular surveillance task. The UAVs are required to

fly in an equilateral triangle formation (to optimize target

location estimation accuracy), with the centre of mass following

a nominated (spiral) trajectory, which reflects the constraints

on the turning radius of the flight paths. Furthermore, the UAVs

need to fly at constant and nearly (but not necessarily exactly)

the same speeds. A decentralized control scheme is designed

and analyzed for the above motion and formation control tasks,

based on a non-hierarchical (i.e. three-coleader) sensing/control

structure.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">547</field>
<field name="author">Shaohao Zhai</field>
<field name="author">Baris Fidan</field>
<field name="title">Single view depth estimation based formation control of robotic swarms: Fundamental design and analysis</field>
<field name="abstract">This paper presents a practical formation

motion control scheme for robotic swarms based on

single view depth estimation. The single view depth

estimation for each robot in the swarm is performed

using a single non-sophisticated camera on the agent,

and the prior information about the heights of the

robots and other objects in the estimation. Here, a nonsophisticated

camera means one that has limited field of

view and limited resolution. First, the vision mechanism

is analyzed and a single view depth estimation scheme

is designed. Then a set of decentralized control laws, to

be incorporated with depth (distance) estimation scheme,

are introduced to move the robotic swarm in formation

from an arbitrary initial position to an arbitrary final

position without deforming the formation shape. The

robots do not have any global positioning sensors, and

they do not communicate with each other. The stability

and performance of the overall system are analyzed

mathematically.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">548</field>
<field name="author">Shaohao Zhai</field>
<field name="author">Baris Fidan</field>
<field name="author">Cagatay Ozturk</field>
<field name="author">Veysel Gazi</field>
<field name="title">Single view depth estimation based formation control of robotic swarms: Obstacle avoidance, simulation, and practical issues</field>
<field name="abstract">In a companion paper we have presented

a practical formation motion control scheme for robotic

swarms based on single view depth estimation. In this

paper we adapt this control scheme to the cases where

there are obstacles to be avoided in the region of interest.

First, a set of distributed control laws for the agents, to

be incorporated with depth (distance) estimation scheme,

are introduced to move the formation from an arbitrary

initial position to an arbitrary final position without

deforming the formation shape or having a collision

with an obstacle. Then, we present simulation results

on formation control using the proposed control scheme

for both obstacle free and with obstacle cases. We later

discuss certain practical issues regarding the proposed

scheme. The robots do not have any global positioning

sensors, and they do not communicate with each other.

The performance of the overall system as well as the

effects of delay and quantization in estimation of distance

are analyzed via simulations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">549</field>
<field name="author">Ilter Koksal</field>
<field name="author">Veysel Gazi</field>
<field name="author">Baris Fidan</field>
<field name="author">Raul Ordonez</field>
<field name="title">Tracking a maneuvering target with a non-holonomic agent using artificial potentials and sliding mode control</field>
<field name="abstract">In this article we consider tracking a maneuvering

target with a non-holonomic agent. The target and the agent

move in 2-dimensional space. The task is to capture/intercept

the moving target using a continuous time control scheme based

on artificial potentials and the sliding mode control technique.

The effectiveness of the proposed control scheme is established

analytically and demonstrated via a set of simulation results.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">550</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Reduction of self-localization errors in multi-agent autonomous formations</field>
<field name="abstract">This paper considers the problem of reduction of self-localization errors in multiagent

autonomous formations when only distance measurement is available to the agents in a

globally rigid formation. It is shown that there is a relationship between the singular values of a

matrix called reduced rigidity matrix and the error induced by measurement error on localization

solution. This fact is exploited to introduce an optimal selection of anchors, agents with exactly

known positions, which results in a small induced error by measurement errors on localization

solution. In the end, some simulation results are presented to demonstrate this optimal anchor

selection in globally rigid formations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">551</field>
<field name="author">Andrew Sutton</field>
<field name="author">Baris Fidan</field>
<field name="author">Dirk van der Walle</field>
<field name="title">Hierarchical UAV formation control for cooperative surveillance</field>
<field name="abstract">In this paper, we analyze the problem of rigidly maintaining a formation of three

unmanned aerial vehicles (UAVs), whilst surveying a region of interest following, as a team,

a particular pre-defined (spiral) trajectory. The UAVs in the formation are constrained to fly

at constant speeds and to maintain certain pre-defined inter-agent distances. A decentralized

proportional-integral (PI) control scheme (involving certain nonlinear switching terms) is

developed for the surveillance and formation maintenance tasks above, based on a hierarchical

(i.e. leader-first follower- second follower) sensing and control structure.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">552</field>
<field name="author">Brian Anderson</field>
<field name="author">Baris Fidan</field>
<field name="author">Brad Yu</field>
<field name="author">Dirk van der Walle</field>
<field name="title">UAV formation control: Theory and application</field>
<field name="abstract">Unmanned airborne vehicles (UAVs) are finding use in military operations and starting to 

find use in civilian operations. UAVs often fly in formation,

meaning that the distances between individual pairs of UAVs stay fixed, and the

formation of UAVs in a sense moves as a rigid entity. In order to maintain the shape

of a formation, it is enough to maintain the distance between a certain number of

the agent pairs; this will result in the distance between all pairs being constant. We

describe how to characterize the choice of agent pairs to secure this shape-preserving

property for a planar formation, and we describe decentralized control laws which

will stably restore the shape of a formation when the distances between nominated

agent pairs become unequal to their prescribed values. A mixture of graph theory,

nonlinear systems theory and linear algebra is relevant. We also consider a particular 

practical problem of flying a group of three UAVs in an equilateral triangle,

with the centre of mass following a nominated trajectory reflecting constraints on

turning radius, and with a requirement that the speeds of the UAVs are constant,

and nearly (but not necessarily exactly) equal.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">553</field>
<field name="author">Baris Fidan</field>
<field name="author">Soura Dasgupta</field>
<field name="author">Brian Anderson</field>
<field name="title">Realistic anchor positioning for sensor localization</field>
<field name="abstract">This paper considers localization of a source or a sensor from distance

measurements. We argue that linear algorithms proposed for this purpose are susceptible to poor noise performance. 

Instead given a set of sensors/anchors of known

positions and measured distances of the source/sensor to be localized from them

we propose a potentially non-convex weighted cost function whose global minimum

estimates the location of the source/sensor one seeks. The contribution of this pa-

per is to provide nontrivial ellipsoidal and polytopic regions surrounding these sensors/anchors 

of known positions, such that if the object to be localized is in this

region, localization occurs by globally exponentially convergent gradient descent in

the noise free case. Exponential convergence in the noise free case represents practical 

convergence as it ensures graceful performance degradation in the presence of

noise. These results guide the deployment of sensors/anchors so that small subsets

can be made responsible for practical localization in geographical areas determined

by our approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">554</field>
<field name="author">Ming Cao</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a Consensus in a Dynamically Changing Environment - A Graphical Approach</field>
<field name="keyword">cooperative control</field>
<field name="keyword"> graph theory</field>
<field name="keyword"> switched systems</field>
<field name="keyword"> multiagent systems</field>
<field name="abstract">This paper presents new graph-theoretic results appropriate for the analysis of a

variety of consensus problems cast in dynamically changing environments. The concepts of rooted,

strongly rooted, and neighbor-shared are defined, and conditions are derived for compositions of

sequences of directed graphs to be of these types. The graph of a stochastic matrix is defined, and

it is shown that under certain conditions the graph of a Sarymsakov matrix and a rooted graph

are one and the same. As an illustration of the use of the concepts developed in this paper, graphtheoretic

conditions are obtained which address the convergence question for the leaderless version

of the widely studied Vicsek consensus problem.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">555</field>
<field name="author">Ming Cao</field>
<field name="author">Stephen Morse</field>
<field name="author">Brian Anderson</field>
<field name="title">Reaching a Consensus in a Dynamically Changing Environment- convergence rates, measurement delays and asynchronous events</field>
<field name="keyword">cooperative control</field>
<field name="keyword"> graph theory</field>
<field name="keyword"> switched systems</field>
<field name="keyword"> convergence rates</field>
<field name="keyword"> delays</field>
<field name="abstract">This paper uses recently established properties of compositions of directed graphs

together with results from the theory of nonhomogeneous Markov chains to derive worst case convergence

rates for the headings of a group of mobile autonomous agents which arise in connection with

the widely studied Vicsek consensus problem. The paper also uses graph-theoretic constructions to

solve modified versions of the Vicsek problem in which there are measurement delays, asynchronous

events, or a group leader. In all three cases the conditions under which consensus is achieved prove

to be almost the same as the conditions under which consensus is achieved in the synchronous,

delay-free, leaderless case.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">556</field>
<field name="author">Andr Hergenhan</field>
<field name="author">Gernot Heiser</field>
<field name="title">Operating Systems Technology for Converged ECUs</field>
<field name="keyword">operating systems</field>
<field name="keyword"> virtualization</field>
<field name="keyword"> microkernels</field>
<field name="keyword"> convergence</field>
<field name="keyword"> electronic control units</field>
<field name="keyword"> infotainment</field>
<field name="abstract">We make a case for a convergence of infotainment with control/convenience functionality in automobiles. While AUTOSAR supports convergence of ECUs for control and convenience functions, it fails to support infotainment. We presented the COQOS software framework which supports this convergence, with the help of virtualization and microkernel technology.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">557</field>
<field name="author">Jukka Ylitalo</field>
<field name="author">Jan Melen</field>
<field name="author">Patrik Salmela</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="title">An Experimental Evaluation of a HIP based Network Mobility Scheme</field>
<field name="abstract">In this paper, the authors present and evaluate a network mobility

scheme based on Host Identity Protocol (HIP). The cryptographic host identifiers

are combined with an authorization mechanism and used for delegating the

mobility management signalling rights between nodes in the architecture. While

the delegation of the signalling rights scheme itself is a known concept, the trust

model presented in this paper differs from the MIPv6 NEMO solution. In the

presented approach, the mobile routers are authorized to send location updates

directly to peer hosts on behalf of the mobile hosts without opening the solution

for re-direction attacks. This is the first time the characteristics of the new

scheme is measured in the HIP moving network context using a real implementation.

The trust model makes it possible to support route optimization and minimize

over-the-air signalling and renumbering events in the moving network. The

measurements also reveal new kinds of anomalies in the protocol implementation

and design when data integrity and confidentiality protection are integrated into

signalling aggregation. The authors propose solutions for these anomalies.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">558</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Patrick Senac</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Michel Diaz</field>
<field name="title">A Proactive Scheme for QoS Enhanced Alternate Path Discovery in a Super-Peer Architecture</field>
<field name="abstract">In the next generation Internet, the network should evolve from a plain communication medium into an endless source of services available to the end-systems. We name these services Overlay Applications . They would be composed of multiple cooperative distributed application elements that would build dynamic communication mesh, namely Overlay Association . In a former contribution, we proposed an unstructured Super-Peer architecture (SPAD) that provides enhanced Quality of Service (QoS) between end-points within an Overlay Association. This architecture aims at discovering and utilizing composite alternate end-to-end paths that experience better QoS than the path given by the default IP routing mechanisms. This paper presents a proactive information dissemination scheme that complements SPAD s mechanisms and significantly improves its performances.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">559</field>
<field name="author">Petteri Nurmi</field>
<field name="author">Eemil Lagerspetz</field>
<field name="author">Wray Buntine</field>
<field name="author">Patrik Flore en</field>
<field name="author">Joonas Kukkonen</field>
<field name="author">Peter Peltonen</field>
<field name="title">Natural Language Retrieval of Grocery Products</field>
<field name="keyword">Grocery Retrieval</field>
<field name="keyword"> User Evaluation</field>
<field name="keyword"> Retrieval Models</field>
<field name="abstract">Grocery shopping is a fundamental everyday activity for 

most people. For many customers, a grocery list is central 

to the grocery shopping experience. Studies have suggested 

that the shopping list serves, e.g., as a memory aid, as a 

tool for budgeting, and as a way to efficiently organize the 

routine shopping visits. The way people write shopping lists 

contrasts with the way stores maintain product information. 

Namely, users tend to use natural language and generic descriptions, whereas stores tend to use highly structured and 

specific information about products. To bridge the semantic gap between customers and stores, we have developed an 

information retrieval system for grocery products. Our goal 

is to use this system as part of a mobile grocery assistant 

that allows users to create and manage shopping lists using 

natural language. The information retrieval system is then 

used to find products from the store that match the items in 

the customer s shopping list. In this paper we describe and 

evaluate our information retrieval system. We have developed the system using fourteen months of shopping basket 

data from a large Finnish supermarket. To evaluate the system, we gathered real shopping lists from customers of the 

supermarket, and performed a user evaluation of the system 

using these lists. The evaluation indicates that our system 

achieves over 80% precision at rank one, and the mean average precision is around 70% for the top ten query results. 

We also compare our system against an off-the-shelf information retrieval system, and show that our system significantly 

improves retrieval accuracy.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">560</field>
<field name="author">Renato iannella</field>
<field name="author">G.R. Gangadharan</field>
<field name="author">V D'Andrea</field>
<field name="author">M Weiss</field>
<field name="author">T Hong-Linh</field>
<field name="author">M Treiber</field>
<field name="author">S Dustdar</field>
<field name="title">Consumer-specified Service Licenses Selection and Composition</field>
<field name="abstract">Service oriented computing represents the convergence of technology with an understanding of cross-organizational business processes. A service license describes the terms and conditions for the use and access of the service in a machine interpretable way. Generally, a service provider defines individual services with corresponding service licenses which consumers have to follow. Often, service consumers are interested in selecting a service based on certain licensing terms and/or in composing individual services depending on their needs. Thus, consumer-specified licenses become pivotal in service compositionas this allows consumers to make a preference on what their service licenses should be and whether they can compose certain services together in a composition satisfying their specified licensing terms. In this paper, we propose an approach allowing service consumers to specify service licensing terms and select services that match licenses and implement this approach within a semi-automated service composition framework. Furthermore, we present a directional matchmaking algorithm to compare a consumer-specified service license with provider-specified service licenses and produce a composite service license satisfying the consumer-specified license</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">561</field>
<field name="author">Petteri Nurmi</field>
<field name="author">Eemil Lagerspetz</field>
<field name="author">Wray Buntine</field>
<field name="author">Patrik Flore en</field>
<field name="author">Joonas Kukkonen</field>
<field name="title">Product Retrieval for Grocery Stores</field>
<field name="keyword">Grocery Retrieval</field>
<field name="keyword"> User Evaluation</field>
<field name="keyword"> Retrieval Models</field>
<field name="abstract">Grocery shopping is a fundamental everyday situation for 

most urban people. For many customers, a grocery list is 

central to the grocery shopping experience. Studies have 

suggested that the shopping list serves, among other things, 

as a memory aid, as a tool for budgeting, and as a way 

to efficiently organize the routine shopping visits. The way 

people normally write shopping lists contrasts with the way 

stores maintain product information. Namely, users tend to 

use natural language and generic item descriptions whereas 

stores tend to use highly structured and specific information about products. To bridge the semantic gap between 

customers and stores, we have developed an information retrieval system for grocery products. We are planning to use 

this system as part of a mobile grocery assistant that allows 

users to create and manage shopping lists using natural language. The information retrieval system is then used to find 

products from the store that match the items in the customer s shopping list. In this paper we describe and evaluate the information retrieval system we have developed. 

We have developed the system using nine months worth of 

shopping basket data from a large Finnish supermarket. To 

evaluate the system, we gathered real shopping lists from 

customers of the supermarket, and performed a user evaluation of the system using these lists. Our results indicate 

that by intelligently combining well-established information 

retrieval techniques, it is possible to build a highly accurate 

grocery retrieval system, even for a language complicated 

from an information retrieval perspective.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">562</field>
<field name="author">Andreas Eidehall</field>
<field name="author">Lars Petersson</field>
<field name="title">Statistical threat assessment for general road scenes using Monte Carlo sampling</field>
<field name="keyword">Decision making</field>
<field name="keyword"> monte carlo</field>
<field name="keyword"> road vehicle safety</field>
<field name="keyword"> threat assessment</field>
<field name="abstract">This paper presents a threat-assessment algorithm for general road scenes. A road scene consists of a number of objects that are known, and the threat level of the scene is based on their current positions and velocities. The future driver inputs of the surrounding objects are unknown and are modeled as random variables. In order to capture realistic driver behavior, a dynamic driver model is implemented as a probabilistic prior, which computes the likelihood of a potential maneuver. A distribution of possible future scenarios can then be approximated using a Monte Carlo sampling. Based on this distribution, different threat measures can be computed, e.g., probability of collision or time to collision. Since the algorithm is based on the Monte Carlo sampling, it is computationally demanding, and several techniques are presented to increase performance without increasing computational load. The algorithm is intended both for online safety applications in a vehicle and for offline data analysis.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">563</field>
<field name="author">Tinne Tuytelaars</field>
<field name="author">Christoph Lampert</field>
<field name="author">Matthew Blaschko</field>
<field name="author">Wray Buntine</field>
<field name="title">Unsupervised Object Discovery: A comparison</field>
<field name="keyword">Object Discovery</field>
<field name="keyword"> Unsupervised Object Recognition</field>
<field name="keyword"> Evaluation</field>
<field name="abstract">The goal of this paper is to evaluate and

compare models and methods for learning to recognize

basic entities in images in an unsupervised setting. In

other words, we want to discover the objects present

in the images by analyzing unlabeled data and searching

for re-occurring patterns. We experiment with various

baseline methods, methods based on latent variable

models, as well as spectral clustering methods. The results

are presented and compared both on subsets of

Caltech256 and MSRC2 - datasets that are larger and

more challenging and that include more object classes

than what has previously been reported in the literature.

A rigorous framework for evaluating unsupervised

object discovery methods is proposed.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">564</field>
<field name="author">Ju Lynn Ong</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">Polyp detection in CT Colonography based on Shape Characteristics</field>
<field name="keyword">Image Shape Analysis</field>
<field name="keyword"> Probability Measures</field>
<field name="abstract">As an alternative procedure to the current methods which consider

only the mean values of shape features to globally characterize a

candidate shape polyps, probability density functions of some

feature variables constructed based on Gaussian and mean

curvatures are used to characterize the global shape of a

candidate lesion. The decision on whether or not this candidate

lesion is a polyp is made by comparing the density functions of

the considered shape feature variables to reference probability

densities of the same variables obtained from a pre-constructed

polyp/non polyp data base. The Kullback-Leibler divergence is used

as a dissimilarity measure to compare these probability densities

and make a decision based on closeness. Simulation results based

on real data are used to illustrate the effectiveness of the

proposed method in comparison to existing ones.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">565</field>
<field name="author">Robby Tan</field>
<field name="title">Visibility in Bad Weather from a Single Image</field>
<field name="abstract">Bad weather, such as fog and haze, can significantly de-

grade the visibility of a scene. Optically, this is due to the

substantial presence of particles in the atmosphere that ab-

sorb and scatter light. In computer vision, the absorption

and scattering processes are commonly modeled by a lin-

ear combination of the direct attenuation and the airlight.

Based on this model, a few methods have been proposed,

and most of them require multiple input images of a scene,

which have either different degrees of polarization or dif-

ferent atmospheric conditions. This requirement is the main

drawback of these methods, since in many situations, it is

difficult to be fulfilled. To resolve the problem, we introduce

an automated method that only requires a single input im-

age. This method is based on two basic observations: first,

images with enhanced visibility (or clear-day images) have

more contrast than images plagued by bad weather; second,

airlight whose variation mainly depends on the distance of

objects to the viewer, tends to be smooth. Relying on these

two observations, we develop a cost function in the frame-

work of Markov random fields, which can be efficiently op-

timized by various techniques, such as graph-cuts or belief

propagation. The method does not require the geometrical

information of the input image, and is applicable for both

color and gray images.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">566</field>
<field name="author">Gary Overett</field>
<field name="author">Lars Petersson</field>
<field name="author">Nathan Brewer</field>
<field name="author">Niklas Pettersson</field>
<field name="author">Lars Andersson</field>
<field name="title">A New Pedestrian Dataset for Supervised Learning</field>
<field name="keyword">pedestrian detection</field>
<field name="keyword"> pedestrian dataset</field>
<field name="abstract">This paper presents a comparative analysis of different pedestrian

dataset characteristics. The main goal of the research is to determine

what characteristics are desirable for improved training and

validation of pedestrian detectors and classifiers. The work focuses

on those aspects of the dataset which affect classification success

using the most common boosting methods.



Dataset characteristics such as image size, aspect ratio, geometric

variance and the relative scale of positive class instances

(pedestrians) within the training window form an integral part of

classification success. This paper will examine the effects of varying

these dataset characteristics with a view to determining the

recommended attributes of a high quality and challenging

dataset. While the primary focus is on characteristics of the positive

training dataset, some discussion of desirable attributes for the

negative dataset is important and is therefore included.



This paper also serves to publish our current pedestrian dataset in

various forms for non-commercial use by the scientific community. We

believe the published dataset to be one of the largest, most flexible,

and representative datasets available for pedestrian/person detection

tasks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">567</field>
<field name="author">Meena Jha</field>
<field name="author">Liam O'Brien</field>
<field name="author">Piyush Maheshwari</field>
<field name="title">Identify Issues and Concerns in Software Reuse</field>
<field name="keyword">Software Reuse</field>
<field name="keyword"> Legacy Software</field>
<field name="keyword"> Software Product Line</field>
<field name="abstract">Software reuse has been a challenge for the research community for many years. Studies have shown that software reuse is a critical aspect for organizations interested in the improvement of software development quality and productivity. However, even toady, with the idea of software product lines, there is still no clear consensus between software reuse development and its impact on quality and productivity. We have completed a survey on software reuse the focus of which is to identify issues and concerns in software reuse which may pave a path to develop a suitable software reuse process.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">568</field>
<field name="author">Mohammad Ali Abam</field>
<field name="author">Mark de Berg</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">A Simple and Efficient Kinetic Spanner</field>
<field name="keyword">computational geometry</field>
<field name="keyword"> kinetic algorithms</field>
<field name="keyword"> spanners</field>
<field name="abstract">We present a kinetic data structure for maintaining a (1+eps)-spanner of size O(n/eps^2) for a set of n moving points in the plane. Assuming the trajectories of the points can be described by polynomials whose degrees are at most s, the number of events processed by our structure is O((n/eps^2) l_{s+2}(n)), where l_{s+2}(n) denotes the maximum length of a (n,s+2)-Davenport-Schinzel sequence. Each event can be handled in O(1) time, plus O(log n) time to update the event queue.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">569</field>
<field name="author">Marc Benkert</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Herman Haverkort</field>
<field name="author">Alexander Wolff</field>
<field name="title">Constructing minimum-interference networks</field>
<field name="keyword">computaional geometry</field>
<field name="keyword"> approximation algorithms</field>
<field name="abstract">A wireless ad-hoc network can be represented as a graph in which the nodes represent wireless devices, and the links represent pairs of nodes

that communicate directly by means of radio signals. The interference caused by a link between two nodes u and v can be defined as the number

of other nodes that may be disturbed by the signals exchanged by u and v. Given the position of the nodes in the plane, links are to be chosen such

that the maximum interference caused by any link is limited and the network fulfills desirable properties such as connectivity, bounded dilation

or bounded link diameter. We give efficient algorithms to find the links in two models. In the first model, the signal sent by u to v reaches exactly

the nodes that are not farther from u than v is. In the second model, we assume that the boundary of a signal s reach is not known precisely and

that our algorithms should therefore be based on acceptable estimations. The latter model yields faster algorithms.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">570</field>
<field name="author">Anika Schumann</field>
<field name="author">Jinbo Huang</field>
<field name="title">A Scalable Jointree Algorithm for Diagnosability</field>
<field name="keyword">diagnosability</field>
<field name="keyword"> jointree</field>
<field name="abstract">Diagnosability is an essential property that determines how accurate any diagnostic reasoning can be on a system given any sequence of observations. An unobservable fault event in a discrete-event system is diagnosable iff its occurrence can always be deduced once sufficiently many subsequent observable events have occurred. A classical approach to diagnosability checking constructs a finite state machine known as a {\it twin plant} for the system, which has a {\it critical path} iff some fault event is not diagnosable. Recent work attempts to avoid the often impractical construction of the global twin plant by exploiting system structure. Specifically, local twin plants are constructed for components of the system, and synchronized with each other until diagnosability is decided. Unfortunately, synchronization of twin plants can remain a bottleneck for large systems; in the worst case, in particular, all local twin plants would be synchronized, again producing the global twin plant. We solve the diagnosability problem in a way that exploits the distributed nature of realistic systems. In our algorithm consistency among twin plants is achieved by message passing on a {\it jointree}. Scalability is significantly improved as the messages computed are generally much smaller than the synchronized product of the twin plants involved. Moreover we use an iterative procedure to search for a subset of the jointree that is sufficient to decide diagnosability. Finally, our algorithm is scalable in practice: it provides an approximate and useful solution if the computational resources are not sufficient.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">571</field>
<field name="author">Boris Aronov</field>
<field name="author">Mark de Berg</field>
<field name="author">Otfried Cheong</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Herman Haverkort</field>
<field name="author">Michiel Smid</field>
<field name="author">Antoine Vigneron</field>
<field name="title">Sparse geometric graphs with small dilation</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> approximation algorithms</field>
<field name="abstract">Given a set S of n points in R^D, and an integer k such that 0 &lt; k &lt; n, we show that a geometric graph with vertex set S, at most n _ 1 + k edges, maximum degree five, and dilation O(n/(k + 1)) can be computed in time O(n log n). For any k, we also construct planar n-point sets for which any geometric graph with n _ 1 + k edges has dilation (n/(k + 1)), a slightly weaker statement holds if the points of S are required to be in convex position.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">572</field>
<field name="author">Sajjad Siddiqi</field>
<field name="author">Jinbo Huang</field>
<field name="title">Probabilistic Sequential Diagnosis by Compilation</field>
<field name="keyword">diagnosis</field>
<field name="keyword"> compilation</field>
<field name="abstract">When a system behaves abnormally, a diagnosis is a set of

system components whose failure explains the abnormality.

It is known that compiling the system model into deterministic decomposable

negation normal form (d-DNNF) allows efficient computation of the

complete set of diagnoses. We extend this approach to

{\em sequential diagnosis}, where a sequence of measurements is

taken to narrow down the set of diagnoses until the actual faults

are identified. 

We propose novel probabilistic heuristics to reduce

the diagnostic cost, defined as the number of measurements.

Our heuristics involve the posterior probabilities of component failures and

the entropies of measurement points. The structure of the system is exploited so that a

joint probability distribution over the faults and system variables

is represented compactly as a Bayesian network, which is then

compiled into d-DNNF. All posterior probabilities required 

are computed exactly and efficiently by

traversing the d-DNNF. Finally, we scale the

approach further by performing the diagnosis in a hierarchical fashion.

Compared with the previous GDE framework, whose

heuristic involves the entropy over the set of diagnoses and 

estimated posterior probabilities, we avoid

the often impractical need to explicitly go through all diagnoses,

and are able to compute the probabilities exactly.

Experiments with ISCAS-85 circuits indicate that our approach can

solve for the first time a set of multiple-fault diagnostic cases on large circuits, with 

good performance in terms of diagnostic cost.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">573</field>
<field name="author">Mohammad Farshi</field>
<field name="author">Panos Giannopoulos</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Improving the stretch factor of a geometric graph by edge augmentation</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> approximation algorithms</field>
<field name="abstract">Given a Euclidean graph G in R^d with n vertices and m edges, we consider the problem of adding an edge to G such that the stretch factor of the resulting graph is minimized. Currently, the fastest algorithm for computing the stretch factor of a graph with positive edge weights runs in O(nm+n^2 log n) time, resulting in a trivial O(n^3m+n^4 log n) time algorithm for computing the optimal edge. First, we show that a simple modi cation yields the optimal solution in O(n^4) time using O(n^2) space. To reduce the running time we consider several approximation algorithms.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">574</field>
<field name="author">Jinbo Huang</field>
<field name="title">Universal Booleanization of Constraint Models</field>
<field name="abstract">While the efficiency and scalability of modern SAT technology offers

an intriguing alternative approach to constraint solving via translation to SAT,

previous work has mostly focused on the translation of specific types

of constraints, such as pseudo Boolean constraints, finite

integer linear constraints, and constraints given as explicit listings of allowed tuples. 

By contrast, we present a translation 

of constraint models to SAT

at language level, using the recently proposed constraint modeling language

MiniZinc, such that any satisfaction or optimization problem written in the language

(not involving floats)

can be automatically Booleanized and solved by one or more calls to a SAT

solver. We discuss the strengths and weaknesses of such a universal

constraint solver, and report on a large-scale empirical evaluation of it against two existing 

solvers for MiniZinc: the finite domain solver distributed with MiniZinc and one 

based on the Gecode constraint programming platform. Our results indicate

that Booleanization indeed offers a competitive alternative, exhibiting superior 

performance on some classes of problems involving large numbers of constraints and 

complex integer arithmetic,

in addition to, naturally, problems that are already largely Boolean.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">575</field>
<field name="author">Gary Overett</field>
<field name="author">Lars Petersson</field>
<field name="title">On the Importance of Accurate Weak Classifier Learning for Boosted Weak Classifiers</field>
<field name="keyword">weak classifiers</field>
<field name="keyword"> boosting</field>
<field name="abstract">Recent work has shown that improving model

learning for weak classifiers can yield significant gains in the

overall accuracy of a boosted classifier. However, most published

classifier boosting research relies only on rudimentary learning

techniques for weak classifiers. So while it is known that improving

the model learning can greatly improve the accuracy of the resulting

strong classifier, it remains to be shown how much can yet be gained

by further improving the model learning at the weak classifier

level. This paper derives a very accurate model learning method for

weak classifiers based on the popular Haar-like features and presents

an investigation of its usefulness compared to the standard and

recent approaches. The accuracy of the new method is shown by

demonstrating the new models ability to predict ROC performance on

validation data. A discussion of the problems in learning accurate

weak hypotheses is given, along with example solutions. It is also

shown that a previous simpler method can be further improved. Lastly,

we show that improving model accuracy does not continue to yield

improved overall classification beyond a certain point. At this point

the learning technique, in this case RealBoost, is unable to make

gains from the improved model data.



The method has been tested on pedestrian detection tasks using

classifiers boosted using the RealBoost boosting algorithm. A subset

of our most interesting results is shown to demonstrate the value of

method.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">576</field>
<field name="author">Zhidong Li</field>
<field name="author">Jing Chen</field>
<field name="author">Adrian Chong</field>
<field name="title">Using Stochastic Gradient-Descent Scheme in Appearance Model Based Face Tracking</field>
<field name="keyword">Object tracking</field>
<field name="keyword"> Mean-shift</field>
<field name="abstract">Active appearance model (AAM) has been widely used in face tracking and recognition. However, accuracy and efficiency are always two main challenges with the AAM search. The paper therefore proposes a fast appearance-model based 3D face tracking algorithm to track a face appearance with significant translation, rotation, and scaling activities by using stochastic meta-descent (SMD) optimization scheme to accelerate the appearance model search and to improve the tracking efficiency and accuracy. The proposed algorithm constructs an active face appearance model by using several semantic landmark points extracted from each frame and then processes the appearance model search to approximate the model translating, rotating, and scaling by using the SMD filter to minimize the appearance difference between the current model and the new observation. We compare the results with both a conventional AAM and a Camshift filter and find that our algorithm outperforms both two in terms of efficiency and accuracy in tracking a fast moving, rotating, and scaling face object in a video sequence.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">577</field>
<field name="author">Zhidong Li</field>
<field name="author">Jing Chen</field>
<field name="author">Nicol N. Schraudolph</field>
<field name="title">An Improved Mean-Shift Tracker with Kernel Prediction and Scale Optimization Targeting for Low-Frame-Rate Video Tracking</field>
<field name="keyword">Object tracking</field>
<field name="keyword"> Mean-shift</field>
<field name="keyword"> multi-kernel</field>
<field name="abstract">The mean-shift (MS) algorithm is widely used in object tracking because of its speed and simplicity. However, it assumes certain overlap of object appearance and smooth change in object scale between consecutive video frames. This assumption is usually violated in a low-frame-rate (LFR) video, which contains fast motion and scale changes. An LFR video is widely adopted in applications such as surveillance systems, where real-time object tracking is highly desirable but the traditional MS algorithm does not perform well. We addressed this problem by proposing a novel and enhanced mean-shift tracker, named SMDShift, that uses kernel prediction and Stochastic Meta-Descent (SMD) optimization method to deal with the kernel position and scale variation when tracking objects in an LFR video. In our experiments, the SMDShift can track fast moving objects with significant scale change in an LFR video sequence on which the traditional mean-shift and Camshift algorithms fail.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">578</field>
<field name="author">Engelhardt Kai</field>
<field name="author">Ralf Huuck</field>
<field name="title">Smaller Abstraction for ACTL* Without Next</field>
<field name="abstract">The successful application of model-checking to large systems relies

 strongly on the choice of good abstractions. In this work we present

 an approach for constructing abstractions when checking next-free

 universal ACTL* properties. It is known that functional

 abstractions are safe and that next-free universal ACTL* is

 insensitive to finite stuttering. We exploit these results by

 introducing a safe next-free abstraction} that

 is typically smaller than the usual functional one while at the same

 time more precise, i.e., it has less spurious counter-examples.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">579</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Ralf Huuck</field>
<field name="author">Sean Seefried</field>
<field name="title">Counterexample Guided Path Reduction for Static Program Analysis</field>
<field name="abstract">In this work we introduce counterexample guided path reduction based on interval constraint solving for static program analysis. The aim of this technique is to reduce the number of false positives by reducing the number of feasible paths in the abstraction iteratively. Given a counterexample, a set of observers is computed which exclude infeasible paths in the next iteration. This approach combines ideas from counterexample guided abstraction refinement for software verification with static analysis techniques that employ interval constraint solving. The advantage is that the analysis becomes less conservative than static analysis, while it benefits from the fact that interval constraint solving deals naturally with loops. We demonstrate that the proposed approach is effective in reducing the number of false positives, and compare it to other static checkers for C/C++ program analysis.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">580</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Ralf Huuck</field>
<field name="author">Rauch Felix</field>
<field name="author">Sean Seefried</field>
<field name="title">Some Assembly Required -- Program Analysis of Embedded System Code</field>
<field name="abstract">Programming embedded system software typically involves more than

 one programming language. Normally, a high-level language such as

 C/C++ is used for application oriented tasks and a low-level

 assembly language for direct interaction with the underlying

 hardware. In most cases those languages are closely interwoven and

 the assembly is embedded in the C/C++ code. Verification of such

 programs requires the integrated analysis of both languages

 at the same time. However, common algorithmic verification tools

 fail to address this issue. In this work we present a model-checking

 based static analysis approach which seamlessly integrates the

 analysis of embedded ARM assembly with C/C++ code analysis. In

 particular, we show how to automatically check

 that the ARM code complies to its interface descriptions. Given interface compliance, we then provide an extended analysis framework for checking general properties of ARM

 code. We implemented this analysis in our source code analysis tool Goanna, and applied to the source code of an L4 micro kernel implementation.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">581</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Ralf Huuck</field>
<field name="author">Sean Seefried</field>
<field name="author">Brauer Joerg</field>
<field name="title">Goanna: Syntactic Software Model Checking</field>
<field name="abstract">Goanna is an industrial-strength static analysis tool used in academia and industry alike to find bugs in C/C++ programs. Unlike existing approaches Goanna uses the off-the-shelf NuSMV model checker as its core analysis engine on a syntactic flow-sensitive program abstraction. The CTL-based model checking approach enables a high degree of flexibility in writing checks, scales to large number of checks, and can scale to large code bases. Moreover, the tool incorporates techniques from constraint solving, classical data flow analysis and a CEGAR inspired counterexample based path reduction. In this paper we describe Goanna's core technology, its features and the relevant techniques, as well as our experiences of using Goanna on large code bases such as the Firefox web browser.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">582</field>
<field name="author">Kee Siong Ng</field>
<field name="author">John Lloyd</field>
<field name="title">Learning Modal Theories</field>
<field name="abstract">This paper discusses how to learn theories that are modal, concentrating on the issue of how

modal hypotheses are formed. Illustrations are given to show the usefulness of the ideas for

agent applications.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">583</field>
<field name="author">Kee Siong Ng</field>
<field name="title">(Agnostic) PAC Learning Concepts in Higher-order Logic</field>
<field name="abstract">This paper studies the PAC and agnostic PAC learnability of some

standard function classes in the learning in higher-order logic

setting introduced by Lloyd et al.

In particular, it is shown that the similarity between

learning in higher-order logic and traditional attribute-value

learning allows many results from computational learning theory to 

be `ported' to the logical setting with ease.

As a direct consequence, a number of non-trivial results in the

higher-order setting can be established with straightforward proofs.

Our satisfyingly simple analysis provides another case for a more

in-depth study and wider uptake of the proposed higher-order

logic approach to symbolic machine learning.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">584</field>
<field name="author">Jing Chen</field>
<field name="author">Jiali Shen</field>
<field name="author">Jian Zhang</field>
<field name="author">Kiandy Wangsa</field>
<field name="title">A Novel Multimedia Database System for Efficient Image/Video Retrieval Based on Hybrid-Tree Structure</field>
<field name="keyword">Multimedia database</field>
<field name="keyword"> hybrid-tree</field>
<field name="keyword"> PCA</field>
<field name="keyword"> temporal &amp; spatial indexing</field>
<field name="abstract">With recent advances in computer vision, image processing and analysis, a retrieval process based on visual content has became a key component in achieving high efficiency image query for large multimedia databases. In this paper, we propose and develop a novel indexing mechanism built on the top of a Xindice database targeted at supporting quick access to image/video data along with strong retrieval functionalities for those high-dimensional metadata. The proposed database applies a native XML database as a backbone, uses hybrid-tree as its database structure, and performs Threshold Algorithm (TA) to conduct similarity search based on a random combination of image features. The database also sketches an index scheme in building a mapping between temporal and spatial domains so as to support both temporal and spatial queries at a higher speed search over the traditional pure temporal or spatial based indexing methods. The simulation results demonstrate that the proposed database system can conduct a more efficiently search activities supporting queries that involve composite features with arbitrary Boolean combinations and can outperform a traditional database employing a linear scanning search scheme.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">585</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Jing Chen</field>
<field name="title">Towards a Stringent Bit-Rate Conformance for Frame-Layer Rate Control in H.264/AVC</field>
<field name="keyword">bit allocation</field>
<field name="keyword"> complexity measure</field>
<field name="keyword"> frame-layer</field>
<field name="keyword"> H.264/AVC</field>
<field name="keyword"> linear R-Q model</field>
<field name="keyword"> rate control</field>
<field name="abstract">This paper presents a novel frame-layer rate control technique that adaptively determines the frame complexity for bit allocation in order to satisfy the target bit-rate constraints without degrading the decoded video significantly. To do this, we first obtain the edge energy of each frame to measure the frame complexity as well as to determine the weighting of a frame for bit allocation. We then present a new bit-rate traffic model for bit allocation to achieve a better conformance to the target bit-rate. Finally, we. integrate the edge energy complexity measure into the rate-quantization (R-Q) model. Our results shows robust improvements over the current rate control methods adopted in H.264/AVC in terms of meeting the target bit-rate as well as determining the quality of the decoded video.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">586</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="author">Jing Chen</field>
<field name="title">Vehicular Traffic Density Estimation via Statistical Methods with Automated State Learning</field>
<field name="abstract">This paper proposes a novel approach of combining an unsupervised clustering scheme called AutoClass with Hidden Markov Models (HMMs) to determine the traffic density state in a Region Of Interest (ROI) of a road in a traffic video. Firstly, low-level features are extracted from the ROI of each frame. Secondly, an unsupervised clustering algorithm called AutoClass is then applied to the low-level features to obtain a set of clusters for each pre-defined traffic density state. Finally, four HMM models are constructed for each traffic state respectively with each cluster corresponding to a state in the HMM and the structure of HMM is determined based on the cluster information. This approach improves over previous approaches that used Gaussian Mixture HMMs (GMHMM) by circumventing the need to make an arbitrary choice on the structure of the HMM as well as determining the number of mixtures used for each density traffic state. The results show that this approach can classify the traffic density in a ROI of a traffic video accurately with the property of being able to handle the varying illumination elegantly.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">587</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Optimal Solution of the Dichromatic Model for Multispectral Photometric Invariance</field>
<field name="keyword">photometric invariance</field>
<field name="keyword"> multispectral imaging</field>
<field name="keyword"> dichromatic model</field>
<field name="keyword"> convex optimisation</field>
<field name="keyword"> reflectance</field>
<field name="abstract">In this paper, we address the problem of photometric invariance in

multispectral imaging making use of an optimisation approach based upon the

dichromatic model. In this manner, we can cast the problem of recovering the

spectra of the illuminant, the surface reflectance and the shading and specular

factors in a structural optimisation setting. Making use of the additional information

provided by multispectral imaging and the structure of image patches, we

recover the dichromatic parameters of the scene. Thus, we formulate a target cost

function combining the dichromatic error and the smoothness priors for surfaces

with uniform reflectance. The dichromatic parameters are recovered through minimising

this cost function in a coordinate descent manner. To this end, we make

use of regular albedo patches in the scene selected automatically. The algorithm

is shown to be robust, and since the cost function is convex in nature, the resulting

solution is globally optimal. We illustrate the effectiveness of our method for

purposes of illuminant spectrum recovery and skin recognition.We also compare

our results to a number of alternatives.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">588</field>
<field name="author">Stephen Herborn</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Dial M for Middlebox Managed Mobility</field>
<field name="keyword">Mobility Management</field>
<field name="keyword"> Identity Delegation</field>
<field name="keyword"> Middleboxes</field>
<field name="abstract">Users may be served by multiple network enabled

terminal devices, each of which may in turn have multiple

network interfaces. This multi-homing at both user and device 

level presents new opportunities for mobility handling. Mobility

may be handled by utilising devices, i.e. middleboxes, that are

able to provide intermediary routing or adaptation services.

This paper presents an approach to enable this kind of

mobility handling via the concept of Personal Networks (PN).

Personal Networks (PN), consist of dynamic conglomerations of

terminal and middlebox devices tasked to facilitate the delivery

of information to and from a single human user. This concept

creates the potential to view mobility handling as a path selection

problem, since there may be multiple valid terminal device and

middlebox configurations that can successfully carry a given

communication session.

We present details and evaluation of our approach, based on

an extension of the Host Identity Protocol, that demonstrate its

simplicity and effectiveness.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">589</field>
<field name="author">David Billington</field>
<field name="author">Grigoris Antoniou</field>
<field name="author">Guido Governatori</field>
<field name="author">Michael Maher</field>
<field name="title">An Inclusion Theorem for Defeasible Logic</field>
<field name="abstract">Defeasible reasoning is a computationally simple nonmonotonic reasoning approach that has attracted significant theoretical and practical attention. It comprises a family of logics that capture different intuitions, among them ambiguity propagation versus ambiguity blocking, and the adoption or rejection of team defeat. This paper provides a compact presentation of the defeasible logic variants, and derives an Inclusion Theorem which shows that different notions of provability in defeasible logic form a chain of levels of proof, while they al exhibit a basic coherence property.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">590</field>
<field name="author">Jin Yu</field>
<field name="author">Vishwanathan S.V.N.</field>
<field name="author">Simon G _nter</field>
<field name="author">Nicol N. Schraudolph</field>
<field name="title">A Quasi-Newton Approach to Nonsmooth Convex Optimization</field>
<field name="keyword">BFGS</field>
<field name="keyword"> Subgradient</field>
<field name="keyword"> Wolfe Conditions</field>
<field name="keyword"> Bundle Methods</field>
<field name="keyword"> BMRM</field>
<field name="keyword"> OCAS</field>
<field name="keyword"> OWL-QN</field>
<field name="abstract">We extend the well-known BFGS quasi-Newton method and its

 limited-memory variant LBFGS to the optimization of nonsmooth

 convex objectives. This is done in a rigorous fashion by

 generalizing three components of BFGS to subdifferentials: The

 local quadratic model, the identification of a descent

 direction, and the Wolfe line search conditions.

 We apply the resulting subLBFGS algorithm to

 L_2-regularized risk minimization with binary

 hinge loss, and its direction-finding component to

 L_1-regularized risk minimization with logistic

 loss. In both settings our generic algorithms perform

 comparable to or better than their counterparts in

 specialized state-of-the-art solvers.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">591</field>
<field name="author">Surya Prakash</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Semisupervised Approach to Space Carving</field>
<field name="keyword">Thermography</field>
<field name="keyword"> 3D Imaging</field>
<field name="keyword"> Infrared Camera</field>
<field name="abstract">In this paper, we present a semi supervised approach to space

carving. We do this by casting the recovery of volumetric data from multiple

views into an evidence combining setting. The method

presented here is statistical in nature and employs, as a

starting point, a manually obtained contour. By making use of this

user-provided information, we learn a prior distribution that is

then used to compute the probability of a voxel being carved. This

evidence combining setting allows us to make use of background

pixel information. As a result, our method combines the advantages

of shape-from-silhouette techniques and statistical space carving

approaches. We provide quantitative results and illustrate the

utility of the method on real-world imagery.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">592</field>
<field name="author">Jane Li</field>
<field name="author">Tim Mansfield</field>
<field name="author">Susan Hansen</field>
<field name="title">Supporting Enhanced Collaboration in Distributed Multidisciplinary Care Team Meetings</field>
<field name="abstract">This paper describes an initiative for exploring the 

design space of an enhanced collaboration platform 

for a multidisciplinary breast cancer team meeting 

distributed between two hospitals. We present our 

socio-technical approach to the system design process 

which is intended to encourage engagement between 

the research technologists and the clinicians without 

adversely affecting the work of the clinicians. We 

report on some initial results of the observational 

study performed by the Braccetto research team and 

current social-technical issues affecting 

communication between the multidisciplinary team 

members during the meetings. We highlight some 

technical and research challenges which include the 

role that medical images play as boundary objects 

for several disciplines in the meeting, the awareness 

requirement and the evaluation of collaboration 

effectiveness.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">593</field>
<field name="author">Ka-Shu Wong</field>
<field name="title">Sound and Complete Inference Rules for SE-Consequence</field>
<field name="abstract">The notion of strong equivalence on logic programs with answer set semantics gives rise to a consequence relation on logic program rules, called SE-consequence. We present a sound and complete set of inference rules for SE-consequence on disjunctive logic programs.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">594</field>
<field name="author">Peter Hutterer</field>
<field name="author">Bruce Thomas</field>
<field name="title">Enabling Co-located Ad-hoc Collaboration on Shared Displays</field>
<field name="keyword">CSCW</field>
<field name="keyword"> GWWS</field>
<field name="keyword"> windowing systems</field>
<field name="abstract">All major desktop environments are designed around the 

assumption of having a single system cursor and a single 

keyboard. Co-located multi-user interaction on a standard 

desktop requires users to physically hand over the 

devices. Existing collaboration applications require 

complicated and limiting setups and no collaboration 

application or toolkit supports ad-hoc transition from a 

traditional single-user desktop to a multi-user 

collaboration environment without restarting applications. 

 

Our Multi-Pointer X server (MPX) allows easy transition 

between a single-user desktop and a multi-user 

collaboration environment. Pointer devices and keyboards 

can be added and removed at any time. Independent 

cursors and keyboard foci for these devices allow users to 

interact with and type into multiple applications 

simultaneously. MPX is compatible with any legacy X 

application and resolves ambiguity in legacy APIs using 

the novel ClientPointer principle. MPX also provides 

new APIs for multi-user applications and thus enables 

fluid integration of single-user and multi-user 

environments.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">595</field>
<field name="author">Surya Prakash</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Stereo techniques for 3D mapping of object surface temperatures</field>
<field name="keyword">thermography</field>
<field name="keyword"> 3D thermal map</field>
<field name="keyword"> infrared imaging</field>
<field name="keyword"> stereo vision</field>
<field name="keyword"> depth map</field>
<field name="abstract">In this paper, we present two stereo methods to obtain 3D mappings of object

surface temperatures. The first method uses a pair of trichromatic cameras to recover the 3D

geometry and then maps the temperature data from a thermal camera onto the recovered

surface. The second method recovers the 3D surface temperature map using a pair of thermal

cameras by exploiting isotherms and epipolar geometry. The former setup is cheaper while

the latter has the novel advantage of estimating object depth in a dark environment. Both

methods make use of a simple technique to calibrate thermal cameras, which makes the

calibration approach akin to that of trichromatic systems. Finally, we conduct a comparative

study of the two proposed approaches focusing on the accuracy of the results, applicability

and robustness.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">596</field>
<field name="author">Pattaraporn Khuwuthyakorn</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Texture Descriptors via Stable Distributions</field>
<field name="keyword">Texture</field>
<field name="keyword"> FFT</field>
<field name="keyword"> stable distributions</field>
<field name="abstract">In this paper, we present a texture descriptor which hinges in the use of the local image statistics so as to recover a compact representation of the texture under study. To this end, here, we make use of stable distributions and their link to Fourier analysis so as to provide a means to compute in a computationally efficient manner a local texture descriptor. This link between stochastic processes and Fourier analysis provides an efficient means to compute texture spectra which can be interpreted as a probability distribution for purposes of recognition and analysis. Making use of our local descriptor, we provide a metric between texture pairs that can be made devoid of rotations on the texture plane by recovering the optimal linear transformation via Procrustes analysis. We demonstrate the utility of our descriptor and its associated metric on a database of real-world textures.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">597</field>
<field name="author">Surya Prakash</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Semi-supervised Approach to Space Carving</field>
<field name="keyword">space carving</field>
<field name="keyword"> volumetric reconstruction</field>
<field name="keyword"> 3D reconstruction</field>
<field name="keyword"> semi-supervised methods</field>
<field name="abstract">In this paper, we present a semi-supervised approach to space carving

by casting the recovery of volumetric data from multiple views into

an evidence combining setting. The method presented here is

statistical in nature and employs, as a starting point, a

manually obtained contour. By making use of this user-provided

information, we obtain probabilistic silhouettes of all successive

images. These silhouettes provide a prior distribution that is then

used to compute the probability of a voxel being carved. This

evidence combining setting allows us to make use of background

pixel information. As a result, our method combines the advantages

of shape-from-silhouette techniques and statistical space carving

approaches. For the carving process, we propose a new voxelated

space. The proposed space is a projective one that

provides a color mapping for the object voxels which is consistent in terms of pixel

coverage with their projection onto the image planes for the imagery under

consideration. We provide quantitative results and illustrate the utility of the method on real-world imagery.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">598</field>
<field name="author">Jyun-Hao Huang</field>
<field name="author">Ali Zia</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Content-based Image Retrieval via Subspace-projected Salient Features</field>
<field name="keyword">LDA</field>
<field name="keyword"> scene recognition</field>
<field name="keyword"> saliency</field>
<field name="abstract">In this paper we present a novel image representation method which treats images as frequency histograms of salient features. The histograms are computed making use of linear discriminant analysis (LDA). The method employs saliency feature extraction and image binarisation. Then subspace-projected features are extracted. Using the saliency maps as the positive and negative labels, the image features are mapped onto a lower-dimensional

space using LDA. This enables us to construct a 3D-histogram by direct binning on the feature space. This gives rise to a ``cube'' of image features which have been projected onto a lower-dimensional space so as to maximise the separability of the salient regions with respect to the background. Image retrieval can be performed by computing the distances between the histograms for the query image and the images in the database. We demonstrate our algorithm on a real-world database and compare our results to those yielded by codebook representation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">599</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Quadratic Programming Approach to Image Labeling</field>
<field name="keyword">Quadratic programming</field>
<field name="keyword"> image labelling</field>
<field name="keyword"> segmentation</field>
<field name="abstract">Image labeling tasks are usually formulated within the framework of discrete Markov Random Fields where the optimal labels are recovered by extremising a discrete energy function. In this paper, we present an alternative continuous relaxation approach to image labeling which makes use of a quadratic cost function over the class labels. The cost function to minimise is convex and its discrete version is equivalent up to a constant additive factor to the target function used in discrete Makov Random Field approaches. Moreover, its corresponding Hessian matrix is given by the graph Laplacian of the adjacency matrix. Therefore, the optimisation of the cost function is governed by the pairwise interactions between pixels in the local neighbourhood. This leads to a sparse Hessian matrix for which the global minimum of the continuous relaxation problem can be ef ciently found by solving a system of linear equations using Cholesky factorisation. We ellaborate on the links between the method and other techniques elsewhere in the literature and provide results on synthetic and real-world imagery. We also provide comparison to competing approaches.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">600</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Alex Smola</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Quoc Le</field>
<field name="title">Estimating Labels from Label Proportions</field>
<field name="keyword">Semi-supervised learning</field>
<field name="keyword"> Gaussian processes</field>
<field name="abstract">Consider the following problem: given several sets of unlabeled observations,

 each set with known label proportions, predict the

 labels of another unlabeled set of observations, also with known

 label proportions. This type of problem appears in areas like e-commerce, spam filtering and improper content detection. We present

 consistent estimators which have bounded estimate deviations with respect to estimates in a fully labeled scenario. Experiments show that such

 estimators work well in practice.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">601</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Fast Hierarchical Approach to Image Segmentation</field>
<field name="keyword">Image segmentation</field>
<field name="keyword"> hierarchical methods</field>
<field name="abstract">In this paper, we propose a hierarchical approach to image segmentation based on the use of a graph regularisation algorithm. The initial segmentation map is obtained using the normalized cut segmentation algorithm. We then refine the segmentataion results by iteratively propagating the class-labels from coarse-to-fine sampling levels. Image segmentation at each intermediate level is recast as a constrained graph regularisation problem that can be solved efficiently. The multi-level nature of our method achieves low computational cost and robustness to noise corruption. We provide experimental results on the Berkeley Image Database and show the efficacy of our method for segmentation of high resolution images.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">602</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Fast Multiple Instance Learning Via L_{1,2} Logistic Regression</field>
<field name="abstract">In this paper, we develop an efficient logistic regression model for multiple instancel earning that combines $L_1$ and $L_2$ regularisation techniques. An $L_1$ regularised logistic regression model is first learned to find out the sparse pattern of the features.

To train the $L_1$ model efficiently, we employ a convex differentiable approximation of the $L_1$ cost function which can be solved by a quasi Newton method. We then train an $L_2$ regularised logistic regression model only on the subset of features with nonzero weights returned by the $L_1$ logistic regression. Experimental results demonstrate the utility and efficiency of the proposed approach compared to a number of alternatives.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">603</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">On Mixtures of Linear SVMs for Nonlinear Classification</field>
<field name="abstract">In this paper, we propose a new method for training mixtures of linear SVM clasifiers for purposes of non-linear data classification. We do this by packaging linear SVMs into a probabilistic formulation and embedding them in the mixture of experts model. The weights of the mixture model are generated by the gating network dependant on the input data. The new mixture of linear SVMs can be then trained efficiently using the EM algorithm. Unlike previous SVM-based mixture of expert models, which use a divide-and-conquer strategy to

reduce the burden of training for large scale data sets, the main purpose of our approach is to improve the efficiency for testing. Experimental results show that our proposed model can achieve the efficiency of linear classifiers in the prediction phase while still maintaining the classification performance of nonlinear classifiers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">604</field>
<field name="author">Bei Na Wei</field>
<field name="author">Yu Shi</field>
<field name="author">Getian Ye</field>
<field name="author">Jie Xu</field>
<field name="title">Developing a Smart Camera for Road Traffic Surveillance</field>
<field name="keyword">Computer vision</field>
<field name="keyword"> embedded systems</field>
<field name="keyword"> FPGA</field>
<field name="keyword"> object tracking</field>
<field name="abstract">This paper discussesin details a hardware implementation of a real-time object tracking algorithm based Harris keypoint detection technique. VHDL design using an FPGA development platform is the development environment. Preliminary results have shown promising real-time performance and tracking accuracy. The targeted application is a smart camera for road traffic flow analysis.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">605</field>
<field name="author">Le Song</field>
<field name="author">AJ Smola</field>
<field name="author">KM Borgwardt</field>
<field name="author">A Gretton</field>
<field name="title">Colored Maximum Variance Unfolding</field>
<field name="abstract">Maximum variance unfolding (MVU) is an effective heuristic for dimensionality reduction. It produces a low-dimensional representation of the data by maximizing the variance of their embeddings while preserving the local distances of the original data. We show that MVU also optimizes a statistical dependence measure which aims to retain the identity of individual observations under the distance preserving constraints. This general view allows us to design colored variants of MVU, which produce low-dimensional representations for a given task, e.g. subject to class labels or other side information.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">606</field>
<field name="author">Alex J Smola</field>
<field name="author">SVN Vishwanathan</field>
<field name="author">Quoc V Le</field>
<field name="title">Bundle Methods for Machine Learning</field>
<field name="abstract">We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1=&#15;) steps to &#15; precision for general convex problems and in O(log(1=&#15;)) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">607</field>
<field name="author">SVN Vishwanathan</field>
<field name="author">Karsten M Borgwardt</field>
<field name="author">Nicol N Schraudolph</field>
<field name="author">Imre Risi Kondor</field>
<field name="title">On Graph Kernels</field>
<field name="keyword">Graph kernels</field>
<field name="keyword"> Linear Algebra in RKHS</field>
<field name="keyword"> Sylvester Equations</field>
<field name="keyword"> Bioinformatics</field>
<field name="keyword">

Rational Kernels</field>
<field name="keyword"> Transducers</field>
<field name="keyword"> Semirings</field>
<field name="keyword"> Diffusion</field>
<field name="keyword"> Random Walks</field>
<field name="keyword"> Regularization on

Graphs</field>
<field name="abstract">We present a uni&#12;ed framework to study graph kernels, special cases of which include the random walk graph kernel (Gartner et al., 2003; Borgwardt et al., 2005), marginalized graph kernel (Kashima et al., 2003, 2004; Mah&#19;e et al., 2004), and geometric kernel on graphs (Gartner, 2002). Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from O(n6) to O(n3). When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels (Kondor and Lafferty, 2002), regularization on graphs (Smola and Kondor, 2003), and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels (Cortes et al., 2002, 2003, 2004) when specialized to graphs reduce to the random walk graph kernel.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">608</field>
<field name="author">Markus Weimer</field>
<field name="author">Alexandros Karatzoglou</field>
<field name="author">Quoc Viet Le</field>
<field name="author">Alex J Smola</field>
<field name="title">COFI RANK -

Maximum Margin Matrix Factorization for Collaborative Ranking</field>
<field name="abstract">In this paper, we consider collaborative filtering as a ranking problem. We present a method which uses Maximum Margin Matrix Factorization and optimizes ranking instead of rating. We employ structured output prediction to optimize directly for ranking scores. Experimental results show that our method gives very good ranking scores and scales well on collaborative filtering tasks.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">609</field>
<field name="author">Jun Zhou</field>
<field name="author">L. Cheng</field>
<field name="author">Walter F. Bischof</field>
<field name="title">Online learning with Novelty Detection in Human-guided Road Tracking</field>
<field name="abstract">Current image processing and pattern recognition algorithms are not robust enough to make automated remote sensing image interpretation feasible. For this reason, we need to develop image interpretation systems that rely on human guidance. In this paper, we tackle the problem of semiautomatic road tracking in aerial photos. We propose an online learning approach that naturally integrates inputs from human experts with computational algorithms to learn road tracking. Human inputs provide the online learner with training examples to generate road predictors. An ensemble of road predictors is learned incrementally and used to automatically track roads. When novel situations are encountered, control is returned back to the human expert to initialize a new training and tracking iteration. Our approach is computationally efficient, and it can rapidly adapt to dynamic situations where the image feature distributions change. Experimental results confirm that our approach is effective and superior to existing methods.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">610</field>
<field name="author">Qinfeng Shi</field>
<field name="author">Wang Li</field>
<field name="author">Li Cheng</field>
<field name="author">Alex Smola</field>
<field name="title">Discriminative Human Action Segmentation and Recognition using Semi-Markov Model</field>
<field name="keyword">Semi-Markov Model</field>
<field name="keyword"> Action Segmentation</field>
<field name="keyword"> Action Recognition</field>
<field name="keyword"> Discriminative Learning</field>
<field name="keyword"> Structured Learning</field>
<field name="abstract">Given an input video sequence of one person conducting a sequence of continuous actions, we consider the problem of jointly segmenting and recognizing actions. We propose a discriminative approach to this problem under a semi-Markov model framework, where we are able to define a set of features over input/output space that captures the characteristics on boundary

frames, action segments and neighboring action segments, respectively. In addition, we show that this method can also be used to recognize the person who performs in this video sequence. A Viterbi-like algorithm is devised to help efficiently solve the induced optimization problem. Experiments on a variety of datasets demonstrate the effectiveness of the proposed method.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">611</field>
<field name="author">Ralf Huuck</field>
<field name="author">Gerwin Klein</field>
<field name="author">Bastian Schlich</field>
<field name="title">Proceedings of the 3rd International Workshop on Systems Software Veri cation (SSV 2008)</field>
<field name="keyword">SSV</field>
<field name="keyword"> Systems Software</field>
<field name="keyword"> Verification</field>
<field name="abstract">This volume contains the proceedings of the 3rd International Workshop on 

Systems Software Verification (SSV) held in Sydney, Australia, February 25 27, 

2008. 



The purpose of SSV is to bring together researchers and developers from both 

academia and industry, who are facing real software and real problems to find 

real, applicable solutions. By real we mean problems such as time-to-market or 

reliability that the industry is facing and is trying to fix in software that is deployed in the market place. A real solution is one that is applicable to the problem in industry and not one that only applies to an abstract, academic toy version of it. SSV discusses software analysis/development techniques and tools, and serves as a platform to discuss open problems and future challenges in dealing with existing and upcoming system code. 



This volume starts with the abstracts of two invited presentation, followed by 13 

papers that were selected by the Programme Committee out of 24 submissions after 

an intense reviewing and discussion phase. Each paper has been examined by at least 

3 reviewers and we would like to thank the members of the Programme Committee 

as well as the external reviewers for their detailed and thorough investigation of 

each contribution. 



The workshop programme comprised the presentation of the 13 accepted papers, 

four tool demonstrations given by 



 Christof Efkemann and Helge Loeding (University of Bremen, Germany), 



 Ansgar Fehnker (NICTA, Australia), 



 Ralf Huuck (NICTA, Australia), 



 Bastian Schlich (RWTH Aachen, Germany), 



and two invited talks presented by 



 Kim Guldstrand Larsen (Aalborg University/CISS, Denmark) and 



 Hongseok Yang (University of London, United Kingdom).</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">612</field>
<field name="author">Bernhard Beckert</field>
<field name="author">Gerwin Klein</field>
<field name="title">5th International Verification Workshop VERIFY'08</field>
<field name="keyword">Verification</field>
<field name="abstract">The VERIFY workshop series aims at bringing together people who are interested in the development of safety and security critical systems, in formal 

methods, in the development of automated theorem proving techniques, and in 

the development of tool support. Practical experiences gained in realistic verifications are of interest to the automated theorem proving community and new 

theorem proving techniques should be transferred into practice. The overall objective of the VERIFY workshops is to identify open problems and to discuss 

possible solutions under the theme What are the verification problems? What 

are the deduction techniques? . 



This volume contains the research papers presented at the 5th International 

Verification Workshop (VERIFY 08) held August 10 11, 2008 in Sydney, Australia. This workshop was the fifth in a series of international meetings since 2002. 



It was affiliated with the 4th International Joint Conference on Automated Reasoning (IJCAR 2008). 

Each paper submitted to the workshop was reviewed by three referees, and an 

intensive discussion on the borderline papers was held during the online meeting 

of the Program Committee. 7 research papers were accepted based on originality, 

technical soundness, presentation, and relevance. We wish to sincerely thank all 

the authors who submitted their work for consideration. And we would like to 

thank the Program Committee members and other referees for their great effort 

and professional work in the review and selection process. Their names are listed 

on the following pages. 



In addition to the contributed papers, the program included two excellent 

keynote talks. We are grateful to Prof. Gilles Barthe (IMDEA Software, Madrid, 

Spain) and Prof. Gernot Heiser (National ICT and Univ. of New South Wales, 

Sydney, Australia) for accepting the invitation to address the workshop.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">613</field>
<field name="author">Zhidong Li</field>
<field name="author">Jing Chen</field>
<field name="title">On Semantic Object Detection with Salient Feature</field>
<field name="abstract">Salient detection is one of the major interests with the computer vision research. Most of the existing salient detection algorithms target at detecting distinct regions without providing much high-level semantic meaning. In this paper we apply the salient detection technology in a combined bottom-up and top-down object detection algorithm, aiming to detect meaningful objects exhibiting salient features. We define salient region as distinct areas detected across the scene in the bottom-up phase, and salient feature to best represent the meaningful semantics of an object in the top-down phase where we apply a boosting algorithm to accommodate and weight all Support Vector Machine (SVM) and enhanced Adaboost classifiers based on selected input features. Final experiments indicate that our proposed object detector is fairly effective compared with the state of the art.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">614</field>
<field name="author">Daniel Harabor</field>
<field name="author">Adi Botea</field>
<field name="title">Hierarchical path planning for multi-size agents in heterogenous environments</field>
<field name="keyword">path-planning</field>
<field name="keyword"> a*</field>
<field name="keyword"> search</field>
<field name="abstract">In this paper we present new techniques for the automated construction of state space representations of complex multi- terrain grid maps with minimal information loss. Our approach involves the use of graph annotations to record the amount of maximal traversable space at each location on a map. We couple this with a cluster-based hierarchical abstraction technique to build highly compact yet complete representations of the original problem. We further outline the design of a new planner, Annotated Hierarchical A* (AHA*), and demonstrate how a single abstract graph can be used to plan for many different agents, including different sizes and terrain traversal capabilities. Our experimental analysis shows that AHA* is able to generate near-optimal solutions to a wide range of problems while maintaining an exponential reduction in comparative effort over low-level search. Meanwhile, our abstraction technique is able to generate approximate representations of large problem-spaces with complex topographies using just a fraction of the storage space required by the original grid map.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">615</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Planning Graphs and Propositional Clause-Learning</field>
<field name="abstract">The planning graph of Blum and Furst is one of the frequently used tools in planning. It is a data structure which can be visualized as a bipartite graph with state variables and actions as nodes and which approximates (upper bound) the set of reachable states with a given number of sets of simultaneous actions.

We show that the contents of planning graphs follow from two more general notions: extended clause learning restricted to 2-literal clauses and the representation of parallel plans consisting of STRIPS actions in the classical propositional logic. This is the first time planning graphs have been given an explanation in terms of the inference methods used in SAT solvers. The work helps in bridging the gap between specialized algorithms devised for planning and general-purpose algorithms for automated reasoning.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">616</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Christos Levcopoulos</field>
<field name="author">Giri Narasimhan</field>
<field name="author">Michiel Smid</field>
<field name="title">Approximate distance oracles for geometric spanners</field>
<field name="abstract">Given an arbitrary real constant \eps &gt; 0, and a geometric graph G in d-dimensional Euclidean space with n points, O(n) edges, and constant dilation, our main result is a data structure that answers (1+\eps)-approximate shortest path length queries in constant time. The data structure can be constructed in O(n log n) time using O(n log n) space. This represents the first data structure that answers (1+\eps)-approximate shortest path queries in constant time, and hence functions as an approximate distance oracle. The data structure is also applied to several other problems. In particular, we also show that approximate shortest path queries between vertices in a planar polygonal domain with ``rounded'' obstacles can be answered in constant time. Other applications include query versions of closest pair problems, and the efficient computation of the approximate dilations of geometric graphs. Finally, we show how to extend the main result to answer (1+\eps)-approximate shortest path length queries in constant time for geometric spanner graphs with m=omega(n) edges. The resulting data structure can be constructed in O(m + n log n) time using O(n log n) space.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">617</field>
<field name="author">Yang Wang</field>
<field name="title">Real-Time Moving Vehicle Detection with Cast Shadow Removal in Video Based on Conditional Random Field</field>
<field name="keyword">Conditional random field</field>
<field name="keyword"> contextual constraint</field>
<field name="keyword"> shadow removal</field>
<field name="keyword"> vehicle detection.</field>
<field name="abstract">This paper presents an approach of moving vehicle detection and cast shadow removal for video based traffic monitoring. Based on conditional random field, spatial and temporal dependencies in traffic scenes are formulated under a probabilistic discriminative framework, where contextual constraints during the detection process can be adaptively adjusted in terms of data-dependent neighborhood interaction. Computationally efficient algorithm has been developed to discriminate moving cast shadows and handle nonstationary background processes for real-time vehicle detection in video streams. Experimental results show that the proposed approach effectively fuses contextual dependencies and robustly detects moving vehicles under heavy shadows even in grayscale video.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">618</field>
<field name="author">Leonid Ryzhyk</field>
<field name="author">Timothy Bourke</field>
<field name="author">Ihor Kuz</field>
<field name="title">Reliable device drivers require well-defined protocols</field>
<field name="keyword">Device drivers</field>
<field name="keyword"> modelling</field>
<field name="keyword"> systems engineering</field>
<field name="abstract">Current operating systems lack well-defined protocols for interaction with device drivers. We argue that this hinders the development of reliable drivers and thereby undermines overall system stability. We present an approach to specify driver protocols using a formalism based on state machines. We show that it can simplify device programming, facilitate static analysis of drivers against protocol specifications, and enable detection of incorrect behaviours at runtime.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">619</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">Daniel Lewis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Summary of NICTA channel measurement results</field>
<field name="keyword"/>
<field name="abstract">Summary of NICTA channel measurement results for on-body to on-body narrowband wireless channels around the 900 and 2400 MHz ISM bands.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">620</field>
<field name="author">Lin Luo</field>
<field name="author">Andrew Zhang</field>
<field name="author">Zhenning Shi</field>
<field name="title">BER Analysis for Asymmetric OFDM Systems</field>
<field name="abstract">To alleviate the high peak-to-average power ratio (PAPR) and

sensitivity to carrier frequency offset (CFO) problems, an

asymmetric OFDM (A-OFDM) system has been recently proposed. In this

paper, the theoretical bit error rate (BER) analysis of the A-OFDM

systems is presented. Various modulation methods, i.e., BPSK and

M-ray QAM, and equalization techniques, i.e., zero forcing (ZF) and

minimum mean square error (MMSE) criteria are considered. Our

analysis shows that the BER performance of the A-OFDM system bridges

that of conventional OFDM and single carrier systems. Certain

advanced equalizers, such as MMSE equalizer, are found to

significantly improve the BER performance of the A-OFDM systems. The

analysis approach is generally applicable to most modulation schemes

and can be extended to any precoded OFDM systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">621</field>
<field name="author">Imtiaz Husain</field>
<field name="author">Jinhong Yuan</field>
<field name="author">Andrew Zhang</field>
<field name="title">Channel shortening through bit error rate minimization for UWB systems</field>
<field name="abstract">Ultra wideband (UWB) communication systems occupy huge bandwidths with very low power spectral densities. This feature make the UWB channels highly rich in resolvable multipaths. To exploit the temporal diversity, the receiver is commonly implemented through a rake. The aim to capture enough signal energy to maintain an acceptable output signal to noise ratio (SNR) dictates a very complicated rake structure with a large number of fingers. Channel shortening or time domain equalizer (TEQ) can simplify the rake receiver design by reducing the number of significant taps in the effective channel. In this paper, we present a TEQ design which minimizes the bit error rate (BER) of the system to perform efficient channel shortening in UWB systems. The performance of the proposed algorithm is compared with a generic TEQ design and other rake structures in UWB channels. It is shown that the proposed algorithm maintains a lower BER along with efficiently shortening the channel.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">622</field>
<field name="author">Ahmed El Zein</field>
<field name="author">Eric McCreath</field>
<field name="author">Alistair Rendell</field>
<field name="author">Alex Smola</field>
<field name="title">Performance Evaluation of the NVIDIA GeForce 8800 GTX GPU for Machine Learning</field>
<field name="abstract">Performance Evaluation of the NVIDIA GeForce 8800 GTX GPU for Machine Learning</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">623</field>
<field name="author">Jenny Liu</field>
<field name="author">Jacky Kueng</field>
<field name="author">Liming Zhu</field>
<field name="author">Kate Foster</field>
<field name="author">Thong Nguyen</field>
<field name="title">Trade-off Analysis Method for Mission Critical Middleware Systems on DSTO Hybrid Test Bed</field>
<field name="keyword">Middleware</field>
<field name="keyword"> Evaluation</field>
<field name="keyword"> Statistical Analysis</field>
<field name="keyword"> COTS</field>
<field name="keyword"> Qualtiy Attributes</field>
<field name="abstract">This project is linked to the previous collaborative project on Evaluating Middleware Architecture for Airborne Mission Systems. It leverages the method for evaluating middleware architectures (MEMS) as one outcome from the previous project. Measurements of performance metrics are collected based on the test scenarios developed in the previous project. Following the evaluation process defined in MEMS, empirical results are collected from the established test bed. One challenge is that large amount of experimental data for analysis is produced from the measurement. Therefore automated analysis is essential with efficient software tool support. In addition, the results of a specific quality attribute are feedbacks to the original MEMS method. Another challenge is integrating both quantitative and qualitative evaluation results of quality attributes to produce a final rigorous evaluation summary. 



This report presents a new method for trade-off analysis at fine grained levels of the middleware architecture. This report encompasses the research work that addresses above two challenges. It focuses on the method of analysing measured results and identifying trade-offs among multiple quality attributes. Based on the analysis, a set of architecture guidelines will be produced on the design and development of mission critical systems to satisfy high dependability requirements. 



The outcomes of this project complete the whole process of middleware architecture evaluation of defence systems. The project will enable DSTO to establish best practices of evaluating quality attributes of defence systems.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">624</field>
<field name="author">Ming Zhao</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Iterative Turbo Channel Estimation for OFDM System over Rapid Dispersive Fading Channel</field>
<field name="keyword">OFDM</field>
<field name="keyword"> channel estimatin</field>
<field name="keyword"> turbo iterative</field>
<field name="keyword"> rapid dispersive fading channel</field>
<field name="abstract">Coherent OFDM detection requires accurate channel state information (CSI). Mobile radio channels are both

time and frequency dispersive, especially at high vehicular speeds, which makes channel estimation a challenging problem in system design. Conventional preamble-based and pilot-aided channel estimation require numerous reference signals, which significantly compromises the system throughput. This paper proposes a novel low complexity iterative turbo channel estimation technique which makes use of preamble, pilots and soft decoded data information in an iterative fashion to improve the system performance over the time and frequency selective fading channel while maintaining the system throughput. The numerical and analytical results show that the proposed technique can approach the performance of systems with perfect CSI with much

fewer preamble and pilots symbols compared to existing channel estimation methods.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">625</field>
<field name="author">Mark Reed</field>
<field name="title">The Business Case for Advanced WiMAX Receivers: Deployment Benefits in the Australian Environment</field>
<field name="keyword">WiMAX</field>
<field name="keyword"> advanced receivers</field>
<field name="abstract">Optimisation of a WiMAX system can occur at various places in the system providing different benefits to the overall system performance. Optimisation is critical to maximise the efficiency of the wireless network reducing CAPEX and OPEX for the operators, improving service provision and the mobile user experience . Advanced receivers promise significant improvements to a WiMAX system performance. In this presentation we investigate the gains possible from advanced receivers and illustrate the system benefits in terms of cell range improvement and average data throughput for a user. Outdoor mobile wireless trial data collected from NICTA s advanced WiMAX solution is used to validate the improvements. Surprisingly for a very small improvement in receiver sensitivity, significant cell area improvements can result; likewise for a given cell range packet throughput rates can also significantly increase. We conclude by looking at Multiple-Input Multiple-Output (MIMO) or advanced multiple antenna structures and the additional benefits they will bring to the network.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">626</field>
<field name="author">Ming Zhao</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Modified Schnorr-Euchner Sphere Decoding for Iterative Spatial Multiplexing MIMO Receiver</field>
<field name="abstract">Recent work has shown that the Sphere Decoder (SD) based on Schnorr-Euchner (SE) Strategy offers significant reductions in the computation complexity compared to all previously proposed sphere decoders with a near-Maximum Likelihood (ML) detection performance. This paper proposed a novel modified SE algorithm for iterative Spatial Multiplexing multiple-input-multiple-output (MIMO) receiver to estimate the maximum a posteriori (MAP) probability of the received symbol sequence. In addition, two schemes are proposed to

further improve the performance and reduce the complexity over iterations, one is to improve the ZF-DFE estimate by using the quadratic approximation of a priori information, the other is to improve the tree search by employing starting point zig-zag technique. Simulation results show that significant performance gain and complexity reduction can be obtained compared to existing SE approach.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">627</field>
<field name="author">Ming Zhao</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">The Performance of Sphere Decoders for Iterative Spatial Multiplexing MIMO Receiver</field>
<field name="abstract">Pohst and Schnorr-Euchner (SE) are two popular sphere decoders (SD) to perform near maximum likelihood (ML) detection in Spatial Multiplexing multiple-input-multiple-output (MIMO) system. Recent work \cite{Caire_03} showed Pohst and SE are two variants of the well known stack sequential decoding and proposed modification to each strategy to shown significant complexity reduction but with near ML performance. This paper proposed modified Pohst and SE algorithms by taking into account the a priori information to estimate the maximum a posteriori (MAP) probability of the received symbol sequence. Simulation results show that significant

performance gain can be achieved from iterative MAP approach and complexity reduction can be obtained for SE enumeration strategy with minor performance loss compared to Phost enumeration algorithm.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">628</field>
<field name="author">Thanassis Boulis</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Matthias Fruth</field>
<field name="author">Annabelle McIver</field>
<field name="title">CaVi - Simulation and Model Checking for Wireless Sensor Networks</field>
<field name="keyword">wireless sensor networks</field>
<field name="keyword"> simulation</field>
<field name="keyword"> model checking</field>
<field name="keyword"> graphical front-end</field>
<field name="abstract">CaVi provides a uniform interface to state-of-the-art simulation methods and formal verification methods for wireless sensor network. Simulation is suitable to examine the behavior of a wireless sensor network in great detail. Due to the probabilistic behavior of these systems, however, the simulation covers only a small fraction of all possible behaviors.

Formal model checking techniques, based on Markov Decision Processes, use less detailed and more abstract models and compute exact probabilities and expected values for the entire behavior, where simulation can only give averages. CaVi allows for creating a single model for simulation, Monte-Carlo simulation, and model checking.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">629</field>
<field name="author">Daniel Lewis</field>
<field name="author">Dino Miniutti</field>
<field name="title">An Introduction to the IEEE 802.15.6 Applications Summary Document</field>
<field name="keyword"/>
<field name="abstract">Describes the Rationale, Purpose and Content of the new Applications Summary Document

Purpose: To promote discussion of the Applications Summary Document within the TG6 group</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">630</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">Daniel Lewis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Narrowband On-Body to Off-Body Channel Characterization for BAN</field>
<field name="keyword"/>
<field name="abstract">This document presents the results of narrowband on-body to off-body wireless channel measurements around the 900 and 2400 MHz ISM bands.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">631</field>
<field name="author">Aleksander Ignjatovic</field>
<field name="title">Local Approximations Based on Orthogonal Differential Operators</field>
<field name="abstract">Let M be a symmetric positive definite moment functional and let $\{P_n^{\cal M}(\omega)\}_{n\in {\Bbb N}}$ be the family of orthonormal polynomials that corresponds to M. We introduce a family of linear differential operators ${\cal K}^n =(-i)^nP_n^{\cal M}(i\frac{d}{dt})$ , called the chromatic derivatives associated with M, which are orthonormal with respect to a suitably defined scalar product. We consider a Taylor type expansion of an analytic function f(t), with the values f(n) (t0) of the derivatives replaced by the values ${\cal K}^n[f](t_0)$ of these orthonormal operators, and with monomials (t _ t0)n/n! replaced by an orthonormal family of "special functions" of the form $( _ 1)^n{\cal K}^n[m](t-t_0)$ , where $m(t) = \sum_{n=0}^{\infty} ( _ 1)^n{\cal M}(\omega^{2n}) t^{2n}/(2n)!$ . Such expansions are called the chromatic expansions. Our main results relate the convergence of the chromatic expansions to the asymptotic behavior of the coefficients appearing in the three term recurrence satisfied by the corresponding family of orthogonal polynomials PMn( _). Like the truncations of the Taylor expansion, the truncations of a chromatic expansion at t = t0 of an analytic function f(t) approximate f(t) locally, in a neighborhood of t0. However, unlike the values of f(n)(t0), the values of the chromatic derivatives Kn[f](t0) can be obtained in a noise robust way from sufficiently dense samples of f(t). The chromatic expansions have properties which make them useful in fields involving empirically sampled data, such as signal processing.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">632</field>
<field name="author">Andhi Janapsatya</field>
<field name="author">Aleksander Ignjatovic</field>
<field name="author">Sri Parameswaran</field>
<field name="author">Joerg Henkel</field>
<field name="title">Instruction trace compression for rapid instruction cache simulation</field>
<field name="abstract">Modern Application Specific Instruction Set Processors (ASIPs) have customizable caches, where the size, associativity and line size can all be customized to suit a particular application. To find the best cache size suited for a particular embedded system, the application(s) is/are executed, traces obtained, and caches simulated. Typically, program trace files can range from a few megabytes to several gigabytes. Simulation of cache performance using large program trace files is a time consuming process. In this paper, a novel instruction cache simulation methodology that can operate directly on a compressed program trace file without the need for decompression is presented. This feature allowed our simulation methodology to have an average speed up of 9.67 times compared to the existing state of the art tool (Dinero IV cache simulator), for a range of applications from the Mediabench suite.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">633</field>
<field name="author">Kai Engelhardt</field>
<field name="author">Yoram Moses</field>
<field name="title">Single-bit messages are insufficient for data link over duplicating channels</field>
<field name="abstract">Ideal communication channels in asynchronous systems are reliable, deliver messages in FIFO order, and do not deliver spurious or duplicate messages. A message vocabulary of size two (i.e., single-bit messages) suffices to encode and transmit messages of arbitrary finite length over such channels. This note proves that single-bit messages are insufficient once channels potentially deliver duplicate messages. In particular, it is shown that no protocol allows the sender to notify the receiver which of three values it holds, over a bidirectional, reliable, FIFO channel that may duplicate messages. This implies that messages must encode some additional control information, e.g., in the form of headers or tags.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">634</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Dietmar Pfahl</field>
<field name="title">Reflections on 10 Years of Software Process Simulation Modeling: A Systematic Review</field>
<field name="keyword">software process simulation</field>
<field name="keyword"> systematic literature review</field>
<field name="abstract">Software process simulation modeling (SPSM) has become an increasingly active research area since its introduction in the late 1980s. Particularly during the last ten years the related research community and the number of publications have been growing. The objective of this research is to provide insights about the evolution of SPSM research during the last 10 years. A systematic literature review was proposed with two subsequent stages to achieve this goal. This paper presents the preliminary results of the first stage of the review that is exclusively focusing on a core set of publication sources. More than 200 relevant publications were analyzed in order to find answers to the research questions, including the purposes and scopes of SPSM, application domains, and predominant research issues. From the analysis the following conclusions could be drawn: (1) Categories for classifying software process simulation models as suggested by the authors of a landmark publication in 1999 should be adjusted and refined to better capture the diversity of published models. (2) Research improving the efficiency of SPSM is gaining importance. (3) Hybrid process simulation models have attracted interest as a possibility to more realistically capture complex real-world software processes.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">635</field>
<field name="author">Ming Huo</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Ross Jeffery</field>
<field name="title">Detection of Consistent Patterns from Process Enactment Data</field>
<field name="keyword">agile method</field>
<field name="keyword"> process recovery</field>
<field name="keyword"> software process modeling</field>
<field name="keyword"> software process improvement</field>
<field name="abstract">Software process improvement has been a focus of industry for many years. To assist with the implementation of process improvement, we provide an approach to recover process enactment data. The goal of our method is to uncover the actual process used and thereby provide evidence for improving the quality of a planned software process that is followed by an organization in the future. The recovered process model (or patterns) is presented at the same level of abstraction as the planned process model. This allows an easy and clear method to identify the distance between a planned process model and the actual project enactment. We investigate the enactment of a defined software process model from the view of understanding the opportunity for process model improvement from the viewpoint of the project managers in the context of a small software development organization. We collected data from one of our collaboration organizations and then applied our method to a case study. The consistencies between a planned process model and the project enactment were measured. The outcomes of our method provide precise information including qualitative and quantitative data to assist project managers with process improvement in future practice. The main contribution of our work is to provide a novel approach to assist software process improvement by recovering a model from process enactment data.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">636</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Toby Walsh</field>
<field name="title">Dynamic Symmetry Breaking Constraints</field>
<field name="keyword">constraint satisfaction</field>
<field name="keyword"> symmetry breaking</field>
<field name="abstract">We present a general method for dynamically posting

symmetry breaking constraints during search. The basic idea is very

simple. Given any set of symmetry breaking constraints, if during

search a symmetry of one of these constraints is entailed and this

is consistent with previously posted symmetry breaking constraints,

then we post this constraint. The method works best with problems

where symmetry can be broken with a small number of symmetry

breaking constraints. We illustrate the method with two examples

where a polynomial number of symmetry breaking constraints break

an exponential number of symmetries. Like existing static meth-

ods for symmetry breaking, this symmetry breaking method benefits

from fast and effective constraint propagation. In addition, like exist-

ing dynamic methods for symmetry breaking, this symmetry break-

ing methods does not conflict with the branching heuristic. Initial

experimental results appear promising.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">637</field>
<field name="author">Ngoc Bao (Betty) Bui</field>
<field name="author">Liming Zhu</field>
<field name="author">Jenny Liu</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Ross Jeffery</field>
<field name="title">Automating Web Service Development Using a Unified Model</field>
<field name="abstract">Web service standards are being developed in a loosely coordinated and constantly evolving manner and there is a lack of Web service modeling approaches that efficiently reflect the status of the standardization. Consequently the development and deployment of Web services tend to be ad-hoc and platform-oriented. This introduces potential interoperability issues and maintenance overhead. This paper proposes a model-driven framework that includes a service modeling language describing functionality and non-functional properties of service-oriented applications in unified models. This service modeling language is based on a Web service meta-model extracted directly from the WS-* standards. We developed a corresponding tool that generates code stubs, configurations and deployment heuristics, along with standard-based artifacts from models. We conducted a real-world case study to validate our approach.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">638</field>
<field name="author">Liming Zhu</field>
<field name="title">Application of the REST Principle</field>
<field name="abstract">REpresentational State Transfer (REST) is the principle behind the WWW success. But the REST principle is not always followed in Web publishing and mashup service design. This talk focuses on the corrent application of the REST principle in publishing. This includes 1) the architecting secrets behind a Resource-Oriented Architecture, 2) the leverage of feed technologies and standard publishing protocols (e.g. RSS and ATOM), and 3) mashup design beyond data. The data publishing architecture in the Australian Lending Industry will be used as an exemplar. The talk includes both mature industry best

practices and new cutting-edge research results in the area.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">639</field>
<field name="author">Luping Zhou</field>
<field name="author">Richard Hartley</field>
<field name="author">Lei Wang</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="title">Regularized Discriminative Direction for Anatomical Shape Difference Analysis</field>
<field name="keyword">Statistical shape analysis</field>
<field name="keyword"> Discriminative direction</field>
<field name="keyword"> Pre-image problem</field>
<field name="keyword"> Shape distribution</field>
<field name="keyword"> Hippocampal shapes</field>
<field name="abstract">Identifying the shape difference between two groups of anatomical

objects is important for medical image analysis and computer-aided

diagnosis. A method called ``discriminative direction'' in the

literature has been proposed to solve this problem. In that

method, the shape difference between groups is identified by

deforming a shape along the discriminative direction. This paper

conducts a thorough study about inferring this discriminative

direction in an efficient and accurate way. First, finding the

discriminative direction is reformulated as a pre-image problem in

kernel-based learning. This provides a complementary but

conceptually simpler solution than the previous method. More

importantly, we find that a shape deforming along the original

discriminative direction cannot faithfully maintain its anatomical

correctness. This unnecessarily introduces spurious shape

differences and leads to inaccurate analysis. To overcome this

problem, this paper further proposes a \textit{regularized}

discriminative direction by requiring a shape to conform to its

underlying distribution when it deforms. Two different approaches

are developed to impose the regularization, one from the

perspective of probability distributions and the other from a

geometric point of view, and their relationship is discussed.

After verifying their superior performance through controlled

experiments, we apply the proposed methods to detecting and

localizing the hippocampal shape difference between sexes. We get

results consistent with other independent research, providing a

more compact representation of the shape difference compared with

the established discriminative direction method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">640</field>
<field name="author">Luping Zhou</field>
<field name="author">Richard Hartley</field>
<field name="author">Lei Wang</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="title">Regularized Discriminative Direction for Anatomical Shape Difference Analysis</field>
<field name="keyword">Statistical shape analysis</field>
<field name="keyword"> Discriminative direction</field>
<field name="keyword"> Pre-image problem</field>
<field name="keyword"> Shape distribution</field>
<field name="keyword"> Hippocampal shapes</field>
<field name="abstract">Identifying the shape difference between two groups of anatomical

objects is important for medical image analysis and computer-aided

diagnosis. A method called ``discriminative direction'' in the

literature has been proposed to solve this problem. In that

method, the shape difference between groups is identified by

deforming a shape along the discriminative direction. This paper

conducts a thorough study about inferring this discriminative

direction in an efficient and accurate way. First, finding the

discriminative direction is reformulated as a pre-image problem in

kernel-based learning. This provides a complementary but

conceptually simpler solution than the previous method. More

importantly, we find that a shape deforming along the original

discriminative direction cannot faithfully maintain its anatomical

correctness. This unnecessarily introduces spurious shape

differences and leads to inaccurate analysis. To overcome this

problem, this paper further proposes a \textit{regularized}

discriminative direction by requiring a shape to conform to its

underlying distribution when it deforms. Two different approaches

are developed to impose the regularization, one from the

perspective of probability distributions and the other from a

geometric point of view, and their relationship is discussed.

After verifying their superior performance through controlled

experiments, we apply the proposed methods to detecting and

localizing the hippocampal shape difference between sexes. We get

results consistent with other independent research, providing a

more compact representation of the shape difference compared with

the established discriminative direction method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">641</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Sara Hakami</field>
<field name="author">Bjorn Landfeldt</field>
<field name="author">Tim Moors</field>
<field name="title">Detection and identification of anomalies in wireless mesh networks using Principal Component Analysis (PCA)</field>
<field name="keyword">Principal Component Analysis</field>
<field name="keyword"> Wireless mesh networks</field>
<field name="keyword"> Anomaly detection</field>
<field name="abstract">Anomaly detection is becoming a powerful and necessary component as wireless networks gain popularity. In this paper, we evaluate the efficacy of PCA based anomaly detection for wireless mesh networks (WMN). PCA based method was originally developed for wired networks. Our experiments show that it is possible to detect different types of anomalies, such as Denial-of-service (DoS) attack, port scan attack \cite{lakhina04diagnosing}, etc., in an interference prone wireless environment. However, the PCA based method is found to be very sensitive to small changes in flows causing non-negligible number of false alarms. This problem prompted us to develop an anomaly identification scheme which automatically identifies the flow(s) causing the detected anomaly and their contributions in terms of number of packets. Our results show that the identification scheme is able to differentiate false alarms from real anomalies and pinpoint the culprit(s) in case of a real fault or threat. Moreover, we also found that the threshold value used in \cite{lakhina04diagnosing} for distinguishing normal and abnormal traffic conditions is based on assumption of normally distributed traffic which is not accurate for current network traffic which is mostly self-similar in nature. Adjusting the threshold also reduced the number of false alarms considerably. The experiments were performed over an 8 node mesh testbed deployed in a suburban area, under different realistic traffic scenarios. Our identification scheme facilitates the use of PCA based method for real-time anomaly detection in wireless networks as it can filter the false alarms locally at the monitoring nodes without excessive computational overhead.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">642</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Karl Michael Goeschka</field>
<field name="author">Aad van Moorsel</field>
<field name="author">Ian Warren</field>
<field name="author">Raymond Wong</field>
<field name="title">Proceedings of the EDOC 2008 Workshop Middleware for Web Services (MWS) 2008</field>
<field name="keyword">middleware</field>
<field name="keyword"> Web service</field>
<field name="keyword"> service-oriented computing</field>
<field name="abstract">Middleware is software that provides common functionality in a distributed computing system. Use of middleware significantly simplifies development of distributed applications, including those using Web service technologies. For example, middleware can support implementation independence and quality of service (QoS) management. 

The Middleware for Web Services (MWS) 2008 workshop is the fourth workshop in the series of international workshops on this topic, which are organized annually at the IEEE EDOC conferences. The MWS 2008 program contains 2 invited keynote presentations and 10 peer-reviewed papers (5 full and 5 short). 

This introduction to the Proceedings of MWS 2008 starts with a brief explanation of the motivation for the workshop and its history. Then, it contains a short de-scription of the keynotes and peer-reviewed papers. After concluding statements, MWS 2008 Program Committee members and external reviewers are listed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">643</field>
<field name="author">Ying Chen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dhammika Jayalath</field>
<field name="title">Clipping Noise Compensation for OFDM Systems</field>
<field name="keyword">clipping compensation </field>
<field name="keyword"> PAPR</field>
<field name="keyword"> OFDM</field>
<field name="abstract">An efficient and low-complexity clipping estimation and compensation scheme is presented. With

clipped signals estimated from two different signal sources and accurately identified phase information,

the proposed scheme can improve the system performance significantly.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">644</field>
<field name="author">Ying Chen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Jayalath Dhammika</field>
<field name="title">Are SC-FDE Systems Robust to CFO?</field>
<field name="keyword">Carrier frequency offset</field>
<field name="keyword"> OFDM</field>
<field name="keyword"> SC-FDE</field>
<field name="abstract">This paper investigates the impact of carrier frequency offset (CFO) on Single Carrier wireless

communication systems with Frequency Domain Equalization (SC-FDE). We show that CFO in SCFDE

systems causes irrecoverable channel estimation error, which leads to inter-symbol-interference

(ISI). The impact of CFO on SC-FDE and OFDM is compared in the presence of CFO and channel

estimation errors. Closed form expressions of signal to interference and noise ratio (SINR) are derived

for both systems, and verified by simulation results. We find that when channel estimation errors are

considered, SC-FDE is similarly or even more sensitive to CFO, compared to OFDM. In particular, in

SC-FDE systems, CFO mainly deteriorates the system performance via degrading the channel estimation.

Both analytical and simulation results highlight the importance of accurate CFO estimation in SC-FDE

systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">645</field>
<field name="author">Ying Chen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dhammika Jayalath</field>
<field name="title">Low-Complexity Estimation of CFO and Frequency Independent I/Q Mismatch for OFDM systems</field>
<field name="keyword">OFDM</field>
<field name="keyword"> CFO</field>
<field name="keyword"> I/Q MISMATCH</field>
<field name="abstract">CFO and I/Q mismatch could cause significant performance degradation to OFDM systems. Their

estimation and compensation are generally difficult as they are entangled in the received signal. In this

paper, we propose some schemes which are robust to any values of CFO and I/Q mismatch. These

estimators are formed based on the observation that a cosine estimator, which is free of I/Q mismatch,

serves much better as the basis for I/Q mismatch estimation than an initial estimate of CFO. The schemes

are divided into three steps, including forming a cosine estimator free of I/Q mismatch interference,

estimating I/Q mismatch using the estimated cosine value, and forming a sine estimator using samples

after I/Q mismatch compensation. Simulation results show that the proposed schemes can improve system

performance significantly, and they are robust to CFO and I/Q mismatch.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">646</field>
<field name="author">Lin Luo</field>
<field name="author">Andrew Zhang</field>
<field name="author">Zhenning Shi</field>
<field name="title">Advanced Receiver Design for Quadrature OFDMA Systems</field>
<field name="keyword">OFDMA</field>
<field name="keyword"> Q-OFDMA</field>
<field name="keyword"> Reseiver Design.</field>
<field name="abstract">Quadrature orthogonal frequency division multiple access (Q-OFDMA) systems have been recently proposed to reduce the peak-to-average power ratio (PAPR) and complexity, and improve carrier frequency offset (CFO) robustness and frequency diversity for the conventional OFDMA systems. However, Q-OFDMA receiver obtains frequency diversity at the cost of noise enhancement, which results in Q-OFDMA systems achieving better performance than OFDMA only in the higher signal to noise ratio (SNR) range. In this paper, we investigate

various detection techniques such as linear zero forcing (ZF) equalization, minimum mean square error (MMSE) equalization, decision feedback equalization (DFE) and turbo joint channel estimation and detection, for Q-OFDMA systems to mitigate the noise enhancement effect and improve the bit error ratio (BER) performance. It is shown that advanced detections, e.g. DFE and turbo receiver, can significantly improve the performance of QOFDMA.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">647</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Properties of Giant Component in Wireless Multi-Hop Networks</field>
<field name="abstract">In this paper, we study the giant component, the

largest component containing a non-vanishing fraction of nodes,

in wireless multi-hop networks in &lt;d (d = 1; 2). We assume that

n nodes are randomly, independently and uniformly distributed

in [0; 1]d, and each node has a uniform transmission range of

r = r(n) and any two nodes can communicate directly with

each other iff their Euclidean distance is at most r. For d =

1, we derive a closed-form analytical formula for calculating

the probability of having a giant component of order above pn

with any fixed 0:5 &lt; p 1. We also investigate the asymptotic

behavior of one dimensional network having a giant component

based on the derived result, which is distinctly different from

two dimensional counterpart. For d = 2, we derive an asymptotic

analytical upper bound on the minimum transmission range at

which the probability of having a giant component of order above

qn for any fixed 0 &lt; q &lt; 1 tends to one as n ! 1. Based on the

result, we show that a significant energy saving can be achieved

if we only require most nodes (e.g. 95%) to be connected to the

giant component rather than require all nodes to be connected.

The results of this paper are of practical significance in the design

and analysis of wireless ad hoc networks and sensor networks.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">648</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Giant Component in Wireless Multi-hop Networks</field>
<field name="abstract">In this paper, we study the giant component, the

largest component containing a non-vanishing fraction of nodes,

in a wireless multi-hop network where n nodes are randomly and

uniformly distributed in [0; 1]d (d = 1; 2) and any two nodes can

communicate directly with each other iff their Euclidean distance

is not larger than the transmission range r. We investigate the

probability that the size of the giant component is at least

a given threshold p with 0:5 &lt; p 1. Firstly, we derive a

closed-form analytical formula for this probability for d = 1.

Secondly, we propose an empirical formula for this probability

using simulations for d = 2. The results of this paper are of

practical value in the design and analysis of wireless ad hoc

networks and sensor networks. In addition, our analysis shows

that a significant energy saving can be achieved if we only require

most nodes (e.g. 95%) to be connected to the giant component

rather than require all nodes to be connected.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">649</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">Graph Theoretic Models and Tools for the Analysis of Dynamic Wireless Multihop Networks</field>
<field name="keyword">Wireless multihop networks</field>
<field name="keyword"> evolving graphs</field>
<field name="abstract">Wireless multihop networks, in various forms, are

being increasingly used in military and civilian applications.

Advanced applications of wireless multihop networks demand

better understanding on their properties. Existing research on

wireless multihop networks has largely focused on static networks,

where the network topology is time-invariant; and there

is comparatively a lack of understanding on the properties of

dynamic networks with dynamically changing topology. In this

paper, we use and extend a recently proposed graph theoretic

model, i.e. evolving graphs, to capture the characteristics of such

networks. We extend and develop the concepts of route matrix,

connectivity matrix and probabilistic connectivity matrix as a

convenient tool to characterize and investigate the properties

of evolving graphs and the associated dynamic networks. The

properties of these matrices are established and their relevance

to the properties of dynamic wireless multihop networks are

introduced.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">650</field>
<field name="author">SehChun Ng</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">Energy Savings Achievable in Connection Preserving Energy Saving Algorithms</field>
<field name="keyword">sensor network</field>
<field name="keyword"> log-normal shadowing</field>
<field name="keyword"> transmit power</field>
<field name="keyword"> energy savings</field>
<field name="abstract">Energy saving is an important design consideration

in wireless sensor networks. In this paper, we analyze the energy

savings that can be achieved in a sensor network where each

sensor is capable of reducing its transmission power from a

maximum power pm, compared with that in a sensor network

where each sensor can only transmit at a constant power level pm.

To achieve a fair comparison, we assume sensors in both types of

sensor networks are connected to the same set of neighbors, i.e. no

connection is lost as a result of a sensor reducing its transmission

power. We further assume that sensors are distributed in a

given area following a Poisson distribution with known node

density and the radio propagation is described by a log-normal

model. Ignoring boundary effect, we establish analytically the

probability for a sensor to achieve an energy saving of at least

h dB. We also obtain the expected percentage of energy savings

which can be substantial. The research reported in the paper

helps to answer questions such as whether the energy savings

achieved by using a sensor with a variable-transmission-power

(and the consequent extension of its lifetime) justify the additional

cost involved in manufacturing it.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">651</field>
<field name="author">Felix Werner</field>
<field name="author">Joaquin Sitte</field>
<field name="author">Frederic Maire</field>
<field name="title">Visual Topological Mapping and Localisation using Colour Histograms</field>
<field name="keyword">Mobile robot</field>
<field name="keyword"> omni-vision</field>
<field name="keyword"> topological mapping</field>
<field name="keyword"> topological localisation</field>
<field name="keyword"> Monte-Carlo localisation</field>
<field name="keyword"> colour histogram</field>
<field name="keyword"> FPGA</field>
<field name="abstract">In this paper we present a system for appearance-based topological mapping and localisation using vision data. The algorithms are designed for robots which are equipped with FPGA~cameras. Such cameras do not provide the entire image to the robot but simple image features like colour histograms. 



Our mapping approach exploits the continuity of the visual appearance of consecutive images from the robots exploration traversal. For topologically mapping the environment colour histograms are clustered whereby each cluster represents a place.



We use a Monte-Carlo localisation strategy combined with the topological map to localise the robot. For a robot equipped with a panoramic camera the proposed strategy works reasonably well, and is capable of overcoming the challenges of severe perceptual aliasing which occurs because of using simple image features and a sparse environment representation through the topological map.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">652</field>
<field name="author">Xiaohong Zhang</field>
<field name="author">Hongxing Wang</field>
<field name="author">Andrew Smith</field>
<field name="author">Brian Lovell</field>
<field name="title">Corner Detection Based on Gradient Correlation Matrices of Planar Curves</field>
<field name="keyword">corner detector</field>
<field name="keyword"> image processing</field>
<field name="abstract">An efficient and novel technique is developed for detecting and localizing corners of planar curves. This paper begins with a discussion of the gradient feature distribution of planar curves, followed by constructing Gradient Correlation Matrices (GCMs) over the region of support (ROS) of planar curves. It is shown that the eigen-structure and determinant of GCMs encode the geometric feature of the curves, such as curvature features and dominant points. Further, the determinant of the GCM is defined as the cornerness measure of planar curves. A comprehensive performance evaluation of the proposed detector is performed using the ACU and Error Index criteria. The corresponding results demonstrate that the GCM detector has a strong corner position response, and possesses better detection and localization performance than several traditional methods.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">653</field>
<field name="author">Desmond Chik</field>
<field name="author">Jochen Trumpf</field>
<field name="author">Nicol N. Schraudolph</field>
<field name="title">Using an Adaptive VAR model for motion prediction in 3D hand tracking</field>
<field name="abstract">A robust VAR-based (vector auto-regressive) model is introduced for motion prediction in 3D hand tracking. This dynamic VAR motion model is learned in an online manner. The kinematic structure of the hand is accounted for in the form of constraints when solving for the parameters of the VAR model. Also integrated into the motion prediction model are adaptive weights that are optimised according to the reliability of past predictions. Experiments on synthetic and real hand sequences show a substantial improvement in tracking performance when the robust VAR motion model is used. In fact, utilising the robust VAR model allows the tracker to handle fast out-of-plane hand movement with severe self-occlusion.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">654</field>
<field name="author">Lei Wang</field>
<field name="author">Luping Zhou</field>
<field name="author">Chunhua Shen</field>
<field name="title">A fast algorithm for creating a compact and discriminative visual codebook</field>
<field name="abstract">In patch-based object recognition, using a compact visual

codebook can boost computational efficiency and reduce memory cost.

Nevertheless, compared with a large-sized codebook, it also risks the loss

of discriminative power. Moreover, creating a compact visual codebook

can be very time-consuming, especially when the number of initial visual

words is large. In this paper, to minimize its loss of discriminative power,

we propose an approach to build a compact visual codebook by maximally

preserving the separability of the object classes. Furthermore, a

fast algorithm is designed to accomplish this task effortlessly, which can

hierarchically merge 10,000 visual words down to 2 in ninety seconds.

Experimental study shows that the compact visual codebook created in

this way can achieve excellent classification performance even after a

considerable reduction in size.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">655</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Real-time pedestrian detection using a boosted multi-layer classifier</field>
<field name="abstract">Techniques for detecting pedestrian in still images have

attached considerable research interests due to its wide applications

such as video surveillance and intelligent transportation

systems. In this paper, we propose a novel simpler

pedestrian detector using state-of-the-art locally extracted

features, namely, covariance features. Covariance

features were originally proposed in [1, 2]. Unlike the work

in [2], where the feature selection and weak classifier training

are performed on the Riemannian manifold, we select

features and train weak classifiers in the Euclidean space

for faster computation. To this end, AdaBoost with weighted

Fisher linear discriminant analysis based weak classifiers

are adopted. Multiple layer boosting with heterogeneous

features is constructed to exploit the efficiency of the Haarlike

feature and the discriminative power of the covariance

feature simultaneously. Extensive experiments show that by

combining the Haar-like and covariance features, we speed

up the original covariance feature detector [2] by up to an

order of magnitude in processing time without compromising

the detection performance. For the first time, the proposed

work enables covariance feature based pedestrian

detection to work real-time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">656</field>
<field name="author">Chris McCarthy</field>
<field name="author">Nick Barnes</field>
<field name="author">Kaarin Anstey</field>
<field name="author">Mark Horswill</field>
<field name="title">Towards a Hazard Perception Assistance System Using visual Motion</field>
<field name="abstract">We report on preliminary work in the application of low-

level visual motion cues to identify p otential hazards during on-road

driving. In conjunction with a clinical study of hazard p erception in older

age drivers, we consider the detection of a range of hazardous scenarios

identi ed as particularly challenging for older drivers in video sequences

used in the clinical study. Central to our approach is the use of visual

motion as a means of estimating self motion, from which we identify

optical ow due to other motions in the scene. We report results obtained

using the same hazard p erception test used in clinical trials.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">657</field>
<field name="author">Guido Governatori</field>
<field name="title">Labelled Modal Tableaux</field>
<field name="keyword">labelled deduction</field>
<field name="keyword"> modal logic</field>
<field name="abstract">Labelled tableaux are extensions of semantic

 tableaux with annotations (labels, indices) whose

 main function is to enrich the modal object language

 with semantic elements. This paper consists of three

 parts. In the first part we consider some options

 for labels: simple constant labels vs labels with

 free variables, logic depended inference rules vs

 labels manipulation based on a label algebra. In the

 second and third part we concentrate on a particular

 labelled tableaux system called KEM using free

 variable and a specialised label

 algebra. Specifically in the second part we show how

 labelled tableaux (KEM) can account for different

 types of logics (e.g., non-normal modal logics and

 conditional logics). In the third and final part we

 investigate the relative complexity of labelled

 tableaux systems and we show that the uses of KEM's

 label algebra can lead to speed up on proofs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">658</field>
<field name="author">Guido Governatori</field>
<field name="author">Duy Hoang Pham</field>
<field name="author">Simon Raboczi</field>
<field name="author">Andrew Newman</field>
<field name="author">Subhasis Thakur</field>
<field name="title">On Extending RuleML for Modal Defeasible Logic</field>
<field name="keyword">RuleML</field>
<field name="keyword"> Defeasible Logic</field>
<field name="abstract">In this paper we present a general methodology to

 extend Defeasible Logic with modal operators. We

 motivate the reasons for this type of extension and

 we argue that the extension will allow for a robust

 knowledge framework in different application

 areas. The paper presents an extension of RuleML to

 capture Modal Defeasible Logic.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">659</field>
<field name="author">Lixiang Xiong</field>
<field name="author">Lavy Libman</field>
<field name="author">Guoqiang Mao</field>
<field name="title">On Cooperative Communication in Ad-Hoc Networks: The Case for Uncoordinated Location-Aware Retransmission Strategies</field>
<field name="abstract">Cooperative communication methods in wireless networks, ranging from relaying by a common neighbor over a single wireless hop to opportunistic routing at the network layer, have been shown in recent years to offer significant performance gains over traditional approaches that ignore the broadcast nature of the wireless medium, and are particularly valuable in environments prone to channel shadowing and fading, such as mobile ad-hoc networks. A common feature of various cooperative methods proposed in the literature is the coordination required to discover the available neighbors and determine the optimal one(s) to be involved in the cooperation. However, the overhead cost of such coordination may be prohibitive in networks with highly dynamic topology (e.g. due to high mobility), as the discovery and negotiation overheads may negate much of the cooperation gains. Accordingly, we present the case for uncoordinated cooperative retransmission, where a node overhearing a frame may retransmit it "blindly" without any prior coordination with the transmitter, intended receiver, or any other neighbors in the vicinity. We pose and solve the problem of finding the optimal retransmission probability as a function of location, and characterize the optimal uncoordinated cooperation region through the solution of an integral equation that depends only on the a priori node density and wireless propagation model. Through numerical evaluation, we demonstrate that uncoordinated cooperation provides a low-overhead viable alternative, with a frame delivery probability that can even exceed that of coordinated cooperation methods in situations with high noise (or low transmission power) and high node density.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">660</field>
<field name="author">Guido Governatori</field>
<field name="author">Subhasis Thakur</field>
<field name="author">Duy Hoang Pham</field>
<field name="title">A Compliance Model of Trust</field>
<field name="keyword">trust</field>
<field name="keyword"> formal contract language</field>
<field name="keyword"> trust</field>
<field name="abstract">We present a model of past interaction trust model based on compliance of expected behaviours</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">661</field>
<field name="author">Jyun-Hao Huang</field>
<field name="author">Zia Ali</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Content-based Image Retrieval via Subspace-projected Salient Features</field>
<field name="keyword">image retrieval</field>
<field name="keyword"> saliency feature</field>
<field name="keyword"> subspace projection</field>
<field name="keyword"> LDA</field>
<field name="abstract">In this paper we present a novel image representation method which treats images as frequency histograms of salient features. The histograms are computed making use of linear discriminant analysis (LDA). The method employs saliency feature extraction and image binarisation. Then subspace-projected features are extracted. Using the saliency maps as the positive and negative labels, the image features are mapped onto a lower-dimensional space using LDA. This enables us to construct a 3D-histogram by direct binning on the feature space. This gives rise to a "cube" of image features which have been projected onto a lower-dimensional space so as to maximise the separability of the salient regions with respect to the background. Image retrieval can be performed by computing the distances between the histograms for the query image and the images in the database. We demonstrate our algorithm on a realworld database and compare our results to those yielded by codebook representation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">662</field>
<field name="author">Brian Davis</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Adam Funk</field>
<field name="author">Valentin Tablan</field>
<field name="author">Kalina Bontcheva</field>
<field name="author">Hamish Cunningham</field>
<field name="author">Siegfried Handschuh</field>
<field name="title">RoundTrip Ontology Authoring</field>
<field name="keyword">Ontology</field>
<field name="keyword"> Natural Language</field>
<field name="keyword"> Semantic</field>
<field name="abstract">Controlled Language (CL) for Ontology Editing tools offer an attractive alternative for naive users wishing to create ontologies, but they are still required to spend time learning the correct syntactic structures and vocabulary in order to use the Controlled Language properly. This paper extends previous work (CLOnE) which uses standard NLP tools to process the language and manipulate an ontology. Here we also generate text in the CL from an existing ontology using template-based (or shallow) Natural Language Generation (NLG). The text generator and the CLOnE authoring process combine to form a RoundTrip Ontology

Authoring environment: one can start with an existing imported ontology or one originally produced using CLOnE, (re)produce the Controlled Language, modify or edit the text as required and then turn the text back into the ontology in the CLOnE environment. Building on previous methodology we undertook an evaluation, comparing the RoundTrip Ontology Authoring process with a well-known ontology editor; where previous work required a CL reference manual with several examples in order to use the controlled language, the use of NLG reduces this learning curve for users and improves on existing results for basic ontology editing tasks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">663</field>
<field name="author">Renato Iannella</field>
<field name="author">G.R. Gangadharan</field>
<field name="author">Michael Weiss</field>
<field name="author">Vincenzo D Andrea</field>
<field name="title">Exploring the Intellectual Rights in the Mashup Ecosystem</field>
<field name="abstract">Over the last three years there has been a rapid proliferation of mashups as an emerging paradigm of Web 2.0. Mashups are applications that combine data and services provided through several open APIs, allowing the quick creation of custom applications by users. However, the intellectual rights associated with services and data associated with mashups are not focused intensively. In this paper, we explore the actors and roles involved in a mashup ecosystem and analyze the intellectual rights associated with mashups.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">664</field>
<field name="author">Nianjun Liu</field>
<field name="author">Nathan Brewer</field>
<field name="title">Using the Shape Characteristics of Rain to Identify and Remove Rain from Video</field>
<field name="keyword"/>
<field name="abstract">Dynamic weather effects such as rain cause rapid, distract-ing motion in a video sequence. This paper aims to remove rain and

similar effects from video footage using a multi-step approach; Regions are identified as being potentially affected by rain if they exhibit a short-duration intensity spike. Falling rain drops are imaged by a video camera in a predictable way, as a streak with a consistent range of possible as-pect ratios. To preserve scene motion, regions identified by this criterion are investigated, and those that do not fit into the expected range of aspect ratios are ignored. Information about the direction of rainfall is

also used to reduce false detections. The effectiveness of this technique is shown on a number of video sequences. The method presented provides advantages over existing techniques.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">665</field>
<field name="author">Junlei Song</field>
<field name="author">Nianjun Liu</field>
<field name="author">Li Cheng</field>
<field name="author">Dianhong Wang</field>
<field name="author">Ke Zhang</field>
<field name="author">Lan Du</field>
<field name="title">Soil Moisture Prediction with Feature Selection Using a Neural Network</field>
<field name="abstract">For the problem of soil moisture prediction, existing approaches in literature [6, 11] usually utilize as many decisive

factors as possible, e.g. rainfall, solar irradiance,drainage, etc. However, the redundancy aspect of the decisive

factors has not been studied rigorously. Previous research work in data mining has shown that removing redundant

features improves rather than deteriorates the prediction accuracy. In this paper, we propose an approach

to the problem of soil moisture prediction, which integrates two components: feature selection and prediction model: A

method is proposed for feature selection that effectively removes the redundant decisive factors; This is followed by

a feedforward neural network to make prediction based on the retained (i.e. non-redundant) decisive factors. Empirical

simulations demonstrate the effectiveness of the proposed approach. In particular, with the help of the proposed

feature selection component to remove redundant decisive factors, the proposed approach is shown to give better prediction

accuracy with lower data collection cost.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">666</field>
<field name="author">Golam Sarwar</field>
<field name="author">Roksana Boreli</field>
<field name="author">Emmanuel Lochin</field>
<field name="title">Evaluation of DCCP for VoIP on satellite links</field>
<field name="keyword">Congestion control</field>
<field name="keyword"> TCP friendly</field>
<field name="keyword"> multimedia</field>
<field name="keyword"> DCCP</field>
<field name="abstract">We present experimental results for the performance of selected voice codecs using the Datagram Congestion Control Protocol (DCCP) with TCP-Friendly Rate Control (TFRC) congestion control mechanism over a satellite link. We evaluate the performance of both constant and variable data rate speech codecs (G.729, G.711 and Speex) for a number of simultaneous calls, using the ITU E-model and identify problem areas and potential for improvement. Our experiments are done on a commercial satellite service using a data stream generated by a VoIP application, configured with selected voice codecs and using the DCCP/CCID4 Linux implementation. We analyse the sources of packet losses which are a main contributor to reduced voice quality when using CCID4 and additionally analyse the effect of jitter which is one of the crucial parameters contributing to VoIP quality and has, to the best of our knowledge, not been considered previously in the published DCCP performance results. We propose modifications to the CCID4 algorithm and demonstrate how these improve the VoIP performance, without the need for additional link information other than what is already monitored by CCID4 (which is the case for Quick Start). We also demonstrate the fairness of the proposed modifications to other flows. We identify the additional benefit of DCCP when used in VoIP admission control mechanisms and draw conclusions about the advantages and disadvantages of the proposed DCCP/ CCID4 congestion control mechanism for use with VoIP applications.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">667</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Fang Chen</field>
<field name="title">Think before you talk: An empirical study of Relationship between Speech pauses and Cognitive load</field>
<field name="keyword">Cognitive Load</field>
<field name="keyword"> Measurement</field>
<field name="keyword"> Speech Features</field>
<field name="keyword"> Pauses</field>
<field name="abstract">Measuring a user s level of cognitive load while they are interacting with the system could offer another dimension to the development of adaptable user interfaces. High levels of cognitive load affect performance and efficiency. However, current methods of measuring cognitive load are physically intrusive and interrupt the task flow. Certain speech features have been shown to change under high levels of load and are good candidates for cognitive load indices for usability evaluation and automatic adaptation of an interface or work environment. A speech-based dual-task user study is presented in which we explore the behaviour of speech pause features in natural speech. The experiment yielded new results confirming that speech pauses are useful indicators of high load versus low load speech. We report an increase in the percentage of time spent pausing from low load to high load tasks. We interpret these results within the framework of Baddeley s modal model of working memory and detail how such a measure could be utilized in the cognitive load measurement.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">668</field>
<field name="author">Fang Chen</field>
<field name="author">Yong Sun</field>
<field name="title">An Efficient Unification-based Multimodal Language Processor for Multimodal Input Fusion</field>
<field name="abstract">Multimodal User Interaction technology aims at building natural and intuitive interfaces allowing a user to interact with computer in a way similar to human-to-human communication, for example, through speech and gestures. As a critical component in a multimodal user interface, Multimodal Input Fusion explores ways to effectively derive the combined semantic interpretation of user inputs through multiple modalities. Based on state of art review on multimodal input fusion approaches, this chapter presents a novel approach to multimodal input fusion based on speech and gesture; or speech and eye tracking. It can also be applied for other input modalities and extended to more than 2 modalities. It is the first time that a powerful combinational categorical grammar is adopted in multimodal input fusion. The effectiveness of the approach has been validated through user experiments, which indicated a low polynomial computational complexity while parsing versatile multimodal input patterns. It is very useful for mobile context. Future trends in multimodal input fusion will be discussed at the end of this chapter.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">669</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Statistical characterization of the dynamic narrowband body area channel</field>
<field name="abstract">A statistical characterization of the narrowband dynamic human on-body area channel, with application to biomedical/health information monitoring, is presented based on measured received signal amplitude. We consider varying amounts of body movement, and a variety of transmit-receive pair (Tx-Rx) locations on the human body. The characterization is presented for two different frequencies, near the 900 MHz and 2400 MHz Industrial, Scientific and Medical (ISM) bands. Common distributions used to describe fading statistics are compared to the received signal component for nine different Tx-Rx pair locations for the subject's body standing, walking and running. The Lognormal distribution provides a good

fitting model, particularly when the subject's body is moving. The fit is independent of Tx-Rx pair locations. The Rayleigh distribution is a very poor fit to the received signal amplitude statistics.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">670</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Stability of narrowband dynamic body area channel</field>
<field name="keyword">Body-Area-Networks</field>
<field name="keyword"> Coherence Time</field>
<field name="abstract">The stability of a dynamic narrowband on-body area channel is

characterized based on real-time measurements of the time domain

channel response at frequencies near the 900MHz and 2400MHz,

Industrial, Scientific and Medical (ISM) bands. A new parameter,

\emph{channel variation factor}, characterizes channel coherence

time. Body movement is considered at various transmit-receive pair

locations on the human body. Movement has considerable impact on the

stability of the channel, a reasonable assumption for coherence time

is approximately 10ms and there is greater temporal stability at the

lower frequency.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">671</field>
<field name="author">Giulia Piovan</field>
<field name="author">Iman Shames</field>
<field name="author">Francesco Bullo</field>
<field name="author">Brian Anderson</field>
<field name="author">Baris Fidan</field>
<field name="title">On frame and orientation localization for relative sensing networks</field>
<field name="abstract">We develop a novel localization theory for planar

networks of nodes that measure each other s relative position,

i.e., we assume that nodes do not have the ability to perform

measurements expressed in a common reference frame. We

begin with some basic definitions of frame localizability and

orientation localizability. Based on some key kinematic relationships,

we characterize orientation localizability for networks

with angle-of-arrival sensing. We then address the orientation

localization problem in the presence of noisy measurements.

Our first algorithm computes a least-square estimate of the

unknown node orientations in a ring network given angle-ofarrival

sensing. For arbitrary connected graphs, our second

algorithm exploits kinematic relationships among the orientation

of node in loops in order to reduce the effect of noise.

We establish the convergence of the algorithm, and through

some simulations we show that the algorithm reduces the meansquare

error due to the noisy measurements.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">672</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Close Target Reconnaissance Using Autonomous UAV Formations</field>
<field name="abstract">In this paper the problem of close target reconnaissance

by a formation of 3 unmanned aerial vehicles

(UAVs) is considered. The overall close target reconnaissance

(CTR) involves subtasks of avoiding obstacles or no-fly-zones,

avoiding inter-agent collisions, reaching a close vicinity of a

specified target position, and forming an equilateral triangular

formation around the target. The UAVs performing the task fly

at constant speeds. A decentralized control scheme is developed

for this overall task considering unidirectional sensing/control

architecture. Relevant analysis and simulation test results are

provided.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">673</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Alessandro Armando</field>
<field name="author">Gilles Dowek</field>
<field name="title">Automated Reasoning - 4th International Conference, IJCAR 2008</field>
<field name="abstract">This volume contains the papers presented at IJCAR 2008, the 4th International 

Joint Conference on Automated Reasoning, held on August 12-15, 2008, in Syd- 

ney (Australia). The IJCAR conference series is aimed at unifying the di erent 

research principles within Automated Reasoning. IJCAR 2008 is the fusion of 

several ma jor international events: 

 CADE: The International Conference on Automated Deduction, 

 FroCoS: The Symposium on Frontiers of Combining Systems, 

 FTP: The Workshop on First-order Theorem Proving, and 

 TABLEAUX: The Conference on Analytic Tableaux and Related Methods. 

Previous versions of IJCAR were held at Seattle (USA) in 2006, Cork (Ire- 

land) in 2004 and Siena (Italy) in 2001. 

These proceedings comprise 4 contributions by invited speakers, 26 research 

papers and 13 system descriptions. It also includes a short overview of the 

CASC-J4 competition for automated theorem proving systems that was con- 

ducted during IJCAR 2008. The invited speakers were Hubert Comon-Lundh, 

Nachum Dershowitz, Aarti Gupta, and Carsten Lutz. Their talks covered a broad 

spectrum of Automated Reasoning, viz., veri cation of security protocols, proof 

theoretical frameworks for rst-order logic, automated decision procedures and 

software veri cation, and description logics. 

The contributed papers were selected from 80 research paper submissions 

and 17 system description submissions. Each submission was reviewed by at 

least three reviewers, and decisions were reached after two weeks of discussion 

through an electronic Program Committee meeting. The submissions, reviews 

and discussion were coordinated using the EasyChair conference management 

system. The accepted papers spanned a wide spectrum of research in Automated 

Reasoning, including saturation, equational reasoning and uni cation, automata- 

based methods, description logics and related logics, sati ability modulo theory, 

decidable logics, reasoning about programs, and higher-order logics. 

The Herbrand Award for distinguished contributions to automated reasoning 

was presented to Edmund M. Clarke in recognition of his role in the invention 

of model checking and his sustained leadership in the area for more than two 

decades. The selection committee for the Herbrand Award consists of the pre- 

vious award winners of the last ten years, the trustees of CADE Inc., and the 

IJCAR 2008 Program Committee. The Herbrand award ceremony and the ac- 

ceptance speech by Professor Clarke were part of the conference programme. 

In addition to the Program Committee and the reviewers, many people con- 

tributed to the success of IJCAR 2008. Geo Sutcli e served as the publicity 

chair and organized the systems competition, CASC-J4. The IJCAR steering 

committee consisted of Alessandro Armando, Franz Baader (chair), Peter Baum- 

gartner, Alan Bundy, Gilles Dowek, Ra jeev Gor e, Bernhard Gramlich, John 

Harrison, and Ullrich Hustadt. Special thanks go to Andrei Voronkov for his 

EasyChair system, which makes many tasks of a program chair much easier.

We would like to thanks all people involved in organizing IJCAR 2008, as 

well as the sponsors the Australian National University, Intel, Microsoft Research 

and NICTA. 

May 2008 

Alessandro Armando 

Peter Baumgartner 

Gilles Dowek</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">674</field>
<field name="author">Gernot Heiser</field>
<field name="title">Hypervisors for Consumer Electronics</field>
<field name="keyword">virtual machines</field>
<field name="keyword"> consumer electronics</field>
<field name="keyword"> operating systems</field>
<field name="abstract">Virtualization, well established in enterprise, is finding its way into embedded systems. However, the use cases differ dramatically between the domains, and this results in significant differences in the requirements on the virtual-machine technology.



This paper examines a number of typical virtualization use cases from the CE domain, and the resulting requirements imposed on the hypervisor. We find that enterprise-style hypervisors are ill-matched to the requirements of the embedded domain, which are characterised by low-overhead communication, real-time capability, small memory footprint, small trusted computing base, and fine-grained control over security. We present the OKL4 hypervisor, a member of the L4 microkernel family, designed for embedded-systems use. We outline OKL4's relevant properties with an emphasis on its security mechanisms, and compare its performance to a version of Xen that has recently been promoted for CE use. We conclude that OKL4 is superior to enterprise-style hypervisors for use in CE devices.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">675</field>
<field name="author">Olivier Mehani</field>
<field name="author">Roksana Boreli</field>
<field name="author">Thierry Ernst</field>
<field name="title">Analysis of TFRC in Disconnected Scenarios and Performance Improvements with Freeze-DCCP</field>
<field name="keyword">DCCP</field>
<field name="keyword"> TFRC</field>
<field name="keyword"> congestion control</field>
<field name="keyword"> IPv6 mobility</field>
<field name="keyword"> delay tolerant networks</field>
<field name="abstract">We present enhancements to the TCP-Friendly Rate Control mechanism (TFRC) which

are designed to better cope with the intermittent connectivity available to

mobile devices or in Delay Tolerant Networks. Our aim is to prevent losses

during disconnected periods and quickly adapt to new network conditions. We

propose to suspend the transmission before disconnections occur in a way

inspired by Freeze-TCP, then probe, in a new way, the network after reconnecting

to enable full use of the newly available bandwidth.



We first evaluate the potential performance gains for realistic network

parameters. We then describe the proposed additions to TFRC and their

implementation within the Datagram Congestion Control Protocol (DCCP) in ns-2.



Comparisons of simulation results for example mobility scenarios show that the

proposed enhancements enable faster recovery upon reconnection as well as

significantly improved adjustment to the newly available network conditions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">676</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Characterization of the Dynamic Narrowband On-Body to Off-Body Area Channel</field>
<field name="abstract">A characterization of the dynamic narrowband on-body to off-body

area channel is presented based on real-time measurements of the

time domain channel response at carrier frequencies near the 900MHz

and 2400MHz Industrial, Scientific and Medical (ISM) bands. A

statistical characterization is presented of received signal

amplitude when the subject's body is standing and walking,

transmitted from the body to a receiver, Rx, off the body, with

various orientations of the subject's body with respect to the

receiver, and various distances from the receiver. Two locations of

the transmitter, Tx, on the human body are considered. The Lognormal

distribution provides a good fitting model with and without

movement. Further, the stability is characterized based on a measure

of channel response variance, which is called here the channel

variation factor and can characterize the channel coherence time.

The on-body to off-body area channel is determined to be generally

stable over a period of 25ms, but the amount of stability is found

to be dependent both on movement, Tx location on the body, and

carrier frequency.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">677</field>
<field name="author">Daniel Harabor</field>
<field name="author">Adi Botea</field>
<field name="title">Hierarchical path planning for multi-size agents in heterogeneous environments</field>
<field name="keyword"/>
<field name="abstract">Path planning is a central topic in games and other research areas, such as robotics. Despite this, very little research addresses problems involving agents with multiple sizes and terrain traversal capabilities.

In this paper we present a new planner, Hierarchical Annotated A* (HAA*), and demonstrate how a single abstract graph can be used to plan for agents with heterogeneous sizes and terrain traversal capabilities. 

Through theoretical analysis and experimental evaluation we show that HAA* is able to generate near-optimal solutions to a wide range of problems while maintaining an exponential reduction in effort over low-level search. HAA* is also shown to require just a fraction of the storage space needed by the original grid map.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">678</field>
<field name="author">Liang Wang</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">ROBUST LANGUAGE IDENTIFICATION BASED ON FUSED PHONOTACTIC INFORMATION WITH MLKSFM PRE-CLASSIFIER</field>
<field name="keyword">Robust language identification</field>
<field name="keyword"> pre-classification</field>
<field name="keyword"> score fusion</field>
<field name="keyword"> phase spectrum</field>
<field name="keyword"> likelihood score bias</field>
<field name="abstract">In this paper we propose a novel language identification system which utilizes fused phonotactic information. Phase spectrum of speech signal is used with the magnitude spectrum in order to obtain a more robust feature representation. Parallel Broad Phone-class Recognition followed by Language Model (PBPRLM) is used in order to remove the bias of the likelihood scores introduced by the inequality of the sizes of phone inventories in traditional PPRLM system. The likelihood scores from the MFCC based and group-delay based PPRLM and PBPRLM systems are fused together by using Gaussian Mixture Model. Also a pre-classification based on Kohonen s map is used in order to maintain the system robustness while handling large amount of target languages. We achieve EER of 6.7% on the 2005 NIST LRE and LID recognition rate of 83.9% on a 22-language task, both with the proposed novel system.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">679</field>
<field name="author">Olivier Mehani</field>
<field name="author">Roksana Boreli</field>
<field name="title">Adapting TFRC to Mobile Networks with Frequent Disconnections</field>
<field name="keyword">TFRC</field>
<field name="keyword"> congestion control</field>
<field name="keyword"> MANET</field>
<field name="keyword"> mobility</field>
<field name="keyword"> DCCP</field>
<field name="abstract">In the context of mobile and pervasive networking, it is not uncommon to experience frequent loss of connectivity. Congestion control algorithms usually mistake resulting packets losses for congestion events and unnecessarily reduce their network usage.

We propose an enhancement to the TCP-Friendly Rate Control (TFRC) protocol to overcome this issue. Our contribution proposes to suspend the data transfer, in a way similar to Freeze-TCP, when an upcoming disconnection is predicted. Furthermore, a probing mechanism is introduced to enable fast adaptation to new network conditions.

We present a description of the additional freezing and resuming mechanisms. This proposal has been implemented in ns-2. Simulation results are thus compared to the regular behavior and show encouraging improvements.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">680</field>
<field name="author">Roy Timo</field>
<field name="author">Alex Grant</field>
<field name="author">Terrence Chan</field>
<field name="author">Gehard Kramer</field>
<field name="title">Source Coding for a Simple Network with Receiver Side Information</field>
<field name="abstract">We consider the problem of source coding with receiver side information for the simple network proposed by R. Gray and A. Wyner in 1974. In this network, a transmitter must reliably transport the output of two correlated information sources to two receivers using three noiseless channels: a public channel which connects the transmitter to both receivers, and two private channels which connect the transmitter directly to each receiver. We extend Gray and Wyner s original problem by permitting side information to be present at each receiver. We derive inner and outer bounds for the achievable rate region and, for three special cases, we show that the outer bound is tight.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">681</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="title">Non-Coherent Receiver with Fractional Sampling for Impulsive UWB Systems</field>
<field name="abstract">We propose a low complexity noncoherent receiver operating at twice the symbol-rate for systems where each data symbol consists of multiple frames/chips. The receiver does not require explicit timing and channel estimation. It implements simple delayed-autocorrelation, followed by sampling at twice the symbol-rate. Simulation results show the receiver achieves performance close to a conventional one with perfect timing.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">682</field>
<field name="author">June Verner</field>
<field name="author">Jennifer Sampson</field>
<field name="author">Narciso Cerpa</field>
<field name="author">Steven Bleistein</field>
<field name="title">What factors lead to software project failure and whose fault was it?</field>
<field name="keyword">software project failure</field>
<field name="keyword"> software project management</field>
<field name="keyword"> failure factors</field>
<field name="keyword"> project risk</field>
<field name="abstract">Most of the literature discussing project failure tends to be rather general, supplying us with lists of risk and failure factors, and focusing on the negative business effects of the failure. Very little research has attempted an in-depth investigation of a number of failed projects to identify exactly what factors are behind the failure and who is to blame for such failures. In this research we analyze data from 70 failed projects. This data provides us with practitioners perspectives on 57 development and management factors for projects they considered failures. Our results show that all projects investigated suffered from many failure factors. For the projects we reviewed the number of failure factors ranged from 6 to 48. While there does not appear to be any single set of overarching failure factors we discovered that nearly all of the projects suffered from organizational factors outside the project manager s control. Many projects additionally had some project management problems. We conclude with suggestions for minimizing the four most common failure factors.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">683</field>
<field name="author">John S. Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">An ontological account of Donald Sch _n's reflection in designing</field>
<field name="keyword">design model(s)</field>
<field name="keyword"> design ontology</field>
<field name="keyword"> design process(es)</field>
<field name="keyword"> design science</field>
<field name="keyword"> reflective practice</field>
<field name="abstract">This paper proposes an ontological model of Donald Sch _n s notion of reflection in the domain of designing. We address two views of this notion. First, we present a functional view that describes reflection in terms of the designer s interactions with the design object and their intended and unintended consequences that then drive further interactions. Second, we present a constructional view that models reflection as a mechanism with a set of properties that distinguish it from other processes in designing. We describe both views using the function-behaviour-structure (FBS) ontology. This elaborates existing accounts of reflection and locates it within a uniform framework of designing, which lays the foundations for a multi-disciplinary approach to studying reflection.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">684</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">Daniel Lewis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Characterisation of large-scale fading in BAN channels</field>
<field name="keyword"/>
<field name="abstract">Measurements results of dynamic BAN channel measurements at 820 MHz with characterisation of large-scale fading due to movement of test subjects</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">685</field>
<field name="author">Tet Fei Yap</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Fang Chen</field>
<field name="title">PHASE BASED FEATURES FOR COGNITIVE LOAD MEASUREMENT SYSTEM</field>
<field name="keyword">feature extraction</field>
<field name="keyword"> speech classification</field>
<field name="keyword"> cognitive load</field>
<field name="keyword"> group delay</field>
<field name="keyword"> frequency modulation</field>
<field name="abstract">Current automatic cognitive load measurement system uses MFCC and prosody as features. These features utilize only the magnitude part of the speech spectrum. This paper aims to improve the performance of this baseline system by introducing phase based features into the system. Three new features are proposed: group delay features, all-pole model based FM features and zero crossing count based FM features. Decrease in performance is observed when phase based features are considered individually or when concatenated with the baseline features. Marginal improvement in performance is observed only when group delay features are fused with baseline feature using linear combination score level fusion.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">686</field>
<field name="author">Laurianne Sitbon</field>
<field name="author">Patrice Bellot</field>
<field name="title">A readability measure for an information</field>
<field name="keyword">information retrieval</field>
<field name="keyword"> readability</field>
<field name="keyword"> dyslexia</field>
<field name="abstract">This paper provides a framework for an estimation of relevance that takes readability into account. The experiments focus on

dyslexic users and are realized on TREC and CLEF ad-hoc task data.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">687</field>
<field name="author">Paul Bannerman</field>
<field name="author">Liming Zhu</field>
<field name="title">Standardization as a Business Ecosystem Enabler</field>
<field name="abstract">This paper develops theoretical positions on the role of the standards body as an enabler in business ecosystems. We advocate that standards can sequester competition from cooperation; the internal consistency of business models is critical for performance; effective standards are non-prescriptive; and requirements specification and compliance checking are essential. We illustrate our theory using a standards body with which we have been working.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">688</field>
<field name="author">kirsty Kitto</field>
<field name="author">Peter Bruza</field>
<field name="author">Laurianne Sitbon</field>
<field name="title">Generalising Unitary Time Evolution</field>
<field name="abstract">In this third Quantum Interaction (QI) meeting it is time to examine

our failures. One of the weakest elements of QI is its continuing lack of

models displaying proper Schr odinger-type time evolution. This paper

presents an overview of the modern generalised approach to the derivation

of time evolution equations in physics, showing how the notion of

symmetry is essential to the extraction of operators in quantum theory.

The form that symmetry might take in non-physical models is explored,

with a number of viable avenues identified.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">689</field>
<field name="author">Laurianne Sitbon</field>
<field name="author">Peter Bruza</field>
<field name="title">On the relevance of documents for semantic representation</field>
<field name="keyword">Natural Language Techniques and Documents</field>
<field name="keyword"> Semantic spaces</field>
<field name="keyword"> Random projection</field>
<field name="abstract">The subject of this paper is the quality

of semantic vector representation with random

projection under various conditions. The main effect

we are watching is the size of the context in which

words are observed. We are also interested in the

stability of such representations since they rely on

random initialization. In particular we investigate the

possibility of stabilising terms representations through

documents representations. The quality of semantic

representation was tested by means of synonym finding

task using the TOEFL test on the TASA corpus. It was

found that small context windows produces the best

semantic vectors with 59.4 % of the questions correctly

answered. Iteratively repeting the projection process

between terms and documents representations was

found not to improve the stability of the representation.

It was also found not to improve the average quality of

representations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">690</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">An iterative projections algorithm for ML factor analysis</field>
<field name="keyword">Factor analysis</field>
<field name="keyword"> maximum likelihood</field>
<field name="keyword"> information geometry</field>
<field name="abstract">Alternating minimization of the information divergence is used

to derive an effective algorithm

for maximum likelihood (ML) factor analysis. The proposed algorithm is derived

as an iterative alternating projections procedure on a model

family of probability distributions defined on the factor analysis model and

a desired family of probability distributions constrained to be concentrated

on the observed data. The algorithm presents the advantage of being simple to implement and stable to converge.

A simulation example that

illustrates the effectiveness of the proposed algorithm for ML factor analysis

is presented.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">691</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">A Novel AIC Variant for Linear Regression Models Based on a Bootstrap Correction</field>
<field name="abstract">The Akaike information criterion, AIC, and its corrected version,

$AIC_{c}$ are two methods for selecting normal linear regression

models. Both criteria were designed as estimators of the expected

Kullback-Leibler information between the model generating the data

and the approximating candidate model. In this paper, a new

corrected variants of AIC is derived for the purpose of small

sample linear regression model selection. The new proposed variant of

AIC is based on asymptotic approximation of bootstrap type

estimates of Kullback-Leibler information. Simulation results which

illustrate better performance of the proposed AIC correction when

applied to polynomial regression in comparison to AIC, $AIC_{c}$

and other criteria are presented. Asymptotic justifications for

the proposed criterion are provided in the Appendix.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">692</field>
<field name="author">Michael Maher</field>
<field name="title">Open Sequential Global Constraints</field>
<field name="abstract">We address the class of global constraints that involve sequences of variables of arbitrary length. Open forms of these constraints allow the appending of variables to a sequence during the execution of a constraint program. We characterize a monotonicity property that justifies the use of filtering algorithms for a (closed) monotonic constraint also for the open version of the constraint. This characterization makes it easy to recognise when a constraint has the property or not. For constraints not having this property, the characterization provides a tight approximation that can be used to filter the constraint while it is open. Finally, we outline how the implementations of some monotonic constraints can handle the appending of variables.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">693</field>
<field name="author">Bo Yin</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Fang Chen</field>
<field name="title">VOICED/UNVOICED PATTERN-BASED DURATION MODELING FOR LANGUAGE IDENTIFICATION</field>
<field name="keyword">duration modeling</field>
<field name="keyword"> articulatory features</field>
<field name="keyword"> quantization</field>
<field name="keyword"> language identification</field>
<field name="abstract">As a major prosodic feature, duration-based features have been explored in related areas but the existing approaches require manually annotated corpus to train the segmentation models, which may be cost and time-consuming. In this paper, a novel duration modeling approach is proposed, in which the segmentation is implemented by using articulatory feature like voicing status. A pair of connected unvoiced and voiced segments is considered as the unit, and the duration of each segment is normalized for each utterance and then quantized into 20 discrete ranges. The ranges of units are later considered as symbol sequences and are modeled by n-gram model, to capture the temporal pattern, which is hypothesized to be different in different languages. The experiments based on the NIST LRE 2005 tasks show a relative 19.7% EER improvement by introducing the proposed duration modeling-based system into a fusion system containing two GMM-UBM based acoustic systems using MFCC and pitch+intensity features.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">694</field>
<field name="author">Bo Yin</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Fang Chen</field>
<field name="title">Language-dependent Contribution Measuring and Weighting for Combining Likelihood Scores in Language Identification Systems</field>
<field name="keyword">Language identification</field>
<field name="keyword"> language-dependent contribution</field>
<field name="keyword"> speech recognition</field>
<field name="keyword"> fusion</field>
<field name="abstract">Developing a fusion-based system is one of the key research issues in modern Language Identification (LID) systems. In this paper we investigate existing fusion techniques for LID systems and propose an alternative solution.By directly utilizing language-dependent contribution information, a novel Language-Dependent Weighting approach is introduced and implemented. We investigate various contribution measures, including LID performances, likelihood ratios, and Kullback-Leibler divergence. These measures are conducted from either development datasets or class models. The advantage of using language-dependent weighting over language-independent weighting is illustrated using a Language-Dependent Contribution Map. Both the OGI and CallFriend databases show a very similar contribution pattern which is related to language characteristics. Experiments on the NIST LRE 2003 task and OGI database demonstrate that the proposed fusion technique outperforms other recent fusion techniques when the amount of available development data is limited. In particular, the system based on Kullback-Leibler divergence achieved the best performance while eliminating the need for development data</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">695</field>
<field name="author">Nathan Robinson</field>
<field name="author">Charles Gretton</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">Abdul Sattar</field>
<field name="title">A Compact and Efficient SAT Encoding for Planning</field>
<field name="keyword"/>
<field name="abstract">In the planning-as-SAT paradigm there have been numerous recent developments towards improving the speed and scalability of planning at the cost of nding a step-optimal parallel plan. These developments have been towards: (1) Query strategies that efficiently yield approximately optimal plans, and (2) Having a SAT procedure compute plans from relaxed encodings of the corresponding decision problems in such a way that con icts in a plan arising from the relaxation are resolved cheaply during a post-processing phase. In this paper we examine a third direction of tightening constraints in order to achieve a more compact, efficient, and scalable SAT-based encoding of the planning problem. For the rst time, we use lifting (i.e., operator splitting) and factoring to encode the corresponding n-step decision problems with a parallel action semantics. To ensure compactness we exploit reachability and neededness analysis of the plangraph. Our encoding also captures state-dependent mutex constraints computed during that analysis. Because we adopt a lifted action representation, our encoding cannot generally support full action parallelism. Thus, our approach could be termed approximate, planning for a number of steps between that required in the optimal parallel case and the optimal linear case. We per- form a detailed experimental analysis of our approach with 3 state-of-the-art SAT-based planners using benchmarks from recent international planning competitions. We nd that our approach dominates optimal SAT-based planners, and is more efficient than the relaxed planners for domains where the plan existence problem is hard.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">696</field>
<field name="author">Nathan Robinson</field>
<field name="author">Charles Gretton</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">Abdul Sattar</field>
<field name="title">Propositional Probabilistic Planning-as-Satisfiability using Stochastic Local Search</field>
<field name="keyword"/>
<field name="abstract">Recent times have seen the development of planners that exploit advances in SAT(is ability) solving technology to achieve good performance. In that spirit we develop the approximate contingent planner PSLSPLAN. Our approach is based on local search for solving stochastic SAT (SSAT) representations of probabilistic planning problems. PSLSPLAN rst constructs an SSAT representation of the n-timestep probabilistic plangraph for the problem at hand. It then iteratively calls a local search procedure to nd a linear plan (sequence of actions) which achieves the goal (i.e. satis es the SSAT formula) with non-zero probability. Linear plans thus generated are merged to create a single contingent plan. Successive iterations progress from deciding the outcomes of stochastic actions in order to nd a linear plan quickly, to sampling the outcomes of actions. Consequently, PSLSPLAN efficiently nds a linear plan which logically achieves the goal. Over time it re nes its contingent plan for likely scenarios. We empirically evaluate PSLSPLAN on benchmarks from the probabilistic track of the 5th International Planning Competition.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">697</field>
<field name="author">Nathan Robinson</field>
<field name="author">Charles Gretton</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">Abdul Sattar</field>
<field name="title">A Compact and Efficient SAT Encoding for Planning</field>
<field name="keyword">Planning SAT Satisfiability Planning</field>
<field name="keyword">as</field>
<field name="keyword">Satisfiability Satisficing</field>
<field name="abstract">In SAT-based planning, optimality can be traded for efficiency and scalability. One approach to this is to plan using a relaxed encoding of the corresponding decision problems. Another approach is to adopt a sub-optimal query strategy e.g. avoid solving some decision problems necessary for optimality. Our approach is to tighten planning constraints to achieve a compact encoding of the decision problems, with a restricted form of parallel action semantics. Our approach is more efficient than existing optimal and relaxed SAT-based planners on some hard benchmarks.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">698</field>
<field name="author">M Pini</field>
<field name="author">F. Rossi</field>
<field name="author">B. Venable</field>
<field name="author">Toby Walsh</field>
<field name="title">Dealing with incomplete agents' preferences and an uncertain agenda in group decision making via sequential majority voting</field>
<field name="abstract">We consider multi-agent systems where 

agents' preferences may be incomplete and are aggregated 

via sequential majority voting. We compute the 

most preferred candidate using a 

a sequence of pairwise comparisons

(also called an agenda) where each comparison is a 

weighted majority vote among the agents.

Incompleteness in the agents' preferences

is common in many real-life settings, due to privacy issues or an

ongoing elicitation process.

We study the computational complexity of determining

various notions of winner.

We show that it is easy to determine

if a candidate wins whatever the agenda. 

On the other hand, it is hard to know whether a

candidate wins in at least one agenda for 

at least one completion of the agents' preferences. 

This is also true if the agenda is a balanced tree. 

These results are useful for preference elicitation, since they 

help understand the complexity of tasks such as constructive and 

destructive control, as well as losers' determination. 

Moreover, since they also show that it is difficult for a candidate to 

know whether they can or cannot win, 

they may not drop out of the election.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">699</field>
<field name="author">Shai Haim</field>
<field name="author">Toby Walsh</field>
<field name="title">Online Estimation of SAT Solving Runtime</field>
<field name="abstract">We present an online method for estimating the cost of solving SAT problems. 

Modern SAT solvers present several challenges to estimate 

search cost including non-chronological backtracking, learning and restarts. 

Our method uses a linear model trained on data gathered

at the start of search. We show the effectiveness of this method 

using random and structured problems. We demonstrate 

that predictions made in early restarts can be used to 

improve later predictions. We also show that we can 

use such cost estimations to select a solver from a portfolio.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">700</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="author">Hiroyuki Yoshida</field>
<field name="author">Janne Nappi</field>
<field name="title">Feature Selection with BIC and PCA for Polyps Detection in CT Colonography</field>
<field name="abstract">Polyps detection is often performed based on a high number of

image features extracted from segmented polyp regions. Selecting

among these image features the most relevant ones for polyps

detection is regarded as one of the important factors for the

large number of false positives in CAD schemes. This selection is

facilitated by representing these image features in a transformed

space. In this paper, we propose a method for image features

selection based on PCA and BIC. A BIC model selection criterion variant

to choose the number of principal components to retain for optimal polyps

detection is proposed. A simulation study that illustrate the

performance of the proposed method for polyps detection is presented.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">701</field>
<field name="author">elodie morin</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">Electronic Colon Cleansing in the Wavelet Domain with a light Expectation-Maximization Segmentation from a Coarse Scale</field>
<field name="abstract">Electronic colon cleansing is a technology that aims at removing residual colonic material from the acquired images. We present here a pipeline that process an automatic electronic colon cleansing. Indeed, remaining colonic material are problematic for an optimal viewing of the colon. The residual colonic material is detected and located on a coarse approximation which is provided by a discrete wavelet transform. It makes faster the completion of an Expectation-Maximization algorithm for segmenting the tissue types within the image. The cleansing task is then performed in the wavelet domain over all the coefficients and we retrieve a ``cleansed'' image at the complete resolution.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">702</field>
<field name="author">Ju Lynn Ong</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">A Method for Classification of Candidate Lesions in CT Colonography</field>
<field name="abstract">Ordinary Procrustes analysis (OPA) attempts to match two shape

configurations and investigate differences between the two. In

this paper, we propose to use OPA and curvature characteristics to

analyse and classify candidate lesions into spherical polyps, flat

polyps and folds. This method can be used to locate these lesions

in CT colonography particularly flat polyps which are usually

missed by traditional methods. Our results on real CTC datasets

show promising results and identify most of the flat polyps that

would otherwise be missed but with a compromise on specificity as

many flat non polyp lesions are also similar in shape and size.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">703</field>
<field name="author">Changseok Bae</field>
<field name="author">Yuk Ying Chung</field>
<field name="author">Mohd Afizi Shukran</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Wei-Chang Yeh</field>
<field name="title">An Intelligent Classification Algorithm for LifeLog Multimedia Applications</field>
<field name="keyword">multimedia information retrieval</field>
<field name="keyword"> audio classification</field>
<field name="keyword"> multimedia diary</field>
<field name="abstract">LifeLog can be used as a stand-alone consumer device to serve as a powerful automated multimedia diary and scrapbook. By using a search engine interface, the user can easily retrieval a specific thread of past transactions, or recall a few seconds ago or from many years earlier in as much detail as is desired, including imagery , audio, or video event. In this paper, we have studied and implemented three audio classification algorithms. The testing results show that KNN is the best classifier to classify the multimedia data.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">704</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Multi-Region Probabilistic Histograms for Robust and Scalable Identity Inference</field>
<field name="keyword">surveillance</field>
<field name="keyword"> scalability</field>
<field name="keyword"> visual words</field>
<field name="keyword"> low resolution</field>
<field name="keyword"> identity inference</field>
<field name="abstract">We propose a scalable face matching system capable of dealing with faces subject to several concurrent and uncontrolled factors, such as variations in pose, expression, illumination, resolution, as well as scale and misalignment problems. Each face is described in terms of multi-region probabilistic histograms of visual words, followed by a normalised distance calculation between the histograms of two faces. We also propose a fast histogram approximation method which dramatically reduces the computational burden with minimal impact on discrimination performance. Experiments on the "Labeled Faces in the Wild" dataset (unconstrained environments) as well as FERET (controlled variations) show that the proposed system obtains performance on par with a more complex method and displays a clear advantage over predecessor systems. Furthermore, the use of multiple regions (as opposed to a single overall region) improves accuracy in most cases, especially when dealing with illumination changes and very low resolution images. The experiments also show that normalised distances can noticeably improve robustness by partially counteracting the effects of image variations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">705</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Manfred Schmidt-Schauss</field>
<field name="title">Parameter Reduction in Grammar-Compressed Trees</field>
<field name="keyword">Compression</field>
<field name="keyword"> Straight-Line</field>
<field name="keyword"> Context-Free Tree Grammar</field>
<field name="abstract">Trees can be conveniently compressed with

linear straight-line

context-free tree grammars.

Such grammars generalize straight-line context-free

string grammars which are widely used in the development

of algorithms that execute directly on compressed structures

(without prior decompression).

It is shown that every linear straight-line

context-free tree grammar can be transformed in polynomial

time into a monadic (and linear) one. A tree grammar is monadic if each

nonterminal uses at most one context parameter.

Based on this result, a polynomial time algorithm is presented

for testing whether a given nondeterministic tree

automaton with sibling constraints accepts a tree

given by a linear straight-line context-free tree grammar.

It is shown that if tree grammars are nondeterministic or

non-linear, then reducing their numbers of parameters cannot

be done without an exponential blow-up in grammar size.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">706</field>
<field name="author">Jenny Liu</field>
<field name="author">Minan Tan</field>
<field name="author">Ian Gorton</field>
<field name="author">Andrew John Clayphan</field>
<field name="title">An Autonomic Middleware Solution for Coordinating Multiple QoS Controls</field>
<field name="keyword">SOA</field>
<field name="keyword"> adaptation</field>
<field name="keyword"> process</field>
<field name="abstract">Adaptive self-managing applications can adapt their behavior

through components that monitor the application behavior and

provide feedback controls. This paper outlines an autonomic approach

to coordinate multiple controls for managing service quality using executable

control models. In this approach, controls are modeled as process

models. Moreover, controls with cross-cutting concerns are provisioned

by a dedicated process model. The flexibility of this approach allows

composing new controls from existing control components. The coordination

of their dependencies is performed within a unified architecture

framework for modeling, deploying and executing these models. We integrate

the process modeling and execution techniques into a middleware

architecture to deliver such features. To demonstrate the practical utilization

of this approach, we employ it to manage fail-over and over-loading

controls for a service oriented loan brokering application. The empirical

results further validate that this solution is not only sensitive to resolving

cross-cutting interests of multiple controls, but also lightweight as it

incurs low computational overhead.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">707</field>
<field name="author">Gunawan Herman</field>
<field name="author">Getian Ye</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Yang Wang</field>
<field name="author">Fang Chen</field>
<field name="title">Mutual Information-Based Method for Selecting Informative Feature Sets</field>
<field name="keyword">feature selection</field>
<field name="keyword"> mutual information</field>
<field name="keyword"> data mining</field>
<field name="keyword"> machine learning</field>
<field name="abstract">Feature selection is one of the fundamental problems in pattern

recognition and data mining. A popular and effective approach to

feature selection is based on information theory, namely the mutual information

of features and class variable. In this paper we compare six different

mutual information-based feature selection methods. Based on the

analysis of the comparison results, we propose a new mutual informationbased

feature selection method. By taking into account both the classdependent

and class-independent correlation among features, the proposed

method selects a less redundant, and thus more informative set of

features. The advantage of the proposed method over other methods is

demonstrated by the results of experiments on UCI datasets.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">708</field>
<field name="author">Jonathan Sprinkle</field>
<field name="author">J. Mikael Eklund</field>
<field name="author">Humberto Gonzalez</field>
<field name="author">Esten Ingar Grotli</field>
<field name="author">Ben Upcroft</field>
<field name="author">Alex Makarenko</field>
<field name="author">William Uther</field>
<field name="author">Hugh Durrant-Whyte</field>
<field name="author">S. Shankar Sastry</field>
<field name="title">Model-Based Design: A report from the trenches of the DARPA Urban Challenge</field>
<field name="keyword">Urban Vehicle</field>
<field name="keyword"> Autonomous System</field>
<field name="abstract">The impact of model-based design on the software engineering community is impressive, and recent research in model transformations, and elegant behavioral specifications of systems has the potential to revolutionize the way in which systems are designed. Such techniques aim to raise the level of abstraction at which systems are specified, to remove the burden of producing application-specific programs with general-purpose programming. For complex real-time systems, however, the impact of model-driven approaches is not nearly so widespread. In this paper, we present a perspective of model-based design researchers who joined with software experts in robotics to enter the DARPA Urban Challenge, and to what extent model-based design techniques were used. Further, we speculate on why, according to our experience and the testimonies of many teams, the full promises of model-based design were not widely realized for the competition. Finally, we present some thoughts for the future of model-based design in complex systems such as these, and what advancements in modeling are needed to motivate small-scale projects to use model-based design in these domains.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">709</field>
<field name="author">Li Cheng</field>
<field name="author">S.V.N. Vishwanathan</field>
<field name="author">Xinhua Zhang</field>
<field name="title">Consistent Image Analogies using Semi-supervised Learning</field>
<field name="abstract">In this paper we study the following problem: given two source images A and A _, and a target 

image B, can we learn to synthesize a new image B _ which relates to B in the same way that A _ 

relates to A? We propose an algorithm which a) uses a semi-supervised component to exploit the 

fact that the target image B is available a priori, b) uses inference on a Markov Random Field 

(MRF) to ensure global consistency, and c) uses image quilting to ensure local consistency. Our 

algorithm can also deal with the case when A is only partially labeled, that is, only small parts of 

A _ are available for training. Empirical evaluation shows that our algorithm consistently produces 

visually pleasing results, outperforming the state of the art.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">710</field>
<field name="author">Choon Hui Teo</field>
<field name="author">Amir Globerson</field>
<field name="author">Sam Roweis</field>
<field name="author">Alexander J. Smola</field>
<field name="title">Convex Learning with Invariances</field>
<field name="abstract">Incorporating invariances into a learning algorithm is a common problem in ma- 

chine learning. We provide a convex formulation which can deal with arbitrary 

loss functions and arbitrary losses. In addition, it is a drop-in replacement for most 

optimization algorithms for kernels, including solvers of the SVMStruct family. 

The advantage of our setting is that it relies on column generation instead of mod- 

ifying the underlying optimization problem directly.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">711</field>
<field name="author">M. Weimer</field>
<field name="author">A. Karatzoglou</field>
<field name="author">Q.V. Le</field>
<field name="author">Alexander J. Smola</field>
<field name="title">COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking</field>
<field name="abstract">In this paper, we consider collaborative ltering as a ranking problem. We present 

a method which uses Maximum Margin Matrix Factorization and optimizes rank- 

ing instead of rating. We employ structured output prediction to optimize directly 

for ranking scores. Experimental results show that our method gives very good 

ranking scores and scales well on collaborative ltering tasks.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">712</field>
<field name="author">Julian McAuley</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Alex Smola</field>
<field name="title">Robust Near-Isometric Matching via Structured Learning of Graphical Models</field>
<field name="keyword">shape matching</field>
<field name="keyword"> graphical models</field>
<field name="keyword"> structured learning</field>
<field name="keyword"> support vector machines</field>
<field name="abstract">Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by ``almost isometric'' transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is \emph{robust} in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">713</field>
<field name="author">Julian McAuley</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Marconi Barbosa</field>
<field name="title">Graph Rigidity, Cyclic Belief Propagation and Point Pattern Matching</field>
<field name="keyword">Point pattern matching</field>
<field name="keyword"> graph matching</field>
<field name="keyword"> graphical models</field>
<field name="keyword"> belief propagation</field>
<field name="keyword"> global rigidity</field>
<field name="keyword"> chordal graphs</field>
<field name="abstract">A recent paper [1] proposed a provably optimal polynomial time 

method for performing near-isometric point pattern matching by means of exact 

probabilistic inference in a chordal graphical model. Its fundamental result is that 

the chordal graph in question is shown to be globally rigid, implying that exact 

inference provides the same matching solution as exact inference in a complete 

graphical model. This implies that the algorithm is optimal when there is no noise 

in the point patterns. In this paper, we present a new graph that is also globally 

rigid but has an advantage over the graph proposed in [1]: Its maximal clique size 

is smaller, rendering inference significantly more efficient. However, this graph is 

not chordal, and thus, standard Junction Tree algorithms cannot be directly 

applied. Nevertheless, we show that loopy belief propagation in such a graph 

converges to the optimal solution. This allows us to retain the optimality guarantee 

in the noiseless case, while substantially reducing both memory requirements and 

processing time. Our experimental results show that the accuracy of the proposed 

solution is indistinguishable from that in [1] when there is noise in the point 

patterns.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">714</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Alex Smola</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Quoc Le</field>
<field name="title">Estimating Labels from Label Proportions</field>
<field name="keyword">Semi-supervised learning</field>
<field name="keyword"> Gaussian processes</field>
<field name="abstract">Consider the following problem: given sets of unlabeled observations,

 each set with known label proportions, predict the labels of another

 set of observations, also with known label proportions. This

 problem appears in areas like e-commerce, spam filtering and

 improper content detection. We present consistent estimators which can

 reconstruct the correct labels with high probability in a uniform

 convergence sense. Experiments show that our method works well in practice.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">715</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Le Song</field>
<field name="author">Alex J. Smola</field>
<field name="title">Kernelized Sorting</field>
<field name="keyword">Sorting</field>
<field name="keyword"> Matching</field>
<field name="keyword"> Kernels</field>
<field name="keyword"> Object Alignment</field>
<field name="keyword"> Hilbert Schmidt Independence Criterion</field>
<field name="abstract">Object matching is a fundamental operation in data analysis. It

 typically requires the definition of a similarity measure between

 the classes of objects to be matched. Instead, we develop an

 approach which is able to perform matching by requiring a similarity

 measure only within each of the classes. This is achieved by

 maximizing the dependency between matched pairs of observations by

 means of the Hilbert Schmidt Independence Criterion. This problem can

 be cast as one of maximizing a quadratic assignment problem with

 special structure and we present a simple algorithm for finding a

 locally optimal solution.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">716</field>
<field name="author">Scott Sanner</field>
<field name="author">Robby Goetschalckx</field>
<field name="author">Kurt Driessens</field>
<field name="title">Real-time Bayesian Search Control for MDPs</field>
<field name="keyword">MDPs</field>
<field name="keyword"> Search Control</field>
<field name="keyword"> Bayesian Methods</field>
<field name="abstract">Recent real-time, search-based solution algorithms toMarkov

decision processes (MDPs) maintain both upper and lower

bounds on the value function at all stages of execution prior

to convergence. While many algorithms already use this information

to detect convergence and to prioritize state exploration,

we propose a novel Bayesian interpretation of these

bounds to evaluate the myopic value of perfect information

(VPI) of exactly knowing a state s value. This VPI analysis

permits two general Bayesian search control modifications to

existing algorithms: (1) we can use it to dynamically adapt

the backup depth in real-time, search-based bounded dynamic

programming approaches, and (2) we can use it for real-time

policy evaluation with a bounded value function produced

by an anytime algorithm. We empirically evaluate both of

these modifications to existing algorithms and analyze their

performance. While Bayesian search control may reduce the

number of backups until convergence for (1), it offers more

substantive improvements for (2), making it an attractive realtime

policy evaluation approach for MDPs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">717</field>
<field name="author">Olivier Chapelle</field>
<field name="author">Chuong B. Do</field>
<field name="author">Quoc Le</field>
<field name="author">Alex Smola</field>
<field name="author">Choon Hui Teo</field>
<field name="title">Tighter Bounds for Structured Estimation</field>
<field name="keyword">large margin methods</field>
<field name="keyword"> structured estimation</field>
<field name="keyword"> tighter surrogate loss</field>
<field name="keyword"> optimization</field>
<field name="keyword"> non-convex</field>
<field name="abstract">Large-margin structured estimation methods work by minimizing a

convex upper bound of loss functions. While they allow for

efficient optimization algorithms, these convex formulations are not

tight and sacrifice the ability to accurately model the true loss.

We present tighter non-convex bounds based on generalizing the

notion of a ramp loss from binary classification to structured

estimation. We show that a small modification of existing

optimization algorithms suffices to solve this modified problem. On

structured prediction tasks such as protein sequence alignment and

web page ranking, our algorithm leads to improved accuracy.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">718</field>
<field name="author">Liming Zhu</field>
<field name="title">Beyond Design-by-Committee: How to Push Alternative Ideas through Open Standards and Open Source</field>
<field name="abstract">As many standards are dominated by major industry players and burdened with heavy processes, it is increasingly difficult to revamp a standard or introduce new ideas into it. Such slow-adapting monster

standards sometimes hurt industries. This talk will explore how some lightweight open standards are introduced through non-traditional ways, from small industry coalitions to academic publications. These

standards are often fully equipped with open source implementations to bootstrap their adoption. We share our own experience in proposing alternatives to big standards (e.g. WSDL/BPEL) and supplementing them through open source implementations. Our approaches range from major paradigm-shifting overhaul to minor micro-format driven extensions. These open standards complement big standards through injecting fresh ideas into the space and pushing openness at different levels.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">719</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Towards Business Value Driven Management of Business Processes and Service-Oriented Architectures</field>
<field name="keyword">Web service management</field>
<field name="keyword"> business-driven management</field>
<field name="keyword"> policy-driven management</field>
<field name="abstract">The existing IT (information technology) system management solutions are predominantly focused on optimization of technical QoS metrics (e.g., response time, availability). However, financial (e.g., profit) and other business value metrics (e.g., customer satisfaction) are more important than technical QoS metrics. This presentation discusses challenges and some achieved results in business value driven management of business processes implemented with service-oriented architectures. 

 We begin with a brief overview of the context of this research the ongoing NICTA work on advanced business process modeling, analysis, implementation, monitoring, and adaptation, as well as our collaboration with the lending industry in Australia. 

 Then, we focus on the sub-project on modeling of business value and business strategies and use of this additional information for run-time adaptation that maximizes business value. We explain the need to model business strategies (e.g., maximizing customer satisfaction) and diverse types of business values (not only financial ones). To illustrate how some of these challenges can be addressed, we first present WS-Policy4MASC, our language for specification of monitoring and control (particularly, adaptation) policies. It extends the Web Services Policy Framework (WS-Policy) by defining new types of policy assertions (goal, action, utility, probability, and meta-policy assertions). Among its original contributions are specification of diverse business values and specification of various control strategies maximizing different business values. We evaluated feasibility of the WS-Policy4MASC solutions by using this language and the corresponding algorithms for business value maximization in the several middleware middleware frameworks: MASC (Manageable and Adaptable Service Compositions), MiniMASC, and MiniMASC+MiniZinc. In particular, the use of the powerful constraint programming language MiniZinc and its solver in MiniMASC+MiniZinc enables finding globally optimal (from the business viewpoint) adaptation of multiple business processes executing at the same time. We examined usefulness of these solutions on a set of realistic scenarios. Further, to facilitate development of Web service systems that can be managed with WS-Policy4MASC and the MASC middleware and to improve alignment between run-time management activities and design-time models, we developed novel Unified Modeling Language (UML) profiles for WS-Policy4MASC.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">720</field>
<field name="author">Paul Bannerman</field>
<field name="title">Risk Management in Software Projects: Is it Good Enough?</field>
<field name="abstract">This presentation reports recent research on risk and risk management in software projects, and draws implications for software process practice. Software projects tend to be complex, highly uncertain ventures. Risk management is recognised as an important process in ensuring software quality and successful project outcomes. High variability in software project performance, however, raises questions about the suitability and effectiveness of risk management prescriptions and practices. The theory and practice of risk and risk management in software projects are reviewed. It is found that risk management research lags the needs of practice and risk management practice lags the prescriptions of research. As a step toward filling these gaps, a proposal is made for moving beyond methodology-based risk management to capability-based threat management.



The notion of risk in software project literature is examined and found to be narrowly conceived for the needs of practice. For example, it focuses on foreseeable threats for which a probability can be determined. Unforeseeable threats are not directly accommodated. The strengths and weaknesses of the three main risk management approaches (checklists, analytical frameworks, and process models) are also discussed.



A study of software project management and risk management practices in a sample of Australian public sector agencies is described. Ten major risk factors are found and compared with private sector experience. The study findings challenge some conventional conceptions of risk and project management. For example, it was found that software projects do not conform to a uniform structure, as assumed in much of the literature. This introduces variations in the risk and project management challenges they face. Findings also suggest that formal project management may be neither necessary nor sufficient for project success.



An integrated threat management framework is proposed to extend current conceptions and improve software project outcomes. The framework integrates issue management, risk management and crisis/disaster management in a way that enables projects to manage a broader spectrum of uncertainty than is possible under risk management as it is currently conceived. Building a real-time threat sense and respond management capability offers a way for software developing organisations to improve their capacity to manage uncertainty. Practical steps to move beyond risk management to threat management are outlined.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">721</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Industrial Standards for Web Services: Achieved Results and Challenges for the Future</field>
<field name="keyword">Web service</field>
<field name="keyword"> service-oriented architecture standard</field>
<field name="abstract">Web services are distributed computing application components that use a number of Extensible Markup Language (XML) based technologies to implement the service-oriented architecture (SOA). Implementation-independence of Web services technologies allows different businesses to collaborate and achieve common business goals despite the fact that the collaborating Web services can be distributed over the Internet, run on different platforms, and implemented in different programming languages. Web service technologies are already embedded in various products and services of all major computing companies and used for diverse purposes in enterprise computing.



Many Web service related technologies are standardized by various industrial standardization bodies (e.g., the World Wide Web Consortium - W3C, the Organization for the Advancement of Structured Information Standards - OASIS, the Object Management Group - OMG, and others). Some Web service technologies are not standardized, e.g., because they are not yet mature enough or because they attempt to solve problems in areas that are viewed as areas for differentiation and competition among companies (not for collaboration and standardization). Public opinions about the industrial standards for Web services vary widely. For example, while some experts advocate additional industrial standards, others consider that there are already too many standards and "standards " in this area. There are also many differences in opinion about usefulness and quality of particular industrial standards for Web services.



The expert panel at IEEE EDOC 2008 will discuss various issues related to the industrial standards for Web Services and will identify some of the challenges for the future. The panel is composed of experienced international experts from academia and industry, who will pass to the audience their insights and personal opinions (which need not be opinions of their employers). Members of the audience are encouraged to ask questions, state their own opinions, and initiate debate.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">722</field>
<field name="author">Paul Bannerman</field>
<field name="title">Risk Implications of Software Project Organization Structures</field>
<field name="keyword">Risk management</field>
<field name="keyword"> software projects</field>
<field name="keyword"> organizational structures</field>
<field name="abstract">Researching organization structures is no longer in fashion. However, a recent study uncovered gaps in our knowledge about the structure of software projects and their associated risks. This paper develops risk profiles from the literature for four common structures (functional, project, matrix and adhocracy forms) and validates them against data from a public sector study. Understanding structure as a source for project risk could unlock many performance problems in software projects.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">723</field>
<field name="author">Timothy Roscoe</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Gernot Heiser</field>
<field name="title">Hype and Virtue</field>
<field name="abstract">In this paper, we question whether hypervisors are really

acting as a disruptive force in OS research, instead arguing that they

have so far changed very little at a technical level. Essentially,

we have retained the conventional Unix-like OS interface and added a 

new ABI based on PC hardware which is highly unsuitable for most

purposes. Despite commercial excitement, focus on hypervisor design

may be leading OS research astray. However, adopting a different

approach to virtualization and recognizing its value to academic

research holds the prospect of opening up kernel research to new

directions.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">724</field>
<field name="author">Dhammika Elkaduwe</field>
<field name="author">Gerwin Klein</field>
<field name="author">Kevin Elphinstone</field>
<field name="title">Verified Protection Model of the seL4 Microkernel</field>
<field name="keyword">seL4</field>
<field name="keyword"> access control</field>
<field name="keyword"> proof</field>
<field name="keyword"> Isabelle/HOL</field>
<field name="abstract">This paper presents a machine-checked high-level security analysis of seL4 an evolution of the

L4 kernel series targeted to secure, embedded devices. We provide an abstract specification of the seL4 access

control system in terms of a classical take-grant model together with a formal proof of its decidability. Using the

decidability property we show how confined subsystems can be enforced. All proofs and specifications in this

paper are machine-checked and developed in the interactive theorem prover Isabelle/HOL</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">725</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Patrick C. K. Hung</field>
<field name="author">Claudio Bartolini</field>
<field name="title">Management of Service-Oriented Systems</field>
<field name="keyword">Service-oriented computing</field>
<field name="keyword"> quality of service (QoS)</field>
<field name="keyword"> application management</field>
<field name="abstract">Management (monitoring and control) of service-oriented systems is needed to ensure their regular operation, attain guaranteed quality of service (QoS), and accommodate changes. Monitoring is used to measure technical QoS (e.g., response time, throughput, availability) and/or business value metrics (e.g., profit, return on investment, number of customers, customer satisfaction). Control is used to reactively/proactively ensure that there are no faults and that the measured quantities are within desired boundaries. To successfully perform monitoring and control, run-time management issues should be considered during design-time software (and system) engineering activities. 

 Management of service-oriented systems can be viewed from several aspects. In this tutorial, we will emphasize QoS management and maximization of business value. The tutorial will first clarify the importance of these topics and why the widely used basic Web service technologies are not enough. Then, it will explain theoretical principles (e.g., service level agreement SLA, policy, intermediary, probe, and multiple request queues) for specification, monitoring, and control of QoS and business value attributes. Next, it will provide a critical analysis of several important research achievements and industrial products in this area. We will also present a brief introduction to business-driven IT management (BDIM) and possible approaches to extend management solutions maximizing QoS into solutions maximizing business value metrics. At the end, a number of open topics and resources for further study will be identified. 

 After attending this tutorial, participants will have general knowledge and understanding of the challenges and fundamental concepts related to the specification, monitoring, and control of QoS and business value metrics of (Web) services and business processes implemented with (Web) services, and open research issues. This knowledge can help them in making decisions about using some of the existing technologies in software systems they engineer and/or in conducting further research in the area.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">726</field>
<field name="author">Pranam Janney</field>
<field name="author">Glenn Geers</field>
<field name="title">Framework for Illumination Invariant Vehicular Traffic Density Estimation</field>
<field name="keyword">Intelligent Transport Systems (ITS)</field>
<field name="keyword"> Invariant Features of Local Textures (IFLT)</field>
<field name="keyword"> Support Vector Machines (SVM)</field>
<field name="keyword"> density estimation</field>
<field name="keyword"> traffic information</field>
<field name="keyword"> parameters</field>
<field name="keyword"> illumination invariance</field>
<field name="abstract">CCTV cameras are becoming a common fixture at the roadside. Their

use varies from traffic monitoring to security surveillance. In this paper a novel

technique, using Invariant Features of Local Textures (IFLT) &amp; Support Vector

Machine (SVM), for estimating vehicular traffic density on a road segment is presented.

The proposed approach is computationally efficient and robust to varying

illumination. Experimental results have shown that the proposed framework can

achieve high performance than extant state-of-the-art techniques in varying illumination

conditions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">727</field>
<field name="author">Jacky Keung</field>
<field name="title">Software Development Cost Estimation using Analogy: A Review</field>
<field name="keyword">Software Cost Estimation</field>
<field name="keyword"> Software Measurement</field>
<field name="keyword"> Analogy</field>
<field name="abstract">Software project managers require reliable methods for estimating software project costs, and it is especially important at the early stage of software cycle. For this purpose, analogy for software cost estimation has been considered as an suitable alternative to regression-based estimation method, and can be used successfully in many circumstances. 



Analogy is not a rocket science, our industrial partners have general background knowledges and amazed with its prediction performance but many still find this approach relative new, especially in many SME software development companies within Australia. 



This paper provides an comprehensive overview of the background and history and the underlying theory of analogy, published in major software engineering journals and conferences over the past 15 years. We also share our experience in the application of cost estimation using analogy, and discuss its strength and weakness.



Recommendations on the dataset quality evaluation and its relevance to the target problem for analogy are discussed in detail, allow researchers and project managers to better understand the nature of the analogy-based approach.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">728</field>
<field name="author">Chris McCarthy</field>
<field name="author">Nick Barnes</field>
<field name="author">Rob mahony</field>
<field name="title">A robust docking strategy for a mobile robot using flow field divergence</field>
<field name="abstract">We present a robust strategy for docking a mobile

robot in close proximity with an upright surface using

optical flow field divergence and proportional feedback control.

Unlike previous approaches, we achieve this without the need

for explicit segmentation of features in the image, and using

complete gradient-based optical flow estimation (i.e. no affine

models) in the optical flow computation. A key contribution

is the development of an algorithm to compute the flow field

divergence, or time-to-contact, in a manner that is robust to

small rotations of the robot during ego-motion. This is done by

tracking the focus of expansion of the flow-field and using this to

compensate for ego rotation of the image. The control law used

is a simple proportional feedback, using the unfiltered flow field

divergence as an input, for a dynamic vehicle model. Closedloop

stability analysis of docking under the proposed feedback

is provided. Performance of the flow field divergence algorithm

is demonstrated using off-board natural image sequences, and

the performance of the closed-loop system is experimentally

demonstrated by control of a mobile robot approaching a wall.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">729</field>
<field name="author">Kazuhiro Inaba</field>
<field name="author">Sebastian Maneth</field>
<field name="title">The Complexity of Translation Membership for Macro Tree Transducers</field>
<field name="abstract">Macro tree transducers (mtts) are

a useful formal model for XML query and transformation languages.

In this paper one of the fundamental decision problems on translations,

namely the "translation membership problem" is studied for mtts.

For a fixed translation, the translation membership problem asks whether

a given input/output pair is element of the translation.

For call-by-name mtts this problem is shown to be NP-complete.

The main result is that translation membership for

call-by-value mtts is in polynomial time.

For several extensions, such as addition of regular

look-ahead or the generalization to multi-return mtts,

it is shown that translation membership still remains in PTIME.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">730</field>
<field name="author">Michael Norrish</field>
<field name="title">Rewriting Conversions Implemented with Continuations</field>
<field name="keyword">Interactive theorem-proving</field>
<field name="keyword"> rewriting</field>
<field name="keyword"> continuations</field>
<field name="keyword"> functional programming</field>
<field name="abstract">We give an continuation-based implementation of rewriting for systems in the LCF tradition. These systems must construct explicit proofs of equations when rewriting, and currently do so in a way that can be very space-inef cient. An explicit representation of continuations improves performance on large terms, and on long-running computations.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">731</field>
<field name="author">Konrad Slind</field>
<field name="author">Michael Norrish</field>
<field name="title">A Brief Overview of HOL4</field>
<field name="abstract">The HOL4 proof assistant supports speci cation and proof in classical higher order logic. It is the latest in a long line of similar systems. In this short overview, we give an outline of the HOL4 system and how it may be applied in formal verification.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">732</field>
<field name="author">Yong Sun</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="author">Vera Chung</field>
<field name="title">Skipping Spare Information in Multimodal Inputs during Multimodal Input Fusion</field>
<field name="keyword">Processing of multimodal input</field>
<field name="keyword"> Spare information in multimodal input.</field>
<field name="abstract">In a multimodal interface, a user can use multiple modalities, such as speech, gesture, and eye gaze etc., to communicate with a system. As a critical component in a multimodal interface, multimodal input fusion explores the ways to effectively interpret the combined semantic interpretation of user s multimodal inputs. Although multimodal inputs may contain spare information, few multimodal input fusion approaches have tackled this issue. This paper proposes a novel multimodal input fusion approach to flexibly skip spare information in multimodal inputs and derive semantic interpretation of them. The experiment confirms that the proposed approach makes human-computer interaction more natural and smooth.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">733</field>
<field name="author">Andrew Dankers</field>
<field name="author">Nick Barnes</field>
<field name="author">Walter Bischof</field>
<field name="author">Alexaner Zelinsky</field>
<field name="title">Humanoid Vision Resembles Primate Archetype</field>
<field name="abstract">Perception in the visual cortex and dorsal stream of the primate brain

includes important visual competencies, such as: a consistent representation of visual

space despite eye movement; egocentric spatial perception; attentional gaze deploy-

ment; and, coordinated stereo fixation upon dynamic objects. These competencies

have emerged commensurate with observation of the real world, and constitute a

vision system that is optimised, in some sense, for perception and interaction. We

present a robotic vision system that incorporates these competencies. We hypothe-

sise that similarities between the underlying robotic system model and that of the

primate vision system will elicit accordingly similar gaze behaviours. Psychophys-

ical trials were conducted to record human gaze behaviour when free-viewing a

reproducible, dynamic, 3D scene. Identical trials were conducted with the robotic

system. A statistical comparison of robotic and human gaze behaviour has shown

that the two are remarkably similar. Enabling a humanoid to mimic the optimised

gaze strategies of humans may be a significant step towards facilitating human-like

perception.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">734</field>
<field name="author">Boris Aronov</field>
<field name="author">Mark de Berg</field>
<field name="author">Otfried Cheong</field>
<field name="author">Herman Haverkort</field>
<field name="author">Antoine Vigneron</field>
<field name="title">Sparse geometric graphs with small dilation</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> approximation algorithms</field>
<field name="abstract">Given a set S of n points in d dimensional Euclidean space, and an integer k such that 0 &lt;= k &lt; n, we show that a geometric graph with vertex set S, at most n _ 1 + k edges, maximum degree five, and dilation O(n/(k + 1)) can be computed in time O(n log n). For any k, we also construct planar n-point sets for which any geometric graph with n _ 1 + k edges has dilation Omega(n/(k + 1)); a slightly weaker statement holds if the points of S

are required to be in convex position.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">735</field>
<field name="author">John Lloyd</field>
<field name="author">Kee Siong Ng</field>
<field name="author">William Uther</field>
<field name="title">Bach: Probabilistic Declarative Programming</field>
<field name="keyword">Probabilistic Programming</field>
<field name="keyword"> Statistical Relational Learning</field>
<field name="abstract">This is a 2 page abstract describing the Bach language and how we use it to model stochastic systems. Its goal is to highlight similarities and differences between Bach and other, related, systems that will be discussed at the workshop.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">736</field>
<field name="author">renato iannella</field>
<field name="author">John Breslin</field>
<field name="author">Adrienne Felt</field>
<field name="author">Michael Maximilien</field>
<field name="title">1st International Workshop on Social Network Interoperability</field>
<field name="keyword">Social Networks</field>
<field name="keyword"> Interoperability</field>
<field name="abstract">The objective of the 1st International Workshop on Social Interoperable Networks - is to bring together the Social Networks community and to discuss the technical interoperability issues facing service providers and end users. The workshop will attract researchers, practitioners, end users, and providers to help understand the current state-of-the-art in social networks interoperability, and to plan pathways forward to enable the community to achieve these goals.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">737</field>
<field name="author">Christian Mueller-Tomfelde</field>
<field name="author">Fang Chen</field>
<field name="title">TEMPORAL AND SPATIAL ASPECTS OF POINTING GESTURES</field>
<field name="abstract">The detailed and profound understanding of the temporal and spatial organisation of human pointing actions is key to enable developers to build applications that successfully incorporate multimodal human computer interaction. Rather than discussing an ideal detection method for manual pointing we will discuss crucial aspects of pointing actions in time and space to develop the right solution for a particular application. 

One core element of pointing in the temporal domain is the so called dwell-time, the time span that people remain nearly motionless during pointing at objects to express their intention. We also discuss important findings about the spatial characteristics of the target representation for the pointing gesture.

The findings foster better understanding of the role of pointing gestures in combination with other modalities and inform developer with substantial knowledge about the temporal-spatial organisation of the pointing gesture.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">738</field>
<field name="author">Tao Yang</field>
<field name="author">Jinhong Yuan</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Convergence behavior analysis Analysis and Detection Switching for the Iterative Receiver of MIMO-BICM Systems</field>
<field name="keyword"/>
<field name="abstract">The iterative receivers with a list sphere detector (LSD) and a parallel interference canceller (PIC) for

MIMO-BICM systems are considered. The convergence behaviors of these iterative detection and decoding

schemes are analyzed via variance transfer (VT) functions. For ergodic channels, we show that the water fall

region of an iterative receiver can be predicted by using a variance exchange graph (VEG). For slow fading

channels, we show that the FER of the iterative receiver is essentially limited by the early interception

rate of the iterative receiver. Moreover, we show that the VT function of an LSD of a small list size is

superior to that of PIC at a high variance region whereas it is worse than that of PIC at a low variance

region. Then, we propose a detection switching (DSW) approach for the iterative receiver and present a

detection switching criterion based on cross entropy. By applying the DSW, we show that the near-capacity

performance can be achieved with a significantly reduced complexity.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">739</field>
<field name="author">Mark Reed</field>
<field name="title">The Business Case for Advanced WiMAX Receivers : Deployment Benefits in the Australian Environment</field>
<field name="keyword">wimax</field>
<field name="keyword"> advanced receivers</field>
<field name="keyword"> MIMO</field>
<field name="abstract">Optimisation of a WiMAX system can occur at various places in the system providing different benefits to the overall system performance. Optimisation is critical to maximise the efficiency of the wireless network reducing CAPEX and OPEX for the operators, improving service provision and the mobile user experience . Advanced receivers promise significant improvements to a WiMAX system performance. In this presentation we investigate the gains possible from advanced receivers and illustrate the system benefits in terms of cell range improvement and average data throughput for a user. Outdoor mobile wireless trial data collected from NICTA s advanced WiMAX solution is used to validate the improvements. Surprisingly for a very small improvement in receiver sensitivity, significant cell area improvements can result; likewise for a given cell range packet throughput rates can also significantly increase. We conclude by looking at Multiple-Input Multiple-Output (MIMO) or advanced multiple antenna structures and the additional benefits they will bring to the network.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">740</field>
<field name="author">Mark Reed</field>
<field name="title">Wireless Communication Modems: From Principles to Practice</field>
<field name="keyword"/>
<field name="abstract">This talk will overview the drivers for broadband wireless communications including the effect of new applications and the handset hardware that is now becoming available. It will review the challenges in deploying a reliable cost-effective wireless network and the challenges operators have in keeping their costs (CAPEX/OPEX) under control as the data requirements of the network increase. Insight will be provided into the network benefits of advanced receivers in the pursuit of a lower cost solution. Finally some insight will be given into future approaches to wireless broadband as a way of delivering on the promise of pervasive multi-megabit wireless broadband communications.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">741</field>
<field name="author">Andrew Zhang</field>
<field name="title">Techniques and Systems for Accurate Wireless Localization in Sports</field>
<field name="keyword">Wireless localization</field>
<field name="keyword"> sports</field>
<field name="abstract">Wireless localization [1{4] refers to locating the geographic coordinates of static or moving nodes

in a wireless system. The system could be an existing infrastructure, e.g., Cellular, WLAN or

sensor network, or one speci&#12;cally developed for localization in e.g., &#12;re&#12;ghting, military or

sports. Localization receives wide attention today, as it is becoming the basis for achieving

better system performance and service. Wireless localization is generally a challenging problem

due to the environment condition: lower signal-to-noise ratio (SNR), multipath, channel fading

and multiuser interference.

Localization has been extensively studied and applied in commercial products. GPS is the

&#12;rst successfully globally deployed service. E911 is another mass application of localization sys-

tems. However, current localization systems usually have low accuracy and updating frequency.

With the emerging need of localization in sports, new localization techniques and sys-

tems are desired. In this report, we will have an overview of current localization techniques and

systems, focusing on those with application potential in sports, and discuss the development of

future localization systems for applications like sports where high accuracy and high updating

frequency are required.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">742</field>
<field name="author">Claudio Bartolini</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Patrick C. K. Hung</field>
<field name="title">Business-Driven Management and Governance of Service-Oriented Systems</field>
<field name="keyword">Service-oriented computing</field>
<field name="keyword"> quality of service (QoS)</field>
<field name="keyword"> service level agreement (SLA)</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> business-driven IT management (BDIM)</field>
<field name="keyword"> IT governance</field>
<field name="keyword"> ITIL</field>
<field name="keyword"> COBIT</field>
<field name="keyword"> value-based software engineering (VBSE)</field>
<field name="abstract">Management (monitoring and control) of service-oriented systems is needed to ensure their regular operation, attain guaranteed quality of service (QoS), and accommodate changes. Monitoring measures technical QoS (e.g., response time, availability) and/or business value metrics (e.g., profit, return on investment, customer satisfaction). Control ensures (reactively and/or proactively) that there are no faults and that the measured quantities are within desired boundaries. IT (information technology) governance is a set of organization s policies, plans, and processes that direct how its IT resources are used over a longer time. To be successful, management and governance issues should be considered not only during deployment and run-time, but also during design-time software engineering activities.



We present how service-oriented software systems can be made more successful from the business viewpoint by using governance and management that maximizes business value metrics. The tutorial first clarifies importance of these topics and why the widely used basic Web service technologies are not enough. Then, it explains theoretical principles for specification, monitoring, and control of QoS and business value metrics. It also provides a critical analysis of several important research achievements and industrial products in this area. Then, we present an introduction to business-driven IT management (BDIM) and possible approaches to extend management solutions maximizing QoS into solutions maximizing business value metrics. Furthermore, we overview the major IT governance frameworks and discuss their relevance for value-based software engineering of service-oriented systems. At the end, a number of open topics and resources for further study are identified.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">743</field>
<field name="author">Franz Baader</field>
<field name="author">Andreas Bauer</field>
<field name="author">Alwen Tiu</field>
<field name="title">Matching linear and non-linear trace patterns with regular policies</field>
<field name="abstract">In this paper, we consider policies that are described by regular languages. 

Such regular policies L are assumed to describe situations that are problematic, 

and thus should be avoided. Given a trace 

pattern u, i.e., a sequence of action symbols and variables, were the variables

 stand for unknown (i.e., not observed) sequences of actions, we ask 

whether u potentially violates a given policy L, i.e., whether the variables 

in u can be replaced by sequences of actions such that the resulting trace 

belongs to L. We determine the complexity of this violation problem, depending

on whether trace patterns are linear or not, and on whether the 

policy is assumed to be xed or not.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">744</field>
<field name="author">Andreas Bauer</field>
<field name="author">Sophie Pinchinat</field>
<field name="title">A Topological Perspective on Diagnosis</field>
<field name="abstract">We propose a topological perspective on the diag- 

nosis problem for discrete-event systems. In an in nitary framework, 

we argue that the construction of a centralized diagnoser 

is conditioned by two fundamental properties: saturation and 

openness. We show that these properties are decidable for _- 

regular languages. Usually, openness is guaranteed implicitly 

in practical settings. In contrast to this, we prove that the 

saturation problem is PSPACE-complete, which is relevant for 

the overall complexity of diagnosis.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">745</field>
<field name="author">Andreas Bauer</field>
<field name="author">Jan J _rjens</field>
<field name="title">Security Protocols, Properties, and their Monitoring</field>
<field name="abstract">This paper examines the suitability and use of runtime veri cation 

as means for monitoring security protocols and their properties. 

In particular, we employ the runtime veri cation framework intro- 

duced in [5] to monitor complex, history-based security-properties 

of the SSL-protocol. We give a detailed account of the methodology, 

compare its formal expressiveness to prior art, and describe 

its application to an open-source Java-implementation of the SSL- 

protocol. In particular, we show how one can make use of runtime 

veri cation to dynamically enforce that assumptions on the 

crypto-protocol implementations (that are commonly made when 

statically verifying crypto-protocol speci cations against security 

requirements) are actually satis ed in a given protocol implementation

 at runtime. Our analysis of these properties shows that some 

important runtime correctness properties of the SSL-protocol exceed

 the commonly used class of safety properties, and as such 

also the expressiveness of other monitoring frameworks.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">746</field>
<field name="author">David Smith</field>
<field name="title">Technical Report Summary 1: Electromagnetic Characterisation through and around human body by simulation using SEMCAD X</field>
<field name="abstract">Contained in this report is some results of electromagnetic simulation using the male human body phantom provided by SPEAG to be used in SEMCAD X [1], proprietary electromagnetic modelling software, and an approximate uniform body and head which has been developed by the author, also in SEMCAD. The simulation in SEMCAD is via the finite difference time domain (FDTD) method, which is in some senses, a first principles form of electromagnetic modelling, working on blocks, or voxels (effectively volumetric pixels), which render a three-dimensional object (including the background which is typically something like air) in terms of it s electromagnetic characteristics, and applying finite differences of Maxwells fundamental electromagnetic equations in terms of electric and magnetic fields in the time domain (This method was originated by Yee in 1966 [2] who specified explicit iterations using this method, a good summary of FDTD is in [3]).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">747</field>
<field name="author">Renato Iannella</field>
<field name="author">Karen Robinson</field>
<field name="author">Patti Aymond</field>
<field name="author">Rex Brooks</field>
<field name="author">Gary Ham</field>
<field name="author">Werner Joerg</field>
<field name="author">Alessandro Trigla</field>
<field name="title">Emergency Data Exchange Language Resource Messaging (EDXL-RM)</field>
<field name="abstract">This XML-based Emergency Data Exchange Language (EDXL) Resource Messaging 

specification describes a suite of standard messages for data sharing among emergency and 

other information systems that deal in requesting and providing emergency equipment, supplies, 

people and teams. This format may be used over any data transmission system, including but not 

limited to the SOAP HTTP binding</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">748</field>
<field name="author">Guido Governatori</field>
<field name="author">J _rg Hoffmann</field>
<field name="author">Shazia Sadiq</field>
<field name="author">Ingo Weber</field>
<field name="title">Detecting Regulatory Compliance for Business Process Models through Semantic Annotation</field>
<field name="keyword">business process management</field>
<field name="keyword"> regulatory compliance</field>
<field name="abstract">A given business process may face a large number of

 regulatory obligations the process may or comply

 with. Providing tools and techniques through which

 an evaluation of the compliance degree of a given

 process can be undertaken is seen as a key objective

 in emerging business process platforms. We address

 this problem through a diagnostic framework that

 provides the ability to assess the compliance gaps

 present in a given process. Checking whether a

 process is compliant with the rules involves

 enumerating all reachable states and is hence, in

 general, a hard search problem. The approach taken

 here allows to provide useful diagnostic information

 in polynomial time. The approach is based on two

 underlying techniques. A conceptually faithful

 representation for regulatory obligations is firstly

 provided by a formal rule language based on a

 non-monotonic deontic logic of violations. Secondly,

 processes are formalized through semantic

 annotations that allow a logical state space to be

 created. The intersection of the two allows us to

 devise an efficient method to detect compliance

 gaps; the method guarantees to detect all

 obligations that will necessarily arise during

 execution, but that will not necessarily be

 fulfilled.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">749</field>
<field name="author">Junae Kim</field>
<field name="author">Chunhua Shen</field>
<field name="author">Lei Wang</field>
<field name="title">Learning Cascaded Reduced-set SVMs Using Linear Programming</field>
<field name="keyword">reduced-set kernels</field>
<field name="keyword"> SVM</field>
<field name="keyword"> cascade</field>
<field name="keyword"> detection</field>
<field name="abstract">This paper proposes a simple and efficient detection framework

that uses reduced-set kernels. We first describe our

approach which reduces the number of kernels. A convex

optimization method is used for calculating the reduced

sets. Following this, we propose a method that optimally

designs the cascade. Our experimental results indicate that

our method minimizes complexity regarding the number of

kernels in the cascaded structure while preserving the low

error rates. Our algorithm generates the optimal weight of

kernels for each cascade stage. This proposed algorithm

achieves high detection-rates at low computational cost.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">750</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="title">Analysis of Bushfire Personnel s Speech Transcriptions for Linguistic Cues of Cognitive Load</field>
<field name="keyword">Cognitive load</field>
<field name="keyword"> measurement</field>
<field name="keyword"> text analysis</field>
<field name="keyword"> speech</field>
<field name="keyword"> transcription analysis</field>
<field name="abstract">In complex, time-critical and data-intense situations users of a system can experience extremely high cognitive demands imposed on their limited working memory which can interfere with their ability to perform and complete the task at hand efficiently. Intelligent adaptive user interface systems which are aware of the users current level of cognitive load could in fact, alleviate these problems by implementing strategies to adjust the behavior, support, user interaction material, and resources needed as per users current cognitive burden to help them complete the task effectively.&lt;p&gt;



Our study presents a speech content analysis approach to the measurement of cognitive load which employs users linguistic features of speech to determine their experienced level of cognitive load. We present the detailed analyses of several linguistic features extracted from the live speech data collected from the subjects, the members of a bushfire incident management team, involved in highly time-critical and data-intense bushfire management tasks around Australia. We discuss the results for nine selected linguistic features showing significant differences between the speech from the low load tasks and the high load tasks.&lt;p&gt;



Despite the fact that the study focuses on bushfire operators speech transcriptions, we believe that the proposed method can be used with any clinical or medical transcriptions of patients speech for the purpose of cognitive load measurement of those patients in order to help the clinicians and/or doctors better understand the mental state of the patients.&lt;p&gt;</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">751</field>
<field name="author">Ralf Huuck</field>
<field name="author">Bastian Schlich</field>
<field name="author">Michael Tapp</field>
<field name="author">Ansgar Fehnker</field>
<field name="title">Automatic Bug Detection in Microcontroller Software by Static Program Analysis</field>
<field name="abstract">Microcontroller software typically consists of a few hundreds lines of code only, but it is rather different from standard application code. The software is highly hardware and platform specific, and bugs are often a consequence of neglecting subtle specifications of the microcontroller architecture. Currently, there are hardly any tools available for analyzing such software automatically. In this paper, we outline some of the specifics of microcontroller software, explain why those programs are different to standard C/C++ code, and we develop a static program analysis for a specific microcontroller, in our case the ATmega16, to spot code deficiencies. We demonstrated how to integrate those analysis rules in a concise manner into our generic static analyzer Goanna and illustrate the results by a case study of an automotive application. The case study highlights that even without formal proof, the proposed static techniques can be valuable in pinpointing software bugs that are otherwise hard to find.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">752</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">An Algorithm for Business Process Compliance</field>
<field name="keyword">business process compliance</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">This paper provides a novel mechanism to check whether business processes

 are compliant with business rules regulating them. The key point is

 that compliance is a relationship between two sets of

 specifications: the specifications for executing a business process

 and the specifications regulating it.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">753</field>
<field name="author">Hanxi Li</field>
<field name="author">Chunhua Shen</field>
<field name="title">Boosting the minimum margin: LPBoost vs. AdaBoost</field>
<field name="keyword">AdaBoost</field>
<field name="keyword"> LPBoost</field>
<field name="keyword"> margin</field>
<field name="abstract">LPBoost seemingly should have better generalization ca-

pability than AdaBoost according to the margin theory [12]

because LPBoost optimizes the minimum margin directly.

Thus far, however, there is no empirical comparison and

theoretical explanation of LPBoost against AdaBoost. We

have conducted an experimental evaluation on the classi-

fication performance of LPBoost and AdaBoost in this pa-

per. Our results show that the LPBoost performs worse than

AdaBoost in most cases. By considering the margin distri-

bution, we present an explanation. Also, our finding indi-

cates that besides the minimum margin, which is directly

and globally optimized in LPBoost, the margin distribution

plays a more important role in terms of the learned strong

classifier s classification performance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">754</field>
<field name="author">Alfonso Gerevini</field>
<field name="author">Patrik Haslum</field>
<field name="author">Derek Long</field>
<field name="author">Alessandro Saetti</field>
<field name="author">Yannis Dimopoulos</field>
<field name="title">Deterministic Planning in the Fifth International Planning Competition: PDDL3 and Experimental Evaluation of the Planners</field>
<field name="abstract">The international planning competition (IPC) is an important driver

for planning research. The general goals of the IPC include pushing

the state of the art in planning technology by posing new scientific

challenges, encouraging direct comparison of planning systems and

techniques, developing and improving a common planning domain

definition language, and designing new planning domains and problems

for the research community. This paper focuses on the deterministic

part of the fifth international planning competition (IPC5),

presenting the language and benchmark domains that we developed for

the competition, as well as a detailed experimental evaluation of the

deterministic planners that entered IPC5, which helps to understand

the state of the art in the field.



We introduce an extension of PDDL, called PDDL3, allowing the user to

express strong and soft constraints about the structure of the desired

plans, as well as strong and soft problem goals. We discuss the

expressive power of the new language focusing on the restricted version

that was used in IPC5, for which we give some basic results about its

compilability into PDDL2. Moreover, we study the relative performance of

the IPC5 planners in terms of solved problems, CPU time, and plan quality;

we analyse their behaviour with respect to the winners of the previous

competition; and we evaluate them in terms of their capability of dealing

with soft goals and constraints, and of finding good quality plans in

general. Overall, the results indicate significant progress in the

field, but they also reveal that some important issues remain open and

require further research, such as dealing with strong constraints and

computing high quality plans in metric-time domains and domains involving

soft goals or constraints.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">755</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Evaluation of Wireless Mesh Network Handoff Approaches for Public Safety and Disaster Recovery Networks</field>
<field name="abstract">In Public Safety and Disaster Recovery (PSDR) scenarios, reliable communication is an imperative. Unfortunately, communication infrastructure is often destroyed or overwhelmed by whatever precipitated the scenario (e.g., a hurricane or terrorist attack). Thus, the PSDR workers must often deploy their own communications infrastructure on-site. Wireless mesh networks (WMN) have been identified as being ideally suited to this task. WMN offer a high-capacity wireless backhaul network, provided by mesh routers, through which clients can connect to one another or with external networks. 

Mobility of clients within the mesh is particularly important for Public Service and Disaster Recovery scenarios. This creates a challenging problem as clients may move out of range of the mesh router they were using to connect to the mesh and need to associate with another. Client handoff mechanisms provide this functionality. In this paper we provide a critical survey of client handoff approaches applicable to IEEE 802.11 WMN evaluating them based on the strict QoS requirements established by the US Department of Homeland Security for PSDR networks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">756</field>
<field name="author">Asad Pirzada</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">ALARM: An Adaptive Load-Aware Routing Metric for Hybrid Wireless Mesh Networks</field>
<field name="abstract">Hybrid Wireless Mesh Networks (WMN) are generally

employed to establish communication during disaster

recovery operations. The Hybrid WMN network is formed

in a spontaneous manner when wireless nodes belonging to

different agencies are operated in the disaster area. These

wireless nodes generally have heterogeneous configurations in

terms of number of transceivers, computational power, battery

resources and mobility pattern. Internet Protocol (IP) acts as the

common platform for integrating these heterogeneous devices.

Routing protocols are engaged to determine paths between sets

of wireless nodes. A number of routing metrics have been

developed to achieve performance gains in multi-radio, multi-hop

WMNs. However, most of these metrics need access to external

information like link quality statistics, channel numbers, etc on a

regular basis. This frequent information exchange coupled with

the incessant variation in mobility and traffic load conditions

causes degraded performance of these metrics in Hybrid WMNs.

In this paper, we present a routing metric named ALARM

specifically designed for Hybrid WMNs. ALARM is computed

using the number of packets queued per wireless interface.

This computed value offers an accurate representation of the

traffic load, link quality, interference and noise levels. With the

help of extensive simulations, we show that our routing metric

outperforms well-know routing metrics like ETT and WCETT

under varying mobility and traffic load conditions in Hybrid

WMNs. We further show the practicality of the metric through

a prototype implementation and provide performance results

obtained from a small-scale testbed deployment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">757</field>
<field name="author">Sandra H Dandach</field>
<field name="author">Baris Fidan</field>
<field name="author">Soura Dasgupta</field>
<field name="author">Brian Anderson</field>
<field name="title">A continuous time linear adaptive source localization algorithm robust to persistent drift</field>
<field name="abstract">The problem of source localization has assumed increased importance in recent years. In this paper, we

formulate a continuous time adaptive localization algorithm, that permits a mobile agent to estimate the

location of a stationary source, using only the measured distance from the source. The algorithm is shown

to be exponentially asymptotically stable, under a persistent excitation condition that has an appealing

interpretation. We quantify the fact that exponential asymptotic stability endows the algorithm with the

ability to track slow, bounded but potentially persistent and nontrivial drift in the source.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">758</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Sylvia Pott</field>
<field name="author">Helmut Seidl</field>
<field name="title">Type Checking of Tree Walking Transducers</field>
<field name="abstract">Tree walking transducers are an expressive

formalism for reasoning about XSLT-like document transformations.

One of the useful properties of tree transducers is

decidability of type checking: given a transducer and input

and output types, it can be checked statically

whether the transducer is type correct, i.e., whether

each document adhering to the input type is 

necessarily transformed into documents adhering to

the output type.

Here, a "type" means a regular set of trees specified by a

finite-state tree automaton. Usually, type checking of tree

transducers is extremely expensive; already for simple

top-down tree transducers it is known to be EXPTIME-complete.

Are there expressive classes of tree transducers for which 

type checking can be performed in polynomial time?

Most of the previous approaches are based on inverse

type inference. The approach presented here goes the

other direction: it uses forward type inference.

This means to infer, given a transducer and an input type,

the corresponding set of output trees. In general, this set 

is not a type, i.e., is not regular. However, its

intersection emptiness with a given type 

can be decided. 

Using this approach it is shown that type checking can be performed in

polynomial time, if (1) the output type is specified by

a deterministic tree automaton and (2) the transducer visits

every input node only a bounded number of times.

If the tree walking transducer is additionally equipped with

accumulating call-by-value parameters, then the complexity of

type checking also depends (exponentially) on the number of such

parameters. For this case

a fast approximative type checking algorithm is presented,

based on context-free tree grammars.

Finally, the approach is generalized from trees to 

forest walking transducers which

additionally support concatenation as a built-in output operation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">759</field>
<field name="author">Brad Yu</field>
<field name="author">Brian Anderson</field>
<field name="author">Soura Dasgupta</field>
<field name="author">Baris Fidan</field>
<field name="title">Control of minimally persistent formations in the plane</field>
<field name="abstract">This paper studies the problem of controlling the shape of a formation of point agents

in the plane. A model is considered where the distance between certain agent pairs is maintained

by one of the agents making up the pair; if enough appropriately chosen distances are maintained,

with the number growing linearly with the number of agents, then the shape of the formation will

be maintained. The detailed question examined in the paper is how one may construct decentralized

nonlinear control laws to be operated at each agent that will restore the shape of the formation in the

presence of small distortions from the nominal shape. Using the theory of rigid and persistent graphs,

the question is answered. As it turns out, a certain submatrix of a matrix known as the rigidity

matrix can be proved to have nonzero leading principal minors, which allows the determination of a

stabilizing control law.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">760</field>
<field name="author">Werayut Saesue</field>
<field name="author">Jian Zhang</field>
<field name="author">Chun Tung Chou</field>
<field name="title">Hybrid Frame-Recursive Block-Based Distortion Estimation Model For Wireless Video Transmission</field>
<field name="keyword">wireless transmission</field>
<field name="keyword"> video coding</field>
<field name="keyword"> data partition</field>
<field name="keyword"> H.264</field>
<field name="keyword"> distortion</field>
<field name="abstract">In wireless environments, video quality can be 

severely degraded due to channel errors. Improving error robustness 

towards the impact of packet loss in error-prone network is 

considered as a critical concern in wireless video networking

research. Data partitioning (DP) is an efficient error-resilient

tool in video codec that is capable of reducing the effect of

transmission errors by reorganizing the coded video bitstream

into different partitions with different levels of importance.

Significant video performance improvement can be achieved if

DP is jointly optimized with unequal error protection (UEP). This

paper proposes a fast and accurate frame-recursive block-based

distortion estimation model for the DP tool in H.264/AVC. The

accuracy of our model comes from appropriately approximating

the error-concealment cross-correlation term (which is neglected

in earlier work in order to reduce computation burden) as a

function of the first moment of decoded pixels.Without increasing

computation complexity, our proposed distortion model can be

applied to both fixed and variable block size intra-prediction and

motion compensation. Extensive simulation results are presented

to show the accuracy of our estimation algorithm.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">761</field>
<field name="author">Leonid Ryzhyk</field>
<field name="author">Peter Chubb</field>
<field name="author">Ihor Kuz</field>
<field name="author">Gernot Heiser</field>
<field name="title">Dingo: Taming Device Drivers</field>
<field name="keyword">Operating Systems</field>
<field name="keyword"> Device Drivers</field>
<field name="keyword"> Reliability</field>
<field name="abstract">Device drivers are notorious for being a major source of failure 

in operating systems. In analysing a sample of real defects in 

Linux drivers we found that a large proportion (39\%) of bugs are 

due to two key shortcomings in the device-driver architecture 

enforced by current operating systems: poorly defined 

communication protocols, and a multithreaded model of computation. 

The former results in drivers that violate expected protocols 

between the device and operating system, while the latter leads to 

concurrency-related faults such as deadlocks and race conditions.



We claim that a better device driver architecture can help reduce 

the occurrence of these faults, and present our Dingo framework as 

constructive proof. Dingo provides a formal, state-machine based, 

language for describing driver protocols, which avoids confusion 

and ambiguity, and helps driver writers implement correct 

protocols. It also enforces an event-driven model of computation, 

which eliminates most concurrency-related faults. Our 

implementation of the Dingo architecture in Linux avoids most 

concurrency errors and reduces the likelihood of protocol errors 

in Dingo drivers, while introducing negligible performance 

overhead. It allows Dingo and native Linux drivers to coexist, 

offering a gradual migration path to more reliable device drivers.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">762</field>
<field name="author">Dave Snowdon</field>
<field name="author">Etienne Le Sueur</field>
<field name="author">Stefan Petters</field>
<field name="author">Gernot Heiser</field>
<field name="title">Koala: A Platform for OS-Level Power Management</field>
<field name="keyword">Energy</field>
<field name="keyword"> DVFS</field>
<field name="abstract">Managing the power consumption of computing platforms is a prohibitively complicated problem thanks to a multitude of hardware configuration options and characteristics. Much of the academic research is based on unrealistic assumptions, and has, therefore, seen little practical uptake. We present an overview of the difficulties facing power management schemes when used in real systems.



We propose Koala, a platform which uses a pre-characterised model at run-time to predict the performance and energy consumption of a piece of software. An arbitrary policy can then be applied in order to dynamically trade performance and energy consumption. We have implemented this system in a recent Linux kernel, and evaluated it by running a variety of benchmarks on a number of different platforms. We observe energy savings of, at times, 30% for a 4% performance loss.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">763</field>
<field name="author">Willem de Bruijn</field>
<field name="author">Herbert Bos</field>
<field name="title">PipesFS: Fast Linux I/O in the Unix Tradition</field>
<field name="keyword">I/O</field>
<field name="keyword"> Pipes</field>
<field name="keyword"> Virtual File System</field>
<field name="keyword"> Linux</field>
<field name="abstract">This paper presents PipesFS, an I/O architecture for Linux 2.6 that increases I/O throughput and adds support for heterogeneous parallel processors by (1) collapsing many I/O interfaces onto one: the Unix pipeline, (2) increasing pipe e ciency and (3) exploiting pipeline modularity to spread computation across all available processors. PipesFS extends the pipeline model to kernel I/O and communicates with applications through a Linux virtual lesystem (VFS), where directory nodes represent operations and pipe nodes export live kernel data. Users can thus interact with kernel I/O through existing calls like mkdir, tools like grep, most languages and even shell scripts. To support performance critical tasks, PipesFS improves pipe throughput through copy, context switch and cache miss avoidance. To integrate heterogeneous processors (e.g., the Cell) it transparently moves operations to the most e cient type of core.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">764</field>
<field name="author">Yu Shi</field>
<field name="title">Smart Cameras for Machine Vision</field>
<field name="abstract">Industrial machine vision is the birth place of smart cameras. Compared to other applications, smart cameras for machine vision are a more mature technology and have a solid and established market share in machine vision camera market. Main end user industry segments include robotics, semiconductor, electronics, pharmaceutical, manufacturing, food, plastics and printing. Main tasks that these smart cameras perform include bar-code reading, part inspection, flaw detection, surface inspection, dimensional measurement, assembly verification, print verification, object sorting, OCR (optical character recognition) and maintenance. One important reason for this relative maturity lies in the fact that the application requirements for machine vision cameras are much less restrictive compared with those for video surveillance cameras for safety and security application. There is also an economic driver behind the birth and growth of smart cameras in machine vision: the pursuit of product quality and productivity gains. Machine vision smart cameras will continue to be one of the main market drivers for the future growth of smart camera technologies.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">765</field>
<field name="author">Nianjun Liu</field>
<field name="author">Nathan Brewer</field>
<field name="author">Li Cheng</field>
<field name="author">Lei Wang</field>
<field name="title">Interactive Lossy Compression for Images and Video</field>
<field name="keyword">Image labelling</field>
<field name="keyword">Segmentation</field>
<field name="abstract">In any given scene, a human observer is typically more interested in some objects than others, and will pay more attention to those objects they are interested in. This paper aims to capture this attention focusing behavior by selectively merging a fine-scale oversegmentation of a frame so that interesting regions

are segmented into smaller regions than uninteresting regions. This results in a new type of image partitioning which reflects in the image the amount of attention we pay to a particular image region. This is done using a novel, interactive method for learning merging rules for images and videos based on defining a weighted distance metric between adjacent oversegments. We present as an example application of this technique a new lossy image and video stream compression method which attempts to minimize the loss in areas of interest.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">766</field>
<field name="author">Liam O'Brien</field>
<field name="author">Paul Brebner</field>
<field name="title">The 1st International Workshop on Quality of Service Concerns in Service Oriented Architectures (QoSCSOA) 2008</field>
<field name="keyword">Quality-of-Service</field>
<field name="keyword"> Quality Attributes</field>
<field name="keyword"> Service Oriented Architecture</field>
<field name="abstract">The paper gives a summary of the 1st International Workshop on Quality-of-Service Concerns in Service Oriented Architectures (QoSCSOA) 2008.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">767</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Getian Ye</field>
<field name="author">Yang Wang</field>
<field name="author">Jie Xu</field>
<field name="author">Gunawan Herman</field>
<field name="title">Finding Shareable Informative Patterns and Optimal Coding Matrix for Multiclass Boosting</field>
<field name="keyword">Multiclass boosting</field>
<field name="keyword"> pattern recognition</field>
<field name="keyword"> informative patterns</field>
<field name="keyword"> pattern mining</field>
<field name="abstract">A multiclass classification problem can be reduced to

a collection of binary problems using an error-correcting

coding matrix that specifies the binary partitions of the

classes. The final classifier is an ensemble of base classifiers

learned on the binary problems and its performance

is affected by two major factors: the qualities of the base

classifiers and the coding matrix. Previous studies either

focus on one of these factors or consider two factors separately.

In this paper, we propose a new multiclass boosting

algorithm called AdaBoost.SIP that considers both two

factors simultaneously. In this algorithm, informative patterns,

which are shareable by different classes rather than

only discriminative on specific single class, are generated at

first. Then the binary partition preferred by each pattern is

found by performing stage-wise functional gradient descent

on a margin-based cost function. Finally, base classifiers

and coding matrix are optimized simultaneously by maximizing

the negative gradient of such cost function. The proposed

algorithm is applied to scene and event recognition

and experimental results show its effectiveness in multiclass

classification.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">768</field>
<field name="author">Bo Yin</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Fang Chen</field>
<field name="title">Investigating Speech Features and Automatic Measurement of Cognitive Load</field>
<field name="keyword"/>
<field name="abstract">The ability to measure cognitive load level in real time is extremely useful for improving the efficiency of interfaces and contents delivering, especially when interfaces and contents get complex in a multimedia environment. Speech is highly suitable for measuring cognitive load due to its non-intrusive nature and ease of collection. In this paper, we investigated the patterns of prosodic features and confirmed it is relevant to cognitive load. We also explored varied classification techniques to capture those relevant patterns of speech features. Gaussian Mixture Model (GMM), Support Vector Machine (SVM), and a hybrid SVM-GMM based classifiers were investigated with MFCC and pitch features. Individual systems and a fusion based system were evaluated on two different task scenarios reading comprehension and Stroop test. The SVM-GMM based system achieved the highest performance on both tasks and improved the accuracy of three levels classification to 75.6% and 82.2%, respectively.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">769</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Efficiently training a better visual detector with sparse Eigenvectors</field>
<field name="abstract">Face detection in still images plays a very important role in many multimedia applications. Since Viola and Jones \cite{Viola2004Robust} proposed the first real-time AdaBoost based object detection system, many efforts have been spent on improving the boosting method. In this work, we propose a new technique to efficiently train object

detectors named Boosted Greedy Sparse Linear Discriminant Analysis (BGSLDA). The technique exploits the sample reweighting property of boosting and class-separabiltity criterion of the GSLDA \cite{Moghaddam2007Fast}. Our experimental results in the domain of highly skewed data distributions \eg face detection demonstrating that the classifiers trained to minimize least square loss (LDA) seem to outperform classifiers trained to minimize a symmetric exponential loss like in AdaBoost and its variants. This finding provides a significant opportunity to argue that Adaboost and its similar approaches are not the only methods that can achieve high classification results for high dimensional data \eg face detection etc.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">770</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Real-time pedestrian detection using a boosted multi-layer classifier</field>
<field name="abstract">Pedestrian detection plays an important role for many

computer vision applications. In this paper, we propose a

new simpler pedestrian detector using state-of-the-art locally

extracted features, namely, covariance features. Unlike

the work in [1], where the feature selection and weak

classifier training are performed on the Riemannian manifold,

we select features and train weak classifiers in the

Euclidean space for faster computation. To this end, AdaBoost

with weighted Fisher linear discriminant analysis

based weak classifiers are designed. Multiple layer boosting

with heterogeneous features is constructed to exploit the

efficiency of the Haar-like feature and the discriminative

power of the covariance feature simultaneously. Extensive

experiments show that by combining the Haar-like and covariance

features, we speed up the original covariance feature

detector [1] by up to an order of magnitude in processing

time without compromising the detection performance.

For the first time, the proposed work made covariance feature

based pedestrian detection work real time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">771</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Performance evaluation of local features in human classification and detection</field>
<field name="abstract">Accurately detecting pedestrians is the first fundamental step for many computer vision applications such as video surveil-

lance, smart vehicles, intersection traffic analysis, etc. This paper presents an experimental study on pedestrian detection

using state-of-the-art local feature extraction and support vector machine (SVM) classifiers. The performance of pedestrian

detection using region covariance, histogram of oriented gradients (HOG) and local receptive fields (LRF) feature descrip-

tors is experimentally evaluated. The experiments are performed on the DaimlerChrysler benchmarking dataset, the MIT

CBCL dataset and the INRIA dataset. All can be publicly accessed. The experimental results show that region covariance

features with radial basis function (RBF) kernel SVM and HOG features with quadratic kernel SVM outperform the combi-

nation of LRF features with quadratic kernel SVM reported in [1]. Furthermore, the results reveal that both covariance and

HOG features perform very well in the context of pedestrian detection.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">772</field>
<field name="author">Renato Iannella</field>
<field name="title">Industry Challenges for Social and Professional Networks</field>
<field name="abstract">The wider Social Network &amp; Professional Network communities will benefit from interoperable standards for data portability, policy expression and accountability, and network migration. These standards need to be developed which addresses both the technical requirements and the business models that drive the service providers to attract the end users. A W3C Social Networks Interoperability Roadmap Incubator Group (XG) would be the best mechanism to drive forward the planning processes, requirements gathering, and establishment of the scope and range of technical standards to address the communities needs</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">773</field>
<field name="author">Cheng Luo</field>
<field name="author">Xiongcai Cai</field>
<field name="author">Jian Zhang</field>
<field name="title">Robust Object Tracking Using the Particle Filtering and Level Set Methods: A Comparative Experiment</field>
<field name="keyword">object tracking</field>
<field name="keyword"> particle filtering</field>
<field name="keyword"> level set method</field>
<field name="abstract">Robust visual tracking has become an important topic of research in computer vision. A novel method for robust object tracking, GATE, improves object tracking in complex environments using the particle filtering and the level set-based active contour method. GATE creates a spatial prior in the state space using shape information of the tracked object to filter particles in the state space in order to reshape and refine the posterior distribution of the particle filtering. This paper describes a comparative experiment that applies GATE and the standard particle filtering to track the object of interest in complex environments using simple features. Image sequences captured by the hand held, stationary and the PTZ camera are utilised. The experimental results demonstrate that GATE is able to solve the ambiguous outlier problem of particle filters in order to deal with heavy clutters in the background, occlusion, low resolution and noisy images, and thus significantly improves the particle filtering in object tracking.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">774</field>
<field name="author">Cheng Luo</field>
<field name="author">Xiongcai Cai</field>
<field name="author">Jian Zhang</field>
<field name="title">GATE: A Novel Robust Object Tracking Method Using the Particle Filtering and Level Set Method</field>
<field name="keyword">object tracking</field>
<field name="keyword"> particle filtering</field>
<field name="keyword"> level set method</field>
<field name="abstract">This paper presents a novel algorithm for robust object tracking based on the particle filtering method employed in recursive Bayesian estimation and image segmentation and optimisation techniques employed in active contour models and level set methods. The proposed Geometric Active contour-based Tracking Estimator, namely GATE, enables particle filters to track object of interest in complex environments using merely a simple feature. GATE creates a spatial prior in the state space using shape information of the tracked object. The created spatial prior is then used to filter particles in the state space in order to reshape and refine the observation distribution of the particle filtering. This improves the performance of the likelihood model in

the particle filtering, so the significantly overall improvement of the particle filtering. The promising performance of our method on real video sequences are demonstrated.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">775</field>
<field name="author">Tet Fei Yap</field>
<field name="title">Product Feasibility Document: Cognitive Load Measurement System</field>
<field name="keyword">Product Feasibility</field>
<field name="keyword"> Cognitive Load</field>
<field name="abstract">Cognitive load refers to the amount of mental stress experienced while performing a task. The cognitive load measurement (CLM) system is a novel invention which automatically measures the mental workload of users in real time. Using the information provided by the CLM system, appropriate actions can be taken to reduce the load level of users experiencing high load. Target users of this invention include traffic control room operators, war fighters in the battlefield and call centre operators. Although some companies might be reluctant to try out this new technology, the prospect of a more efficient operation and potential cost savings will be enough to offset this initial reluctance. Barriers to entry for this invention is low with no direct competitors in the market currently. Nevertheless, technological limitations are identified as possible risks which might prevent the CLM system from being deployed in the market. However, after breaking through into the target markets, the CLM system will maintain sustainable competitive advantage due to protection from existing patents and also strong IP strategies. With financial projections predicting strong profits three years after start up, commercialisation of the CLM system is thus concluded to be a viable prospect.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">776</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Simultaneous Photometric Invariance and Shape Recovery</field>
<field name="keyword">photometric invariants</field>
<field name="keyword"> reflectance models</field>
<field name="keyword"> shape</field>
<field name="keyword"> multispectral imaging</field>
<field name="abstract">In this paper, we identify the constraints under which

the generally ill-posed problem of simultaneous recovery of

surface shape and its photometric invariants can be rendered

tractable. We examine the cases where a single or

more images are acquired using different lighting directions

with known illuminant power. Given these conditions, we

state the constraints upon which the recovery of the surface

geometry and its photometric parameters can be estimated.

With these constraints, we then show how the recovery process

may be formulated as an optimisation algorithm which

aims to fit the reflectance models under study to the image

reflectance. The approach presented here is general and

can be applied to a family of reflectance models that are

based on the Fresnel reflection theory. Thus, we provide

a theoretical and computational background for recovering

shape, material index of refraction and microscopic roughness

from multi-spectral images.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">777</field>
<field name="author">Pattaraporn Khuwuthyakorn</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">Texture Descriptors via Stable Distributions</field>
<field name="keyword">Texture Descriptors</field>
<field name="keyword"> Fourier Analysis</field>
<field name="keyword"> Statistical methods</field>
<field name="abstract">In this paper, we present a texture descriptor which hinges in the use of the local image statistics so as to recover a compact representation of the texture under study. To this

end, here, we make use of stable distributions and their link to Fourier analysis so as to provide a means to compute in a computationally efficient manner a local texture descriptor. This link between stochastic processes and Fourier analysis provides an efficient means to compute texture spectra which can be interpreted as a probability distribution for purposes of recognition and analysis. Making use of our local descriptor, we provide a metric between texture pairs that can be made devoid of rotations on the texture plane by recovering the optimal linear transformation via Procrustes analysis. We demonstrate the utility of our descriptor and its associated metric on a database of real-world textures.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">778</field>
<field name="author">Peter Carr</field>
<field name="author">Richard Hartley</field>
<field name="title">Improved Single Image Dehazing using Geometry</field>
<field name="abstract">Images captured in foggy weather conditions exhibit losses in quality which are dependent on distance. If the depth and atmospheric conditions are known, one can enhance the images (to some degree) by compensating for the effects of the fog. Recently, several investigations have presented methods for recovering depth maps using only the information contained in a single foggy image. Each technique estimates the depths pixels independently, and assumes that neighbouring pixels have similar depths.



In this work, we employ the fact that images containing fog are captured from outdoor cameras. As a result, the scene geometry is usually dominated by a ground plane. More importantly, objects which appear towards the top of the image are usually further away. We show how this preference (implemented as a soft constraint) is compatible with the alpha-expansion optimization technique and illustrate how it can be used to improve the robustness of single image dehazing techniques.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">779</field>
<field name="author">Peter Carr</field>
<field name="title">GPU Accelerated Multimodal Background Subtraction</field>
<field name="abstract">Although trivial background subtraction algorithms (such as temporal averaging) can execute quite quickly, they do not give useful results in most situations. More complex algorithms usually provide better results, but are typically too slow for widespread use. Here, we examine the architecture of the GPU and describe how a multimodal background subtraction algorithm can be implemented on graphics hardware to provide useful results in real-time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">780</field>
<field name="author">Tamir Yedidya</field>
<field name="author">Richard Hartley</field>
<field name="author">Jean-Pierre Guillon</field>
<field name="title">Automatic Detection of Pre-Ocular Tear Film Break-Up Sequence in Dry Eyes</field>
<field name="abstract">Dry Eye Syndrome is a common disease in the western world, 

with effects from uncomfortable itchiness to

permanent damage to the ocular surface. Nevertheless, 

there is still no objective test that provides reliable results. 

We extend our method for the automated detection of dryness signs to include the break

up time(BUT), analysis of the degree of thinning of

the tear film and detection of meniscus induced dryness, 

the last two have not been previously addressed.

Our motivation is to help the clinician to automatically 

detect and analyze various signs related to dry

eyes. The method has been tested on over 100 videos

taken from 30 different patients. When compared to an

analysis done by a specialized optometrist, our method

is demonstrated to provide an accurate estimation for

the BUT and the extent of the disease.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">781</field>
<field name="author">Tamir Yedidya</field>
<field name="author">Richard Hartley</field>
<field name="author"/>
<field name="title">Tracking of Blood Vessels in Retinal Images Using Kalman Filter</field>
<field name="abstract">We present an automatic method to segment the

blood vessels in retinal images. Our method is based on

tracking the center of the vessels using the Kalman filter.

We define a linear model to track the blood vessels,

suitable for both the detection of wide and thin vessels

in noisy images. The estimation of the next state is

computed by using gradient information, histogram of

the orientations and the expected structure of a vessel.

Seed points are detected by a set of matched filters in

different widths and orientations. Tracking is carried

out for all detected seed points, however we retrace the

segmentation for seeds with small confidence. Our algorithm

also handles branching points by proceeding in

the previous moving direction when no dominant gradient

information is available. The method is tested on

the public DRIVE database [10] and shows good results

with a low false positive rate.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">782</field>
<field name="author">David Marlow</field>
<field name="author">Philip Kilby</field>
<field name="author">Geoff Mercer</field>
<field name="title">Examining methods for maximising ship classifications in maritime surveillance</field>
<field name="abstract">(Extended abstract)



Both Royal Australian Air Force (RAAF) and Border Protection Command (BPC) aircraft fly maritime surveillance missions for the Australian Government on a regular basis. These missions involve searching particular areas of interest (AIs) for illegal fishing or people smuggling activities. In this work a square AI is considered, with an aircraft tasked to search for ships using its sensors (e.g., radar, electro-optical). Waypoints (points that must be visited) are included in the AI to ensure that the entire AI is covered. The aim for a particular search is to detect and classify as many ships as possible, while doing so in the shortest time. Depending on the ship density, the aircraft may not have time to search the entire AI.

In this paper an augmentation of the traditional Travelling Salesman Problem (TSP) is considered, where the ships (cities) move with random velocities (dynamic TSP), have different start and end points (open TSP) and there is incomplete a priori knowledge of the problem space (on-line TSP), making this problem much more challenging.



Earlier work (Marlow et al, 2007) considered a baseline case of an S-shaped search pattern and a default heading (the route flown when there are no ships currently detected) direct to the next waypoint. This paper considers three extensions with an aim to increase the level of ship classifications: these are 1) alternative initial flight paths, 2) alternative default headings and 3) including ghost ships in the search. 



The principle behind the S-shaped pattern is that the aircraft will cover the entire AI with its sensors, giving aircrew the best chance of detecting all ships in the AI. This paper considers alternative spiral waypoint patterns, both an inspiral (from one corner of the AI, spiraling towards the centre) and an outspiral (the reverse). These approaches are theoretically more likely to detect ships that enter the AI during the mission.



The direct-to-waypoint default heading will minimise the travel time, but it also may potentially result in the aircraft not covering the entire AI with its sensors, particularly if it has already been diverted significantly from the wayline (the direct line between waypoints). In this work, a perpendicular return to the wayline is considered, which increases the distance travelled but is also likely to increase the probability of detecting more ships. A third option is also considered in which the aircraft continually aims for the midpoint on the wayline of the perpendicular intercept point and the waypoint.



The object of ghost ships is to direct the aircraft to fly to areas of the AI that it may not otherwise visit. This may particularly be the case in low-density environments where the aircraft has to substantially divert from the wayline to classify ships and, in returning, inadequately cover other areas where ships may be present. Ghost ships remain in the current tour until they are detected , whereupon they are removed.



Results suggest that, at lower densities, the perpendicular default heading and including ghost ships provide an overall improvement in classifications of the order of a few percentage points, which in real terms translates to an extra 1-2 ships on average. Significantly it also translates to an increase in the percentage of cases where 100% classifications are achieved. These improvements generally come at a cost of increased distance travelled by the aircraft and thus greater fuel consumption. In the case of ghost ships, there is an additional cost in computational time due to the requirement to include them in the tour. Beyond the critical ship density (where classifications cannot physically reach 100%), these variations offer no real advantage and in some cases are counter-productive, so the baseline pattern is more appropriate in these circumstances.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">783</field>
<field name="author">Max Ott</field>
<field name="title">Global Experimental Testbeds for Studying Future Internet Technologies</field>
<field name="abstract">This slides are from a keynote talk given at PDCAT'08. The first part of the talk provides an overview of the various Future Internet related large-scale experimental facilities, such as GENI in the US and FIRE in Europe.



The second part of the talk describes a few of the challenges we face when designing and deploying large scale testbeds. It especially stresses the need to make testbeds itself a research topic - after all testbeds are large distributed systems with challenging features.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">784</field>
<field name="author">Lili Abdullah</field>
<field name="author">June Verner</field>
<field name="title">Outsourced Strategic IT Systems Development Risk</field>
<field name="keyword">Risks</field>
<field name="keyword"> strategic IT development</field>
<field name="keyword"> outsourcing</field>
<field name="keyword"> client perspective</field>
<field name="abstract">Organizations are increasingly outsourcing the development of strategic IT systems to external vendors. Because of the importance of such systems to a client organization, unaddressed risks that develop into serious problems can have a huge negative impact on the organization. There is much reported research on IT development and IT outsourcing risk, and risk management. However, research on risks in outsourced strategic IT system development, from the client perspective, have largely been ignored. We developed a risk framework for outsourced strategic IT development based on the literature. This paper reviews three reported cases of unsuccessful strategic IT system development projects and provides an initial validation of our outsourced risk framework. A number of risks critical to a client are identified. These risks fall into several categories associated with organizational environment, team, user, complexity, contract, requirements, planning and control, and execution. If critical outsourcing risks are clearly understood, a client organization can develop a strategy for managing such risks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">785</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="author">Nadine Marcus</field>
<field name="title">Analysis of Collaborative Speech for Grammatical Cues of Cognitive Load</field>
<field name="keyword">Cognitive Load</field>
<field name="keyword"> Measurement</field>
<field name="keyword"> Speech</field>
<field name="keyword"> Text Analysis</field>
<field name="keyword"> Grammatical features</field>
<field name="keyword"> Collaborative Interaction</field>
<field name="abstract">In complex and time-critical situations, system users can experience extremely high cognitive load, which can interfere with task completion. An understanding of the users cognitive load will enable us to alleviate these problems by implementing strategies to adjust the system s behavior, support, and resources needed as per their cognitive burden and help them complete the task effectively. Moreover, for complex collaborative tasks where many users have to cooperate to solve task-related problems, understanding cognitive demands can be very helpful.&lt;p&gt; 



We present a speech content analysis method to determine cognitive load of users working in a highly complex, time-critical, and data-intense collaborative environment. A detailed study of the use of a part-of-speech namely pronouns, was carried out for bushfire management team members involved in several tasks around Australia. The team members worked collaboratively to share information, assign tasks, plan activities, and carry out operations to manage the fire. During each bushfire management task, there were several cognitively low and high load events to manage. All the critical members communication was recorded for analysis purposes. &lt;p&gt;



The results show that team members use singular pronouns (e.g. I, you, he, she) and plural pronouns (e.g. we, they) differently in different load situations. Specifically, it was observed that members used more singular pronouns under low load tasks than high load tasks, i.e. there was a negative correlation between singular pronouns and cognitive load; the higher the cognitive demand, the lesser use of singular pronouns. In contrast, they used more plural pronouns under high load tasks, i.e.; the higher the cognitive load, the more they used plural pronouns.&lt;p&gt;



This finding leads us to conclude that in collaborative team working environments, when dealing with low cognitive load tasks, members prefer to perform tasks individually. In contrast, when dealing with complex and high cognitive load tasks, they try to involve other team members in order to share the high and otherwise unmanageable cognitive load amongst others, to effectively solve the problem and improve the team s overall performance.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">786</field>
<field name="author">Yin Kia Chiam</field>
<field name="author">Mark Staples</field>
<field name="author">Liming Zhu</field>
<field name="title">Representation of Quality Attribute Techniques Using SPEM and EPF Composer</field>
<field name="keyword">Quality Attribute Techniques</field>
<field name="keyword"> SPEM</field>
<field name="keyword"> EPF Composer</field>
<field name="abstract">There are many development techniques used to assist development teams to achieve required levels of product quality such as safety, security and performance. These ``Quality Attribute Techniques'' aim to identify, eliminate, reduce, control and minimise potential quality problems in the development of critical systems. Although widely used, these techniques are not normally well represented in software process models. This paper proposes two alternative representations of Quality Attribute Techniques using the SPEM metamodel and Eclipse Process Framework (EPF) Composer and shows how these techniques can be incorporated into software development process models. Safety techniques have been selected as a case example for evaluation. The evaluation identifies advantages and limitations of the SPEM and EPF Composer in terms of their ability to support representation and integration of Quality Attribute Techniques. Some improvements to SPEM and EPF Composer are suggested.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">787</field>
<field name="author">Oliver Nagy</field>
<field name="author">Leon Craven</field>
<field name="author">Leif Hanlen</field>
<field name="title">A different Approach to the Dimensionality of Wave Fields</field>
<field name="abstract">This paper finds the number of essential Fourier coefficients required

to synthesise an arbitrary wave field composed of plane waves from all

directions. We analyse the problem in rectangular coordinates for 1D, 2D

and 3D regions, and our findings closely match those of other authors

who used cylindrical and spherical regions. However, their results

require knowledge about wave equations, Bessel functions, spherical

harmonics and other advanced mathematical concepts, whereas our results

follow from a simple Fourier analysis of plane waves in rectangular

coordinates. We also show that the plane wave results are good

predictions for point source waves.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">788</field>
<field name="author">June Verner</field>
<field name="author">Jennifer Sampson</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Barbara Kitchenham</field>
<field name="author">Nur Azzah Abu Bakar</field>
<field name="title">Guidelines for Industrially-Based Multiple Case Studies in Software Engineering</field>
<field name="keyword">case study research</field>
<field name="keyword"> case study design</field>
<field name="keyword"> software engineering</field>
<field name="abstract">Without careful methodological guidance, case studies in software engineering are difficult to plan, design and execute. While there are a number of broad guidelines for case study research, there are none that specifically address the needs of a software engineer undertaking multiple case studies in an industrial setting. Through a synthesis of existing best practices in case study research, we provide a set of comprehensive guidelines for conducting multiple case studies in software engineering research. Our guidelines can assist software engineering researchers with all stages of multiple case study research, although in this paper we concentrate on the early phases, such as focusing on the case study and detailed plan design. To date, three exploratory research projects found our guidelines very useful. We illustrate our guidelines with examples from one of these projects.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">789</field>
<field name="author">David Smith</field>
<field name="author">Dino Miniutti</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="title">Matlab code for generating BAN fading power profile</field>
<field name="keyword"/>
<field name="abstract">Matlab code that can be used to generate a signal (in terms of received power with respect to transmit power) for a mobile wireless body area network based on extensive measurement campaign carried out at NICTA.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">790</field>
<field name="author">Nazrina Khurshid</field>
<field name="author">Paul Bannerman</field>
<field name="author">Mark Staples</field>
<field name="title">Overcoming the First Hurdle: Why Organizations Do Not Adopt CMMI</field>
<field name="keyword">Software Process Improvement</field>
<field name="keyword"> SPI</field>
<field name="keyword"> CMMI</field>
<field name="keyword"> adoption</field>
<field name="abstract">This paper further examines why some software development organizations decide not to adopt CMMI by replicating an earlier Australian study in another country. The study examines data collected from the efforts of three consulting firms to sell a CMMI Level 2 program subsidized by the Malaysian government. The most frequently cited reasons for not adopting CMMI were: the program was too costly; the companies were unsure of the benefits; the organization was too small; and/or the organization had other priorities. The Malaysian study extends and generally supports the Australian study (differences were found in the frequency ordering of reasons and two new reason categories had to be introduced). It also adds to our understanding of CMMI adoption decisions. Based on the results, we conclude that to achieve broader impact in practice, software process improvement (SPI) researchers need to develop a stronger cost-benefit analysis for SPI, recognising it as a business investment rather than just a product or process quality improvement technique, and provide flexible entry options to enable more companies of difference sizes to take the adoption leap.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">791</field>
<field name="author">Liming Zhu</field>
<field name="author">Tu Tak Tran</field>
<field name="author">Mark Staples</field>
<field name="author">Ross Jeffery</field>
<field name="title">Technical Software Development Process in the XML Domain</field>
<field name="abstract">Background: A Technical Development Process (TDP) is a development process for a particular technology, such as XML, service orientation, object orientation or a programming language. Unlike software development life-cycle processes, TDPs provide concrete and detailed guidance to software engineers working in a particular technology domain. TDPs are currently not well understood in terms of description, modelling and interactions with life-cycle processes. Aim: In this paper, we investigate what are TDPs in the XML domain and how can TDPs be modelled using existing development process modelling notations and tools. Method: We extracted XML specific TDPs from literatures, interviews and internal documentation within software development organizations and conducted systematic verifications and validations. Results: We identify different types of TDPs in the XML domain and propose mechanisms to model TDPs using Software Process Engineering Meta-models (SPEM) in the Eclipse Modelling Framework (EPF). Conclusion: The results demonstrate the feasibility of explicitly identifying and modelling of TDPs in the context of software process modelling and how they are used in software development. The results help further bridge the gap between macro-processes (life-cycle and management-centred processes) and micro-processes (e.g. developer-centred TDPs).</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">792</field>
<field name="author">Lei Zhang</field>
<field name="author">Patrick.Senac Senac</field>
<field name="author">Roksana Boreli</field>
<field name="author">Michel Diaz</field>
<field name="title">Optimization of WiMax modulation scheme with a cross layer erasure code</field>
<field name="keyword">WiMax</field>
<field name="keyword">FEC</field>
<field name="keyword">adaptive modulation</field>
<field name="keyword">ARQ</field>
<field name="abstract">WIMAX (Worldwide Interoperability for Microwave Access) is a promising new networking technology that potentially offers high speed and wide area wireless access services that complement consistently the 3G and WiFi access networks capabilities. The standard proposes an adaptive modulation scheme which allows WiMax nodes to communicate from various modulation coding schemes according to the link quality. However, the standard does not define a detailed link adaptation algorithm and currently, the most largely used modulation adaptation technique is based on a channel quality lookup table. We argue that this method

is not able to make the best adaptation decisions and delivers a sub-optimal goodput in numerous communication contexts. In this paper, we propose a novel cross layer based modulation adaptation mechanism which incorporates the use of adaptive erasure code with the physical layer information to significantly improve the goodput and transmission efficiency. Simulation results show that our proposal adapts more efficiently to real environments and achieves a significant gain on the goodput delivered to mobile nodes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">793</field>
<field name="author">John S Gero</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Understanding innovation as change of value systems</field>
<field name="keyword">Situations; Function</field>
<field name="keyword">Behaviour</field>
<field name="keyword">Structure Ontology; Agent</field>
<field name="keyword">Based Systems</field>
<field name="abstract">This paper presents a view of innovation as a process that changes value systems of producers and adopters of creative design ideas. Value systems comprise interpretations of the function and behaviour of an artefact, encapsulated in a producer s or adopter s situation. Changes in these value systems can be induced using distinct classes of processes. The paper shows that innovation requires changes in the encapsulated value systems of both producers and adopters, driven by their interactions with the artefact and with each other. This provides the basis for a computational, agent-based framework for testing models of innovation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">794</field>
<field name="author">Liam O'Brien</field>
<field name="title">A Framework for Scope, Cost and Effort Estimation for Service Oriented Architecture (SOA) Projects</field>
<field name="keyword">Service Oriented Architecture</field>
<field name="keyword"> Scoping</field>
<field name="keyword"> Cost</field>
<field name="keyword"> Effort</field>
<field name="keyword"> Framework</field>
<field name="abstract">Determining the scope, size, cost and effort of a Service Oriented Architecture (SOA) project is important to building the business case and securing the funding for the project. Little work has been done in examining the various aspects of SOA projects and in having a systematic approach to determining scope, cost and effort. This paper outlines a framework for capturing and using details about various aspects of SOA projects that are used in determining the scope and estimating cost and effort. The paper examines different types of SOA projects and from these we have built a framework for capturing the activities that need to be undertaken in such projects, the artifacts that are used for planning and determining the scope of the work that needs to be done and capturing the various features and factors that are part of a cost model for SOA projects. We describe the framework in detail and show the various dimensions of the framework that will impact on the scope, cost and effort including the technical and social/cultural aspects as well as the maturity of the organization in terms of its experience in undertaking SOA projects.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">795</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Smith</field>
<field name="author">Daniel Lewis</field>
<field name="author">Andrew Zhang</field>
<field name="title">Key-sharing via channel randomness in body area networks: Is everyday movement sufficient?</field>
<field name="keyword">Body-area-networks</field>
<field name="abstract">We consider secure communication for Body-Area-Networks (BAN's). We examine the near-body radio channel of BAN's as a source of common randomness between two sensors. The movement of the subject and associated fading is used to hide a secure key from Eve. We examine recently approved radio channel models of the IEEE 802.15.6 Task Group, and show that the common randomness is too low rate for unconditional encoding. We find a key-generation rate around 2bits/second. We suggest the channel randomness may be better used in generating perpetually new keys for an AES-style encryption -- eg, a 128bits key every minute -- via a randomness scavenging procedure.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">796</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Gordon Plotkin</field>
<field name="title">Configuration Structures, Event Structures and Petri Nets</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> Configuration structures</field>
<field name="keyword"> Event structures</field>
<field name="keyword"> Petri nets</field>
<field name="keyword"> Propositional Logic.</field>
<field name="abstract">In this paper the correspondence between safe Petri

nets and event structures, due to Nielsen, Plotkin and Winskel, is

extended to arbitrary nets without self-loops, under the collective

token interpretation. To this end we propose a more general form of event

structure, matching the expressive power of such nets. These new event

structures and nets are connected by relating both notions with

&lt;I&gt;configuration structures&lt;/I&gt;, which can be regarded as

representations of either event structures or nets that capture their

behaviour in terms of action occurrences and the causal relationships

between them, but abstract from any auxiliary structure.

&lt;p&gt;

A configuration structure can also be considered logically, as a class

of propositional models, or - equivalently - as a propositional theory

in disjunctive normal from. Converting this theory to conjunctive

normal form is the key idea in the translation of such a structure

into a net.

&lt;p&gt;

For a variety of classes of event structures we characterise the

associated classes of configuration structures in terms of their

closure properties, as well as in terms of the axiomatisability of the

associated propositional theories by formulae of simple prescribed

forms, and in terms of structural properties of the associated Petri

nets.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">797</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Bas Luttik</field>
<field name="author">Nikola Tr ka</field>
<field name="title">Branching Bisimilarity with Explicit Divergence</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> labelled transition systems</field>
<field name="keyword"> modal logic</field>
<field name="keyword"> branching bisimilarity</field>
<field name="keyword"> divergence</field>
<field name="keyword"> coloured traces.</field>
<field name="abstract">We consider the relational characterisation of branching bisimilarity with explicit divergence. We prove that it is an equivalence and that it coincides with the original definition of branching bisimilarity with explicit divergence in terms of coloured traces. We also establish a correspondence with several variants of an action-based modal logic with until- and divergence modalities.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">798</field>
<field name="author">Peter Baumgartner</field>
<field name="author">John Slaney</field>
<field name="title">Constraint Modelling: A Challenge for First Order Automated Reasoning</field>
<field name="abstract">Cadoli et. al. noted the potential of first order automated reasoning for the purpose of analysing constraint models, and reported some encouraging initial experimental results. We are currently pursuing a very similar research program with a view to incorporating deductive technology in a state of the art constraint programming platform. Here we outline our own view of this application direction and discuss new empirical findings on a more extensive range of problems than those considered in the previous literature. While the opportunities presented by reasoning about constraint models are indeed exciting, we also find that there are formidable obstacles in the way of a practicaly useful implementation.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">799</field>
<field name="author">Luping Zhou</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="author">Richard Hartley</field>
<field name="author">Chantal Meslin</field>
<field name="author">Janine Walker</field>
<field name="title">Hippocampal Shape Analysis for AD Using an Efficient Hypothesis Test

and Regularized Discriminative Deformation</field>
<field name="abstract">In this paper, we present a framework to perform statistical shape

analysis for manually segmented hippocampi, which includes an

efficient permutation test for detecting subtle class differences,

and a regularized ``discriminative direction" method for

visualizing the shape discrepancy. Fisher permutation and

bootstrap tests are preferred to traditional hypothesis tests

which impose assumptions on the distribution of the samples. In

this paper, an efficient algorithm is adopted to rapidly perform

the {\em exact} tests. We extend this algorithm to multivariate

data by projecting the shape descriptors onto an ``informative

direction'' that preserves the original discriminative information

as much as possible to generate a scalar test statistic. This

``informative direction'' is further used to seek a ``discriminative

direction'' to isolate the discriminative shape difference between

classes from the individual variability. Compared with the

existing method, the ``discriminative direction" used in this

paper is regularized by requiring the newly generated shapes to

respect the underlying shape manifold as well as reflecting the

essential shape differences in two populations. Hence a more

accurate localization of difference is produced. We apply our

methods to analyzing the hippocampal shapes between controls 

and Alzheimer's Disease (AD) based on the publicly available

OASIS MRI database, and report the findings.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">800</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Three Common Mistakes in Modeling and Analysis of QoS of Service-Oriented Systems</field>
<field name="keyword">quality of service</field>
<field name="keyword"> contract</field>
<field name="keyword"> performance analysis</field>
<field name="keyword"> performance prediction</field>
<field name="keyword"> Web service selection</field>
<field name="keyword"> Web service composition</field>
<field name="keyword"> anti-pattern</field>
<field name="abstract">Since the basic Web service technologies did not address QoS issues, a huge body of academic and industrial works studied various aspects of QoS modeling/specification, analysis, monitoring, and control for service-oriented systems. Unfortunately, some of these works adopted oversimplifications that are often not appropriate for the complex reality. In this discussion session, we will point out to three such recurring oversimplifications: 1) specifying provider s QoS guarantees without limiting the number of requests; 2) using past QoS measurements to predict future QoS without taking into consideration context of requests; and 3) predicting response time of a sequence of services as a simple addition of their response times without discussing circumstances under which such calculation is valid/invalid. A significant number of authors (often independently from each other) used these oversimplifications without informing readers about their limitations and consequences. We will discuss why such oversimplifications are mistakes and anti-patterns in QoS modeling and anal-ysis for service-oriented systems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">801</field>
<field name="author">Stefan B _ttcher</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Wojciech Rytter</field>
<field name="title">08261 Abstracts Collection -- Structure-Based Compression of Complex Massive Data</field>
<field name="keyword">Data compression</field>
<field name="keyword"> algorithms for compressed strings and trees</field>
<field name="keyword"> XML-compression</field>
<field name="abstract">From June 22, 2008 to June 27, 2008 the Dagstuhl Seminar 08261

``Structure-Based Compression of Complex Massive Data'' was held 

in the International Conference and Research Center (IBFI), Schloss

Dagstuhl. During the seminar, several participants presented their

current research, and ongoing work and open problems were

discussed. Abstracts of the presentations given during the seminar as

well as abstracts of seminar results and ideas are put together in

this paper. The first section describes the seminar topics and goals

in general. Links to extended abstracts or full papers are provided,

if available.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">802</field>
<field name="author">Stefan B _ttcher</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Wojciech Rytter</field>
<field name="title">08261 Executive Summary -- Structure-Based Compression of Complex Massive Data</field>
<field name="keyword">Compression</field>
<field name="keyword"> Succinct Data Structure</field>
<field name="keyword"> Pattern Matching</field>
<field name="keyword"> Text Search</field>
<field name="keyword"> XML Query</field>
<field name="abstract">From 22nd June to 27th of June 2008, the Dagstuhl Seminar 

``08261 Structure-Based Compression of

Complex Massive Data'' took place at the

Conference and Research Center (IBFI) in Dagstuhl.

22 researchers with interests in theory and application

of compression and computation on compressed structures

met to present their current work and to discuss

future directions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">803</field>
<field name="author">Stefan B _ttcher</field>
<field name="author">Markus Lohrey</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Wojciech Rytter</field>
<field name="title">Structure-Based Compression of Complex Massive Data, 22.06. - 27.06.2008</field>
<field name="keyword">Compression</field>
<field name="keyword"> Succinct Data Structure</field>
<field name="keyword"> Pattern Matching</field>
<field name="keyword"> Text Search</field>
<field name="keyword"> XML Query</field>
<field name="abstract">From 22nd June to 27th of June 2008, the Dagstuhl Seminar

``08261 Structure-Based Compression of

Complex Massive Data'' took place at the

Conference and Research Center (IBFI) in Dagstuhl.

22 researchers with interests in theory and application

of compression and computation on compressed structures

met to present their current work and to discuss

future directions.</field>
<field name="date">2008</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">804</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Stefan Edelkamp</field>
<field name="author">Victor Schuppan</field>
<field name="author">Dragan Bosnaki</field>
<field name="author">Anton Wijs</field>
<field name="author">Husain Aljazzar</field>
<field name="title">Survey on Directed Model Checking</field>
<field name="abstract">This article surveys and gives historical accounts to the algorithmic

essentials of directed model checking, a promising bug-hunting

technique to mitigate the state explosion problem. In the enumeration

process, successor selection is prioritized. We discuss existing guidance

and methods to automatically generate them by exploiting system abstractions.

We extend the algorithms to feature partial-order reduction

and show how liveness problems can be adapted by lifting the search

space. For deterministic, finite domains we instantiate the algorithms

to directed symbolic, external and distributed search. For real-time domains

we discuss the adaption of the algorithms to timed automata and

for probabilistic domains we show the application to counterexample

generation. Last but not least, we explain how directed model checking

helps to accelerate finding solutions to scheduling problems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">805</field>
<field name="author">Glenn Geers</field>
<field name="author">Paul Tyler</field>
<field name="author">Bernhard Hengst</field>
<field name="author">Enyang Huang</field>
<field name="author">Douglas Quail</field>
<field name="title">Enhanced Roundabout Metering</field>
<field name="abstract">Roundabouts are widely used traffic control measures that function particularly well under balanced, light-to-medium approach traffic flows. However, even moderate flow imbalance can cause disruption and lengthy delays. One solution is to meter traffic on one or more of the approaches to increase both the entry capacity of the roundabout and reduce delay. In this paper the early-stages of a joint project between NICTA and the Roads and Traffic Authority of New South Wales (RTA) to develop novel control methodologies for roundabout metering is discussed, some preliminary results presented and future plans outlined.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">806</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="title">Non-Coherent Receiver with Fractional Sampling for Impulsive UWB Systems</field>
<field name="abstract">We propose a low complexity noncoherent receiver operating at twice the symbol-rate for systems where each data symbol consists of multiple frames/chips. The receiver does not require explicit timing and channel estimation. It implements simple delayed-autocorrelation, followed by sampling at twice the symbol-rate. Simulation results show the receiver achieves performance close to a conventional one with perfect timing.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">807</field>
<field name="author">Paul Bannerman</field>
<field name="title">Software Development Governance: A Meta-management Perspective</field>
<field name="keyword">Software</field>
<field name="keyword"> development</field>
<field name="keyword"> governance</field>
<field name="keyword"> management</field>
<field name="keyword"> engineering</field>
<field name="abstract">Software development governance is a nascent field of research. Establishing how it is framed early, can significantly affect the progress of contributions. This position paper considers the nature and role of governance in organizations and in the software development domain in particular. In contrast to the dominant functional and structural perspectives, an integrated view of governance is proposed as managing the management of an organizational domain (that is, a meta-management perspective). These concepts are developed into principles applied to software development governance and illustrated by a case study.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">808</field>
<field name="author">Paul Bannerman</field>
<field name="title">Software Architecture: Organizational Perspectives</field>
<field name="keyword">Software architecture</field>
<field name="keyword"> business value</field>
<field name="keyword"> governance</field>
<field name="keyword"> capability</field>
<field name="abstract">This paper examines software architecture through the lenses of four basic organizational perspectives and identifies opportunities for research to improve the positioning, acceptance and support of the discipline within organizations. It concludes that while software architecture may rest on firm technical foundations, its position in the organization is not as solid.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">809</field>
<field name="author">Karl Michael Goeschka</field>
<field name="author">Schahram Dustdar</field>
<field name="author">Frank Leymann</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Proceedings of the 3rd Workshop on Middleware for Service Oriented Computing (MW4SOC 2008)</field>
<field name="keyword">service-oriented computing</field>
<field name="keyword"> Web services</field>
<field name="keyword"> middleware</field>
<field name="keyword"> quality of service</field>
<field name="keyword"> reliability</field>
<field name="abstract">Service Oriented Computing (SOC) is a computing paradigm broadly pushed by vendors, utilizing services to support the rapid development of distributed applications in heterogeneous environments. The visionary promise of SOC is a world of cooperating services being loosely coupled to flexibly create dynamic business processes and agile applications that may span organisations and computing platforms and can nevertheless adapt quickly and autonomously to changes of requirements or context. Consequently, the subject of Service Oriented Computing is vast and enormously complex, spanning many concepts and technologies that find their origins in diverse disciplines like Workflow Management Systems, Component Based Computing, classical Web applications, and Enterprise Application Integration including Message Brokers and Middleware. In addition, there is a strong need to merge technology with an understanding of business processes and organizational structures, a combination of recognizing an enterprise's pain points and the potential solutions that can be applied to correct them. Middleware, on the other hand, is defined as the software layer in a distributed computing system that lies between the operating system and the applications on each site of the system (ObjectWeb consortium). Middleware is the enabling technology of system and enterprise application integration (EAI) and therefore it clearly plays a key role for SOC. While the immediate need of middleware support for Service Oriented Architectures (SOA) is evident, current approaches and solutions still fall short by primarily providing support for only the EAI aspect of SOC and do not sufficiently address composition support, service management and monitoring. Moreover, quality properties (in particular dependability and security) need to be addressed not only by interfacing and communication standards, but also in terms of integrated middleware support. But what makes these issues so different in a SOA setting? Why for instance is traditional middleware support for transaction processing different to transaction processing in SOA, reflecting different types of atomicity needs? One answer lies in the administrative heterogeneity, the loose coupling between coarse-grained operations and longrunning interactions, high dynamicity, and the required flexibility during run-time. Recently,

massive-scale and mobility were added to the challenges for Middleware for SOC. The highly dynamic modularity and need for flexible integration of services (e.g. Web service implementations) may, therefore, require new middleware architectures, protocols, and services. These considerations also lead to the question to what extent service-orientation at the middleware layer itself is beneficial (or not). Recently emerging Middleware as service offerings, from providers like Amazon or from the open source community, support this trend towards infrastructure services that can be purchased and consumed over the Internet. However, this model may not be suitable for all kinds of middleware functions, including those addressing dependability. Providing end-to-end properties and addressing cross-cutting concerns in a cross-organizational SOA is a particular challenge and the limits and benefits thereof have still to be investigated.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">810</field>
<field name="author">Michael Maher</field>
<field name="title">Open Contractible Global Constraints</field>
<field name="abstract">Open forms of global constraints allow the addition of new variables to an argument during the execution of a constraint program. Such forms are needed for difficult constraint programming problems where problem construction and problem solving are interleaved. However, in general, filtering that is sound for a global constraint can be unsound when the constraint is open.



This paper provides a simple characterization, called contractibility, of the constraints where filtering remains sound when the constraint is open. With this characterization we can easily determine whether a constraint is contractible or not. In the latter case, we can use it to derive the strongest contractible approximation to the constraint. We demonstrate how specific algorithms for some closed contractible constraints are easily adapted to open constraints.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">811</field>
<field name="author">Aditi Barthwal</field>
<field name="author">Michael Norrish</field>
<field name="title">Verified, Executable Parsing</field>
<field name="keyword">mechanisation</field>
<field name="keyword"> parsing</field>
<field name="keyword"> slr</field>
<field name="abstract">We describe the mechanisation of SLR parsing, covering background properties of context-free languages and grammars, as well as the construction of an SLR automaton. Among the various properties proved about the parser we show, in particular, soundness: if the parser results in a parse tree on a given input, then the parse tree is valid with respect to the grammar, and the leaves of the parse tree match the input; completeness: if the input is in the language of the grammar then the parser constructs the correct parse tree for the input with respect to the grammar; and non-ambiguity: grammars successfully converted to SLR automata are unambiguous. 



We also develop versions of the algorithms that are executable by automatic translation from HOL to SML. These alternative versions of the algorithms require some interesting termination proofs.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">812</field>
<field name="author">Silvia Richter</field>
<field name="author">Jordan T. Thayer</field>
<field name="author">Wheeler Ruml</field>
<field name="title">The Joy of Forgetting: Faster Anytime Search via Restarting</field>
<field name="keyword">Heuristic Search</field>
<field name="keyword"> Real-time Systems</field>
<field name="keyword"> Planning Algorithms</field>
<field name="abstract">Anytime search algorithms solve optimisation problems by quickly

finding a usually suboptimal solution and then finding improved

solutions when given additional time. To deliver a solution quickly,

they are typically greedy with respect to the heuristic cost-to-go

estimate h. In this paper, we first show that this low-h bias can

cause poor performance if the heuristic is inaccurate. Building on

this observation, we then present a new anytime approach that restarts

the search from the initial state every time a new solution is found.

We demonstrate the utility of our method via experiments in PDDL

planning as well as other domains. We show that it is particularly

useful for hard optimisation problems like planning where heuristics

may be quite inaccurate and inadmissible, and where the greedy

solution makes early mistakes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">813</field>
<field name="author">Terence Chen</field>
<field name="author">Roksana Boreli</field>
<field name="author">Thava Iyer</field>
<field name="title">A Cross layer Scheme for Maximising the Combination of Wireless VoIP Capacity and Quality</field>
<field name="keyword">cross-layer</field>
<field name="keyword"> WiMax</field>
<field name="keyword"> VoIP</field>
<field name="abstract">We present a mechanism for adjusting the combination of the number of simultaneous VoIP calls and voice codec rate based on the monitored wireless link parameters. We have implemented this mechanism which combines call admission control and setting of variable voice codec rate (and corresponding quality) dynamically in the Asterix PBX using the Speex voice codec. The mechanism includes dynamic parameter adjustments of currently active VoIP calls, with the capability to change quality on a per voice frame basis. It additionally limits acceptance of new calls until there is sufficient capacity to maintain a re-defined minimum quality for the existing calls. We have also implemented the wireless link capacity estimator, which, based on the parameters measured in the receiver, estimates the data rate currently available on the wireless link. Our implementation is for the pre-WiMax service using Navini customer premisses equipment (CPE), offered by Unwired in Australia. We are currently working with a certified WiMax equipment supplier and will extend the scheme to full WiMax implementation in the following months.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">814</field>
<field name="author">Paulette Lieby</field>
<field name="author">Brendan McKay</field>
<field name="author">Jeanette McLeod</field>
<field name="author">Ian Wanless</field>
<field name="title">Subgraphs of random k-edge-coloured k-regular graphs</field>
<field name="abstract">Let G = G(n) be a randomly chosen k-edge-coloured k-regular graph with

2n vertices, where k = k(n). Equivalently, G is the union of a random set of k

disjoint perfect matchings. Let h = h(n) be a graph with m = m(n) edges such

that m2 + mk = o(n). Using a switching argument, we find an asymptotic esti-

mate of the expected number of subgraphs of G isomorphic to h. Isomorphisms

may or may not respect the edge colouring, and other generalisations are also

presented. Special attention is paid to matchings and cycles.

The results in this paper are essential to a forthcoming paper of McLeod in

which an asymptotic estimate for the number of k-edge-coloured k-regular graphs

for k = o(n5/6) is found.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">815</field>
<field name="author">Csaba Veres</field>
<field name="author">Jennifer Sampson</field>
<field name="author">Steven Bleistein</field>
<field name="author">Karl Cox</field>
<field name="author">June Verner</field>
<field name="title">Using Semantic Technologies to Enhance a Requirements Engineering Approach for Alignment of IT with Business Strategy</field>
<field name="keyword">semantic technologies</field>
<field name="keyword"> requirements engineering</field>
<field name="keyword"> strategic alignment</field>
<field name="abstract">B-SCP is a promising framework addressing alignment

of IT with business strategy from a requirements engineering

persective. A problem with the B-SCP framework

is that it is extremely difficult to track dependencies between

requirements in a project of realistic complexity. We discuss

how the RDF data model with OWL semantics, will greatly

benefit an implementation using B-SCP. Our contribution is

to extend B-SCP by describing an ontology data structure for

representing the requirements and the complex rules which map

them together. Seven Eleven-Japan is used as an exemplar to

demonstrate improved productivity and consistency of B-SCP.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">816</field>
<field name="author">Luke Cole</field>
<field name="author">Nick Barnes</field>
<field name="title">Insect Inspired Three Dimensional Centring</field>
<field name="abstract">This paper describes work in a new project

based on a collaboration between experts in low

vision and a computer vision research group.

The focus of the project is to develop assistive

devices for individuals with severe and

profound vision impairment resulting from diseases

such as Age-related Macular Degeneration

and Retinitis Pigmentosa. We describe focus

groups that are being conducted to understand

such needs. To assist with tasks such as

navigation and obstacle avoidance for an individual

who is walking, knowledge of self-motion

is essential. In this context we present a new

implementation of a wide angle camera visual

motion recovery algorithm suitable for use on a

low cost, low power, light-weight wearable sensing

device. For wearable sensing, camera paths

are far more erratic than for ground based vehicles

such as wheeled robots or cars. Also,

weight from computing, cameras and batteries

is a major issue.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">817</field>
<field name="author">Leif Hanlen</field>
<field name="title">Estimating Workload and Effort Based on Wearable Sensors</field>
<field name="abstract">We outline progress toward building a predictor of workload based on instantaneous acceleration in 3-dimensions and synchronised heart-rate. Rating of Perceived Exertion (RPE) is used as a proxy measure of workload. 



We conclude that the range of athletes responses is too small for reliable use, and the prediction of fatigue is likely to be more reliable with a personalisation of the measure. 



This work was developed in partnership with the AIS, under the Workload/Athlete Performance Estimate package.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">818</field>
<field name="author">Jun Zhou</field>
<field name="author">Li Cheng</field>
<field name="author">Walter F. Bischof</field>
<field name="title">Spatial-Temporal Modeling of Interactive Image Interpretation</field>
<field name="abstract">We consider the problem of spatial-temporal modeling of interactive image interpretation. The interactive process is composed of a sequential prediction step and a change detection step. Combining the two steps

leads to a semi-automatic predictor that can be applied to a time-series, yields good predictions, and requests new human input when a change point is detected. The model can effectively capture changes of image features and gradually adapts to them. We propose an online frame-work that naturally addresses these problems in a uni&#12;ed manner. Our empirical study with a synthetic data set and a road tracking dataset demonstrate the efficiency of the proposed approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">819</field>
<field name="author">Quang Nguyen</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">A Graph-based Feature Combination Approach to Object Tracking</field>
<field name="abstract">In this paper, we present a feature combination approach to object

tracking based upon graph embedding techniques. The method presented here

abstracts the low complexity features used for purposes of tracking to a relational

structure and employs graph-spectral methods to embed them into a manifold.

This gives rise to a feature combination scheme which minimises the mutual

cross-correlation between features and is devoid of free parameters. This allows

an analytical solution making use of matrix factorisation techniques. The new target

location is recovered making use of a weighted combination of target-centre

shifts corresponding to each of the features under study, where the weights arise

from a cost function governed by the embedding process. This treatment permits

the update of the weights in an on-line fashion in a straightforward manner.

We illustrate the performance of our method in real-world image sequences and

compare our results to a number of alternatives.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">820</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="author">Baris Fidan</field>
<field name="author">Jia Fang</field>
<field name="author">Stephen Morse</field>
<field name="title">On the critical Radii in wireless Sensor Networks</field>
<field name="abstract">In this chapter, we summarize the graphical properties of a wireless sensor network to attain certain

properties such as connectivity, k-connectivity, unique localizability and easily localizability with low computational cost.

For a sensor network whose underlying graph is a unit disk graph, the critical sensing radii required to acquire these

properties are given.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">821</field>
<field name="author">Veysel Gazi</field>
<field name="author">Baris Fidan</field>
<field name="author">Shaohao Zhai</field>
<field name="title">Single view depth estimation based formation control of robotic swarms: Implementation using realistic robot simulator</field>
<field name="abstract">In earlier articles we had developed a formation control method based on single view depth estimation. In this paper, we implement that strategy on a robotic swarm composed of non-holonomic agents using the physics based Webots robot simulator. First, we review the single view depth estimation based scheme and the distributed control laws for the agents. Then, we discuss the

related modifications due to the non-holonomic constraints of the agents and some related implementation issues and present some simulation results obtained using the proposed control scheme. The scheme is based completely on local information implying that neither global position information nor communication between robots are needed for the implementation of the algorithm.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">822</field>
<field name="author">Vladimir Tosic</field>
<field name="title">On Quality of Service (QoS) Specification and Analysis for XML Web Services and Their Compositions</field>
<field name="keyword">quality of service (QoS)</field>
<field name="keyword"> Web services</field>
<field name="keyword"> service-oriented computing</field>
<field name="keyword"> service-level agreement</field>
<field name="keyword"> policy</field>
<field name="abstract">In many commercial applications of XML (Extensible Markup Language) Web services, it is necessary to know not only provided functionality, but also (guaranteed and/or achieved) quality of service (QoS). Potential QoS metrics of XML Web services include response time, throughput, availability, reliability, and many others. Comprehensive specification of QoS is necessary to perform QoS management (monitoring and control) activities and can be also used for XML Web service selection. In particular, such comprehensive QoS specification includes information about what QoS metrics are monitored, where, when, by what entity, as well as the desired (guaranteed) ranges of values and corresponding corrective control actions. Unfortunately, the basic XML Web service technologies do not currently support comprehensive QoS specification. 

 This seminar will start with an explanation of the importance of QoS specification for XML Web services and why the widely used basic XML Web service technologies are not enough. It will then describe the most widely used concepts for QoS specification, such as contract, service level agreement (SLA), class of service, and policy. This will be accompanied by critical analysis of the main languages used in QoS specification for XML Web services: WS-Agreement, Web Service Level Agreement (WSLA), and WS-Policy. Next, QoS analysis in XML Web service compositions (e.g., how to determine QoS of a Web service composition from known QoS of the composed XML Web services) will be discussed. An important advanced aspect of the seminar is that three common mistakes in QoS specification and analysis for XML Web services will be pointed out and analysed. These are: 1) specifying provider s QoS guarantees without limiting the number of requests; 2) using past QoS measurements to predict future QoS without taking into consideration context of the past requests; and 3) predicting QoS of a service composition using simple mathematical operations on QoS metrics of the individual composed services (e.g., adding response times of services in a sequence) without discussing circumstances under which such calculation is valid/invalid. At the end of the seminar, some resources for further exploration of the topic of QoS specification and analysis (as well as QoS monitoring and control) for XML Web services will be identified.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">823</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Patrick C. K. Hung</field>
<field name="title">Contract-Based Quality of Service (QoS) Monitoring and Control of XML Web Services</field>
<field name="keyword">quality of service (QoS)</field>
<field name="keyword"> Web service</field>
<field name="keyword"> service-level agreement (SLA)</field>
<field name="abstract">Tutorial Goals: 1) Explain that QoS specification and management are crucial for achieving the vision of service-oriented computing; 2) Inform that there have been many academic and industrial works in the area; 3) Summarize and analyze the main past results; 4) List and discuss open issues; 5) Provide a foundation for future research and/or decision-making by the participants

 ||

 Presentation Outline: 1) Introduction: Importance of QoS for WS; 2) Approaches to specification of QoS for WS; 3) Languages for specification of QoS for WS; 4) Approaches to management (monitoring and control) of QoS for WS; 5) Research and industrial tools for management of QoS for WS; 6) Summary: Past results and open issues; 7) Answers to questions and discussion</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">824</field>
<field name="author">Marcus Hutter</field>
<field name="author">Daniil Ryabko</field>
<field name="title">On the Possibility of Learning in Reactive Environments with Arbitrary Dependence</field>
<field name="keyword">Reinforcement learning</field>
<field name="keyword"> asymptotic average value</field>
<field name="keyword">self-optimizing policies</field>
<field name="keyword"> (non) Markov decision processes.</field>
<field name="abstract">We address the problem of reinforcement learning in which

observations may exhibit an arbitrary form of stochastic dependence

on past observations and actions, i.e.\ environments more general

than (PO)MDPs. The task for an agent is to attain the best possible

asymptotic reward where the true generating environment is unknown

but belongs to a known countable family of environments. We find

some sufficient conditions on the class of environments under which

an agent exists which attains the best asymptotic reward for any

environment in the class. We analyze how tight these conditions are

and how they relate to different probabilistic assumptions known in

reinforcement learning and related fields, such as Markov Decision

Processes and mixing conditions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">825</field>
<field name="author">Robby Goetschalckx</field>
<field name="author">Scott Sanner</field>
<field name="author">Kurt Driessens</field>
<field name="title">Cost-sensitive Parsimonious Linear Regression</field>
<field name="keyword">Machine Learning</field>
<field name="keyword"> Sparse Methods</field>
<field name="abstract">We examine linear regression problems where some features

may only be observable at a cost (e.g., in medical

domains where features may correspond to diagnostic tests

that take time and costs money). This can be important in

the context of data mining, in order to obtain the best predictions

from the data on a limited cost budget. We define

a parsimonious linear regression objective criterion that

jointly minimizes prediction error and feature cost. We modify

least angle regression algorithms commonly used for

sparse linear regression to produce the ParLiR algorithm,

which not only provides an efficient and parsimonious solution

as we demonstrate empirically, but it also provides

formal guarantees that we prove theoretically.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">826</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">A Simulator for the Dynamic On-Body Area Propagation Channel</field>
<field name="abstract">A radio communications network of miniaturized

sensors and/or actuating devices on the human body is referred to

as a wireless body area network (BAN). Understanding the on-body

radio channel requires characterization of the propagation. Such a

characterization can be used to develop suitable prediction tools

for the on-body radio propagation channel. Various studies of

propagation characteristics for on-body transmission exist e.g.

[Obayashi98].



In consideration of propagation characteristics of the (very)

narrowband on-body radio channel we address the following:

* Can the (very) narrowband BAN radio channel be effectively

characterized using a well-known statistical fading model, which

can approximate any transmit/receive (Tx/Rx) position on body

and include subject movement?



* Is it possible to develop an efficient simulator for channel gain of this dynamic (very) narrowband BAN radio channel based on the statistical fading model, which

approximates measurements and important second order statistics

of the fading model such as average level crossing rate (LCR)

and average fade duration (AFD)?



We find the channel gain for this

(very) narrowband dynamic channel considering an agglomerate of

measured on-body radio channel data is effectively characterized by

a Weibull distribution. Furthermore we develop a simple, easy

to implement simulator for received signal power profile, which

matches theoretical fading statistics for the Weibull model such as

average LCR and AFD, and approximates the AFD and average LCR over a

large agglomerate of measured data.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">827</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Can we engineer better process models?</field>
<field name="keyword">Design theory</field>
<field name="keyword"> Design methodology</field>
<field name="keyword"> Process modelling</field>
<field name="keyword"> Function-Behaviour-Structure</field>
<field name="abstract">This paper proposes an engineering design view of process modelling that helps address the issues of complexity and rigidity of many current process models. In this view, processes are modelled as artefacts that need to be designed, realised and managed throughout their life cycle. The paper argues that the issues of complexity and rigidity are symptoms of the more fundamental problem of delineation. This problem describes the difficulty of identifying and specifying the relationships between the various models that describe the engineering view of processes. Finally, the paper shows that the notion of design features from engineering design, represented using the function-behaviour-structure (FBS) ontology, can provide the basis for addressing the delineation problem and substantially improving process models.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">828</field>
<field name="author">Joshua Ho</field>
<field name="author">R. Koundinya</field>
<field name="author">Tiberio Caetano</field>
<field name="author">C. G. Dos Remedios</field>
<field name="author">M. Charleston</field>
<field name="title">Inferring Differential Leukocyte Activity from Antibody Microarrays using a Latent Variable Model</field>
<field name="keyword">Antibody microarray</field>
<field name="keyword"> latent variable models</field>
<field name="keyword"> Bayesian network</field>
<field name="keyword"> EM algorithm</field>
<field name="abstract">Recent development of cluster of differentiation (CD) antibody arrays has enabled expression

levels of many leukocyte surface CD antigens to be monitored simultaneously.

Such membrane-proteome surveys have provided a powerful means to detect changes

in leukocyte activity in various human diseases, such as cancer and cardiovascular diseases.

The challenge is to devise a computational method to infer differential leukocyte

activity among multiple biological states based on antigen expression profiles. Standard

DNA microarray analysis methods cannot accurately infer differential leukocyte activity

because they often fail to take the cell-to-antigen relationships into account. Here we

present a novel latent variable model (LVM) approach to tackle this problem. The idea

is to model each cell type as a latent variable, and represent the class-to-cell and cellto-

antigen relationships as a LVM. Once the parameters of the LVM are learned from

the data, differentially active leukocytes can be easily identified from the model. We

describe the model formulation and assumptions which lead to an efficient expectation maximizationalgorithm. Our LVM method was applied to re-analyze two cardiovascular

disease datasets. We show that our results match existing biological knowledge better

than other methods such as gene set enrichment analysis. Furthermore, we discuss how

our approach can be extended to become a general framework for gene set analysis for

DNA microarrays.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">829</field>
<field name="author">Andreas Bauer</field>
<field name="author">Jan Juerjens</field>
<field name="author">Yijun Yu</field>
<field name="title">Tools for Traceable Security Verification</field>
<field name="abstract">Dependable systems evolution has been identi ed by the UK

Computing Research Committee (UKCRC) as one of the current grand

challenges for computer science. We present work towards

addressing this challenge which focusses on one facet of

dependability, namely data security: We give an overview on an

approach for modelbased security veri cation which provides a

traceability link to the implementation. The approach uses a

design model in the UML security extension UMLsec which can be

formally veri ed against high-level security requirements such as

secrecy and authenticity. An implementation of the speci cation

can then be veri ed against the model by making use of run-time

veri cation through the traceability link. The approach supports

software evolution in so far as the traceability mapping is

updated when refactoring operations are regressively performed

using our tool-supported refactoring technique. The proposed

method has been applied to an implementation of the Internet

security protocol SSL.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">830</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="author">Nadine Marcus</field>
<field name="title">Analysis of Speech for Cognitive Load Indices</field>
<field name="keyword">Cognitive Load</field>
<field name="keyword"> Measurement</field>
<field name="keyword"> Speech</field>
<field name="keyword"> Grammar</field>
<field name="keyword"> Linguistic Features</field>
<field name="abstract">We present results from two studies where users speech was analysed to determine their cognitive load for the task at hand. Certain speech features have been shown to change under high levels of load and are good candidates for cognitive load indices. Firstly, we present a dual-task speech based user study in which we explored three speech features: pause length, pause frequency and latency to response. These features were evaluated for their diagnostic capacity. Pause length and latency to response are shown to be useful indicators of high load versus low load speech. Our other study presents a speech content analysis approach to the measurement of cognitive load which employs users linguistic and grammatical features of speech to determine their experienced level of cognitive load. We present the analyses of several linguistic features extracted from the speech data collected from the subjects (the members of a bushfire incident management team) involved in highly time-critical and data-intense bushfire management tasks around Australia. We discuss the results for nine selected linguistic features showing significant differences between the speech from the low load tasks and the high load tasks. We also present some preliminary results of pronoun analysis for the bushfire management collaborative speech.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">831</field>
<field name="author">Matthew Chapman</field>
<field name="author">Gernot Heiser</field>
<field name="title">vNUMA: A Virtual Shared-Memory Multiprocessor</field>
<field name="keyword">operating systems</field>
<field name="keyword"> virtual machines</field>
<field name="keyword"> distributed systems</field>
<field name="abstract">vNUMA, for virtual NUMA, is a virtual machine that presents a cluster as a virtual shared-memory multiprocessor. It is designed to make the computational power of clusters available to legacy applications and operating systems.



We present the design and Itanium-based implementation of vNUMA, and its trade-offs. We discuss in detail the enhancements to standard protocols that were made when implementing distributed shared memory inside a hypervisor instead of middleware. We examine the scalability of vNUMA on a small cluster, and analyse some of the design choices.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">832</field>
<field name="author">Michael Maher</field>
<field name="title">Open Constraints in a Boundable World</field>
<field name="abstract">Open forms of global constraints allow the addition of new variables to an argument during the execution of a constraint program.

Such forms are needed for difficult constraint programming problems

where problem construction and problem solving are interleaved.

We introduce a new model of open global constraint

where the length of the sequence of variables can be constrained

but there is no a priori restriction on the variables that might be added.

In general, propagation that is sound for a global constraint can be unsound when

the constraint is open.

We identify properties of constraints that simplify the design of algorithms for propagation

by identifying when no propagation can be done,

and use them to design propagation algorithms for several open global constraints.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">833</field>
<field name="author">Hee-Kap Ahn</field>
<field name="author">Sang-Won Bae</field>
<field name="author">Otfried Cheong</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Aperture-Angle and Hausdorff-Approximation of Convex Figures</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> algorithms</field>
<field name="abstract">The aperture angle a(x,Q) of a point x in Q in the plane with respect to a convex polygon Q is the angle of the smallest cone with apex x that contains Q. The aperture angle approximation error of a compact convex set C in the plane with respect to an inscribed convex polygon Q \subset C is the minimum aperture angle of any x in C\Q with respect to Q. We show that for any compact convex set C in the plane and any k &gt; 2, there is an inscribed convex k-gon Q \subset C with aperture angle approximation error (1 _ 2/(k+1)) \pi. This bound is optimal, and settles a conjecture by Fekete from the early 1990s.



The same proof technique can be used to prove a conjecture by Brass: If a polygon P admits no approximation by a sub-k-gon (the convex hull of k vertices of P) with Hausdorff distance s, but all subpolygons of P (the convex hull of some vertices of P) admit such an approximation, then P is a (k + 1)-gon. This implies the following result: For any k &gt; 2 and any convex polygon P of perimeter at most 1 there is a sub-k-gon Q of P such that the Hausdorff-distance of P and Q is at most (1/k+1) sin (\pi/(k+1)).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">834</field>
<field name="author">Gary Overett</field>
<field name="author">Lars Petersson</field>
<field name="title">Boosting a Heterogeneous Pool of Fast HOG Features for Pedestrian and Sign Detection</field>
<field name="abstract">This paper presents a fast Histogram of Oriented Gradients (HOG) based weak classifier that is extremely fast to compute and highly discriminative. This feature has been developed in an effort to balance the required processing and memory bandwidth so as to eliminate bottlenecks during runtime evaluation. The feature is the next generation in a series of features based on a novel precomputed image for HOG based features. It is more balanced in terms of processing and memory requirements than its predecessors, has a larger and richer feature space, and is more discriminant on a per feature basis.



 Unlike its predecessors, it is heterogeneous in terms of computational complexity. I.e. it has fast and slow variants. In order to optimize our feature selections between the faster and slower features available we implement a recently proposed modification to the RealBoost feature selection rule. This modification provides an additional means to balance processing and memory bandwidth on ordinary PC architectures.



 This feature is suitable for use within typical boosting frameworks. It is compared to Haar and Rectangular HOG features, as well as its related parent features. The new feature, which we call LiteHOG+, is found to be both more discriminant, and faster than its predecessor LiteHOG on a pedestrian detection task. LiteHOG+ shows promising results in some road sign detection tasks.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">835</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">Phase Transition Properties in K-connected Wireless Multi-hop Networks</field>
<field name="abstract">Consider a wireless multi-hop network formed by

distributing a total of n nodes randomly and uniformly in the

unit cube [0, 1]d (d = 1, 2, 3) and connecting any two distinct

nodes directly iff (if and only if) their Euclidean distance is not

greater than a given threshold r(n).We study the phase transition

phenomenon of a k-connected (k __ N) multi-hop network in

this paper. We show that the phase transition of k-connectivity

becomes sharper as n increases. We derive a generic analytical

formula for the phase transition width for large n and for any

fixed k __ N in d-dimensional space. The result in this paper is

important for understanding phase transition behavior, and it

provides valuable insight into the design and implementation of

wireless multi-hop networks.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">836</field>
<field name="author">Kevin Buchin</field>
<field name="author">Maike Buchin</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Detecting single file movement</field>
<field name="keyword">Algorithms</field>
<field name="keyword"> computational geometry</field>
<field name="keyword"> GISciece</field>
<field name="abstract">We study the problem of detecting a single file behavior in a set of trajectories. A group of entities is moving in single file if they are following each other, one behind the other. This movement pattern occurs often, among animals, humans, and vehicles. It is challenging to detect because it does not have a fixed layout.



In this paper we first model the notion of following behind, on which we base our definition of single file. We present efficient algorithms for detecting following behind and single file behaviors.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">837</field>
<field name="author">Basem Suleiman</field>
<field name="title">Models and Algorithms for Business Value-Driven Adaptation of Business Processes and Software Infrastructure</field>
<field name="keyword">Business Value-Driven</field>
<field name="keyword"> Business Processes</field>
<field name="keyword"> Business Value Metrics</field>
<field name="keyword"> Business Process Adaptation</field>
<field name="abstract">This research investigates how to provide automated analysis and decision-making support for adaptation of business processes and underlying software infrastructure, in the way that both maximizes business value metrics (BVMs) (e.g., profit, return on investment- ROI) and maintains alignment between business strategies and adaptation decisions. Among its expected contributions are improved modeling of business value metrics and business strategies in business process models and novel business value-driven techniques and algorithms that support primarily automatic and dynamic (run-time) adaptations, but also manual and static (design-time) adaptations. The proposed solutions will be implemented in software prototypes to demonstrate feasibility. Their usefulness will be evaluated through realistic case studies.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">838</field>
<field name="author">Ralph Becket</field>
<field name="author">Others Many</field>
<field name="author">Jinbo Huang</field>
<field name="author">Others Many</field>
<field name="title">The Many Roads Leading to Rome: Solving Zinc Models by Various Solvers</field>
<field name="abstract">Zinc is a solver-independent modelling language designed to

support very high level modelling and easy experimentation

with different solving technologies for the same problem.

In this position paper we illustrate the many ways in which

we can reformulate and solve a Zinc model using various solving technologies.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">839</field>
<field name="author">Suyu Kong</field>
<field name="author">Manas Kamal Bhuyan</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Tracking of Persons for Video Surveillance of Unattended Environments</field>
<field name="keyword">surveillance</field>
<field name="keyword"> tracking</field>
<field name="abstract">This paper describes a visual surveillance system for remote monitoring of unattended environments. For the purpose of ef ciently tracking multiple people in the presence of occlusions, we propose: (i) to combine blob matching with particle filtering, and (ii) to augment these tracking algorithms with a novel colour appearance model. The proposed system ef ciently counteracts the shortcomings of the two algorithms by switching from one to the other during occlusions. Results on public datasets as well as real surveillance videos from a metropolitan railway station demonstrate the ef cacy of the proposed system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">840</field>
<field name="author">Shaokang Chen</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Sai Sun</field>
<field name="author">Brian Lovell</field>
<field name="title">Representative Feature Chain for Single Gallery Image Face Recognition</field>
<field name="keyword">surveillance</field>
<field name="keyword"> face processing</field>
<field name="abstract">Under the constraint of using only a single gallery image per person, this paper proposes a fast multi-class pattern classification approach to 2D face recognition robust to changes in pose, illumination, and expression (PIE). This work has three main contributions: (1) we propose a representative face space method to extract robust features, (2) we apply a learning method to weight features in pairs, (3) we combine the feature pairs into a feature chain in order to find the weights for all features. The approach is evaluated for face recognition under PIE changes on three public databases. Results show that the method performs considerably better than several other appearance-based methods and can reliably recognise faces at large pose angles without the need for fragile pose estimation pre-processing. Moreover, computational load is low (comparable to standard eigenface methods), which is a critical factor in wide-area surveillance applications.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">841</field>
<field name="author">Shaokang Chen</field>
<field name="author">Ting Shan</field>
<field name="author">Brian Lovell</field>
<field name="title">Combining Classifiers in Rotated Face Space</field>
<field name="keyword">Face recognition</field>
<field name="keyword"> APCA classifiers</field>
<field name="abstract">Face recognition is a very complex classification problem due to nuisance variations in different conditions. Normally no single classifier can discriminate patterns well when unpredictable variations and a huge number of classes are involved. Combining multiple classifiers can improve discriminability over the best single classifier. In this paper, we present a way to combine classifiers for face recognition problem based on APCA classifiers. The proposed combinator generates various classifiers by rotating various face spaces and fusing them by applying a weighted distance measure. The combined classifier is tested on the Asian Face Database with 856 images. Experiments show a 30% reduction in classification error rate of our combined classifier and illustrates that combining classifiers from different face spaces may perform better than those based on a single face space.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">842</field>
<field name="author">Guido Governatori</field>
<field name="author">Shazia Sadiq</field>
<field name="title">The Journey to Business Process Compliance</field>
<field name="keyword">Compliance</field>
<field name="keyword"> Business Process</field>
<field name="keyword"> Deontic Logic</field>
<field name="keyword"> Defeasible Logic</field>
<field name="abstract">It is a typical scenario that many organisations have their business

 processes specified independently of their business obligations (which

 includes contractual obligations to business partners, as well as

 obligations a business has to fulfil against regulations and industry

 standards). This is because of the lack of guidelines and tools that

 facilitate derivation of processes from contracts but also because of the

 traditional mindset of treating contracts separately from business

 processes. This chapter will provide a solution to one specific problem

 that arises from this situation, namely the lack of mechanisms to check

 whether business processes are compliant with business contracts. The

 chapter begins by defining the space for business process compliance and

 the eco-system for ensuring that process are compliant. The key point is

 that compliance is a relationship between two sets of specifications: the

 specifications for executing a business process and the specifications

 regulating a business. The central part of the chapter focuses on a logic

 based formalism for describing both the semantics of normative

 specifications and the semantics of compliance checking procedures.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">843</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Surrogate Regret Bounds for Proper Losses</field>
<field name="keyword">Learning Theory</field>
<field name="keyword"> Surrogate Loss</field>
<field name="keyword"> Bregman Divergence</field>
<field name="keyword"> Scoring Rules</field>
<field name="keyword"> Reductions</field>
<field name="keyword"> Link Functions</field>
<field name="abstract">We present tight surrogate regret bounds for the class of proper 

	(i.e., Fisher consistent) losses.

 The bounds generalise the margin-based bounds due to

 Bartlett et al. The proof uses Taylor's theorem and

 leads to new representations for loss and regret and a simple proof of the

 integral representation of proper losses. We also present a different

 formulation of a duality result of Bregman divergences which leads to a

 demonstration of the convexity of composite losses using canonical link

 functions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">844</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="title">Non-Coherent Receiver with Fractional Sampling Rate</field>
<field name="abstract">Poster presented for AusCTW 2009, Non-coherent fractional sampling UWB receiver</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">845</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Characterizations of the dynamic narrowband on-body to on-body, and on-body to off-body area channel</field>
<field name="abstract">A summary of statistical characterizations is presented of the

dynamic narrowband on-body and on-body to off-body area channel at

carrier frequencies near the 900 MHz and 2400 MHz Industrial,

Scientific and Medical Bands. In consideration of an approximate

bandwidth of 10 MHz and an indoor office environment, the Lognormal

distribution is found to provide a good fitting model, independent

of transmit/receive (Tx/Rx) antenna location, especially with

movement in the on-body channel, and with and without movement in

the off-body channel. Based on these results it is demonstrated

that model fitting is relatively independent of Line-of-Sight (LOS)/

Non Line-of-Sight (NLOS) transmission scenarios. It is similarly

shown that the Rayleigh distribution is a very poor fit to the

received amplitude statistics. Thus radio channel dependent standard

design considerations for typical mobile/stationary cellular radio

scenarios may not be applicable to wireless body area networks

(WBAN).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">846</field>
<field name="author">Nicholas Fitzroy-Dale</field>
<field name="author">Ihor Kuz</field>
<field name="title">Towards automatic optimisation of componentised systems</field>
<field name="abstract">Use of hardware-based memory protection to implement a componentised system is an effective way to enforce isolation between untrusted software components. Unfortunately this type of system design can lead to poor performance. Manual optimisation is error-prone and difficult. Instead, we describe a system to perform automatic optimisation of components, relying on three major functional units: a method to reconfigure the component system, simulations of each component in order to determine performance characteristics, and a system simulator that makes use of those characteristics to construct a ranking of optimisations. We start with a simple model and iteratively expand it until it is suitable for a wide variety of performance-measurement scenarios, and show that a small amount of information provided with each component allows for a wide variety of optimisation checks, such as scheduling, threading, and cache performance. We present our initial results with this system and discuss a number of interesting extensions.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">847</field>
<field name="author">Ju Lynn Ong</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">Mean Shape Models for Polyp Detection in CT Colonography</field>
<field name="keyword">mean shape</field>
<field name="keyword"> polyps detection</field>
<field name="abstract">A global representation of polyp and non polyp shapes 	 

are constructed following a point distribution model (PDM) 	 

as an alternative to current methods which only inspect lo-	 

cal shape characteristics at a point on the surface. The 	 

decision on whether or not a candidate lesion is a polyp 	 

can then be made by comparing the minimum Euclidean 	 

distance of the candidate lesion to the constructed mean 	 

shapes. The model closer in distance to the candidate le-	 

sion is selected to represent that particular lesion -polyp 	 

or non polyp. This shape model can also be used to investi-	 

gate the shape variability of the different lesions detected by 	 

constructing an allowable shape domain for each of these 	 

lesions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">848</field>
<field name="author">Huidong Jin</field>
<field name="author">Olivier DeVel</field>
<field name="author">Ke Zhang</field>
<field name="author">Nianjun Liu</field>
<field name="title">Knowledge Discovery from Honeypot Data for Monitoring Malicious Attacks</field>
<field name="abstract">Owing to the spread of worms and botnets, cyber attacks

have signi cantly increased in volume, coordination and sophistication.

Cheap rentable botnet services, e.g., have resulted in sophisticated bot-

nets becoming an e ective and popular tool for committing online crime

these days. Honeypots, as information system traps, are monitoring or

de ecting malicious attacks on the Internet. To understand the attack

patterns generated by botnets by virtue of the analysis of the data col-

lected by honeypots, we propose an approach that integrates a cluster-

ing structure visualisation technique with outlier detection techniques.

These techniques complement each other and provide end users both a

big-picture view and actionable knowledge of high-dimensional data. We

introduce KNOF (K-nearest Neighbours Outlier Factor) as the outlier

de nition technique to reach a trade-o between global and local outlier

de nitions, i.e., Kth-Nearest Neighbour (KNN) and Local Outlier Factor

(LOF) respectively.We propose an algorithm to discover the most signi -

cant KNOF outliers. We implement these techniques in our hpdAnalyzer

tool. The tool is successfully used to comprehend honeypot data. A series

of experiments show that our proposed KNOF technique substantially

outperforms LOF and, to a lesser degree, KNN for real-world honeypot

data.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">849</field>
<field name="author">Ying Chen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dhammika Jayalath</field>
<field name="title">Clipping Noise Compensation for OFDMA Systems</field>
<field name="abstract">Abstract This paper presents an efficient low-complexity clipping

noise compensation scheme for PAR reduced orthogonal

frequency division multiple access (OFDMA) systems. Conventional

clipping noise compensation schemes proposed for OFDM

systems are decision directed schemes which use demodulated

data symbols. Thus these schemes fail to deliver expected

performance in OFDMA systems where multiple users share a

single OFDM symbol and a specific user may only know his/her

own modulation scheme. The proposed clipping noise estimation

and compensation scheme does not require the knowledge of

the demodulated symbols of the other users, making it very

promising for OFDMA systems. It uses the equalized output and

the reserved tones to reconstruct the signal by compensating the

clipping noise. Simulation results show that the proposed scheme

can significantly improve the system performance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">850</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Extraction of FM Components from Speech Signals Using an All-pole Model</field>
<field name="abstract">Frequency modulation has recently emerged as a promising model for characterising the phase of a speech signal. Proposed is a novel technique for extracting the frequency modulation (FM) components from the subband speech signal, using a second-order all-pole model. Evaluation of a speaker recognition system employing FM features, extracted using the proposed technique, on the NIST 2001 database reveals improvement over MFCC baseline and significant improvements over the discrete energy separation algorithm and a Hilbert transform based approach in terms of equal error rate.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">851</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">FM Features for Forensic Speaker Recognition</field>
<field name="keyword">frequency modulation</field>
<field name="keyword"> automatic forensic speaker recognition</field>
<field name="abstract">Frequency modulation (FM) information from the speech signal is herein proposed to complement the conventional amplitude based features for automatic forensic speaker recognition systems. In addition to presenting the AM-FM model of speech used to generate the proposed frequency modulation features, the significance of frequency modulation for speaker recognition is discussed. Evaluation results from an automatic forensic speaker recognition system combining FM and MFCC features are shown to out-perform those of a system employing MFCC features alone, in terms of all typical metrics, such as detection error trade-off curves, Tippett curves and applied probability of error curves.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">852</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">An Investigation of Sub-band FM Feature Extraction in Speaker Recognition</field>
<field name="keyword">Frequency modulation</field>
<field name="keyword"> automatic speaker recognition</field>
<field name="keyword"> Mel scale</field>
<field name="keyword"> filter bank</field>
<field name="abstract">Following recent evidence that FM features extracted from a sub-band decomposition of speech are highly uncorrelated, this paper investigates the effect of the number of auditory scale sub-bands in FM based front-end processing. For this study, a newly developed robust FM extraction method based on the least square differential ratio is used to extract features, comprising one FM component per sub-band. Automatic speaker recognition experiments were conducted on the cellular NIST 2001 database, with the number of filters in the front-end varied from 6 to 26. Performance degradation was observed for very low numbers of filters and very high numbers of filters. Results show that for a 4 kHz speech bandwidth, a minimum of 10 and a maximum of 18 sub-bands is a suitable choice for speech front-end applications such as automatic speaker recognition.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">853</field>
<field name="author">Steve Glass</field>
<field name="author">Marius Portmann</field>
<field name="author">Muthukkumarasamy Vallipuram</field>
<field name="title">Detecting Man-in-the-Middle and Wormhole Attacks in Wireless Mesh Networks</field>
<field name="keyword"/>
<field name="abstract">Wireless networks are being used increasingly in industrial, health care, military and public-safety environments.In these environments security is extremely important because a successful attack against the network may pose a threat to human life. To secure such wireless networks against hostile attack requires both preventative and detective measures. In this paper we propose a novel intrusion detection mechanism that identi?es man-in-the-middle and wormhole attacks against wireless mesh networks by external adversaries. A simple modification to the wireless MAC protocol is proposed to expose the presence of an adversary conducting a man-in-the-middle or wormhole attack. We evaluate the modified MAC protocol experimentally and show the detection mechanism to have a high detection rate, no false positives and a small computational and communication overhead.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">854</field>
<field name="author">Steve Glass</field>
<field name="author">Marius Portmann</field>
<field name="author">Muthukkumarasamy Vallipuram</field>
<field name="title">A Software-Defined Radio Receiver for APCO Project 25 Signals</field>
<field name="keyword">public</field>
<field name="keyword">safety software</field>
<field name="keyword">defined</field>
<field name="keyword">radio</field>
<field name="abstract">APCO Project 25 (P25) is the digital communications standard that has widespread deployment amongst emergency ?rst-responders in several different countries. This paper describes the implementation of a low-cost software-defined radio receiver for APCO Project 25 signals. The P25 receiver has been developed as part of an investigation into the security of the P25 protocol suite and provides low-level access to the actual message traffic using the WireShark packet sniffer. The proposed P25 receiver is a useful diagnostic and security analysis tool. Our initial experience suggests that the flexibility of the software-defined radio approach is well-suited to meeting the varying needs of public-safety radio communications.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">855</field>
<field name="author">Ansgar Fehnker</field>
<field name="title">Formal Methods in the Wireless Network Domain</field>
<field name="abstract">Formal Methods have a long track record of developing tools, methods and techniques, to verify protocols, software and hardware systems. Wireless Networks combine these areas in a characteristic way. Due to the nature of the network nodes, hardware is comparatively small as is the software running on it. Due to the nature of the network algorithms and protocols should be distributed and concurrent. An important aspect of a wireless networks is that nodes use multi-hop communication on an unreliable medium, and that the network is subject to dynamic changes and environmental interference.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">856</field>
<field name="author">Ansgar Fehnker</field>
<field name="title">Creating Correct Network Protocols</field>
<field name="abstract">Presentation for PhD defence of Oscar Wibling, Uppsala University. See http://uu.diva-portal.org/smash/record.jsf?pid=diva2:172804</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">857</field>
<field name="author">Chunhua Shen</field>
<field name="author">Hanxi Li</field>
<field name="title">On the Dual Formulation of Boosting Algorithms</field>
<field name="keyword">AdaBoost</field>
<field name="keyword"> LogitBoost</field>
<field name="keyword"> LPBoost</field>
<field name="keyword"> Lagrange duality</field>
<field name="keyword"> linear programming</field>
<field name="keyword"> entropy maximization</field>
<field name="abstract">We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost

and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these

boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin

distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately,

AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column

generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results

to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers

are needed to build the ensemble using our proposed optimization technique.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">858</field>
<field name="author">Michael Maher</field>
<field name="author">Nina Narodytska</field>
<field name="author">Claude-Guy Quimper</field>
<field name="author">Toby Walsh</field>
<field name="title">Flow-Based Propagators for the SEQUENCE and Related Global Constraints</field>
<field name="keyword"/>
<field name="abstract">We propose new filtering algorithms for the SEQUENCE constraint and several extensions 

which are based on network flows. Our propagator for the SEQUENCE constraint enforces domain consistency in $O(n^2)$ time down a branch of the search tree. This improves upon the best existing

domain consistency algorithm by a factor of $O(log n)$. The flows used in these algorithms

are derived from a linear program. Some of them differ from the flows used to propagate global

constraints like GCC since the domains of the variables are encoded as costs on 

the edges rather than capacities. Such flows are efficient for maintaining bounds consistency over large domains and may be useful for other global arithmetic constraints.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">859</field>
<field name="author">Vidhyasaharan Sethu</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Phonetic and Speaker Variations in Automatic Emotion Classification</field>
<field name="keyword">emotion classification</field>
<field name="keyword"> speaker normalisation</field>
<field name="keyword"> phoneme recognition</field>
<field name="abstract">The speech signal contains information that characterises the speaker and the phonetic content, together with the emotion being expressed. This paper looks at the effect of this speaker- and phoneme-specific information on speech-based automatic emotion classification. The performances of a classification system using established acoustic and prosodic features for different phonemes are compared, in both speaker-dependent and speaker-independent modes, using the LDC Emotional Prosody speech corpus. Results from these evaluations indicate that speaker variability is more significant than phonetic variations. They also suggest that some phonemes are easier to classify than others.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">860</field>
<field name="author">Vidhyasaharan Sethu</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Empirical Mode Decomposition Based Weighted Frequency Feature for Speech-Based Emotion Classification</field>
<field name="keyword">Emotion classification</field>
<field name="keyword"> instantaneous frequency</field>
<field name="keyword"> empirical mode decomposition</field>
<field name="keyword"> hidden Markov models</field>
<field name="keyword"> front-end processing</field>
<field name="abstract">This paper focuses on speech based emotion classification utilizing acoustic data. The most commonly used acoustic features are pitch and energy, along with prosodic information like rate of speech. We propose the use of a novel feature based on instantaneous frequency obtained from the speech, in addition to the aforementioned features, in order to take into account the vocal tract parameters as well as vocal chord excitation. The proposed features employ the recently emerged empirical mode decomposition to decompose speech into AM-FM signals that are symmetric about zero and suitable for Hilbert transformation to extract the instantaneous frequency. The proposed features provide a relative increase in classification accuracy of approximately 9% when appended to established acoustic features.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">861</field>
<field name="author">Ting Shan</field>
<field name="author">Brian Lovell</field>
<field name="author">Shaokang Chen</field>
<field name="title">Face Recognition Robust to Head Pose from One Sample Image</field>
<field name="keyword">Face Recognition</field>
<field name="keyword"> Adaptive Principal Component Analysis</field>
<field name="abstract">Most face recognition systems only work well under quite constrained environments. In particular, the illumination conditions, facial expressions and head pose must be tightly controlled for good recognition performance. In 2004, we proposed a new face recognition algorithm, Adaptive Principal Component Analysis (APCA) [4], which performs well against both lighting variation and expression change. But like other eigenface-derived face recognition algorithms, APCA only performs well with frontal face images. The work presented in this paper is an extension of our previous work to also accommodate variations in head pose. Following the approach of Cootes et al, we develop a face model and a rotation model which can be used to interpret facial features and synthesize realistic frontal face images when given a single novel face image. We use a Viola-Jones based face detector to detect the face in real-time and thus solve the initialization problem for our Active Appearance Model search. Experiments show that our approach can achieve good recognition rates on face images across a wide range of head poses. Indeed recognition rates are improved by up to a factor of 5 compared to standard PCA.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">862</field>
<field name="author">Hans L. Bodlaender</field>
<field name="author">Corinne Feremans</field>
<field name="author">Alexander Grigoriev</field>
<field name="author">Eelko Penninkx</field>
<field name="author">Rene Sitters</field>
<field name="author">Thomas Wolle</field>
<field name="title">On the minimum corridor connection problem and other generalized geometric problems</field>
<field name="abstract">In this paper we discuss the complexity and approximability of the minimum corridor connection problem where, given a rectilinear decomposition of a rectilinear polygon into ``rooms'', one has to find the minimum length tree along the edges of the decomposition such that every room is incident to a vertex of the tree. We show that the problem is strongly NP-hard and give a subexponential time exact algorithm. For the special case when the room connectivity graph is $k$-outerplanar the algorithm running time becomes cubic. We develop a polynomial time approximation scheme for the case when all rooms are fat and have nearly the same size. When rooms are fat but are of varying size we give a polynomial time constant factor approximation algorithm.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">863</field>
<field name="author">David Shepherd</field>
<field name="author">Fredrik Braennstroem</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">Adaptive Optimization of an Iterative Multiuser Detector for Turbo-Coded CDMA</field>
<field name="keyword">Multiuser detection</field>
<field name="keyword"> transmission technology</field>
<field name="keyword"> CDMA</field>
<field name="keyword"> wireless access</field>
<field name="keyword"> source/channel coding</field>
<field name="keyword"> transmission technology.</field>
<field name="abstract">Extrinsic information transfer (EXIT) charts are utilized to optimize the iterative multiuser detector receiver in a multiuser turbo-coded CDMA system. The (receive) power levels are optimized for the system load using a constrained nonlinear optimization approach. The optimal decoding schedule is derived dynamically using the power optimized EXIT chart and a Viterbi search algorithm. Dynamic scheduling is shown to be a more flexible approach which results in a more stable QoS for a typical system configuration than one-shot scheduling, and large complexity savings over a receiver without scheduling.

We verify through simulations that complexity savings of over 50% and power savings of over 8dB can be achieved. We show that the optimized power levels combined with adaptive scheduling allows for efficient utilization of receiver resources for heavily loaded systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">864</field>
<field name="author">Mohammad Emam Hossain</field>
<field name="author">June Verner</field>
<field name="author">Muhammad Ali Babar</field>
<field name="title">Towards a Framework for Using Agile Approaches in Distributed Software Development</field>
<field name="keyword">Agile approaches</field>
<field name="keyword"> Distributed Software Development</field>
<field name="keyword"> Case study</field>
<field name="abstract">As agile methods and Distributed Software Development (DSD) have become increasingly popular, DSD project managers have been increasingly exploring the viability of using agile approaches in their development environments. Despite the expected benefits of using an agile approach with a DSD project, the overall combining mechanisms of the two approaches are not clearly understood. To address this challenge, we have proposed a conceptual framework, based on the research literature. This framework is expected to aid a project manager in deciding what agile strategies are effective for a particular DSD project, taking into consideration the project s contextual factors. We use an industry-based case study to explore the various components of our conceptual framework. Our case study is planned and conducted according to specific case study guidelines. We identify the agile practices and agile supporting practices used by a DSD project manager. We conclude with future research directions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">865</field>
<field name="author">Duy Hoang Pham</field>
<field name="author">Subhasis Thakur</field>
<field name="author">Guido Governatori</field>
<field name="title">Settling on the Group's Goals: An n-Person Argumentation Game Approach</field>
<field name="keyword">Multi-agent system</field>
<field name="keyword"> defeasible logic</field>
<field name="keyword"> argumentation system</field>
<field name="abstract">Argumentation games have been proved to be a robust and flexible

tool to resolve conflicts among agents. An agent can propose its explanation and

its goal known as a claim, which can be refuted by other agents. The situation is

more complicated when there are more than two agents playing the game.

We propose a weighting mechanism for competing premises to tackle with

conflicts from multiple agents in an n-person game. An agent can defend its proposal

by giving a counter-argument to change the opinion of the majority of

opposing agents. During the game, an agent can exploit the knowledge that other

agents expose in order to promote and defend its main claim.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">866</field>
<field name="author">Bathiya Senanayake</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">An Optimal Asynchronous IDMA Receiver</field>
<field name="keyword">IDMA</field>
<field name="keyword"> ML</field>
<field name="keyword"> asynchronous</field>
<field name="abstract">In this paper we construct an optimal IDMA receiver where the timing

for each user is asynchronous. We derive the maximum likelihood

receiver for a chip asynchronous IDMA system. A super-block

structure is used to formulate a band diagonal matrix model for

asynchronous IDMA. We show that the cost function for the ML decoder

can be viewed as a path through a structured trellis. We use a

dynamic programming algorithm to find the maximum likelihood

transmitted bit sequence. We compare the performance of the optimal

receiver with the conventional iterative receiver. Our results show

that the optimal asynchronous IDMA receiver outperforms the

iterative IDMA receiver.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">867</field>
<field name="author">Efstratios Kontopoulos</field>
<field name="author">Nick Bassiliades</field>
<field name="author">Guido Governatori</field>
<field name="author">Grigoris Antoniou</field>
<field name="title">Extending a Defeasible Reasoner with Modal and Deontic Logic Operators</field>
<field name="abstract">Defeasible logic is a non-monotonic formalism that 

deals with incomplete and conflicting information. 

Modal logic deals with necessity and possibility, exhibiting defeasibility; thus, it is possible to combine de- 

feasible logic with modal operators. This paper reports 

on the extension of the DR-DEVICE defeasible reasoner with modal and deontic logic operators. The aim 

is a practical defeasible reasoner that will take advantage of the expressiveness of modal logics and the 

flexibility to define diverse agent types and behaviors.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">868</field>
<field name="author">Guido Boella</field>
<field name="author">Guido Governatori</field>
<field name="author">Joris Hulstijn</field>
<field name="author">R gis Riveret</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Leendert van der Torre</field>
<field name="title">Time and Defeasibility in FIPA ACL Semantics</field>
<field name="keyword">Agent Communication Language</field>
<field name="keyword"> Defeasible Logic</field>
<field name="abstract">Inferences about speech acts are often conditional, non-monotonic, and involve the issue of time. Most agent communication languages, however, ignore these issues, due to 

the dif culty to combine them in a single formalism. This 

paper addresses such issues in defeasible logic, and shows 

how to express a semantics for ACLs in order to make non-monotonic inferences on the basis of speech acts</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">869</field>
<field name="author">Chunhua Shen</field>
<field name="author">Junae Kim</field>
<field name="author">Wang hanzi</field>
<field name="title">Generalized Kernel-based Visual Tracking</field>
<field name="abstract">Kernel-based mean shift (MS) trackers have proven to be a promising alternative to

 stochastic particle filtering trackers. Despite its popularity, MS trackers have two

 fundamental drawbacks: (1) The template model can only be built from a single image;

 (2) It is difficult to adaptively update the template model. 

 In this work we generalize the plain MS trackers and attempt to overcome these two

 limitations. 

 

 It is well known that modeling and maintaining a representation of a target object is

 an important component of a successful visual tracker. 

 However, little work has been done on building a robust template model for

 kernel-based MS tracking. In contrast to building a template from a single frame, we

 train a robust object representation model from a large amount of data. 

 Tracking is viewed as a binary classification problem, and 

 a discriminative classification rule is learned to distinguish between 

 the object and background. 

 We adopt a support vector machine (SVM) for training. The tracker is then

 implemented by maximizing the classification score. An iterative optimization scheme

 very similar to MS is derived for this purpose. Compared with the plain MS

 tracker, it is now much easier to incorporate on-line template adaptation to cope

 with inherent changes during the course of tracking. To this end, a sophisticated

 on-line support vector machine is used.

 We demonstrate successful localization and tracking on various data sets.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">870</field>
<field name="author">Fawad Nazir</field>
<field name="author">Hulmut Prendinger</field>
<field name="title">Bridging The Knowledge Divide : Increasing Social Scientific Awareness using Second Life</field>
<field name="keyword">Knowledge Divide</field>
<field name="keyword"> Second Life</field>
<field name="abstract">The term digital divide refers to gap between people with effective access to digital and information technology and those without. Similarly in this paper we propose to a term "Knowledge Divide" to refer to the gap between the knowledge and awareness of a scientist and a layman. Scientists work for the betterment of the society and its pity that the society do not know and do not understand what scientists are doing. Moreover the people in the society can not participate in the experiments and give feedback. Currently scientists rely on making promotional videos, animations, etc to create awareness about their work. These techniques are one way transfer of knowledge and is not very interesting and effective. In this paper we propose to use second life to create scientific simulations in a way that a layman can see and interact with them. For an example, in this paper we discuss global warming simulations for instance, rainfall and water level simulations in second life. In this paper we visualizing the global warming projections. We used Environment Modeling Language (EML), C sharp based simulation engine and neural networks to provide a virtual reality based visualization in second life. Given the data from the climate change projection models we visualize it in second life.



The main objectives of our research work are:



1. To develop collaborative visualization techniques for atmosphere climate change models.

2. To contribute to the improvement of global warming simulation.

3. To facilitate international collaborations, collaborative research and social awareness.



The silent features of our research are:



1. Users/avatars can create climate effects such rain and water-level.

2. Users/avatars can play frame by frame simulations in virtual world and also zoom into the simulations effects.

3. User/avatars can have multi-dimensional global warming simulations. For instance, a user can see the affect his/her of climate change model on a physical-biogeochemical climate model.



Realizing global warming projections in second life will allow anyone, including non-experts, to experiment and experience the effects of global warming on their lands/houses/islands etc. Previously such data and results were only accessible to researchers working in dedicated laboratories and having high-performance computing resources.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">871</field>
<field name="author">Alex Penev</field>
<field name="author">Raymond Wong</field>
<field name="title">Finding similar pages in a social tagging repository</field>
<field name="keyword">Web</field>
<field name="keyword"> tagging</field>
<field name="keyword"> social bookmarking</field>
<field name="keyword"> del.icio.us</field>
<field name="abstract">Social tagging describes a community of users labeling web content with tags. It is a simple activity that enriches our knowledge about resources on the web. For a computer to help users search the tagged repository, it must know when tags are good or bad. We describe TagScore, a scoring function that rates the goodness of tags. The tags and their ratings give us a succinct synopsis for a page. We `find similar' pages in Del.icio.us by comparing synopses. Our approach gives good correlation to the full cosine similarity but is hundreds of times faster.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">872</field>
<field name="author">Alex Penev</field>
<field name="author">Raymond Wong</field>
<field name="title">Grouping hyperlinks for improved voice/mobile accessibility</field>
<field name="keyword">Web</field>
<field name="keyword"> accessibility</field>
<field name="keyword"> clustering</field>
<field name="keyword"> links</field>
<field name="keyword"> metadata</field>
<field name="abstract">The majority of websites focus on the presentation of content as opposed to meeting standards or accessibility requirements. Accessibility is important for the future of the web: sites that are easier to use on a wider range of devices will likely attract more traffic. A factor guiding the evaluation of accessibility is how easy it is to find something on a page.



We propose an on-demand automatic clustering of a page's hyperlinks to help a blind user locate a desired link quickly. Clustering related links into cohesive groups allows users to focus attention on a subset of content. Another application is to help create portal pages for hand-held mobile devices.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">873</field>
<field name="author">Alex Penev</field>
<field name="author">Raymond Wong</field>
<field name="title">TagScore: Approximate similarity using tag synopses</field>
<field name="keyword">Web</field>
<field name="keyword"> tagging</field>
<field name="keyword"> social bookmarking</field>
<field name="keyword"> del.icio.us</field>
<field name="abstract">Collaborative tagging is the aggregate effort by a community of online users to annotate web content with metadata labels called tags. It is a simple activity that enriches our knowledge about digital content, and has gained popularity with services such as Del.icio.us. Del.icio.us has a large repository that evolves daily, presenting interesting new problems for IR.



We present TagScore, a scoring function to rate the goodness of Del.icio.us tags for their associated web page. It gives us a succinct synopsis for a page that we can use to efficiently find similar pages. Using real Del.icio.us data, we show that our approach gives good correlation to cosine similarity but is several hundred times faster and requires minimal storage overhead.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">874</field>
<field name="author">Matthew Gebski</field>
<field name="author">Alex Penev</field>
<field name="author">Raymond Wong</field>
<field name="title">Grouping Categorical Anomalies</field>
<field name="keyword">data mining</field>
<field name="keyword"> anomaly</field>
<field name="keyword"> outlier</field>
<field name="keyword"> categorical</field>
<field name="abstract">We present an approach for discovery of groups of unusual data points that are anomalous for similar reasons. This differs from clustering in that the points that are grouped may be quite `distant' and can use categorical attributes, and differs from anomaly detection in that we are not looking for individual outliers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">875</field>
<field name="author">Dominic Heah</field>
<field name="author">Zainab Zaidi</field>
<field name="title">A testbed implementation of monitoring assisted local route recovery scheme for wireless mesh networks</field>
<field name="abstract">In this paper, we present experimental results from testbed implementation of a simple monitoring assisted local route routing scheme. The scheme exploits the broadcast nature of wireless transmissions at special routers with added monitoring

functionalities. These routers passively listen to the transmissions

in the neighborhood and compare the forwarding behavior against pre-specified thresholds. If forwarded traffic exceeds specified threshold, as a result of a malfunction, e.g., Denial-of-service (DoS) attack or node failure, these routers

try to isolate the node causing the problem by routing the traffic around it. The performance

evaluation of our scheme shows considerable improvement in data delivery rates.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">876</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Brian Mark</field>
<field name="title">Mobility Tracking Based on Autoregressive Models</field>
<field name="keyword">Mobility model</field>
<field name="keyword"> Geolocation</field>
<field name="keyword"> Autoregressive model</field>
<field name="keyword"> Kalman filter</field>
<field name="keyword"> Yule-Walker equations</field>
<field name="abstract">We propose an integrated scheme for tracking the mobility

of a user based on autoregressive

models that accurately capture the

characteristics of realistic user movements in wireless networks.

The mobility parameters are obtained from training data by computing

MMSE (Minimum Mean Square Error) estimates. Estimation of the

mobility state, which consists of the position of the mobile station

is accomplished via an extended Kalman filter using signal

measurements from the wireless network. By combining of mobility parameter

and state estimation in an integrated framework, we obtain an efficient and accurate

real-time mobility tracking scheme that can be applied in a variety

of wireless networking applications. We consider

two variants of an autoregressive mobility model in our study and validate

the proposed mobility tracking scheme using mobile trajectories collected from drive-test data. Our simulation results validate the accuracy of the proposed tracking scheme

even when only a small number of data samples are available for initial training.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">877</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Planning and SAT</field>
<field name="abstract">The planning problem in Artificial Intelligence was the first application of SAT to reasoning about transition systems and a direct precursor to the use of SAT in a number of other applications

including bounded model-checking in computer-aided verification. This chapter presents the main ideas about encoding goal reachability problems as a SAT problem, including parallel plans and different forms of constraints for speeding up SAT solving, as well as algorithms for solving the AI planning problem with a SAT solver. Finally, more general planning problems that require the use of QBF or other generalizations of SAT are briefly discussed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">878</field>
<field name="author">Asia Slowinska</field>
<field name="author">Herbert Bos</field>
<field name="title">Pointless Tainting? Evaluating the Practicality of Pointer Tainting</field>
<field name="keyword">dynamic taint analysis</field>
<field name="keyword"> pointer tainting</field>
<field name="abstract">This paper evaluates pointer tainting, an incarnation of Dynamic

Information Flow Tracking (DIFT), which has recently

become an important technique in system security.

Pointer tainting has been used for two main purposes: detection

of privacy-breaching malware (e.g., trojan keyloggers

obtaining the characters typed by a user), and detection

of memory corruption attacks against non-control data

(e.g., a buffer overflow that modifies a user s privilege level).

In both of these cases the attacker does not modify control

data such as stored branch targets, so the control flow of

the target program does not change. Phrased differently, in

terms of instructions executed, the program behaves normally .

As a result, these attacks are exceedingly difficult to

detect. Pointer tainting is considered one of the onlymethods

for detecting them in unmodified binaries. Unfortunately, almost

all of the incarnations of pointer tainting are flawed.

In particular, we demonstrate that the application of pointer

tainting to the detection of keyloggers and other privacybreaching

malware is problematic. We also discuss whether

pointer tainting is able to reliably detect memory corruption

attacks against non-control data. We found that pointer

tainting generates itself the conditions for false positives.We

analyse the problems in detail and investigate various ways

to improve the technique. Most have serious drawbacks in

that they are either impractical (and incur many false positives

still), and/or cripple the technique s ability to detect

attacks. In conclusion, we argue that depending on architecture

and operating system, pointer tainting may have some value in detecting memory corruption attacks (albeit with

false negatives and not on the popular x86 architecture), but

it is fundamentally not suitable for automated detecting of

privacy-breachingmalware such as keyloggers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">879</field>
<field name="author">Andreas Ernst</field>
<field name="author">Mark Horn</field>
<field name="author">Philip Kilby</field>
<field name="author">Mohan Krishnamoorthy</field>
<field name="title">Dynamic scheduling of recreational rental vehicles with Revenue Management extensions</field>
<field name="keyword">Scheduling</field>
<field name="keyword"> Networks and graphs</field>
<field name="keyword"> Revenue management</field>
<field name="keyword"> Vehicle Routing</field>
<field name="abstract">The Rental Fleet Scheduling Problem (RFSP) arises in vehicle rental operations that

o er a wide variety of vehicle types to customers, and allow a rented vehicle to migrate 

to a setdown depot other than the pickup depot. When there is a shortage of vehicles

of a particular type at a depot, vehicles may be relocated to that depot, or vehicles of

similar types may be substituted. The RFSP involves assigning vehicles to rentals so as

to minimise the costs of these operations, and arises in both static and on-line contexts.

The authors have adapted a well-known assignment algorithm for application in the on-

line context. In addition, a network- ow algorithm with more comprehensive coverage of

problem conditions is used to investigate the determination of rental pricing using Rev-

enue Management principles. The paper concludes with an outline of the algorithms use

in supporting the operations of a large recreational vehicle rental company.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">880</field>
<field name="author">Jun Yang</field>
<field name="author">Yang Wang</field>
<field name="author">Getian Ye</field>
<field name="author">Arcot Sowmya</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Jie Xu</field>
<field name="title">Feature clustering for vehicle detection and tracking in road traffic surveillance</field>
<field name="keyword">MAP estimation</field>
<field name="keyword"> Monte Carlo methods</field>
<field name="keyword"> Clustering methods</field>
<field name="keyword"> Object detection</field>
<field name="keyword"> Tracking</field>
<field name="abstract">In this paper, we formulate the feature clustering problem as a general Bayesian model and solve it using MCMC. The proposed approach exhibits two advantages over existing methods: Bayesian model handles arbitrary objective functions and MCMC guarantees global optimal solution. Our algorithm is validated on real-world traffic video sequences for the task of vehicle detection and tracking, and is shown to outperform the state-of-the-art approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">881</field>
<field name="author">Tommer Leyvand</field>
<field name="author">Daniel Cohen-Or</field>
<field name="author">Gideon Dror</field>
<field name="author">Dani Lischinski</field>
<field name="title">Data-Driven Enhancement of Facial Attractiveness</field>
<field name="keyword">facial attractiveness</field>
<field name="keyword"> machine learning</field>
<field name="keyword"> optimization</field>
<field name="keyword"> warping</field>
<field name="abstract">When human raters are presented with a collection of shapes and 

asked to rank them according to their aesthetic appeal, the results 

often indicate that there is a statistical consensus among the raters. 

Yet it might be dif cult to de ne a succinct set of rules that capture 

the aesthetic preferences of the raters. In this work, we explore 

a data-driven approach to aesthetic enhancement of such shapes. 

Speci cally, we focus on the challenging problem of enhancing the 

aesthetic appeal (or the attractiveness) of human faces in frontal 

photographs (portraits), while maintaining close similarity with the 

original. 

The key component in our approach is an automatic facial attrac- 

tiveness engine trained on datasets of faces with accompanying fa- 

cial attractiveness ratings collected from groups of human raters. 

Given a new face, we extract a set of distances between a variety of 

facial feature locations, which de ne a point in a high-dimensional 

 face space . We then search the face space for a nearby point with 

a higher predicted attractiveness rating. Once such a point is found, 

the corresponding facial distances are embedded in the plane and 

serve as a target to de ne a 2D warp eld which maps the origi- 

nal facial features to their adjusted locations. The effectiveness of 

our technique was experimentally validated by independent rating 

experiments, which indicate that it is indeed capable of increasing 

the facial attractiveness of most portraits that we have experimented 

with.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">882</field>
<field name="author">Gernot Heiser</field>
<field name="title">Many-Core Chips A Case for Virtual Shared Memory</field>
<field name="keyword">operating systems</field>
<field name="keyword"> virtual machines</field>
<field name="keyword"> hypervisors</field>
<field name="keyword"> multicore</field>
<field name="abstract">We make the case for virtual shared memory (VSM) for supporting future many-core chips. VSM is a shared memory abstraction implemented over distributed memory by a hypervisor, providing the operating system direct access to all memory in the system. VSM on a distributed-memory system, such as a many-core chip with local memory associated with each core or small group of cores, provides a non-uniform memory model to the operating system. We argue, based on our experience with a prototype called vNUMA (implemented on a cluster), that this model can perform well for NUMA-aware software. The indirection layer provided by the virtualization provides benefits to hardware manufacturers, as it can absorb certain faults, including faulty nodes and packet losses in the interconnect.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">883</field>
<field name="author">Jie Xu</field>
<field name="author">Getian Ye</field>
<field name="author">Yang Wang</field>
<field name="author">Gunawan Herman</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Jun Yang</field>
<field name="title">Incremental EM for Probabilistic Latent Semantic Analysis on Human Action Recognition</field>
<field name="keyword">PLSA</field>
<field name="keyword"> latent topic model</field>
<field name="keyword"> action recognition</field>
<field name="keyword"> EM</field>
<field name="abstract">Human action recognition is a significant task in automatic understanding systems for video surveillance. Probabilistic Latent Semantic Analysis (PLSA) model has been used to learn and recognize human actions in videos. Specifically, PLSA employs the expectation maximization (EM) algorithm for parameter estimation during the training. The EM algorithm is an iterative estimation scheme that is guaranteed to find a local maximum of the likelihood function. However its convergence usually takes a large number of iterations. For action recognition with large amount of training data, this would result in long training time. This paper presents an incremental version of EM to speed up the training time of PLSA without loss of performance accuracy. The proposed algorithm is tested on two challenging human action datasets. Experimental results demonstrate that the proposed algorithm converges with fewer number of full passes compared with the batch EM algorithm. And the trained PLSA models achieve comparable or better recognition accuracies than those using batch EM training.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">884</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Matthew Hennessy</field>
<field name="author">Carroll Morgan</field>
<field name="title">Testing Finitary Probabilistic Processes (Extended Abstract)</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> CSP</field>
<field name="keyword"> transition systems</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> simulation</field>
<field name="keyword"> failure simulation</field>
<field name="keyword"> modal characterisations</field>
<field name="keyword"> divergence.</field>
<field name="abstract">This paper provides modal- and relational characterisations of may- and

must-testing preorders for recursive CSP processes with divergence,

featuring probabilistic as well as nondeterministic choice. May

testing is characterised in terms of simulation, and must testing in

terms of failure simulation. To this end we develop weak transitions

between probabilistic processes, elaborate their topological properties,

and capture divergence in terms of partial distributions.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">885</field>
<field name="author">Liang Wang</field>
<field name="title">AUTOMATIC SPOKEN LANGUAGE IDENTIFICATION</field>
<field name="keyword">Language identification</field>
<field name="keyword"> phoneme recognition</field>
<field name="keyword"> language modeling</field>
<field name="keyword"> Kohonen-map pre-classification</field>
<field name="keyword"> likelihood score fusion</field>
<field name="keyword"> phase spectrum</field>
<field name="keyword"> Hilbert-Huang transform</field>
<field name="abstract">Automatic spoken language identification (LID) refers to the task of identifying the language spoken in a particular utterance by an unknown speaker. This thesis presents several novel techniques to improve the robustness of the language identification system, especially while handling a large number of target languages. 

The parallel phoneme recognition followed by language modeling (PPRLM) architecture represents one of the most successful approaches to the LID task, hence the PPRLM is used as the baseline LID system in this thesis. Two schemes are proposed to improve the performance of the baseline system, namely the phoneme inventories modification and the discounting method for language modeling. 

A novel prosodic feature based tonal and non-tonal language classification system is described. The tonal and non-tonal language classification system can also serve as the pre-classification for the PPRLM LID system, to maintain the robustness of the LID system while handling a large number of target languages. 

A novel use of the Kohonen self-organizing feature map (KSFM) in the LID task is also proposed. The performance of KSFM, multi-layer KSFM (MLKSFM) and the hierarchical MLKSFM are examined. With a novel segment-based, normalized feature vector and a properly selected competitive layer topology, the multi-layer KSFM shows a promising LID accuracy of 82.7% compared to the baseline system of 79.2% on an 11-language task. Due to the instability of the KSFM based system when the classification is performed with a large number of classes, the MLKSFM is employed as the pre-classification for the LID task. For the PPRLM with MLKSFM pre-classification, a better LID accuracy of 75.4% is obtained compared to the PPRLM with the tonal and non-tonal language pre-classification of 72.5% on a 16-language task. 

In further experiments for improving the robustness of the LID system, the phase spectrum features the modified group delay features (MODGDF), and the Hilbert-Huang Transform (HHT) based features are examined. The parallel broad phone class recognition followed by language modeling (PBPRLM) is investigated in order to remove the bias caused by the inequality for the sizes of the phoneme inventories across different languages. The Gaussian Mixture Model based fusion technique is proposed to obtain a more robust likelihood score. 

With the use of all the novel techniques proposed in this thesis, an Equal Error Rate (EER) of 6.6% on the 2005 NIST LRE and a LID identification rate of 84.3% on a 22-language task can be achieved.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">886</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Dynamic Posting of Static Symmetry Breaking Constraints</field>
<field name="abstract">We have proposed a method for dynamically post-

ing symmetry breaking constraints to eliminate

value interchangeability [1]. We now extend this

method to work with any type of symmetry break-

ing constraint for any type of symmetry. We prove

that this method is correct in general. That is, it

only eliminates symmetric solutions. We also iden-

tify some simple conditions when it eliminates all

symmetric solutions. We illustrate the method with

a common type of symmetry where both variables

and values partition into interchangeable sets, and

a polynomial number of symmetry breaking con-

straints breaks an exponential number of symme-

tries. This hybrid approach inherits good proper-

ties of both dynamic and static symmetry breaking

methods: we can have fast and efficient propaga-

tion on the posted symmetry breaking constraints,

yet we do not conflict with the branching heuristic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">887</field>
<field name="author">Liming Zhu</field>
<field name="author">Jenny Liu</field>
<field name="title">Model Driven Development with Non-Functional Aspects</field>
<field name="abstract">Model Driven Development (MDD) refers to the systematic use of models as primary engineering artefacts throughout a software development life cycle. In recently years, MDD has been increasingly employed to guide development with a focus on system modelling, code generation from models and white-box analysis of models. However, compositional system analysis regarding early Non-Functional Aspects/Properties (NFP) remains difficult. In this paper, we critically review the state-of-the-art of MDD in the context of non-functional aspects and answer the following two questions:

1)How to model Non-Functional Aspect/Property (NFP). The focus is to understand the different sub-types of a non-functional aspects and its compositional and emergent nature.

2) How models can be used for analysing Non-functional Aspect/Property(NFP). This focuses on the analysis models in the form of reasoning frameworks (both qualitative and quantitative) behind each non-functional aspect.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">888</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Tan Tien</field>
<field name="author">Cheng Yunlei</field>
<field name="title">ETX could result in lower throughput</field>
<field name="keyword">Expected Transmission Count (ETX)</field>
<field name="keyword"> Wireless mesh networks</field>
<field name="keyword"> routing</field>
<field name="abstract">ETX (Expected Transmission Count) is the best routing metric for single radio wireless mesh network (WMN) in terms of throughput as it selects the links with highest packet delivery paths. In our experiments, however, we found that under some scenarios which commonly occur in normal practice, e.g., when data rates approach channel capacity, ETX could yield lower throughput than other metrics, such as, minimum hop-count or RTT (Round Trip Time). The paper presents the experimental evidence of performance anomaly in ETX, identifies the operating conditions when anomaly occurs, and discusses the results of our investigation regarding potential causes. %, such as, probing overhead, load-sensitivity, etc., using simulations and testbed experiments. Results show that probing, though incurs small overhead, plays a major role in performance degradation of ETX. When node buffers are full because of higher data rate, prioritized control packets, including probes, push data packets outside the queue. When alternative paths are not very much different to each other, ETX appears worse than hop-count because of this additional buffer overflow loss. Although, this loss becomes insignificant when penalty of selecting a lossy path is higher. We also observed that ETX exhibits route oscillations due to load-sensitivity but strong correlation of its load-sensitivity with degraded performance is not found.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">889</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Combining Symmetry Breaking and Global Constraints</field>
<field name="keyword">global constraints</field>
<field name="keyword"> symmetry breaking</field>
<field name="abstract">We propose a new family of constraints

which combine together lexicographical ordering

constraints for symmetry breaking with 

other common global constraints. We give

a general purpose propagator for this family

of constraints, and show how to improve its 

complexity by exploiting properties of the included

global constraints.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">890</field>
<field name="author">Jean Christoph Jung</field>
<field name="author">Pedro Barahona</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Toby Walsh</field>
<field name="title">Two Encodings of DNNF Theories</field>
<field name="abstract">The paper presents two new compilation schemes of De-

composable Negation Normal Form (DNNF) theories into Conjunc-

tive Normal Form (CNF) and Linear Integer Programming (MIP), re-

spectively. We prove that the encodings have useful properties such

as unit propagation on the CNF formula achieves domain consistency

on the DNNF theory. The approach is evaluated empirically on ran-

dom as well as real-world CSP-problems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">891</field>
<field name="author">Jeff Li</field>
<field name="title">Cognitive and Integrated Digital Home via Dynamic Media Access</field>
<field name="keyword">Transmission media</field>
<field name="keyword"> signal processing</field>
<field name="keyword"> cognitive radio</field>
<field name="keyword"> software-defined radio (SDR)</field>
<field name="keyword"> MAC</field>
<field name="keyword"> PHY</field>
<field name="keyword"> home networking</field>
<field name="abstract">This paper proposes a cognitive and integrated communication system that can support any combination of transmission media (TM) available in a future digital home. To avoid the

overhead with frequent hand-over in relation to authentication, association and synchronization, the proposed Integrated and Scalable Digital Baseband Processor (ISDBP) treats

different TM as the physical resource of a single system, enabling dynamic media access (DMA) in addition to dynamic spectrum access (DSA). To allow the re-use of existing

application software and the Internet Protocol (IP) infrastructure, we propose a new architecture and focus the improvements on the sub-systems below the networking layer.



Two new sub-systems have been proposed in the architecture: the cognitive home environment models (CHEM) and the intelligence engine(IE). The CHEM is a real-time model of the complete home environment including all home devices, sensors, their features and status. The IE is a set of algorithms which shall real-time configure the ISDBP based on the information from the CHEM. The IE shall also determine the level of security and quality of services (QoS) to apply. Unlike the conventional baseband processor which is standard or protocol specific, supports a specific transmission medium and has relatively fixed throughput and QoS, the proposed baseband processor will be integrated for any combination of TM including wireless in different frequency bands, e.g. 2.4/5/60 GHz and phoneline/coax/power/fibre cables. The throughput of the base-band processor shall also be scalable from 10 Kbps for low-cost sensors to 10 Gbps for multiple streams of uncompressed HD-video contents.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">892</field>
<field name="author">Maria Pini</field>
<field name="author">Francesca Rossi</field>
<field name="author">Brent Venable</field>
<field name="author">Toby Walsh</field>
<field name="title">Manipulation and gender neutrality in stable marriage procedures</field>
<field name="keyword">Social choice</field>
<field name="keyword"> stable marriage</field>
<field name="abstract">The stable marriage problem is a well-known problem

of matching men to women so that no man and woman who

are not married to each other both prefer each other. 

Such a problem has a wide variety of

practical applications ranging, from matching resident doctors to

hospitals, to matching students to schools.

A well-known algorithm to solve this problem is the

Gale-Shapley algorithm, which runs in polynomial time.



It has been proven that stable marriage procedures

can always be manipulated. While the Gale-Shapley

algorithm is easy to manipulate,

we prove that there exist stable marriage

procedures which are NP-hard to manipulate.

We also consider the relationship between voting theory and stable marriage

procedures, showing that voting rules which are NP-hard to manipulate can be

used to define stable marriage procedures which are themselves NP-hard

to manipulate. Finally, we consider the issue that stable marriage

procedures like Gale-Shapley favor one gender over the other,

and we show how to use voting and scoring rules to make any stable marriage

procedure gender neutral.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">893</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="title">On managing business processes variants</field>
<field name="keyword">business process modelling</field>
<field name="keyword"> constraints</field>
<field name="abstract">Variance in business process execution can be the result of several situations, such as disconnection between documented models and business operations, workarounds in spite of process execution engines, dynamic change and exception handling, flexible and ad-hoc requirements, and collaborative and/or knowledge intensive work. It is imperative that effective support for managing process variances be extended to organizations mature in their BPM (Business Process Management) uptake so that they can ensure organization wide consistency, promote reuse and capitalize on their BPM investments. This paper presents an approach for managing business processes that is conducive to dynamic change and the need for flexibility in execution. The approach is based on the notion of process constraints. It further provides a technique for effective utilization of the adaptations manifested in process variants. In particular, we will present a facility for discovery of preferred variants through effective search and retrieval based on the notion of process similarity, where multiple aspects of the process variants are compared according to specific query requirements. The advantage of this approach is the ability to provide a quantitative measure for the similarity between process variants, which further facilitates various BPM activities such as process reuse, analysis and discovery.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">894</field>
<field name="author">Ruopeng Lu</field>
<field name="author">Shazia Sadiq</field>
<field name="author">Guido Governatori</field>
<field name="author">Yang Xiaoping</field>
<field name="title">Defining Adaptation Constraints for Business Process Variant</field>
<field name="keyword">business process modelling</field>
<field name="keyword"> process variants management</field>
<field name="keyword">constraint based BPM</field>
<field name="abstract">In current dynamic business environment, it has been argued that certain characteristics of ad-hocism in business processes are desirable. Such business processes typically have a very large number of instances, where design decisions for each process instance may be made at runtime. In these cases, predictability and repetitiveness cannot be counted upon, as the complete process knowledge used to define the process model only becomes available at the time after a specific process instance has been instantiated. The basic premise is that for a class of business processes it is possible to specify a small number of essential constraints at design time, but allow for a large number of execution possibilities at runtime. The objective of this paper is to conceptualise a set of constraints for process adaptation at instance level. Based on a comprehensive modelling framework, business requirements can be transformed to a set of minimal constraints, and the support for specification of process constraints and techniques to ensure constraint quality are developed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">895</field>
<field name="author">John Thornton</field>
<field name="author">Duc Nghia Pham</field>
<field name="title">Using Cost Distributions to Guide Weight Decay in Local Search for SAT</field>
<field name="keyword">local search</field>
<field name="keyword"> parameter adapting</field>
<field name="abstract">Although clause weighting local search algorithms have produced some of the best results on a range of challenging satisfiability (SAT) benchmarks, this performance is dependent on the careful hand-tuning of sensitive parameters. When such hand-tuning is not possible, clause weighting algorithms are generally outperformed by self-tuning WalkSAT-based algorithms such as AdaptNovelty$^+$ and AdaptG$^2$WSAT.



In this paper we investigate tuning the weight decay parameter of two clause weighting algorithms using the statistical properties of cost distributions that are dynamically accumulated as the search progresses. This method selects a parameter setting both according to the speed of descent in the cost space and according to the shape of the accumulated cost distribution, where we take the shape to be a predictor of future performance. In a wide ranging empirical study we show that this automated approach to parameter tuning can outperform the default settings for two state-of-the-art algorithms that employ clause weighting (PAWS and gNovelty$^+$). We also show that these self-tuning algorithms are competitive with three of the best-known self-tuning SAT local search techniques: RSAPS, AdaptNovelty$^+$ and AdaptG$^2$WSAT.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">896</field>
<field name="author">James Cheney</field>
<field name="author">Ren Vestergaard</field>
<field name="author">Michael Norrish</field>
<field name="title">Formalizing adequacy</field>
<field name="abstract">Adequacy is an important criterion for judging the correctness of formal reasoning. The issue is particularly subtle in the expansive case of approaches to languages with name-binding. We posit that adequacy of a novel representation technique is best addressed by formalizing an isomorphism or, more generally, an interpretation explicating the new approach in terms of a more conventional one. We present an example formalization of an isomorphism relating nominal and higher-order abstract syntax techniques. We also outline steps towards a systematic framework that could be used for proving adequacy results automatically, which we believe would help make representation techniques more transparent to end-users of mechanized metatheory verification systems, and provide insight into the relative merits of different approaches</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">897</field>
<field name="author">Zainab Zaidi</field>
<field name="author">Sara Hakami</field>
<field name="author">Bjorn Landfeldt</field>
<field name="author">Tim Moors</field>
<field name="title">Real-time detection of traffic anomalies in wireless mesh networks (WMN)</field>
<field name="keyword">Anomaly detection</field>
<field name="keyword"> Wireless mesh networks</field>
<field name="keyword"> Principal Component Analysis</field>
<field name="keyword"> Chi-square statistics</field>
<field name="keyword"> Denial-of-service</field>
<field name="keyword"> Port scan</field>
<field name="abstract">Anomaly detection is emerging as a necessary component as wireless networks gain popularity. Anomaly detection has been addressed broadly in wired networks and powerful methods have been developed for correct detection of a variety of known attacks and other anomalies. In this paper, we propose a real-time anomaly detection and identification scheme for wireless mesh networks (WMN) using components from previous methods developed for wired networks. Experiments over a WMN testbed show the effectiveness of the proposed scheme in isolating different types of anomalies, such as Denial-of-service (DoS) attacks, port scan attacks, etc. Our scheme uses Chi-square statistics and it is based on similar ideas as the scheme presented by Lakhina et al., although it has lower computational complexity. The original method by Lakhina et al.\ was developed for wired networks and used Principal Component Analysis (PCA) for reducing the dimensions of observed data and Hotelling's $t^2$ statistics to distinguish between normal and abnormal traffic conditions. However, in our studies we found that dimension reduction is the most computationally intensive process of the scheme. In this paper we propose an alternative way of reducing dimensions using flow variances in a Chi-square test. Experimental results show that the Chi-square test performs similarly well to the PCA-based method at merely a fraction of the computations. Moreover, we propose an automatic identification scheme to pin-point the cause of the detected anomaly and its contribution in terms of additional or lack of traffic. Our results and comparison with other statistical tools show that the Chi-square test and the PCA-based method with identification scheme make powerful tools for real-time detection of various anomalies in an interference prone wireless networking environment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">898</field>
<field name="author">Phu Ngoc Le</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">Improvement of Vietnamese Tone Classification using FM and MFCC Features</field>
<field name="keyword">Tone classification</field>
<field name="keyword"> FM</field>
<field name="keyword"> fusion</field>
<field name="keyword"> Gaussian Mixture Model</field>
<field name="abstract">This paper focuses on tone classification for the Vietnamese speech. Traditionally, tone was classified or recognized by the fundamental frequency F0. However, our experimental results indicate that along with the fundamental frequency, Mel Frequency Cepstrum Coefficients (MFCC) and Frequency Modulation (FM) also carry a significant amount of tone information in the Vietnamese speech. Therefore, the proposed method takes into account these two types of features to improve the classification accuracy. The experimental results show that the proposed classification system provides an improvement of 7.5% in accuracy, compared to the conventional system based on F0 alone.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">899</field>
<field name="author">Chunhua Shen</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Jian Zhang</field>
<field name="title">Efficiently Training A Better Visual Detector With Sparse Eigenvectors</field>
<field name="abstract">Real-time object detection has many applications in video

 surveillance, teleconference and

 multimedia retrieval \etc. Since Viola and Jones

 \cite{Viola2004Robust} proposed the first real-time AdaBoost

 based face detection system, much effort has been spent on

 improving the boosting method.

 

 In this work, we first show that

 feature selection methods other than boosting can also be used

 for training an efficient object detector. In particular, we

 introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)

 \cite{Moghaddam2007Fast} for its conceptual simplicity and 

 computational efficiency; and

 slightly better detection performance is achieved compared with

 \cite{Viola2004Robust}. 

 Moreover, we propose a new technique,

 termed Boosted Greedy Sparse Linear Discriminant Analysis

 (BGSLDA), to efficiently train detection cascade.

 BGSLDA exploits

 the sample re-weighting property of boosting and the

 class-separability criterion of GSLDA. Experiments

 in the domain of highly skewed data distributions, \eg, face

 detection, demonstrates that classifiers trained with the

 proposed BGSLDA outperforms AdaBoost and its variants. This finding

 provides a significant opportunity to argue that AdaBoost and

 similar approaches are not the only methods that can achieve

 high classification results for high dimensional data in 

 object detection.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">900</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Pat Morin</field>
<field name="author">Michiel Smid</field>
<field name="title">Algorithms for Marketing-Mix Optimization</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> algorithms</field>
<field name="abstract">Algorithms for determining quality/cost/price tradeo&#11;ffs in saturated markets are considered.

A product is modeled by d real-valued qualities whose sum determines the unit cost of producing the

product. This leads to the following optimization problem: given a set of n customers, each of whom

has certain minimum quality requirements and a maximum price they are willing to pay, design a new

product and select a price for that product in order to maximize the resulting pro&#12;t.

An O(n log n) time algorithm is given for the case, d = 1, of linear products, and O(n(log n)^{d+1})

time approximation algorithms are given for products with any constant number, d, of qualities.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">901</field>
<field name="author">Andrew Boyton</field>
<field name="title">A Verified Shared Capability Model</field>
<field name="keyword">Shared Capabilities</field>
<field name="keyword"> Interactive Theorem Proving</field>
<field name="abstract">This paper presents a high-level access control model of the seL4 microkernel.

We extend an earlier formalisation by Elkaduwe et al with non-determinism, 

explicit sharing of capability storage, and a delete-operation for entities.

We formally prove that this new model can enforce system-global security policies 

as well as authority confinement.

The motivation for dealing with sharing explicitly in the high-level, abstract

access control model is to simplify the refinement proof towards the seL4 

implementation considerably. To our knowledge this is the first formal, 

machine-checked access control model with explicit sharing of authority.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">902</field>
<field name="author">Gerwin Klein</field>
<field name="author">Ralf Huuck</field>
<field name="author">Bastian Schlich</field>
<field name="title">Operating System Verification</field>
<field name="abstract">This is the editorial for the JAR special issue on Operating System Verification.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">903</field>
<field name="author">Peter Sunehag</field>
<field name="author">Jochen Trumpf</field>
<field name="author">SVN Vishwanathan</field>
<field name="author">Nic Schraudolph</field>
<field name="title">Variable Metric Stochastic Approximation Theory</field>
<field name="keyword">gradient</field>
<field name="keyword"> optimization</field>
<field name="keyword"> online</field>
<field name="keyword"> convex</field>
<field name="abstract">We provide a variable metric stochastic approximation theory. In doing so, we provide a convergence theory

 for a large class of online variable metric methods including the recently introduced online versions of the BFGS algorithm and

 its limited-memory LBFGS variant. We also discuss the implications of our results for learning from

 expert advice.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">904</field>
<field name="author">Peter Carr</field>
<field name="author">Richard Hartley</field>
<field name="title">Solving Multilabel Graph Cut Problems using Multilabel Swap</field>
<field name="abstract">Approximate solutions to labelling problems can be found using binary graph cuts and the alpha-beta swap algorithm. In some specific cases, an exact solution can be computed by constructing a multilabel graph. However, in many practical applications the multilabel graph construction is unfeasible due to its excessively large memory requirements. In this work, we expand the concept of alpha-beta swap to consider larger sets of labels at each iteration, and demonstrate how this approach is able to produce good approximate solutions to problems which can be solved using multilabel graph cuts. The advantages of the multilabel swap algorithm are that it requires significantly less memory than the multilabel graph and is able to produce an approximate solution faster than alpha-beta swap. We demonstrate the merits of our approach by considering the denoising problem. We illustrate how multilabel swap can be used in a recursive fashion to produce a good solution quickly and without requiring excessive amounts of memory.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">905</field>
<field name="author">Gerwin Klein</field>
<field name="author">Philip Derrin</field>
<field name="author">Kevin Elphinstone</field>
<field name="title">Experience Report: seL4 -- Formally Verifying a High-Performance Microkernel</field>
<field name="keyword">seL4</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> Isabelle</field>
<field name="keyword"> formal verification</field>
<field name="abstract">We report on our experience using Haskell as an executable specification

language in the formal verification of the seL4 microkernel. The verification

connects an abstract operational specification in the theorem prover Isabelle/HOL to a

C implementation of the microkernel. We describe how this project differs 

from other efforts, and examine the effect of using Haskell

in a large-scale formal verification. The kernel comprises 8,700 lines 

of C code; the verification more than 150,000 lines of proof script.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">906</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Siddharth Mitra</field>
<field name="author">Mayank Agarwal</field>
<field name="author">Amit Yadav</field>
<field name="author">Derek Eager</field>
<field name="author">Niklas Carlsson</field>
<field name="title">Characterizing Web-based Video Sharing Workloads</field>
<field name="keyword">workload characterization</field>
<field name="keyword"> video sharing</field>
<field name="abstract">A video sharing service allows ``user generated'' video clips to be

uploaded, and users of the service to view, rate, and comment on

uploaded videos. Prior work has focused mostly on the

YouTube video sharing service. While YouTube is arguably the

most popular video sharing service, studying the workload

characteristics of other video sharing services, and identifying

invariant properties as well as significant differences, is an

important step towards building a broader understanding of this type

of service. In this paper, we present and analyze workload data from

four video sharing services. In aggregate, our traces contain meta-data on 1.8 million videos which together acquired more than 6 billion views.

We identify seven key invariants of these workloads, concerning

aspects such as the video popularity distribution, use of social and interactive features, and the uploading of new content.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">907</field>
<field name="author">Niklas Carlsson</field>
<field name="author">Derek Eager</field>
<field name="author">Anirban Mahanti</field>
<field name="title">Peer-assisted On-demand Video Streaming with Selfish Peers</field>
<field name="keyword">BitTorrent-like systems</field>
<field name="keyword"> peer-assisted streaming</field>
<field name="keyword"> tit-for-tat</field>
<field name="abstract">Systems delivering stored video content using a peer-assisted 

approach are able to serve large numbers of concurrent requests by utiliz- 

ing upload bandwidth from their clients to assist in delivery. In systems 

providing download service, BitTorrent-like protocols may be used in 

which tit-for-tat policies provide incentive for clients to contribute up- 

load bandwidth. For on-demand streaming delivery, however, in which 

clients begin playback well before download is complete, all prior pro- 

posed protocols rely on peers at later video play points uploading data 

to peers at earlier play points that do not have data to share in return. 

This paper considers the problem of devising peer-assisted protocols for 

streaming systems that, similar to download systems, provide e ective 

 tit-for-tat incentives for clients to contribute upload bandwidth. We 

propose policies that provide such incentives, while also providing short 

start-up delays, and delivery of (almost) all video frames by their respec- 

tive playback deadlines</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">908</field>
<field name="author">Daniel Harabor</field>
<field name="title">Clearance-based Pathfinding and Hierarchical Annotated A* Search</field>
<field name="keyword"/>
<field name="abstract">Real-time strategy (RTS) games often feature a wide number unit types for the player to control. One of my favourite titles from the past, Westwood's seminal Red Alert, had many classes of differently sized units: small infantry soldiers, medium-sized Jeeps and large tanks. In Red Alert 3, the most recent incarnation of the series, the diversity is increased even further by the introduction of units with terrain-specific movement capabilities. From a pathfinding perspective this introduces an interesting question: how can we efficiently search for valid routes for variable-sized agents in rich environments with many types of terrain?



Hierarchical Annotated A* (HAA*) is a path planner which is able to efficiently address this problem by first analysing the terrain of a game map and then building a much smaller approximate representation that captures the essential topographical features of the original.



In this article I want to outline the two major aspects of HAA*. First, I ll discuss how one can analyse a grid map to automatically extract clearance-related topographical information. Second, I ll explain how HAA* is able to use this information to build space-efficient abstractions that allow a range of agents with different sizes and terrain traversal capabilities to very quickly find a high quality path through a static multi-terrain environment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">909</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Uwe Waldmann</field>
<field name="title">Superposition and Model Evolution Combined</field>
<field name="keyword">first</field>
<field name="keyword">order theorem proving</field>
<field name="abstract">We present a new calculus for first-order theorem proving with

 equality, \MESUP, which generalizes both the Superposition calculus

 and the Model Evolution calculus (with equality) by integrating

 their inference rules and redundancy criteria in a non-trivial way. The main motivation is to

 combine the advantageous features of both---rather

 complementary---calculi in a single framework. For instance, Model

 Evolution, as a lifted version of the propositional DPLL procedure,

 contributes a non-ground splitting rule that effectively allows now

 to split a clause into \emph{non} variable disjoint subclauses.

 In the paper we present the calculus in detail. Our main result is

 its completeness under semantically justified redundancy criteria and

 simplification rules.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">910</field>
<field name="author">Matthew Robards</field>
<field name="author">Peter Sunehag</field>
<field name="title">Semi-Markov kMeans Clustering and Activity Recognition From Body-Worn Sensors</field>
<field name="keyword">clustering</field>
<field name="keyword"> semi-Markov</field>
<field name="keyword"> time series</field>
<field name="keyword"> subsequence</field>
<field name="keyword"> ECG</field>
<field name="keyword"> Activity Recognition</field>
<field name="abstract">Subsequence clustering aims to find patterns that appear repeatedly in time series data.

We introduce a novel subsequence clustering technique that we call semi-Markov kmeans clustering.

Furthermore, arising as a step in this method is a novel algorithm, here called the semi-Markov nearest centroid method, for labeled segmentation of time series. The clustering results

in ideal examples of the repeating patterns, which can be used as parameters for the semi-Markov nearest centroid method. We are applying the new clustering technique to activity recognition from body-worn

sensors by showing how it can enable a system to learn from data that is only annotated by an ordered

list of activity types that has been undertaken. This kind of annotation, unlike a detailed segmentation of

the sensor data, is easily provided by a non-expert user.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">911</field>
<field name="author">David Smith</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Dynamic channel measurements around 400MHz for Body Area Networks</field>
<field name="abstract">We present details of channel measurements around 400MHz for on-body to on-body radio communication using human subjects. The results extend previous works IEEE 802.15-08-354-01-0006</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">912</field>
<field name="author">David Smith</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Power delay profiles for dynamic narrowband body area network channels</field>
<field name="abstract">Power delay profile analysis for dynamic BAN channel measurements around 2400MHz, 900MHz and 400MHz</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">913</field>
<field name="author">Laurianne Sitbon</field>
<field name="title">Towards enhanced users semantic profiles</field>
<field name="abstract">Search has a lot to do with what it is that is actually being searched. Traditionally, this has been documents, but our notion of a document is blurring. People are also interested today in connecting with people through social networks or discovering new information in their area of 

interest through blogs. Recommender systems respond to this new challenge by providing various topic profiles of the user. However we strongly believe that sentiment analysis has a key role to play in this type of recommendation since the information here doesn t deal with facts only but about opinions. More generally the next search generation we envision here goes beyond the factual aspect of information to reach more user generated content.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">914</field>
<field name="author">Laurianne Sitbon</field>
<field name="title">Next Generation Search for All</field>
<field name="abstract">As the digital information takes more space in everyday life, users suffering from language impairments are left in a gap. It is essential 

that search systems take into account such impairments to help users. Language impairments vary so much from one user to another that individual profiles need to be built in order to deal with each of them individually. By collaborating with cognitive sciences only the way to build and use such profiles are discovered. A use case of an adaptation of the whole information retrieval process for dyslexic users will be presented, where we deal the management of inputs from the users as well as the selection and presentation of search results. The proposed solutions have been built according to the cognitive process involved in writing and reading that affect dyslexic users.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">915</field>
<field name="author">Worapan Kusakunniran</field>
<field name="author">Qiang Wu</field>
<field name="author">Jian Zhang</field>
<field name="author">Hongdong Li</field>
<field name="title">Automatic gait recognition using weighted binary pattern on video</field>
<field name="keyword">gait recognition</field>
<field name="keyword"> local binary pattern</field>
<field name="keyword"> human identification</field>
<field name="keyword"> variance</field>
<field name="abstract">Human identification by recognizing the spontaneous gait recorded in real-world setting is always tough and is

a not fully resolved problem in human biometrics research. Several issues contribute to the challenges of this task. They include various poses, different clothes, moderate to large changes of normal walking manner due to carrying diverse goods when walking, and the uncertainty of the environments where the people are walking. To achieve a better gait recognition, this paper proposes a novel method which is based on Weighted Binary Pattern (WBP). WBP first constructs binary pattern from a sequence of aligned silhouettes. Then, adaptive weighting technique is applied to discriminate significances of the bits in gait signatures. Being

compared with most of existing methods in the literatures, this method can better deal with gait frequency, local spatial-temporal human pose features, and global body shape statistics. The proposed method is testified on several well known benchmark databases. The extensive and encouraging experimental results show that the proposed algorithm achieves high accuracy, but with low complexity and computational time.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">916</field>
<field name="author">Trinabh Gupta</field>
<field name="author">Sanchit Garg</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Niklas Carlsson</field>
<field name="author">Martin Arlitt</field>
<field name="title">Characterization of FriendFeed A Web-based Social Aggregation Service</field>
<field name="keyword">Social aggregation</field>
<field name="keyword"> workload characterization</field>
<field name="abstract">It is increasingly common for users to have accounts with

multiple different social networking services. Motivated by

this scenario, social aggregation services have been intro-

duced, which aggregate the information available through

various services. FriendFeed is one such service. Using ve

weeks of activity of more than 100,000 active users, we delve

into questions such as what types of services users aggregate

content from, the relative popularity of services, who follows

the aggregated content feeds, and why.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">917</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Sijun Lu</field>
<field name="author">Jian Zhang</field>
<field name="author">Li Cheng</field>
<field name="author">Li Wang</field>
<field name="title">Event Classification in Video using Human Articulation Approach</field>
<field name="abstract">This paper presents an approach for event recognition in video using the human appearance part-based technique. Different good feature points are tracked in the sequences containing different human actions using optical flow. Different models are built based on the training data to represent the geometric and movement characteristics of each event. For each model, one decomposable triangulated graph will be assigned with nodes representing different parts and events implying the dependency among nodes. Different graph structures are tested to retrieve the most representative graph of each event. Dynamic programming is applied during the searching phase to reduce the computation costs. Experiment results of this technique on KTH dataset show that this technique works quite well even with simple normalized geometric statistics features.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">918</field>
<field name="author">Alex Smola</field>
<field name="author">Le Song</field>
<field name="author">Choon Hui Teo</field>
<field name="title">Relative Novelty Detection</field>
<field name="keyword"/>
<field name="abstract">Novelty detection is an important tool for unsupervised data

analysis. It relies on finding regions of low density within which

events are then flagged as novel. By design this is dependent on the

underlying measure of the space. In this paper we derive a

formulation which is able to address this problem by allowing for a

reference measure to be given in the form of a sample from an

alternate distribution. We show that this optimization problem can

be solved efficiently and that it works well in practice.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">919</field>
<field name="author">Hongdong Li</field>
<field name="title">Efficient Reduction of L-infinity Geometry Problems</field>
<field name="abstract">A new and fast algorithm for performing L-infinity optimization for multiview 3d reconstruction.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">920</field>
<field name="author">Simon Winwood</field>
<field name="author">Gerwin Klein</field>
<field name="author">Thomas Sewell</field>
<field name="author">June Andronick</field>
<field name="author">David Cock</field>
<field name="author">Michael Norrish</field>
<field name="title">Mind the Gap: A Verification Framework for Low-Level C</field>
<field name="keyword">seL4</field>
<field name="keyword"> Isabelle</field>
<field name="keyword"> OS verification</field>
<field name="keyword"> C</field>
<field name="abstract">This paper presents the formal Isabelle/HOL framework we use to

prove refinement between an executable, monadic specification and

the C implementation of the seL4 microkernel. We describe the

refinement framework itself, the automated tactics it supports, and

the connection to our previous C verification framework.

We also report on our experience in applying the framework

to seL4. The characteristics of this microkernel

verification are the size of the target (8,700 lines of C code),

the treatment of low-level programming constructs, the

focus on high performance, and the large subset of the C programming

language addressed, which includes pointer

arithmetic and type-unsafe code.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">921</field>
<field name="author">Pattaraporn Khuwuthyakorn</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">An affine Invariant Hyperspectral Texture Descriptor Based Upon Heavy-tailed Distributions and Fourier Analysis</field>
<field name="keyword">Hyperspectral Texture Recognition</field>
<field name="keyword"> Fourier Analysis</field>
<field name="keyword"> Statistical Methods and Learning Applications</field>
<field name="keyword"> Features and Image Descriptors</field>
<field name="abstract">In this paper, we address the problem of recovering a hyperspectral texture descriptor. We do this by viewing the wavelength-indexed bands corresponding to the texture in the image as those arising from a stochastic process whose statistics can be captured making use of the relationships between moment generating functions and Fourier kernels. In this manner, we can interpret the probability distribution of the hyper-spectral texture as a heavy-tailed one which can be rendered invariant to affine geometric transformations on the texture plane making use of the spectral power of its Fourier cosine transform. We do this by recovering

the affine geometric distortion matrices corresponding to the probability density function for the texture under study. This treatment permits the development of a robust descriptor which has a high information compaction property and can capture the space and wavelength correlation for the spectra in the hyperspectral images. We illustrate the utility of our descriptor for purposes of recognition and provide results on real-world datasets. We also compare our results to those yielded by a number of alternatives.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">922</field>
<field name="author">Yin Kia Chiam</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="title">Quality Attribute Techniques Framework</field>
<field name="keyword">Quality Attribute Techniques</field>
<field name="keyword"> Product Quality</field>
<field name="keyword"> Software Process Improvement</field>
<field name="keyword"> Process Tailoring</field>
<field name="abstract">The quality of software is achieved during its development. Development teams use various techniques to investigate, evaluate and control potential quality problems in their systems. These ``Quality Attribute Techniques'' target specific product qualities such as safety or security. This paper proposes a framework to capture important characteristics of these techniques. The framework is intended to support process tailoring, by facilitating the selection of techniques for inclusion into process models that target specific product qualities. We use risk management as a theory to accommodate techniques for many product qualities and lifecycle phases. Safety techniques have motivated the framework, and safety and performance techniques have been used to evaluate the framework. The evaluation demonstrates the ability of quality risk management to cover the development lifecycle and to accommodate two different product qualities. We identify advantages and limitations of the framework, and discuss future research on the framework.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">923</field>
<field name="author">Leonid Ryzhyk</field>
<field name="author">Peter Chubb</field>
<field name="author">Ihor Kuz</field>
<field name="author">Etienne Le Sueur</field>
<field name="author">Gernot Heiser</field>
<field name="title">Automatic Device Driver Synthesis with Termite</field>
<field name="keyword">device drivers</field>
<field name="keyword"> software synthsis</field>
<field name="keyword"> reliability</field>
<field name="abstract">Faulty device drivers cause significant damage through 

 downtime and data loss. The problem can be mitigated by an 

 improved driver development process that guarantees 

 correctness by construction. We achieve this by synthesizing 

 drivers automatically from formal specifications of device 

 interfaces, thus reducing the impact

 of human error on driver reliability and also cutting down on

 development costs.



 We present a concrete driver synthesis approach and tool 

 called Termite. We discuss the methodology, the technical and 

 practical limitations of driver synthesis, and provide an 

 evaluation of non-trivial drivers for Linux, generated using 

 our tool. We show that the performance of the generated 

 drivers is on a par with the equivalent manually developed 

 drivers. Furthermore, we demonstrate that device 

 specifications can be reused across different operating 

 systems by generating a driver for FreeBSD from the same 

 specification used for Linux.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">924</field>
<field name="author">Kostia Robert</field>
<field name="title">Night-Time Traffic Surveillance - A Robust Framework for Multi-Vehicle Detection, Classification and Tracking</field>
<field name="abstract">Traffic data extraction is an increasing demand for applications such as traffic lights control, population evacuation, or to reduce traffic issues including congestion, pollution, delays, and accidents. We present in this paper a new framework to reliably detect, classify and track multiple vehicles at night-time. The system shows excellent performance after an evaluation procedure involving many cameras and different conditions. The vehicle detection consists of detecting its two headlights. To avoid false positives and make the detector reliable, a second stage seeks clues of vehicle's presence through a decision tree composed of feature-based and appearance-based classifiers. Finally, the vehicles are tracked over frames. A Kalman filter is associated with a reasoning module. The tracker is designated to be fast, stable, as well as dealing safely with partial and total occlusions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">925</field>
<field name="author">Rafal Kolanski</field>
<field name="author">Gerwin Klein</field>
<field name="title">Types, Maps and Separation Logic</field>
<field name="keyword">Isabelle/HOL</field>
<field name="keyword"> Virtual Memory</field>
<field name="keyword"> seL4</field>
<field name="abstract">This paper presents a separation-logic framework for reasoning about

low-level C code in the presence of virtual memory. We describe our

abstract, generic Isabelle/HOL framework for reasoning about virtual

memory in separation logic, and we instantiate this framework to a

precise, formal model of ARMv6 page tables. The logic supports the

usual separation logic rules, including the frame rule, and extends

separation logic with additional basic predicates for mapping virtual

to physical addresses. We build on earlier work to parse potentially

type-unsafe, system-level C code directly into Isabelle/HOL and

further instantiate the separation logic framework to C.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">926</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">MOBIX: System for managing MOBility using Information eXchange</field>
<field name="abstract">It is evident that mobile devices of the future will have multiple wireless interfaces. For small, energy-constrained devices, determining network availability by keeping all radio interfaces turned on at all times will negatively impact battery lifetime even when these interfaces are idle. Predicting future network availability from user history requires a period of training and learning user habits. This method will fail when users deviate from their routines constantly or move to locations not visited before. 



We propose a different approach to determining network availability of mobile nodes which leverages on the fact that nodes on the move will meet other nodes who will be able to share conditions of networks they have recently encountered. This paper presents MOBIX, a system where nodes exchange information about network conditions using short-range communication such as Bluetooth. Our simulation results show that the required number of nodes needed for 100\% success is not unrealistic of densely populated metropolitan areas. Even with relatively low population densities, we can expect a data store hit more than 50\% of the time. Although our evaluation used WiFi, our scheme can easily be extended for other technologies such as GSM and WiMax.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">927</field>
<field name="author">Gerwin Klein</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Gernot Heiser</field>
<field name="author">June Andronick</field>
<field name="author">David Cock</field>
<field name="author">Philip Derrin</field>
<field name="author">Dhammika Elkaduwe</field>
<field name="author">Kai Engelhardt</field>
<field name="author">Rafal Kolanski</field>
<field name="author">Michael Norrish</field>
<field name="author">Thomas Sewell</field>
<field name="author">Harvey Tuch</field>
<field name="author">Simon Winwood</field>
<field name="title">seL4: Formal Verification of an OS Kernel</field>
<field name="keyword">seL4</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> Isabelle</field>
<field name="keyword"> formal verification</field>
<field name="abstract">Complete formal verification is the only known way to guarantee that a system is free of programming errors.

We present our experience in performing the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, and hardware, and we used a unique design approach that fuses formal and operating systems techniques. To our knowledge, this is the first formal proof of functional correctness of a complete, general-purpose operating-system kernel. Functional correctness means here that the implementation always strictly follows our high-level abstract specification of kernel behaviour. This encompasses traditional design and implementation safety properties such as the kernel will never crash, and it will never perform an unsafe operation. It also proves much more: we can predict precisely how the kernel will behave in every possible situation.

seL4, a third-generation microkernel of L4 provenance, comprises 8,700 lines of C code and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">928</field>
<field name="author">Thanes Wassantachat</field>
<field name="author">Zhidong Li</field>
<field name="author">Jing Chen</field>
<field name="author">Yang Wang</field>
<field name="author">Evan (Leishen) Tan</field>
<field name="title">Traffic Density Estimation with On-line SVM Classifier</field>
<field name="keyword">Online SVM</field>
<field name="keyword"> background Modelling</field>
<field name="keyword"> Loop Detector</field>
<field name="keyword"> Traffic Density Estimation</field>
<field name="abstract">Determining the vehicular traffic density in an intelligent transport system (ITS) is presently attained mainly through loop detectors (LD), traffic radars and surveillance cameras. However, the difficulties and cost of installing loop detectors and traffic radars tend to be significant. Currently, a more advance method of circumventing this is to develop a sort of virtual loop detector (VLD) by using video content understanding technology [2] to simulate behavior of a loop detector and to further estimate the traffic flow from a surveillance camera. However, such a virtual loop detector requires supervised training with certain human-aid efforts in its setup. The difficulties also arise when attempting to obtain a reliable and real-time VLD under changing illumination, challenging weather conditions and static shadows. In this paper, we study the effectiveness of using texture feature in describing the traffic density, and propose a real-time VLD by using an on-line SVM classifier together with background modeling technique (OSVM-BG) to estimate the traffic density states probabilistically and automatically. The system uses the feedback from the background modeling to training and updating its SVM kernel so as to adapt itself to the difficult lighting environment. From the testing we find that the system outperforms several existing algorithms and achieves an average accuracy at around 90% under various illumination changes, weather conditions and especially changing static shadows and in daytime.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">929</field>
<field name="author">Guido Governatori</field>
<field name="author">Renato Iannella</field>
<field name="title">Modelling and Reasoning Languages for Social Networks Policies</field>
<field name="keyword">social networks</field>
<field name="keyword"> open digital right language</field>
<field name="keyword"> policy languages</field>
<field name="abstract">Policy languages (such as privacy and rights) have had little impact on the wider community. Now that Social Networks have taken off, the need to revisit Policy languages and realign them towards Social Networks requirements has become more apparent. One such language is explored as to its applicability to the Social Networks masses. We also propose a computationally oriented model to represent, reason with and execute policies for Social Networks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">930</field>
<field name="author">Fawad Nazir</field>
<field name="author">Victoria Uren</field>
<field name="author">Andriy Nikolov</field>
<field name="title">Algorithms for Generating Ontology Based Visualization from Semantic Search Results</field>
<field name="keyword">Semantic Search</field>
<field name="keyword"> Semantic Search Visualization</field>
<field name="keyword"> Information Visualization</field>
<field name="abstract">The paper augments the results achieved in the

research of Semantic Web technologies for semantic search

application, underlying the importance of integrating information

visualization into semantic search engines (i.e. SemSearch). We

present the results and methodology of our preliminary implementation

of the semantic search visualization engine (SSViz).

SSViz, takes as input the semantic structure of SemSearch

results and reproduces them into a summarized, conclusive

and meaningful graphical representation. Depending on the

semantic search results our algorithms select the best visual

representations. The current version of SSViz supports visual

formats of directed graphs, charts and tables. We implemented

SSViz in java, using yWorks visualization API s, JFreeChart API

for charts, Sesame RDF data store and Sparql query language.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">931</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Fang Chen</field>
<field name="author">Sharon Oviatt</field>
<field name="title">Multimodal Input</field>
<field name="keyword">multimodal interfaces</field>
<field name="keyword"> cognitive load measurement</field>
<field name="keyword"> multimodal interaction</field>
<field name="abstract">Multimodal interfaces involve user input through two or more combined modes,

such as speech, pen, touch, manual gestures, gaze, and head and body movements.

They represent a new direction for computer interfaces, and a paradigm

shift away from conventional graphical interfaces because they involve recognitionbased

technologies designed to handle continuous and simultaneous input from

parallel incoming streams, processing of input uncertainty using probabilistic

methods, distributed processing, and time-sensitive architectures. This new

class of interfaces aims to recognize naturally occurring forms of human language,

which will be discussed in this chapter. The advent of multimodal interfaces

based on recognition of human speech, gaze, gesture, and other natural

behavior represents only the beginning of a progression toward computational

interfaces capable of relatively human-like sensory perception.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">932</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Jyrki Katajainen</field>
<field name="author">Damian Merrick</field>
<field name="author">Cahya Ong</field>
<field name="author">Thomas Wolle</field>
<field name="title">Compressing Spatio-Temporal Trajectories</field>
<field name="keyword">trajectory</field>
<field name="keyword"> compression</field>
<field name="keyword"> Douglas-Peucker</field>
<field name="keyword"> path simplification</field>
<field name="abstract">A trajectory is a sequence of locations, each associated with a timestamp, describing the movement of a point. Trajectory data is becoming increasingly available and the size of recorded trajectories is getting larger. In this paper we study the problem of compressing planar trajectories such that the most common spatio-temporal queries can still be answered approximately after the compression has taken place. In the process, we develop an implementation of the Douglas-Peucker path-simplification algorithm which works efficiently even in the case where the polygonal path given as input is allowed to self-intersect. For a polygonal path of size $n$, the processing time is $O(n \log^k n)$ for $k = 2$ or $k = 3$ depending on the type of simplification.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">933</field>
<field name="author">Bipin Gopalakrishna Pillai</field>
<field name="author">Thas Nirmalathas</field>
<field name="title">OSNR And Chromatic Dispersion Monitoring Using Wiener-Hopf Equation</field>
<field name="keyword">Optical signal-to-noise ratio</field>
<field name="keyword"> signal monitoring</field>
<field name="keyword"> chromatic dispersion</field>
<field name="keyword"> Wiener-Hopf equation</field>
<field name="abstract">We perform simultaneous monitoring of OSNR and residual chromatic dispersion by solving the Wiener-Hopf equation using matrix inversion. The effectiveness of this method over a wide range of received optical power is investigated using simulations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">934</field>
<field name="author">Thava Iyer</field>
<field name="author">Roksana Boreli</field>
<field name="author">Golam Sarwar</field>
<field name="author">Christoph Dwertmann</field>
<field name="title">DART: Enhancing Data Acceleration with Compression for Satellite Links</field>
<field name="keyword">transport protocol</field>
<field name="keyword">tcp</field>
<field name="keyword">rate based</field>
<field name="abstract">We present a new rate based transport protocol, DART, which has been implemented within a 7-ip satellite router product. DART is based on the Space Communications Protocol Suite Transport Protocol (SCPS-TP) framework and and includes integrated lossless data compression. We evaluate the performance of this protocol on two live commercially available satellite systems, IPSTAR and Inmarsat BGAN. We compare the experimental performance of the new congestion control with a number of TCP flavors previously proposed for use on satellite links: standard TCP with SACK option, SCPS-TP, TCP Vegas and TCP Hybla. Our performance measure is the goodput defined as the effective application rate, which takes into account the overheads introduced by the protocols used. We analyse the performance results and show that, in situations where congestion is present on the satellite link, the proposed rate based scheme outperforms the established techniques. We additionally highlight the benefits of compression when fully integrated into the transport protocol. Further, we demonstrate the fairness of the proposed scheme to other flows using the same congestion control scheme and additionally evaluate fairness for standard TCP Reno. When used in a proxy architecture like the satellite router product, the proposed congestion control will significantly increase the use of shared bandwidth by individual data flows. When used with parallel TCP connections, the proposed scheme will react and adjust it's rate to TCP flows in a way which is reasonably fair.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">935</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Sebastien Ardon</field>
<field name="title">Design and Validation of a Reliable Rate Based Transport Protocol: The Chameleon Protocol</field>
<field name="abstract">TFRC protocol has not been designed to enable reliability. Indeed, the birth of TFRC results from the need of a congestion controlled and realtime transport protocol in order to carry multimedia traffic. Historically, and following the anarchical deployment of congestion control mechanisms implemented on top of UDP protocol, the IETF decided to standardize such protocol in order to provide to multimedia applications developers a framework for their applications. In this paper, we propose to design a reliable rate-based transport protocol based on TFRC. This design is motivated by finding an alternative to TCP where its oscillating behaviour is known to be counterproductive over certain networks such as VANET. However, we found unexpected and interesting results compared to TFRC partly inherited from the smooth behavior of TFRC in the context of wired networks. In particular, we show that TFRC can realize shorter data transfer compare to TCP over a complex and realistic topology. We firstly detail and fully benchmark our protocol in order to verify that our resulting prototype inherits from the good properties of TFRC in terms of TCP-friendliness. As a second contribution, we also propose a ns-2 implementation for testing purpose to the networking community. Following these preliminary tests, we drive a set of non-exhaustive experiments to illustrate some interesting behaviour of this protocol in the context of wired networks.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">936</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Patrick Senac</field>
<field name="title">Towards Sender-Based TFRC</field>
<field name="keyword">Transport</field>
<field name="keyword"> TFRC</field>
<field name="keyword"> Protocol Implementation</field>
<field name="abstract">Pervasive communications are increasingly sent over mobile devices and personal digital assistants. This trend is currently observed by mobile phone service providers which have measured a significant increase in multimedia traffic. To better carry multimedia traffic, the IETF standardized a new TCP Friendly Rate Control (TFRC) protocol. However, the current receiver-based TFRC design is not well suited to resource limited end systems. In this paper, we propose a scheme to shift resource allocation and computation to the sender. This sender-based approach led us to develop a new algorithm for loss notification and loss-rate computation. We detail the complete implementation of a user-level prototype and demonstrate the gain obtained in terms of memory requirements and CPU processing compared to the current design. We also evaluate the performance obtained in terms of stability and fairness with TCP and we note this shifting solves security issues raised by classical TFRC implementations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">937</field>
<field name="author">Renato Iannella</field>
<field name="title">Towards E-Society Policy Interoperability</field>
<field name="keyword">E</field>
<field name="keyword">Society Social Networks Policy</field>
<field name="keyword">Oriented Web Policy Languages Privacy; Rights</field>
<field name="abstract">The move towards the Policy-Oriented Web is destined to provide support for policy expression and management in the core web layers. One of the most promising areas that can drive this new technology adoption is e-Society communities. With so much user-generated content being shared by these social networks, there is the real danger that the implicit sharing rules that communities have developed over time will be lost in translation in the new digital communities. This will lead to a corresponding loss in confidence in e-Society sites. The Policy-Oriented Web attempts to turn the implicit into the explicit with a common framework for policy language interoperability and awareness. This paper reports on the policy driving factors from the Social Networks experiences using real-world use cases and scenarios. In particular, the key functions of policy-awareness - for privacy, rights, and identity - will be the driving force that enables the e-Society to appreciate new interoperable policy regimes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">938</field>
<field name="author">Arif Khan</field>
<field name="author">Yang Wang</field>
<field name="author">Jian Zhang</field>
<field name="title">Appearance-based Re-identification of People in Video</field>
<field name="keyword">Color Context Descriptor for people</field>
<field name="keyword"> Appearance model</field>
<field name="keyword"> Color historgam in spatial distribution</field>
<field name="abstract">This paper introduces the topic of appearance-based re-identification

of people in video. This work is based on color information of people s clothing.

Most of the work described in the literature uses full body histogram. This

paper evaluates the histogram method and describes ways of including spatial

color information. The paper proposes a color-based appearance descriptor

called Color Context People Descriptor. All the methods are evaluated extensively.

The results are reported in the experiments. It is concluded at the end

that adding spatial color information greatly improves the re-identification results.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">939</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Himanshu Gupta</field>
<field name="author">Vinay Ribeiro</field>
<field name="title">Revisiting Coexistence of Poissonity and Self-Similarity in Internet Traffic</field>
<field name="abstract">The immense popularity of new-age ``Web 2'' applications such as

YouTube, Flickr, and Facebook, and non-Web applications such as

Peer-to-Peer (P2P) file sharing, Voice over IP, online games, and

media streaming have significantly altered the composition of Internet

traffic with respect to what it was a few years ago. In light of these

changes, this paper revisits Internet traffic characteristics and

models that were proposed when ``traditional'' Web traffic was the

largest contributor to Internet traffic. Specifically, we are

interested in studying whether or not the following characteristics,

namely : (1) traffic is {\em self similar} and {\em long range}

dependent,

%at all time scales, 

and (2) traffic can be approximated by

{\em Poisson} at smaller time scales, characteristics that were

established during the era of traditional Web traffic, are still

valid. Our experiments on recent traces shows that these traffic

characteristics continue to hold. We further argue that current

Internet traffic can be viewed to have two key constituents, namely

Web+ and P2P+; Web+ traffic consists of traffic from both Web 1.0 and

Web 2.0 applications; P2P+ traffic consists largely of traffic from P2P

applications and other non-Web applications excluding applications on

well-known ports such as FTP and SMTP. We then show that both Web+

and P2P+ components exhibit self similar behaviour and can be

approximated by Poisson at smaller time scales.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">940</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Minimization of the effect of noisy measurements on localization of multi-agent autonomous formations</field>
<field name="abstract">This paper considers the problem of reduction of self-localization errors in multi-agent autonomous

formations when only distance measurements are available to the agents in a globally rigid formation.

It is shown that there is a relationship between different selections of anchors, agents with exactly

known positions, and the error induced by measurement error on localization solution. This fact is

exploited to develop a mechanism to select anchors in order to minimize the effects of inter-agent

distance measurement errors on localization solution. Finally, some simulation results are presented to

demonstrate the optimal anchor selection for a particular general class of formations, the globally rigid

formations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">941</field>
<field name="author">Franck Cassez</field>
<field name="author">Jan J. Jessen</field>
<field name="author">Kim G. Larsen</field>
<field name="author">Jean-Fran _ois Raskin</field>
<field name="author">Pierre-Alain Reynier</field>
<field name="title">Automatic Synthesis of Robust and Optimal Controllers - An Industrial Case Study.</field>
<field name="keyword">timed automata</field>
<field name="keyword"> controller synthesis</field>
<field name="abstract">In this paper, we show how to apply recent tools for the automatic

 synthesis of robust and near-optimal controllers for a real

 industrial case study. We show how to use three different classes of

 models and their supporting existing tools, TiGA for

 synthesis, PHAVer for verification, and Simulink for

 simulation, in a complementary way. We believe that this case study

 shows that our tools have reached a level of maturity that allows us

 to tackle interesting and relevant industrial control problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">942</field>
<field name="author">Franck Cassez</field>
<field name="title">The Dark Side of Timed Opacity.</field>
<field name="keyword">timed automata</field>
<field name="keyword"> security</field>
<field name="keyword"> opacity</field>
<field name="abstract">In this paper we extend the notion of opacity, defined for

 discrete-event systems, to dense-time systems. We define the timed

 opacity problem for timed automata and study its algorithmic status.

 We show that for the very restrictive class of Event Recording Timed

 Automata, the opacity problem is already undecidable leaving no hope

 for an algorithmic solution to the opacity problem in dense-time.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">943</field>
<field name="author">Thanassis Boulis</field>
<field name="author">Yuri Tselishchev</field>
<field name="title">Wireless Sensor Network Tesbed for Structural Health Monitoring of Bridges</field>
<field name="keyword">Wireless Sensor Networks</field>
<field name="keyword"> Structural Health Monitoring</field>
<field name="keyword"> Testbed</field>
<field name="abstract">The Road and Traffic Authority (RTA) of NSW, partnered with NICTA, are looking into wireless sensor

network technologies to monitor the structural health of bridges. The task involves exploring different

sensing, networking, and distributed computation approaches for the specific application of Structural

Health Monitoring (SHM). The exploration has to happen on-site with real world conditions. We argue

that the on-site exploration needs an over-provisioned testbed that will provide the sensing nodes with

power and an alternate wired communication channel. Even though the implementation and deployment

of such a testbed is more expensive than deploying a single standalone WSN, we show that a single

standalone WSN cannot operate throughout the life cycle of experimentation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">944</field>
<field name="author">Jian Zhang</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="title">AN OVERVIEW OF FAST PEDESTRIAN DETECTION: FEATURE SELECTION AND CASCADE FRAMEWORK OF BOOSTED FEATURES</field>
<field name="keyword">Feature selection</field>
<field name="keyword"> boosting</field>
<field name="keyword"> weighted Fisher linear discriminant analysis and Greedy Sparse Linear Discriminant Analysis</field>
<field name="abstract">Efficiently and accurately detecting pedestrians plays a crucial role in many vision applications such as video surveillance, multimedia retrieval and smart car etc. In order to find the right feature for this task, we first present a comprehensive experimental study on pedestrian detection using state-of-the-art locally-extracted features. Building upon our findings, we propose a new, simpler pedestrian detecting framework based on the covariance features. We conduct feature selection and weak classifier training in the Euclidean space for faster computation. To this end, two machine learning algorithms have been designed: AdaBoost with weighted Fisher linear discriminant analysis (WLDA) based weak classifiers and Greedy Sparse Linear Discriminant Analysis (GSLDA). To further accelerate the detection, we employ a faster strategy, multiple cascade layers with heterogeneous features, to exploit the efficiency of the Haar-like features and the discriminative power of the covariance features. Experimental results shown on different datasets prove that the new pedestrian detection is not only comparable to the performance of the state-of-the-art pedestrian detectors but it also performs at a faster speed.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">945</field>
<field name="author">Tahir Mehmood</field>
<field name="author">Lavy Libman</field>
<field name="title">Towards Optimal Forwarding in Wireless Networks: Opportunistic Routing Meets Network Coding</field>
<field name="abstract">In wireless networks, transmissions can generally be overheard by unintended nodes in the vicinity of the sender and receiver, potentially causing interference to their own communications. Much of the related research has considered this as a nuisance and resulted in various proposals for overcoming the interference using scheduling, channel assignment, and many other mechanisms. In recent years, there has been growing attention to methods that aim to take advantage of the broadcast nature of the wireless medium and the ability of nodes to overhear their neighbors' transmissions. Two of the most important such methods are opportunistic routing (OR) and wireless network coding (NC), exemplified by the ExOR and COPE protocols, respectively. In this paper, we study the potential benefits (in terms of reducing transmissions) of forwarding schemes that combine elements from both the OR and NC approaches, when traffic on a bidirectional unicast connection between two nodes is relayed by multiple common neighbors. We present a dynamic programming algorithm to find the optimal scheme as a function of link error probabilities, and demonstrate that it can achieve up to 20% reduction in the number of transmissions compared to either OR or NC employed alone, even in a simple scenario of two common neighbors between the connection endpoints.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">946</field>
<field name="author">Ben Gilbert</field>
<field name="title">Phase Locked Loop Synthesis and Simulation using Matlab</field>
<field name="keyword">Phase Locked Loop</field>
<field name="keyword"> PLL</field>
<field name="keyword"> Synthesizer</field>
<field name="keyword"> Matlab</field>
<field name="abstract">Matlab worksheets for the synthesis and simulation of 2nd, 3rd, and 4th order systems using passive loop filters. The method employed is derived from Dean Banerjee's Book "PLL Performance, Simulation, and Design" 4th Edition.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">947</field>
<field name="author">Ben Gilbert</field>
<field name="title">Phase Locked Loop Synthesis and Simulation using Matlab</field>
<field name="keyword">Phase Locked Loop</field>
<field name="keyword"> PLL</field>
<field name="keyword"> Synthesizer</field>
<field name="keyword"> Matlab</field>
<field name="abstract">Matlab worksheets for the synthesis and simulation of 2nd, 3rd, and 4th order systems using passive loop filters. The method employed is derived from Dean Banerjee's Book "PLL Performance, Simulation, and Design" 4th Edition.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">948</field>
<field name="author">Ko-Hsin Cindy Wang</field>
<field name="author">Adi Botea</field>
<field name="title">Tractable Multi-Agent Path Planning on Grid Maps</field>
<field name="keyword">Multiagent Planning</field>
<field name="keyword"> Motion and Path Planning</field>
<field name="abstract">Multi-agent path planning on grid maps is a challenging problem and has numerous real-life applications. Running a centralized, systematic search such as A* is complete and cost-optimal but scales up poorly in practice, since both the search space and the branching factor grow exponentially in the number of mobile units. Decentralized approaches, which decompose a problem into several subproblems, can be faster and can work for larger problems. However, existing decentralized methods offer no guarantees with respect to completeness,

running time, and solution quality.



To address such limitations, we introduce MAPP, a tractable algorithm for multi-agent path planning on grid maps. We show that MAPP has low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions. As it runs in low-polynomial time, MAPP is incomplete in the general case. We identify a class of problems for which our algorithm is complete. We believe that this is the first study that formalises restrictions to obtain a tractable class of multi-agent path planning problems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">949</field>
<field name="author">Karina Delgado</field>
<field name="author">Scott Sanner</field>
<field name="author">Leliane Nunes de Barros</field>
<field name="title">Efficient Solutions to Factored MDPs with Imprecise Transition Probabilities</field>
<field name="keyword">MDPs</field>
<field name="keyword"> Model Uncertainty</field>
<field name="abstract">Markov Decision Processes with Imprecise Transition Probabilities

(MDP-IPs) can model a wide range of fully observable

sequential decision making problems when the transition

probabilities are not completely specified. Such specification

of imprecise transition probabilities can model scenarios such

as uncertainty during the elicitation ofMDP transitionmodels

from an expert or data, or non-stationary transition distributions

with known constraints. While solutions to the MDP-IP

are well-known, they do not typically exploit factored structure

in the MDP-IP. To this end, we propose a dynamic programming

algorithm for the (approximately) optimal solution

of MDP-IPs that exploits factored structure using a combination

of credal dynamic Bayesian networks and a novel extension

of Algebraic Decision Diagrams (ADDs) that we call

Parameterized ADDs (PADDs). Noting that the key computational

bottleneck in the solution of MDP-IPs is the need

to repeatedly solve nonlinear constrained optimization problems,

we apply a theoretical analysis to determine when previous

nonlinear programming solutions can be reused without

exceeding a fixed error budget. In this way, we show how

to obtain efficient, bounded approximately optimal solutions

to factored MDP-IPs that are up to an order of magnitude

faster than state-of-the-art flat dynamic programming approaches.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">950</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Unified definition of heuristics for classical planning</field>
<field name="abstract">In many types of planning algorithms distance heuristics play an important role. Most of the earlier works restrict to STRIPS operators, and their application to a more general language with disjunctivity and conditional effects first requires an exponential size reduction to STRIPS operators.

I present direct formalizations of a number of distance heuristics for a general operator description language in a uniform way, avoiding the exponentiality inherent in earlier reductive approaches. The formalizations use formulae to represent the conditions under which operators have given effects. The exponentiality shows up in satisfiability tests with these formulae, but would appear to be a minor issue because of the small size of the formulae.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">951</field>
<field name="author">Jussi Rintanen</field>
<field name="title">Compact representation of sets of binary constraints</field>
<field name="abstract">We address the problem of representing big sets of binary constraints compactly. Binary constraints in the form of 2-literal clauses are ubiquitous in propositional formulae that represent real-world problems ranging from model-checking problems in computer-aided verification to AI planning problems. Current satisfiability and constraint solvers are applicable to very big problems, and in some cases the physical size of the problem representations prevents solving the problems, not their computational difficulty. Our work is motivated by this observation.

We propose graph-theoretic techniques based on cliques and bicliques for compactly representing big sets of binary constraints that have the form of 2-literal clauses. An n,m biclique in a graph associated with the constraints can be very compactly represented with only n+m binary constraints and one auxiliary variable. Cliques in the graph are associated with at-most-one constraints, and can be represented with a logarithmic number of binary constraints. The clique representation turns out to be a special case of the biclique representation. We demonstrate the effectiveness of the biclique representation in making the representation of big planning problems practical.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">952</field>
<field name="author">Jussi Rintanen</field>
<field name="author">Keijo Heljanko</field>
<field name="author">Ilkka Niemel _</field>
<field name="title">Planning as satisfiability: parallel plans and algorithms for plan search</field>
<field name="abstract">We address two aspects of constructing plans efficiently by means of satisfiability testing: efficient encoding of the problem of existence of plans of a given number t of time points in the propositional logic and strategies for finding plans, given these formulae for different values of t.

For the first problem we consider three semantics for plans with parallel operator application in order to

make the search for plans more efficient. The standard semantics requires that parallel operators are independent and can therefore be executed in any order. We consider a more relaxed definition of parallel plans which was first proposed by Dimopoulos et al., as well as a normal form for parallel plans that requires every operator to be executed as early as possible. We formalize the semantics of parallel plans emerging in this setting and present translations of these semantics into the propositional logic. The sizes of the translations are asymptotically optimal. Each of the semantics is constructed in such a way that there is a plan following the semantics exactly when there is a sequential plan, and moreover, the existence of a parallel plan implies the existence of a sequential plan with as many operators as in the parallel one.

For the second problem we consider strategies based on testing the satisfiability of several formulae representing plans of n time steps for several values of n concurrently by several processes. We show that big efficiency gains can be obtained in comparison to the standard strategy of sequentially testing the satisfiability of formulae for an increasing number of time steps.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">953</field>
<field name="author">Matt Ruan</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">A Universal Frequency Offset Estimator for OFDM Applications</field>
<field name="keyword">WiMAX</field>
<field name="keyword"> CFO</field>
<field name="keyword"> Fractional carrier frequency offset</field>
<field name="keyword"> estimation</field>
<field name="abstract">This paper studies fractional carrier frequency offset (CFO) estimation for orthogonal frequency-division multiplexing (OFDM) systems. In the IEEE 802.16 (WiMAX) standard,

a training symbol structure with three highly correlated but not identical segments is specified. The existing CFO estimation methods require the segments in the training symbol to be identical, so cannot work for the generalized training structure. In this paper, we proposed a universal fractional CFO estimator to solve the problem. Both the analytical and numerical results are presented to confirm the performance of the proposed method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">954</field>
<field name="author">Matt Ruan</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">A Hybrid Integer Carrier Frequency Offset Estimator for Practical OFDM Systems</field>
<field name="keyword">Synchronization</field>
<field name="keyword"> frequency estimation</field>
<field name="abstract">In orthogonal frequency-division multiplexing (OFDM) systems, the integer part of carrier frequency offset (CFO) as a multiple of subcarrier spacing needs to be estimated from the frequency-domain signal. The performance of the autocorrelation based method is quite poor, while the channel knowledge based method is too complex for hardware real-time implementation. To provide improved tradeoffs between the performance and complexity, we propose a new hybrid algorithm that filters away unlikely integer CFO candidates with the

autocorrelation method and determines the most likely estimate with channel knowledge. The analytical performance of the proposed hybrid method agrees with the simulation results and

both demonstrate the cost-effectiveness of the method even in challenging wireless channel conditions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">955</field>
<field name="author">Ming Zhao</field>
<field name="author">Zhenning Shi</field>
<field name="author">Mark Reed</field>
<field name="title">A Reduced-State-Space Markov Chain Monte Carlo Method for Iterative Spatial Multiplexing MIMO</field>
<field name="keyword">mcmc</field>
<field name="keyword">mimo</field>
<field name="keyword"> iterative receiver</field>
<field name="abstract">Markov Chain Monte Carlo (MCMC) method applied as Multiple-Input-Multiple-Output (MIMO) detector has shown near capacity performance. However, the conventional MCMC method suffers from an error floor in the high signal-to noise (SNR) region. This paper proposes a novel robust reduced state-space MCMC (RSS-MCMC) method, which utilizes the a priori information for the first time to qualify the reliable decoded bits from the entire signal space. The new robust MCMC method is developed to deal with the unreliable bits by using the reliably decoded bit information to cancel the interference that they generate. The performance comparison shows that the new technique has improved performance compared to the conventional approach, and further complexity reduction can be obtained with the assistance of the a priori information. Furthermore, the complexity and performance tradeoff of the new method can be optimized for practical realizations</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">956</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Femtocells: Opportunities and Challenges</field>
<field name="keyword">Femtocell</field>
<field name="keyword"> macrocell</field>
<field name="keyword"> home NodeB</field>
<field name="keyword"> HUE and interference management.</field>
<field name="abstract">The motivation of this course is to provide students and practicing engineers with overview and insight into femtocells (wireless access points for the cellular system). The presentation will discuss the motivation for using femtocell as a means of fixed-mobile convergence, the business case as well as the technical challenges. The tutorial focuses on real-world challenges and provides insight and solutions to them by research engineers that have been working in the area for a number of years.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">957</field>
<field name="author">Geoffrey Chu</field>
<field name="author">Peter Stuckey</field>
<field name="author">Maria Garcia De La Banda</field>
<field name="title">Using relaxations in Maximum Density Still Life</field>
<field name="abstract">The Maximum Density Sill-Life Problem is to ll an n n 

board of cells with the maximum number of live cells so that the board 

is stable under the rules of Conway s Game of Life. We reformulate the 

problem into one of minimising wastage rather than maximising the 

number of live cells. This reformulation allows us to compute strong up- 

per bounds on the number of live cells. By combining this reformulation 

with several relaxation techniques, as well as exploiting symmetries via 

caching, we are able to nd close to optimal solutions up to size n = 100, 

and optimal solutions for instances as large as n = 69. The best previous 

method could only nd optimal solutions up to n = 20.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">958</field>
<field name="author">Geoffrey Chu</field>
<field name="author">Peter Stuckey</field>
<field name="title">Minimizing the maximum number of open stacks by customer search</field>
<field name="keyword">minimization of open stacks</field>
<field name="keyword"> constraint programming</field>
<field name="keyword"> dynamic programming</field>
<field name="abstract">We describe a new exact solver for the minimization of open 

stacks problem (MOSP). By combining nogood recording with a branch 

and bound strategy based on choosing which customer stack to close 

next, our solver is able to solve hard instances of MOSP some 5-6 orders 

of magnitude faster than the previous state of the art. We also derive 

several pruning schemes based on dominance relations which provide 

another 1-2 orders of magnitude improvement. One of these pruning 

schemes largely subsumes the e ect of the nogood recording. This allows 

us to reduce the memory usage from an potentially exponential amount 

to a constant 

 __2Mb for even the largest solvable instances. We also show 

how relaxation techniques can be used to speed up the proof of optimality 

by up to another 3-4 orders of magnitude on the hardest instances.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">959</field>
<field name="author">Geoffrey Chu</field>
<field name="author">Christian Schulte</field>
<field name="author">Peter Stuckey</field>
<field name="title">Con dence based Work Stealing in Parallel Constraint Programming</field>
<field name="keyword">parallelism. adaptive search</field>
<field name="keyword"> constraint programming</field>
<field name="abstract">In parallel constraint solving, work stealing not only allows 

for dynamic load balancing, but also determines which parts of the search 

tree are searched next. Thus the place from where work is stolen has a 

dramatic e ect on the e ciency of a parallel search algorithm. In this 

paper we examine quantitatively how optimal work stealing can be per- 

formed given an estimate of the relative solution densities of the sub- 

trees at each node in the search tree and show how this is related to 

the branching heuristic strength. We propose an adaptive work stealing 

algorithm that automatically performs di erent work stealing strategies 

based on the strength of the branching heuristic at each node. Many 

parallel depth- rst search patterns arise naturally from our algorithm. 

Our algorithm is able to produce near perfect or super linear algorithmic 

e ciencies on all problems tested. Real speedups using 8 threads ranges 

from 4-5 times speedup to super linear speedup.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">960</field>
<field name="author">Alan Frisch</field>
<field name="author">Peter Stuckey</field>
<field name="title">The Proper Treatment of Unde nedness in Constraint Languages</field>
<field name="keyword">undefinedness</field>
<field name="keyword"> partial functions</field>
<field name="keyword"> modelling languages</field>
<field name="abstract">Any su ciently complex constraint modelling language has 

the ability to express unde ned values, for example division by zero, or 

array index out of bounds. In this paper we give the rst systematic treat- 

ment of unde nedness for constraint languages. We present three alter- 

native semantics for unde nedness, and show how we can map models 

that contain possibly unde ned expressions into equivalent models under 

each of the three semantics which do not contain unde ned expression. 

The resulting models can be implemented using existing constraint solv- 

ing technology.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">961</field>
<field name="author">Maria Garcia De La Banda</field>
<field name="author">Peter Stuckey</field>
<field name="author">Geoffrey Chu</field>
<field name="title">Solving Talent Scheduling with Dynamic Programming</field>
<field name="abstract">We give a dynamic programming solution to the problem of scheduling scenes to minimize the cost of the talent. Starting 

from basic dynamic program, we show a number of ways to improve the dynamic programming solution, by preprocessing 

and restricting the search. We show how by considering a bounded version of the problem, and determining lower and 

upper bounds, we can improve the search. We then show how ordering the scenes from both ends can drastically reduce 

the search space. The nal dynamic programming solution is, orders of magnitude faster than competing approaches, and 

 nds optimal solutions to larger problems than were considered previously.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">962</field>
<field name="author">Jason J Saleem</field>
<field name="author">Alissa L Russ</field>
<field name="author">Penelope Sanderson</field>
<field name="author">Todd R Johnson</field>
<field name="author">Dean F Sittig</field>
<field name="title">Current challenges and opportunities for better integration of human factors research with development of clinical information systems.</field>
<field name="keyword">health informatics</field>
<field name="keyword"> clinical information systems</field>
<field name="keyword"> human factors</field>
<field name="keyword"> cognition</field>
<field name="keyword"> user interface design</field>
<field name="keyword"> mental workload</field>
<field name="keyword"> situation awareness</field>
<field name="keyword"> cognitive systems engineering</field>
<field name="abstract">Clinical information system developers and implementers have begun to look to other scientific disciplines for new methods, tools, and techniques to help them better understand clinicians and their organizational structures, clinical work environments, capabilities of clinical information and communications technology, and the way these structures and processes interact. The goal of this article is to help CIS researchers, developers, implementers, and evaluators better understand the methods, tools, techniques, and literature of the field of human factors and vice versa. The article discusses six sub-themes: 1) Mental Workload and Situation Awareness; 2) Workflow and Task Analysis; 3) Clinical Decision Making and Decision Support; 4) Distributed Cognition; 5) Informatics and Patient Safety; and 6) User Interface Design and Evaluation. Integrating the methods, tools, and lessons learned from each of these six areas of human factors research early in clinical information system design and incorporating them iteratively during development can improve user performance, user satisfaction, and integration into clinical workflow. Ultimately, this approach will improve clinical information systems and healthcare delivery.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">963</field>
<field name="author">Christian Knauer</field>
<field name="author">Maarten L _ffler</field>
<field name="author">Marc Scherfenberg</field>
<field name="author">Thomas Wolle</field>
<field name="title">The directed Hausdorff distance between imprecise point sets</field>
<field name="keyword">imprecise point sets</field>
<field name="keyword"> Hausdorff distance</field>
<field name="keyword"> computational geometry</field>
<field name="abstract">We consider the directed Hausdorff distance between point sets in the plane, where one or both point sets consist of imprecise points. An imprecise point is modeled by a disc given by its centre and a radius. The actual position of an imprecise point may be anywhere within its disc. Due to the direction of the Hausdorff Distance and whether its tight upper or lower bound is computed there are several cases to consider. For every case we either show that the computation is NP-hard or we present an algorithm with a polynomial running time.

Further we give several approximation algorithms for the hard cases and show that one of them cannot be approximated better than with factor 3, unless P=NP.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">964</field>
<field name="author">Kelvin Cheng</field>
<field name="author">Masa Takatsuka</field>
<field name="title">Hand Pointing Accuracy for Vision-Based Interactive Systems</field>
<field name="keyword">Hand Pointing</field>
<field name="keyword"> Pointing Accuracy</field>
<field name="keyword"> Computer Vision.</field>
<field name="abstract">Vision-based hand pointing interactive systems always assumed that users physical pointing accuracy are perfect, but this may not be the case. We investigated the accuracy provided by users in three pointing strategies. Result showed that pointing inaccuracy can be as high as 239mm at 3 metres away and suggest that the line-up method provides the best accuracy overall.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">965</field>
<field name="author">Tofazzal Hossain</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="title">Symbol Timing Estimation Using Near ML Techniques and Statistical Performance Evaluation for Binary Communications</field>
<field name="keyword">BER</field>
<field name="keyword"> CRB</field>
<field name="keyword"> Flat fading channel</field>
<field name="keyword"> Near Maximum</field>
<field name="abstract">In this paper, we analyze the performance of a non-data aided near maximum likelihood (NDA-NML) estimator for symbol timing recovery in binary digital communications. The analysis of the estimator is performed for an additive noise only channel and the simulation results are extended to a flat fading channel. The probability distribution of the timing estimates is derived, presented and compared with simulation results. The performance of the estimator is presented in terms of the bit error rate (BER) and the error variance of the estimates. The BER is computed when the estimator is operating under additive white Gaussian noise (AWGN) channel and Rayleigh fading channel. The variance of the estimates is computed for the noise only case and compared with the Cramer Rao bound (CRB) and modified Cramer Rao bound (MCRB).</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">966</field>
<field name="author">Adi Botea</field>
<field name="author">Andre Augusto Cire</field>
<field name="title">Incremental Heuristic Search for Planning with Temporally Extended Goals and Uncontrollable Events</field>
<field name="abstract">Planning with temporally extended goals and uncontrollable events has recently been introduced as a formal model for system reconfiguration problems. An important application is to automatically reconfigure a real-life system in such a way that its subsequent internal evolution is consistent with a temporal goal formula.



In this paper we introduce an incremental search algorithm and a search-guidance heuristic, two generic planning enhancements. An initial problem is decomposed into a series of subproblems, providing two main ways of speeding up a search. Firstly, a subproblem focuses on a part of the initial goal. Secondly, a notion of action relevance allows to explore with higher priority actions that are heuristically considered to be more relevant to the subproblem at hand.



Even though our techniques are more generally applicable, we restrict our attention to planning with temporally extended goals and uncontrollable events. Our ideas are implemented on top of a successful previous system that performs online learning to better guide planning and to safely avoid potentially expensive searches. In experiments, the system speed performance is further improved by a convincing margin.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">967</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Kelvin Cheng</field>
<field name="author">Markus Rittenbruch</field>
<field name="title">Exploring Manual Interaction and Social Behaviour Patterns in Intensely Collaborative Teamwork</field>
<field name="keyword">Manual gestures</field>
<field name="keyword"> CSCW</field>
<field name="keyword"> Multi-touch table</field>
<field name="keyword"> paper vs. digital</field>
<field name="abstract">This paper presents the results of a comparative study of 4-person collaborative teams working at a traditional table with pen and paper vs. a multi-touch table with digital keyboards and notepads. We compare the social behaviours of giving and taking during intensely collaborative teamwork, namely the differences between paper-based behaviour, digital-object based behaviour and a mixed condition behaviour where both paper and digital objects were used. Differences in sharing behaviour may be attributed to the degree of ownership afforded by digital objects on a touch display vs. paper objects.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">968</field>
<field name="author">Ke Jia</field>
<field name="author">Nianjun Liu</field>
<field name="author">lei Wang</field>
<field name="author">Li Cheng</field>
<field name="title">Pavement Scene Interpretation and Obstacle Detection by Large Margin Image Labeling</field>
<field name="abstract">This paper presents a novel discriminative approach for pave-ment scene understanding and obstacle detection in real-world images.

It overcomes the heavy constraints in previous systems such as a simple background, a speci&#12;c obstacle, etc. The approach we exploited extends the bundle method to incorporate pairwise correlations among neighboring pixels, and adopts graph-cuts as the inference engine to attain the approximation efficiently. A set of robust features on both local and multi-scale level is also introduced that captures the general statistical properties of pavements and obstacles. The proposed approach is validated on real-world image database, and outperforms the current state-of-the-art visioned-based methods</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">969</field>
<field name="author">Ke Jia</field>
<field name="author">Li Cheng</field>
<field name="author">Nianjun Liu</field>
<field name="title">Efficient Learning to Label Images</field>
<field name="abstract">Conditional random field methods (CRFs) have gained popularity for image Labeling tasks in recent years. In this paper, we describe an alternative discriminative approach, by extending the large margin principle to incorporate spatial correlations among neighboring pixels. In particular, by explicitly enforcing the submodular condition, graphcuts is conveniently integrated as the inference engine to attain the optimal label assignment efficiently. Our approach allows learning a model with thousands of parameters, this is further facilitated by parallel computation in the learning phase. In addition to node and edge feature functions for enforcing local label consistency, our algorithm is shown to be capable of readily incorporating higher-order scene context. Empirical studies on a variety of image datasets suggests that our approach performs competitively comparing to the state-of-the-art scene labeling methods</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">970</field>
<field name="author">Yong Wong</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Regression Based Non-Frontal Face Synthesis for Improved Identity Verification</field>
<field name="keyword">image synthesis</field>
<field name="keyword"> biometrics</field>
<field name="abstract">We propose a low-complexity face synthesis technique which transforms a 2D frontal view image into views at specific poses, without recourse to computationally expensive 3D analysis or iterative fitting techniques that may fail to converge. The method first divides a given image into multiple overlapping blocks, followed by synthesising a non-frontal representation through applying a multivariate linear regression model on a low-dimensional representation of each block. To demonstrate one application of the proposed technique, we augment a frontal face verification system by incorporating multi-view reference (gallery) images synthesised from the frontal view. Experiments on the pose subset of the FERET database show considerable reductions in error rates, especially for large deviations from the frontal view.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">971</field>
<field name="author">Anushiya Kannan</field>
<field name="author">Baris Fidan</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Derivation of Flip Ambiguity Probabilities to Facilitate Robust Sensor Network Localization</field>
<field name="abstract">Erroneous local geometric realizations in some parts

of the network due to their sensitivity to certain distance measurement

errors is a major problem in wireless sensor network

localization. This may in turn affect the localization of either

the entire network or a large portion of it. This phenomenon

is well-described using the notion of flip ambiguity in rigid

graph theory. In this paper we analytically derive an expression

for the flip ambiguity probabilities of arbitrary neighborhoods

in two dimensional sensor networks. This probability can be

used to mitigate flip ambiguities in two ways: 1) If an unknown

sensor finds the probability of flip ambiguity on its location

estimate larger than a predefined threshold, it may choose not

to localize itself 2) Every known neighbor can be assigned

with a confidence factor to its estimated location, reflecting

the probability of flip ambiguity; a sensor with an initially

unknown location can then choose only those known neighbors

with a confidence factor greater than a predefined threshold. A

recent study by co-authors have shown that the performance of

sequential and cluster based localization schemes in the literature

can be significantly improved by correctly identifying and

removing neighborhoods with possible flip ambiguities from the

localization process. One motivation of this paper is to enhance

the performance of the robustness criterion presented in that

study by accurately identifying the flip ambiguity probabilities

of arbitrary neighborhoods. The various simulations done in this

study show that our analytical calculations of the probability

of flip ambiguity matches with the simulated detection of the

probability very accurately.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">972</field>
<field name="author">Priscilla Kan John</field>
<field name="author">Lachlan Blackhall</field>
<field name="author">Alban Grastien</field>
<field name="title">Diagnosing structural changes in hybrid dynamical systems</field>
<field name="keyword">structural changes</field>
<field name="keyword"> hybrid</field>
<field name="keyword"> dynamical</field>
<field name="abstract">A method for diagnosing the operational mode of a hybrid dynamical system is

presented. This methodology allows discrete diagnosis capabilities to be integrated with the

continuous time dynamics of hybrid dynamical networks. The method is novel in that the

diagnosis is not performed on the output trajectory of the network but rather on the fundamental

governing dynamics of the system, thus not requiring the computation of residuals. This allows

for the diagnosis of arbitrary switching events that cause structural changes in the system. The

chosen methodology can be partially decentralised and makes diagnoses possible even when only

a portion of the system can be observed. Computational results are presented for the case where the mode

changes are purely structural, i.e. where only the interconnection structure of the system is

changing.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">973</field>
<field name="author">Priscilla Kan John</field>
<field name="author">Lachlan Blackhall</field>
<field name="author">Alban Grastien</field>
<field name="author">David Hill</field>
<field name="title">Diagnosability of Networks of Hybrid Systems</field>
<field name="keyword">diagnosability</field>
<field name="keyword"> networks</field>
<field name="keyword"> hybrid</field>
<field name="abstract">Results for the diagnosability of hybrid dynamical networks are presented. These

results give the conditions for which arbitrary events (including faults) in such a network can be

detected and isolated using a general class of indicator functions. These results emerge from the

overlap of the control theoretic fault detection and isolation and diagnosis communities. The

interaction of the many systems in the network is exploited to achieve diagnosability conditions

dependent only on the number of potential events in the network rather than the number of

interconnected systems. The algorithmic complexity of the diagnosability conditions and of

choosing a minimal indicator set that guarantee diagnosability are also addressed.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">974</field>
<field name="author">Paul Brebner</field>
<field name="title">Is Non Functional Testing becoming more Business Critical?</field>
<field name="abstract">Invited keynote panel discussion</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">975</field>
<field name="author">Paul Brebner</field>
<field name="author">Jon Gray</field>
<field name="title">Virtulisation Technology in an SOA Environment</field>
<field name="keyword"/>
<field name="abstract">Server virtualisation is addressing tomorrow s environmental challenges by enabling higher average server utilisation and reduced electricity consumption resulting in decreased green house emissions. In this presentation Server Oriented Performance Modelling (SOPM) technology that enables the performance and scalability of SOAs to be predicted on virtualised server environments is examined using the NICTA developed ePASA simulation tool. The presentation draws on empirically trialled and validated research with collaborating Australian government departments.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">976</field>
<field name="author">Paul Brebner</field>
<field name="title">Virtualisation Technology In An SOA Environment It May Be Green and Agile, But Do You Understand The Risks?</field>
<field name="abstract">In this presentation we provide an overview of NICTA s performance modelling approach, and then explore a number of alternative deployment scenarios for an example SOA application an emissions trading system. Our modelling approach provides insights into the capacity, resource requirements, and carbon emissions for different deployment options of this example trading system, including fixed servers, server virtualization, and computing on demand (cloud computing using Amazon EC2). We conclude with an overview of other potential problems and benefits of virtualization in an SOA context.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">977</field>
<field name="author">Roksana Boreli</field>
<field name="author">Thava Iyer</field>
<field name="author">Christoph Dwertmann</field>
<field name="author">Golam Sarwar</field>
<field name="author">Klok Frits</field>
<field name="title">A Quickly Deployable Platform for Multi-User Communications Over Satellite Links</field>
<field name="keyword">satellite</field>
<field name="keyword"> TCP</field>
<field name="keyword"> VoIP</field>
<field name="keyword"> protocol</field>
<field name="abstract">We describe the research outcomes and resulting implementation and validation of a prototype IP telephony and data acceleration system for satellite communications. The system has been designed as a quickly deployable solution for generic use with fixed or mobile satellite services on land or sea. It has been developed in NICTA within the Office in a Box project and is currently being commercialised in 7-ip. Our main research contribution is a TCP proxy for long delay links, based on a novel congestion control and scheduling algorithm which incorporates lossless data compression. Additional contribution is in proposing and developing a silence suppression enhancement to the Asterisk IP telephony gateway. We describe the system and software architecture and software modules which include the satellite modem controller for the management of Inmarsat broadband satellite services. The system has been tested on a number of satellite services including Inmarsat BGAN, IPSTAR and VSAT systems. We present results of validation testing for multi-user IP telephony and data services and show performance improvement compared to standard solutions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">978</field>
<field name="author">Vikas Reddy</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">An Efficient and Robust Sequential Algorithm for Background Estimation in Video Surveillance</field>
<field name="keyword">surveillance</field>
<field name="keyword"> background estimation</field>
<field name="abstract">Many computer vision algorithms such as object tracking and event detection assume that a background model of the scene under analysis is known. However, in many practical circumstances it is unavailable and needs to be estimated from cluttered image sequences. In this paper we propose a sequential technique for background estimation in such conditions, with low computational and memory requirements. The first stage is somewhat similar to that of the recently proposed agglomerative clustering background estimation method, where image sequences are analysed on a patch by patch basis. For each patch location a representative set is maintained which contains distinct patches obtained along its temporal line. The novelties lie in iteratively filling in background areas by selecting the most appropriate candidate patches according to the combined frequency responses of extended versions of the candidate patch and its neighbourhood. It is assumed that the most appropriate patch results in the smoothest response, indirectly enforcing the spatial continuity of structures within a scene. Experiments on real-life surveillance videos demonstrate the advantages of the proposed method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">979</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Reformulating Global Grammar Constraints</field>
<field name="keyword">constraint satisfaction</field>
<field name="keyword"> formal languages</field>
<field name="abstract">An attractive mechanism to specify 

global constraints in rostering

and other domains is via formal languages. For instance, 

the \regular and \CFG constraints specify

constraints in terms of the %fixed-length 

languages

accepted by an automaton

and a context-free grammar

respectively. 

Taking advantage of the fixed length

of the constraint, we give an algorithm to transform 

a context-free grammar into an automaton. 

We then 

study the use of minimization techniques

to reduce the size of such automata and speed up propagation. 

We show that minimizing such automata after

they have been unfolded and domains initially

reduced can give automata that are more compact than

minimizing before unfolding and reducing. 

Experimental results show that such transformations

can improve the size of rostering

problems that we can

``model and run''.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">980</field>
<field name="author">Christian Bessiere</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Circuit Complexity and Decompositions of Global Constraints</field>
<field name="keyword">constraint satisfaction</field>
<field name="keyword"> satisfiability</field>
<field name="keyword"> circuit complexity</field>
<field name="abstract">We show that tools from circuit complexity can be used

 to study decompositions of global constraints. 

 In particular, we study decompositions of 

 global constraints into conjunctive normal form with the property

 that unit propagation on the decomposition enforces the same level 

 of consistency as a specialized propagation algorithm. We prove that a

 constraint propagator has a a polynomial size decomposition if

 and only if it can be computed by a polynomial size monotone Boolean

 circuit. Lower bounds on the size of monotone Boolean circuits thus 

 translate to lower bounds on the size of decompositions of global

 constraints. For instance, we prove that there is no polynomial sized 

 decomposition of the domain consistency propagator for the \alldiff 

 constraint.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">981</field>
<field name="author">Christian Bessiere</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Claude-Guy Quimper</field>
<field name="author">Toby Walsh</field>
<field name="title">Decompositions of All Different, Global Cardinality and Related Constraints</field>
<field name="keyword">constraint satisfaction</field>
<field name="keyword"> satisfiability</field>
<field name="abstract">We show that some common and important 

global constraints like \alldiff and \gcc 

can be decomposed into simple arithmetic 

constraints on which we achieve bound 

or range consistency, and in some cases

even greater pruning. 

These decompositions can be easily

added to new solvers. 

They also provide other constraints

with access to the state of the propagator by

sharing

of variables. Such sharing

can be used to improve propagation between constraints. 

We report experiments with our decomposition

in a pseudo-Boolean solver.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">982</field>
<field name="author">Peizhao Hu</field>
<field name="author">Ryan Wishart</field>
<field name="author">Jimmy Ti</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">MeshVision: An Adaptive Wireless Mesh Network Video Surveillance System</field>
<field name="keyword">MeshVision</field>
<field name="keyword"> wireless mesh networks</field>
<field name="keyword"> wireless camera surveillance system</field>
<field name="abstract">The major surveillance camera manufacturers have begun incorporating wireless networking functionality into their products to enable wireless access. However, the video feeds from such cameras can only be accessed within the transmission range of the cameras. These cameras must be connected to backbone infrastructure in order to access them from more than one hop away. This network infrastructure is both time-consuming and expensive to install, making it impractical in many rapid deployment situations (for example to provide temporary surveillance at a crime scene). To overcome this problem, we propose the MeshVision system that incorporates wireless mesh network functionality directly into the cameras. Video streams can be pulled from any camera within a network of MeshVision cameras, irrespective of how many hops away that camera is. To manage the trade-off between video stream quality and the number of video streams that could be concurrently accessed over the network, MeshVision uses a Bandwidth Adaptation Mechanism. This mechanism monitors the wireless network looking for drops in link quality or signs of congestion and adjusts the quality of existing video streams in order to reduce that congestion. A significant benefit of the approach is that it is low cost, requiring only a software upgrade of the cameras.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">983</field>
<field name="author">Asad Pirzada</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">ALARM: An Adaptive Load-Aware Routing Metric for Hybrid Wireless Mesh Networks</field>
<field name="abstract">Hybrid Wireless Mesh Networks (WMN) are generally employed to establish communication during disaster recovery operations. The Hybrid WMN network is formed in a spontaneous manner when wireless nodes belonging to different agencies are operated in the disaster area. These wireless nodes generally have heterogeneous configurations in terms of number of transceivers, computational power, battery resources and mobility pattern. Internet Protocol (IP) acts as the common platform for integrating these heterogeneous devices. Routing protocols are engaged to determine paths between sets of wireless nodes. A number of routing metrics have been developed to achieve performance gains in multi-radio, multi-hop WMNs. However, most of these metrics need access to external information like link quality statistics, channel numbers, etc on a regular basis. This frequent information exchange coupled with the incessant variation in mobility and traffic load conditions causes degraded performance of these metrics in Hybrid WMNs. In this paper, we present a routing metric named ALARM specifically designed for Hybrid WMNs. ALARM is computed using the number of packets queued per wireless interface. This computed value offers an accurate representation of the traffic load, link quality, interference and noise levels. With the help of extensive simulations, we show that our routing metric outperforms well-know routing metrics like ETT and WCETT under varying mobility and traffic load conditions in Hybrid WMNs. We further show the practicality of the metric through a prototype implementation and provide performance results obtained from a small-scale testbed deployment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">984</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Challenges and Recent Advances in QoS Provisioning, Signaling, Routing and MAC in MANETs</field>
<field name="abstract">Mobile Ad hoc Networks (MANET), which comprise of mobile nodes connected wirelessly, are emerging as a very important technology for future generation of wireless networking. MANETs are being used in numerous application domains from emergency rescue and relief to wireless sensor networks. Quality of Service (QoS) provisioning in MANETs is of utmost importance in order to support real-time communications (such as audio and video) over MANETs. However, QoS provisioning in highly mobile wireless networks such as MANETs is a very challenging problem compared to provisioning of QoS in wired IP networks. The main reasons behind this being unpredictable node mobility, wireless multi-hop communication, contention for wireless channel access, limited battery power and range of mobile devices as well as the absence of a central coordination authority in MANETs. This paper presents the current state of the art in MANET QoS Provisioning schemes at Routing, Transport and Medium Access Layers as well as the MANET QoS Signaling and Provisioning Models. Previous surveys have only looked at QoS provisioning models, signaling and routing. This paper presents a complete survey of the challenges and recent protocols being developed for QoS provisioning in MANET across multiple network layers as well as the various QoS signaling and QoS models being developed for MANETs</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">985</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">Evaluation of Wireless Mesh Network Handoff Approaches for Public Safety and Disaster Recovery Networks</field>
<field name="abstract">In Public Safety and Disaster Recovery (PSDR) scenarios, reliable communication is an imperative. Unfortunately, communication infrastructure is often destroyed or overwhelmed by whatever precipitated the scenario (e.g., a hurricane or terrorist attack). Thus, the PSDR workers must often deploy their own communications infrastructure on-site. Wireless mesh networks (WMN) have been identified as being ideally suited to this task. WMN offer a high-capacity wireless backhaul network, provided by mesh routers, through which clients can connect to one another or with external networks. Mobility of clients within the mesh is particularly important for Public Service and Disaster Recovery scenarios. This creates a challenging problem as clients may move out of range of the mesh router they were using to connect to the mesh and need to associate with another. Client handoff mechanisms provide this functionality. In this paper we provide a critical survey of client handoff approaches applicable to IEEE 802.11 WMN evaluating them based on the strict QoS requirements established by the US Department of Homeland Security for PSDR networks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">986</field>
<field name="author">Steve Glass</field>
<field name="author">vallipuram muthukkumarasamy</field>
<field name="author">Marius Portmann</field>
<field name="title">Securing Wireless Mesh Networks</field>
<field name="abstract">Now found in domestic, commercial, industrial, military, and healthcare applications, wireless networks are becoming ubiquitous. Wireless mesh networks (WMNs) combine the robustness and performance of conventional infrastructure networks with the large service area and self-organizing and self-healing properties of mobile ad hoc networks. In this article, the authors consider the problem of ensuring security in WMNs, introduce the IEEE 802.11s draft standard, and discuss the open security threats faced at the network and data-link layers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">987</field>
<field name="author">Vladimir Tosic</field>
<field name="author">Rasangi Pumudu Karunaratne</field>
<field name="author">Qinghua Lu</field>
<field name="title">Specification of Context for Management of Service-Oriented Systems with WS-Policy4MASC</field>
<field name="keyword">web service management</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> context</field>
<field name="keyword"> context management context-sensitive operation</field>
<field name="abstract">Specification of monitored context properties and their influence on operation of service-oriented systems and on management activities is a prerequisite for context-sensitive operation. We researched context specification for a management system performing various management activities and potentially used by mobile service-oriented systems. Due to the similarities between processing and use of context properties and processing and use of quality of service (QoS) metrics, we decided to model context properties analogously to QoS metrics. We built our solutions for specification of context properties and related management activities into two languages: the Web Service Offerings Language (WSOL) and WS-Policy4MASC, the latter of which is the focus of this book chapter. WS-Policy4MASC is a powerful extension of the Web Services Policy Framework (WS-Policy) with constructs for specification of information necessary for run-time policy-driven management. The presented constructs related to context increase usefulness of WS-Policy4MASC for management of mobile service-oriented systems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">988</field>
<field name="author">Longbin Chen</field>
<field name="author">Julian McAuley</field>
<field name="author">Rogerio Feris</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Matthew Turk</field>
<field name="title">Shape Classification Through Structured Learning of Matching Measures</field>
<field name="abstract">Many traditional methods for shape classification involve

establishing point correspondences between shapes to

produce matching scores, which are in turn used as similarity

measures for classification. Learning techniques have

been applied only in the second stage of this process, after

the matching scores have been obtained. In this paper,

instead of simply taking for granted the scores obtained

by matching and then learning a classifier, we learn the

matching scores themselves so as to produce shape similarity

scores that minimize the classification loss. The solution

is based on a max-margin formulation in the structured

prediction setting. Experiments in shape databases reveal

that such an integrated learning algorithm substantially improves

on existing methods.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">989</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Julian McAuley</field>
<field name="author">Li Cheng</field>
<field name="author">Quoc Le</field>
<field name="author">Alex Smola</field>
<field name="title">Learning Graph Matching</field>
<field name="abstract">As a fundamental problem in pattern recognition,

graph matching has applications in a variety of fields, from

computer vision to computational biology. In graph matching,

patterns are modeled as graphs and pattern recognition amounts

to finding a correspondence between the nodes of different

graphs. Many formulations of this problem can be cast in

general as a quadratic assignment problem, where a linear

term in the objective function encodes node compatibility and

a quadratic term encodes edge compatibility. The main research

focus in this theme is about designing efficient algorithms for

approximately solving the quadratic assignment problem, since

it is NP-hard. In this paper we turn our attention to a different

question: how to estimate compatibility functions such that the

solution of the resulting graph matching problem best matches

the expected solution that a human would manually provide.

We present a method for learning graph matching: the training

examples are pairs of graphs and the labels are matches

between them. Our experimental results reveal that learning

can substantially improve the performance of standard graph

matching algorithms. In particular, we find that simple linear

assignment with such a learning scheme outperforms Graduated

Assignment with bistochastic normalisation, a state-of-the-art

quadratic assignment relaxation algorithm.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">990</field>
<field name="author">Toby Walsh</field>
<field name="title">Where are the really hard manipulation problems? The phase transition in manipulating the veto rule</field>
<field name="abstract">Voting is a simple mechanism to aggregate the preferences of agents. Many voting rules have been shown to be NP-hard to manipulate. However, a number of recent theoretical results suggest that this complexity may only be in the worst-case since manipulation is often easy in practice. In this paper, we show that empirical studies are useful in improving our understanding of this issue. We demonstrate that there is a smooth transition in the probability that a coalition can elect a desired candidate using the veto rule as the size of the manipulating coalition increases. We show that a rescaled probability curve displays a simple and universal form independent of the size of the problem. We argue that manipulation of the veto rule is asymptotically easy for many independent and identically distributed votes even when the coalition of manipulators is critical in size. Based on this argument, we identify a situation in which manipulation is computationally hard. This is when votes are highly correlated and the election is ``hung''. We show, however, that even a single uncorrelated voter is enough to make manipulation easy again.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">991</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Mohaddeseh Nosratighods</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Computationally Efficient Frame-averaged FM Feature Extraction for Speaker Recognition</field>
<field name="keyword">frequency modulation</field>
<field name="keyword"> speaker recognition</field>
<field name="keyword"> frame averaged</field>
<field name="abstract">Recently, subband frame-averaged frequency modulation (FM) as a complementary feature to amplitude-based features for several speech based classification problems including speaker recognition has shown promise. One problem with using FM extraction in practical implementations is computational complexity. Proposed is a computationally efficient method to estimate the frame-averaged FM component in a novel manner, using zero crossing counts and the zero crossing counts of the differentiated signal. FM components, extracted from subband speech signals using the proposed method, form a feature vector. Speaker recognition experiments conducted on the NIST 2008 telephone database show that the proposed method successfully augments mel frequency cepstrum coefficients (MFCCs) to improve performance, obtaining 17% relative reductions in equal

error rates when compared with an MFCC-based system.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">992</field>
<field name="author">Scott Sanner</field>
<field name="author">Robby Goetschalckx</field>
<field name="author">Kurt Driessens</field>
<field name="author">Guy Shani</field>
<field name="title">Bayesian Real-time Dynamic Programming</field>
<field name="keyword">MDPs</field>
<field name="keyword"> Dynamic Programming</field>
<field name="keyword"> Heuristic Search</field>
<field name="keyword"> Real-time Systems</field>
<field name="abstract">Real-time dynamic programming (RTDP) solves Markov decision processes

(MDPs) when the initial state is restricted, by focusing dynamic

programming on the envelope of states reachable from an initial state

set. RTDP often provides performance guarantees without visiting the

entire state space. Building on RTDP, recent work has sought to

improve its efficiency through various optimizations, including

maintaining upper and lower bounds to both govern trial termination

and prioritize state exploration. In this work, we take a Bayesian

perspective on these upper and lower bounds and use a value of perfect

information (VPI) analysis to govern trial termination and exploration

in a novel algorithm we call VPI-RTDP. VPI-RTDP leads to an

improvement over state-of-the-art RTDP methods, empirically yielding

up to a three-fold reduction in the amount of time and number of

visited states required to achieve comparable policy performance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">993</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Challenges and Recent Advances in QoS Provisioning in Wireless Mesh networks</field>
<field name="abstract">Wireless mesh networks (WMNs) comprising of mobile

and static nodes connected wirelessly are emerging as a

key technology for future generation of wireless

networks. WMNs self-organize, self-configure and selfheal

themselves and can increase the coverage of

conventional infrastructure-based wireless LANs and

MANs without significant additional infrastructure

deployments. Due to these unique features, WMNs are

being used in many applications ranging from emergency

response situations to wireless metropolitan area

networks. Quality of Service (QoS) provisioning in

WMNs is of utmost importance in order to support realtime

audio and video communications. However, QoS

provisioning in highly mobile wireless networks such as

WMNs is a very challenging problem compared to

provisioning of QoS in wired IP networks. The main

reasons for this are unpredictable node mobility, wireless

multi-hop communication, contention for wireless

channel access, limited battery power and wireless range

of mobile devices, as well as the absence of a central

coordination authority in WMNs. This paper describes

the challenges and the state of the art in provisioning of

QoS over WMNs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">994</field>
<field name="author">Basem Suleiman</field>
<field name="author">Fuyuki Ishikawa</field>
<field name="title">A Constraint-Based Approach for Developing Consistent Contracts in Composite Service</field>
<field name="keyword">constraint satisfaction</field>
<field name="keyword"> composite contracts</field>
<field name="keyword"> composite services</field>
<field name="keyword"> consistency checking</field>
<field name="keyword"> quality of service (QoS).</field>
<field name="abstract">A key problem that challenges the designers of service-oriented systems is ensuring the consistency of contract parameter values in service composition. This paper utilizes constraint satisfaction approach to examine the problem at design time and by focusing on quality of service (QoS) contract parameters. It proposes a generic framework to formalize service contract composition as a constraint satisfaction problem (CSP). It also introduces an initial tool design for automating composite contract consistency checking and adaptation based on QoS parameters. The tool aims at supporting service orchestrators to specify appropriate contract parameter values and adapt them so that consistency of composite contracts is ensured. Further, it enables them to analyze and reason about violation percentages during contract negotiation phase. The benefits of the proposed CSP framework and the tool design have been illustrated through a Stock Manager Web service composition scenario</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">995</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">Liming Zhu</field>
<field name="title">Towards Concise Architectures for Flexible Business Processes</field>
<field name="abstract">This chapter proposes a view of business processes as designed artefacts that are ontologically no different than artefacts in domains such as mechanical and software engineering. This view distinguishes three concerns for designing processes: architecture, implementation and adaptation. We show that current process modelling approaches conflate these aspects, often leading to high complexity and inflexibility of the resulting process models. We use a generalisation of the feature concept in engineering design, represented using the function-behaviour-structure (FBS) ontology, as the basis of a new approach to concisely specifying business process architectures that allow for more process flexibility.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">996</field>
<field name="author">Kelvin Cheng</field>
<field name="author">Masa Takatsuka</field>
<field name="title">Initial Evaluation of a Bare-Hand Interaction Technique for Large Displays using a Webcam</field>
<field name="keyword">Large displays interaction</field>
<field name="keyword"> hand pointing</field>
<field name="keyword"> monocular computer vision.</field>
<field name="abstract">dTouch is a novel 3D pointing system that attempts to allow interaction with large displays from the use of a single webcam. An initial evaluation to demonstrate the feasibility of our pointing technique is presented. We compared our prototype with a popular 2D pointing technique, used in the EyeToy game for the PlayStation console, in a usability study. Result shows that the two techniques are comparable, each with its pros and cons. We concluded that it is possible to use our technique and a webcam to allow interaction with large displays. With further development, our technique can serve as a basis for the design of the next generation interactive monocular vision systems, due to the added flexibility to the user s location.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">997</field>
<field name="author">Silvia Richter</field>
<field name="author">Malte Helmert</field>
<field name="title">Preferred Operators and Deferred Evaluation in Satisficing Planning</field>
<field name="keyword">Planning</field>
<field name="keyword"> Heuristic Search</field>
<field name="abstract">Heuristic forward search is the dominant approach to satisficing

planning to date. Most successful planning systems, however, go beyond

plain heuristic search by employing various search-enhancement

techniques. One example is the use of helpful actions or preferred

operators, providing information which may complement heuristic

values. A second example is deferred heuristic evaluation, a search

variant which can reduce the number of costly node evaluations.

Despite the wide-spread use of these search-enhancement techniques

however, we note that few results have been published examining their

usefulness. In particular, while various ways of using, and possibly

combining, these techniques are conceivable, no work to date has

studied the performance of such variations. In this paper, we address

this gap by examining the use of preferred operators and deferred

evaluation in a variety of settings within best-first search. In

particular, our findings are consistent with and help explain the good

performance of the winners of the satisficing tracks at IPC 2004 and 2008.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">998</field>
<field name="author">Brian Anderson</field>
<field name="author">Peter Belhumeur</field>
<field name="author">Stephen Morse</field>
<field name="author">Walter Whiteley</field>
<field name="author">Richard Yang</field>
<field name="author">Tolga Eren</field>
<field name="author">David Goldenberg</field>
<field name="title">Graphical properties of easily localizable sensor networks</field>
<field name="abstract">The sensor network localization problem is one of determining the Euclidean positions of all sensors in a network given knowledge of the Euclidean positions of some, and knowledge of a number of inter-sensor distances. This paper identifies graphical properties which can ensure unique localizability, and further sets of properties which can ensure not only unique localizability but also provide guarantees on the associated computational complexity, which can even be linear in the number of sensors on occasions. Sensor networks with minimal connectedness properties in which sensor transmit powers can be increased to increase the sensing radius lend themselves to the acquiring of the needed graphical properties. Results are presented for networks in both two and three dimensions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">999</field>
<field name="author">Yu Shi</field>
<field name="author">Fabio Dias Real</field>
<field name="title">Smart Cameras: Fundamentals and Classifications</field>
<field name="keyword">Smart camera</field>
<field name="keyword"> machine vision</field>
<field name="keyword"> intelligent image processing</field>
<field name="keyword"> classification</field>
<field name="abstract">Since late 1990s smart cameras have gained significant popularity and market acceptance, especially in surveillance and machine vision industries. A smart camera is a vision system that can perform tasks far beyond simply taking photos and recording videos. Thanks to the purposely developed intelligent image processing and pattern recognition algorithms running on the increasingly powerful micro-processors, smart cameras can detect motion, measure objects, read vehicle number plates, and even recognize human behaviors. They are essential components for building active and automated control systems for many applications, and hold

great promise for being pervasive and intelligent sensors in the future. In this chapter we provide a technical definition of smart cameras, and discuss some of their fundamental aspects. A tentative classification of smart cameras is provided based on their system architectures. We also discuss in some details technologies that are necessary to build smart cameras, including image sensors and embedded processors. Several existing smart camera devices are presented as examples, and a review on the increasingly wider range of applications is given in the last section of the chapter.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1000</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Ursula Goltz</field>
<field name="author">Jens-Wolfhard Schicke-Uffmann</field>
<field name="title">On Distributability of Petri Nets (extended abstract)</field>
<field name="keyword">reactive systems</field>
<field name="keyword"> Petri nets</field>
<field name="keyword"> distributed systems</field>
<field name="keyword"> asynchronous interaction</field>
<field name="keyword"> equivalence notions</field>
<field name="abstract">We formalise a general concept of distributed systems as sequential

components interacting asynchronously. We define a corresponding

class of Petri nets, called LSGA nets, and precisely characterise

those system specifications which can be implemented as LSGA nets up

to branching ST-bisimilarity with explicit divergence.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1001</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="title">MOBIX: System for managing MOBility using Information eXchange</field>
<field name="abstract">We propose a different approach to determining network availability of mobile nodes which leverages on the fact that nodes on the move will meet other nodes who will be able to share conditions of networks they have recently encountered. This paper presents MOBIX, a system where nodes exchange information about network conditions using short-range communication such as Bluetooth. Our simulation results show that the required number of nodes needed for 100\% success is not unrealistic of densely populated metropolitan areas. Even with relatively low population densities, we can expect a data store hit more than 50\% of the time. Although our evaluation used WiFi, our scheme can easily be extended for other technologies such as GSM and WiMax.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1002</field>
<field name="author">Ansgar Fehnker</field>
<field name="author">Rena Bakhshi</field>
<field name="title">On the Impact of Modelling Choices for Distributed Information Spread -- A Comparative Study</field>
<field name="keyword">Model checking</field>
<field name="keyword"> PRISM</field>
<field name="keyword"> shuffling algorithm</field>
<field name="keyword"> PeerSim</field>
<field name="abstract">We consider a distributed shuffling algorithm for sharing data in a distributed network. Nodes executing the algorithm periodically contact each other, and exchange data. The behavior of the algorithm is probabilistic in nature; a node chooses a random peer, and sends a random subset of its local data. Moreover, the algorithm exhibits nondeterministic behavior; the order in which nodes initiate an exchange is not specified.



For the shuffling algorithm we build several formal models using the probabilistic model checker PRISM. Despite

the well known state-space explosion problem, we were able to model a network of up to 15 nodes. In addition, we implement two equational models in Matlab, a discrete model and its continuous alternative, as well as the algorithm itself in the peer-to-peer network simulator PeerSim. By comparing different modelling frameworks, we further explore the impact of modelling choices, such as different scheduling policies and the notion of rounds. The comparison of different models allowed us to discover hidden assumptions in these alternative modelling frameworks, which helps with the interpretation of the obtained results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1003</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Marc van Kreveld</field>
<field name="author">Giri Narasimhan</field>
<field name="title">Region-restricted clustering for geographic data mining</field>
<field name="keyword">computational geometry</field>
<field name="keyword"> approximation algorithms</field>
<field name="abstract">Clustering problems in a complex geographical setting are often required to incorporate the type and extent of land cover within a region. Given a set P of n points in a geographical setting, with the constraint that the points of P can only occur in one type of land cover, an interesting problem is the detection of clusters. First, we extend the definition of clusters and define the concept of a region-restricted cluster that satisfies the following properties: (i) the cluster has sufficient number of points, (ii) the cluster points are confined to a small geographical area, and (iii) the amount of land cover of the specific type in which the points lie is also small. Next, we give efficient exact and approximation algorithms for computing such clusters. The exact algorithm determines all axis-parallel squares with exactly m out of n points inside, size at most some prespecified value, and area of a given land cover type at most another prespecified value, and runs in O(nmlog^2n+(nm+nnf)log^2nf) time, where nf is the number of edges that bound the regions with the given land cover type. The approximation algorithm allows the square to be a factor 1+ too large, and runs in O(nlogn+n/ 2+nf log^2nf+(n log^2nf)/(m ^2)) time. We also show how to compute largest clusters and outliers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1004</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Michiel Smid</field>
<field name="title">On Spanners of Geometric Graphs</field>
<field name="keyword">Computational geometry</field>
<field name="keyword"> geometric networks</field>
<field name="abstract">Given a connected geometric graph G, we consider the problem of constructing a t-spanner of G having the minimum number of edges. We prove that for every real number t with 1 &lt; t &lt; 1/4*log n, there exists a connected geometric graph G with n vertices, such that every t-spanner of G contains Omega _(n^{1+1/t}) edges. This bound almost matches the known upper bound, which states that every connected weighted graph with n vertices contains a t-spanner with O(n^{1+2/(t _ 1)}) edges. We also prove that the problem of deciding whether a given geometric graph contains a t-spanner with at most K edges is NP-hard. Previously, this NP-hardness result was only known for non-geometric graphs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1005</field>
<field name="author">Jenny Liu</field>
<field name="author">Xin Liang</field>
<field name="author">Lingzhi Xu</field>
<field name="author">Mark Staples</field>
<field name="author">Mark Staples</field>
<field name="title">Using Architecture Integration Patterns to Composing Enterprise Mashups</field>
<field name="keyword">patterns</field>
<field name="keyword"> architecture</field>
<field name="keyword"> mashup</field>
<field name="keyword"> integration</field>
<field name="abstract">Enterprise mashups deal with cooperate data and various sources of information to compose new value-added applications. The architecture design of enterprise mashups encompasses integration issues - it needs to integrate heterogeneous data and/or compose new situational applications from existing infrastructures. We envisage that architecture integration patterns can be applied not only as architecture solutions to mashup development, but also to help us develop practical mashup techniques. In this paper, we combine several common architecture integration patterns, namely Pipes and Filters, Data Federation, and Model-View-Control to compose enterprise mashups. A number of techniques are also developed to customize these patterns for the specific mashup needs. We illustrate our approach by a property valuation service derived from the real-world setting.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1006</field>
<field name="author">Quincy Tse</field>
<field name="title">Improving Message Reception in VANETs</field>
<field name="keyword">VANET</field>
<field name="keyword"> Cooperative Forwarding</field>
<field name="keyword"> Slow fading</field>
<field name="keyword"> Shadowing</field>
<field name="keyword"> Geocasting</field>
<field name="abstract">Vehicular ad hoc networks (VANETs) enables various novel applications to improve the safety and the efficiency of the road transport system by allowing vehicles to communicate with each other and to the road infrastructure. In particular, cooperative collision avoidance (CCA) and cooperative collision warning (CCW) systems have been proposed as one of the next-generation safety systems. Both of these systems require reliable and timely delivery of messages to all vehicles within a geographic area.



However, the road presents a challenging environment for wireless communication, with complex multipath fading caused by metallic objects around each node, and large vehicles and buildings causing significant shadowing. This difficult radio channel, combined with highly dynamic node topology makes reliable communication difficult, severely limiting the usefulness of CCA and CCW.



In this research, I investigate the severity of shadowing caused by large vehicles on roads using physical measurements. In addition, a MAC/network-level cooperative geocasting retransmission algorithm will be developed in order to improve the reliability of the system for CCA and CCW applications.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1007</field>
<field name="author">Fu Zhouyu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">An Instance Selection Approach to Multiple Instance Learning</field>
<field name="abstract">Multiple-instance Learning (MIL) is a new paradigm

of supervised learning that deals with the classification of

bags. Each bag is presented as a collection of instances

from which features are extracted. In MIL, we have usu-

ally confronted with a large instance space for even mod-

erately sized data sets since each bag may contain many

instances. Hence it is important to design efficient instance

pruning and selection techniques to speed up the learning

process without compromising on the performance. In this

paper, we address the issue of instance selection in multiple

instance learning and propose the IS-MIL, an Instance Se-

lection framework for MIL, to tackle large-scale MIL prob-

lems. IS-MIL is based on an alternative optimisation frame-

work by iteratively repeating the steps of instance selec-

tion/updating and classi er learning, which is guaranteed

to converge. Experimental results demonstrate the utility

and efficiency of the proposed approach compared to the

alternatives.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1008</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Formation and evolution of the ionospheric plasma density shoulder and its relationship to the superfountain effects investigated during the 6 November 2001 great storm</field>
<field name="keyword">superfountain</field>
<field name="keyword"> mid-latitude shoulder</field>
<field name="keyword"> night-time Weddell Sea Anomaly</field>
<field name="abstract">This study investigates the 6 November 2001 great storm s impact on the topside

ionosphere utilizing data from the onboard TOPEX/Poseidon-NASA altimeter, Defense

Meteorological Satellite Program Special Sensor Ions, Electrons and Scintillation

instruments and ACE interplanetary observatory. A set of field-aligned profiles

demonstrate the storm evolution, caused by the precursor and promptly penetrating

interplanetary eastward electric (E) fields, and strong equatorward winds reducing

chemical loss, during the long-duration negative BZ events. At daytime-evening, the

forward fountain experienced repeated strengthening, as the net eastward E field suddenly

increased. The resultant symmetrical equatorial anomaly exhibited a continuous increase,

while the energy inputs at both auroral regions were similar. In both hemispheres, by

progressing poleward, a midlatitude shoulder exhibiting increased plasma densities, a

plasma-density dropoff (steep gradient) and a plasma depletion appeared. These features

were maintained while the reverse fountain operated. At the dropoff, elevated

temperatures indicated the plasmapause. Consequently, the plasma depletion was the

signature of plasmaspheric erosion. In each hemisphere, an isolated plasma flow,

supplying the minimum plasma, was detected at the shoulder. Plasmaspheric compression,

due to the enhanced E fields, could trigger this plasma flow. Exhibiting strong longitudinal

variation at evening-nighttime, the shoulder increased 306% over the southeastern

Pacific, where the nighttime Weddell Sea Anomaly (WSA) appeared before the storm.

There, the shoulder indicated the storm-enhanced equatorward section of the quiet time

WSA. Owing to the substantial equatorward plasmapause movement, a larger

poleward section of the quiet time WSA eroded away, leaving a large depletion behind.

This study reports first these (northern, southern) plasma flows and dramatic storm effects

on a nighttime WSA.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1009</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Distinctive plasma density features of the topside ionosphere and their electrodynamics investigated during southern winter</field>
<field name="keyword">electrodynamics</field>
<field name="keyword"> SAMA</field>
<field name="keyword"> plasma density features</field>
<field name="abstract">This study utilizes a novel technique to map the Defense Meteorological Satellite

Program (DMSP) data across the two hemispheres to learn about the morphology and

plasma composition of the topside ionosphere, and the underlying ionospheric dynamics.

In the southern winter hemisphere, the regional maps tracked a heavy-ion (Ni-O+) trough,

aurora zone, polar hole, and large plasma density depletion. The latter appeared in the

region of the South Atlantic Magnetic Anomaly (SAMA). The electron temperature (Te)

map detected the thermal characteristics of these features, while the plasma drifts and flux

maps tracked their dynamics. Results show that there were special electrodynamic

effects in the SAMA region due to the low magnetic field and high conductivity. These

increased the vertical downward (VZ) and the westward (VY) drifts. Independently, the VZ

and VY maps registered the affected area that was depleted in heavy ions and rich in light

ions. Some field-aligned profiles tracked the impact of these SAMA effects on the

heavy-ion trough, which was a stagnation trough and appeared markedly differently at

different longitudes. At trough latitudes ((56 4)&#1;S (geomagnetic) when Dstav = 0 nT), the

elevated electron temperatures forming a Te peak indicated subauroral heating effects. A

statistical study modeled the magnetic activity dependence of the Te peak s magnitude and

location and revealed their linear correlation with the activity level. Statistically, the Te

peak increased [10.226 1.355]&#1;K and moved equatorward [0.051 0.009]&#1;

(geomagnetic) per 1 nT decrease in the averaged Dst index. Per 1 nT increase in the

averaged AE index, its magnitude increased [1.315 0.444]&#1;K and the equatorward

movement was [0.014 0.003]&#1;.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1010</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Investigating the relationships among the South Atlantic Magnetic Anomaly, southern nighttime midlatitude trough, and nighttime Weddell Sea Anomaly during southern summer</field>
<field name="keyword">electrodynamics of the SAMA</field>
<field name="keyword"> southern mid-latitude trough</field>
<field name="keyword"> night-time WSA</field>
<field name="abstract">This study utilized the multi-instrument data of the Defense Meteorological Satellite

Program to investigate the evening/nighttime topside ionosphere during the 1996/1997

southern summer. A series of regional surface maps were constructed and permitted the

tracking of the topside ionosphere s plasma density features, plasma composition,

thermal structures, and vertical and horizontal plasma flows. These maps tracked a

complete nighttime Weddell Sea Anomaly (WSA) and strong horizontal plasma flows that

registered the high-conductivity regions of the South Atlantic Magnetic Anomaly

(SAMA). These regions developed over the southeastern Pacific, just equatorward of the

WSA, and over the South Atlantic. A heavy-ion stagnation trough developed poleward

of the SAMA affected regions. Thus, the trough appeared on the WSA s equatorward

side. During periods of increasing magnetic activity, the plasmapause was the WSA s

poleward boundary. A statistical study modeled the trough s magnetic activity dependence

and revealed a strong east-west hemispherical difference that was due to the SAMA

effects. When the AE6 was 0 nT, the trough appeared at (57.49 2.82)&#1;S (geomagnetic)

over the southwestern hemisphere. Owing to the SAMA s special electrodynamic effects,

the trough developed at lower latitudes, (42.39 3.04)&#1;S, over the southeastern

hemisphere. Meanwhile, the plasmapause occurred at &#1;(62.5 4)&#1;S, and the WSA s peak

appeared at &#1;(56.2 4)&#1;S. Hence, there was a &#1;20&#1; (lat) separation between the trough

and the plasmapause over the southeastern hemisphere. Between 210&#1;E and 330&#1;E

(geographic), the WSA filled this gap. With increasing magnetic activity, the trough in the

SAMA affected regions moved poleward at a rate of (0.0157 0.004)&#1;S/nT. Elsewhere, it

had a (0.0196 0.002)&#1;S/nT equatorward movement.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1011</field>
<field name="author">Abdolhossein Sarrafzadeh</field>
<field name="author">Samuel Alexander</field>
<field name="author">Farhad Dadgostar</field>
<field name="author">Chao Fan</field>
<field name="author">Abbas Bigdeli</field>
<field name="title">How do you know that I don t understand? A look at the future of intelligent tutoring systems</field>
<field name="keyword">Affective tutoring systems; Lifelike agents; Emotion detection; Facial expressions; Human</field>
<field name="keyword">computer</field>
<field name="abstract">Many software systems would significantly improve performance if they could adapt to the emotional state of the user, for example if Intelligent Tutoring Systems (ITSs), ATM s, ticketing machines could recognise when users were confused, frustrated or angry they could guide the user back to remedial help systems so improving the service. Many researchers now feel strongly that ITSs would be significantly enhanced if computers could adapt to the emotions of students. This idea has spawned the developing field of affective tutoring systems (ATSs): ATSs are ITSs that are able to adapt to the affective state of students. The term affective tutoring system can be traced back as far as Rosalind Picard s book Affective Computing in 1997.



This paper presents research leading to the development of Easy with Eve, an ATS for primary school mathematics. The system utilises a network of computer systems, mainly embedded devices to detect student emotion and other significant bio-signals. It will then adapt to students and displays emotion via a lifelike agent called Eve. Eve s tutoring adaptations are guided by a case-based method for adapting to student states; this method uses data that was generated by an observational study of human tutors. This paper presents the observational study, the case-based method,the ATS itself and its implementation on a distributed computer systems for real-time performance,and finally the implications of the findings for Human Computer Interaction in generaland e-learning in particular. Web-based applications of the technology developed in this research are discussed throughout the paper.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1012</field>
<field name="author">Tharmarajah Thiruvaran</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Analysis of band structures for speaker-specific information in FM feature extraction</field>
<field name="keyword">Speaker recognition</field>
<field name="keyword"> frequency modulation</field>
<field name="keyword"> scatter matrix</field>
<field name="abstract">Frequency modulation (FM) features are typically extracted using a filterbank, usually based on an auditory frequency scale, however there is psychophysical evidence to suggest that this scale may not be optimal for extracting speaker specific information. In this paper, speaker-specific information in FM features is analyzed as a function of the filterbank structure at the feature, model and classification stages. Scatter matrix based separation measures at the feature level and Kullback-Leibler distance based measures at the model level are used to analyze the discriminative contributions of the different bands. Then a series of speaker recognition experiments are performed to study how each band of the FM feature contributes to speaker recognition. A new filter bank structure is proposed that attempts to maximize the speaker-specific information in the FM feature for telephone data. Finally, the distribution of speaker-specific information is analyzed for wideband speech.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1013</field>
<field name="author">Vidhyasaharan Sethu</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Pitch Contour Parameterisation based on Linear Stylisation for Emotion Recognition</field>
<field name="keyword">Pitch stylisation</field>
<field name="keyword"> emotion classification</field>
<field name="keyword"> prosodic features</field>
<field name="keyword"> speech synthesis</field>
<field name="abstract">The pitch contour contains information that characterises the emotion being expressed by speech, and consequently features extracted from pitch form an integral part of many automatic emotion recognition systems. While pitch contours may have many small variations and hence are difficult to represent compactly, it may be possible to parameterise them by approximating the contour for each voiced segment by a straight line. This paper looks at such a parameterisation method in the context of emotion recognition. Listening tests were performed to subjectively determine if the linearly stylised contours were able to sufficiently capture information pertaining to emotions expressed in speech. Furthermore these parameters were used as features for an automatic 5-class emotion classification system. The use of the proposed parameters rather than pitch statistics resulted in a relative increase in accuracy of about 20%.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1014</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">An investigation of the northern-hemisphere mid-latitude night-time plasma density enhancements and their relations to the mid-latitude night-time trough during summer</field>
<field name="keyword">mid-latitude trough</field>
<field name="keyword"> northern mid-latitude night-time plasma enhancements</field>
<field name="keyword"> plasmasphere-ionosphere coupling</field>
<field name="abstract">This study has utilized regional surface maps and field-aligned latitudinal profiles derived from multi-instrument DMSP-F15 data. It investigates northern mid-latitude plasma enhancements in the night-time topside ionosphere during summer and their relation to the mid-latitude heavy-ion stagnation trough. The tracked mid-latitude summer enhancements showed significant longitudinal variations. Their best development occurred in the 50oN-60oN/110oE-170oE (geographic) region. Proven by CTIP wind velocity simulations, equatorward winds were strongest there. This region is the northern-hemisphere equivalent of the Weddell Sea Anomaly s locality. There, northern summer plasma enhancements resembled to the night-time WSA, a southern summer-equinoctial phenomenon, and thus, were named as WSA-like feature. Their plasma density increased mainly due to the mechanical or direct effects of the equatorward winds. However, as results show, the WSA-like feature was an ordinary northern mid-latitude night-time enhancement occurring in summer. This was further confirmed by the WSA-like feature s relation to the trough and by the trough s movement. Over the northern hemisphere, the summer trough appeared at (62.461 2.93)oN (geomagnetic), poleward of the WSA-like feature and other mid-latitude plasma enhancements, and moved equatorward with a rate of (0.097 0.04)oN/nT when magnetic activity increased. As our previous WSA investigation indicates, the southern summer trough develops at lower latitudes, at (47.1564 2.16)oS, equatorward of the night-time WSA, and moves poleward with a rate of (0.0497 0.003)oS/nT with increasing magnetic activity under the combined influence of the night-time WSA and South Atlantic Magnetic Anomaly.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1015</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="author">Abdul Sattar</field>
<field name="title">A Defeasible Logic for Modelling Policy-based Intentions and Motivational Attitudes</field>
<field name="keyword">defeasible logic</field>
<field name="keyword"> policy based intention</field>
<field name="keyword"> logical omniscience</field>
<field name="abstract">In this paper we show how defeasible logic could formally

 account for the non-monotonic properties involved in motivational

 attitudes like intention and obligation. Usually,

 \emph{normal} modal operators are used to represent such attitudes

 wherein classical logical consequence and the rule of necessitation

 comes into play, i.e., $\vdash A / \vdash \Box A$, that is from

 $\vdash A$ derive $\vdash\Box A$. This means that

 such formalisms are affected by the Logical Omniscience

 problem. We show that policy-based intentions

 exhibit non-monotonic behaviour which could be captured through a

 non-monotonic system like defeasible logic. To this end we outline a

 defeasible logic of intention that specifies how modalities can be

 introduced and manipulated in a non-monotonic setting without giving

 rise to the problem of logical omniscience. In a similar way we show

 how to add deontic modalities defeasibly and how to integrate them

 with other motivational attitudes like beliefs and goals. Finally we

 show that the basic

 aspect of the BOID architecture is captured by this

 extended framework.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1016</field>
<field name="author">Guido Governatori</field>
<field name="author">Duy Hoang Pham</field>
<field name="title">DR-CONTRACT: An Architecture for e-Contracts in Defeasible Logic</field>
<field name="keyword">defeasible logic</field>
<field name="keyword"> RuleML</field>
<field name="keyword"> Contract Architecture and Language</field>
<field name="abstract">We introduce the DR-CONTRACT architecture to represent and reason on

 e-Contracts. The architecture extends the DR-device architecture by

 a deontic defeasible logic of violation. We motivate the choice for

 the logic and we show how to extend RuleML to capture the notions

 relevant to describe e-contracts for a monitoring perspective in

 Defeasible Logic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1017</field>
<field name="author">Ehsan Norouznezhad</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Adam Postula</field>
<field name="author">Brian C. Lovell</field>
<field name="title">A high resolution smart camera with GigE Vision extension for surveillance applications</field>
<field name="keyword">Intelligent Video Surveillance</field>
<field name="keyword"> Smart Camera</field>
<field name="keyword"> GigE Vision</field>
<field name="abstract">Intelligent video surveillance is currently a hot topic in computer vision research. The goal of intelligent video surveillance is to process the captured video from the monitored area, extract specific information and take appropriate action based on that information. Due to the high computational complexity of vision tasks and the realtime nature of these systems, current software-based intelligent video surveillance systems are unable to perform sophisticated operations. Smart cameras are a key component for future intelligent surveillance systems. They use embedded processing to offload computationally intensive vision tasks from the host processing computers and increasingly reduce the required communication bandwidth and data flows over the network. This paper reports on the design of a high resolution smart camera with a GigE Vision extension for automated video surveillance systems. The features of the new camera interface standard, GigE Vision will be introduced and its suitability for video surveillance systems will be described. The surveillance framework for which the GigE Vision standard has been developed is presented as well as a brief overview of the proposed smart camera.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1018</field>
<field name="author">Ting Shan</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Brian C. Lovell</field>
<field name="author">Shaokang Chen</field>
<field name="title">Robust Face Recognition Technique for a Real-Time Embedded Face Recognition System</field>
<field name="abstract">In this chapter, we propose a pose variability compensation technique, which synthesizes realistic fron tal face images from nonfrontal views. It is based on modeling the face via active appearance models and estimating the pose through a correlation model. The proposed technique is coupled with adaptive principal component analysis (APCA), which was previously shown to perform well in the presence of both lighting and expression variations. The proposed recognition techniques, though advanced, are not computationally intensive. So they are quite well suited to the embedded system environment. Indeed, the authors have implemented an early prototype of a face recognition module on a mobile camera phone so the camera can be used to identify the person holding the phone.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1019</field>
<field name="author">Shaokang Chen</field>
<field name="author">Erik Berglund</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Experimental Analysis of Face Recognition on Still and CCTV images</field>
<field name="abstract">Although automatic identity inference based on faces has shown success when using high quality images, for CCTV

based images it is hard to attain similar levels of performance. Furthermore, compared to recognition based on

static images, relatively few studies have been done for video based face recognition. In this paper, we present an empirical analysis and comparison of face recognition using high quality and CCTV images in several important aspects: image quality (including resolution, noise, blurring and interlacing) as well as geometric transformations (such as translations, rotations and scale changes). The results show that holistic face recognition can be tolerant to image quality degradation but can also be highly influenced by geometric

transformations. In addition, we show that camera intrinsics have much influence when using different cameras

for collecting gallery and probe images the recognition rate is considerably reduced. We also show that the

classification performance can be considerably improved by straightforward averaging of consecutive face images

from a CCTV video sequence.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1020</field>
<field name="author">Shaokang Chen</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Sai Sun</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Representative Feature Chain for Single Gallery Image Face Recognition</field>
<field name="abstract">Under the constraint of using only a single gallery image per person, this paper proposes a fast multi-class pattern classification approach to 2D face recognition robust to changes in pose, illumination, and expression (PIE). This work has three main contributions: (1) we propose a representative face space method to extract robust features,(2) we apply a learning method to weight features in pairs,(3) we combine the feature pairs into a feature chain in order to find the weights for all features. The approach is evaluated

for face recognition under PIE changes on three public databases. Results show that the method performs considerably better than several other appearance-based methods and can reliably recognise faces at large pose angles without the need for fragile pose estimation pre-processing. Moreover, computational load is low (comparable to standard eigenface methods), which is a critical factor in wide-area surveillance applications.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1021</field>
<field name="author">Brian C. Lovell</field>
<field name="author">Shaokang Chen</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Erik Berglund</field>
<field name="author">Conrad Sanderson</field>
<field name="title">On Intelligent Surveillance Systems and Face Recognition for Mass Transport Security</field>
<field name="abstract">We describe a project to trial and develop enhanced surveillance technologies for public safety. A key technology is robust recognition of faces from low-resolution CCTV footage where there may be as few as 12 pixels between the eyes. Current commercial face recognition systems require 60-90 pixels between the eyes as well as tightly controlled image capture conditions. Our group has thus concentrated on fundamental face recognition issues such as robustness to low resolution and image capture conditions as required for uncontrolled CCTV surveillance. In this paper, we propose a fast multi-class pattern classification approach to enhance PCA and FLD methods for 2D face recognition under changes in pose, illumination, and expression. The method first finds the optimal weights of features pairwise and constructs a feature chain in order to determine the weights for all features. Computational load of the proposed approach is extremely low by design, in order to facilitate usage in automated surveillance. The method is evaluated on the PIE, FERET, and Asian Face databases, with the results showing that the method performs remarkably well compared to several benchmark appearance-based methods. Moreover, the method can reliably recognize faces with large pose angles from just one gallery image.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1022</field>
<field name="author">Vamsi Krishna Madasu</field>
<field name="author">Brian C. Lovell</field>
<field name="title">An Automatic Off-line Signature Verification and Forgery Detection System</field>
<field name="abstract">This chapter presents an off-line signature verification and forgery detection system based on fuzzy mod eling. The various handwritten signature characteristics and features are first studied and encapsulated to devise a robust verification system. The verification of genuine signatures and detection of forgeries is achieved via angle features extracted using a grid method. The derived features are fuzzified by an exponential membership function, which is modified to include two structural parameters. The structural parameters are devised to take account of possible variations due to handwriting styles and to reflect other factors affecting the scripting of a signature. The efficacy of the proposed system is tested on a large database of signatures comprising more than 1,200 signature images obtained from 40 volunteers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1023</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Investigating the relation of the southern daytime mid-latitude trough with the daytime Weddell Sea Anomaly during equinoxes</field>
<field name="keyword">southern mid-latitude trough</field>
<field name="keyword"> daytime WSA</field>
<field name="keyword"> upwelling events</field>
<field name="abstract">A detailed study with statistical investigations was conducted on the morning topside ionosphere for the 1996/97 equinoctial period utilizing DMSP data and CTIP wind simulations. Highly structured plasma densities, including a daytime Weddell Sea Anomaly (WSA; ~9LT) over the South-Eastern Pacific and localized high-latitude plasma enhancements (~8LT) in the Australian sector, were tracked. Poleward and equatorward winds respectively were strongest there. High-resolution regional surface maps and field-aligned profiles tracked their thermal structures, O+ and H+ composition, vertical drifts (VZ, m/s) and horizontal plasma flows {FY; [i+/(cm2s)]}. A daytime heavy-ion (Ni-O+) stagnation trough [(54.5 4.5)oS; geomagnetic] appeared equatorward of both the WSA [60.0 4.0)oS (centre), 185oE-310oE; geographic] and these enhancements [&gt;(62.0 3.5)oS; 30oE-180oE]. Meanwhile, the plasmapause developed poleward of the WSA and equatorward of these enhancements. Plasma convection became significantly increased by enhanced electron temperatures nearby upflows underlying localized plasma enhancements. Upflow velocities, modeled as ~694.5 m/s, shows close similarities to the actual direct measurements of ~700 m/s. Both slow and enhanced plasma convections promoted stagnation events and trough formation. However, regular trough features became significantly altered by the development of a downward dipping poleward wall in the WSA region and by the absence of regular trough walls in the region of localized plasma enhancements. Statistically, the trough- and localized plasma enhancement-locations varied linearly with magnetic activity and showed an equatorward movement with increasing activity. A rate of (0.026 0.002)oS/nT for the trough and (0.067 0.003)oS/nT for the localized plasma enhancements were modeled. This higher (~2x) rate of movement indicates the localized plasma enhancements more responsiveness.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1024</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Large-scale travelling ionospheric disturbances impacting equatorial ionization anomaly development in the local morning hours of the Halloween Superstorms on 29-30 October 2003</field>
<field name="keyword">large-scale travelling ionospheric disturbances</field>
<field name="keyword"> equatorial anomaly-like features</field>
<field name="keyword"> storm-generated equatorward wind surges</field>
<field name="abstract">This study investigates the development of EIA-like features during the Halloween Superstorms. These features are similar to a well-developed equatorial ionospheric anomaly (EIA) and therefore suggest superfountain effects. We tracked EIA-like features in the early morning sector of 29-30 October 2003 and at around midday on 30 October. Their plasma environment was studied with field-aligned plasma density, vertical plasma drift, electron temperature and vertical plasma flow profiles. CTIP simulations reproduced storm-generated equatorward wind surges. When these EIA-like features appeared early morning on 29-30 October, there was no forward fountain circulation, only large-scale travelling ionospheric disturbances (TIDs) and strong equatorward winds were present. These provide observational evidence that large-scale TIDs created large plasma depletions over the dip equator and with the aid of equatorward winds some large enhancements at ~ 30oN (geomagnetic) resulting in the development of early morning EIA-like features. We identified TID signatures in the equatorial and mid-latitude ionosonde and magnetometer data, and studied EIA-like features at around midday on 30 October utilizing non-field-aligned TOPEX TEC (2000UT) and published CHAMP TEC (2030UT; 2200UT) line plots. According to our analysis, the EIA-like features seen in the TEC data were created by large-scale TIDs at 2000UT and 2030UT, and by the combined effects of large-scale TIDs and forward superfountain at 2200UT. CTIP simulations demonstrate the crucial role of the neutral winds mechanical or direct effects in the development of high plasma densities at low mid-latitudes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1025</field>
<field name="author">M. Hanmandlu</field>
<field name="author">S. Susan</field>
<field name="author">V.K. Madasu</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Fuzzy Co-Clustering of Medical Images using Bacterial Foraging</field>
<field name="keyword">Fuzzy Clustering</field>
<field name="keyword"> Validity Measure</field>
<field name="keyword"> Colour Segmentation</field>
<field name="keyword"> Co-Clustering and Bacteria Foraging</field>
<field name="abstract">A novel modification of the Fuzzy Clustering for Categorical Multivariate date (FCCM) algorithm termed as

'Fuzzy Co-Clustering Algorithm for Images (FCCI) , is proposedfor clustering ofmedical images. The main aim

ofthis work is to segment regions ofinterest in histo-pathological images which consist ofgroups ofsimilar cells indicating some form of abnormality in the animal tissue. The proposed method relies on improved colour

clustering results when FCCI is applied on images as compared to the conventional clustering techniques. The

method also categorizes different types of lesions based on the co-clustering results. The objective function is optimized using the bacterialforaging algorithm which gives image specific values to the parameters involved in the algorithm. The colour segmentation results are found to be more accurate, producing wellformed and valid clusters having "crisp' values ofmembership function with lesser number ofiterations. The algorithm results in distinct co-clusters ranked in the order oftheir memberships.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1026</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">How Do Agents Comply with Norms?</field>
<field name="keyword">compliance</field>
<field name="keyword"> Business Process</field>
<field name="keyword"> agents</field>
<field name="keyword"> formal contract logic</field>
<field name="abstract">The import of the notion of institution in the design of MASs

 requires to develop formal and efficient methods for modeling the

 interaction between agents' behaviour and normative systems. This

 paper discusses how to check whether agents' behaviour is compliant

 with the rules regulating them. The key point of our approach is

 that compliance is a relationship between two sets of

 specifications: the specifications for executing a process and the

 specifications regulating it. We propose a logic-based formalism for

 describing both the semantics of normative specifications and the

 semantics of compliance checking procedures.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1027</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">MILIS: Multiple Instance Learning with Instance Selection</field>
<field name="abstract">Multiple-instance learning (MIL) is a novel paradigm of supervised learning that deals with the classification of bags. Each bag is presented as a collection of instances from which features are extracted. We usually confront with a large instance space even for moderately sized data sets in MIL, because each bag may contain many instances. Hence, it is important to design efficient instance pruning and selection techniques to speed up the training process without compromising on the performance. In this paper, we address the issue of instance selection in MIL. We propose MILIS, a novel MIL algorithm based on adaptive instance selection. We do this in an EM-like fashion by intertwining the steps of instance selection and classifier learning in an iterative manner, which is guaranteed to converge. Initial instance selection is achieved by a simple yet effective kernel density estimator on the negative instances. Experimental results demonstrate the utility and efficiency of the proposed approach as compared to the state-of-the-art.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1028</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Discriminant Absorption Feature Learning for Material Classification</field>
<field name="abstract">In this paper, we develop a novel approach to object material identification in spectral imaging by combining the use of invariant spectral absorption features and statistical machine learning techniques. Our method hinges in the relevance of spectral absorption features for material identification and casts the problem into a pattern recognition setting by making use of an invariant representation of the most discriminant band-segments in the spectra. Thus, here, we view the identification problem as a classification task, which is effected based upon those invariant absorption bands in the spectra which are most discriminative between the materials under study. To robustly recover those bands that are most relevant to the identification process, we make use of discriminant learning. To illustrate the utility of our method for purposes of material identification, we perform experiments on both terrestrial and remotely sensed hyperspectral imaging data and compare our results to those yielded by an alternative.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1029</field>
<field name="author">Eric Fabre</field>
<field name="author">Loig Jezequel</field>
<field name="author">Patrik Haslum</field>
<field name="author">Sylvie Thiebaux</field>
<field name="title">Cost-Optimal Factored Planning: Promises and Pitfalls</field>
<field name="keyword">automated planning</field>
<field name="keyword"> factorisation</field>
<field name="keyword"> distributed planning</field>
<field name="keyword"> weighted automata</field>
<field name="abstract">Factored planning methods aim to exploit locality to efficiently

solve large but loosely coupled planning problems

by computing solutions locally and propagating limited information

between components. However, all factored planning

methods presented so far work with representations that require

certain parameters to be bounded (e.g. number of coordination

points between local plans considered); the satisfaction

of those bounds by a given problem instance is difficult

to establish a priori, and the influence of those parameters on

the problem complexity is unclear. We present an instance of

the factored planning framework using a representation of the

(regular) sets of local plans by finite automata, which does not

require any such bound. By substituting weighted automata,

we can even do factored cost-optimal planning. We test an

implementation of the method on the few standard planning

benchmarks that we have found to be amenable to factoring.

We show that this method runs in polynomial time under conditions

similar to those considered in previous work, but not

only under those conditions. Thus, what constitutes an essential

measure of factorability remains obscure.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1030</field>
<field name="author">Fuyuki Ishikawa</field>
<field name="author">Basem Suleiman</field>
<field name="author">Kayoko Yamamoto</field>
<field name="author">Shinichi Honiden</field>
<field name="title">Physical Interaction in Pervasive Computing: Formal Modeling, Analysis and Verification</field>
<field name="keyword">Pervasive Computing</field>
<field name="keyword"> Formal Specification</field>
<field name="keyword"> Event Calculus</field>
<field name="abstract">Application software in pervasive computing is required to control devices embedded in the environment by being aware of the contexts on which e&#27;ectiveness of the devices depend. Developers face di&#30;culties to enumerate involved physical prerequisites for e&#27;ective use of devices and undesirable situations to be avoided, as well as de&#28;ne consistent behaviors of the application software. This study provides a theoretical framework for formal modeling of requirements, assumptions and behaviors for application software in pervasive computing. This study speci&#28;cally focuses on prerequisites for physical (visual, audio, etc.) interactions, which are de&#28;ned and examined in terms of scopes and their relationships not limited to tree structures. This study also explores analysis and veri&#28;cation based on the formal modeling, using of an existing reasoner.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1031</field>
<field name="author">Philip Kilby</field>
<field name="title">Analysing the future of traffic state technology</field>
<field name="keyword">Traffic Control</field>
<field name="keyword"> Traffic State Modelling</field>
<field name="abstract"/>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1032</field>
<field name="author">Thibaut Feydy</field>
<field name="author">Peter Stuckey</field>
<field name="title">Lazy clause generation reengineered</field>
<field name="keyword"/>
<field name="abstract">Lazy clause generation is a powerful hybrid approach to com- 

binatorial optimization that combines features from SAT solving and - 

nite domain (FD) propagation. In lazy clause generation nite domain 

propagators are considered as clause generators that create a SAT de- 

scription of their behaviour for a SAT solver. The ability of the SAT 

solver to explain and record failure and perform con ict directed back- 

jumping are then applicable to FD problems. The original implementa- 

tion of lazy clause generation was constructed as a small nite domain 

propagation engine inside a SAT solver. In this paper we show how to 

engineer a lazy clause generation solver by embedding a SAT solver in- 

side an FD solver. The resulting solver is exible, e cient and easy to 

use. We give experiments illustrating the e ect of di erent design choices 

in engineering the solver.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1033</field>
<field name="author">Shai Haim</field>
<field name="author">Toby Walsh</field>
<field name="title">Restart Strategy Selection using Machine Learning Techniques</field>
<field name="abstract">Restart strategies are an important factor in the performance

of conflict-driven Davis Putnam style SAT solvers. Selecting a

good restart strategy for a problem instance can enhance the performance

of a solver. Inspired by recent success applying machine learning

techniques to predict the runtime of SAT solvers, we present a method

which uses machine learning to boost solver performance through a smart

selection of the restart strategy. Based on easy to compute features, we

train both a satisfiability classifier and runtime models. We use these

models to choose between restart strategies. We present experimental

results comparing this technique with the most commonly used restart

strategies. Our results demonstrate that machine learning is effective in

improving solver performance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1034</field>
<field name="author">Asad Amir Pirzada</field>
<field name="author">Marius Portmann</field>
<field name="author">Ryan Wishart</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">SafeMesh: A Wireless Mesh Network Routing Protocol for Incident Area Communications</field>
<field name="keyword">Wireless mesh network</field>
<field name="keyword"> Crisis management</field>
<field name="keyword"> Incident area communications</field>
<field name="abstract">Reliable broadband communication is becoming increasingly important during disaster recovery and emergency response operations. In situations where infrastructure-based communication is not available or has been disrupted, an Incident Area Network needs to be dynamically deployed, i.e. a temporary network that provides communication services for efficient crisis management at an incident site. Wireless Mesh Networks (WMNs) are multi-hop wireless networks with self-healing and self-configuring capabilities. These features, combined with the ability to provide wireless broadband connectivity at a comparably low cost, make WMNs a promising technology for incident management communications. This paper specifically focuses on hybrid WMNs, which allow both mobile client devices as well as dedicated infrastructure nodes to form the network and provide routing and forwarding functionality. Hybrid WMNs are the most generic and most flexible type of mesh networks and are ideally suited to meet the requirements of incident area communications. However, current wireless mesh and ad-hoc routing protocols do not perform well in hybrid WMN, and are not able to establish stable and high throughput communication paths. One of the key reasons for this is their inability to exploit the typical high degree of heterogeneity in hybrid WMNs. SafeMesh, the routing protocol presented in this paper, addresses the limitations of current mesh and ad-hoc routing protocols in the context of hybrid WMNs. SafeMesh is based on the well-known AODV routing protocol, and implements a number of modifications and extensions that significantly improve its performance in hybrid WMNs. This is demonstrated via an extensive set of simulation results. We further show the practicality of the protocol through a prototype implementation and provide performance results obtained from a small-scale testbed deployment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1035</field>
<field name="author">Basem Suleiman</field>
<field name="title">Models and Algorithms for Business Value-Driven Adaptation of Business Processes and Software Infrastructure</field>
<field name="keyword">Business Process Adaptation</field>
<field name="keyword"> Business Value Metrics</field>
<field name="keyword"> Software Infrastructure</field>
<field name="keyword"> Analysis and Decision-Making</field>
<field name="abstract">This research investigates how to provide automated analysis and decision-making support for adaptation of business processes and underlying software infrastructure, in the way that both maximizes business value metrics (e.g., profit, return on investment) and maintains alignment between business strategies and adaptation decisions. Among its expected contributions are improved modeling of business value metrics and

business strategies in business process models and novel business value-driven techniques and algorithms

that support primarily automatic and dynamic (runtime)adaptations, but also manual and static (designtime)

adaptations. The proposed solutions will be implemented in software prototypes to demonstrate feasibility.

Their usefulness will be evaluated through realistic case studies.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1036</field>
<field name="author">Karen Kua</field>
<field name="author">Julien Epps</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">LS Regularization of Group Delay Features for Speaker Recognition</field>
<field name="keyword">Speaker recognition</field>
<field name="keyword"> group delay</field>
<field name="keyword"> least squares regularization</field>
<field name="abstract">Due to the increasing use of fusion in speaker recognition systems, features that are complementary to MFCCs offer opportunities to advance the state of the art. One promising feature is based on group delay, however this can suffer large variability due to its numerical formulation. In this paper, we investigate reducing this variability in group delay features with least squares regularization. Evaluations on the NIST 2001 and 2008

SRE databases show a relative improvement of at least 6% and 18% EER respectively when group delay-based system is fused with MFCC-based system.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1037</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Colin Sim</field>
<field name="author">Morteza Biglari-Abhari</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Face Detection on Embedded Systems</field>
<field name="abstract">Over recent years automated face detection and recognition (FDR) have gained significant attention from the commercial and research sectors. This paper presents an embedded face detection solution aimed at addressing the real-time image processing requirements within a wide range of applications. As face detection is a computationally intensive task, an embedded solution would give rise to opportunities for discrete economical devices that could be applied and integrated into a vast majority of applications. This work focuses on the use of FPGAs as the embedded prototyping technology where the thread of execution is carried out on an embedded soft-core processor. Custom instructions have been utilized as a means of applying software/hardware partitioning through which the computational bottlenecks are moved to hardware. A speedup by a factor of 110 was achieved from employing custom instructions and software optimizations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1038</field>
<field name="author">Kelvin Cheng</field>
<field name="author">Masa Takatsuka</field>
<field name="title">Interaction Paradigms for Bare-Hand Interaction with Large Displays at a Distance</field>
<field name="abstract">Large displays are now more widely used than ever before, yet the computer mouse remains the most common input tool for large displays users. The main drawback of the mouse is that it is only a tool for mapping hand movements to on-screen cursor movements, for the manipulation of on-screen objects. Such an indirect mapping is due to differences between input space (usually a horizontal table used by the mouse) and output space (display) (Hinckley, 2003). This indirectness enlarges the Gulf of Execution and the Gulf of Evaluation (Norman, 1986), and thus reduces users freedom and efficiency.

Much effort has been made on making this interaction more natural and more intuitive for users. The use of computer vision for this purpose has been well researched as it provides freedom and mobility to the users and allows them to interact at a distance. However, while previous works seemingly employ a similar interaction model, the actual model itself has not been well researched.

This chapter investigates the underlying interaction models for bare-hand interaction with large display systems from a distance. It also looks into the feasibility of using low-cost monocular computer vision to implement such a system. We summarise important work historically and investigate significant related research in this area and take a close look at the current trend in immersive interaction and the state of the art techniques that researchers around the world have proposed to improve interactions with large surfaces.

Theories and techniques that make interaction with computer display as easy as pointing to real world objects are also explored. We present studies that were conducted to investigate the way human point at objects naturally with their hand and examine the inadequacy in existing pointing systems. In order to understand the reason for this difference and to understand the mechanism of hand pointing with respect to human-computer interaction, we introduce and formalize several models that underpin the pointing strategies observed in our experiments, as well as those used in many of the previous interactive systems (the Point, Touch and dTouch models).

The dTouch model will be presented to be one of the best targeting strategies studied. The eye-fingertip method is a pointing technique that uses the dTouch model to allow direct, unobtrusive and accurate interaction from a distance. To demonstrate the use of the dTouch method, we propose one such system using the eye-fingertip method as well as monocular vision. We call this the dTouch pointing system . By taking into account the location of the user and the interaction area available, a dynamic virtual touchscreen can be estimated between the display and the user. This allows a more consistent interaction area from the user s point of view. 

We also present an implementation of our pointing system in a proof-of-concept prototype. We detail the process and mathematics involved in estimating the eye and fingertip position, thereby determining the user s pointing position on-screen. In addition, we also present results which demonstrate the accuracy of our proof-of-concept prototype.

Results from our investigation suggested that it is possible to allow natural user interaction with large displays using low-cost monocular computer vision. Furthermore, models developed and lessons learnt can assist designers to develop more accurate and natural interactive systems that make use of human s natural pointing behaviours.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1039</field>
<field name="author">Anthony Collins</field>
<field name="author">Anastasia Bezerianos</field>
<field name="author">Gregor Mcewan</field>
<field name="author">Markus Rittenbruch</field>
<field name="author">Rainer Wasinger</field>
<field name="author">Judy Kay</field>
<field name="title">Understanding File Access Mechanisms for Embedded Ubicomp Collaboration Interfaces</field>
<field name="keyword">Tabletop interface</field>
<field name="keyword"> single display groupware</field>
<field name="keyword"> le system UI</field>
<field name="abstract">The ubiquitous computing vision includes Single Display

Groupware (SDG) devices embedded in our normal envi-

ronment. This paper explores the nature of interfaces for

supporting people in accessing their les on such displays.

To do this, we designed a study comparing people s interac-

tion with two very different classes of tabletop le system

access interface: Focus explicitly designed for tabletops and

the familiar hierarchical Windows Explorer. We designed

a within-subjects double-crossover study where participants

collaboratively completed 4 planning tasks. Based on video,

logs, questionnaires and interviews, we conclude that both

the classes of interface have a place. Notably, Focus con-

tributed to improved collaboration and better use of the in-

terface. Our results inform a set of recommendations for

future interfaces for this important class of interaction, sup-

porting access to les for collaboration at tabletop devices

embedded in the environment.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1040</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">MOBIX: System for managing MOBility using Information eXchange</field>
<field name="abstract">{NONE - this is a poster}</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1041</field>
<field name="author">Renato Iannella</field>
<field name="title">Towards a National Emergency Warning Framework</field>
<field name="abstract">The Emergency Warning System Framework (EWSF) describes and captures the interoperation of 

agencies and delivery channels that provides the functions for the national emergency warning framework. The EWSF is composed of three key functions; Message Distribution, Warning Message Management, and Warning Message Delivery.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1042</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="author">Christine Owen</field>
<field name="author">Gregory Hickey</field>
<field name="title">Cognitive Load Measurement from User s Linguistic Speech Features for Adaptive Interaction Design</field>
<field name="keyword">Cognitive Load</field>
<field name="keyword"> Measurement</field>
<field name="keyword"> Linguistic Features</field>
<field name="keyword"> Language usage</field>
<field name="keyword"> Word Categories</field>
<field name="keyword"> Interaction Design</field>
<field name="keyword"> Bushfire Management</field>
<field name="abstract">An adaptive interaction system, which is aware of the user s current cognitive load (CL), can change its response, presentation and flow of interaction material accordingly, to improve user s experience and performance. We present a speech content analysis approach to CL measurement, which employs users linguistic features of speech to determine their experienced CL level. We show analyses of several linguistic features, extracted from speech of personnel working in computerized incident control rooms and involved in highly complex bushfire management tasks in Australia. We present the re-sults of linguistic features showing significant differences between the speech from the cognitively low load and high load tasks. We also discuss how the method may be used for user interface evaluation and interaction design improvement.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1043</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Biologically Inspired Ant-Based Routing In Mobile Ad hoc Networks (MANET): A Survey</field>
<field name="keyword">MANET</field>
<field name="keyword"> Ant Routing</field>
<field name="keyword"> Wireless Networks</field>
<field name="keyword"> IEEE 802.11</field>
<field name="abstract">This paper presents the state of the art in biologically inspired ant based routing in wireless Mobile Ad hoc NETworks (MANET). The motivation for using ant-like mobile agents for providing routing information to mobile hosts in MANETs stems from the fact that ant-like mobile agents do not require high bandwidth overhead compared to MANET proactive routing protocols for disseminating the topology information in the network. The increased connectivity information provided by the ants can be used to for making better routing decisions which can improve the network performance without causing high overhead.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1044</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Challenges and Evaluation of the State of the Art Routing Protocols for Wireless Mesh Networks</field>
<field name="keyword">WMN</field>
<field name="keyword"> Routing</field>
<field name="keyword"> IEEE 802.111</field>
<field name="keyword"> Wireless</field>
<field name="abstract">Wireless mesh networks (WMNs) comprise of mobile and static nodes which communicate wirelessly. WMNs have several unique features such as self-organizing, self-configuring and self-healing and are being used in many applications such as metropolitan area networks and disaster and rescue operations etc. Routing in WMNs is challenging as the network topology and connectivity in WMN are very dynamic due to unpredictable node mobility, interference, distributed wireless channel access and limited battery power of mobile devices in WMNs. This paper describes the challenges and the state of the art in provisioning of Routing over WMNs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1045</field>
<field name="author">Jie Xu</field>
<field name="author">Getian Ye</field>
<field name="author">Yang Wang</field>
<field name="author">Wei Wang</field>
<field name="author">Jun Yang</field>
<field name="title">On-line Learning for PLSA-Based Visual Recognition</field>
<field name="keyword">PLSA</field>
<field name="keyword"> latent topic model</field>
<field name="keyword"> scene recognition</field>
<field name="keyword"> online learning</field>
<field name="abstract">Probabilistic Latent Semantic Analysis (pLSA) is one of

latent topic modeling approaches and has been successfully

applied for scene recognition. The learning process employed

in conventional pLSA is batch learning and hence

it may not be feasible when dealing with a large image corpus

which contains a huge number of features. In this paper,

we propose an on-line learning algorithmfor pLSA that

builds scene models incrementally. The proposed can automatically

and adaptively learn from image streams over

the entire learning life. The pLSA usually relies on the existence

of a finite vocabulary or codebook of features for

scene recognition. In order to facilitate codebook formation,

we present a codebook adaptation algorithm that captures

the full characteristics of features during on-line learning.

We evaluate the performance of the proposed algorithm

using the often used OT dataset. Experimental results

clearly demonstrate that the proposed algorithm can achieve

comparable recognition performance than the conventional

PLSA algorithm.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1046</field>
<field name="author">Thi Khanh Van Tran</field>
<field name="author">Raymond Wong</field>
<field name="author">William Cheung</field>
<field name="author">Jiming Liu</field>
<field name="title">Mobile Information Exchange and Integration: From Query to Application Layer</field>
<field name="keyword">Decentralised Systems</field>
<field name="keyword"> Version Control</field>
<field name="keyword"> Mobile Data Management</field>
<field name="keyword"> Heterogeneous Informa- tion Exchange</field>
<field name="keyword"> Information Integration</field>
<field name="keyword"> XML</field>
<field name="keyword"> Query Translation</field>
<field name="keyword"> Schema Inference</field>
<field name="abstract">Due to the popularity of mobile devices, more and more commercial applications have been developed on these devices. While commercial applications are mostly backed by relational database systems, numerous database engines have been ported to or built on these devices, for example, SQLite. Since connectivity can be unstable or slow, applications such as iAnywhere have considered offline operations while data can be synchronized with the database server whenever the devices are online. On the other hand, while Web-based and XML content are very common these days, unfortunately, these mobile versions of database engines failed to fully support them. This paper considers a translation-based method with a decentralized versioning system in place to support

offline operations. Web and XML contents are stored and versioned in a distributed manner and can be synchronized with each other without connecting to a server. The schema of these data can be automatically generated on device. With these schema, a translation engine which allows querying these data using SQL by translating the query to a corresponding XML query is facilitated. We believe this framework support mobile data applications on XML or Web data in a seamless manner. Finally, an initial prototype has been implemented and described in this paper.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1047</field>
<field name="author">Vidhyasaharan Sethu</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="title">Speaker Dependency of Spectral Features and Speech Production Cues for Automatic Emotion Classification</field>
<field name="abstract">Spectral and excitation features, commonly used in automatic emotion classification systems, parameterise different aspects of the speech signal. This paper groups these features as speech production cues, broad spectral measures and detailed spectral measures and looks at how they differ in their performance in both speaker dependent and speaker independent systems. The extent of speaker normalisation on these features is also considered. Combinations of different features are then compared in terms of classification accuracies. Evaluations were conducted on the LDC emotional speech corpus for a five class problem. Results indicate that MFCCs are very discriminative but suffer from speaker variability. Further, results suggest that the best front end for a speaker independent system is a combination of pitch, energy and formant information.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1048</field>
<field name="author">Fawad Nazir</field>
<field name="author">Jianhua Ma</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Time Critical Content Delivery using Predictable Patterns in Mobile Social Networks</field>
<field name="keyword">Mobile Social Networks</field>
<field name="keyword"> Content Delivery</field>
<field name="abstract">In Mobile Social Networks (MSN) individuals with similar interests or commonalities connect to each other using the mobile phones. MSN are special kind of Ad-hoc Networks in which wireless connectivity (i.e. encounters and re-encounters) with social peers is predictable. Most of the recent research proposes routing framework to exploit these predictable patterns of social encounters to identify the best information carriers. The best information carriers are selected based on the high probability of encounter with potential information recipients. In this approach an important variable is ignored i.e. time of encounter. Considering encounter time in the protocol gives time assurance of message delivery for time critical application. Therefore in this paper we address the research question, how to exploit people's predictable social patterns to improve the content delivery performance and lower end-to-end delay in time critical applications? Our assumption is that the people follow similar mobility patterns daily (i.e. Monday to Friday). In this paper we verify our research question and assumptions using real trace data of 100 users carrying Nokia 6600 smart phones over the course of nine months. Furthermore, we model, analyze and propose algorithms for social encounter based content delivery system for time critical applications. The simple heuristics and initial study presented in this paper achieve a timely content delivery using predictable patterns while lowering system wide traffic flooding.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1049</field>
<field name="author">Justin Bedo</field>
<field name="author">Geoffrey John MacIntyre</field>
<field name="author">Izhak Haviv</field>
<field name="author">Adam Kowalczyk</field>
<field name="title">Supervised learning for detection of transcription factor binding sites</field>
<field name="keyword">SVM</field>
<field name="keyword"> PWM</field>
<field name="keyword"> transcription factor</field>
<field name="keyword"> prediction</field>
<field name="keyword"> machine learning</field>
<field name="keyword"> DNA</field>
<field name="keyword"> binding</field>
<field name="abstract">It is typically thought that the interaction between transcription factor (TF)

and DNA is mediated by the ability of the TF to recognise a canonical DNA

sequence, and the chromatin structure that renders the DNA accessible to the

TF. As the DNA patterns recognised by TFs are short and degenerate, \textit{in

silico} genome-wide characterisation of TF binding sites in humans yields

large numbers of false positive predictions. Much effort has therefore been

placed on mapping regions of open chromatin to aid TF binding site prediction

at a genomic level. In contrast, we have developed a supervised prediction

method for genome-wide discovery of TF binding sites that uses sequence

information only and does not rely on information about chromatin state. Using

a single chromosome TF binding profile, we trained a support vector machine

(SVM) which predicts TF binding with an accuracy superior to standard

position-weight matrix (PWM) approaches. The ability to use sequence

information alone to predict binding sites suggests that chromatin

modifications are not a necessary condition for TF binding, but are

potentially an observed effect of the TF binding. Furthermore, our results

suggest PWMs are incapable of capturing the available sequence information and

that new models are needed if the sequence information is to be used

efficiently for binding prediction. Investigations into the sequence features

used by the SVM reveal not only a single TF recognition site, but sites for

many known co-operative TFs. This highlights the advantages of a holistic

approach to predicting TF binding.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1050</field>
<field name="author">Baris Fidan</field>
<field name="author">Julien Hendrickx</field>
<field name="author">Brian Anderson</field>
<field name="title">Edge Contraction Based Maintenance of Rigidity in Multi-Agent Formations During Agent Loss</field>
<field name="abstract">This paper proposes a systematic approach to the problem of restoring rigidity after loss of an agent, for two dimensional rigid multi-agent formations based on a particular

graph operation, the edge contraction operation. A rigidity

maintenance method is proposed, for the cases where an agent is

lost in an arbitrary two-dimensional rigid formation, to restore

rigidity by transferring all links to which this agent was incident

on to one of its neighbors. From a graph theoretical point of

view, this corresponds to contraction of a certain edge incident

to the vertex representing the agent being lost.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1051</field>
<field name="author">Iman Shames</field>
<field name="author">Pouyan Bibalan</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Polynomial Methods in Noisy Network Localization</field>
<field name="abstract">In this paper we introduce a polynomial method for addressing sensor network localization problems when the

measurements are noisy. We compare the result obtained applying

this method with the result obtained by other methods in the

literature. Later in the paper we propose methods from algebraic

geometry to aid us solve the problem in a more elegant way.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1052</field>
<field name="author">Iman Shames</field>
<field name="author">Brian Anderson</field>
<field name="author">Xiaofan Wang</field>
<field name="author">Baris Fidan</field>
<field name="title">Network Synchronizability Enhancement Using Convex Optimization</field>
<field name="abstract">In this paper we propose a method for enhancing

synchronizability using convex optimization. This method is

based on adding new edges to the network, later the performance

of the proposed method is tested through providing

some numerical examples. Furthermore, a comparison with

another method for adding edges, namely edge addition using

an eigenvector criterion is presented. Moreover, the extension

of the proposed method to the networks with different edge

weights is described.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1053</field>
<field name="author">Iman Shames</field>
<field name="author">Soura Dasgupta</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="title">Circumnavigation Using Distance Measurements</field>
<field name="abstract">Consider a stationary agent A at an unknown

location and a mobile agent B that must move to the vicinity

of and then circumnavigate A at a prescribed distance from

A. In doing so, B can only measure its distance from A, and

knows its own position in some reference frame. This paper

considers this problem, which has applications to surveillance

or maintaining an orbit. In many of these applications it is

difficult for B to directly sense the location of A, e.g. when

all that B can sense is the intensity of a signal emitted by

A. This intensity does, however provide a measure of the

distance. We propose a nonlinear periodic continuous time

control law that achieves the objective. Fundamentally, B must

exploit its motion to estimate the location of A, and use

its best instantaneous estimate of where A resides, to move

itself to achieve the ultimate circumnavigation objective. The

control law we propose marries these dual goals and is globally

exponentially convergent. We show through simulations that it

also permits B to approximately achieve this objective when A

experiences slow, persistent and potentially nontrivial drift.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1054</field>
<field name="author">Rabie Soukieh</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="title">Obstacle Avoidance of Non-holonomic Unicycle Robots Based on Fluid Mechanical Modeling</field>
<field name="abstract">This paper is concerned with obstacle avoidance of

robots moving on a plane, based on a fluid mechanical principle

known as the Circle Theorem. Considering the motion region

as a fictitious fluid environment surrounding the obstacles, fluid

streamlines are calculated which correspond to unique smooth

paths that a mobile robot can follow without colliding with

the obstacles. The design and analysis are initially performed

assuming simple integrator dynamics for the agent, and later

extended for more realistic non-holonomic unicycle dynamic

agent models, with the help of proportional integral (PI) control

and backstepping principles. Both point and non-point (ellipse)

geometric models are considered for the agents in design

and analysis. The fluid dynamics based designs developed for

obstacle avoiding motion control of agents with non-holonomic

unicycle dynamics are novel, and successfully tested via an

extensive set of simulations. Application of the developed

designs for motion control of unmanned aerial vehicles (UAVs)

under the constraint of constant speed is also presented.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1055</field>
<field name="author">Rabie Soukieh</field>
<field name="author">Iman Shames</field>
<field name="author">Baris Fidan</field>
<field name="title">Obstacle Avoidance of Robotic Formations Based on Fluid Mechanical Modeling</field>
<field name="abstract">This paper is on obstacle avoidance of swarms of

robots moving in certain geometric planar formations. Focus

is given to a particular obstacle avoidance approach, which is

based on the fluid mechanical principle known as the Circle

Theorem. Considering the motion region as a fictitious fluid

environment surrounding the obstacles, fluid streamlines are

calculated which correspond to unique smooth paths that a

robot or a robotic formation can follow without colliding with

the obstacles. The design and analysis are initially performed

assuming simple integrator dynamics for each agent, and later

extended for more realistic non-holonomic unicycle dynamic

agent models, with the help of proportional integral (PI)

control. The fluid dynamics based designs developed for obstacle

avoiding motion control of agents, moving in a prescribed rigid

formation are novel, and successfully tested via an extensive set

of simulations. Application of the developed designs for motion

control of unmanned aerial vehicle (UAV) formations under the

constraint of constant speed is also presented.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1056</field>
<field name="author">Michael Maher</field>
<field name="title">SOGgy Constraints: Soft Open Global Constraints</field>
<field name="keyword">global constraints</field>
<field name="keyword"> open constraints</field>
<field name="keyword"> soft constraints</field>
<field name="abstract">We investigate soft open constraints. We generalize and unify classes of soft constraints and adapt them to the open setting. We give sufficient conditions for generalized classes of decomposition-based and edit-based soft constraints to be contractible. Finally, we outline a propagator for an open generalized edit-based soft REGULAR constraint.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1057</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Getian Ye</field>
<field name="author">Yang Wang</field>
<field name="author">Wei Wang</field>
<field name="author">Jie Xu</field>
<field name="author">Gunawan Herman</field>
<field name="author">Jun Yang</field>
<field name="title">Multi-class Graph Boosting with Subgraph Sharing for Object Recognition</field>
<field name="keyword">Multiclass boosting</field>
<field name="keyword"> graph boosting</field>
<field name="keyword"> object recognition</field>
<field name="abstract">In this paper, we present a novel multi-class graph boosting algorithm with emphasis

on application in multi-class object recognition. Graph-based image representation strategy,

which depicts image as a graph composed by a collection of attributed vertices and

edges, offers a compact and interpretable representation for images. Aided by it, graph

classification method (e.g. graph boosting) is able to be applied on object recognition to

achieve both high prediction performance and interpretability.

The original graph boosting is only designed for two-class situations, for multi-class

(more than two classes) cases, the prevalent one-vs-all strategy is applied. The shortcoming

is that the weak classifier formed by subgraph is forced to distinguish one class

against the rest. However, intrinsically a large number of informative subgraphs are not

distinctive on any specific single class but shared by a few classes. In this work, we

propose to share informative subgraphs for multi-class graph classification within boosting

framework. And the experimental results on multi-class object recognition show the

effectiveness of the proposed method.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1058</field>
<field name="author">Mohammad Emam Hossain</field>
<field name="title">Using Scrum in Global Software Development: A Systematic Literature Review</field>
<field name="keyword">Global software development</field>
<field name="keyword"> agile approaches</field>
<field name="keyword"> Scrum</field>
<field name="keyword"> systematic literature reviews</field>
<field name="abstract">There is a growing interest in applying agile approaches practices in Global Software Development (GSD) projects. Recently, some studies have reported the use of Scrum practices in distributed development projects. However, little is known about how these practices are carried out in reality and what the outcomes are. We have conducted a systematic literature review to identify, synthesize and present the findings from the primary studies that report using Scrum practices in GSD projects. Our search strategy identified 583 papers, of which 20 were identified as primary papers relevant to our research. We extracted data from these papers to identify various challenges of using Scrum in GSD. Current strategies to deal with the identified challenges have also been extracted. This paper presents the review s findings that are expected to help researchers and practitioners to understand the current state of use of Scrum practices in GSD.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1059</field>
<field name="author">M.K. Bhuyan</field>
<field name="author">Brian C Lovell</field>
<field name="author">Abbas Bigdeli</field>
<field name="title">Tracking with Multiple Cameras for Video Surveillance</field>
<field name="keyword">detection and tracking methods</field>
<field name="keyword"> multiple-camera system</field>
<field name="keyword"> fusion of tracks</field>
<field name="abstract">The large shape variability and partial occlusions challenge most object detection and tracking methods for nonrigid

targets such as pedestrians. Single camera tracking is limited in the scope of its applications because of the limited

field of view (FOV) of a camera. This initiates the need for a multiple-camera system for completely monitoring and

tracking a target, especially in the presence of occlusion. When the object is viewed with multiple cameras, there is

a fair chance that it is not occluded simultaneously in all the cameras. In this paper, we developed a method for the

fusion of tracks obtained from two cameras placed at two different positions. First, the object to be tracked is identified on the basis of shape information measured by MPEG-7 ART shape descriptor. After this, single camera tracking is performed by the unscented Kalman filter approach and finally the tracks from the two cameras are fused. A sensor network model is proposed to deal with the situations in which the target moves out of the field of view of a camera and reenters after sometime. Experimental results obtained demonstrate the effectiveness of our proposed scheme for tracking objects under occlusion.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1060</field>
<field name="author">Nathan Brewer</field>
<field name="author">Nianjun Liu</field>
<field name="author">Lei Wang</field>
<field name="author">Li Cheng</field>
<field name="title">User-attention Driven Adaptive Image Partitioning</field>
<field name="keyword">image partition</field>
<field name="keyword"> Pattern recognition</field>
<field name="abstract">Image partitioning separates an image into multiple visually and semantically homogeneous regions, providing a summary of visual content. Knowing that human observers focus on interesting objects or regions when interpreting a scene, and envisioning the usefulness of this focus in many computer vision tasks, this paper develops a user-attention adaptive image partitioning approach. Given a set of pairs of oversegments labeled by a user as "should be merged" or "should not be merged", the proposed approach aims to produce a fine partitioning in user defined interesting areas, to retain interesting information, and a coarser partitioning in other regions to provide a parsimonious representation.To achieve this, a novel Markov Random Field (MRF) model is used to optimally infer the relationship ("merge" or "not merge") among oversegment pairs, in which the graph nodes describe the relationship between pairs, rather than oversegments themselves. By training an SVM classifier to provide the data term, a graph-cut algorithm is employed to infer the best MRF configuration. We discuss the difficulty in translating this configuration back to an image labelling, and develop a non-trivial post-processing to refine the configuration further. Experimental verification on benchmark data sets demonstrates the effectiveness of the proposed approach, and its extensive practical applications.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1061</field>
<field name="author">Ildiko Horvath</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Storm-enhanced plasma density features, daytime polar cap plasma enhancements and their underlying plasma circulation investigated during superstorms</field>
<field name="keyword">superfountain</field>
<field name="keyword"> SED features</field>
<field name="keyword"> daytime polar plasma enhancements</field>
<field name="keyword"> SAPS-E-fields</field>
<field name="abstract">This study investigates the effects of November superstorms on the daytime-evening northern topside ionosphere. We utilize multi-instrument interplanetary and geomagnetic data to analyze the interplanetary shock/electric (E) field events. To assess their impact, field-aligned DMSP (Defense Meteorological Satellite Program) passes are employed. We tracked evening storm-enhanced density (SED) features and daytime polar plasma enhancements occurring regularly that allowed us to learn about the basic mechanisms responsible for their development. Results show that large vertical downward plasma flows supplied the plasma building up the SED features and polar plasma enhancements detected. These plasma formations occurred during the main phase with the forward superfountain and during a sub-storm with the reverse fountain. These provide observational evidence that primary downward plasma flows, triggered by detachment processes caused by the westward sub-auroral polarization stream (SAPS) E-fields, existed and became transported sunward. At SED-latitudes, these primary downward plasma flows were intensified by the forward superfountain when its upward plasma flow spilled over into the equatorial anomaly and cascaded down the magnetic field lines. When the reverse fountain was active, such intensification was not available to the primary downward plasma flows at SED-latitudes. As the downward streaming plasma found its way to the polar cap, it resulted in the development of daytime polar plasma enhancements. Statistically, daytime polar plasma enhancements occurred at all longitudes contradicting the myth that the North American region is better suited for the development of large plasma enhancements in the polar cap.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1062</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Colin Sim</field>
<field name="author">Morteza Biglari-Abhari</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Face Detection on Embedded Systems</field>
<field name="keyword">automated face detection and recognition</field>
<field name="keyword"> real-time image processing</field>
<field name="abstract">Over recent years automated face detection and recognition (FDR) have gained significant attention from the commercial and research sectors. This paper presents an embedded face detection solution aimed at addressing the real-time image processing requirements within a wide range of applications. As face detection is a computationally intensive task, an embedded solution would give rise to opportunities for discrete economical devices that could be applied and integrated into a vast majority of applications. This work focuses on the use of FPGAs as the embedded prototyping technology where the thread of execution is carried out on an embedded soft-core processor. Custom instructions have been utilized as a means of applying software/hardware partitioning through which the computational bottlenecks are moved to hardware. A speedup by a factor of 110 was achieved from employing custom instructions and software optimizations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1063</field>
<field name="author">Yean Choon Ham</field>
<field name="author">Yu Shi</field>
<field name="title">Developing a Smart Camera for Gesture Recognition in HCI Applications</field>
<field name="keyword">Smart camera</field>
<field name="keyword"> human computer interactions</field>
<field name="keyword">gesture recognition</field>
<field name="keyword"> FPGA</field>
<field name="keyword"> embedded systems</field>
<field name="abstract">Smart cameras are becoming more popular in human

computer interaction (HCI). One of HCI research areas,

multimodal user interface (MMUI) allows user to interact with a

computer by using his or her natural communication modalities,

such as speech, pen, touch, gestures, eye gaze, and facial

expression. This paper presents the hardware and software codesign

and implementation of KLT (Kanade Lucas Tomasi)

tracking algorithm in a FPGA-based smart camera prototype for

recognize simple hand gestures consisting of a CMOS image

sensor capture unit and FPGA main video processor. This

tracking system uses face and hand detections as a tool to detect

and track gesture (face and hand motion). This gesture tracking

system that are based on Harris Keypoint Detection algorithm

and KLT tracking algorithm has been successfully implemented

in FPGA and shows promising result in real time performance.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1064</field>
<field name="author">Jenny Liu</field>
<field name="author">Xin Liang</field>
<field name="author">Liming Zhu</field>
<field name="title">A Component-based Approach to Developing Thematic Mashups</field>
<field name="keyword">patterns</field>
<field name="keyword"> architecture</field>
<field name="keyword"> mashup</field>
<field name="keyword"> integration</field>
<field name="abstract">Mashup provides a way of forming new applications

from existing Web content using APIs provided by different

Web sites. Such a nature makes mashup a promising

technology to deliver a Web-based Enterprise application

with rich information of various themes, so called thematic

mashup. However, the development of thematic mashup is

ad-hoc and mostly from the scratch, which can be a barrier

for Enterprise applications to leverage mashups. For an

Enterprise application, the complexity of business logic and

the requirements to adapt to a changing business environment

demand a development method that helps to achieve

reusability and maintainability. Component-based development

method has proved practical to reduce the complexity

of software development by reusing modular components.

In this paper, we propose a reference architecture with key

components identified for developing thematic mashups.

We further demonstrate the usage of this reference architecture

in a case study - developing a thematic mashup

for a Web-based property valuation application. The contribution

of this paper is applying a well-established software

engineering discipline to emerging mashup applications,

and providing insights of the techniques in developing

mashups.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1065</field>
<field name="author">Leif Hanlen</field>
<field name="title">Key-sharing via channel randomness in narrowband body area networks: Is everyday movement sufficient?</field>
<field name="abstract">We consider secure communication for Body-Area-Networks (BAN's). We examine the near-body radio channel of BAN's as a source of common randomness between two sensors. The movement of the subject and associated fading is used to hide a secure key from Eve. We examine recently approved radio channel models of the IEEE 802.15.6 Task Group, and show that the common randomness is too low rate for unconditional encoding. We find a key-generation rate around 2bits/second. We suggest the channel randomness may be better used in generating perpetually new keys for an AES-style encryption -- eg, a 128bits key every minute -- via a randomness scavenging procedure.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1066</field>
<field name="author">Daniel Dallas</field>
<field name="author">Leif Hanlen</field>
<field name="title">Optimal Transmission Range and Node Degree for Multihop Routing in Wireless Sensor Networks</field>
<field name="abstract">The energy consumed by radio transceivers is a significant factor 

limiting the lifetime of wireless sensor networks (WSNs). 

Maximizing WSN lifetime is the overall aim of this work, which 

proposes that optimal routing be defined with a network design 

that minimizes the energy consumed by transceivers. This 

definition of optimal routing is greatly influenced by transmission 

range, which is adjustable, and therefore a candidate predictor or 

independent variable that affects the criterion, which is to 

minimize energy consumption and thereby maximize lifetime. 

The effects of transmission range depend on the physical 

dimensions and node density of the WSN being observed. This 

dependency binds the result to a particular design and therefore, to 

be more general, it is noted that transmission range directly 

controls neighborhood size known as node degree which 

forms a more generic predictor of optimal routing as defined in 

the empirical experiments of this work. 

Increasing node degree saves energy by reducing hop-count. 

This benefit, however, is vitiated in a complicated tradeoff that is 

understood via a broad view of the routing behavior. A final 

investigation into the primary causes of wasted energy identifies 

future work that will enable WSNs to reap the energy savings 

made possible by the improved transmission range available with 

new transceivers.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1067</field>
<field name="author">Barbara Staudt Lerner</field>
<field name="author">Stefan Christov</field>
<field name="author">Leon J. Osterweil</field>
<field name="author">Reda Bendraou</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">Alexander Wise</field>
<field name="title">Exception handling patterns for process modeling</field>
<field name="keyword">Exception handling patterns</field>
<field name="keyword"> process modeling</field>
<field name="keyword"> process modeling languages</field>
<field name="abstract">Abstract Process modeling allows for analysis and improvement of processes that coordinate multiple people and tools working together to carry out a task. Process modeling typically focuses on the normative process, that is, how the collaboration transpires when everything goes as desired. Unfortunately, real-world processes rarely proceed that smoothly. A more complete analysis of a process requires that the process model also include details about what to do when exceptional situations arise. We have found that, in many cases, there are abstract patterns that capture the relationship between exception handling tasks and the normative process. Just as object-oriented design patterns facilitate the development, documentation, and maintenance of object-oriented programs, we believe that process patterns can facilitate the development, documentation, and maintenance of process models. In this paper, we focus on the exception handling patterns that we have observed over many years of process modeling. We describe these patterns using three process modeling notations: UML 2.0 Activity Diagrams, BPMN, and Little-JIL. We present both the abstract structure of the pattern as well as examples of the pattern in use. We also provide some preliminary statistical survey data to support the claim that these patterns are found commonly in actual use and discuss the relative merits of the three notations with respect to their ability to represent these patterns.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1068</field>
<field name="author">Oliver Nagy</field>
<field name="author">Leif Hanlen</field>
<field name="title">Information on an elastic rubber string</field>
<field name="abstract">This paper shows how to use an ideal rubber string, which is clamped down at both ends, as a memory device. The information is stored on the string by giving it an initial shape, where the Fourier coefficients of this shape represent the code bits. Once the string is let go, the wave equation governs the oscillatory movements of the string, but it retains the information in the Fourier coefficients. We show two possibilities to recover this information: analysing a snapshot of the string shape, and analysing its motion at a particular point. Both methods are successful, but for most practical cases the spatial analysis yields a better achievable rate. While the ideal string is used as the intuitive model, our results automatically extend to arbitrary one dimensional wave resonators with clamped down (Dirichlet) boundaries.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1069</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Carey Williamson</field>
<field name="author">martin Arlitt</field>
<field name="author">Phillipa Gill</field>
<field name="author">Niklas Carlsson</field>
<field name="title">Understanding Organizational Use of Web-based Services: Challenges and Insights</field>
<field name="abstract">The World Web Web (WWW) has evolved from an information retrieval tool to a platform that provides numerous functionalities and services (e.g., communication, social networking, entertainment as well as information retrieval). As the WWW and Web-based services become more important to the daily operations of organizations, a need for systematically monitoring and assessing their use emerges. In this paper, we examine two concurrently collected one week-long traces of HTTP activity from a large enterprise and large university. We provide insights on the types of Web-based services in use today, who provides them, and who (or what) is using them. We also describe the challenges that exist for extracting such information across all Web-based service activity. Furthermore, we present a quantitative view of how the WWW has evolved over the past decade, and discuss the implications of the changes. Finally, we discuss how organizations dependence of Web-based services and the evolution of the WWW may affect the importance and complexity of organizations monitoring their service usage.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1070</field>
<field name="author">Mark Reed</field>
<field name="author">Leif Hanlen</field>
<field name="author">Giovanni Corazza</field>
<field name="title">Return Link Code Acquisition with DS-CDMA for High Capacity Multiuser Systems under Realistic Conditions</field>
<field name="abstract">Acquisition of the code timing in a direct-sequence 

code-division multiple-access system at the base station must 

take place before signal detection and decoding is possible. Code 

acquisition under severe multiple access interference conditions 

with time varying codes makes the task even more difficult. 

Inefficient designs lead to large number of false alarms and/or 

missed detections. This paper details a realistic powerful code ac- 

quisition technique for the uplink of direct-sequence code-division 

multiple-access systems under high loaded situations, where the 

number of users is greater than the processing gain. Under 

this high multiple access interference condition the DS-CDMA 

acquisition problem becomes very difficult and conventional 

search methods simply fail. The method discussed utilises soft 

data from a multiuser detector to reduce the interference received 

by the acquisition unit. Numerical results validate performance 

under realistic conditions with amplitude, phase and frequency 

impairments.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1071</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="author">Dan Fang</field>
<field name="title">NICTA Proposal</field>
<field name="keyword">BAN proposal super</field>
<field name="keyword">beacon TDMA CDMA channel measurement modelling</field>
<field name="abstract">Partial proposal for the IEEE 802.15.6 PHY and MAC standard.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1072</field>
<field name="author">Christoph Dwertmann</field>
<field name="author">Ergin Mesut</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Max Ott</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Ivan Seskar</field>
<field name="title">Mobile Experiments Made Easy with OMF/Orbit</field>
<field name="abstract">We propose to showcase the experiment facility control, management &amp; measurement framework (OMF) with an experiment demonstrating a common investigative life cycle. The demo scenario will highlight the steps from the idea, to simulations, and finally testbed simulations on the ORBIT outdoor testbed using a variety of network access networks, such as WiFi and Wimax.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1073</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Garg Sanchit</field>
<field name="author">Trinabh Gupta</field>
<field name="author">Niklas Carlsson</field>
<field name="title">Evolution of an Online Social Aggregation Network: An Empirical Study</field>
<field name="abstract">Tendency of individuals to develop relationships

with similar individuals, to form relationships

based on mutual acquaintances and proximity, and to 

form relationships based on common interests are documented 

in the social science literature as factors contributing to evolution of social networks. 

In this paper, we analyze the evolution an {\em online social aggregator}, 

FriendFeed, that collates content generated by 

participating individuals on a variety of Web 2.0 services and allows easy dissemination

of the aggregated content to other participants of the aggregator. 

While arguably a niche service that aims to tap the ``information 

hungry'' Web users, FriendFeed offers us an opportunity 

to understand how common interest affects

evolution of online social networks. Using data collected between

September 2008 and April 2009, we show that although preferential attachment

captures the evolution of the network, its influence varies significantly based on 

when a user joined the service. In particular, preferential attachment

 does not appear to apply to new entrants in the network. 

 Analysis suggests that proximity bias plays an

important role in link formation. We study the influence of

common foci and find that individuals have a greater affinity toward those with 

similar interests.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1074</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">A DBN Approach for Network Availability Prediction</field>
<field name="keyword">Network Availability Prediction</field>
<field name="keyword"> Context Prediction</field>
<field name="keyword"> Dynamic Bayesian Networks</field>
<field name="abstract">Modern mobile devices are increasingly capable of simultaneously connecting to multiple access networks with different characteristics. Restricted coverage combined with user mobility will vary the availability of networks for a mobile device. Most proposed solutions for such an environment are reactive in nature, such as performing a vertical handover to the network that offers the highest bandwidth. But the cost of the handover may not be justified if that network is only available for a short time. Knowledge of future network availability and their capabilities are the basis for proactive schemes which will improve network selection and utilization. We have previously proposed a prediction model that can use any available context such as GSM Location Area, WLAN presence or even whether the power cable is plugged in, to predict network availability. 

As it may not be possible to sense all of the context variables that influence future network availability, in this paper we introduce a generic, new model incorporating a hidden variable to account for this. Specifically, we propose a Dynamic Bayesian Network based context prediction model to predict network availability. When the predictions were performed for WLAN availability with the real user data collected in our experiments, this model shows 20% or more improvement than both of our earlier proposals of order 1 and 2 Semi-Markov models.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1075</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="author">Leif Hanlen</field>
<field name="title">TMS320C6713-DSP Based FSK Modem with Timing, Frequency, and Phase Synchronisation</field>
<field name="abstract">In this paper we present a Texas Instruments 

TMS320C6713- DSP based communication testbed, designed 

for testing algorithms, channel measurements and performance 

analysis of digital receivers. The testbed is used to implement 

a frequency shift keying (FSK) modem with synchronisation 

capabilities at the base band. The modem operates at a data 

rate of 600bps with a sampling frequency of 48 KHz. Here, we 

present the design and architecture of the modem, and some 

results obtained using Matlab.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1076</field>
<field name="author">Michael von Tessin</field>
<field name="title">Towards a Formally Verifiable Multiprocessor Microkernel</field>
<field name="keyword">formal verification</field>
<field name="keyword"> multiprocessor</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> seL4</field>
<field name="keyword"> Isabelle/HOL</field>
<field name="abstract">Embedded systems in safety-critical areas require a high degree of assurance with regards to correctness and reliability. While extensive testing and using redundant subsystems can increase the degree of assurance, they can never guarantee that a system is correct and bug-free.



Microkernels are increasingly being used in today's embedded systems. They improve security and reliability by providing isolation between system components. NICTA's seL4 project adapted the L4 microkernel's API to enable fine-grained dissemination of authority via capabilities. This allows controlled communication between otherwise isolated components. In order to raise the degree of assurance, NICTA's L4.verified project aims to formally verify the isolation properties of seL4's API and the functional correctness of the complete C implementation. This will guarantee the absence of bugs and yield the highest possible degree of assurance. Guaranteed isolation is desired mainly because faults in one component of the system are guaranteed not to affect another, potentially vital component.



In order not to have to deal with concurrency in seL4's proofs, which is inherently hard, seL4 is a uniprocessor microkernel and does not allow preemption within the kernel except from a few well-defined preemption points in long-running system calls. However, with embedded applications increasing in complexity and demanding more and more computing power, embedded multiprocessor/multicore systems gain additional popularity. Thus, my work aims to provide the same guarantees for a multiprocessor version of seL4 as we already have for the uniprocessor version.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1077</field>
<field name="author">Joerg Hoffmann</field>
<field name="author">Ingo Weber</field>
<field name="author">Guido Governatori</field>
<field name="title">On Compliance Checking for Clausal Constraints in Annotated Process Models</field>
<field name="keyword">regulatory compliance</field>
<field name="keyword"> business process modelling</field>
<field name="abstract">Compliance management is important in several

 industry sectors where there is a high incidence of

 regulatory control. It must be ensured that business

 practices, as reflected in business processes,

 comply with the rules. Such compliance checks are

 challenging due to (1) the different life cycles of

 rules and processes, and (2) their dis</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1078</field>
<field name="author">Carlos Aydos</field>
<field name="author">Bernhard Hengst</field>
<field name="author">William Uther</field>
<field name="title">Kalman Filter Process Models for Urban Vehicle Tracking</field>
<field name="keyword">Kalman filter</field>
<field name="keyword"> process model</field>
<field name="keyword"> unscented</field>
<field name="keyword"> vehicle tracking</field>
<field name="keyword"> traffic</field>
<field name="abstract">Faced with increasing congestion on urban roads, authorities need better real-time traffic information to manage traffic. Kalman Filters are efficient algorithms that can be adapted to track vehicles in urban traffic given noisy sensor data. A Kalman Filter process model that approximates dynamic vehicle behaviour is a reusable subsystem for modelling the dynamics of a multi-vehicle traffic system. The challenge is choosing an appropriate process model that produces the smallest estimation errors. This paper provides a comparative analysis and evaluation of Linear and Unscented Kalman Filters process models for urban traffic applications.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1079</field>
<field name="author">Pranam Janney</field>
<field name="author">Glenn Geers</field>
<field name="title">Texture Classification using Invariant Features of Local Textures (IFLT)</field>
<field name="keyword">Texture classification</field>
<field name="keyword"> Categorisation</field>
<field name="keyword"> Wavelets</field>
<field name="keyword"> Haar Filters</field>
<field name="keyword"> Invariance</field>
<field name="keyword"> Illumination</field>
<field name="keyword"> Rotation </field>
<field name="keyword"> Local textures</field>
<field name="abstract">In this paper, we present a texture descriptor algorithm called Invariant Features of Local Textures (IFLT). IFLT generates scale, rotation and (essentially) illumination invariant descriptors from a small neighbourhood of pixels around a centre pixel or a texture patch. Texture classification experiments were carried out on the Brodatz, Outex and KTH-TIPS2 databases. Demonstrated texture classification accuracy exceeds the previously published state-of-the-art at a significantly lower computational cost. Experiments also suggests that IFLT descriptors are in a sense intuitive texture descriptors.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1080</field>
<field name="author">Pranam Janney</field>
<field name="author">Glenn Geers</field>
<field name="title">Advanced framework for Illumination Invariant Traffic Density Estimation</field>
<field name="keyword">traffic estimation</field>
<field name="keyword"> illumination invariant</field>
<field name="keyword"> IFLT</field>
<field name="keyword"> textures</field>
<field name="keyword"> machine learning</field>
<field name="keyword"> SVM</field>
<field name="keyword"> error reduction</field>
<field name="keyword"> linear kernel</field>
<field name="keyword"> density</field>
<field name="keyword"> cellular automata</field>
<field name="abstract">CCTV cameras are becoming a common fixture at the roadside. Their use varies from traffic monitoring to security surveillance. In this paper an advanced two-stage framework for estimating vehicular traffic density on a road segment is presented. The proposed approach is computationally efficient and robust to varying illumination. The method is novel because it combines state-of-the-art image processing techniques with a simple traffic model in order to increase robustness. Experimental results have shown that the proposed framework can achieve higher performance than existing stateof- the-art techniques under conditions of varying illumination.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1081</field>
<field name="author">Nicola Stokes</field>
<field name="author">Yi Li</field>
<field name="author">Lawrence Cavedon</field>
<field name="author">Justin Zobel</field>
<field name="title">Exploring criteria for successful query expansion in the genomic domain</field>
<field name="keyword">Passage retrieval for genomic queries </field>
<field name="keyword"> Knowledge based query expansion </field>
<field name="keyword"> Corpus based query expansion </field>
<field name="keyword"> Pseudo relevance feedback </field>
<field name="keyword"> Concept</field>
<field name="keyword">based normalisation passage ranking </field>
<field name="keyword"> TREC 2006 Genomics Track</field>
<field name="abstract">Query Expansion is commonly used in Information Retrieval to overcome vocabulary mismatch issues, such as synonymy between the original query terms and a relevant document. In general, query expansion experiments exhibit mixed results. Overall TREC Genomics Track results are also mixed; however, results from the top performing systems provide strong evidence supporting the need for expansion. In this paper, we examine the conditions necessary for optimal query expansion performance with respect to two system design issues: IR framework and knowledge source used for expansion. We present a query expansion framework that improves Okapi baseline passage MAP performance by 185%. Using this framework, we compare and contrast the effectiveness of a variety of biomedical knowledge sources used by TREC 2006 Genomics Track participants for expansion. Based on the outcome of these experiments, we discuss the success factors required for effective query expansion with respect to various sources of term expansion, such as corpus-based cooccurrence statistics, pseudo-relevance feedback methods, and domain-specific and domain-independent ontologies and databases. Our results show that choice of document ranking algorithm is the most important factor affecting retrieval performance on this dataset. In addition, when an appropriate ranking algorithm is used, we find that query expansion with domain-specific knowledge sources provides an equally substantive gain in performance over a baseline system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1082</field>
<field name="author">Wern Wong</field>
<field name="author">David Martinez</field>
<field name="author">Lawrence Cavedon</field>
<field name="title">Extraction of named entities from tables in gene mutation literature</field>
<field name="keyword">Information extraction</field>
<field name="keyword"> tables</field>
<field name="keyword"> biomedical applications</field>
<field name="abstract">We investigate the challenge of extracting

information about genetic mutations from tables,

an important source of information in scientific

papers. We use various machine learning algorithms

and feature sets, and evaluate performance in

extracting fields associated with an existing handcreated

database of mutations. We then show how

classifying tabular information can be leveraged for

the task of named entity detection for mutations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1083</field>
<field name="author">David Martinez</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Lawrence Cavedon</field>
<field name="author">Tim Baldwin</field>
<field name="title">Facilitating biomedical systematic reviews using text classification and ranked text retrieval</field>
<field name="keyword"/>
<field name="abstract">Searching and selecting articles to be

included in systematic reviews is a real challenge for

healthcare agencies responsible for publishing these

reviews. The current practice of manually reviewing

all papers returned by complex hand-crafted boolean

queries is human labour-intensive. We demonstrate

a two-stage searching system that takes advantage

of ranked queries and support-vector machine text

classification to assist retrieval of relevant articles,

and to restrict results to higher-quality documents.

Our proposed approach shows significant work saved

in the systematic review process over a baseline of a

keyword-based retrieval system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1084</field>
<field name="author">Stefan Pohl</field>
<field name="author">Alistair Moffat</field>
<field name="title">Measurement Techniques and Caching Effects</field>
<field name="abstract">Overall query execution time consists of the time spent transferring data from disk to memory, and the time spent performing actual computation. In any measurement of overall time on a given hardware confi&#12;guration, the two separate costs are aggregated. This makes it hard to reproduce results and to infer which of the two costs is actually affected by modi&#12;fications proposed by researchers. In this paper we show that repeated submissions of the same query provides a means to estimate the computational fraction of overall query execution time. The advantage of separate measurements is exempli&#12;ed for a particular optimization that is, as it turns out, reducing computational costs only. Finally, by exchange of repeated query terms with surrogates that have similar document-frequency, we are able to measure the natural caching e&#11;ffects that arise as a consequence of term repetitions in query logs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1085</field>
<field name="author">Nicola Stokes</field>
<field name="author">Yi Li</field>
<field name="author">Alistair Moffat</field>
<field name="author">Jiawen Rong</field>
<field name="title">An empirical study of the effects of NLP components on Geographic IR performance</field>
<field name="abstract">Natural Language Processing (NLP) techniques, such as toponym detection and resolution, are an integral part of most Geographic Information Retrieval (GIR) architectures. Without these components, synonym detection, ambiguity resolution and accurate toponym expansion would not be possible. However, there are many important factors affecting the success of an NLP approach to GIR, including toponym detection errors, toponym resolution errors, and query overloading. The aim of this paper is to determine how severe these errors are in state-of-the-art systems, and to what extent they affect GIR performance. We show that a careful choice of weighting schemes in the IR engine can minimize the negative impact of these errors on GIR accuracy. We provide empirical evidence from the GeoCLEF 2005 and 2006 datasets to support our observations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1086</field>
<field name="author">Stefan Pohl</field>
<field name="author">Alistair Moffat</field>
<field name="title">Term-frequency surrogates in text similarity computations</field>
<field name="keyword">Information retrieval</field>
<field name="keyword"> inverted index</field>
<field name="keyword"> skip pointer</field>
<field name="keyword"> proximity query</field>
<field name="keyword"> efficiency</field>
<field name="keyword"> effectiveness.</field>
<field name="abstract">Inverted indexes on external storage

perform best when accesses are ordered and data is

read sequentially, so that seek times are minimized.

As a consequence, the various items required to

compute Boolean, ranked and phrase queries are

often interleaved in the inverted lists. While suitable

for query types in which all items are required, this

arrangement has the drawback that other query

types notably pure ranked queries and conjunctive

Boolean queries do not require access to word

position information, and that component of each

posting must be bypassed when these queries are being

handled. In this paper we show that the term frequency

component of each posting can be completely replaced

by a surrogate that allows skipping of positional

information interleaved in inverted lists, and obtain

significant speedups in ranked query execution without

increasing the index size, and without harming

retrieval effectiveness. We also explore two methods

of reconstituting approximations to the original

term frequencies that can be employed if use of

the surrogates is deemed too risky. Our simple

improvement can thus be used with all ranking

functions that make use of term frequencies.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1087</field>
<field name="author">Bader Aljaber</field>
<field name="author">Nicola Stokes</field>
<field name="author">James Bailey</field>
<field name="author">Yi Li</field>
<field name="title">Exploring the benefit of contextual information for boosting TREC Genomic IR performance</field>
<field name="keyword">Passage Retrieval</field>
<field name="keyword"> Contextual Document Expansion and Ranking Strategies</field>
<field name="abstract">Query Expansion is a widely used

technique that augments a query with synonymous and

related terms in order to address a common issue in

ad hoc retrieval: the vocabulary mismatch problem,

where relevant documents contain query terms that are

semantically similar, but lexically distinct. Standard

query expansion techniques include pseudo relevance

feedback and ontology-based expansion. In this

paper, we explore the use of contextual information

as a means of expanding the context surrounding

the unit of retrieval, rather than the query, which

in this case is a document passage. The ad hoc

retrieval task that we focus on in this paper was

investigated at the TREC 2006 Genomic tracks, where

systems were required to retrieve relevant answer

passages. The most commonly reported indexing

strategy was passage indexing. Although this simplifies

post-retrieval processing, retrieval performance

can be hurt as valuable contextual information in

the containing document is lost. The focus of this

paper is to investigate various contextual evidence of

similarity outside of the passage such as: query/fulltext

similarity, query/citation sentence similarity,

query/title similarity, query/abstract similarity. These

similarity scores are then used to boost the rank of

passages that exhibit high contextual evidence of

query similarity. Our experimental results suggest that

document context provides the strongest evidence of

contextual information for this task.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1088</field>
<field name="author">Peter Baumgartner</field>
<field name="author">Evgenij Thorstensen</field>
<field name="title">Instance Based Methods - An Overview</field>
<field name="keyword">Automated Theorem Proving</field>
<field name="keyword"> Instance-Based Methods</field>
<field name="abstract">Instance-based methods are a specific class of methods for automated

 proof search in first-order logic. This article provides an overview

 of the major methods in the area and discusses their properties and

 relations to the more established resolution methods.

 It also discusses some recent trends on refinements and applications.



This overview is rather brief and informal, but we provide a

comprehensive literature list to follow-up on the details.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1089</field>
<field name="author">David Smith</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Maximal ratio combining performance analysis in Practical Rayleigh Fading Channels</field>
<field name="abstract">The authors present a novel theoretical method to analyse the maximal ratio combining (MRC) receivers using BPSK and M-PSK modulation in spatially correlated Rayleigh fading channels, with perfect and imperfect channel knowledge, in terms of antenna array configuration and parameters of scatterer distributions. In this analysis closed form expressions for error probabilities of these modulations with MRC are derived, allowing for non-distinct eigenvalues from the channel gain correlation matrix. The results of performance analysis assuming different receiver configurations and scattering scenarios presented give valuable insight into the performance of MRC in practical Rayleigh fading scenarios for isotropic and non-isotropic scatterer distributions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1090</field>
<field name="author">David Smith</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Maximal ratio combining performance analysis in spatially correlated Rayleigh fading channels</field>
<field name="abstract">New Results for the performance analysis of maximal ratio combining (MRC) using BPSK and M-PSK modulation in spatially correlated Rayleigh fading channels are presented in terms of antenna array configuration and parameters of scatterer distributions. Revised closed-form expressions for probabilities of bit error and symbol error of BPSK and M-PSK modulations respectively are given with MRC which allow for non-distinct eigenvalues (or closely spaced eigenvalues) from the correlation matrix at the receiver. The results of performance analysis assuming different receiver configurations and scattering scenarios give valuable insights into the performance of MRC in realistic Rayleigh fading scenarios for isotropic and non-isotropic scatterer distributions</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1091</field>
<field name="author">David Smith</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Maximal Ratio Combining Performance Analysis in Spatially Correlated Rayleigh Fading Channels with Imperfect Channel Knowledge</field>
<field name="abstract">New results for the performance analysis of maximal ratio combining (MRC) using BPSK modulation in spatially correlated Rayleigh fading channels with imperfect channel knowledge are presented in terms of antenna array configuration and parameters of scatterer distributions. The utility of recently developed expressions for bit error probability is enhanced by application of a spatial correlation formulation for arbitrarily spaced antennas and general scatterer distributions. The results of performance analysis give valuable insight into the performance of MRC in realistic Rayleigh fading scenarios with imperfect channel state information for both isotropic and non-isotropic fading scenarios</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1092</field>
<field name="author">David Smith</field>
<field name="author">Thushara Abhayapala</field>
<field name="title">Generalised space-time modelling of Rayleigh MIMO channels</field>
<field name="abstract">A recently proposed space-time model for Rayleigh fading for a multiple-input multiple-output (MIMO) channel with arbitrary spatial separation at the transmitter and the receiver is extended. A space-frequency cross spectrum is derived from a space-time cross correlation for a non-isotropic scatterer distribution around the receiver. A simulation study is performed into the space-time correlation formulation to gain understanding of the changing of some relevant parameters and applying different non-isotropic distributions. The formulations presented can be applied to analyse any numbers of transmit and receive antennas operating in typical time-selective MIMO fading channels, and hence can be used in the design of space-time modems</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1093</field>
<field name="author">David Smith</field>
<field name="title">Improved quasi-orthogonal space-time block codes with hexagonal lattice modulation</field>
<field name="abstract">This paper addresses the improvement of performance of one class of quasi-orthogonal space-time block codes, for four transmit antennas, by taking the constituent code symbols from hexagonal lattices. This improvement is obtained over quasi-orthogonal block codes without constellation rotation. Due to full transmitter diversity there is similar performance to those codes with optimal constellation rotation, without the inherent constellation expansion occurred through the use of constellation rotated symbols. The improvement does not extend to other classes of quasi-orthogonal space-time block codes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1094</field>
<field name="author">David Smith</field>
<field name="title">Improved Differential Unitary Space-Time Signal Design for Two to Six Transmit Antennas</field>
<field name="abstract">This paper addresses optimization of differential unitary space-time modulation (DUSTM) for more than two transmit antennas. We demonstrate that the union bound on the block error probability of DUSTM codes can be considerably simplified for various cases of non-group DUSTM constellations. We also demonstrate that rotation matrices applied to diagonal DUSTM codes can be considered as special cases of Alamouti style block codes. Utilizing these two results, and some simplified design criteria, various DUSTM codes are derived for more than two transmit antennas which give superior performance to previously proposed codes over a wide range of SNR.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1095</field>
<field name="author">Min Zhang</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Dhammika Jayalath</field>
<field name="author">David Smith</field>
<field name="author">Chandra Athaudage</field>
<field name="title">Matched Rotation Precoding: a new Paradigm in Space-Frequency Coding</field>
<field name="abstract">This paper proposes an efficient rate one space-frequency block code (SFBC) for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) system. The proposed SFBC incorporates concept of matched rotation precoding (MRP) to achieve full transmit diversity and optimal system performance for arbitrary number of transmit antennas, subcarrier interval and subcarrier grouping. The MRP exploits the inherent rotation property of SFBC and has relaxed restrictions on subcarrier interval and subcarrier grouping, making it ideal for adaptive/time varying systems. The lowerbound of the coding gain for MRP is derived and shown that it is useful when designing a SFBC for practical scenarios e.g. when transmitters have only partial knowledge of power delay profile or when the power delay profile has only a few dominant delayed paths. Simulation results show that the MRP can achieve a similar or better performance than existing SFBCs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1096</field>
<field name="author">Tofazzal Hossain</field>
<field name="author">Kandeepan Sithamparanathan</field>
<field name="author">David Smith</field>
<field name="title">Timing Synchronization for Fading Channels with Different Characterizations using Near ML Techniques</field>
<field name="abstract">In this paper, we analyze the performance of

a non-data aided near maximum likelihood (NDA-NML)

estimator for symbol timing recovery in wireless communications.

The performance of the estimator is evaluated

for an additive noise only channel. Performance analysis

is extended to fading channels characterized by Rayleigh

fading, Weibull fading and log-normal fading, appropriate

to a variety of transmission scenarios. The probability

distribution of the maxima and probability distribution of

timing estimates are derived, presented and compared with

simulation results. The performance of the estimator is

presented in terms of the bit error rate (BER) and the error

variance of the estimates. The BER is computed when the

estimator is operating under additive white Gaussian noise

(AWGN) channel and fading channels. The variance of the

estimates is computed for the noise only case and compared

with the Cramer Rao bound (CRB) and modified Cramer

Rao bound (MCRB).</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1097</field>
<field name="author">Paul Brebner</field>
<field name="title">Service-Oriented Performance Modeling the MULE Enterprise Service Bus (ESB) Loan Broker Application</field>
<field name="abstract">Since 2006 NICTA has been developing and trialing Service-

Oriented Performance Modeling (SOPM), a method and tool support for early

life-cycle performance modeling of large-scale heterogeneous Service Oriented

Architectures (SOAs). This technology enables software architects to rapidly

build performance models of SOAs in terms of composite and simple services.

The models are therefore easy to develop, understand and maintain. They are

powerful to use, to address business, architectural, and risks associated with

mission critical services. Enterprise Service Buses (ESBs) are an increasingly

common style of SOA infrastructure and implementation technology that we

have encountered and modeled in e-Government SOA projects. In this paper we

show the application of our SOPM approach to the MULE Enterprise Service

Bus Loan Broker application in a laboratory context. We give a high-level

outline of the SOPM method, and introduce the MULE ESB and Loan Broker

application. We describe how a SOPM of the Loan Broker application is built

in terms of application business-logic services and MULE infrastructure service

components, and parameterized with measurements from an experimental testbed.

We demonstrate the validity of the approach in an initial scenario, and then

explore modeling alternative deployment and application scenarios. The utility

of the models are illustrated with example predictions of metrics such as

performance, scalability, and concurrency.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1098</field>
<field name="author">Adi Botea</field>
<field name="author">A. Anbulagan</field>
<field name="title">Analysing the Behaviour of Crossword Puzzles</field>
<field name="keyword">Problem Hardness</field>
<field name="keyword"> Model Counting</field>
<field name="keyword"> Benchmark Problem</field>
<field name="keyword"> CSP</field>
<field name="keyword"> Search in AI</field>
<field name="abstract">Crosswords have been an extremely popular puzzle across the world for many decades.

The automated generation of crosswords grids is a multi-disciplinary artificial intelligence domain. 

The puzzle has been used as a benchmark in constraint programming.

Recently, there has been a games-specific interest, aiming at developing powerful crosswords-specific engines. Despite its importance, no detailed study of this domain has been available in the past.

In this paper, we study the domain dynamics when the dictionary size and the number of

blocked cells vary, showing a multi-dimensional phase transition phenomenon.

We then introduce a model counting formula for the crosswords domain, which is useful, 

among other things, to predict the hardness of a problem. We bridge our theoretical contributions

with the task of generating crosswords grids in practice, by integrating the quality of solutions

into an easy-hard-easy transition analysis. Our analysis shows that problems whose solutions 

have better quality guarantees fall either in the hard region of an easy-hard-easy transition, or 

in the leftmost easy region, where no or very few solutions exist. This points out the need for 

better grid composition engines, able to find good-quality solutions in the hard region within 

a reasonable time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1099</field>
<field name="author">A. Anbulagan</field>
<field name="author">Alban Grastien</field>
<field name="title">Importance of Variables Semantic in CNF Encoding of Cardinality Constraints</field>
<field name="keyword">CNF Encoding</field>
<field name="keyword"> Cardinality Constraints</field>
<field name="keyword"> Diagnosis Problem</field>
<field name="keyword"> SAT Solving</field>
<field name="abstract">In the satisfiability domain, it is well-known that a SAT algorithm may solve a problem 

instance easily and another instance hardly, whilst these two instances are equivalent 

CNF encodings of the original problem. Moreover, different algorithms may disagree 

on which encoding makes the problem easier to solve. In this paper, we focus on the 

CNF encoding of cardinality constraints, which states that exactly k propositional 

variables in a given set are assigned to true. We demonstrate the importance of the 

semantics of the SAT variables in the encoding of this constraint. We implement 

several variants of the CNF encoding in which variables with close semantic are grouped. 

We then examine these new encodings on problems generated from diagnosis of 

discrete-event system. Our results demonstrate that both stochastic and systematic 

SAT algorithms can now solve most of the problem instances, which were unreachable 

previously. The results also indicate that, on average cases, there is an encoding that 

suits well both SLS and clause learning DPLL algorithms.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1100</field>
<field name="author">Alban Grastien</field>
<field name="author">A. Anbulagan</field>
<field name="title">Incremental Diagnosis of DES with a Non-Exhaustive Diagnosis Engine</field>
<field name="keyword">Diagnosis</field>
<field name="keyword"> Discrete-Event Systems</field>
<field name="keyword"> Incremental Diagnosis</field>
<field name="abstract">The incremental diagnosis of discrete-event systems is the problem of updating the

diagnosis when new observations are available. Most approaches use diagnosis engines that

return all the diagnoses, and concatenate the diagnoses of the new observations to the previous

diagnoses. In this paper, we define the non-exhaustive diagnosis engines, which return only the

preferred diagnosis, and show how these engines can be used for the incremental diagnosis. This

is done by mean of so-called prediction windows that specify a delay that is sufficient to return

a correct and accurate diagnosis.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1101</field>
<field name="author">Alban Grastien</field>
<field name="title">Symbolic Testing of Diagnosability</field>
<field name="keyword">Diagnosability</field>
<field name="keyword"> Symbolic Methods</field>
<field name="keyword"> Discrete-Event Systems</field>
<field name="abstract">Diagnosability ensures that the occurrence of a failure on the system can always be diagnosed by a diagnosis engine. 

In this paper, we explore symbolic techniques to test diagnosability of DES. 

We present several algorithms using symbolic representations, 

and show how to combine decentralised approach with symbolic approach. 

Finally, we discuss how to extract the minimal set of sensors that ensures diagnosability.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1102</field>
<field name="author">Kostia Robert</field>
<field name="title">Video-based traffic monitoring at day and night time</field>
<field name="keyword">Traffic monitoring</field>
<field name="keyword"> multiple vehicle detection</field>
<field name="keyword"> multiple vehicle tracking</field>
<field name="keyword"> computer vision</field>
<field name="keyword"> hierarchical feature-based vehicle detection</field>
<field name="keyword"> headlights</field>
<field name="keyword"> windshields</field>
<field name="keyword"> machine learning</field>
<field name="keyword"> vehicle appearance</field>
<field name="abstract">Due to the recent progress in computer vision to interpret images and sequence of images, the video camera is a promising sensor for traffic monitoring and traffic surveillance at low cost. This paper focuses on the detection and tracking of multiple vehicles present in the field of view of a camera. The camera might be fixed on a post at low to medium height (between 4 to 15 meters). Until now, the vehicle detection has been mainly performed by the widely used technique called background subtraction, which is based on detecting changes in an image sequence. While there has been long research on this technique, it still faces many challenges, which make the current commercial systems not robust enough for real applications. We present in this paper a new framework to detect vehicles, based on a hierarchy of features detection and fusion. The first layer of the hierarchy extract image features. The next layer fuses image features to detect vehicle features such as headlights or windshields. A last layer fuses the vehicle features to detect a vehicle with more confidence. This approach is thus road illumination agnostic and allows vehicles to be detected day and night. The vehicle features are tracked over frames. We use a constant acceleration tracking model augmented with traffic-domain rules to handle the occlusion problems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1103</field>
<field name="author">Renato Iannella</field>
<field name="title">A Framework For the Policy-Oriented Web in Social Networks</field>
<field name="keyword">Policy</field>
<field name="keyword"> Social Networks</field>
<field name="abstract">The key motivation of the Policy-Oriented Web (POW) regime is a focus on collaborative web interactions - typified by today s Social Networks. POW is taking the Web away from today s simple browser/server transfer of information to a more sophisticated model based on predefined and dynamic information reflecting interactions between people and resources (ie web objects) across time and space. A web object is the smallest possible resource (or information bit) that needs to be managed by a POW infrastructure. It will have needs that require policy management to enable it to successfully be part of a collaborative web 

interaction.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1104</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">EMUNE: Architecture for Effective Mobile Usage of Heterogeneous Networks</field>
<field name="keyword">Network Interface Selection</field>
<field name="keyword"> Network Availability Prediction</field>
<field name="keyword"> Data Transfer Scheduling</field>
<field name="abstract">With the mobile communication market increasingly moving towards value-added services the network cost will need to be included in the service offering itself. This will lead service providers to optimize network usage based on real cost rather than the simplified network plans sold to consumers traditionally. Meanwhile, today s mobile devices are increasingly containing multiple radios, enabling users on the move to take advantage of the heterogeneous wireless network environment. In addition, we observe that many bandwidth intensive services such as video on demand and software updates are essentially non real-time and buffers in mobile devices are effectively unlimited. We therefore propose EMUNE, a new transfer service which leverages these aspects. It supports opportunistic bulk transfers in high bandwidth networks while adapting to device power concerns, application requirements and user preferences of cost &amp; quality.

Our proposed architecture consists of an API and two main functional units. The well defined API hides all internal complexities from a programmer and provides easy access to the functionalities. The prediction engine infers future network and bandwidth availability. The scheduling engine takes the output of the prediction engine as well as the availability of power, application requirements and user preferences into account and determines which interface to use, when and for how long for all outstanding data transfer requests. Then it accordingly executes the inferred data transfer schedule. The initial results from the implementation of EMUNE s and of the prediction engine form a sound basis for further exploration of the proposed architecture.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1105</field>
<field name="author">Leif Hanlen</field>
<field name="title">Statistical characterization of the dynamic narrowband body area channel</field>
<field name="keyword">wireless</field>
<field name="keyword"> body-area-networks</field>
<field name="abstract">Slides for conference paper, tatistical characterization of the dynamic narrowband body area channel, presented at ISABEL in 2008.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1106</field>
<field name="author">Gary Overett</field>
<field name="author">Lars Petersson</field>
<field name="title">Fast Features for Time Constrained Object Detection</field>
<field name="abstract">This paper concerns itself with the development and design of fast features suitable for time constrained object detection. Primarily we consider three aspects of feature design; the form of the precomputed datatype (e.g. the integral image), the form of the features themselves (i.e. the measurements made of an image), and the models/weak-learners used to construct weak classifiers (class, non-class statistics). The paper is laid out as a guide to feature designers, demonstrating how appropriate choices in combining the above three characteristics can prevent bottlenecks in the run-time evaluation of classifiers. This leads to reductions in the computational time of the features themselves and, by providing more discriminant features, reductions in the time taken to reach specific classification error rates. While many aspects of feature design will present trade off decisions, we show that there are some feature improvements available with no change in the evaluation speed of classifiers.



 Results are compared using variants of the well known Haar-like feature types, Rectangular Histogram of Oriented Gradient (RHOG) features and a special set of Histogram of Oriented Gradient features. This set is based upon a new way of organizing and evaluating histograms of oriented gradients producing features with exceptional speed advantages as they require only one access to main memory per feature evaluation, compared to between six and nine for a typical Haar-like feature. Experimental results suggest the adoption of this set of features for time-critical applications.



 Time-constrained comparisons are presented using pedestrian and road sign detection problems. Comparison results are presented on time-error plots, which are a replacement of the traditional ROC performance curves.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1107</field>
<field name="author">Franck Cassez</field>
<field name="title">How to Install PHAVer on Mac OS X</field>
<field name="keyword">Hybrid systems</field>
<field name="keyword"> PHAVer</field>
<field name="abstract">This short note explains how to install PHAVer (and the

 needed libraries) for a single user on Mac OS X

 running Leopard (10.5.6). It might also work for

 earlier versions (you can report to me in case of a

 successful install on earlier version).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1108</field>
<field name="author">Franck Cassez</field>
<field name="author">J r my Dubreil</field>
<field name="author">Herv Marchand</field>
<field name="title">Dynamic Observers for the Synthesis of Opaque Systems</field>
<field name="keyword">security</field>
<field name="keyword"> opacity</field>
<field name="keyword"> dynamic observation</field>
<field name="abstract">In this paper, we address the problem of synthesizing \emph{opaque}

 systems. A secret predicate $S$ over the runs of a system $G$ is

 \emph{opaque} to an external user having partial observability over

 $G$, if s/he can never infer from the observation of a run of $G$

 that the run belongs to $S$. We first investigate the case of

 \emph{static} partial observability where the set of events the user

 can observe is fixed a priori. In this context, we show that

 checking whether a system is opaque is PSPACE-complete, which

 implies that computing an optimal static observer ensuring opacity

 is also a PSPACE-complete problem.



 Next, we introduce \emph{dynamic} partial observability where the

 set of events the user can observe changes over time.



 We show how to check that a system is opaque \wrt to a dynamic

 observer and also address the corresponding synthesis problem: given

 a system $G$ and secret states $S$, compute the set of dynamic

 observers under which $S$ is opaque. Our main result is that the set

 of such observers can be finitely represented and can be computed in

 EXPTIME.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1109</field>
<field name="author">Morgan Deters</field>
<field name="author">Roberto Nieuwenhuis</field>
<field name="author">Peter Stuckey</field>
<field name="author">Ignasi Abio</field>
<field name="title">Reducing Chaos in SAT-like Search: Finding Solutions Close to a Given One</field>
<field name="abstract">SAT and its extensions like SAT Modulo Theories or Lazy 

Clause Generation can handle many problems e ciently and fully auto- 

matically. However, their activity-based variable selection heuristics make 

search chaotic, i.e., extremely sensitive to the initial conditions. There- 

fore, re-running with just one additional input constraint usually gives 

a completely di erent solution, a well-known unpleasant drawback of 

SAT-like techniques. 

Here we propose several approaches for handling such slightly extended 

problems in order to nd close solutions, i.e., with few changes to the 

original one. We discuss the respective tradeo s of our approaches in 

terms of solving time and distance to the closest solution (according to 

suitable metrics) and provide results of careful experiments on industrial 

scheduling benchmarks.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1110</field>
<field name="author">Michael Codish</field>
<field name="author">Samir Genaim</field>
<field name="author">Peter Stuckey</field>
<field name="title">A Declarative Encoding of Telecommunications Feature Subscription</field>
<field name="abstract">This paper describes the encoding of a telecommunications fea- 

ture subscription con guration problem to propositional logic and 

its solution using a state-of-the-art Boolean satisfaction solver. The 

transformation of a problem instance to a corresponding proposi- 

tional formula in conjunctive normal form is obtained in a declar- 

ative style. An experimental evaluation indicates that our encoding 

is considerably faster than previous approaches based on the use 

of Boolean satisfaction solvers. The key to obtaining such a fast 

solver is the the careful design of the Boolean representation and 

of the basic operations in the encoding. The choice of a declara- 

tive programming style makes the use of complex circuit designs 

relatively easy to incorporate into the encoder and to ne tune the 

application.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1111</field>
<field name="author">Raphael Reischuk</field>
<field name="author">Christian Schulte</field>
<field name="author">Peter Stuckey</field>
<field name="author">Guido Tack</field>
<field name="title">Maintaining State in Propagation Solvers</field>
<field name="abstract">Constraint propagation solvers interleave propagation, re- 

moving impossible values from variable domains, with search. The state 

of the solver is modi ed during propagation. But search requires the 

solver to return to a previous state. Hence a propagation solver must 

determine how to maintain state during propagation and forward and 

backward search. In this paper we set out the possible ways in which a 

propagation solver can choose to maintain state, and the restrictions that 

such choices place on the resulting system. We provide experiments to 

illustrate the result of various choices for the three principle state com- 

ponents of a solver: variables, propagators, and dependencies between 

them. We also make the rst realistic comparison of trailing versus copy- 

ing for state restoration.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1112</field>
<field name="author">Michael Norrish</field>
<field name="title">Complete Integer Decision Procedures as Derived Rules in HOL</field>
<field name="abstract">I describe the implementation of two complete decision procedures for integer Presburger arithmetic in the HOL theorem-proving system. The rst procedure is Cooper s algorithm, the second, the Omega Test. Between them, the algorithms illustrate three different implementation techniques in a fully expansive system.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1113</field>
<field name="author">Alban Grastien</field>
<field name="author">Priscilla Kan John</field>
<field name="title">Wizards of Oz -- Description of the 2009 DXC Entry</field>
<field name="keyword">Diagnosis</field>
<field name="abstract">This paper describes Wizards of Oz, our entry to Tiers

1 and 2 of the industrial track of DX Competition. We

present the algorithm along with the hypotheses made,

the known limitations, and the possible improvements.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1114</field>
<field name="author">peter bruza</field>
<field name="author">kirsty kitto</field>
<field name="author">brentyn ramm</field>
<field name="author">Laurianne Sitbon</field>
<field name="author">dawei song</field>
<field name="title">Concept combination, emergence and abduction</field>
<field name="keyword">conceptual combination</field>
<field name="keyword"> abduction</field>
<field name="keyword"> tensor.</field>
<field name="abstract">Humans frequently produce emergent properties or associates when com- 

bining concepts in new ways. This presentation will examine the manner 

in which concept combination can generate emergent properties suggest- 

ing that the process is abductive in nature. A tensor based approach is 

used to model concept combinations which allows such combinations to be 

viewed as interactions in a quantum-like way. Free association norm data 

is used to motivate the underlying basis of the dimensional space. Concept 

combinations will be viewed in a spectrum according to the degree of non- 

separability of the corresponding tensor representations. It is conjectured 

that non-separable tensors correspond to conceptual combinations with the 

propensity to yield emergent property.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1115</field>
<field name="author">david novakovic</field>
<field name="author">Peter Bruza</field>
<field name="author">Laurianne Sitbon</field>
<field name="title">Inducing Shades of Word Meaning by Matrix Methods</field>
<field name="abstract">This article explores two matrix methods to induce the ``shades of meaning" (SoM) of a word. 

A matrix representation of a word is computed from a corpus of traces based on the given word. Non-negative Matrix Factorisation (NMF) and Singular Value Decomposition (SVD) compute a set of vectors corresponding to a potential shade of meaning. The two methods were evaluated based on loss of conditional entropy with respect to two sets of manually tagged data. One set reflects concepts generally appearing in text, and the second set comprises words used for investigations into word sense disambiguation. Results show that for NMF consistently outperforms SVD for inducing inducing both SoM of general concepts as well as word senses. The problem of inducing the shades of meaning of a word is more subtle than that of word sense induction</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1116</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Process flexibility: A design view and specification schema</field>
<field name="abstract">This paper proposes a framework of process flexibility based on a view of processes as design objects. It is represented using the function-behaviour-structure (FBS) ontology of designing. The paper shows how the FBS ontology allows extending and generalising recent work on flexibility in engineering design, and how it allows applying this work to processes. The resulting framework provides a comprehensive account of process flexibility that subsumes existing approaches. Finally, the paper presents a specification schema for process flexibility, illustrated using examples of a property valuation process in the Australian lending industry.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1117</field>
<field name="author">Wern Wong</field>
<field name="author">David Martinez</field>
<field name="author">Lawrence Cavedon</field>
<field name="title">Extraction of Named Entities from Tables in Gene Mutation Literature</field>
<field name="abstract">Information extraction and text mining are receiving growing attention as useful techniques for addressing the crucial information bottleneck in the biomedical domain. We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how this technique can be leveraged to improve on existing named entity detection systems for mutations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1118</field>
<field name="author">Yuye Zhang</field>
<field name="author">Laurence Park</field>
<field name="author">Alistair Moffat</field>
<field name="title">Parameter Sensitivity in Rank-Biased Precision</field>
<field name="keyword">Rank-Biased Precision</field>
<field name="keyword"> Evaluation</field>
<field name="keyword"> System Comparison</field>
<field name="abstract">Rank-Biased Precision (RBP) is a retrieval evaluation metric that assigns an effectiveness score to a ranking by computing a geometricly weighted sum of document relevance values, with the monotonicly decreasing weights in the geometric distribution determined via a persistence parameter p. Despite exhibiting various advantageous traits over well known existing measures such as Average Precision, RBP has the drawback of requiring the designer of any experiment to choose a value for p. Here we present a method that allows retrieval systems evaluated using RBP with different p values to be compared. The proposed approach involves calculating two critical bounding relevance vectors for the original RBP score, and using those vectors to calculate the range of possible RBP scores for any other value of p. Those bounds may then be sufficient to allow the outright superiority of one system over the other to be established. In addition, the process can be modified to handle any RBP residuals associated with either of the two systems. We believe the adoption of the comparison process described in this paper will greatly aid the uptake of RBP in evaluation experiments.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1119</field>
<field name="author">Phu Ngoc Le</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Julien Epps</field>
<field name="title">A Non-Uniform Subband Approach to Speech-Based Cognitive Load Classification</field>
<field name="keyword">cognitive load</field>
<field name="keyword"> classification</field>
<field name="keyword"> subband</field>
<field name="keyword"> frequency dependence</field>
<field name="abstract">Speech has recently been recognized as an attractive method for the measurement of cognitive load. Current speech-based cognitive load measurement systems utilize acoustic features derived from auditory-motivated frequency scales. This paper aims to investigate the distribution of speech information specific to cognitive load discrimination as a function of frequency. We found that this distribution is neither uniform nor very similar to the Mel auditory scale and based on our experiments, we propose a novel non-uniform filterbank for acoustic feature extraction to classify cognitive load. Experimental results showed that the use of the proposed filterbank provided a relative improvement of about 10.4%, compared with the classification accuracy of the traditional cognitive load classification system based on a Mel-scale filterbank.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1120</field>
<field name="author">Terence Chen</field>
<field name="author">Olivier Mehani</field>
<field name="author">Roksana Boreli</field>
<field name="title">Trusted Routing for VANET</field>
<field name="keyword">vanet</field>
<field name="keyword"> trust</field>
<field name="keyword"> routing</field>
<field name="abstract">To build trust between two entities in VANET is particularly difficult due to lack of infra-structure, openness of wireless links and the usually highly dynamic network topology. To address these problems, we propose an effective trusted routing mechanism that provides node-to-node trust and routability verification, without assistance of Certificate Authorities (CA). By applying this framework to the Optimized Link State Routing Protocol(OLSR), we demonstrate how this mechanism can be used to establish trusted routes.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1121</field>
<field name="author">Olivier Mehani</field>
<field name="author">Roksana Boreli</field>
<field name="author">Thierry Ernst</field>
<field name="title">Context-Adaptive Vehicular Network Optimization</field>
<field name="keyword">IPv6 mobility</field>
<field name="keyword"> performance optimization</field>
<field name="keyword"> constraint satisfaction problem</field>
<field name="keyword"> context-aware</field>
<field name="abstract">We propose a framework to optimize the communication performance and mobility

management in vehicular networks. By having a single unified decision algorithm

taking into account both stack-related and external contextual information such

as GPS localization or signaling from other nodes, advice can be provided to

every layer in the network stack to allow for globally optimized, faster and

more accurate adaptation to the current conditions. We present how key example

scenarios would benefit from such a system. We describe an instance of this

framework using a constraint satisfaction problem (CSP) approach. We also

describe our prototype implementation of the network data collection system and

give some timing evaluation for a given constraint solver.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1122</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="author">Ivan Seskar</field>
<field name="author">Guillaume Jourjon</field>
<field name="title">OMF: a Control and Management Framework for Networking Testbeds</field>
<field name="abstract">Networking testbeds are playing an increasingly important role in the development of new communication technologies. Testbeds are traditionally built for a particular project or to study a specific technology. An alternative approach is to federate existing testbeds to a) cater for experimenter needs which cannot be fulfilled by a single testbed, and b) provide a wider variety of environmental settings at different scales. These heterogenous settings allow the study of new approaches in environments similar to what one finds in the real world. 



This paper presents OMF, a control, measurement, and management framework for testbeds. It describes through some examples the versatility of OMF's current architecture and gives directions for federation of testbeds through OMF. In addition, this paper introduces a comprehensive experiment description language that allows an experimenter to describe resource requirements and their configurations, as well as experiment orchestration. Researchers would thus be able to reproduce their experiment on the same testbed or in a different environment with little changes. Along with the efficient support for large scale experiments, the use of testbeds and support for repeatable experiments will allow the networking field to build a culture of cross verification and therefore strengthen its scientific approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1123</field>
<field name="author">Jun Zhou</field>
<field name="author">Fu Zhouyu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Learning the Optimal Transformation of Salient Features for Image Classification</field>
<field name="abstract">In this paper, we address the problem of recovering an optimal salient image descriptor transformation for image

classification. Our method involves two steps. Firstly, a binary salient map is generated to specify the regions of interest for subsequent image feature extraction. To this end, an optimal cut-off value is recovered by maximising Fisher s linear discriminant separability measure so as to separate the salient regions from the background of the scene. Next, image descriptors are extracted in the foreground region in order to be optimally transformed. The descriptor optimisation problem is cast in a regularised risk minimisation

setting, in which the aim of computation is to recover the optimal transformation up to a cost function. The cost function is convex and can be solved using quadratic programming. The results on unsegmented Oxford Flowers

database show that the proposed method can achieve classification performance that are comparable to those provided by alternatives elsewhere in the literature which employ pre-segmented images.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1124</field>
<field name="author">Joel Veness</field>
<field name="author">William Uther</field>
<field name="author">Alan Blair</field>
<field name="author">Silver David</field>
<field name="title">Bootstrapping from Game Tree Search</field>
<field name="keyword">reinforcement learning</field>
<field name="keyword"> gradient descent</field>
<field name="keyword"> minimax search</field>
<field name="abstract">In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuel's checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1125</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="title">Energy aware network selection using traffic estimation</field>
<field name="keyword">Vertical handover</field>
<field name="keyword"> traffic estimation</field>
<field name="keyword"> network selection</field>
<field name="keyword"> energy aware</field>
<field name="abstract">This paper proposes an energy aware handoff algorithm based on

empirically measured energy consumption of UMTS and 802.11 WLAN

networks on an Android mobile phone. The handoff algorithm uses

estimates of application traffic to find the minimum energy cost

alternative by comparing the cost of using UMTS with the cost of

performing an opportunistic downward vertical handoff to a WLAN and

using WLAN for the transfer and the eventual upward vertical handoff

back to UMTS. Our experiments show that the energy cost of UMTS is

nearly equal to WLAN as a function of transfer time, but for bulk

transfers, transferring a byte of data over UMTS can be up to a hundred

times as expensive as over WLAN. Further, we discovererd that the

energy cost of the vertical handoff is quite high, comparable to

transferring 0.1 - 0.8 MB of data over UMTS. To calculate the energy

cost of data transfers before they take place, we propose and evaluate

a distributed traffic estimation mechanism. The mechanism can predict

how much data will be transferred due to a user action, i.e. clicking

of an URL link. We provide initial results on the accuracy of the

mechanisms using two estimation algorithms. Finally, we perform a

numerical analysis on the the handoff algorithm to evaluate when a

vertical handoff would be beneficial using empirically measured

parameters.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1126</field>
<field name="author">Roksana Boreli</field>
<field name="author">Ge Yu</field>
<field name="author">Thava Iyer</field>
<field name="author">Christoph Dwertmann</field>
<field name="author">Shankar Pathmasuntharam</field>
<field name="title">Intelligent Middleware for High Speed Maritime Mesh Networks with Satellite Communications</field>
<field name="keyword">middleware</field>
<field name="keyword"> maritime mesh</field>
<field name="keyword"> WiMax</field>
<field name="keyword"> satellite</field>
<field name="abstract">We provide an overview of a Maritime Intelligent Transport System

(ITS) which consists of a high speed maritime mesh network,

satellite communications and middleware components. The work is an

outcome of a collaboration project between I$^{\textrm{\tiny{2}}}$R

and NICTA. We describe details of the developed proof of concept

platform, including the architecture and software modules. We

further provide details of the validation testing of the POC system

and outline plans for future enhancements.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1127</field>
<field name="author">Zhouyu Fu</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">Mixing Linear SVMs for Nonlinear Classification</field>
<field name="keyword">EM Algorithm</field>
<field name="keyword"> Classification</field>
<field name="keyword"> Mixture of Experts</field>
<field name="keyword"> Model Selection</field>
<field name="keyword"> Support Vector Machines</field>
<field name="abstract">In this paper, we address the problem of combining linear Support Vector Machines (SVMs) for classification

of large-scale nonlinear data sets. The motivation is to exploit both the efficiency of linear SVMs in learning and prediction and the power of nonlinear SVMs in classification. To this end, we develop a linear SVM mixture model that exploits a divide-and-conquer strategy by partitioning the feature space into sub-regions of linearly separable data-points and learning a linear SVM for each of these regions. We do this implicitly by deriving a generative model over the joint data and label distributions. Consequently, we can impose priors on the mixing coefficients and do implicit model selection in a top-down manner during the parameter estimation process. This guarantees the sparsity of the learned model. Experimental results show that the proposed method can achieve the efficiency of linear SVMs in the prediction phase while still providing a classification performance comparable to nonlinear SVMs.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1128</field>
<field name="author">Jenny Liu</field>
<field name="author">Kate Foster</field>
<field name="author">Thong Nguyen</field>
<field name="author">Jacky Keung</field>
<field name="title">Quality Assessment of Mission Critical Middleware System Using MEMS</field>
<field name="keyword">middleware</field>
<field name="keyword"> architecture evaluation</field>
<field name="keyword"> experiment</field>
<field name="keyword"> assessment process</field>
<field name="abstract">Architecture evaluation methods provide general guidelines

to assess quality attributes of systems, which are not

necessarily straightforward to practice with. With COTS

middleware based systems, this assessment process is further

complicated by the complexity of middleware technology

and a number of design and deployment options. Efficient

assessment is key to produce accurate evaluation results

for stakeholders to ensure good decisions are made on

system acquisition. In this paper, a systematic evaluation

method called MEMS is developed to provide some structure

to this assessment process. MEMS produces the evaluation

plan with thorough design of experiments, definition

of metrics and development of techniques for measurement.

This paper presents MEMS and its application to a mission

critical middleware system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1129</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Generalised Pinsker Inequalities</field>
<field name="abstract">We generalise the classical Pinsker inequality

which relates variational divergence to Kullback-

Liebler divergence in two ways: we consider

arbitrary f-divergences in place of KL divergence,

and we assume knowledge of a sequence

of values of generalised variational divergences.

We then develop a best possible inequality for

this doubly generalised situation. Specialising

our result to the classical case provides a new

and tight explicit bound relating KL to variational

divergence (solving a problem posed by

Vajda some 40 years ago). The solution relies

on exploiting a connection between divergences

and the Bayes risk of a learning problem

via an integral representation.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1130</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Information, Divergence and Risk for Binary Experiments</field>
<field name="abstract">We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds),

proper scoring rules, matching losses, cost curves, ROC-curves and information. We do

this by systematically studying integral and variational representations of these objects

and in so doing identify their primitives which all are related to cost-sensitive binary

classi&#12;cation. As well as clarifying relationships between generative and discriminative

views of learning, the new machinery leads to tight and more general surrogate loss

bounds and generalised Pinsker inequalities relating f-divergences to variational divergence.

The new viewpoint illuminates existing algorithms: it provides a new derivation

of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy

to Fisher Linear Discriminants. It also suggests new techniques for estimating

f-divergences.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1131</field>
<field name="author">Christoph Dwertmann</field>
<field name="author">Mesut Erdin</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Max Ott</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Ivan Seskar</field>
<field name="title">Mobile Experiments Made Easy with OMF/Orbit</field>
<field name="abstract">We propose to showcase the experiment facility control, management &amp; measurement framework (OMF) with an experiment demonstrating a common investigative life cycle. The demo scenario will highlight the steps from the idea, to simulations, and finally testbed simulations on the ORBIT outdoor testbed using a variety of network access networks, such as WiFi and Wimax.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1132</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Autonomic Business-Driven Dynamic Adaptation of Service-Oriented Systems and the WS-Policy4MASC Support for Such Adaptation</field>
<field name="keyword">business-driven IT management</field>
<field name="keyword"> key performance indicator (KPI)</field>
<field name="keyword"> autonomic computing</field>
<field name="keyword"> dynamic adaptation</field>
<field name="keyword"> policy-driven management</field>
<field name="keyword"> policy specification</field>
<field name="keyword"> policy conflict resolution</field>
<field name="keyword"> service-oriented computing</field>
<field name="keyword"> Web service</field>
<field name="keyword"> Web service management</field>
<field name="abstract">When a need for dynamic adaptation of an information technology (IT) system arises, often several alternative approaches can be taken. Maximization of technical quality of service (QoS) metrics (e.g., response time, availability) need not maximize business value metrics (e.g., profit, customer satisfaction). The goal of autonomic business-driven IT system management (BDIM) is to ensure that operation and adaptation of IT systems maximizes business value metrics, with minimal human intervention. We present how our WS-Policy4MASC language for specification of management policies for service-oriented systems supports autonomic BDIM. WS-Policy4MASC extends WS-Policy with new types of policy assertions: goal, action, probability, utility, and meta-policy assertions. Its main distinctive characteristics are description of diverse business value metrics and specification of policy conflict resolution strategies for maximizations of business value metrics according to various business strategies. Our decision making algorithms use this additional WS-Policy4MASC information to choose the adaptation approach best from the business viewpoint.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1133</field>
<field name="author">Shihab Hamid</field>
<field name="author">Bernhard Hengst</field>
<field name="title">Learning and Recognition of 3D Visual Objects in Real-Time</field>
<field name="keyword">Computer vision</field>
<field name="keyword"> SIFT</field>
<field name="keyword"> object recognition</field>
<field name="keyword"> aspect-graphs</field>
<field name="keyword"> GPU</field>
<field name="abstract">Quickly learning and recognising familiar objects seems almost automatic for humans, yet it remains a challenge for machines. This paper describes an integrated object recognition system including several novel algorithmic contributions using a SIFT feature appearance-based approach to rapidly learn incremental 3D representations of objects as aspect-graphs. A fast recognition scheme applying geometric and temporal constraints localizes and identifies the pose of 3D objects in a video sequence. The system is robust to significant variation in scale, orientation, illumination, partial deformation, occlusion, focal blur and clutter and recognises objects at near real-time video rates.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1134</field>
<field name="author">Weihong Wang</field>
<field name="author">Jian Zhang</field>
<field name="author">Chunhua Shen</field>
<field name="author"/>
<field name="title">A two-layer Night-Time vehicle detector</field>
<field name="abstract">We present a two-layer night time vehicle detector. At the first layer, headlight detection [ref] is conducted to allocate areas (eg, bounding box) where are the possible pairs of the headlights in the image, the Haar feature based Adaboost framework are then applied to decide the vehicle front at night time. This approach has achieved significant performance for vehicle detection at night time. Our results showed that the proposed algorithm can reach over 90% of detection rate at 1.5% false positive rate. Without any code optimization, it also performs at a faster speed compared to Haar feature based Adaboost approach</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1135</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="author">Chunhua Shen</field>
<field name="author">Jian Zhang</field>
<field name="title">Incremental Training a Detector Using Online Sparse Eigen-decomposition</field>
<field name="keyword">Object detection</field>
<field name="keyword"> asymmetry</field>
<field name="keyword"> greedy sparse linear discriminant analysis</field>
<field name="keyword"> online linear discriminant analysis</field>
<field name="keyword"> feature selection</field>
<field name="keyword"> cascade classifier</field>
<field name="abstract">The ability to efficiently and accurately detect predefined

objects is very crucial in many vision applications. Recently,

offline object detectors have shown a tremendous success.

However, one major drawback of offline techniques is that a

complete set of training data has to be collected beforehand.

In addition, once learned, an offline detector can not make use

of newly arriving data. To alleviate these drawbacks, online

learning has been adopted with the following objectives:- the

technique should be computational and storage efficient; and the

updated classifier must maintain its high classification accuracy.

In this paper, we propose an effective and efficient framework for

learning an adaptive greedy sparse linear discriminant analysis

(GSLDA) model for object detection. Unlike many existing

algorithms, which apply exponential loss, our online algorithm

makes use of LDA s learning criterion which aims to maximize

the class-separation criterion. We demonstrate the robustness

and efficiency of our methods on digit and face data sets. Our

results confirm that object detection tasks benefit significantly

when trained in an online manner.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1136</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Waqar Mahmood</field>
<field name="title">Towards Reliable Communication over WLAN</field>
<field name="keyword">Congestion window</field>
<field name="keyword"> Bit error rate</field>
<field name="keyword"> Disconnections</field>
<field name="abstract">This paper presents the comparative results of

legacy TCP protocol with TCP Detection and Recovery (TCPDR)

over Infrastructure based Wireless Networks. The newly

proposed architecture of TCP-DR minimizes the losses due to

channel errors and short disconnections. It enhances the TCP

Reno functionality by modifying modules at the base station,

which mitigates the bandwidth under utilization problem over

infrastructure-based wireless network communication. The

simulation results illustrate that performance can be improved

up to 43% in the presence of high channel errors and short

disconnections.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1137</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Interference in Body Area Networks: Distance does not dominate</field>
<field name="abstract">Inter-network interference is a significant source of difficulty for wireless body area networks. Movement, proximity and the lack of central coordination all contribute to this problem. We compare the interference power of multiple BAN devices when a group of people move randomly within an office area. We find that the path loss trend is dominated by local variations in the signal, and not free-space path loss exponent.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1138</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Interference in Body Area Networks: Are signal-links and interference-links independent?</field>
<field name="abstract">Network-to-network interference is a challenging problem in wireless body area networks which move frequently and operate in a non-coordinated way. In this paper, we characterize interference based on field measurements. We examine the correlation and independence between the desired (signal) and interference channels. The results show that in a long period, the two channels are statistically uncorrelated and show less dependence, however, in a short period, they may become correlated, particularly when the body/body component remains still.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1139</field>
<field name="author">Franck Cassez</field>
<field name="author">J r my Dubreil</field>
<field name="author">Herv Marchand</field>
<field name="title">Dynamic Observers for the Synthesis of Opaque Systems</field>
<field name="keyword">security</field>
<field name="keyword"> opacity</field>
<field name="keyword"> synthesis</field>
<field name="abstract">In this paper, we address the problem of synthesizing \emph{opaque}

 systems. A secret predicate $S$ over the runs of a system $G$ is

 \emph{opaque} to an external user having partial observability over

 $G$, if s/he can never infer from the partial observation of a run

 of $G$ that the run satisfies $S$. We first investigate the case of

 \emph{static} partial observability where the set of events the user

 can observe is fixed a priori. In this context, we show that

 checking whether a system is opaque is PSPACE-complete, which

 implies that computing an optimal static observer ensuring opacity

 is also PSPACE-complete.

%

 Next, we introduce \emph{dynamic} partial observability where the

 set of events the user can observe changes over time.

%

 We show how to check that a system is opaque wrt a dynamic observer

 and also address the corresponding synthesis problem: given a system

 $G$ and secret states $S$, compute the set of dynamic observers

 under which $S$ is opaque. Our main result is that the synthesis

 problem can be solved in EXPTIME.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1140</field>
<field name="author">Worapan Kusakunniran</field>
<field name="author">Jian Zhang</field>
<field name="author">Qiang Wu</field>
<field name="author">Hongdong Li</field>
<field name="title">Multiple Views Gait Recognition using View Transformation Model of Gait Energy Image</field>
<field name="keyword">multiple view</field>
<field name="keyword"> gait recognition</field>
<field name="keyword"> singular value decomposition</field>
<field name="keyword"> linear discriminant analysis</field>
<field name="abstract">Gait is one of well recognized biometrics that has been widely used for human identi cation. However, viewing direction change causes many dif culties to the current gait recognition because the viewing angle under which the gait signature database was established may not be the same as the viewing angle when the probe data will be obtained. This paper proposes a new multi-view gait recognition approach which tackles the problems mentioned above. This method does not reconstruct 3D view information using calibration. Being different from other approaches of same category, this new method creates a so called View Transformation

Model (VTM) based on spatial-domain Gait Energy Image (GEI) by adopting Singular Value Decomposition (SVD) technique. To further improve the performance

of the proposed VTM, Linear Discriminant Analysis (LDA) is used to optimize the obtained GEI feature vectors. When implementing SVD there are a few practical problems such as large matrix size and over- tting. In this paper, reduced SVD is introduced to alleviate the effects caused by these problems. Using the well constructed VTM, the viewing angles of gallery gait data and probe gait data can be transformed into the same direction. Thus, gait signatures can be measured without dif culties. The extensive experiments show that the proposed algorithm can signi cantly improve the multiple view gait recognition performance when being

compared to the similar methods inliterature.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1141</field>
<field name="author">Renato Iannella</field>
<field name="author">Sarath Indrakanti</field>
<field name="title">Partial Social Profiles: How to be a Social Butterfly and Urban Citizen at the Same Time</field>
<field name="abstract">Social Networks have been a world-wide phenomenon and their proliferation poses a serious interoperability and usability challenge to both end users and service providers. Recently governments have seen this new opportunity to gather input from public citizens on their activities or to promote their services via this new Web 2.0 vehicle.



Many Web users have many different social networks accounts and utilise them in different ways depending on the context. For example, more friendly chat on FaceBook, more professional on LinkedIn, and a bit daring interaction on Hi5. Maintaining these online profiles is cumbersome and time consuming. A new social network will find it harder to attract new members simply because of the effort involved in maintaining yet-another-profile.



What is needed is the ability for a user to control their own profile and, more importantly, what aspects of the profile they will expose to different social networks. This gives the user the idea of centralised control, even though the data could be distributed, by empowering them to decide how they can be viewed and contextualised by different services. So to be a Social Butterfly on Hi5 they can expose characteristics relevant to that site (for example, my favourite drink), and to be a good Urban Citizen they can expose relevant attributes (for example, my home address) and many other contextual roles in between.



Participation is key to social networks and critical if such a network is to be successful in the government sphere of engaging with the public. And the ability to slice-up your profile depending on the context of the social network will be the driving factor that expands participation beyond the big players like FaceBook and MySpace. Also a user should be able to store their profile or parts of their profile at a location of their choice. For example, a user might want to store their personal information such as home address and telephone number on FaceBook and their work-related information such as office address and office telephone number on LinkedIn, or may even want to store their entire profile locally on their PC. This is not possible today. Open standards for interoperable social profiles are the enabling technology. They allow more social networks to give users more control of their profiles. 



Currently social network aggregators such as Friendfeed and Cliqset aggregate user information into one place but do not give the user a choice of where to store their profiles. Neither do they explicitly inform the user if their aggregated profile is being stored at their servers. A user may trust a certain network with holding only their social information but not their professional information. Also distributing parts of a user's profile to multiple locations and/or allowing the user to store their profile locally on their PC enables the removal of data silos. This also enhances user privacy and in turn much greater participation in new social networks intended for a specific purpose such as allowing Urban Citizens to raise their voice towards a certain cause.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1142</field>
<field name="author">Stefan Lehmann</field>
<field name="author">Scott Bolland</field>
<field name="author">Roger Remington</field>
<field name="author">Michael Humphreys</field>
<field name="author">Selina Fothergill</field>
<field name="author">Samuel Hasenbosch</field>
<field name="author">Andrew Neal</field>
<field name="title">Evaluation of a Model of Expert Decision Making in Air Traffic Control</field>
<field name="keyword">Decision Making</field>
<field name="keyword"> Expertise</field>
<field name="keyword"> Air Traffic Control</field>
<field name="abstract">Experts are capable of performing complex tasks using relatively simple strategies that are adapted to fit the constraints of the problem at hand. The resolution of conflicts between aircraft, for example, is regarded as a very complex optimisation problem, yet air traffic controllers are able to perform the task using a small set of simple strategies. Much of the expertise of the air traffic controller lies in the ability to select the appropriate strategy for the problem. In this project, we are developing a model of expert decision making for the air traffic control conflict resolution task. This is part of a broader project that is developing a new approach for simulating the tasks that a human operator performs, and the workload that the human experiences while carrying out those tasks. 



We model expert decision making as a serial search process in a hierarchical tree, in which the selection of a decision option for further evaluation is constrained by the situation. In this paper, we will present an analysis of the behaviour of an initial version of this model. The key aim of this analysis is to compare the model s behaviour against the behaviour of expert controllers under varying scenario complexity. The analysis relies on both data from our model simulation runs and recordings of the activities of 14 air traffic Controllers. It is based on the frequencies of different intervention classes in four different static aircraft scenarios of varying complexity.



This paper is structured as follows: We will first present a functional overview of our conflict resolution model, including its underlying operational concepts. Both the aims and the general methodology underlying the analysis of the model will then be described. Results will be presented, identifying both the behavioural similarities and discrepancies between the model and human air traffic controllers. There is close agreement between the model and the humans in the selection of aircraft for intervention, although the humans are more variable than the model. Human controllers are also more variable in the solutions that they generate, and have a tendency to prefer vertical solutions, unlike the model which has a preference for lateral solutions. The results suggest that the modelling approach that we are taking has promise, and that it is possible to simulate expert decision making in complex dynamic tasks using relatively simple models of the decision process.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1143</field>
<field name="author">Christian Bessiere</field>
<field name="author">Emmanuel Hebrard</field>
<field name="author">Brahim Hnich</field>
<field name="author">Zeynep Kiziltan</field>
<field name="author">Toby Walsh</field>
<field name="title">Range and Roots: Two common patterns for specifying and propagating counting and occurrence constraints</field>
<field name="keyword"/>
<field name="abstract">We propose Range and Roots which are two common patterns useful for specifying a wide range of counting and occurrence constraints. We design specialised propagation algorithms for these two patterns. Counting and occurrence constraints specified using these patterns thus directly inherit a propagation algorithm. To illustrate the capabilities of the Range and Roots constraints, we specify a number of global constraints taken from the literature. Preliminary experiments demonstrate that propagating counting and occurrence constraints using these two patterns leads to a small loss in performance when compared to specialised global constraints and is competitive with alternative decompositions using elementary constraints.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1144</field>
<field name="author">Maria Pini</field>
<field name="author">Francesca Rossi</field>
<field name="author">Brent Venable</field>
<field name="author">Toby Walsh</field>
<field name="title">Aggregating partially ordered preferences</field>
<field name="abstract">Preferences are not always expressible via complete linear orders: some-

times it is more natural to allow for the presence of incomparable outcomes. This

may hold both in the agents' preference ordering and in the social order. In this

paper we consider this scenario and we study what properties it may have. In par-

ticular, we show that, despite the added expressivity and ability to resolve con

icts

provided by incomparability, classical impossibility results (such as Arrow's theorem,

Muller-Satterthwaite's theorem, and Gibbard-Satterthwaite's theorem) still hold. We

also prove some possibility results, generalizing Sen's theorem for majority voting.

To prove these results, we de&#12;ne new notions of unanimity, monotonicity, dictator,

triple-wise value-restriction, and strategy-proofness, which are suitable and natural

generalizations of the classical ones for complete orders.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1145</field>
<field name="author">Mark Staples</field>
<field name="author">Mahmood Niazi</field>
<field name="title">Two case studies on small enterprise motivation and readiness for CMMI</field>
<field name="abstract">The Capability Maturity Model Integration (CMMI) is a structured representation of process areas that can support an organization's software development competency. CMMI can be used as a framework for Software Process Improvement (SPI). Some large organizations have received productivity and product quality benefits from achieving high "levels" of CMMI. However, CMMI is sometimes thought to be difficult to apply to Small and Medium-sized Enterprises (SMEs), and its relevance to SMEs is not yet clear. This paper describes two case studies of small enterprises that adopted CMMI for SPI. We examine their motivation for SPI, and also their organizational readiness for SPI. The two companies in our case study are similar, but have contrasting reasons for adopting SPI. Company A adopted CMMI for marketing reasons, and Company B listed many reasons. Although a valid business rationale, marketing reasons are usually maligned within the SPI community. Nonetheless, Company A had the highest level of readiness. We suggest that having clear goals for SPI is critical, even if those goals are not stereotypical.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1146</field>
<field name="author">Diego Arroyuelo</field>
<field name="author">Francisco Claude</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Veli M _kinen</field>
<field name="author">Gonzalo Navarro</field>
<field name="author">Kim Nguyen</field>
<field name="author">Jouni Siren</field>
<field name="author">Niko V _lim _ki</field>
<field name="title">Fast In-Memory XPath Search over Compressed Text and Tree Indexes</field>
<field name="abstract">A large fraction of an XML document typically consists

of text data.

The XPath query language allows text search via

the equal, contains, and starts-with predicates.

Such predicates can efficiently be implemented using

a compressed self-index of the document's text nodes.

Most queries, however, contain some parts of querying

the text of the document, plus some parts

of querying the tree structure.

It is therefore a challenge to choose an appropriate

evaluation order for a given query, which optimally leverages

the execution speeds of the text and tree indexes.

Here the SXSI system is introduced; it stores the tree

structure of an XML document using a bit array of

opening and closing brackets, and stores the text nodes of the

document using a global compressed self-index.

On top of these indexes sits an XPath query engine that is

based on tree automata. The engine uses fast counting queries

of the text index in order to dynamically determine whether to

evaluate top-down or bottom-up with respect to

the tree structure. The resulting system has several

advantages over existing systems:

(1) on pure tree queries (without text search) such

as the XPathMark queries, the SXSI system performs on par or

better than the fastest known systems MonetDB and Qizx,

(2) on queries that use text search, SXSI outperforms

the existing systems by 1--3 orders of magnitude

(depending on the size of the result set),

and

(3) with respect to memory consumption, SXSI outperforms

all other systems for counting-only queries.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1147</field>
<field name="author">Jorge Baier</field>
<field name="author">Adi Botea</field>
<field name="title">Improving Planning Performance Using Low-Conflict Relaxed Plans</field>
<field name="abstract">The FF relaxed plan heuristic is one of the most effective techniques in domain-independent satisficing planning and is used by many state-of-the-art heuristic-search planners. However, it may sometimes provide quite inaccurate information, since its relaxation strategy, which ignores the delete effects of actions,

may oversimplify a problem's structure.



In this paper, we propose a novel algorithm for computing relaxed plans which -- although still relaxed --

aim at respecting much of the structure of the original problem. We accomplish this by generating relaxed plans that attempt to reduce the number of conflicts. An action $a$ will add a conflict when added to a relaxed plan if the resulting plan is provably illegal (i.e, not executable) in the un-relaxed problem. As a second contribution, we propose a new lookahead strategy, in the spirit of Vidal's YAHSP lookahead, that can better exploit the contents of relaxed plans. In our experimental analysis, we show that the resulting heuristic improves over the FF heuristic in a number of domains, most notably when lookahead is enabled. Moreover, the resulting system, which uses our new lookahead, is competitive with state-of-the-art planners, and even better in terms of the number of solved problems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1148</field>
<field name="author">Akihiro Kishimoto</field>
<field name="author">Alex Fukunaga</field>
<field name="author">Adi Botea</field>
<field name="title">Scalable, Parallel Best-First Search for Optimal Sequential Planning</field>
<field name="abstract">Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate parallel algorithms for optimal sequential planning, with an emphasis on exploiting distributed memory computing clusters. In particular, we focus on an approach which distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in the optimal sequential version of the Fast Downward planner. The scaling behavior of the algorithm is evaluated experimentally on clusters using up to 128 processors, a significant increase compared to previous work in parallelizing planners. We show that this approach scales well, allowing us to effectively utilize the large amount of distributed memory to optimally solve problems which require hundreds of gigabytes of RAM to solve. We also show that this approach scales well for a single, shared-memory multicore machine.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1149</field>
<field name="author">Laurianne Sitbon</field>
<field name="title">Recommending urban citizens to social butterflies</field>
<field name="keyword">semantic profile</field>
<field name="keyword"> users matching</field>
<field name="keyword"> matrix representations</field>
<field name="abstract">Social Networks are going through a new era with their expansion on Internet. While most of the virtual connections are reflecting real life connections, online tools can also be used to create real life connections from the virtual connections. Civic engagement could be improved by improving connections between citizens and the creation of groups on diverse topics online and in real life. The behavior of users on the web has evolved from a wandering form (navigation through links between web pages) to a requesting form (with the golden years of search engines) and now is heading towards a passive form where the system are more and more expected to provide users with meaningful recommendations. 

Most online social networks suggest groups or other users to one user based on a kinks analysis through the network. However users share more than that with the system nowadays and we believe that all the semantic information available about users and groups should be used in the recommendation process. In the context of local civic engagement, geographic and possibly demographic information should also be taken into account to ensure accurate linkages. 

Standard approaches in the area of content-based recommender systems usually deal with the contents as bag-of-words and are based on major topics of interest. However, politically-oriented contents can very well be very similar in terms of topics but totally opposed in terms of perspectives. If one thinks of 2 users holding opposite opinions on a civic topic, being recommended to each other might dramatically decrease their trust into the recommendations made by the system. 

The approach we propose to deal with the diversity of sources of information (geographic and semantic) and to deal with the opinion held by users on topic at the same time is based on a matrix representation of a user, or a group of users. The matrix representation contains co-occurrences between the words used in the users environment, which then reflects the perspectives in relation to the topics. Similar topics held in different perspectives would then be represented in different ways. A matrix representation also allows for the integration of discrete data in the model of the user. Finally, many reduction schemes can be used to reduce the vocabulary mismatch that could exist between similar users. 

The most important question still to be solved is how to define the similarity or complementarity between two users or between one user and an existing group and account for partial agreement or disagreement given that people are usually interested in many different things at once.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1150</field>
<field name="author">Min Zhang</field>
<field name="author">Thushara Abhayapala</field>
<field name="author">Dhammika Jayalath</field>
<field name="author">David Smith</field>
<field name="author">Athaudage Chandra</field>
<field name="title">Space-Frequency Block Code with Matched Rotation for MIMO-OFDM System with Limited Feedback</field>
<field name="abstract">This paper presents a novel matched rotation precoding (MRP) scheme to design a rate one space-frequency block code (SFBC) and a multirate SFBC for MIMO-OFDM systems with limited feedback. The proposed rate one MRP and multirate MRP can achieve full transmit diversity and optimal system performance for arbitrary number of antennas, subcarrier intervals, and subcarrier groupings, with limited channel knowledge required by the transmit antennas. The optimization process of rate one MRP is simple and easily visualized so that the optimal rotation angle can be derived explicitly, or intuitively for some cases. The multirate MRP has a complex optimization process, but it has a better spectral efficiency and provides a relatively smooth balance between system performance and transmission rate. Simulations show that the proposed SFBC with MRP can overcome the diversity loss in specific propagation scenario, improve the system performance always, and demonstrate flexible performance with large performance gain. Therefore proposed SFBCs with MRP demonstrate flexibility and feasibility so that it is more suited for a practical MIMO-OFDM system with dynamic parameters.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1151</field>
<field name="author">Renato Iannella</field>
<field name="title">First Steps Towards An Abstract Model for The Policy-Aware Web</field>
<field name="abstract">The Policy-Aware Web is the promise for supporting policy management at the Web infrastructure level. A policy is any set of rules or statements that capture and express the requirements of individuals and organisations from a corporate, legal, best practices, and/or social perspective. Currently, policy languages exist that cover and broadly address privacy, access control, and obligation management areas. However, what is missing is an overall framework and architecture for these policy languages to interoperate and provide an accountable, enforceable, flexible and trusted experience for the web community. This paper looks at the technical issues in moving towards a common policy framework and model that both encapsulates the many vertical interests and provides dependable architectures for deployment. The common framework will also provide for cross-policy interaction and semantic understanding.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1152</field>
<field name="author">Joshua Ho</field>
<field name="author">Maurizio Stefani</field>
<field name="author">Cristobal G dos Remedios</field>
<field name="author">Michael A Charleston</field>
<field name="title">A model selection approach to discover age-dependent gene expression patterns using quantile regression models</field>
<field name="keyword">Microarray analysis</field>
<field name="keyword"> quantile regression</field>
<field name="keyword"> brain ageing</field>
<field name="abstract">Background: It has been a long-standing biological challenge to understand the molecular regulatory

mechanisms behind mammalian ageing. Harnessing the availability of many ageing microarray datasets, a

number of studies have shown that it is possible to identify genes that have age-dependent differential

expression (DE) or differential variability (DV) patterns. The majority of the studies identify interesting genes

using a linear regression approach, which is known to perform poorly in the presence of outliers or if the

underlying age-dependent pattern is non-linear. Clearly a more robust and flexible approach is needed to identify

genes with various age-dependent gene expression patterns.

Results: Here we present a novel model selection approach to discover genes with linear or non-linear

age-dependent gene expression patterns from microarray data. To identify DE genes, our method fits three

quantile regression models (constant, linear and piecewise linear models) to the expression profile of each gene,

and selects the least complex model that best fits the available data. Similarly, DV genes are identified by fitting

and comparing two quantile regression models (non-DV and the DV models) to the expression profile of each

gene. We show that our approach is much more robust than the standard linear regression approach in

discovering age-dependent patterns. We also applied our approach to analyze two human brain ageing datasets

and found many biologically interesting gene expression patterns, including some very interesting DV patterns,

that have been overlooked in the original studies. Furthermore, we propose that our model selection approach can be extended to discover DE and DV genes from microarray datasets with discrete class labels, by

considering different quantile regression models.

Conclusions: In this paper, we present a novel application of quantile regression models to identify genes that

have interesting linear or non-linear age-dependent expression patterns. One important contribution of this paper

is to introduce a model selection approach to DE and DV gene identification, which is most commonly tackled

by null hypothesis testing approaches. We show that our approach is robust in analyzing real and simulated

datasets. We believe that our approach is applicable in many ageing or time-series data analysis tasks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1153</field>
<field name="author">Tsong Yueh Chen</field>
<field name="author">Joshua Ho</field>
<field name="author">Huai Liu</field>
<field name="author">Xiaoyuan Xie</field>
<field name="title">An innovative approach for testing bioinformatics programs using metamorphic testing</field>
<field name="keyword">Bioinformatics</field>
<field name="keyword"> software testiing</field>
<field name="keyword"> metamorphic testing</field>
<field name="abstract">Background: Recent advances in experimental and computational technologies have fueled the

development of many sophisticated bioinformatics programs. The correctness of such programs is

crucial as incorrectly computed results may lead to wrong biological conclusion or misguide

downstream experimentation. Common software testing procedures involve executing the target

programwith a set of test inputs and then verifying the correctness of the test outputs. However, due

to the complexity ofmany bioinformatics programs, it is often difficult to verify the correctness of the

test outputs. Therefore our ability to perform systematic software testing is greatly hindered.

Results: We propose to use a novel software testing technique, metamorphic testing (MT), to

test a range of bioinformatics programs. Instead of requiring a mechanism to verify whether an

individual test output is correct, the MT technique verifies whether a pair of test outputs conform

to a set of domain specific properties, called metamorphic relations (MRs), thus greatly increases

the number and variety of test cases that can be applied. To demonstrate how MT is used in

practice, we applied MT to test two open-source bioinformatics programs, namely GNLab and

SeqMap. In particular we show that MT is simple to implement, and is effective in detecting faults in

a real-life program and some artificially fault-seeded programs. Further, we discuss how MT can be

applied to test programs from various domains of bioinformatics.

Conclusion: This paper describes the application of a simple, effective and automated technique

to systematically test a range of bioinformatics programs.We show how MT can be implemented in

practice through two real-life case studies. Since many bioinformatics programs, particularly those

for large scale simulation and data analysis, are hard to test systematically, their developers may

benefit from using MT as part of the testing strategy. Therefore our work represents a significant

step towards software reliability in bioinformatics.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1154</field>
<field name="author">Xiaoyuan Xie</field>
<field name="author">Joshua Ho</field>
<field name="author">Christian Murphy</field>
<field name="author">Gail Kaiser</field>
<field name="author">Baowen Xu</field>
<field name="author">Tsong Yueh Chen</field>
<field name="title">Application of Metamorphic Testing to Supervised Classifiers</field>
<field name="keyword">Machine learning</field>
<field name="keyword"> classification</field>
<field name="keyword"> software testing</field>
<field name="keyword"> software quality</field>
<field name="abstract">Many applications in the field of scientific computing -

such as computational biology, computational linguistics,

and others - depend on Machine Learning algorithms to

provide important core functionality to support solutions

in the particular problem domains. However, it is difficult

to test such applications because often there is no test

oracle to indicate what the correct output should be for

arbitrary input. To help address the quality of such software,

in this paper we present a technique for testing the

implementations of supervised machine learning classification

algorithms on which such scientific computing software

depends. Our technique is based on an approach called

 metamorphic testing , which has been shown to be effective

in such cases. More importantly, we demonstrate

that our technique not only serves the purpose of verification,

but also can be applied in validation. In addition

to presenting our technique, we describe a case study we

performed on a real-world machine learning application

framework, and discuss how programmers implementing

machine learning algorithms can avoid the common pitfalls

discovered in our study. We also discuss how our findings

can be of use to other areas outside scientific computing, as

well.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1155</field>
<field name="author">Colleen B Estigoy</field>
<field name="author">Fredrik Pont n</field>
<field name="author">Jacob Odeberg</field>
<field name="author">Benjamin Herbert</field>
<field name="author">Michael Guilhaus</field>
<field name="author">Michael Charleston</field>
<field name="author">Joshua Ho</field>
<field name="author">Darryl Cameron</field>
<field name="author">Cristobal dos Remedios</field>
<field name="title">Intercalated discs: multiple proteins perform multiple functions in non-failing and failing human hearts</field>
<field name="keyword">Heart Failure</field>
<field name="keyword"> intercalated disc proteins</field>
<field name="keyword"> biophysics</field>
<field name="abstract">The intercalated disc (ICD) occupies a central

position in the transmission of force, electrical continuity

and chemical communication between cardiomyocytes.

Changes in its structure and composition are strongly

implicated in heart failure. ICD functions include: maintenance

of electrical continuity across the ICD; physical links

between membranes and the cytoskeleton; intercellular

adhesion; maintenance of ICD structure and function; and

growth. About 200 known proteins are associated with

ICDs, 40% of which change in disease. We systemically

reviewed cardiac immunohistochemical data on the Human

Protein Atlas (HPA) web site, ExPASy protein binding data

and published papers on ICDs. We identified 43 proteins not

previously reported, and confirmed 37 proteins that have

previously been described. In addition, 102 proteins not present on the HPA web site but were described in ICDs in

the literature. We group these into clusters that demonstrate

functionally interactive groups of proteins demonstrating

that ICDs play a key role in cardiomyocyte function.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1156</field>
<field name="author">Lars Jermiin</field>
<field name="author">Joshua Ho</field>
<field name="author">Kwok Wai Lau</field>
<field name="author">Vivek Jayaswal</field>
<field name="title">A tool for detecting compositional heterogeneity among aligned nucleotide sequences</field>
<field name="keyword">Bioinformatics</field>
<field name="keyword"> sequence analysis</field>
<field name="keyword"> compositional heterogeneity</field>
<field name="abstract">Compositional heterogeneity is a poorly appreciated attribute of aligned nucleotide and amino acid

sequences. It is a common property of molecular phylogenetic data, and it has been found to occur across

sequences and/or across sites. Most molecular phylogenetic methods assume that the sequences have

evolved under globally stationary, reversible, and homogeneous conditions, implying that the sequences

should be compositionally homogeneous. The presence of the above-mentioned compositional heterogeneity

implies that the sequences must have evolved under more general conditions than is commonly

assumed. Consequently, there is a need for reliable methods to detect under what conditions alignments of

nucleotides or amino acids may have evolved. In this chapter, we describe one such program. SeqVis is

designed to survey aligned nucleotide sequences. We discuss pros-et-cons of this program in the context of

other methods to detect compositional heterogeneity and violated phylogenetic assumptions. The benefits

provided by SeqVis are demonstrated in two studies of alignments of nucleotides, one of which contained

7542 nucleotides from 53 species.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1157</field>
<field name="author">A Mohamed</field>
<field name="author">R Koundinya</field>
<field name="author">F Junius</field>
<field name="author">W Dyer</field>
<field name="author">A Yong</field>
<field name="author">Joshua Ho</field>
<field name="author">L Kritharides</field>
<field name="author">C dos Remedios</field>
<field name="title">CD antibody microarrays as an objective tool to differentiate between inflammatory conditions of coronary arteries</field>
<field name="abstract">Background: Currently there are no objective blood tests that could accurately differentiate between Stable Angina (SA) and Unstable Angina (UA). As inflammation is important in both SA and UA, we investigated whether antibody microarrays can effectively reveal the immune response of various leukocytes at different stages of disease progression, and eventually lead to a novel diagnostic tool. 

Methods: Peripheral Blood Mononuclear Cell (PBMC) suspensions from patients with SA (n=21) UA (n=8) and NonSTEMI (n=11) were applied to a microarray of CD antibodies, which differentially captured them on 82 CD antibody spots. An intensity profile pattern was generated according to the amount of light scattered and potential markers were identified using the Significance Analysis of Microarrays (SAM) algorithm. 

Results: Intensity profiles revealed the 12 most significant markers that strongly differentiated SA from UA and NonSTEMI were: kappa1/2, CD95, CD32, CD16, lambda1/2, kappa1/4, lambda1/4, CD56, CD158b, FMLPR, CDw199, CD86. Using these markers differentiated ~90% of patients correctly. Interestingly, intensity profiles of NonSTEMI with infraction __3 days prior to blood collection tended to cluster with those of UA, while the intensity profiles of NonSTEMI with infraction &gt;3 days prior to sampling tended to cluster with SA patients suggesting a change in the leukocyte inflammatory marker status towards a more stable profile.

Conclusion: Antibody microarrays may become useful as a novel diagnostic tool to distinguish between SA, UA and NonSTEMI and may provide insights on the inflammatory status of leukocytes associated with disease progression to instablility and infarction.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1158</field>
<field name="author">Paul Bannerman</field>
<field name="title">Risk Implications of Systems &amp; Software Project Organisation Structures</field>
<field name="abstract">Risk management is a function that is common to systems and software engineering and integration projects, offering the potential to significantly contribute to delivery outcomes. However, prior research indicates that while awareness of the importance of risk management is widespread, theoretical support and application in practice remain underdeveloped. This presentation reports on new research that highlights a source of risk that has received little attention in the literature or practice; namely, the structuring of systems and software work within the context of major organisational stakeholders.



Current research has tended to focus on systems and software processes, ignoring how these activities are structured other than to assume they are organised as a project , according to best practice. The research reported in this presentation suggests this may be a false and risky assumption. Indeed, the structuring of systems and software projects may introduce additional deeply embedded risks that have been previously undetected and left uncontrolled. In addition to highlighting these risks, this research is important because, in relating to projects, it impacts almost all systems and software endeavours.



Specifically, three central propositions are discussed:

1. Projects are not uniformly structure

2. Project structure matters

3. Governance can mediate structures and mitigate structure-related risks



Prior research by the author encountered unexpected findings relating to the organization of systems and software projects. It was found that organisations do not use a uniform project structure as assumed in best practice bodies of knowledge. Rather four distinct forms were identified: pure project; operational activity; hybrid form; and breakthrough event. Each has different risk implications. Risk profiles were developed for each form, resulting in a set of twenty one project structure-related risk factors. The factors are illustrated with case examples.



A complication in managing these risks, however, is that structure-related risks are often invisible within projects or cannot be effectively controlled within a project when they arise between structural entities involved in the project. Being external to the project itself, project governance is ideally positioned to mediate (bridge) different structural entities in a project (such as the project itself, the parent organisation and any influencing organisations such as client organisations and providers) and oversee the mitigation of structure-related risks to improve overall project outcomes. Case examples are provided to illustrate structure-related issues in software projects and the role that project governance might play.



Without such a mechanism to manage structure-related risks, we will continue to build deeply embedded risks into software projects before they actually start. Implications for research and practice are discussed as well as directions for future research. This research extends our understanding of risk and can help improve outcomes in systems and software activities.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1159</field>
<field name="author">Paul Brebner</field>
<field name="author">Liam O'Brien</field>
<field name="author">Jon Gray</field>
<field name="title">Performance Modeling Evolving Enterprise Service Oriented Architectures</field>
<field name="keyword">Enterprise Service Oriented Architecture</field>
<field name="keyword"> SOA</field>
<field name="keyword"> performance modeling</field>
<field name="keyword"> SOA evolution</field>
<field name="abstract">NICTA has developed a Service Oriented

Performance Modeling technology, and for several

years has conducted field-trials of the technology in

collaboration with government and non-government

projects. The technology enables rapid modeling and

performance prediction of large-scale heterogeneous

Service Oriented Architectures (SOAs), using a

methodology and tool support to model and simulate

complex service compositions. Evolution in Enterprise

Service Oriented Architectures (ESOAs) presents

significant challenges to managing risks associated

with performance and scalability. Four of the field-trials

involved the construction and use of models to

specifically address performance risks related to the

evolution of service architectures, resulting from, and

to explore changes in: architecture, services,

deployment, resources, and loads. This paper first

presents some unifying observations on the

architectural commonalities of evolving enterprise

services, discusses the performance and scalability

challenges, and describes some common approaches to

addressing them. We then demonstrate Service

Oriented Performance Modeling applied to an

illustrative example of enterprise service evolution. We

conclude with a presentation of four case-studies

which summarize our experiences with performance

modeling evolving service architectures.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1160</field>
<field name="author">Aleks Jakulin</field>
<field name="author">Wray Buntine</field>
<field name="author">Timothy La Pira</field>
<field name="author">Holly Brasher</field>
<field name="title">Analyzing the U.S. Senate in 2003: Similarities, Clusters, and Blocs</field>
<field name="abstract">In this paper, we apply information theoretic measures to voting in the U.S. Senate in 2003. We assess the associations between pairs of senators and groups of senators based on the votes they cast. For pairs, we use similarity-based methods, including hierarchical clustering and multidimensional scaling. To identify groups of senators, we use principal component analysis. We also apply a discrete multinomial latent variable model that we have developed. In doing so, we identify blocs of cohesive voters within the Senate and contrast it with continuous ideal point methods. We find more nuanced blocs than simply the two-party division. Under the bloc-voting model, the Senate can be interpreted as a weighted vote system, and we are able to estimate the empirical voting power of individual blocs through what-if analysis.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1161</field>
<field name="author">Petar Rajkovic</field>
<field name="author">Dragan Jankovic</field>
<field name="author">Vladimir Tosic</field>
<field name="title">A Software Solution for Ambulatory Healthcare Facilities in the Republic of Serbia</field>
<field name="keyword">electronic health record (EHR)</field>
<field name="keyword"> medical information system (MIS)</field>
<field name="keyword"> code genration</field>
<field name="keyword"> rapid application development</field>
<field name="abstract">During the last decade, the Ministry of Health of the Republic of Serbia, a developing country, funded several projects aiming to make the public healthcare system more efficient through the use of information technology (IT). This system development experience paper presents the first results of our project that develops software support for public ambulatory healthcare facilities, concentrating efforts on improving the work environment of general practice (GP) doctors. While earlier projects in Serbia centered on organizational and financial aspects of healthcare processes, our project focuses on medical aspects and results in a medical information system (MIS) based on a specially developed electronic health record (EHR). We also present a brief overview of our supporting tools for data modeling and application generation, developed for easier realization of MIS. Another goal of the project is to support interoperation and cooperation of ambulatory and clinical MIS within the University Clinical Center of Nis (there is no similar interoperation in any city in Serbia). The main contribution of this system development experience paper is to show how we have been applying and adapting modern MIS concepts in challenging circumstances of a developing country. These experiences could be relevant for other countries with similar challenges.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1162</field>
<field name="author">Qinghua Lu</field>
<field name="author">Phillip John McKerrow</field>
<field name="author">Zhi Quan Zhou</field>
<field name="title">Design and Performance of a Minimal Real-Time Operating System in a Safe Language: Experience with Java on the Sun SPOT</field>
<field name="keyword">Real-time operating system</field>
<field name="keyword"> Java</field>
<field name="keyword"> Sun SPOT</field>
<field name="keyword"> embedded system</field>
<field name="keyword"> safe language</field>
<field name="keyword"> green thread</field>
<field name="keyword"> performance measurement</field>
<field name="keyword"> TINI</field>
<field name="keyword"> kernel overhead.</field>
<field name="abstract">Real-time operating systems (RTOSes) are required to run for years without human intervention, and never fail. Safety is a concern when they control physical equipment. One strand of real-time operating system (RTOS) research is looking at the question: can developing an RTOS in a safe language result in a system that an errant process cannot crash? In this paper, we examine the advantages and problems of writing an RTOS in a safe language, namely Java. Then we design, implement and measure the performance of a minimal RTOS to schedule processes as threads on a Sun SPOT micro-controller.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1163</field>
<field name="author">Xiwei (Sherry) Xu</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="author">Jenny Liu</field>
<field name="title">An Architecting Method for Distributed Process-Intensive Systems</field>
<field name="keyword">REST</field>
<field name="keyword"> process-instensive systems</field>
<field name="abstract">This paper introduces an architecting method for distributed process-intensive systems. Traditional methods (e.g. object-orientation, structured analysis or component/service-based designs) decompose a process-intensive system into entities with attached domain-specific operations (process constituents). This results in fine-grained Remote Procedure Calls in distributed systems which are often detrimental to quality attributes such as performance, loose-coupling, adaptability and interoperability. Our method tailors the REpresentational State Transfer (REST) principles used for hypermedia data transfer to process-intensive systems by making process constituents into resources, and attaching a set of standard operations. Distributed processes interoperate by adhering to these operations and exchanging process information. In our method, process information exchange contains not only typical meta-information about a process, but also process fragments that indicate possible next-steps and interconnectedness for process coordination purposes. We have implemented our method in a Web environment and conducted a case study providing initial validation of its benefits.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1164</field>
<field name="author">Kelvin Cheng</field>
<field name="author">Benjamin Itzstein</field>
<field name="author">Paul Sztajer</field>
<field name="author">Markus Rittenbruch</field>
<field name="title">A unified multi-touch &amp; multi-pointer software architecture for supporting collocated work on the desktop</field>
<field name="keyword">Software Architecture</field>
<field name="keyword"> Multi-touch</field>
<field name="keyword"> Tabletop</field>
<field name="keyword"> multiple pointers</field>
<field name="abstract">Most current multi-touch capable interactive user interfaces for tabletop are built from custom toolkits that are decoupled from, and on top of, the "Desktop" provided by the underlying Operating System. However, this approach requires that each individual touch system build their own suite of touch capable custom applications (such as photo browsers), usually resulting in limited functionality. In this paper, we propose a software architecture for supporting and integrating multi-touch capability on existing desktop systems, where multi-touch and multiple single pointer input can be used simultaneously to manipulate existing application windows. We present an example implementation of this architecture on the Linux Operating System, demonstrating the possibility of using touch displays in a collaborative work environment on the tabletop. The paper concludes with lessons learnt and technical challenges from our experience.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1165</field>
<field name="author">David Smith</field>
<field name="author">Andrew Zhang</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Temporal Correlation of the Dynamic On-Body Area Radio Channel</field>
<field name="abstract">Temporal autocorrelation of the dynamic on-body radio channel is characterized empirically and theoretically based on measurements. The channel coherence time is found to be approximately 20 to 70~ms for a human subject either walking or running. Using a Weibull distribution approximation to the channel taps there is a good match

between empirical and theoretical results for large correlation coefficients.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1166</field>
<field name="author">Brian Lam</field>
<field name="author">Guido Governatori</field>
<field name="title">The making of SPINdle</field>
<field name="keyword">Defeasible Logic</field>
<field name="keyword"> Modal Defeasible Logic</field>
<field name="keyword"> Reasoning</field>
<field name="abstract">This paper presents the design and implementation of SPINdle - a defeasible

logic reasoner that is capable to perform efficient and scalable reasoning

on defeasible logic theories (including theories with over 1 million rules). The

implementation covers both the standard and modal extensions to defeasible logics.

It can be used as a standalone theory prover and can be embedded into any

applications as a defeasible logic rule engine. It allows users or agents to issues

queries, on a given knowledge base or a theory generated on the fly by other applications,

and automatically produces the conclusions of its consequences. The

theory can also be represented using XML for agent communication, which is a

common scenario in the Semantic Web. SPINdle is written in JAVA and is open

source.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1167</field>
<field name="author">Worapan Kusakunniran</field>
<field name="author">Hongdong Li</field>
<field name="author">Jian Zhang</field>
<field name="title">A DIRECT METHOD TO SELF-CALIBRATE A SURVEILLANCE CAMERA BY OBSERVING A WALKING PEDESTRIAN</field>
<field name="keyword">Camera calibration</field>
<field name="keyword"> video surveillance</field>
<field name="keyword"> object tracking</field>
<field name="abstract">Recent efforts show that it is possible to calibrate a surveillance camera simply from observing a walking human. This procedure can be seen as a special application of the camera self-calibration technique. Several methods have been proposed along this line, but most of them have certain restrictions, such as require the human walking at a constant speed, or require two orthogonal lines marked on the ground, etc. This has hindered their applicability. In this paper we propose a new method that removes most of these restrictions. By clever uses of the cross-ratio relationship in projective geometry, our method shows it is possible to directly estimate a full 3x4 camera projection matrix without rst decomposing it into physical parameters like focal-length, optical center, etc. Extensive experiments on real data show our algorithm performs well in real situations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1168</field>
<field name="author">Mark Staples</field>
<field name="author">Paul Bannerman</field>
<field name="author">Xi Chen</field>
<field name="title">Implementing CMMI Specific Practices Incrementally - Where Do You Start?</field>
<field name="abstract">Whether you use the staged or continuous representation, CMMI encourages you to adopt a fixed collection of Specific Practices within each Process Area. But if you want to implement your process improvement incrementally, which Specific Practice should you target first? We have analyzed the text of the specification of CMMI to see what work products are used and produced by Specific Practices, and will present the graphs we have constructed that show the inter-dependencies between Specific Practices and Process Areas in Maturity Level 2. We will also show you how we can augment these graphs with information about SMEs' perceived value of CMMI Specific Practices. The goal of the research has been to help companies plan a path to target the implementation of high value Specific Practices first.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1169</field>
<field name="author">Wray Buntine</field>
<field name="title">Estimating Likelihoods for Topic Models</field>
<field name="keyword">topic models</field>
<field name="keyword"> likelihood</field>
<field name="keyword"> MCMC</field>
<field name="abstract">Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\it topic} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of such methods, improving on the recent Left-to-Right algorithm of Wallach.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1170</field>
<field name="author">John Lim</field>
<field name="author">Nick Barnes</field>
<field name="title">Directions of Egomotion from Antipodal Points</field>
<field name="keyword">egomotion</field>
<field name="keyword"> multiview-geometry</field>
<field name="keyword"> vision</field>
<field name="abstract">We present a novel geometrical constraint on the egomotion

of a single, moving camera. Using a camera with

a large field-of-view (FOV), the optical flow measured at

a single pair of antipodal points on the image sphere constrains

the set of all possible camera motion directions to a

subset region. By considering the flow at many such antipodal

point pairs, it is shown that the intersection of all subset

regions arising from each pair yields an estimate on the

directions of motion. These antipodal point constraints rely

on the geometrical properties of using a spherical representation

of the image as well as the larger information content

available from a large FOV. An algorithm using these constraints

was implemented and tested on both simulated and

real images. Results show comparable performance to the

state of the art in the presence of noise and outliers whilst

processing in constant time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1171</field>
<field name="author">John Lim</field>
<field name="author">Nick Barnes</field>
<field name="title">Robust Visual Homing with Landmark Angles</field>
<field name="abstract">This paper presents a novel approach to visual

homing for robot navigation on the ground plane, using only

the angles of landmark points. We focus on a robust approach,

leading to successful homing even in real, dynamic environments

where significant numbers of landmark points are wrong or

missing. Three homing algorithms are presented, two are shown

to be provably convergent, and the other shown to converge

empirically. Results from simulations under noise and robot

homing in real environments are provided.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1172</field>
<field name="author">Haifeng Zhao</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jianfeng Lu</field>
<field name="author">Jing-yu Yang</field>
<field name="title">Automatic Detection of Defective Zebrafish Embryos via Shape Analysis</field>
<field name="abstract">In this paper, we present a graph-based approach to automatically

detect defective zebrafish embryos. Here, the

zebrafish is segmented from the background using a texture

descriptor and morphological operations. In this way, we

can represent the embryo shape as a graph, for which we

propose a vectorisation method to recover clique histogram

vectors for classification. The clique histogram represents

the distribution of one vertex with respect to its adjacent

vertice. This treatment permits the use of a codebook approach

to represent the graph in terms of a set of codewords

that can be used for purposes of support vector machine

classification. The experimental results show that the

method is not only effective but also robust to occlusions

and shape variations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1173</field>
<field name="author">Hayoung Yoon</field>
<field name="author">JongWon Kim</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="title">Mobility Emulator for DTN and MANET Applications</field>
<field name="abstract">Repeatable experiments to study the performance or behavior of the modern wireless mobile adhoc network (MANET) systems such as a delay (or disruption) tolerant network (DTN) and its application is a challenging task. The best way to do this is by building a mobile testbed which can carry the mobile devices and test applications or network protocols on top of it. Those methods require a lot of investment for management as well as setup cost. Therefore, most of algorithms and applications considering device mobility as a critical ingredient have been tested under the trace-based analysis and computer simulations. However, those evaluation methods are not enough to reflect various natures of mobile wireless networks. In this paper, we propose an On/Off-based mobility emulation method and its implementation that virtually migrates applications over static-grid testbed to mimic the device mobility. This emulation method assists DTN and MANET application and algorithm developers to evaluate their work in repeatable ways on the lab-scale static-grid testbed. Through extensive experimental analysis and case studies from two different testbeds, we show that the proposed emulation method can successfully recreate mobile traces while it keeps important characteristics of mobility trace data such as contact and inter-contact time distributions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1174</field>
<field name="author">Stefan M. Petters</field>
<field name="author">Martin Lawitzky</field>
<field name="author">Ryan Heffernan</field>
<field name="author">Kevin Elphinstone</field>
<field name="title">Towards Real Multi-Criticality Scheduling</field>
<field name="keyword">real-time</field>
<field name="abstract">Componentised systems, in particular those with

fault confinement through address spaces, are currently emerging

as a hot topic in embedded systems research. This paper extends

the unified rate-based scheduling framework RBED in several

dimensions to fit the requirements of such systems: We have

removed the requirement that the deadline of a task is equal

to its period. The introduction of inter-process communication

reflects the need to communicate. Additionally we also discuss

server tasks, budget replenishment and the low level details

needed to deal with the physical reality of systems. While a

number of these issues have been studied in previous work

in isolation, we focus on the problems discovered and lessons

learned when integrating solutions. We report on our experiences

implementing the proposed mechanisms in a commercial grade

OKL4 microkernel as well as an application with soft real-time

and best-effort tasks on top of it.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1175</field>
<field name="author">B. Parker</field>
<field name="author">S. Gunter</field>
<field name="author">Justin Bedo</field>
<field name="title">Stratification bias in low signal microarray studies.</field>
<field name="abstract">Background

When analysing microarray and other small sample size biological datasets, care is needed to avoid various biases. We analyse a form of bias, stratification bias, that can substantially affect analyses using sample-reuse validation techniques and lead to inaccurate results. This bias is due to imperfect stratification of samples in the training and test sets and the dependency between these stratification errors, i.e. the variations in class proportions in the training and test sets are negatively correlated.



Results

We show that when estimating the performance of classifiers on low signal datasets (i.e. those which are difficult to classify), which are typical of many prognostic microarray studies, commonly used performance measures can suffer from a substantial negative bias. For error rate this bias is only severe in quite restricted situations, but can be much larger and more frequent when using ranking measures such as the receiver operating characteristic (ROC) curve and area under the ROC (AUC). Substantial biases are shown in simulations and on the van 't Veer breast cancer dataset. The classification error rate can have large negative biases for balanced datasets, whereas the AUC shows substantial pessimistic biases even for imbalanced datasets. In simulation studies using 10-fold cross-validation, AUC values of less than 0.3 can be observed on random datasets rather than the expected 0.5. Further experiments on the van 't Veer breast cancer dataset show these biases exist in practice.



Conclusion

Stratification bias can substantially affect several performance measures. In computing the AUC, the strategy of pooling the test samples from the various folds of cross-validation can lead to large biases; computing it as the average of per-fold estimates avoids this bias and is thus the recommended approach. As a more general solution applicable to other performance measures, we show that stratified repeated holdout and a modified version of k-fold cross-validation, balanced, stratified cross-validation and balanced leave-one-out cross-validation, avoids the bias. Therefore for model selection and evaluation of microarray and other small biological datasets, these methods should be used and unstratified versions avoided. In particular, the commonly used (unbalanced) leave-one-out cross-validation should not be used to estimate AUC for small datasets.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1176</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Simplistic Hashing for Building a Better Bloom Filter on Randomized Data</field>
<field name="keyword">bloom filter</field>
<field name="keyword"> data redundancy</field>
<field name="keyword"> hash function</field>
<field name="abstract">User demands to have access to complete and accurate information requires integration of data from distributed stores. Those stores provide dynamically changing data that could be partially redundant because of many intentional or unintentional reasons. The unintentional reasons could be the way the data was collected by those information stores and intentional reasons could be replication or the nature of the content description language. Whatever the reason is, a need for a filtering mechanism during information retrieval so that redundancy of data could be removed before transmitting on the network arises. This paper proposes an improvement to the randomized redundant data filtering by the support of an efficient hashing algorithm. We evaluate these hashing algorithms for building a bloom filter on randomized data.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1177</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="title">Models for an Energy-Efficient P2P Delivery Service</field>
<field name="keyword">Energy Consumption</field>
<field name="keyword"> P2P network</field>
<field name="keyword"> Distributed algorithm</field>
<field name="abstract">Data and service delivery have been historically based on network centric architectures, with datacentres being the focal sources. The amount of energy consumed by these datacentres has become an emerging issue for the companies operating them. Thus, many contributions propose solutions to improve the current datacentre architecture and deployments to make them more energy efficient. A recently proposed approach argues for removing the datacentres from the delivery architecture. Their functionalities will instead be distributed at the edge of the network, directly within operator-managed home devices such as a Set-Top-Box (STB). This paper presents a study of the overall energy consumption required by such a community of STBs in order to provide the same services as datacentres. This paper also proposes a preliminary distributed algorithm to further reduce this overall energy consumption. This algorithm will be deployed over managed peer-to-peer network of STBs. It will make optimization decisions and instruct unused STBs to switch Off to save energy without altering the general Service Level Agreement. This will effectively create a controlled churn, which will require a controlled redeployment and/or duplication of the data and services.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1178</field>
<field name="author">Meena Jha</field>
<field name="author">Liam O'Brien</field>
<field name="author">Piyush Maheshwari</field>
<field name="title">Identify Issues and Concerns in Software Reuse</field>
<field name="keyword">Software reuse</field>
<field name="keyword"> Reliability</field>
<field name="abstract">Software Reuse has been a challenge for the research community for many years. 

Software reuse cannot possibly become an engineering discipline as long as issues 

and concerns have not been clearly understood and dealt with. Studies have shown that 

software reuse is a critical aspect for organizations interested in the improvement of 

software development quality and productivity. However, even toady, with the idea of 

software product lines, there is still no clear consensus between software reuse 

development and its impact on quality and productivity. We have completed a survey on 

software reuse the focus of which is to identify issues and concerns in software reuse 

which may pave a path to develop a suitable software reuse process. This paper outlines

the results of that survey.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1179</field>
<field name="author">Junae Kim</field>
<field name="author">Chunhua Shen</field>
<field name="author">Lei Wang</field>
<field name="title">A Scalable Algorithm for Learning a Mahalanobis Distance Metric</field>
<field name="keyword">A distance metric</field>
<field name="keyword"> a postive semidefinite matrix</field>
<field name="abstract">A distance metric that can accurately reflect the intrinsic

characteristics of data is critical for visual recognition tasks. An effective

solution to defining such a metric is to learn it from a set of training samples.

In this work, we propose a fast and scalable algorithm to learn a Mahalanobis

distance. By employing the principle of margin maximization

to secure better generalization performances, this algorithm formulates

the metric learning as a convex optimization problem with a positive

semidefinite (psd) matrix variable. Based on an important theorem that

a psd matrix with trace of one can always be represented as a convex

combination of multiple rank-one matrices, our algorithm employs a differentiable

loss function and solves the above convex optimization with

gradient descent methods. This algorithm not only naturally maintains

the psd requirement of the matrix variable that is essential for metric

learning, but also significantly cuts down computational overhead,

making it much more efficient with the increasing dimensions of feature

vectors. Experimental study on benchmark data sets indicates that,

compared with the existing metric learning algorithms, our algorithm

can achieve higher classification accuracy with much less computational

load.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1180</field>
<field name="author">Peng Wang</field>
<field name="author">Chunhua Shen</field>
<field name="author">Hong Zheng</field>
<field name="author">Ren Zhang</field>
<field name="title">A Variant of the Trace Quotient Formulation for Dimensionality Reduction</field>
<field name="keyword">dimensionality reduction</field>
<field name="keyword"> semidefinite programming</field>
<field name="abstract">Due to its importance to classification and clustering, dimensionality reduction or distance metric learning has been studied in depth in recent years. In this work, we demonstrate the weakness of a widely-used class separability criterion trace quotient for dimensionality reduction and propose new criteria for the dimensionality reduction problem. The proposed optimization problem can be efficiently solved using semidefinite programming. Experiments on classification and clustering are performed to evaluate the proposed algorithm. Results show the advantage of the our proposed algorithm.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1181</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="author">Ju Lynn Ong</field>
<field name="title">MULTIRESOLUTION COLONIC POLYP DETECTION IN CT COLONOGRAPHY USING SPHERICAL WAVELETS</field>
<field name="abstract">The description of lesion shapes with curvature-based features

is a widespread approach in polyp detection methods.

These methods are motivated by the need to compactly and

accurately encode the different existing shape forms on the

colon wall. However, the colon wall presents a number of

small convex shape that resemble polyps which increases

drastically the number of false positive (FP) detected. In this

paper a method based on multiresolution shape processing using

the spherical wavelet transform is proposed. The method

aims to reduce the number of FPs detected while preserving

true positives. Simulation results illustrating the effectiveness

of the method are presented.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1182</field>
<field name="author">Duy Hoang Pham</field>
<field name="author">Guido Governatori</field>
<field name="author">Subhasis Thakur</field>
<field name="title">Extended Defeasible Reasoning for Common Goals in n-Person Argumentation Games</field>
<field name="keyword">Arti cial intelligence</field>
<field name="keyword"> Defeasible reasoning</field>
<field name="keyword"> Argumentation systems</field>
<field name="abstract">Argumentation games have been proved to be a robust and exible tool to resolve 

con icts among agents. An agent can propose its explanation and its goal known as a claim, 

which can be refuted by other agents. The situation is more complicated when there are more 

than two agents playing the game. 



We propose a weighting mechanism for competing premises to tackle with con icts from multiple 

agents in an n-person game. An agent can defend its proposal by giving a counter-argument 

to change the opinion of the majority of opposing agents. Furthermore, using the extended 

defeasible reasoning an agent can exploittheknowledgethatotheragentsexposeinordertopromote and defend its main claim.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1183</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">Changing Legal Systems: Legal Abrogations and Annulments in Defeasible Logic</field>
<field name="keyword">norm dynamics</field>
<field name="keyword"> defeasible logic</field>
<field name="keyword"> temporal defeasible logic</field>
<field name="keyword"> annulment</field>
<field name="keyword"> abrogation</field>
<field name="abstract">In this paper we investigate how to represent and reason about legal 

abrogations and annulments in Defeasible Logic. We examine some options that 

embed in this setting, and in similar rule-based systems, ideas from belief and 

base revision. In both cases, our conclusion is negative, which suggests to adopt a 

different logical model. This model expresses temporal aspects of legal rules, and 

distinguishes between two main timelines, one internal to a given temporal version

of the legal system, and another relative to how the legal system evolves over 

time. Accordingly, we propose a temporal extension of Defeasible Logic suitable 

to express this model and to capture abrogation and annulment. We show that 

the proposed framework overcomes the dif culties discussed in regard to belief 

and base revision, and is suf ciently exible to represent many of the subtleties 

characterizing legal abrogations and annulments.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1184</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">How Do Agents Comply with Norms?</field>
<field name="keyword">agents</field>
<field name="keyword"> business processes</field>
<field name="keyword"> compliance</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">The import of the notion of institution in the design of MASs

 requires to develop formal and efficient methods for modeling the

 interaction between agents' behaviour and normative systems. This

 paper discusses how to check whether agents' behaviour complies

 with the rules regulating them. The key point of our approach is

 that compliance is a relationship between two sets of

 specifications: the specifications for executing a process and the

 specifications regulating it. We propose a formalism for

 describing both the semantics of normative specifications and the

 semantics of compliance checking procedures.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1185</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="author">Thanassis Boulis</field>
<field name="author">David Smith</field>
<field name="title">Network-to-network interference measurements</field>
<field name="abstract">Network-to-network interference measurements for nearby, uncoordinated BANs, where the networks cause co-channel interference. Implications for interference mitigation</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1186</field>
<field name="author">Wray Buntine</field>
<field name="title">Estimating Likelihoods for Topic Models</field>
<field name="keyword">topic models</field>
<field name="keyword"> likelihood</field>
<field name="keyword"> MCMC</field>
<field name="abstract">Topic models are a discrete analogue to principle component analysis and independent component

analysis that model topic at the word level within a document. They have many variants

such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image

analysis and recommender systems. However, only recently have reasonable methods for

estimating the likelihood of unseen documents, for instance to perform testing or model comparison,

become available. This paper explores a number of such methods, improving on the

recent Left-to-Right algorithm of Wallach.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1187</field>
<field name="author">Wray Buntine</field>
<field name="title">An Approach to Discrete Component Analysis: DCA v0.200 Theory Companion</field>
<field name="keyword">topic models</field>
<field name="keyword"> DCA</field>
<field name="keyword"> Gibbs sampling</field>
<field name="keyword"> Dirichlets</field>
<field name="abstract">This report is the background theory for Discrete Component Analysis software called DCA. Currently

the software is run in stand-alone mode, and scavengers data streaming libraries and

Dirichlet utilities from the older MPCA system1. The software itself is written in the C language

and compiles on a Linux and a Mac OS X environment. The models presented here are a hierarchical

extension of discrete component analysis. This is known under many names [2], such as

LDA, multi-aspect models, multinomial PCA, etc.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1188</field>
<field name="author">Wray Buntine</field>
<field name="title">DCA 0.200: Discrete Component Analysis Software</field>
<field name="keyword"/>
<field name="abstract">This report is the software development and installation guide for the Discrete Component Analysis

(DCA) software for Version 0.200. It includes a description of input and output files, the C

code base, prerequisites and installation, and a guide to future work.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1189</field>
<field name="author">Wray Buntine</field>
<field name="title">DCA 0.200 User Guide</field>
<field name="keyword">DCA software</field>
<field name="keyword"> user guide</field>
<field name="abstract">This report is a user guide for the Discrete Component Analysis (DCA) software in development,

Version 0.200. It shows how to use the software, and should be read in conjunction with the

worked examples in the examples/ directory.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1190</field>
<field name="author">Akihiro Kishimoto</field>
<field name="author">Alex Fukunaga</field>
<field name="author">Adi Botea</field>
<field name="title">Parallel Best-First Search for Optimal Sequential Planning (Research Statement)</field>
<field name="abstract">Being a short, two-page summary, there is no abstract in this paper.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1191</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Restricted Global Grammar Constraints</field>
<field name="abstract">We investigate the global GRAMMAR constraint over restricted classes of context free grammars like deterministic and unambiguous context-free grammars. We show that detecting disentailment for the GRAMMAR constraint in these cases is as hard as parsing an unrestricted context free grammar.We also consider the class of linear grammars and give a propagator that runs in quadratic time. Finally, to demonstrate the use of linear grammars, we show that a weighted linear GRAMMAR constraint can efficiently encode the EDITDISTANCE constraint, and a conjunction of the EDITDISTANCE constraint and the REGULAR constraint.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1192</field>
<field name="author">Meena Jha</field>
<field name="author">Liam O'Brien</field>
<field name="title">Identify Issues and Concerns in Software Reuse in Software Product Lines</field>
<field name="keyword">Software Reuse</field>
<field name="keyword"> Software Product Line</field>
<field name="keyword"> Domain Engineering</field>
<field name="abstract">One of the reasons for introducing software product lines (SPL) is the reduction of costs through reusing common assets for different products. Developing assets to be reused in different products is often not easy. Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced when reusing software and other assets. In an attempt to understand the obstacles to implementing software reuse in SPL we have conducted a survey to investigate how software reuse is adopted in SPL so as to provide the necessary degree of support for engineering software product line applications and to identify some of the issues and concerns in software reuse. This survey also gathers information from SPL practitioners on what influences the selection of software to reuse within a software product line. This paper reports the results of that survey.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1193</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Ursula Goltz</field>
<field name="author">Jens-Wolfhard Schicke</field>
<field name="title">Symmetric and Asymmetric Asynchronous Interaction</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> Petri nets</field>
<field name="keyword"> distributed systems</field>
<field name="keyword"> asynchronous interaction</field>
<field name="keyword"> semantic equivalences.</field>
<field name="abstract">We investigate classes of systems based on different interaction patterns with the aim of achieving distributability. As our system model we use Petri nets. In Petri nets, an inherent concept of simultaneity is built in, since when a transition has more than one preplace, it can be crucial that tokens are removed instantaneously. When modelling a system which is intended to be implemented in a distributed way by a Petri net, this built-in concept of synchronous interaction may be problematic. To investigate the problem we assume that removing tokens from places can no longer be considered as instantaneous. We model this by inserting silent (unobservable) transitions between transitions and their preplaces. We investigate three different patterns for modelling this type of asynchronous interaction. Full asynchrony assumes that every removal of a token from a place is time consuming. For symmetric asynchrony, tokens are only removed slowly in case of backward branched transitions, hence where the concept of simultaneous removal actually occurs. Finally we consider a more intricate pattern by allowing to remove tokens from preplaces of backward branched transitions asynchronously in sequence (asymmetric asynchrony).



We investigate the effect of these different transformations of instantaneous interaction into asynchronous interaction patterns by comparing the behaviours of nets before and after insertion of the silent transitions. We exhibit for which classes of Petri nets we obtain equivalent behaviour with respect to failures equivalence.



It turns out that the resulting hierarchy of Petri net classes can be described by semi-structural properties. In case of fully symmetric asynchrony and symmetric asynchrony, we obtain precise characterisations; for asymmetric asynchrony we obtain lower and upper bounds.



We briefly comment on possible applications of our results to Message Sequence Charts.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1194</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Implementation and Application of Automata, 14th International Conference, CIAA 2009, Sydney, Australia, July 14-17, 2009. Proceedings</field>
<field name="abstract">The 14th International Conference on Implementation and Application of Automata

(CIAA 2009) was held in NICTA's Neville Roach Laboratory at the

University of New South Wales, Sydney, Australia on July 14{17, 2009.

This volume of Lecture Notes in Computer Science contains the papers that

were presented at CIAA 2009, as well as abstracts of the posters and short

papers that were presented at the conference. The volume also includes papers

or extended abstracts of the three invited talks presented by Gonzalo Navarro on

Implementation and Application of Automata in String Processing, by Christoph

Koch on Applications of Automata in XML Processing, and by Helmut Seidl on

Program Analysis through Finite Tree Automata.

The 23 regular papers were selected from 42 submissions covering various

&#12;elds in application, implementation, and theory of automata and related structures.

This year, 6 additional papers were selected as \short paper"; at the

conference these were allocated the same presentation length as regular papers.

Each paper was reviewed by at least three Program Committee members, with

the assistance of external referees. Papers were submitted by authors from the

following countries: Australia, Austria, Belgium, Brazil, Canada, China, Czech

Republic, Finland, France, Germany, India, Italy, Republic of Korea, Japan,

Latvia, The Netherlands, Portugal, Russian Federation, Spain, South Africa,

Turkey, United Arab Emirates, and USA.

I wish to thank all who made this conference possible: the authors for submitting

their papers, the Program Committee members and external referees (listed

on the next pages) for giving their valuable opinions and writing reports about

the submitted papers, and the three invited speakers for giving presentations

related to Implementation and Application of Automata. Finally, I would like to

express my gratitude to the sponsors (listed on the next pages), local organizers,

and to the editors of Lecture Notes in Computer Science, in particular to Alfred

Hofmann, for their help in publishing this volume in a timely manner.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1195</field>
<field name="author">Hongdong Li</field>
<field name="title">Consensus Set Maximization with Guaranteed Global Optimality for robust geometry estimation</field>
<field name="abstract">A new method for optimal robust estimation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1196</field>
<field name="author">Otfried Cheong</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Hyo-Sil Kim</field>
<field name="author">Daria Schymura</field>
<field name="author">Fabian Stehn</field>
<field name="title">Measuring the Similarity of Geometric Graphs</field>
<field name="abstract">What does it mean for two geometric graphs to be similar? We propose a distance for geometric graphs that we show to be a metric, and that can be computed by solving an integer linear program. We also present experiments using a heuristic distance function.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1197</field>
<field name="author">Mattias Andersson</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Christos Levcopolous</field>
<field name="title">Restricted mesh simplification using edge contractions</field>
<field name="abstract">We consider the problem of simplifying a planar triangle mesh using edge contractions, under the restriction that the resulting vertices must be a subset of the input set. That

is, contraction of an edge must be made onto one of its adjacent vertices, which results in removing the other adjacent vertex. We show that if the perimeter of the mesh consists

of at most ve vertices, then we can always nd a vertex not on the perimeter which can be removed in this way. If the perimeter consists of more than ve vertices such a

vertex may not exist. In order to maintain a higher number of removable vertices under the above restriction, we study edge ips which can be performed in a visually smooth

way. A removal of a vertex which is preceded by one such smooth operation is called a 2-step removal. Moreover, we introduce the possibility that the user de nes \important"

vertices (or edges) which have to remain intact. Given m such important vertices, or edges, we show that a simpli cation hierarchy of size O(n) and depth O(log(n=m)) can

be constructed by 2-step removals in O(n) time, such that the simpli ed graph contains the m important vertices and edges, and at most O(m) other vertices from the input

graph. In some triangulations, many vertices may not even be 2-step removable. In order to provide the option to remove such vertices, we also de ne and examine k-step removals.

This increases the lower bound on the number of removable vertices.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1198</field>
<field name="author">Marc Benkert</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Christian Knauer</field>
<field name="author">rene van Oostrum</field>
<field name="author">Alexander Wolff</field>
<field name="title">A polynomial-time approximation algorithm for a geometric dispersion problem</field>
<field name="abstract">We consider the following packing problem. Let a be a fixed real in (0; 1]. We are given a

bounding rectangle E and a set R of n possibly intersecting unit disks whose centers lie

in E. The task is to pack a set B of m disjoint disks of radius a into E such that no disk

in B intersects a disk in R, where m is the maximum number of unit disks that can be

packed. In this paper we present a polynomial-time algorithm for a = 2/3. So far only

the case of packing squares has been considered. For that case Baur and Fekete have

given a polynomial-time algorithm for a = 2/3 and have shown the problem cannot be

solved in polynomial time for any a &gt; 13/14 unless P = NP.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1199</field>
<field name="author">Mohammad Ali Abam</field>
<field name="author">Mark de Berg</field>
<field name="author">Mohammad Farshi</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Region-Fault Tolerant Geometric Spanners</field>
<field name="abstract">We introduce the concept of region-fault tolerant spanners for planar

point sets, and prove the existence of region-fault tolerant spanners of small size.

For a geometric graph G on a point set P and a region F, we define G __F to

be what remains of G after the vertices and edges of G intersecting F have been

removed. A C-fault tolerant t-spanner is a geometric graph G on P such that for

any convex region F, the graph G __F is a t-spanner for Gc(P) __F, where Gc(P) is

the complete geometric graph on P. We prove that any set P of n points admits

a C-fault tolerant (1 + ")-spanner of size O(n log n), for any constant " &gt; 0; if

adding Steiner points is allowed then the size of the spanner reduces to O(n), and

for several special cases we show how to obtain region-fault tolerant spanners of

O(n) size without using Steiner points. We also consider fault-tolerant geodesic

t-spanners: this is a variant where, for any disk D, the distance in G __D between

any two points u, v __ P \D is at most t times the geodesic distance between u and

v in R2 \D. We prove that for any P we can add O(n) Steiner points to obtain a

fault-tolerant geodesic (1 + ")-spanner of size O(n).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1200</field>
<field name="author">Ju Lynn Ong</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">FALSE POSITIVE REDUCTION IN CT COLONOGRAPHY USING SPECTRAL COMPRESSION AND CURVATURE TENSOR SMOOTHING OF SURFACE GEOMETRY</field>
<field name="abstract">Existing polyp detection methods rely heavily on curvaturebased

characteristics to differentiate between lesions. However,

as curvature is a local feature and a second order differential

quantity, noise caused by small bumpy structures and

incoherent curvature fields of a discretized volume or surface

can greatly increase the number of false positives (FPs) detected.

This paper investigates a spectral compression and

curvature tensor smoothing algorithm with the aim to reduce

the number of FPs detected while preserving true positives.

Simulation results give 96% sensitivity for polyps &gt;10mm

while reducing FPs by 92%.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1201</field>
<field name="author">Harvey Tuch</field>
<field name="title">Formal verification of C systems code: Structured types, separation logic and theorem proving</field>
<field name="abstract">Systems code is almost universally written in the C programming language or

a variant. C has a very low level of type and memory abstraction and

formal reasoning about C systems code requires a memory model that

is able to capture the semantics of C pointers and types. At the same time,

proof-based verification demands abstraction, in particular from the

aliasing and frame problems.



In this paper we present a study in the mechanisation of two proof

abstractions for pointer program verification in the Isabelle/HOL theorem

prover, based on a low-level memory model for C.

The language's type system presents challenges for the

multiple independent typed heaps (Burstall-Bornat)

and separation logic proof techniques.

In addition to issues arising from explicit value size/alignment, padding,

type-unsafe casts and pointer address

arithmetic,

structured types such as C's arrays and structs are

problematic

due to the non-monotonic nature of pointer and lvalue validity

in the presence of the unary &amp;-operator.

For example, type-safe updates through pointers to fields of a

struct break the independence of updates across typed heaps or

separating-and-conjuncts. We provide models and

rules that are able to cope with these language features and types, eschewing

common over-simplifications and utilising expressive shallow embeddings

in higher-order logic. Two case studies are provided that demonstrate

the applicability of the mechanised models to real-world systems code;

a working of the standard in-place list reversal example and

an overview of the verification of the L4 microkernel's memory

allocator.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1202</field>
<field name="author">Felix Werner</field>
<field name="author">Frederic Maire</field>
<field name="author">Joaquin Sitte</field>
<field name="title">Topological SLAM using fast Vision Techniques</field>
<field name="abstract">In this paper we propose a method for vision only topological simultaneous localisation and mapping (SLAM). Our approach does not use motion or odometric information but a sequence of colour histograms from visited places. In particular, we address the perceptual aliasing problem which occurs using external observations only in topological navigation. 



We propose a Bayesian inference method to incrementally build a topological map by inferring spatial relations from the sequence of observations while simultaneously estimating the robot's location. The algorithm aims to build a small map which is consistent with local adjacency information extracted from the sequence measurements. Local adjacency information is incorporated to disambiguate places which otherwise would appear to be the same.



Experiments in an indoor environment show that the proposed technique is capable of dealing with perceptual aliasing using visual observations only and successfully performs topological SLAM.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1203</field>
<field name="author">Niklas Carlsson</field>
<field name="author">Derek Eager</field>
<field name="author">Anirban Mahanti</field>
<field name="title">Using Torrent Inflation to Efficiently Serve the Long Tail in Peer-assisted Content Delivery Systems</field>
<field name="abstract">A peer-assisted content delivery system uses the upload bandwidth of its clients to assist in delivery of popular content. In peer-assisted systems using a BitTorrent-like protocol, a content delivery server seeds the offered files, and active torrents form when multiple clients make closely-spaced requests for the same content. Scalability is achieved in the sense of being able to accommodate arbitrarily high request rates for individual files. Scalability with respect to the number of files, however, may be much more difficult to achieve, owing to a ``long tail'' of lukewarm or cold files for which the server may need to assume most or all of the delivery cost. This paper first addresses the question of how best to allocate server resources among multiple active torrents. We then propose new content delivery policies that use some of the available upload bandwidth from currently downloading clients to ``inflate'' torrents for files that would otherwise require substantial server bandwidth. Our performance results show that, in many cases, torrent inflation enables the ``long tail'' to be served with greatly improved efficiency. Finally, a new ``taxation'' policy is proposed in which some fraction of peer upload bandwidth is prioritized for use in other torrents. The proposed policy is shown to yield large reductions in download times for lukewarm/cold files, when sufficient ``tax'' bandwidth is available, at the cost of modestly increased download times for clients downloading popular files.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1204</field>
<field name="author">Gunawan Herman</field>
<field name="author">Getian Ye</field>
<field name="author">Yang Wang</field>
<field name="author">Jie Xu</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="title">Multiple Instance Learning with Appearance-Based Relational Information of Instances</field>
<field name="keyword">Multi-instance learning</field>
<field name="keyword"> image and text categorization</field>
<field name="abstract">Multi-instance learning has applications in many domains, including image and text categorization. In multi-instance learning, similar instances can be found in bags with different labels, and a better classification performance can be expected if we are able to make a distinction between such instances. One of the most effective approaches to multi-instance learning is by using Support Vector Machines with multi-instance kernels. In this paper we propose a simple multi-instance kernel, called MIR-Kernel, that takes into account the appearance-based relational information of instances when computing the affinity between two bags. The aim of MIR-Kernel is to capture the context in which the instances occur within the bags, so that similar instances from bags with different labels can be differentiated. We evaluate the proposed MIR-Kernel on image and text categorization using various datasets.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1205</field>
<field name="author">Gilles Benattar</field>
<field name="author">Franck Cassez</field>
<field name="author">Didier Lime</field>
<field name="author">Olivir Henri Roux</field>
<field name="title">Synthesis of Non-Interferent Timed Systems</field>
<field name="keyword">security</field>
<field name="keyword"> timed automata</field>
<field name="keyword"> synthesis</field>
<field name="abstract">In this paper, we focus on the synthesis of secure timed systems

 which are given by timed automata.

 The security property that the system must satisfy is a

 \emph{non-interference} property. 

 Various notions of non-interference have been defined in the

 literature, and in this paper we focus on \emph{Strong

 Non-deterministic Non-Interference} (SNNI)

 and we study the two following problems: ($1$) check whether it is

 possible to enforce a system to be SNNI; if yes ($2$) compute a

 sub-system which is SNNI.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1206</field>
<field name="author">Franck Cassez</field>
<field name="title">A Note on Fault Diagnosis Algorithms</field>
<field name="keyword">diagnosability</field>
<field name="keyword"> discrete event systems</field>
<field name="abstract">In this paper we review algorithms for checking diagnosability of discrete-event systems and timed automata. We 

point out that the diagnosability problems in both cases reduce to the emptiness problem for (timed) Buchi automata. 

Moreover, it is known that checking whether a discrete-event system is diagnosable can also be reduced to checking bounded diagnosability. We establish a similar result 

for timed automata. We also provide a synthesis of the complexity results for the different fault diagnosis problems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1207</field>
<field name="author">Julian McAuley</field>
<field name="author">Teofilo de Campos</field>
<field name="author">Gabriela Csurka</field>
<field name="author">Frorent Perronnin</field>
<field name="title">Hierarchical Image-Region Labelling via Structured Learning</field>
<field name="abstract">We present a graphical model which encodes a series of hierarchical constraints for

classifying image regions at multiple scales. We show that inference in this model can be

performed efficiently and exactly, rendering it amenable to structured learning. Rather

than using feature vectors derived from images themselves, our model is parametrised

using the outputs of a series of first-order classifiers. Thus our model learns which classifiers

are useful at different scales, and also the relationships between classifiers at different

scales. We present promising results on the VOC2007 and VOC2008 datasets.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1208</field>
<field name="author">Felix Werner</field>
<field name="author">Frederic Maire</field>
<field name="author">Joaquin Sitte</field>
<field name="author">Howie Choset</field>
<field name="author">Stephen Tully</field>
<field name="author">George Kantor</field>
<field name="title">Topological SLAM using Neighbourhood Information of Places</field>
<field name="abstract">Perceptual aliasing makes topological navigation a difficult task. In this paper we present a general approach for topological SLAM~(simultaneous localisation and mapping) which does not require motion or odometry information but only a sequence of noisy measurements from visited places. We propose a particle filtering technique for topological SLAM which relies on a method for disambiguating places which appear indistinguishable using neighbourhood information extracted from the sequence of observations. The algorithm aims to induce a small topological map which is consistent with the observations and simultaneously estimate the location of the robot.



The proposed approach is evaluated using a data set of sonar measurements from an indoor environment which contains several similar places. It is demonstrated that our approach is capable of dealing with severe ambiguities and, and that it infers a small map in terms of vertices which is consistent with the sequence of observations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1209</field>
<field name="author">Michael Broughton</field>
<field name="author">Jeni Paay</field>
<field name="author">Jesper Kjeldskov</field>
<field name="author">Kenton O'Hara</field>
<field name="author">Jane Li</field>
<field name="author">Matthew Phillips</field>
<field name="title">Being here: Designing for Distributed Hands-On Collaboration in Blended Interaction Spaces</field>
<field name="abstract">This paper describes a concept for supporting distributed 

hands-on collaboration through interaction design for the 

physical and the digital workspace. The Blended 

Interaction Spaces concept creates distributed work 

environments in which collaborating parties all feel that 

they are present here rather than there . We describe 

thinking and inspirations behind the Blended Interaction 

Spaces concept, and summarize findings from fieldwork 

activities informing our design. We then exemplify the 

Blended Interaction Spaces concept through a prototype 

implementation of one of four concepts. Based on our 

experience with the prototype we discuss challenges of 

designing Blended Interaction Spaces.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1210</field>
<field name="author">Stephen Tully</field>
<field name="author">George Kantor</field>
<field name="author">Howie Choset</field>
<field name="author">Felix Werner</field>
<field name="title">A Multi-Hypothesis Topological SLAM Approach for Loop Closing on Edge-Ordered Graphs</field>
<field name="abstract">We present a method for topological SLAM that speci cally targets loop closing for edge-ordered graphs. Instead of using a heuristic approach to accept or reject loop closing, we propose a probabilistically grounded multi-hypothesis technique that relies on the incremental construction of a map/state hypothesis tree. Loop closing is introduced automatically within the tree expansion, and likely hypotheses are chosen based on their posterior probability after a sequence of sensor measurements. Careful pruning of the hypothesis tree keeps the growing number of hypotheses under control and a recursive formulation reduces storage and computational costs.

Experiments are used to validate the approach.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1211</field>
<field name="author">Alana Mohamed</field>
<field name="author">Sean Lal</field>
<field name="author">Angus Brown</field>
<field name="author">Rodney Lui</field>
<field name="author">Joshua Ho</field>
<field name="author">Lisa Nguyen</field>
<field name="author">Andy Yong</field>
<field name="author">Filip Braet</field>
<field name="author">Yingying Su</field>
<field name="author">Wayne Dyer</field>
<field name="author">Frank Junius</field>
<field name="author">Bob Cumming</field>
<field name="author">S Benedict Freedman</field>
<field name="author">Leonard Kritharides</field>
<field name="author">Cris G dos Remedios</field>
<field name="title">How to Interrogate the Cellular Immune System in Patients With Ischemic Heart Disease</field>
<field name="keyword">Inflammation</field>
<field name="keyword"> Antibody microarray</field>
<field name="keyword"> Bioinformatics</field>
<field name="abstract">Coronary artery disease (CAD) is caused by atherosclerosis, a disease of the large arteries. Blockage of the coronary circulation can result in ischaemia and eventual myocardial infarction. Patients with CAD may be classified into two groups: stable CAD where patients are asymptomatic or have stable effort angina, and unstable CAD manifesting as acute coronary syndrome. The latter is further divided into patients with unstable angina and patients with myocardial infarction (MI). Stable angina is characterised by a relatively stable stenosis in one or more coronary arteries. Unstable CAD is characterised by an unstable atherosclerotic plaque with either rupture or erosion of the fibrous cap exposing the pro-thrombogenic core. This may lead to downstream vessel occlusion. The challenge is to identify those individuals who will progress from stable to unstable disease. This is likely to involve an ongoing inflammatory response, which is the basis for the disease. Leukocytes play a key role in this process. We conclude that the state of coronary artery disease-associated inflammation can be monitored by: (1) quantifying the expression of protein markers (cluster of differentiation or CD antigens) on the surface of peripheral blood mononuclear cells isolated from both stable and unstable angina patients; and (2) qualitative analysis of cellular fragments (microparticles) released into the plasma as a result of inflammation-induced apoptosis.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1212</field>
<field name="author">Paul Brebner</field>
<field name="author">Jon Gray</field>
<field name="author">Liam O'Brien</field>
<field name="title">Performance Modelling Power Consumption and Carbon Emissions for Server Virtualization of Service Oriented Architectures (SOAs)</field>
<field name="keyword">Performance Modelling</field>
<field name="keyword"> Service Oriented Architecture</field>
<field name="keyword"> SOA</field>
<field name="keyword"> Server Virtualization</field>
<field name="keyword"> Cloud Computing</field>
<field name="keyword"> Power Consumption</field>
<field name="keyword"> Carbon Emissions</field>
<field name="keyword"> Green ICT</field>
<field name="abstract">Server Virtualization is driven by the goal of

reducing the total number of physical servers in an

organisation by consolidating multiple applications on shared

servers. Expected benefits include more efficient server

utilisation, and a decrease in green house gas emissions.

However, Service Oriented Architectures combined with

Server Virtualization may significantly increase risks such as

saturation and Service Level Agreement (SLA) violations.

Since 2006 National ICT Australia Ltd (NICTA) has been

developing a technology for the performance modelling of

large scale heterogeneous Service Oriented Architectures

(SOAs). The technology has been empirically trialled, refined

and validated with collaborating Australian Government

agencies to address critical performance risks. Many

government SOAs are developed, tested and deployed on

virtualized hardware, and we have developed the capability to

model the performance of SOAs deployed on virtual servers.

In this paper we provide an overview of NICTA s SOA

performance modelling approach, and then explore a number

of alternative deployment scenarios for an example SOA based

on a synthetic carbon emission trading system. We show how

our modelling approach provides insights into the relationship

between workloads, services, and resource requirements, and

can therefore be used to predict server power consumption and

carbon emissions. We model and evaluate four different

deployment options including planned and optimised

resources, server virtualization, and computing-on-demand

(cloud computing using Amazon EC2). We conclude with an

overview of other potential problems and benefits of SOA

virtualization.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1213</field>
<field name="author">Peter Carr</field>
<field name="author">Richard Hartley</field>
<field name="title">Minimizing Energy Functions on 4-connected Lattices using Elimination</field>
<field name="abstract">We describe an energy minimization algorithm for functions

defined on 4-connected lattices, of the type usually encountered

in problems involving images. Such functions are often minimized

using graph-cuts/max-flow, but this method is only applicable

to submodular problems. In this paper, we describe an algorithm

that will solve any binary problem, irrespective of whether it is 

submodular or not, and for multilabel problems we use alpha-expansion.

The method is based on the elimination algorithm, which eliminates

nodes from the graph until the remaining function is submodular.

It can then be solved using max-flow. Values of eliminated variables are 

recovered using back-substitution.

We compare the algorithm's performance against alternative methods

for solving non-submodular problems, with favourable results.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1214</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Evaluation - The Crucial Part of Research (and Your Ph.D. Dissertation, too)</field>
<field name="keyword">critical thinking</field>
<field name="keyword"> research methodology</field>
<field name="keyword"> evaluation</field>
<field name="abstract">This is a short invited presentation about the importance of evaluation in research (including Ph.D. research). It is a part of a course lecture for the UNSW graduate course GSOE9400 "Engineering Postgraduate Research Essentials" (http://www.eng.unsw.edu.au/workshops/induction/index.htm).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1215</field>
<field name="author">Bjorn Landfeldt</field>
<field name="author">Suparerk Manitpornsut</field>
<field name="author">Azzedine Boukerche</field>
<field name="title">Efficient Channel Assignment Algorithms for Infrastructure WLANs Under Dense Deployment</field>
<field name="keyword">Radio Spectrum Management</field>
<field name="keyword"> Computer Network Performance</field>
<field name="keyword"> Wireless LAN</field>
<field name="abstract">It is well known that WLAN based on the IEEE 802.11 stan- 

dard su ers from interference and scalability problems due 

to a limited number of non-overlapping channels. In order 

to mitigate the interference problem, channel assignment al- 

gorithms have been a popular research topic in recent years. 

It has been shown that such algorithms can greatly reduce 

the interference among wireless access points. However, in 

this paper we show that previously proposed channel as- 

signment algorithms may lead to an increased number of 

hidden nodes in dense network deployments. We also show 

that this can signi cantly decrease the performance of the 

network. Furthermore, we present results from experiments 

showing that the RTS/CTS mechanism is unable to solve the 

hidden node problem in infrastructure WLANs and there- 

fore, careful consideration needs to be taken when choosing 

channel assignment strategies in densely deployed wireless 

networks. To this end, we propose two novel channel assign- 

ment algorithms. Using a simulation study, we show that 

the proposed algorithms can outperform traditional channel 

assignment in densely deployed scenarios, in terms of QoS 

sensitive VoIP support without compromising the aggregate 

throughput and are therefore a better performing alternative 

in such settings.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1216</field>
<field name="author">Michael Vistein</field>
<field name="author">Frank Ortmeier</field>
<field name="author">Wolfgang Reif</field>
<field name="author">Ralf Huuck</field>
<field name="author">Ansgar Fehnker</field>
<field name="title">An Abstract Specification Language for Static Program Analysis</field>
<field name="keyword">static analysis</field>
<field name="keyword"> specification language</field>
<field name="keyword"> CTL</field>
<field name="keyword"> xpath</field>
<field name="abstract">Static program analysers typically come with a set of hard-coded checks, leaving little room for the user

to add additional properties. In this work, we present a uniform approach to enable the speci&#12;cation of

new static analysis checks in a concise manner. In particular, we present our GPSL/GXSL language to

defi&#12;ne new control flow checks. The language is closely related to CTL speci&#12;cations that are combined

with XPath queries. Moreover, we provide a number of speci&#12;cations as implemented in our tool Goanna,

and report on our experiences of adding new checks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1217</field>
<field name="author">Joshua Ho</field>
<field name="author">Ming-Wei Lin</field>
<field name="author">Stephen Adelstein</field>
<field name="author">Cristobal dos Remedios</field>
<field name="title">Customising an antibody leukocyte capture microarray for Systemic Lupus Erythematosus: Beyond biomarker discovery</field>
<field name="keyword">Antibody microarray </field>
<field name="keyword"> cluster of differentiation antigens </field>
<field name="keyword"> Systemic Lupus Erythematosus </field>
<field name="keyword"> autoimmune disease </field>
<field name="keyword"> bioinformatics</field>
<field name="abstract">Systemic Lupus Erythematosus (SLE) is a complex autoimmune disease that has

heterogeneous clinical manifestation with diverse patterns of organ involvement,

autoantibody profiles and varying degrees of severity of disease. Research and clinical

experience indicate that different subtypes of SLE patients will likely benefit from more

tailored treatment regimes, but we currently lack a fast and objective test with high enough

sensitivity to enable us to perform such sub-grouping for clinical use. In this article, we

review how proteomic technologies could be used as such an objective test. In particular, we

extensively review many leukocyte surface markers that are known to have an association

with the pathogenesis of SLE, and we discuss how these markers can be used in the further

development of a novel SLE-specific antibody leukocyte capture microarray. In addition, we

review some bioinformatics challenges and current methods for using the data generated by

these cell-capture microarrays in clinical use. In a broader context, we hope our experience in

developing a disease specific cell-capture microarray for clinical application can be a guide to

other proteomic practitioners who intend to extend their technologies to develop clinical

diagnostic and prognostic tests for complex diseases.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1218</field>
<field name="author">Kong Suyu</field>
<field name="title">INTELLIGENT VISUAL SURVEILLANCE: ROBUST PERSON AND VEHICLE TRACKING FOR INTELLIGENT VISUAL SURVEILLANCE</field>
<field name="keyword">pedestrian classification</field>
<field name="keyword"> tracking system</field>
<field name="keyword"> blob matching</field>
<field name="keyword"> particle filtering</field>
<field name="abstract">In this book, we describe the proposed pedestrian classification and tracking system that is able to track and label multiple people in an outdoor environment such as a railway station. We propose an approach that combines blob matching with particle filtering to track multiple people in the scene, i.e., the proposed method selects the successful features of blob matching and particle filtering for tracking. In addition, a novel appearance model derived from the colour information from both the moving regions and theoriginal input colour image is proposed to track people in the event of poor foreground extraction. Additionally, the proposed appearance model also includes spatial information of the human body in both vertical and horizontal directions, making location more accurate. Furthermore, a novel method to extract cars from moving regions including shadow area, based on shape and colour information, is proposed. Chamfer template matching score, and non-shadow region edge score, are applied as the shape information; while the shadow confidence score (SCS) is used as the colour information.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1219</field>
<field name="author">John Lim</field>
<field name="author">Nick Barnes</field>
<field name="author">Hongdong Li</field>
<field name="title">Estimating Relative Camera Motion from the Antipodal-Epipolar Constraint</field>
<field name="abstract">This paper introduces a novel antipodal-epipolar constraint on relative camera motion. By using antipodal points, which are available in large Field-of-View cameras, the translational and rotational motions of a camera are geometrically decoupled, allowing them to be separately estimated as two problems in smaller dimensions. This paper presents a new formulation based on discrete camera motions, which works over a larger range of motions compared to previous differential techniques using antipodal points. We demonstrate the use of our constraints with two robust and practical algorithms, one based on the RANSAC sampling strategy, and one based on Hough-like voting. Lastly, we experimentally demonstrate that the proposed constraint works and that the algorithms using it perform accurately and robustly in noisy simulations and in real image sequences.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1220</field>
<field name="author">N. Nethercote</field>
<field name="author">Peter Stuckey</field>
<field name="author">Ralph Becket</field>
<field name="author">S. Brand</field>
<field name="author">Gregory Duck</field>
<field name="author">Guido Tack</field>
<field name="title">MiniZinc: Towards a Standard CP Modelling Language.</field>
<field name="abstract">There is no standard modelling language for constraint programming (CP)

problems. Most solvers have their own modelling language. This makes it

difficult for modellers to experiment with different solvers for a problem.



In this paper we present MiniZinc, a simple but expressive CP modelling

language which is suitable for modelling problems for a range of solvers and

provides a reasonable compromise between many design possibilities. Equally

importantly, we also propose a low-level solver-input language called FlatZinc,

and a straightforward translation from MiniZinc to FlatZinc that preserves

all solver-supported global constraints. This lets a solver writer support

MiniZinc with a minimum of effort they only need to provide a simple FlatZinc

front-end to their solver, and then combine it with an existing

MiniZinc-to-FlatZinc translator. Such a front-end may then serve as a stepping

stone towards a full MiniZinc implementation that is more tailored to the

particular solver.



A standard language for modelling CP problems will encourage experimentation

with and comparisons between different solvers. Although MiniZinc is not

perfect - no standard modelling language will be - we believe its simplicity,

expressiveness, and ease of implementation make it a practical choice for a

standard language.</field>
<field name="date">2014</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1221</field>
<field name="author">Jack Tsai</field>
<field name="author">Tim Moors</field>
<field name="title">Opportunistic Multipath Routing in Wireless Mesh Networks</field>
<field name="keyword">mesh</field>
<field name="keyword"> opportunistic</field>
<field name="keyword"> multipath</field>
<field name="keyword"> routing</field>
<field name="abstract">This paper investigates combining opportunistic routing techniques with multipath routing for achieving reliability and timeliness in fast-changing network conditions. We present two approaches, WIMOP and DOMR, based on source routing and distributed routing, respectively. Instead of using broadcast packets as in most opportunistic routing work, we use unicast with promiscuous listening so that the reliability at each hop can be increased through retransmissions, while maintaining the broadcasting property required by opportunistic routing. We evaluate our work in NS2 against single path routing and MORE. Our results show that using the same amount of redundant data, our approaches were able to achieve better reliability than MORE. In addition, DOMR also has the advantage over WIMOP that it requires significantly less computational time.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1222</field>
<field name="author">Jeff Li</field>
<field name="title">A Low-cost 60 GHz OFDM modem</field>
<field name="keyword">60 GHz</field>
<field name="keyword"> Gigabit wireless modem</field>
<field name="keyword"> LLR</field>
<field name="keyword"> OFDM</field>
<field name="keyword"> QAM</field>
<field name="keyword"> Reed-Solomon code</field>
<field name="keyword"> Soft-decision decoding</field>
<field name="keyword"> Viterbi algorithm</field>
<field name="abstract">Abstract A simple soft-demapping scheme for Rectangular Quadrature Amplitude Modulation (R-QAM) signals is developed for a low-cost 60 GHz OFDM baseband chip design, conforming to the IEEE802.15.3C standard. The scheme is equally applicable to a generic bit-interleaved Orthogonal Frequency Division Multiplexing (OFDM) modulation for both square QAM (S-QAM) and R-QAM constellations. The channel coding employs the concatenation of convolutional and Reed-Solomon (RS) codes. Simulation shows this simple soft-demapping scheme effectively achieves the same performance of an optimum log-likelihood ratio (LLR) demapper. The exact formula for the difference between the proposed approximate LLR and the optimum LLR is derived.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1223</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Patrick Senac</field>
<field name="title">Promoting the Use of Reliable Rate Based Transport Protocols: The Chameleon Protocol</field>
<field name="abstract">TFRC protocol has not been designed to enable reliability. Indeed, the birth of TFRC results from the need of a congestion controlled and realtime transport protocol in order to carry multimedia traffic. Historically, and following the anarchical deployment of congestion control mechanisms implemented on top of UDP protocol, the IETF decided to standardize such protocol in order to provide to multimedia applications developpers a framework for their applications. However, certain applications (like some P2P clients) do not find among current transport protocols an adpated solution and prefer the use of UDP protocol in order to implement their own congestion control and restransmission mechanisms on top of it. This proof that both congestion control variants of the DCCP protocol does not fill the gap between TCP and UDP and that one transport protocol is still missing. This motivate the present contribution which proposes to design and validate a reliable rate-based protocol based on the combined use of TFRC, SACK and an adapted flow control for rate-based congestion control. We argue that TFRC is a perfect alternative to window-based congestion control as most of today applications need to interact with the transport layer. Furthermore, we seek to demonstrate that the combined use of TFRC and SACK can help designing new transport protocols as both mechanisms share the common goal of improving the QoS delivered to flows by offering respectively a mechanism for enhancing flows rate smoothness and a mechanism for loss recovery. Their combined use, associated with their potential modification, offers a source of performance improvements that need applications developpers. We fully detail and benchmark our protocol in order to verify that our resulting prototype inherits from the good properties of TFRC in terms of TCP-friendliness and propose a Java and ns-2 implementation for testing purpose to the networking community.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1224</field>
<field name="author">Tim Baldwin</field>
<field name="author">SuNam Kim</field>
<field name="author">Francis Bond</field>
<field name="author">Sanae Fujita</field>
<field name="author">David Martinez</field>
<field name="author">Takaaki Tanaka</field>
<field name="title">A Reexamination of MRD-based Word Sense Disambiguation</field>
<field name="keyword">Text Analysis</field>
<field name="keyword"> Language Parsing and Understanding</field>
<field name="abstract">This paper reconsiders the task of MRD-based word sense disambiguation, in extending the basic

Lesk algorithm to investigate the impact on WSD performance of different tokenisation schemes

and methods of definition extension. In experimentation over the Hinoki Sensebank and the

Japanese Senseval-2 dictionary task, we demonstrate that sense-sensitive definition extension over

hyponyms, hypernyms and synonyms, combined with definition extension and word tokenisation

leads to WSD accuracy above both unsupervised and supervised baselines. In doing so, we demon-

strate the utility of ontology induction and establish new opportunities for the development of

baseline unsupervised WSD methods.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1225</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Iftikar</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Realistic Data Transfer Scheduling with Uncertainty</field>
<field name="keyword">Heterogeneous networking with mobility</field>
<field name="keyword"> Optimization with multi-interfaces</field>
<field name="keyword"> Two-stage stochastic linear program</field>
<field name="abstract">Next Generation Networks will be comprised of different access technologies. We are already seeing the emergence of mobile devices with the capability of connecting to heterogeneous networks with different capabilities and constraints. In addition, many bandwidth intensive applications have rather relaxed real time constraints allowing for alternative scheduling mechanisms which can take into account user preferences, network characteristics as well as future network resource availability to better exploit network heterogeneity. The current approaches either simply react to changes, or assume that availability predictions are perfect.

In this paper, we propose a scheduling scheme based on stochastic modeling to account for prediction errors. The scheme optimizes overall user utility gain considering imperfect predictions taken over realistic time intervals while catering for different applications needs. We use 180 days of real user data of many users to demonstrate that it consistently out-performs other non-stochastic and greedy approaches in typical networking environments.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1226</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="author">Mohsin Iftikhar</field>
<field name="title">Mobile Data Transfer Scheduling with Uncertainty</field>
<field name="keyword">Scheduling</field>
<field name="keyword"> Mobile communication</field>
<field name="keyword"> Stochastic optimization</field>
<field name="abstract">Multi-interfaced mobile devices can connect to heterogeneous wireless access networks with different capabilities and constraints. Additionally, many bandwidth intensive applications have rather relaxed real-time constraints allowing for alternative scheduling mechanisms which can take into account user preferences, network characteristics as well as future network resource availability to better exploit network heterogeneity. The current approaches either simply react to changes, or assume that availability predictions are perfect.

In this paper, we propose a scheduling scheme based on stochastic modeling to account for prediction errors. The scheme optimizes overall user utility gain considering imperfect predictions taken over realistic time intervals while catering for different applications needs. We use 60 days of real user data of many users to demonstrate that it consistently outperforms other non-stochastic and greedy approaches in typical networking environments.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1227</field>
<field name="author">Jhoanna Pedrasa</field>
<field name="author">Michael Angelo Pedrasa</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Information Exchange for Enhanced Network Selection</field>
<field name="abstract">Current devices use a network selection policy that is mostly driven by the physical layer, choosing the point of attachment with the highest Received Signal Strength Indicator (RSSI). Unfortunately for 802.11 networks, RSSI is not a good indicator of actual network performance as it is normally the bandwidth to the Internet and not the wireless signal conditions which dictates the quality of service a user might experience. Worse, the AP may belong to a pay service which renders it inaccessible to the user. 



MOBIX is a system which leverages on the fact that nodes on the move will meet other nodes who will be able to share conditions of networks they have recently used. MOBIX exchanges reports with other nodes it encounters using a short-range communication channel such as Bluetooth. Our simulation results show that exchanging throughput information resulted in 70% success rate over relying on RSSI measurements alone. Using our power measurements, we show that we can achieve energy savings of more than 80%.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1228</field>
<field name="author">Yuchao Dai</field>
<field name="author">Jochen Trumpf</field>
<field name="author">Hongdong Li</field>
<field name="author">Nick Barnes</field>
<field name="author">Richard Hartley</field>
<field name="title">Rotation Averaging with Application to Camera-Rig Calibration</field>
<field name="abstract">We present a method for calibrating the rotation between two cameras

in a camera rig in the case of non-overlapping fields of view and in

a globally consistent manner. First, rotation averaging strategies

are discussed and an $L_1$-optimal rotation averaging algorithm is

presented which is more robust than the $L_2$-optimal mean and the

direct least squares mean. Second, we alternate between rotation

averaging across several views and conjugate rotation averaging to

achieve a global solution. Various experiments both on synthetic

data and a real camera rig are conducted to evaluate the performance

of the proposed algorithm. Experimental results suggest that the

proposed algorithm realizes global consistency and a high precision

estimate.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1229</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Semantic Content Distribution with Aggregated Profiles</field>
<field name="abstract">Content-based networks have similar characteristics with traditional publish/subscribe system. These networks are more dynamic, scalable and disseminate the data based on user interests. End users subscribe to network according to their interest profiles, publishers publish contents to the network and network delivers them asynchronously according to user subscriptions. Expressive subscription methodology of user interests and compact interest representation is essential to build scalable content-based network. This paper presents an approach for distributing RDF documents in information dissemination network called Semantic Content-based Network (SCBN). This architecture supports information dissemination with complex information representation through graphs. This paper has the following two key contributions. In first part, we present a semantic based user profile description and methodology for user interest subscription to semantic content-based network. Second part describes a distributed profile aggregation framework for generating generic and compact profiles in the network.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1230</field>
<field name="author">Fawad Nazir</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Why Mobile Device Manufacturers Need Games? Games With A Purpose Perspective</field>
<field name="keyword">Games with a purpose</field>
<field name="keyword"> Mobile Social Networks</field>
<field name="keyword"> Simulation Models</field>
<field name="keyword"> GWAP</field>
<field name="abstract">In Mobile Social Networks (MSN) individuals with similar

interests or commonalities, connect to each other using the

mobile phones. Validation of protocols for MSN relies al-

most exclusively on simulations. Thus a simulation using a

mobility model that captures the behavior of nodes in the

real world is required. The current simulations techniques

use random models to generate dynamic MSN. However, the

random models are not suitable for MSN simulations as the

human mobility patterns are not random, rather based on

some predictable social movement patterns.

Inspired by the concept of "Games With Purpose"1, in this

paper we propose to build a game in Second Life(SL) to solve

the problem of MSN protocol validation through simulation.

In our game real users participate using their avatars; while

users enjoy the game we collect the relevant data for MSN

validation. SL is a virtual world simulator accessible via

the Internet having more than ve hundred thousand ac-

tive users. SL can capture dynamics of movement models

as avatars have di erent movement speed, di erent move-

ment patterns and di erent neighbors. In this demo we will

demonstrate our game in Second Life (SL) and discuss how

can games be used to solve MSN protocol validation prob-

lem.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1231</field>
<field name="author">Gerwin Klein</field>
<field name="author">Thomas Sewell</field>
<field name="author">Simon Winwood</field>
<field name="title">Refinement in the formal verification of seL4</field>
<field name="keyword">Isabelle/HOL</field>
<field name="keyword"> seL4</field>
<field name="abstract">We present an overview of the different refinement frameworks used in the

L4.verified project to formally prove the functional correctnes of

the seL4 microkernel. The verification is conducted in the

interactive theorem prover Isabelle/HOL and proceeds in two large refinement

steps: one proof between two monadic, functional specifications 

in HOL and one proof between such a monadic specification and a C program.

To connect these proofs into one overall theorem, we map both refinement

statements into a common overall framework.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1232</field>
<field name="author">Penelope Sanderson</field>
<field name="author">David Liu</field>
<field name="author">Simon Jenkins</field>
<field name="title">Auditory displays in anesthesiology</field>
<field name="keyword">Auditory displays</field>
<field name="keyword"> auditory alarms</field>
<field name="keyword"> sonification</field>
<field name="keyword"> earcons</field>
<field name="keyword"> patient monitoring</field>
<field name="abstract">Purpose of review. We outline and discuss recent work on auditory displays, covering both auditory alarms that indicate technical or physiological threshold levels, and informative auditory displays that provide a continuous awareness of a patient s well-being.

Recent findings. The struggle to make auditory alarms informative proceeds with work on two fronts. In one approach, researchers are developing and evaluating auditory alarm displays to indicate the source and urgency of off-normal states, and are relying on the emergence of smart software algorithms to reduce the false positive rate. In a complementary approach, other researchers are providing information about the patient s well-being in normal as well as abnormal states, generalising the advantages of variable-tone pulse oximetry to other systems and other auditory display formats. In either approach, a multidisciplinary team is essential in the design and evaluation of auditory displays. Finally, because informative auditory displays may subtly change clinical practice, there are repercussions for training. 

Summary. Auditory display in anesthesia can extend well beyond auditory alarms to displays that give the anesthesiologist a continuous peripheral awareness of patient well-being. Much more rigorous approaches should be taken to evaluating auditory displays so they add information rather than noise.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1233</field>
<field name="author">David Liu</field>
<field name="author">Simon Jenkins</field>
<field name="author">Penelope Sanderson</field>
<field name="title">Patient Monitoring with Head Mounted Displays</field>
<field name="keyword">Head-mounted display</field>
<field name="keyword"> HMD</field>
<field name="keyword"> head-up display</field>
<field name="keyword"> head-worn display</field>
<field name="keyword"> monitoring</field>
<field name="abstract">Purpose of review. Head-mounted displays (HMDs) are head-worn display devices that project an information display over the wearer s field of view. This article reviews a recent program of research that investigates the advantages and disadvantages of monitoring with HMDs, and discusses the design considerations and implementation issues that must be addressed before HMDs can be clinically adopted.

Recent findings. HMDs let anesthesiologists spend a larger proportion of their time in the operating room looking towards the patient and surgical field, and a correspondingly smaller proportion of time looking at the standard monitors. Anesthesiologists can detect patient events faster with an HMD when they are busy performing procedures, but not during normal monitoring. There was no evidence of anesthesiologists performance or monitoring behavior being affected by perceptual issues with the HMD, and no evidence that more events were missed with the HMD due to inattentional blindness.

Summary. Anesthesiologists may be able to monitor their patients more effectively when an HMD is used in conjunction with existing monitors, but several engineering and implementation issues need to be resolved before HMDs can be adopted in practice. Further research is needed on the design of information displays for HMDs.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1234</field>
<field name="author">Fawad Nazir</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Participatory Mobile Social Network Simulation Environment</field>
<field name="keyword">Mobility Models</field>
<field name="keyword"> Virtual Worlds</field>
<field name="abstract">Abstract In Mobile Social Networks(MSN) individuals with

similar interests or commonalities, connect to each other using

the mobile phones. Validation of protocols for these networks

relies almost exclusively on simulations. Thus a simulation using

a mobility model that captures the behavior of nodes in the real

world is needed. The current simulations techniques use random

models to generate dynamic MSN. However, the random models

are not suitable for MSN simulations. In this paper we use Second

Life(SL)1 as a simulation environment, which can support dynamics

in simulation models by allowing real users to participate

using their avatars. SL is a virtual world simulator accessible

via the Internet having more than five hundred thousand active

users. SL can capture dynamics of movement models as avatars

have different movement speed, different movement patterns and

different neighbors. Therefore, in this paper we propose the

Virtual Social Simulated Environment (VSSE). VSSE consists of

basic simulation using SL Bots (computer controlled SL agents)

and protocols to allow avatars to participate in the simulation.

Thus making it a participatory MSN simulation environment. We

present the design of our state-driven grid region, a prototype

implementation based on the daily mobility patterns and compare

our system with a similar real-world experiment. To the best

of our knowledge this is the first time that a participatory

MSN simulations environment has been proposed, which allows

anyone, including non-experts, to experiment and experience the

technology. Moreover, this approach allows to validate protocols

by providing a close to real life simulation environment for

researchers.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1235</field>
<field name="author">Chunhua Shen</field>
<field name="author">Junae Kim</field>
<field name="author">Lei Wang</field>
<field name="author">Anton van den Hengel</field>
<field name="title">Positive Semidefinite Metric Learning with Boosting</field>
<field name="keyword">positive semidefinite metric learning</field>
<field name="keyword"> mahalanobis distance metric</field>
<field name="keyword"> boosting</field>
<field name="abstract">The learning of appropriate distance metrics is a critical problem in image classification and retrieval. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. BOOSTMETRIC is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1236</field>
<field name="author">Mohammad Emam Hossain</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">Hye-Young Paik</field>
<field name="author">June Verner</field>
<field name="title">Risks Identification and Mitigation Processes for Using Scrum in Global Software Development: A Conceptual Framework</field>
<field name="keyword">Scrum</field>
<field name="keyword"> Global Software Development</field>
<field name="keyword"> Risk</field>
<field name="abstract">There is growing interest in applying agile practices in Global Software Development (GSD) projects. But project stakeholder distribution in GSD creates a number of challenges that make it difficult to use some agile practices. Moreover, little is known about what the key challenges or risks are, and how GSD project mangers deal with these risks while using agile practices. We conduct a Systematic Literature Review (SLR) following existing guidelines to identify primary papers that discuss the use of Scrum practices in GSD projects. We identify key challenges, due to global project distribution, that restrict the use of Scrum and explore the strategies used by project managers to deal with these challenges. Our findings are consolidated into a conceptual framework and we discuss various elements of this framework. This research is relevant to project managers who are seeking ways to use Scrum in their globally distributed projects.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1237</field>
<field name="author">Vikas Reddy</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="author">Abbas Bigdeli</field>
<field name="title">An Efficient Background Estimation Algorithm for Embedded Smart Cameras</field>
<field name="abstract">Segmentation of foreground objects of interest from an image sequence is an important task in most smart cameras.Background subtraction is a popular and efficient technique used for segmentation. The method assumes that a background model of the scene under analysis is known. However, in many practical circumstances it is unavailable and needs to be estimated from cluttered image sequences. With embedded systems as the target platform, in this paper we propose a sequential technique for background estimation in such conditions, with low computational and memory requirements. The first stage is somewhat similar to that of the recently proposed agglomerative clustering background estimation method, where image sequences are analysed on a block by block basis. For each block location a representative set is maintained which contains distinct blocks obtained along its temporal line. The novelties lie in iteratively filling in background areas by selecting the most appropriate candidate blocks according to the combined frequency responses of extended versions of the candidate block and its neighbourhood. It is assumed that the most appropriate block results in the smoothest response, indirectly enforcing the spatial continuity of structures within a scene. Experiments on real-life surveillance videos demonstrate the advantages of the proposed method.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1238</field>
<field name="author">Paul Bannerman</field>
<field name="author">Mark Staples</field>
<field name="title">Capability-based Software Engineering Performance</field>
<field name="abstract">This presentation reports the progress results of a research project with Australian industry collaborators to provide a more encompassing yet flexible way of improving the performance of software developing organizations than is possible using current software process improvement (SPI) approaches. The project was driven, in part, by companies seeking to build important capabilities that were not addressed by current solutions such as CMMI. It views software engineering as an integral part of a business venture rather than just as a technical activity.



The capability-based focus of the project is based on the view that performance is a function of the resources available to an organization to carry out its chosen activities. Performance is improved by building and exploiting the most appropriate set of capabilities for its purposes.



It presents a framework of the software organization as a collection of technical, relational and delivery capabilities required for successful software development and delivery. In this view, capabilities are more than processes; they are a composite of the knowledge, skills, routines, processes, technologies and values that organizations need to successfully conduct their activities. Some of these capabilities will be common to most organizations but others will be distinctive and, therefore, sources of performance-enhancing value.



The project aims to develop tools and techniques to identify the specific set of capabilities relevant to a particular organization, and determine their value as potential/differentiators (unlike SPI approaches, which typically prescribe a generic set of processes); measure capability and performance levels for ongoing monitoring (unlike SPI approaches such as CMMI which typically measure only once); and make improvements in targeted capability areas (which is difficult to do without ongoing monitoring). The framework enables technical capabilities to be positioned in a context of higher level resources that can be measured at an organizational level (unlike most SPI approaches which only target low level technical processes).</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1239</field>
<field name="author">Mohammad Emam Hossain</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">June Verner</field>
<field name="title">How Can Agile Practices Minimize Global Software Development Co-ordination Risks?</field>
<field name="keyword">Agile</field>
<field name="keyword"> Global Software Development</field>
<field name="keyword"> Coordinating mechanisms</field>
<field name="abstract">The distribution of project stakeholders in Global Software Development (GSD) projects provides significant challenges related to project communication, coordination and control processes. There is a growing interest in applying agile practices in GSD projects in order to leverage the advantages of both approaches, and in some cases, GSD project managers use agile practices to reduce project distribution challenges. We use an existing coordination framework to identify GSD coordination problems due to temporal, geographical and socio-cultural distances. An industry-based case study is used to describe, explore and explain the use of agile practices to reduce development coordination challenges.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1240</field>
<field name="author">Mohammad Emam Hossain</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">June Verner</field>
<field name="title">Towards a Framework for Using Agile Approaches in Global Software Development</field>
<field name="keyword">Agile approaches</field>
<field name="keyword"> Global Software Development</field>
<field name="keyword"> Case study</field>
<field name="abstract">As agile methods and Global Software Development (GSD) are become increasingly popular, GSD project managers have been exploring the viability of using agile approaches in their development environments. Despite the expected benefits of using an agile approach with a GSD project, the overall combining mechanisms of the two approaches are not clearly understood. To address this challenge, we propose a conceptual framework, based on the research literature. This framework is expected to aid a project manager in deciding what agile strategies are effective for a particular GSD project, taking into account project context. We use an industry-based case study to explore the components of our conceptual framework. Our case study is planned and conducted according to specific published case study guidelines. We identify the agile practices and agile supporting practices used by a GSD project manager in our case study and conclude with future research directions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1241</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Smith</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Performance of Piconet Co-existence Schemes in Wireless Body Area Networks</field>
<field name="abstract">Coexistence of multiple wireless body area networks (WBAN) is a very challenging problem because each piconet can have a large number of sensors and their movement is unpredictable. Moreover, suitable global coordination schemes do not exist as there is no natural choice of coordinator between piconets. Adaptive schemes that work well with low-occupancy channels, such as listen before transmit, are not a wise global solution because of the potential for high levels of traffic in any one area. In this paper we investigate the performance of three classic multiple-access schemes - namely TDMA, FDMA and CDMA - for (inter-network) piconet coexistence. We first consider a theoretical analysis of these schemes and then simulate each scheme using real-world interference measurements. It is found that co-channel interference could significantly degrade system performance if left unchecked, and that TDMA and FDMA are better choices than CDMA in terms of co-channel interference mitigation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1242</field>
<field name="author">Tet Fei Yap</field>
<field name="author">Julien Epps</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="title">Glottal Features For Speech-Based Cognitive Load Classification</field>
<field name="keyword">cognitive load</field>
<field name="keyword"> glottal features</field>
<field name="keyword"> GMM classification</field>
<field name="keyword"> voice quality</field>
<field name="abstract">Cognitive load measurement is important when designing adaptive interfaces that optimize the performance of users working on high mental load tasks. Recent research on automatic speech-based measurement system indicates that cognitive load information is more prominent in the frequency region below 1 kHz. This study investigates the effects of cognitive load on glottal parameters (open quotient, normalized amplitude quotient and speed quotient), and proposes a system employing these parameters as features for cognitive load classification. Analysis of the glottal parameter distributions suggests that an increase in cognitive load can be

related to a more creaky voice quality. Additionally, three-class classification results show that score-level fusion of systems based on the glottal features and baseline features (MFCCs, pitch, intensity and shifted delta cepstra) improves the baseline accuracy from 79% to 84%.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1243</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Removing the Redundancy from Distributed Semantic Web Data</field>
<field name="abstract">Peer-to-peer databases have proven to be an effective way for sharing data. However, distributed knowledge management in P2P databases brings variety of non-trivial challenges along with its benefits. Such challenges include determining the right content provider(s) and removing the duplicate data transfer if relatively larger portion of data is redundant in distributed providers. The aim of this paper is to address redundancy reduction problem such that excessive bandwidth usage due to in-network duplicate data transfer can be minimized. We provide analytical and experimental evaluation of our schemes in terms of the number and size of the packets that flows in the network while keeping confidence level of results high.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1244</field>
<field name="author">Phu Ngoc Le</field>
<field name="author">Julien Epps</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="title">A Study of Voice Source and Vocal Tract Filter Based Features in Cognitive Load Classification</field>
<field name="keyword">Cognitive load</field>
<field name="keyword"> voice source</field>
<field name="keyword"> vocal tract filter</field>
<field name="keyword"> speech classification</field>
<field name="keyword"> speech production system</field>
<field name="abstract">Speech has been recognized as an attractive method for the measurement of cognitive load. Previous approaches have used mel frequency cepstral coefficients (MFCCs) as discriminative features to classify cognitive load. The MFCCs contain information from both the voice source and the vocal tract, so that the individual contributions of each to cognitive load variation are unclear. This paper aims to extract speech features related to either the voice source or the vocal tract and use them to discriminate between cognitive load levels in order to identify the individual contribution of each for cognitive load measurement. Voice source-related features are then used to improve the performance of current cognitive load classification systems, using adapted Gaussian mixture models. Our experimental result shows that the use of voice source feature could yield around 12% reduction in relative error rate compared with the baseline system based on MFCCs, intensity, and pitch contour.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1245</field>
<field name="author">Gerwin Klein</field>
<field name="title">Correct OS kernel? Proof? Done!</field>
<field name="keyword">seL4</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> Isabelle</field>
<field name="keyword"> formal verification</field>
<field name="abstract">Two years ago Gernot Heiser demanded in this venue "Your System

 is Secure? Prove it!" He also mentioned the

 L4.verified project at NICTA which is doing just

 that. This proof is now completed and in this article I'm showing

 what we have proved and what that means for security.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1246</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Baris Fidan</field>
<field name="title">Localization Algorithms and Strategies for Wireless Sensor networks</field>
<field name="keyword">Localization</field>
<field name="abstract">Edited book on localization</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1247</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Baris Fidan</field>
<field name="title">Introduction to Wireless Sensor Network Localization</field>
<field name="keyword">wireless sensor networks</field>
<field name="keyword"> localization</field>
<field name="keyword"> AOA</field>
<field name="keyword"> RSS</field>
<field name="keyword"> TDOA</field>
<field name="abstract">Abstract Localization is an important aspect in the field of wireless sensor networks that has attracted significant research interest recently. The interest in wireless sensor network localization is expected to grow further with the advances in the wireless communication techniques and the sensing techniques, and the consequent proliferation of wireless sensor network applications. This chapter provides an overview of various aspects involved in the design and implementation of wireless sensor network localization systems. These can be broadly classified into three categories: the measurement techniques in sensor network localization, sensor network localization theory and algorithms, and experimental study and applications of sensor network localization techniques. This chapter also gives a brief introduction to the other chapters in the book with a focus on explaining how these chapters are related to each other and how topics covered in each chapter fit into the architecture of this book and the big picture of wireless sensor network localization.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1248</field>
<field name="author">Yang Yang</field>
<field name="author">Honglin Hu</field>
<field name="author">Jing Xu</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Relay Technologies for WiMAX and LTE-Advanced Mobile Systems</field>
<field name="keyword">WiMAX</field>
<field name="keyword"> LTE-Advanced</field>
<field name="keyword"> and Relay</field>
<field name="abstract">Relay technologies have been actively studied and considered in the standardization process of next generation mobile broadband communication systems. This paper first introduces and compares different relay types in 3GPP LTE-Advanced and WiMAX standards. Simulation results show that relay technologies can effectively improve service coverage and system throughput. Three relay transmission schemes are then summarized and evaluated in terms of transmission efficiency under different radio channel conditions. Finally, a centralized pairing scheme and a distributed pairing scheme are developed for effective relay selection. Simulation results show that the proposed schemes can maximize the number of served UEs and the overall throughput of a cell in a realistic multiple-RS-multiple-UE scenario.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1249</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Phase Transition Width of K-connectivity in Wireless Multi-hop Networks</field>
<field name="keyword">phase transition width</field>
<field name="keyword"> k-connectivity</field>
<field name="keyword"> connectivity</field>
<field name="abstract">In this paper, we study the phase transition behavior

of k-connectivity (k = 1; 2; :::) in wireless multi-hop networks

where a total of n nodes are randomly and independently

distributed following a uniform distribution in the unit cube

[0; 1]d (d = 1; 2; 3), and each node has a uniform transmission

range r(n). It has been shown that the phase transition of

k-connectivity becomes sharper as the total number of nodes

n increases. In this paper we investigate how fast such phase

transition happens, and derive a generic analytical formula for

the phase transition width of k-connectivity for large enough n

and for any fixed positive integer k in d-dimensional space by

resorting to a Poisson approximation for the node placement.

This result also applies to mobile networks where nodes always

move randomly and independently. Our simulations show that

to achieve a good accuracy, n should be larger than 200 when

k = 1 and d = 1; and n should be larger than 600 when

k 3 and d = 2; 3. The results in this paper are important

for understanding the phase transition phenomenon; and it also

provides valuable insight into the design of wireless multi-hop

networks and the understanding of its characteristics.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1250</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Giant Component of Wireless Multi-hop Networks in the Presence of Shadowing</field>
<field name="abstract">In this paper, we study the transmission power to

secure connectivity of a network. Instead of requiring all nodes to

be connected, we require only that a large fraction (e.g. 95%) be

connected, termed the giant component. We show that with this

slightly relaxed requirement on connectivity, substantial energy

savings can be achieved for a large scale network. Specifically, we

assume that a total of n nodes are randomly, independently and

uniformly distributed in a unit square in &lt;2, and each node has

a uniform transmission power and any two nodes are directly

connected if and only if the power received by one node from the

other node, as determined by the log-normal shadowing model,

is larger than or equal to a given threshold. Firstly, we derive

an upper bound on the minimum transmission power at which

the probability of having a giant component of order above qn

for any fixed q 2 (0; 1) tends to one as n ! 1. Secondly,

we derive a lower bound on the minimum transmission power

at which the probability of having a connected network tends

to one as n ! 1. We then show that the ratio of the above

transmission power required for a giant component to the above

transmission power required for a connected network tends to

zero as n ! 1. This implies a significant energy saving if we

only require most nodes (e.g. 95%) to be connected rather than

requiring all nodes to be connected. This result is also applicable

for any other arbitrary channel model which satisfies certain

intuitively reasonable conditions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1251</field>
<field name="author">Anushiya Kannan</field>
<field name="author">Baris Fidan</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Analysis of Flip Ambiguities for Robust Sensor Network Localization</field>
<field name="abstract">Erroneous local geometric realizations in some parts

of the network due to the sensitivity to certain distance measurement

errors is a major problem in wireless sensor network

localization, which may in turn affect the reliability of the

localization of the whole or a major portion of the sensor

network. This phenomenon is well-described using the notion

of flip ambiguity in rigid graph theory. In this paper, we

present a formal geometric analysis of flip ambiguity problems

in planar sensor networks via quantification of the likelihood

of flip ambiguities in arbitrary sensor neighborhood geometries.

Based on this analysis, we establish a robustness criterion to

detect flip ambiguities in such neighborhood geometries. Beside

analysis, the established robustness criterion is embedded in

certain localization algorithms to enhance the reliability of the

produced location estimates by eliminating neighborhoods with

flip ambiguities from being included in the localization process.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1252</field>
<field name="author">Ta Xiaoyuan</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">Phase Transition Width of Connectivity of Wireless Multi-hop Networks in Shadowing Environment</field>
<field name="abstract">Abstract In this paper, we study the well-known phase transition

behavior of connectivity in a wireless multi-hop network,

but, in contrast to other studies, in a shadowing environment.

We consider that a total of n nodes are randomly, independently

and uniformly distributed on a unit square in &lt;2, each node has

a uniform transmission power and any two nodes are directly

connected if and only if the power received by one node from the

other node, as determined by the log-normal shadowing model,

is larger than or equal to a given threshold. We extend the results

obtained under the unit disk communication model in previous

work to the more realistic log-normal shadowing model, and

derive an analytical formula for the phase transition width of

connectivity for large n. We also demonstrate how our results

can be extended to higher dimensional networks and to other

channel models.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1253</field>
<field name="author">Nikzad Babaii Rizvandi</field>
<field name="author">Javid Taheri</field>
<field name="author">Albert Zomaya</field>
<field name="author">Young Choon Lee</field>
<field name="title">Linear Combinations of DVFS-enabled Processor Frequencies to Modify the Energy-Aware Scheduling Algorithms</field>
<field name="keyword">Energy consumption</field>
<field name="keyword"> cluster</field>
<field name="keyword"> Dynamic Voltage-frequency scaling</field>
<field name="abstract">The energy consumption issue in distributed com-

puting systems has become quite critical due to environmental

concerns. In response to this, many energy-aware scheduling

algorithms have been developed primarily by using the dynamic

voltage-frequency scaling (DVFS) capability incorporated in

recent commodity processors. The majority of these algorithms

involve two passes: schedule generation and slack reclamation.

The latter is typically achieved by lowering processor frequency

for tasks with slacks. In this paper, we revisit this energy reduc-

tion technique from a different perspective and propose a new

slack reclamation algorithm which uses a linear combination of

the maximum and minimum processor frequencies to decrease

energy consumption. This algorithm has been evaluated based

on results obtained from experiments with three different sets

of task graphs: 1,500 randomly generated task graphs, and 300

taskgraphsofeachoftworeal-worldapplications(Gauss-Jordan

and LU decomposition). The results show that the amount of

energy saved in the proposed algorithm is 13.5% , 25.5% and

0.11% for random, LU decomposition and Gauss-Jordan task

graphs, respectively; these percentages for the reference DVFS-

based algorithm are 12.4% , 24.6% and 0.1% , respectively.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1254</field>
<field name="author">Yuri Tselishchev</field>
<field name="author">Thanassis Boulis</field>
<field name="author">Lavy Libman</field>
<field name="title">Experiences and Lessons from Implementing a Wireless Sensor Network MAC Protocol in the Castalia Simulator</field>
<field name="keyword">Wireless Sensor Networks</field>
<field name="keyword"> MAC protocol</field>
<field name="keyword"> Result Reproducability</field>
<field name="abstract">We describe our experience from the implementation of the T-MAC protocol for wireless sensor

networks in the open-source Castalia simulator. Notwithstanding the popularity of the protocol in the research literature in recent years, we find several practical issues that

are not addressed in the original protocol description, which lead to a degree of freedom in the protocol design and implementation and have an impact on its resulting performance. These issues include the ability of the underlying physical layer and hardware to efficiently detect the activation events in the protocol, and necessary changes to the collision resolution and clock synchronization procedures in the presence of varying sleep patterns. Our results highlight the need for rigorous detail in protocol descriptions in the research literature and provide important insights into some of the common pitfalls.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1255</field>
<field name="author">Nicolas Cherbuin</field>
<field name="author">Paulette Lieby</field>
<field name="author">Nick Barnes</field>
<field name="author">Chantal Reglade-Meslin</field>
<field name="author">Jerome Maller</field>
<field name="author">Perminder Sachdev</field>
<field name="author">Kaarin Anster</field>
<field name="title">A new measure of hippocampal surface (W-index) is a better predictor of memory change in young-old +adults than raw volume</field>
<field name="abstract">A number of methods have been developed to detect structural brain changes associated with

+neurodegenerative processes. Volumetric measures including manual tracing of single structures,

+regional volume analyses, and semi-automated segmentation of cortical and subcortical regions of

+interest have proved very useful in tracking disease progression in the more advanced stages of

+dementia. However, such methods are usually not sensitive enough to detect the early brain changes

+occurring in pre-clinical stages which in the case of dementia are thought to begin in the fourth

+decade of life or earlier. To overcome this limitation a number of research groups have

+investigated global measures describing variation in the shape of certain brain structures to

+detect subtle early changes not usually detectable with volumetric measures. For example,

+Csernansky et al. (2005) showed that inward deformation of the hippocampal surface was an early

+predictor of dementia onset in non-demented elderly subjects. Sabatolli et al. (2008) found that

+individuals with Alzheimer's disease and dementia with Lewy bodies could be discriminated based on

+local differences in hippocampal shape. And Thompson et al. (2004) showed that maps representing

+radial measures of hippocampal surface could differentiate between individuals suffering from AD

+and cognitively normal older adults.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1256</field>
<field name="author">Jukka Perkio</field>
<field name="author">Tinne Tuytelaars</field>
<field name="author">Wray Buntine</field>
<field name="title">Exploring Scale-Induced Feature Hierarchies in Natural Images</field>
<field name="keyword">topic model</field>
<field name="keyword"> image words</field>
<field name="keyword"> hierarchical topics</field>
<field name="abstract">Recently there has been considerable interest in topic models

 based on the bag-of-features representation of images. The strong

 independence assumption inherent in the bag-of-features

 representation is not realistic however: patches often overlap

 and share underlying image structures. Moreover, important

 information with respect to relative scales of the features is

 completely ignored, for the sake of scale invariance.

 Considering both spatial and scale-based

 constraints one can derive spatially

 constrained natural feature hierarchies within images.

 We explore the use of topic models that build such spatially constrained

 scale-induced hierarchies of the features in an unsupervised fashion.



 Our model uses standard topic models as a starting

 point. We then incorporate information about the hierarchical

 and spatial relations of the features into the model. We

 illustrate the hierarchical nature of the resulting models using

 data sets of natural images, including the MSRC2 dataset as well

 as a challenging set of images of trees collected from the Internet.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1257</field>
<field name="author">Brian Lam</field>
<field name="author">Subhasis Thakur</field>
<field name="author">Guido Governatori</field>
<field name="author">Abdul Sattar</field>
<field name="title">A model to Coordinate UAVs in urban environment using defeasible logic</field>
<field name="abstract">In this paper we show how a non-monotonic rule based system (defeasible logic) can be integrated with numerical computation engines. To this end we simulate a physical system from which we obtain numerical information. The physical system perceives information from its environment and it sends some predicates which are used by the defeasible logic reasoning engine to make decision and then these decisions are realized by the physical system as it takes action based the decision made by the reasoning engine. We consider a scenario where UAVs have to navigate through an urban environment. The UAVs are autonomous and the is no centralized control. The goal of the UAVs is to navigate without any collision with each other or with any building. In case of a possible collision the concerned UAVs communicate with each other and they uses background knowledge or some travel guidelines to resolve the conflict.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1258</field>
<field name="author">Liam O'Brien</field>
<field name="title">Keynote Talk: Scope, Cost and Effort Estimation for SOA Projects</field>
<field name="keyword">scope</field>
<field name="keyword"> cost</field>
<field name="keyword"> effort</field>
<field name="keyword"> framework</field>
<field name="abstract">Current approaches to costing Service Oriented Architecture (SOA) projects are very limited and have only been applied to specific types of SOA such as Service Development or SOA Application Development. We are developing a Scope, Cost and Effort Estimation Framework that covers a wide range of SOA projects. As the framework is being developed we are engaging with industry and government to work with them on their SOA projects to apply the framework.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1259</field>
<field name="author">M. Asif Khawaja</field>
<field name="author">Fang Chen</field>
<field name="author">Marcus Nadine</field>
<field name="title">Using Language Complexity to Measure Cognitive Load for Adaptive Interaction Design</field>
<field name="abstract">An adaptive interaction system, which is aware of the users current cognitive load, can change its response, presentation and interaction flow to improve users experience and their task performance. In this paper, we propose a novel speech content analysis approach for measuring users cognitive load, based on their language and dialogue complexity. We have analysed the transcribed speech of operators working in computerized incident control rooms and involved in highly complex bushfire management tasks in Australia. The resulting patterns of language complexity show significant differences between the speech from cognitively low load and high load tasks. We also discuss the value of using this approach of cognitive load measurement for user interface evaluation and interaction design improvement.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1260</field>
<field name="author">Ho (Eric) Choi</field>
<field name="author">Ronnie Taib</field>
<field name="author">Yu Shi</field>
<field name="author">Fang Chen</field>
<field name="title">Multimodal User Interface for Traffic Incident Management in Control Room</field>
<field name="keyword">Multmodal user interface</field>
<field name="keyword"> speech recognition</field>
<field name="keyword"> gesture recognition</field>
<field name="keyword"> traffic incident management</field>
<field name="abstract">Efficient road traffic incident management in metropolitan areas is crucial for the smooth traffic flow and the mobility and safety of community. Traffic incident management requires fast and accurate collection and retrieval of critical data, such as incident conditions and contact information for the intervention crew, public safety organisations and other resources. Access to critical data by traffic control operators can be facilitated through various human-computer interfaces. This paper describes the judicious introduction of a multimodal interaction paradigm to the user interfaces for incident handling in a metropolitan transport management centre. Two research prototypes supporting speech and gestural interactions have been developed based on the User Centred Design methodology, and their evaluations have been conducted through user studies. The user studies on the prototypes suggest that multimodal user interfaces can provide traffic control operators with intuitive, cognitively efficient ways to record traffic incident conditions, facilitate fast retrieval of contact details, and support time-critical incident handling. The research prototypes described herein represent some initial steps for the longer-term deployment of advanced multimodal user interface systems for emergency management.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1261</field>
<field name="author">Vladimir Tosic</field>
<field name="title">2009 13th Enterprise Distributed Object Computing Conference Workshops, EDOCW: Proceedings of the IEEE EDOC 2009 Workshops and Short Papers</field>
<field name="keyword">enterprise computing</field>
<field name="keyword"> distributed computing</field>
<field name="keyword"> business process management</field>
<field name="keyword"> middleware</field>
<field name="keyword"> Web services</field>
<field name="keyword"> service-oriented architecture</field>
<field name="keyword"> quality of service</field>
<field name="keyword"> mobile computing</field>
<field name="keyword"> security</field>
<field name="keyword"> privacy</field>
<field name="keyword"> business networks</field>
<field name="keyword"> business ecosystems</field>
<field name="keyword"> enterprise architecture</field>
<field name="keyword"> se</field>
<field name="abstract">EDOC 2009 Short Papers organized by Jo o Paulo A. Almeida, Gillian Dobbie. 



EDOC 2009 Workshops: 

1) Dynamic and Declarative Business Processes (DDBP) 2009 organized by Dragan Ga evi , Georg Grossmann, Sylvain Hall ; 

2) Middleware for Web Services (MWS) 2009 organized by Hye-young Paik, Karl Michael G _schka, Aad Van Moorsel, Raymond Wong, Ian Warren; 

3) Advances in Quality of Service Management (AQuSerM) 2009 organized by Liam O Brien, Iman Hafiz Poernomo, Guijun Wang; 

4) Mobile Technology in Enterprise Computing Systems (MTECS) 2009 organized by Tiong-Thye Goh, Siddhartha Bose, Wee Keong Ng, Longbing Cao, Vincent C.S. Lee; 

5) International Workshop on Security and Privacy in Enterprise Computing (InSPEC) 2009 organized by Rafael Accorsi, Ernesto Damiani, Frank Innerhofer-Oberperfler, Florian Kerschbaum; 

6) Service-Oriented Business Networks and Ecosystems (SOBNE) 2009 organized by Axel Korthaus, Alistair Barros; 

7) Service Oriented Enterprise Architecture for Enterprise Engineering (SoEA@EE) 2009 organized by Selmin Nurcan, Rainer Schmidt; 

8) Vocabularies, Ontologies and Rules for the Enterprise (VORTE) 2009 organized by Jens Dietrich, Dragan Ga evi .</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1262</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Rossella Rubino</field>
<field name="title">Implementing Temporal Defeasible Logic for Modeling Legal Reasoning</field>
<field name="keyword">temporal reasoning</field>
<field name="keyword"> defeasible logic</field>
<field name="keyword"> normative reasoning</field>
<field name="abstract">In this paper we briefly present an efficient implementation of temporal

defeasible logic, and we argue that it can be used to efficiently capture the

the legal concepts of persistence, retroactivity and periodicity. In

particular, we illustrate how the system works with a real life example of a

regulation.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1263</field>
<field name="author">Vida Dujmovic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Pat Morin</field>
<field name="author">Thomas Wolle</field>
<field name="title">Notes on Large Angle Crossing Graphs</field>
<field name="keyword">graph drawing</field>
<field name="keyword"> edge crossings</field>
<field name="keyword"> right-angle crossings</field>
<field name="keyword"> planar graphs</field>
<field name="abstract">A graph $G$ is an $\alpha$ angle crossing ($\alpha$AC) graph if

every pair of crossing edges in $G$ intersect at an angle of at least

$\alpha$. The concept of right angle crossing (RAC) graphs ($\alpha=\pi/2$)

was recently introduced by Didimo et al. 

It was shown that any RAC graph with $n$ vertices has at most $4n-10$ edges

and that there are infinitely many values of $n$ for which there exists a RAC

graph with $n$ vertices and $4n-10$ edges. In this paper, we give upper

and lower bounds for the number of edges in $\alpha$AC graphs for all

$0 &lt; \alpha &lt; \pi/2$.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1264</field>
<field name="author">Pengyi Yang</field>
<field name="author">Bing Bing Zhou</field>
<field name="author">Joshua Ho</field>
<field name="author">Albert Zomaya</field>
<field name="title">genetic ensemble approach for gene-gene interaction identification</field>
<field name="abstract">Background: It has been increasingly clear that

gene-gene interactions and gene-environment interactions are

ubiquitous and fundamental for complex diseases characterization.

Despite the considerable effort in developing statistical models and

algorithmic strategies, detecting and characterizing such

interaction relationships remain challenging. As a consequence,

emphasis is now on the development and incorporation of multiple

algorithms where each has unique property and advantages for

improving the overall chance in identifying such interaction

relationships.

Methods: In this paper, we propose to use a hybrid system

which combines Genetic Algorithm (GA) and an ensemble of classifiers

(called genetic ensemble) for identifying such gene-gene and

gene-environment interactions underlying complex diseases. The

identification of interaction relationships is treated as a data

mining problem of combinatorial feature selection. By collecting

various single nucleotide polymorphisms (SNP) subsets as well as

environmental factors generated in multiple GA runs, patterns of

gene-gene and gene-environment interactions can be extracted using a

simple combinatorial ranking method. Also considered in this study

is the idea of combining identification results obtained from

multiple algorithms. A novel formula based on pairwised double

fault is designed to quantify the complementary degree of them.

Conclusions: Our simulation study demonstrated that the

proposed genetic ensemble system has comparable power to Multifactor

Dimensionality Reduction (MDR) and is slightly better than

Polymorphism Interaction Analysis (PIA) which are the two other most

popular methods for gene-gene interaction identification. More

importantly, the identification results generated by genetic

ensemble system are highly complementary to those obtained by PIA

and MDR. Experimental results from our simulation study and the real

world data application demonstrated the effectiveness of the

proposed genetic ensemble system, as well as the potential benefits

of combining identification results of different algorithms.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1265</field>
<field name="author">John Slaney</field>
<field name="author">A Anbulagan</field>
<field name="title">Towards a Generic CNF Simplifier for Minimising Structured Problem Hardness</field>
<field name="keyword">satisfiability</field>
<field name="keyword"> preprocessing</field>
<field name="keyword"> resolution</field>
<field name="abstract">CNF simplifiers play a very important role in minimising structured problem hardness. Altough they can be used in an in-search process, most of them serve in a pre-search phase and are developed based on the old resolution procedure. Based on our understanding about problem structure, in the paper, we extend the single pre-search process to a multiple one in order to further simplify the hard structure in a problem. This extension boosts the performance of state-of-the-art clause learning and lookahead based SAT solvers when solving both satisfiable and unsatisfiable instances of many real-world hard combinatorial problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1266</field>
<field name="author">Tim Baldwin</field>
<field name="author">Manuel Paul Anil Kumar Joseph</field>
<field name="title">Restoring punctuation and casing in English text</field>
<field name="abstract">This paper explores the use of machine learning techniques to restore punctuation and case in English text, as part of which it investigates the co-dependence of case information and punctuation. We achieve an overall F-score of .619 for the task using a variety of lexical and contextual features, and iterative retagging.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1267</field>
<field name="author">Pengyi Yang</field>
<field name="author">Bing Bing Zhou</field>
<field name="author">zhang zili</field>
<field name="author">Albert Zomaya</field>
<field name="title">A multi-filter enhanced genetic ensemble system for gene selection and sample classification of microarray data</field>
<field name="abstract">Background: Feature selection techniques are critical to the analysis of high dimensional datasets. This is

especially true in gene selection from microarray data which are commonly with extremely high

feature-to-sample ratio. In addition to the essential objectives such as to reduce data noise, to reduce data

redundancy, to improve sample classification accuracy, and to improve model generalization property, feature

selection also helps biologists to focus on the selected genes to further validate their biological hypotheses.

Methods: In this paper we describe an improved hybrid system for gene selection. It is based on a recently

proposed genetic ensemble (GE) system. To enhance the generalization property of the selected genes or gene

subsets and to overcome the overfitting problem of the GE system, we devised a mapping strategy to fuse the

goodness information of each gene provided by multiple filtering algorithms. This information is then used for

initialization and mutation operation of the genetic ensemble system.

Conclusions: We used four benchmark microarray datasets (including both binary-class and multi-class

classification problems) for concept proving and model evaluation. The experimental results indicate that the

proposed multi-filter enhanced genetic ensemble (MF-GE) system is able to improve sample classification

accuracy, generate more compact gene subset, and converge to the selection results more quickly. The MF-GE

system is very flexible as various combinations of multiple filters and classifiers can be incorporated based on the data characteristics and the user preferences.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1268</field>
<field name="author">Pattaraporn Khuwuthyakorn</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">Feature Extraction in Hyperspectral Imagery for EPP Classification</field>
<field name="keyword">Hyperspectral imaging</field>
<field name="keyword"> pest recognition&amp;classification</field>
<field name="abstract">Hyperspectral images are multi-dimensional data captured from and beyond the frequencies of the visible light range. They contain much more spectral bands than the trichromatic and multispectral images. The pixel-level information in each band of the hyperspectral imagery exhibits specific spectral characteristics of the targets of research. This allows distinguishing types of materials which may appear as the same colour to the human eyes. The traditional hyperspectral imaging application is in mining industries and geology for identifying materials and their compositions. As the hyperspectral sensors become cheaper and more practical, the access of this technology to public becomes more feasible. The application of the technology has expanded to broader range of areas, such as, ecology, agriculture and surveillance, etc. It enables the employment of multi-dimensional spectral signatures to perform pest recognition. This hinges on the notion that different pests have different characteristic from the light reflecting, whose variation with wavelength provides a unique description of insects, which can be used for detection and classification of pests into relatively narrow categories. This paper introduces such a practice. We derive a robust method for extracting texture features of insects, making use of high dimensional spectral-texture information. These hyperspectral signatures are used to generate a description of insects, which is based upon Fourier analysis. Our descriptor contains a high information compaction property and can capture the space and wavelength correlation for the spectra in the images under study. This permits capturing the structural features of the plant pests of interest. In the experiment section, we illustrate the utility of our method for plant pest recognition and present results on an Oriental Fruit Moth image dataset.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1269</field>
<field name="author">Yasir Mustafa</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Amelia Azman</field>
<field name="author">Brian Lovell</field>
<field name="title">Face Detection System Design for Real Time High Resolution Smart Camera</field>
<field name="abstract">Abstract Recognizing faces in a crowd in real-time is a key feature which would significantly enhance Intelligent Surveillance Systems. Using a smart camera as a tool to extract faces for recognition would greatly reduce the computational load on the main processing unit. Main processing unit would not be overloaded by the demands of the high data rates of the video and could be designed solely for face recognition. The challenge

is with the increasing speed and resolution of the camera sensors, a fast and robust face detection system is required for real time operation. In this paper we report on a multiple-stage face detection system that is designed for implementation on an FPGA based high resolution smart camera system. The system consist of filter stages to greatly reduce the region of interest in video image, followed by a face detection stage to accurately locate the faces. For filter stage, the algorithm is designed to be very fast so that it can be processed in real time. Meanwhile, for face detection stage, a hardware and software co-design technique is utilised to accelerate it.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1270</field>
<field name="author">Liam O'Brien</field>
<field name="author">Iman Hafiz Poernomo</field>
<field name="author">Guijan Wang</field>
<field name="title">Workshop Summary: Advances in Quality of Service Management (AQuSerM) 2009</field>
<field name="keyword">Quality</field>
<field name="keyword">of</field>
<field name="keyword">Service; Sevice Management; Monitoring</field>
<field name="abstract">This workshop is concerned with advances in QoS-oriented techniques and tools for managing enterprise architectures, encompassing approaches to monitoring, diagnostics, runtime analysis and prediction and adaptation. Model-driven and service-oriented approaches are a special focus of the workshop.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1271</field>
<field name="author">Chris McCarthy</field>
<field name="author">Nick Barnes</field>
<field name="author">Rob Mahony</field>
<field name="title">A robust docking strategy for a mobile robot using flow field divergence</field>
<field name="keyword">Image motion analysis</field>
<field name="keyword"> optical flow</field>
<field name="keyword"> time-to-contact</field>
<field name="keyword"> focus of expansion</field>
<field name="keyword"> robot vision systems.</field>
<field name="abstract">We present a robust strategy for docking a mobile robot in close

proximity with an upright surface using optical flow field

divergence and proportional feedback control.

Unlike previous approaches, we achieve this without the

need for explicit segmentation of features in the

image, and using complete gradient-based optical flow estimation

(i.e. no affine models) in the optical flow computation.

A key contribution is the development of an algorithm to compute the flow

field divergence, or time-to-contact, in a manner that is robust to

small rotations of the robot during ego-motion. This is done by

tracking the focus of expansion of the flow-field and using

this to compensate for ego rotation of the image.

The control law used is a simple proportional feedback,

using the unfiltered flow field divergence as an input, for a dynamic

vehicle model.

Closed-loop stability analysis of 

docking under the proposed feedback is provided.

Performance of the flow field

divergence algorithm is demonstrated using off-board natural

image sequences, and the performance of the closed-loop system is

experimentally demonstrated by control of a mobile robot

approaching a wall.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1272</field>
<field name="author">Tamir Yedidya</field>
<field name="author">Peter Carr</field>
<field name="author">Richard Hartley</field>
<field name="author">Jean-Pierre Guillon</field>
<field name="title">Enforcing Monotonic Temporal Evolution in Dry Eye Images</field>
<field name="abstract">We address the problem of identifying dry areas in the tear film as part

of a diagnostic tool for dry-eye syndrome. The requirement is to identify and measure

the growth of the dry regions to provide a time-evolving map of degrees of

dryness. We segment dry regions using a multi-label graph-cut algorithm on the

3D spatio-temporal volume of frames from a video sequence. To capture the fact

that dryness increases over the time of the sequence, we use a time-asymmetric

cost function that enforces a constraint that the dryness of each pixel monotonically

increases. We demonstrate how this increases our estimation s reliability

and robustness. We tested the method on a set of videos and suggest further research

using a similar approach.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1273</field>
<field name="author">Farhan Ahammed</field>
<field name="author">Albert Zomaya</field>
<field name="author">Max Ott</field>
<field name="title">Location verification in vehicular ad hoc networks</field>
<field name="abstract">Vehicular ad hoc networks are another variation of wireless ad hoc networks where the nodes of the network are the automobiles themselves and may also include some fixed structures located beside the roads. This system allows the vehicles to communicate with one another to avoid collisions and provide up-to-date information about the traffic conditions. Location verification is a technique which is used to ensure that information about the location of vehicles being disclosed is correct.



There are a few strategies of discovering a vehicle's current position to choose from. Some basic ones are looked at in some detail before an in-depth analysis of current schemes and protocols is presented. Most solutions found in the literature concerned with location verification are designed for wireless sensor ad hoc networks. As a consequence, some assumptions that are made include limited power source, a constant or slowly decreasing density or a network-wide cryptographically secure system to name a few. Not all assumptions should or have to be made for vehicular ad hoc networks which are designed for general public use. Each scheme presented in this chapter attacks the problem in its own unique way and is useful in different conditions.



The proposed solutions are separated according to certain classification scheme which highlights some of their strengths and weaknesses. These solutions are investigaed in the context of providing location verification in vehicular ad hoc networks. Certain features are helpful for use in vehicular ad hoc networks while others are not. Noting these "pros and cons" are of importance in this chapter and is the basis of the final discussion and concluding remarks.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1274</field>
<field name="author">Ben Upcroft</field>
<field name="author">Alexei Makarenko</field>
<field name="author">Alex Brooks</field>
<field name="author">Michael Moser</field>
<field name="author">Robert Fitch</field>
<field name="author">Alen Alempijevic</field>
<field name="author">Ashod Donikian</field>
<field name="author">Jonathan Sprinkle</field>
<field name="author">William Uther</field>
<field name="title">Empirical Evaluation of an Autonomous Vehicle in an Urban Environment</field>
<field name="keyword"/>
<field name="abstract">Operation in urban environments creates unique challenges for research in autonomous ground vehicles. Due to the presence of tall trees and buildings in close proximity to traversable areas, GPS outage is likely to be frequent and physical hazards pose real threats to autonomous systems. In this paper, we describe a novel autonomous platform developed by the Sydney-Berkeley Driving Team for entry into the 2007 DARPA Urban Challenge competition. We report empirical results analyzing the performance of the vehicle while navigating a 560-meter test loop multiple times in an actual urban setting with severe GPS outage. We show that our system is robust against failure of global position estimates and can reliably traverse standard two-lane road networks using vision for localization. Finally, we discuss ongoing efforts in fusing vision data with other sensing modalities.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1275</field>
<field name="author">James Petterson</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Julian McAuley</field>
<field name="author">Jin Yu</field>
<field name="title">Exponential Family Graph Matching and Ranking</field>
<field name="abstract">We present a method for learning max-weight matching predictors in bipartite

graphs. The method consists of performing maximum a posteriori estimation

in exponential families with sufficient statistics that encode permutations and

data features. Although inference is in general hard, we show that for one very

relevant application document ranking exact inference is efficient. For general

model instances, an appropriate sampler is readily available. Contrary to existing

max-margin matching models, our approach is statistically consistent and, in ad-

dition, experiments with increasing sample sizes indicate superior improvement

over such models. We apply the method to graph matching in computer vision as

well as to a standard benchmark dataset for learning document ranking, in which

we obtain state-of-the-art results, in particular improving on max-margin variants.

The drawback of this method with respect to max-margin alternatives is its run-

time for large graphs, which is comparatively high.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1276</field>
<field name="author">Jacky Keung</field>
<field name="author">Jenny Liu</field>
<field name="author">Kate Foster</field>
<field name="author">Thong Nguyen</field>
<field name="title">A Statistical Method for Middleware System Architecture Evaluation</field>
<field name="keyword">Software Measurement</field>
<field name="keyword"> Software Architecture Evaluation</field>
<field name="abstract">The architecture of complex software systems is a collection of decisions that are very expensive to change. This makes effective software architecture evaluation methods essential in today s system development for mission critical systems. 

We have previously developed MEMS for evaluating middleware architectures, which provides an effective assessment of important quality attributes and their characterizations. To provide additional quantitative assessments on the overall system performance using actual runtime data, we employed a set of statistical procedures in this work. Our proposed assessment procedures comprises a standard sensitivity analysis procedure that utilizes leverage statistics to identify and remove influential data points, and an estimator for evaluating system stability and a metric for evaluating system load capacity. 

Experiments were conducted using real runtime datasets. Results show that our procedures effectively identified and isolated abnormal data points, and provided valuable statistics to show system stability.

Our approach thus provides a sound statistical basis to support software architecture evaluation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1277</field>
<field name="author">David Smith</field>
<field name="author">Dino Miniutti</field>
<field name="author">Leif Hanlen</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Dynamic Narrowband Body Area Communications: Link-Margin Based Performance Analysis and Second-Order Temporal Statistics</field>
<field name="abstract">A dynamic narrowband on-body area communications scenario is

characterized with respect to link margin as a difference between

system operating point, in terms of receive power, and receiver

sensitivity. The characterization is based on an extensive

measurement campaign near the 900 MHz ISM bands, with a number of

different human subjects moving at a range of speeds in an

indoor-office scenario. Key implications for operating reliability

in terms of outages, meeting latency requirements, infeasibility of

interleaving and limits upon packet duration are drawn from this

link margin analysis pertinent to body-area-communications system

design. The need for receive hardware with good receiver sensitivity

is highlighted. Further to this context important second-order

statistics are given, such as fade duration, as well as a novel

measure: average fade magnitude. Distributions are given to

accurately characterize these second order statistics. Along with

link margin analysis, this modeling can be used to verify possible

implementations, and help in system design.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1278</field>
<field name="author">Eddie Li</field>
<field name="author">Jacky Keung</field>
<field name="title">Software Cost Estimation Framework for Service-Oriented Architecture Systems using Divide-and-Conquer Approach</field>
<field name="keyword">Software Cost Estimation</field>
<field name="keyword"> Service Oriented Architecture</field>
<field name="keyword"> SOA</field>
<field name="abstract">Due to the complexity of Service-Oriented Architecture (SOA), software cost estimation for SOA-based system development is more difficult than that for traditional system development. There is a limit number of work about cost and effort estimation for SOA-based systems. On the other hand, existing cost estimation approaches are inadequate to address the complex service-oriented systems. This paper proposes a novel framework based on the Divide-and-Conquer (D&amp;C) theory to satisfy the cost estimation problem for building a SOA-based system. Through dealing with separated development parts, the D&amp;C framework can help organizations simplify and regulate SOA implementation cost estimation. Furthermore, both cost estimation modeling and software sizing work can be satisfied respectively by switching the corresponding metrics within this framework. Given the requirement of developing these metrics, this ongoing framework also brings the future research in four different directions according to the separated cost estimation sub-problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1279</field>
<field name="author">Scott Sanner</field>
<field name="author">William Uther</field>
<field name="author">Karina Delgado</field>
<field name="title">Approximate Dynamic Programming with Affine ADDs</field>
<field name="keyword">Problem Solving</field>
<field name="keyword"> Control Methods</field>
<field name="keyword"> Dynamic programming</field>
<field name="keyword"> Plan execution</field>
<field name="keyword"> formation</field>
<field name="keyword"> and generation</field>
<field name="abstract">The Affine ADD (AADD) is an extension of the Algebraic Decision

Diagram (ADD) capable of compactly representing context-specific,

additive and multiplicative structure in functions from a discrete

domain to a real-valued range. In this paper, we introduce a novel

algorithm for efficiently finding AADD approximations, allowing us to

develop the MADCAP algorithm for AADD-based structured approximate

dynamic programming (ADP) with factored MDPs.

MADCAP requires less time and space to

achieve comparable or better approximate solutions than the current

state-of-the-art ADD-based MDP ADP algorithm of APRICODD and can provide

approximate solutions for problems on which APRICODD runs out of memory.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1280</field>
<field name="author">Chen Cai</field>
<field name="author">Bernhard Hengst</field>
<field name="author">Getian Ye</field>
<field name="author">Enyang Huang</field>
<field name="author">Yang Wang</field>
<field name="author">Carlos Aydos</field>
<field name="author">Glenn Geers</field>
<field name="title">On the Performance of Adaptive Traffic Signal Control</field>
<field name="keyword">Traffic Signal</field>
<field name="keyword"> Sensor Error</field>
<field name="keyword"> Queue Estimation</field>
<field name="abstract">In this paper, we present a study in understanding sensing error s impact on traffic signal control performance. Adaptive traffic signal control systems depend on information from traffic sensors to interpret the state of traffic. Signal timings are adjusted at real time according to the state of traffic. Queue length is an important element of the state of traffic, and errors in estimating queue length influences control decision and hence the performance. This paper presents the first attempt to quantify the effects of sensing error on control performance in the field of traffic control. A novel technique to estimate queue length using data from single loop detector is presented, and estimations are compared with parallel observations. The results show that moderate overestimation of queue length may significantly improve control performance. The benefit from overestimation suggests including arriving traffic in system state, and using look-ahead algorithms to calculate signal timings.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1281</field>
<field name="author">Paul Bannerman</field>
<field name="title">Managing Structure-related Software Project Risk: A New Role for Project Governance</field>
<field name="keyword">Software project</field>
<field name="keyword"> risk management</field>
<field name="keyword"> organization structure</field>
<field name="keyword"> governance</field>
<field name="abstract">This paper extends recent research on the risk implications of software project organization structures by considering how structure-related risk might be managed. Projects, and other organizations involved in projects, are usually structured according to common forms. These organizational entities interact with each other, creating an environment in which risks relating to their structural forms can impact the project and its performance. The nature of the phenomenon is examined and an approach to managing structure-related risk is proposed, responsibility for which is assigned as a new role for project governance. Due to the structural and relational nature of these risks, the project is poorly place to manage the risks. The paper argues that having identified this previously overlooked source of risk in software projects, risk management practices need to be augmented with additional analyses to identify, analyze and assess these risks to improve project outcomes and the delivery of quality software. The argument is illustrated and initially validated with two case studies of software projects. Implications for research and practice are drawn and directions for future research are suggested, including extending the theory to apply to virtual organizations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1282</field>
<field name="author">Thanassis Boulis</field>
<field name="author">Yuri Tselishchev</field>
<field name="author">Lavy Libman</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="title">Impact of Wireless Channel Temporal Variation on MAC Design for Body Area Networks</field>
<field name="keyword">Body Area Networks</field>
<field name="keyword"> Channel modeling</field>
<field name="keyword"> Medium Access Control (MAC)</field>
<field name="abstract">We investigate the impact of the wireless channel temporal variations on the design of Medium Access Control

(MAC) protocols for Body Area Networks (BAN). More specifically, the deep fades are the features of the

channel that mostly affect the behaviour of the MAC. Our measurements-based channel model captures large

and small time-scale signal correlations giving an accurate picture of the signal variation and thus an accurate picture of the fades. We test the effect of the channel on the performance of the 802.15.4 MAC both in

contention access mode and TDMA access mode. We show that there are considerable differences to the

performance of the MAC compared to simulations that do not model channel temporal variation. Furthermore,

explaining the behaviour of the MAC under a temporal varying channel, we can suggest specific design choices

for the emerging BAN MAC standard.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1283</field>
<field name="author">Suronapee Phoomvuthisarn</field>
<field name="author">Jenny Liu</field>
<field name="author">Jun Han</field>
<field name="title">An Architectural Approach to Composing Reputation-based Trustworthy Services</field>
<field name="abstract">In SOA, Reputation-Based Trust (RBT) mechanism is applied to achieve trust management. RBT enables services to assess the trust level of other services based on the reputation accumulated from user recommendations. A key challenge to apply RBT is to prevent the inciting behavior of users when they provide recommendations -- they might give an unfair rating to benefit themselves. In this paper, we propose a novel architectural approach to integrating auction mechanisms into the trust framework to prevent benefits from untruthful incentives. In this architecture we define an auction-based trust negotiation protocol and realize it in the trust framework. The contribution of our architecture is that it scales and produces accurate results to achieve protection against untruthful incentives, especially when a majority of ratings are unfair, without the potential increase in a computation overhead. An example on a travel agent scenario is devised to collect empirical evidence.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1284</field>
<field name="author">Vikas Reddy</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">A Low Complexity Algorithm for Background Estimation from Cluttered Image Sequences in Surveillance Contexts</field>
<field name="keyword">background estimation</field>
<field name="keyword"> cluttered videos</field>
<field name="keyword"> surveillance.</field>
<field name="abstract">For the purposes of foreground estimation, the true background model is unavailable in many practical circumstances and needs to be estimated from cluttered image sequences. We propose a sequential technique for static background estimation in such conditions, with low computational and memory requirements. Image sequences are analysed on a block-by-block basis. For each block location a representative set is maintained which contains distinct blocks obtained along its temporal line. The background estimation is carried out in a Markov Random Field framework, where the optimal labelling solution is computed using iterated conditional modes. The clique potentials are computed based on the combined frequency response of the candidate block and its neighbourhood. It is assumed that the most appropriate block results in the smoothest response, indirectly enforcing the spatial continuity of structures within a scene. Experiments on real-life surveillance videos demonstrate that the proposed method obtains considerably better background estimates (both qualitatively and quantitatively) than median filtering and the recently proposed "intervals of stable intensity" method. Further experiments on the Wallflower dataset suggest that the combination of the proposed method with a foreground segmentation algorithm results in improved foreground segmentation.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1285</field>
<field name="author">John Judge</field>
<field name="author">John Judge</field>
<field name="author">Colin Kestell</field>
<field name="author">Antoni Blazewicz</field>
<field name="title">Engaging International Students Through the Setting of Challenging Mini-Projects</field>
<field name="keyword">Artemis</field>
<field name="keyword"> masters</field>
<field name="keyword"> outreach</field>
<field name="keyword"> mechatronics</field>
<field name="keyword"> high school</field>
<field name="abstract">This paper reflects upon the introduction of a Co-operative Project to the Curriculum for Masters Students studying Advanced Digital Control at the University of Adelaide s School of Mechanical Engineering. The rationale for the project s structure is presented and its effectiveness is assessed through consideration of student feedback and the success of the student team in a series of national and international competitions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1286</field>
<field name="author">Baris Fidan</field>
<field name="author">Samuel P. Drake</field>
<field name="author">Brian Anderson</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Anushiya Kannan</field>
<field name="title">Collinearity Problems in Passive Target Localization Using Direction Finding Sensors</field>
<field name="abstract">In passive target localization using direction finding

(DF), there are particular sensor-target placements that cause

large biases in the estimates or the failure of estimates to converge

to a unique solution. Identification of such problematic configurations

is crucial for implementing estimation and tracking

algorithms effectively. In this paper we propose four methods

for characterizing near-collinearity problems in a sensor-target

configuration which enable one to quickly and easily identify

cases in which DF-based localization will fail to obtain a solution

or give unreliable results.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1287</field>
<field name="author">Na Cen</field>
<field name="author">Kaiyu Cheng</field>
<field name="author">Baris Fidan</field>
<field name="title">Formation Control of Robotic Swarms Based on Sonar Sensing</field>
<field name="abstract">Because of the simpleness, robustness and accuracy,

sonar sensing has wide applications in navigation, target tracking

and distance estimation for both civilian and defence use. This

paper presents the applications of sonar sensors in the area of

shape maintenance of multi-robot systems moving in formation.

Each robot is equipped with simple sonar sensors and the multi-robot

systems are required to move from an arbitrary initial

position to an arbitrary final position without deforming the

formation shape</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1288</field>
<field name="author">Tet Fei Yap</field>
<field name="author">Eliathamby Ambikairajah</field>
<field name="author">Julien Epps</field>
<field name="author">Ho (Eric) Choi</field>
<field name="title">Cognitive Load Classification Using Formant Features</field>
<field name="keyword">cognitive load</field>
<field name="keyword"> formant features</field>
<field name="keyword"> GMM classification</field>
<field name="keyword"> formant trajectory modeling</field>
<field name="abstract">The ability to automatically classify different cognitive

load levels can be very useful, especially in the field of

human computer interaction, as human task performance

is related to the cognitive load experienced. Although

Mel-frequency cepstral coefficients (MFCCs) are commonly

used in current speech-based cognitive load classification

systems, they offer relatively little insight into

how cognitive load affects the speech spectrum and physical

speech production system an area of research which

remains poorly understood. Since formants are directly

related to the physical characteristics of the vocal tract,

we propose the novel use of formant frequencies, bandwidths

and formant-based regression coefficients for cognitive

load classification. Three-class classification results

showed that formant frequencies performed comparably

to MFCCs. Additionally, formant frequency-based regression

coefficients outperformed MFCC-based regression

coefficients by a relative improvement of about 11%.

These results imply that formants contain important cognitive

load information.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1289</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Leveraging from Neighboring Mobile Contents using Podcasting</field>
<field name="abstract">This paper presents enhanced mobile multimedia content delivery mode using podcasting. The purpose of proposed solution is to efficiently and in a timely manner discover contents and optimize processing and energy conservation due to message exchange for content discovery in multimedia capable handheld devices.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1290</field>
<field name="author">Philip Kilby</field>
<field name="author">Andrew Verden</field>
<field name="author">Lanbo Zheng</field>
<field name="title">The cost of flexible routing</field>
<field name="keyword">Vehicle Routing</field>
<field name="keyword"> Constraint Programming</field>
<field name="keyword"> Fleet Logistics</field>
<field name="abstract">We outline an architecture for solving instances of the Vehicle

Routing Problem that have arbitrary constraints that must be observed

by solutions. The system uses a Constraint Programming (CP) system to

model, propagate and check constraints. The use of the CP system

allows the system to be very flexible -- producing solutions for

essentially arbitrary constraints that model the business practices of

the companies that will use the the system. However, this flexibility

comes at the price of increased execution time, and may effect

solution quality. The primary contribution of the paper is to examine

some facets of the trade-off between flexibility, solution quality

and execution cost.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1291</field>
<field name="author">David Smith</field>
<field name="title">Electromagnetic characterisation through and around human body by simulation using SEMCAD X</field>
<field name="abstract">Contained in this report is some results of electromagnetic simulation using the male human body phantom provided by SPEAG to be used in SEMCAD X [1], proprietary electromagnetic modelling software, and an approximate uniform body and head which has been developed by the author, also in SEMCAD. The simulation in SEMCAD is via the finite difference time domain (FDTD) method, which is in some senses, a first principles form of electromagnetic modelling, working on blocks, or voxels (effectively volumetric pixels), which render a three-dimensional object (including the background which is typically something like air) in terms of it s electromagnetic characteristics, and applying finite differences of Maxwells fundamental electromagnetic equations in terms of electric and magnetic fields in the time domain (This method was originated by Yee in 1966 [2] who specified explicit 

iterations using this method, a good summary of FDTD is in [3]).</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1292</field>
<field name="author">Pengyi Yang</field>
<field name="author">Liang Xu</field>
<field name="author">Bing Bing Zhou</field>
<field name="author">Zili Zhang</field>
<field name="author">Albert Zomaya</field>
<field name="title">A particle swarm based hybrid system for imbalanced medical data sampling</field>
<field name="abstract">Background: Medical and biological data are commonly with small sample size, missing values, and most

importantly, imbalanced class distribution. In this study we propose a particle swarm based hybrid system for

remedying the class imbalance problem in medical and biological data mining. This hybrid system combines the

particle swarm optimization (PSO) algorithm with multiple classifiers and evaluation metrics for evaluation

fusion. Samples from the majority class are ranked using multiple objectives according to their merit in

compensating the class imbalance, and then combined with the minority class to form a balanced dataset.

Results: One important finding of this study is that different classifiers and metrics often provide different

evaluation results. Nevertheless, the proposed hybrid system demonstrates consistent improvements over several

alternative methods with three different metrics. The sampling results also demonstrate good generalization on

different types of classification algorithms, indicating the advantage of information fusion applied in the hybrid system.

Conclusions: The experimental results demonstrate that unlike many currently available methods which often

perform unevenly with different datasets the proposed hybrid system has a better generalization property which

alleviates the method-data dependency problem. From the biological perspective, the system provides indication

for further investigation of the highly ranked samples, which may result in the discovery of new conditions or disease subtypes.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1293</field>
<field name="author">Franck Cassez</field>
<field name="author">Nicolas Markey</field>
<field name="title">Control of Timed Systems</field>
<field name="abstract">In this book Chapter we address the problem of controller synthesis for timed systems. By 

timed systems we refer to systems which are subject to quantitative (hard) real-time 

constraints. We assume the reader is familiar with the basics of Timed Automata 

theory, or has read Chapter 1 and Chapter 2 in this book.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1294</field>
<field name="author">Franck Cassez</field>
<field name="author">Stavros Tripakis</field>
<field name="title">Fault Diagnosis of Timed Systems</field>
<field name="abstract">In this book Chapter, we review the main results pertaining

 to the problem of fault diagnosis of timed

 automata. Timed automata are introduced in Chapter 1

 and Chapter 2 in this book, and the reader not

 familiar with this model is invited to read them

 first.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1295</field>
<field name="author">renato iannella</field>
<field name="title">The Policy-Aware Web: New Semantics, Technologies and Infrastructure for the Web</field>
<field name="keyword">Policy Languages</field>
<field name="keyword"> Semantic Web</field>
<field name="keyword"> Policy-Aware Web</field>
<field name="keyword"> Privacy</field>
<field name="keyword"> Rights</field>
<field name="keyword"> Identity.</field>
<field name="abstract">The Policy-Aware Web is the promise for supporting semantic policy management at the Web infrastructure level. A policy is any set of rules or statements that capture and express the re- quirements of individuals and organisations from a corporate, legal, best practices, and/or social perspective. Currently, pol- icy languages exist that cover and broadly address privacy, access control, and obligation management areas. However, what is missing is an overall framework and architecture for these policy languages to interoperate and provide an account- able, enforceable, flexible and trusted experience for the web community. This paper looks at the technical issues in moving towards a common policy framework and model that both en- capsulates the many vertical interests and provides dependable architectures for deployment. The common framework will also provide for cross-policy interaction and semantic under- standing. The policy-aware web will be a set of new technolo- gies that will enhance the distributed systems of the future web.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1296</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Laurent Dairaine</field>
<field name="author">Guillaume Jourjon</field>
<field name="title">gTFRC, a TCP Friendly QoS-aware Rate Control for Diffserv Assured Service</field>
<field name="abstract">This study addresses the end-to-end congestion control support over the DiffServ Assured Forwarding (AF) class. The resulting Assured Service (AS) provides a minimum level of throughput guarantee. In this context, this article describes a new end-to-end mechanism for continuous transfer based on TCP-Friendly Rate Control (TFRC). The proposed approach modifies TFRC to take into account the QoS negotiated. This mechanism, named gTFRC, is able to reach the minimum throughput guarantee whatever the flow s RTT and target rate. Simulation measurements and implementation over a real QoS testbed demonstrate the efficiency of this mechanism either in over-provisioned or exactly-provisioned network. In addition, we show that the gTFRC mechanism can be used in the same DiffServ/AF class with TCP or TFRC flows.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1297</field>
<field name="author">Amelia Azman</field>
<field name="author">Abbas Bigdeli</field>
<field name="author">Yasir Mustafah</field>
<field name="author">Morteza Biglari-Abhari</field>
<field name="author">Brian Lovell</field>
<field name="title">Exploiting Bayesian Belief Network for Adaptive IP-reuse Decision</field>
<field name="abstract">A smart camera processor has to perform substantial amount of processing of data-intensive operations. Hence, it

is vital to identify critical segments of the processing load by involving HW/SW codesign in smart camera system design. This paper presents a novel fully automatic hybrid framework that combines heuristic and knowledge-based approaches to partition, allocate and schedule IP modules efficiently. In this work, the concept of Bayesian Belief Network (BBN) is utilised and incorporated into the proposed framework. In the experiment section of this paper, we report a comparison of our proposed framework with three previously published work: A BBN based method proposed by a research group from the University of Arizona, the exhaustive algorithm and finally the with greedy algorithms.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1298</field>
<field name="author">Dimosthenis Pediaditakis</field>
<field name="author">Yuri Tselishchev</field>
<field name="author">Thanassis Boulis</field>
<field name="title">Performance and scalability evaluation of the Castalia Wireless Sensor Network simulator</field>
<field name="keyword">Castalia</field>
<field name="keyword"> WSN</field>
<field name="keyword"> wireless sensor network</field>
<field name="keyword"> simulator</field>
<field name="keyword"> performance</field>
<field name="keyword"> scalability</field>
<field name="abstract">Castalia is an open-source simulator for wireless sensor networks and body area networks which is widely used in the academic and research community. This paper presents a basic evaluation study of Castalia, reporting computation time and memory usage for a variety of scenarios/benchmarks. Moreover, key parameters, such as network size, simulation time, fraction of mobile nodes are varied to reveal Castalia s scalability potential. We discuss our results and explain counterintuitive findings in performance. The results and their explanation can be used by Castalia users as a guide to determine the limits they can push their simulations, as well as to make parameter choices that trade-off accuracy for performance. They also provide an indication of Castalia s performance capabilities to potential users.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1299</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="title">From Learning to Researching - Ease the shift through testbeds</field>
<field name="abstract">This papers presents an enhancement of a previously introduced e-learning platform. This update is based on the feedbacks from students and teachers who used it and the integration of a state of the

art management framework.

This enhancement allows three main improvements to the previously introduced networking platform. First, this allows an easier transition from basic to advanced student. Second, development new experiments is facilitated by the use of a robust and modular management framework. Third, this integration is taking part in the current federation and uni&#12;cation process of testbeds around the world.

Furthermore, in parallel to the enhancement of the e-learning platform, we integrated a new emulation module in the testbed management framework. This new module allows to extend the spectrum of possible topologies on a given testbed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1300</field>
<field name="author">Jolyon White</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="title">Measurement Architectures for Network Experiments with Disconnected Mobile Nodes</field>
<field name="abstract">Networking researchers using testbeds containing mobile nodes face the problem of measurement collection from disconnected nodes. We solve this problem efficiently by adding a proxy server to the Orbit Measurement Library (OML) to transparently bu&#11;ffer measurements on disconnected nodes, and we give results showing our solution in action. We then add a flexible filtering and feedback mechanism on the server that enables a tailored hierarchy of measurement collection servers throughout the network, live context-based steering of experiment behaviour, and live context-based control of the measurement collection process itself. We finish by comparing our architecture to existing work.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1301</field>
<field name="author">Worapan Kusakunniran</field>
<field name="author">Qiang Wu</field>
<field name="author">Jian Zhang</field>
<field name="author">Hongdong Li</field>
<field name="title">Support Vector Regression for Multi-View Gait Recognition based on Local Motion Feature Selection</field>
<field name="keyword">Gait recognition</field>
<field name="keyword"> human identification</field>
<field name="keyword"> view transformation</field>
<field name="keyword"> regression</field>
<field name="abstract">Gait is a well recognized biometric feature that is used to identify a human at a distance. However, in the real environment,

appearance changes of individuals due to viewing angle changes cause many dif culties for gait recognition. This paper re-formulates this challenge as a regression problem. A novel solution is proposed to create a View Transformation Model (VTM) from the different point of view based on Support Vector Regression (SVR). To facilitate the process of regression, a new method is proposed to seek local motion feature under one viewing angle, Region of Interest (ROI), for predicting thec orresponding motion

information under another viewing angle. Thus, the well constructed VTM is able to transfer gait information under one viewing angle into another viewing angle. This proposal can achieve view-independent gait recognition. It will normalize gait features under various viewing angles into a common viewing angle before similarity measurement is carried out. The extensive experimental results based on widely adopted benchmark dataset demonstrate that the proposed algorithm can achieve signi cantly better performance than the existing methods inliterature.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1302</field>
<field name="author">Pattaraporn Khuwuthyakorn</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">Statistical Affine Invariant Hyperspectral Texture Descriptors Based Upon Harmonic Analysis.</field>
<field name="abstract">This chapter focuses on the problem of recovering a hyperspectral texture descriptor based upon harmonic analysis. The chapter departs from the use of Fourier transforms to model hyperspectral textures in terms of probability distributions. This provides a link to affine geometric transformations between texture planes and the Fourier domain. Moreover, the use of Fourier analysis permits, in a straightforward manner, the use of harmonic analysis to study these descriptors in the context of Hilbert spaces. This in turn provides a connection to functional analysis to capture the spectral cross-correlation between bands in the image for the generation of a descriptor with a high energy compaction ratio. The method permits the computation of descriptors based upon orthogonal bases with high information compaction properties which can capture the space and wavelength correlation for the spectra in hyperspectral images. We illustrate the robustness of the descriptors to affine transformations by providing a sensitivity analysis on the textures under study and show their utility for purposes of recognition.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1303</field>
<field name="author">Matt Ruan</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Successive Multiuser Detection and Interference Cancelation for Contention Based OFDMA Ranging Channel</field>
<field name="keyword">Orthogonal frequency-division multiple access (OFDMA)</field>
<field name="keyword"> synchronization</field>
<field name="abstract">In this letter, we propose a successive multiuser detector (SMUD) for contention based OFDMA ranging channel compliant to the IEEE 802.16 (WiMAX) standard. A ranging channel consists of a set of subcarriers in specific time slots shared by multiple users, so the multiple access interference (MAI) limits the performance of ranging detectors. Different from existing methods that treat the MAI as noise, the proposed SMUD successively detects the channel paths of active ranging signals and cancels their interference for further detection. This approach significantly suppresses the MAI and improves both user detection and parameter estimation performance.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1304</field>
<field name="author">Shengbo Guo</field>
<field name="author">Scott Sanner</field>
<field name="title">Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries</field>
<field name="keyword">preference elicitation</field>
<field name="keyword"> decision-making under uncertainty</field>
<field name="abstract">Preference elicitation (PE) is an very important component of interactive

decision support systems that aim to make optimal recommendations to

users by actively querying their preferences. In this paper, we present

three principles important for PE in real-world problems: (1) multiattribute, (2) low cognitive load, and (3) robust to noise. In light of three requirements, we introduce

an approximate PE framework based on a variant of TrueSkill for

performing efficient closed-form Bayesian updates and query selection

for a multiattribute utility belief state --- a novel PE approach that

naturally facilitates the efficient evaluation of value of information

(VOI) for use in query selection strategies. Our VOI query strategy satisfies all three principles

and performs on par with the most accurate algorithms on experiments with a synthetic

data set.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1305</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Surrogate Regret Bounds for Proper Losses</field>
<field name="keyword">theory</field>
<field name="keyword"> bounds</field>
<field name="keyword"> convexity</field>
<field name="abstract">We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a demonstration of the convexity of composite losses using canonical link functions.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1306</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Generalised Pinsker Inequalities</field>
<field name="keyword">theory</field>
<field name="keyword"> bounds</field>
<field name="keyword"> divergence</field>
<field name="abstract">We generalise the classical Pinsker inequality which relates variational divergence to Kullback-Liebler divergence in two ways: we consider arbitrary f -divergences in place of KL divergence, and we assume knowledge of a sequence of values of generalised variational divergences. We then develop a best possible inequality for this doubly generalised situation. Specialising our result to the classical case provides a new and tight explicit bound relating KL to variational divergence (solving a problem posed by Vajda some 40 years ago). The solution relies on exploiting a connection between divergences and the Bayes risk of a learning problem via an integral representation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1307</field>
<field name="author">Qinfeng Shi</field>
<field name="author">James Petterson</field>
<field name="author">Gideon Dror</field>
<field name="author">John Langford</field>
<field name="author">Alex Smola</field>
<field name="author">Alex Strehl</field>
<field name="author">Vishy Vishwanathan</field>
<field name="title">Hash Kernels</field>
<field name="abstract">We propose hashing to facilitate e cient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1308</field>
<field name="author">Qinfeng Shi</field>
<field name="author">James Petterson</field>
<field name="author">Gideon Dror</field>
<field name="author">John Langford</field>
<field name="author">Alex Smola</field>
<field name="author">Vishy Vishwanathan</field>
<field name="title">Hash Kernels for Structured Data</field>
<field name="abstract">We propose hashing to facilitate e cient kernels. This generalizes previous work using 

sampling and we show a principled way to compute the kernel matrix for data streams and 

sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. 

This has applications to estimation on strings and graphs.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1309</field>
<field name="author">Shengbo Guo</field>
<field name="author">Scott Sanner</field>
<field name="title">Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries</field>
<field name="keyword">N/A</field>
<field name="abstract">Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences. The PE task consists of (a) querying the user about their preferences and (b) recommending an item that maximizes the user's latent utility.

Of course, a PE system is limited by real-world performance constraints that require phase (a) to be efficient while ensuring phase (b) can make an optimal recommendation with high certainty. Bayesian approaches to

PE have received interest in recent years due to their robust handling of noise in the elicitation process, however, previous work has either relied on expensive sampling methods or on expensive EM refitting of mixture models to deal with the lack of a closed-form for the utility belief update. In this work, we propose to avoid both of these problems by adapting the Bayesian ranking approach of TrueSkill to multiattribute Bayesian

PE, which allows us to efficiently maintain and update the belief representation in real-time and naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies. Our best VOI query strategy is both space- and time-efficient (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with a real-world dataset.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1310</field>
<field name="author">Iman Shames</field>
<field name="author">Brian Anderson</field>
<field name="author">Baris Fidan</field>
<field name="title">On the Use of Convex Optimization in Sensor Network Localization and Synchronization</field>
<field name="abstract">In this paper we report some new results obtained in the field of multi-agent systems that are

based on convex optimization. First, we provide review of a set of polynomial function optimization

tools including sum of squares (SOS) and semidefinite programming (SDP). Then we present several

applications of these tools in various multiagent system localization and synchronization tasks. As the

first application, we propose a method based on SOS relaxation for agent localization using noisy

measurements and describe the solution through SDP. Later, we apply this method to address the

problems of cooperative target localization in the presence of noise and robot pose determination based

on range measurements. Then we introduce the problem of anchor selection for minimizing the effect

of noise in sensor networks via SDP. We use the same machinery to propose a method based on SDP to

enhance synchronizability in networks. We do so by proposing a distributed algorithm for adding new

edges to the network to enhance synchronizability. Finally, we present a method to identify the node in

a network loss of which inflicts the most damage on the synchronizability of the network. Conclusions

are presented in the last section.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1311</field>
<field name="author">Vasanta Chaganti</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="title">Second Order Statistics for Many-link Body Area Networks</field>
<field name="abstract">We present results from a measurement campaign using 40 simultaneous links. The channel gain is shown to fit a log-normal distribution, and the level crossing rates and fade distributions are compared to derived theoretical second order log-normal statistics. Of the two subjects, subject-2 showed a better fit to the lognormal distribution and the resulting second order statistics. Log-normal distribution is seen to be a significant indicator of indoor on body environments.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1312</field>
<field name="author">Leon Craven</field>
<field name="author">Oliver Nagy</field>
<field name="author">Leif Hanlen</field>
<field name="title">Sparsity Enhancing Window Functions for Analogue-to-Information Conversion with Compressed Sensing</field>
<field name="keyword">Analogue-To-Information Conversion</field>
<field name="keyword"> Window Function</field>
<field name="keyword"> Compressed Sensing</field>
<field name="abstract">We show that data reconstruction with analogue-to-information converters can generally be improved by applying a window function. For data recovery via compressed sensing, the choice of window function depends on the number of samples acquired, and any window is better than no window. We also demonstrate that windows can be applied a posteriori in random sampling analogue-to-information converter systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1313</field>
<field name="author">Akshay Asthana</field>
<field name="author">Goecke Roland</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Tom Gedeon</field>
<field name="title">Learning based Automatic Face Annotation for Arbitrary Poses and Expressions from Frontal Images Only</field>
<field name="keyword">Active Appearance Model</field>
<field name="keyword"> Computer Vision</field>
<field name="keyword"> Gaussian Processes</field>
<field name="abstract">Statistical approaches for building non-rigid deformable 

models, such as the Active Appearance Model (AAM), have 

enjoyed great popularity in recent years, but typically re- 

quire tedious manual annotation of training images. In this 

paper, a learning based approach for the automatic anno- 

tation of visually deformable objects from a single anno- 

tated frontal image is presented and demonstrated on the 

example of automatically annotating face images that can 

be used for building AAMs for tting and tracking. This 

approach employs the idea of initially learning the corre- 

spondences between landmarks in a frontal image and a set 

of training images with a face in arbitrary poses. Using 

this learner, virtual images of unseen faces at any arbitrary 

pose for which the learner was trained can be reconstructed 

by predicting the new landmark locations and warping the 

texture from the frontal image. View-based AAMs are then 

built from the virtual images and used for automatically an- 

notating unseen images, including images of different fa- 

cial expressions, at any random pose within the maximum 

range spanned by the virtually reconstructed images. The 

approach is experimentally validated by automatically an- 

notating face images from three different databases.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1314</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Kristian Kersting</field>
<field name="author">Mark Reid</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Wray Buntine</field>
<field name="title">Kernel Conditional Quantile Estimation via Reduction Revisited</field>
<field name="keyword">Regression</field>
<field name="keyword"> Quantile Regression</field>
<field name="keyword"> Gaussian Processes</field>
<field name="abstract">Quantile regression refers to the process of es- 

timating the quantiles of a conditional distribution and has 

many important applications within econometrics and data 

mining, among other domains. In this paper, we show how 

to estimate these conditional quantile functions within a Bayes 

risk minimization framework using a Gaussian process prior. 

The resulting non-parametric probabilistic model is easy to 

implement and allows non-crossing quantile functions to be 

enforced. Moreover, it can directly be used in combination 

with tools and extensions of standard Gaussian Processes 

such as principled hyperparameter estimation, sparsi cation, 

and quantile regression with input-dependent noise rates. No 

existing approach enjoys all of these desirable properties. 

Experiments on benchmark datasets show that our method 

is competitive with state-of-the-art approaches.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1315</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Tiberio Caetano</field>
<field name="author">John Lim</field>
<field name="author">Dale Schuurmans</field>
<field name="title">Convex Relaxation of Mixture Regression with Efficient Algorithms</field>
<field name="keyword">Regression</field>
<field name="keyword"> Mixture of Regression</field>
<field name="keyword"> Convex Relaxation</field>
<field name="keyword"> Expectation Maximization</field>
<field name="abstract">We develop a convex relaxation of maximum a posteriori estimation of a mixture 

of regression models. Although our relaxation involves a semide nite matrix vari- 

able, we reformulate the problem to eliminate the need for general semide nite 

programming. In particular, we provide two reformulations that admit fast algo- 

rithms. The rst is a max-min spectral reformulation exploiting quasi-Newton de- 

scent. The second is a min-min reformulation consisting of fast alternating steps of 

closed-form updates. We evaluate the methods against Expectation-Maximization 

in a real problem of motion segmentation from video data.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1316</field>
<field name="author">Novi Quadrianto</field>
<field name="author">James Petterson</field>
<field name="author">Alex J. Smola</field>
<field name="title">Distribution Matching for Transduction</field>
<field name="keyword">Transduction; Maximum Mean Discrepancy; Large</field>
<field name="keyword">Scale Algorithm</field>
<field name="abstract">Many transductive inference algorithms assume that distributions over training 

and test estimates should be related, e.g. by providing a large margin of separation 

on both sets. We use this idea to design a transduction algorithm which can be 

used without modi cation for classi cation, regression, and structured estimation. 

At its heart we exploit the fact that for a good learner the distributions over the 

outputs on training and test sets should match. This is a classical two-sample 

problem which can be solved ef ciently in its most general form by using distance 

measures in Hilbert Space. It turns out that a number of existing heuristics can be 

viewed as special cases of our approach.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1317</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Alex J. Smola</field>
<field name="author">Le Song</field>
<field name="author">Tinne Tuytelaars</field>
<field name="title">Kernelized Sorting</field>
<field name="keyword">Sorting</field>
<field name="keyword"> Matching</field>
<field name="keyword"> Kernels</field>
<field name="keyword"> Object Alignment</field>
<field name="keyword"> Hilbert Schmidt Independence Criterion</field>
<field name="abstract">Object matching is a fundamental operation in data analysis. It typically requires the de nition of a similarity measure 

between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring 

a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of 

obser vations by means of the Hilber t Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic 

assignment problem with special structure and we present a simple algorithm for nding a locally optimal solution.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1318</field>
<field name="author">William Malcolm</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Lakhdar Aggoun</field>
<field name="title">State Estimation Schemes for Independent Component Coupled Hidden Markov Models</field>
<field name="keyword">Factorial Hidden Markov Models</field>
<field name="keyword"> Change of Measure Techniques</field>
<field name="keyword"> Martingales</field>
<field name="keyword"> EM Algorithms</field>
<field name="keyword"> Filtering</field>
<field name="keyword"> Smoothing</field>
<field name="keyword"> Parameter Estimation</field>
<field name="keyword"> $M$-ary Detection</field>
<field name="abstract">Conventional Hidden Markov models generally consist of a Markov chain

observed through a linear map corrupted by additive noise. This general class

of model has enjoyed a huge and diverse range of applications, for example,

speech processing, biomedical signal processing and more recently

 quantitative finance. However, a lesser known extension of this class of

model, is the so-called Factorial Hidden Markov Model (FHMM). FHMMs also have

diverse applications, notably in machine learning, artificial intelligence

and speech recognition [13,17].

FHMMs extend the usual class of HMMs, by supposing the partially observed

state process is a finite collection of distinct Markov chains, either

statistically independent or dependent.

There is also considerable current activity in applying collections of

partially observed Markov chains to complex action recognition problems, see for

example [6].



In this article we consider the Maximum Likelihood (ML) parameter

estimation problem for FHMMs. Much of the extant literature

concerning this problem presents parameter estimation schemes

based on full data log-likelihood EM algorithms. This approach

can be slow to converge and often imposes heavy demands

on computer memory. The latter point is particularly relevant for

the class of FHMMs, where state space dimensions are relatively

large.



The contribution in this article, however, is to develop new recursive

formulae for a filter-based EM algorithm that can be implemented online.

Our new formulae are equivalent ML estimators, however, these formulae

are purely recursive and so, significantly reduce numerical complexity

and memory requirements. A computer simulation is included to demonstrate

the performance of our results.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1319</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Alex J. Smola</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Quoc Le</field>
<field name="title">Estimating Labels from Label Proportions</field>
<field name="keyword">unsupervised learning</field>
<field name="keyword"> Gaussian processes</field>
<field name="keyword"> classi cation and prediction</field>
<field name="keyword"> probabilistic models</field>
<field name="keyword"> missing variables</field>
<field name="abstract">Consider the following problem: given sets of unlabeled observations, each set with known label 

proportions, predict the labels of another set of observations, possibly with known label propor- 

tions. This problem occurs in areas like e-commerce, politics, spam ltering and improper content 

detection. We present consistent estimators which can reconstruct the correct labels with high prob- 

ability in a uniform convergence sense. Experiments show that our method works well in practice.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1320</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Kristian Kersting</field>
<field name="author">Zhao Xu</field>
<field name="title">Gaussian Process</field>
<field name="keyword">Non-parametric Bayesian</field>
<field name="keyword"> Classification</field>
<field name="keyword"> Regression</field>
<field name="abstract">The Gaussian processes generalize multivariate Gaussian distributions over finite dimensional vectors to infinite dimensionality.

Specifically, it is a stochastic process which has Gaussian distributed finite dimensional marginal distributions, hence the name.

In doing so, it defines a distribution over functions, i.e. each draw from a Gaussian process is a function.

The Gaussian processes provide a principled, practical, and probabilistic approach to inference and learning in kernel machines.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1321</field>
<field name="author">Stefan Pohl</field>
<field name="author">Justin Zobel</field>
<field name="author">Alistair Moffat</field>
<field name="title">Extended Boolean Retrieval for Systematic Biomedical Reviews</field>
<field name="keyword">Information retrieval</field>
<field name="keyword"> extended Boolean retrieval</field>
<field name="keyword"> p-norm</field>
<field name="keyword"> effectiveness</field>
<field name="keyword"> systematic review</field>
<field name="keyword"> biomedical</field>
<field name="abstract">Searching for relevant documents is a laborious task

involved in preparing systematic reviews of biomedical

literature. Currently, complex Boolean queries are iteratively

developed, and then each document of the final

query result is assessed for relevance. However, the result

set sizes of these queries are hard to control, and in

practice it is difficult to balance the competing desires

to keep result sets to a manageable volume, and yet not

exclude relevant documents from consideration.

Ranking overcomes these problems by allowing the

user to choose the number of documents to be inspected.

However, previouswork did not show significant improvements

over the Boolean approach when ranked keyword

queries based on terms in the Boolean queries, review title,

research question or inclusion criteria were used.

The extended Boolean retrieval model also provides

ranked output, but existing complex Boolean queries can

be directly used as formal description of the complex

information needs occurring in this domain. In this paper

we show that extended Boolean retrieval is able to find

a larger quantity of relevant documents than previous

approaches when comparable (or greater) numbers of

documents are inspected for relevance.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1322</field>
<field name="author">Shengbo Guo</field>
<field name="author">Scott Sanner</field>
<field name="title">Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries</field>
<field name="keyword">decision making under uncertainty</field>
<field name="keyword"> preference elicitation</field>
<field name="abstract">Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences. In this paper, we outline five principles important for PE in real-world problems: (1) real-time, (2) multiattribute, (3) low cognitive load, (4) robust to noise, and (5) scalable. In light of these requirements, we introduce an approximate PE framework based on a variant of TrueSkill for performing efficient closed-form Bayesian updates and query selection for a multiattribute utility belief state --- a novel PE approach that naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies. Our best VOI query strategy satisfies all five principles (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with synthetic and real-world datasets.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1323</field>
<field name="author">Markus Rittenbruch</field>
<field name="author">Gregor McEwan</field>
<field name="title">An Historical Reflection of Awareness in Collaboration</field>
<field name="keyword">CSCW</field>
<field name="keyword"> Awareness</field>
<field name="abstract">Mutual awareness has been a focus point of research in Human Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW) since the early 1990s. At its essence, mutual awareness refers to a fundamental quality of collaborative work, the ability of co-workers to perceive each others activities and expressions and relate them to a joint context. In this chapter, we explore the history of awareness concepts by analysing existing literature in order to identify trends, research questions, research approaches and classification schemes throughout different stages of research into awareness. We have adopted a historical angle in the hope that it will allow us to show how awareness research has progressed over time. We document this development using three different phases: (1) Early exploration of awareness (approximately 1990 1994), (2) Diversification and research prototypes (approximately 1995 1999) and (3) Extended models and specialisation (approximately 2000 now). While these phases are to some extent arbitrary and overlapping, they allow us to highlight differences in research focus at the time and understand research in context.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1324</field>
<field name="author">Markus Rittenbruch</field>
<field name="author">Tim Mansfield</field>
<field name="author">Stephen Viller</field>
<field name="title">Design and Evaluation of Intentionally Enriched Awareness</field>
<field name="abstract">In this chapter we introduce and explore the notion of intentionally enriched awareness . Intentional enrichment refers to the process of actively engaging users in the awareness process by enabling them to express intentions. We initially look at the phenomenon if sharing intentional information in related collaborative systems. We then explore the concept of intentional enrichment through designing and evaluating the AnyBiff system which allows users to freely create, share and use a variety of biff applications. Biffs are simple representation of pre-defined activities. Users can select biffs to indicate that they are engaged in an activity. We summarise the results of a trial which allowed us to gain insights into the potential of the AnyBiff prototype and the underlying biff concept to implement intentionally enriched awareness. Our findings show that intentional disclosure mechanisms in the form of biffs were successfully used in a variety of contexts. Users actively engaged in the design of a large variety of biffs and explored many different uses of the concept. The study revealed a whole host of issues with regard to intentionally enriched awareness which give valuable insight into the conception and design of future applications in this area.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1325</field>
<field name="author">Tyrel Russell</field>
<field name="author">Toby Walsh</field>
<field name="title">Manipulating Tournaments in Cup and Round Robin Competitions</field>
<field name="keyword">Social choice</field>
<field name="keyword"> manipulation</field>
<field name="keyword"> computational complexity</field>
<field name="abstract">In sports competitions, teams can manipulate the result by,

for instance, throwing games. We show that we can decide how to manipulate round robin and cup competitions, two of the most popular

types of sporting competitions in polynomial time. In addition, we show

that &#12;finding the minimal number of games that need to be thrown to

manipulate the result can also be determined in polynomial time. Finally, we show that there are several di&#11;erent variations of standard cup

competitions where manipulation remains polynomial.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1326</field>
<field name="author">Enrico Pilotto</field>
<field name="author">Francesca Rossi</field>
<field name="author">Brent Venable</field>
<field name="author">Toby Walsh</field>
<field name="title">Compact Preference Representation in Stable Marriage Problems</field>
<field name="keyword">Social choice</field>
<field name="keyword"> stable marrage</field>
<field name="keyword"> preferences</field>
<field name="abstract">The stable marriage problem has many practical applications in two sided

markets like those that assign doctors to hospitals, students to schools, or

buyers to vendors. Most algorithms to find stable marriages assume that the participants

explicitly expresses a preference ordering. This can be problematic when

the number of options is large or has a combinatorial structure. We consider therefore

using CP-nets, a compact preference formalism in stable marriage problems.

We study the impact of this formalism on the computational complexity of stable

marriage procedures, as well as on the properties of the solutions computed

by these procedures. We show that it is possible to model preferences compactly

without significantly increasing the complexity of stable marriage procedures and

whilst maintaining the desirable properties of the matching returned.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1327</field>
<field name="author">Armin Biere</field>
<field name="author">Marijn Heule</field>
<field name="author">Hans van Maaren</field>
<field name="author">Toby Walsh</field>
<field name="title">Handbook of Satisfiability</field>
<field name="abstract">Satisfiability (SAT) related topics have attracted researchers from various disciplines: logic, applied areas such as planning, scheduling, operations research and combinatorial optimization, but also theoretical issues on the theme of complexity and much more, they all are connected through SAT. My personal interest in SAT stems from actual solving: The increase in power of modern SAT solvers over the past 15 years has been phenomenal. It has become the key enabling technology in automated verification of both computer hardware and software. Bounded Model Checking (BMC) of computer hardware is now probably the most widely used model checking technique. The counterexamples that it finds are just satisfying instances of a Boolean formula obtained by unwinding to some fixed depth a sequential circuit and its specification in linear temporal logic. Extending model checking to software verification is a much more difficult problem on the frontier of current research. One promising approach for languages like C with finite word-length integers is to use the same idea as in BMC but with a decision procedure for the theory of bit-vectors instead of SAT. All decision procedures for bit-vectors that I am familiar with ultimately make use of a fast SAT solver to handle complex formulas. Decision procedures for more complicated theories, like linear real and integer arithmetic, are also used in program verification. Most of them use powerful SAT solvers in an essential way. Clearly, efficient SAT solving is a key technology for 21st century computer science. I expect this collection of papers on all theoretical and practical aspects of SAT solving will be extremely useful to both students and researchers and will lead to many further advances in the field. Edmund Clarke (FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering at Carnegie Mellon University, winner of the 2007 A.M. Turing Award)</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1328</field>
<field name="author">Mohammad Farshi</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Experimental study of geometric t-spanners</field>
<field name="keyword">algorithms</field>
<field name="keyword"> spanners</field>
<field name="keyword"> geometric networks</field>
<field name="abstract">The construction of t-spanners of a given point set has received a lot of attention, especially from a theoretical perspective. In this article, we experimentally study the performance and quality of the most common construction algorithms for points in the Euclidean plane. We implemented the most well-known t-spanner algorithms and tested them on a number of different point sets. The experiments are discussed and compared to the theoretical results, and in several cases, we suggest modifications that are implemented and evaluated. The measures of quality that we consider are the number of edges, the weight, the maximum degree, the spanner diameter, and the number of crossings. This is the first time an extensive comparison has been made between the running times of construction algorithms of t-spanners and the quality of the generated spanners.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1329</field>
<field name="author">Bojan Djordjevic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Anh Pham</field>
<field name="author">Thomas Wolle</field>
<field name="title">Detecting Regular Visit Patterns</field>
<field name="keyword">trajectory</field>
<field name="keyword"> regular pattern</field>
<field name="keyword"> efficient algorithms</field>
<field name="abstract">We are given a trajectory $\T$ and an area $\A$. $\T$ might intersect $\A$ several times,

and our aim is to detect whether $\T$ visits $\A$ with some regularity,

e.g.~what is the longest time span that a GPS-GSM equipped elephant visited a specific lake on a daily (weekly or yearly) basis,

where the elephant has to visit the lake {\em most} of the days (weeks or years), but not necessarily on {\em every} day (week or year).



During the modelling of such applications, we encountered an elementary problem on bitstrings, that we call {\sc LDS (LongestDenseSubstring)}.

The bits of the bitstring correspond to a sequence of regular time points,

in which a bit is set to $1$ if and only if the trajectory $\T$ intersects the area $\A$ at the corresponding time point.

For the LDS problem, we are given a string $s$ as input and want to output a longest substring of $s$,

such that the ratio of $1$'s in the substring is at least a certain threshold.



In our model, LDS is a core problem for many applications that aim at detecting regularity of $\T$ intersecting $\A$.

We propose an optimal algorithm to solve LDS,

and also for related problems that are closer to applications,

we provide efficient algorithms for detecting regularity.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1330</field>
<field name="author">Mohammad Ali Abam</field>
<field name="author">Mark de Berg</field>
<field name="author">Mohammad Farshi</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Geometric Spanners for Weighted Point Sets</field>
<field name="abstract">Let (S,d) be a finite metric space, where each element p in S has a non-negative weight w(p). We study spanners for the set S with respect to weighted distance function dw, where dw(p,q) is w(p)+ d(p,q)+w(q) if p not equal to q and 0 otherwise. We present a general method for turning spanners with respect to the d-metric into spanners with respect to the dw-metric. For any given e &gt; 0, we can apply our method to obtain (5 + e)-spanners with a linear number of edges for three cases: points in Euclidean space Rd, points in spaces of bounded doubling dimension, and points on the boundary of a convex body in Rd where d is the geodesic distance function.

We also describe an alternative method that leads to (2 + e)-spanners for points in R^d and for points on the boundary of a convex body in R^d. The number of edges in these spanners is O(n log n). This bound on the stretch factor is nearly optimal: in any finite metric space and for any e &gt; 0, it is possible to assign weights to the elements such that any non-complete graph has stretch factor larger than 2-e.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1331</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Toby Walsh</field>
<field name="title">Breaking Generator Symmetry</field>
<field name="keyword">symmetry</field>
<field name="keyword"> global constraints</field>
<field name="keyword"> double lex</field>
<field name="abstract">Dealing with large numbers of symmetries is often problematic. One solution is to focus on just symmetries that generate the symmetry group. Whilst there are special cases where breaking just the symmetries in a generating set is complete, there are also cases where no irredundant generating set eliminates all symmetry. However, focusing on just generators improves tractability. We prove that it is polynomial in the size of the generating set to eliminate all symmetric solutions, but NP-hard to prune all symmetric values. Our proof considers row and column symmetry, a common type of symmetry in matrix models where breaking just generator symmetries is very effective. We show that propagating a conjunction of lexicographical ordering con-

straints on the rows and columns of a matrix of decision variables is NP-hard.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1332</field>
<field name="author">Christian Bessiere</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Nina Narodytska</field>
<field name="author">Claude-Guy Quimper</field>
<field name="author">Toby Walsh</field>
<field name="title">Decomposition of the NVALUE Constraint</field>
<field name="keyword">global constraints</field>
<field name="keyword"> decompositions</field>
<field name="abstract">We study decompositions of NVALUE, a global constraint that can be used to model a wide range of problems where values need to be counted. Such decompositions are useful as few constraints toolkits provide propagators for this global constraint. One of our decompositions maintains a global view as enforcing bound consistency on the decomposition achieves bound consistency on the original global constraint. Our experiments demonstrate that this decomposition is ef cient enough to be used in practice</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1333</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Derek Eager</field>
<field name="author">Niklas Carlsson</field>
<field name="author">Siddharth Mitra</field>
<field name="author">Mayank Agrwal</field>
<field name="author">Amit Yadav</field>
<field name="title">Characterizing Web-based Video Sharing Workloads</field>
<field name="abstract">Video sharing services that allow ordinary Web

users to upload video clips of their choice and watch video clips

uploaded by others have recently become very popular.

This paper attempts to identify \emph{invariants} in

video sharing workloads, through comparison of the

workload characteristics of four popular video sharing services.

Our traces contain meta-data on approximately 1.8 million

videos which together have been viewed approximately 6 billion times.

Using these traces, we study the similarities and differences in use

of several Web 2.0 features such as ratings, comments, favorites,

and propensity of uploading content.

In general, we find that active contribution, such as video uploading

and rating of videos, is much less prevalent than passive use.

While uploaders in general are skewed with respect to the number

of videos they upload, the fraction of multi-time uploaders is

found to differ by a factor of two between two of the sites.

The distributions of life-time measures of video popularity are found

to have heavy-tailed forms that are similar across the four sites.

Finally, we consider implications for system design of the

identified invariants. To gain further insight into caching in

video sharing systems, and the relevance to caching of life-time

popularity measures, we gathered an additional data set tracking

views to a set of approximately 1.3 million videos from one of the

services, over a twelve week period.

We find that life-time popularity measures have some relevance for

large cache (hot set) sizes (i.e., a hot set defined according to one

of these measures is indeed relatively ``hot''), but that this

relevance substantially decreases as cache size decreases, owing to

churn in video popularity.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1334</field>
<field name="author">Georgios Katsirelos</field>
<field name="author">Toby Walsh</field>
<field name="title">Symmetries of Symmetry Breaking Constraints</field>
<field name="keyword">Symmetry breaking</field>
<field name="keyword"> constraint satisfaction</field>
<field name="abstract">Symmetry is an important feature of many constraint

programs. We show that any symmetry acting

on a set of symmetry breaking constraints can

be used to break symmetry. Different symmetries

pick out different solutions in each symmetry class.

We use these observations in two methods for eliminating

symmetry from a problem. These methods

are designed to have many of the advantages of

symmetry breaking methods that post static symmetry

breaking constraint without some of the disadvantages.

In particular, the two methods prune

the search space using fast and efficient propagation

of posted constraints, whilst reducing the

conflict between symmetry breaking and branching

heuristics. Experimental results show that the

two methods perform well on some standard benchmarks.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1335</field>
<field name="author">Adrian Bishop</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="author">Kutluyil Dogancay</field>
<field name="author">Pubudu Pathirana</field>
<field name="title">Optimality Analysis of Sensor-Target Localization Geometries</field>
<field name="abstract">The problem of target localization involves estimating the position of a target from multiple and typically noisy measurements of the target

position. It is well known that the relative sensor-target geometry can significantly affect the performance of any particular localization

algorithm. The localization performance can be explicitly characterized by certain measures, for example, by the Cramer-Rao lower bound

(which is equal to the inverse Fisher information matrix) on the estimator variance. In addition, the Cramer-Rao lower bound is commonly

used to generate a so-called uncertainty ellipse which characterizes the spatial variance distribution of an efficient estimate, i.e. an estimate

which achieves the lower bound. The aim of this work is to identify those relative sensor-target geometries which result in a measure of the

uncertainty ellipse being minimized. Deeming such sensor-target geometries to be optimal with respect to the chosen measure, the optimal

sensor-target geometries for range-only, time-of-arrival-based and bearing-only localization are identified and studied in this work. The

optimal geometries for an arbitrary number of sensors are identified and it is shown that an optimal sensor-target configuration is not, in

general, unique. The importance of understanding the influence of the sensor-target geometry on the potential localization performance is

highlighted via formal analytical results and a number of illustrative examples.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1336</field>
<field name="author">Willy Yap</field>
<field name="author">Tim Baldwin</field>
<field name="title">Experiments on Pattern-based Relation Learning</field>
<field name="keyword">relation extraction</field>
<field name="keyword"> information extraction</field>
<field name="abstract">Relation extraction is the task of extracting semantic relations such as synonymy or hypernymy between word pairs from corpus data. Past work in relation extraction has concentrated on manually creating templates to use in directly extracting word pairs for a given semantic relation from corpus text. Recently, there has been a move towards using machine learning to automatically learn these patterns. We build on this research by running experiments investigating the impact of corpus type, corpus size and different parameter settings on learning a range of lexical relations.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1337</field>
<field name="author">Akshay Asthana</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Tom Gedeon</field>
<field name="author">Roland Goecke</field>
<field name="title">Learning-based Face Synthesis for Pose-Robust Recognition from Single Image</field>
<field name="keyword">Gaussian Process Regression</field>
<field name="keyword"> Active Appearance Models</field>
<field name="abstract">Face recognition in real-world conditions requires the ability to deal with a number of conditions, such as variations in pose, illumination and expression. In this paper, we focus on variations in head pose and use a computationally efficient regression-based approach for synthesising face images in different poses, which are used to extend the face recognition training set. In this data-driven approach, the correspondences between facial landmark points in frontal and non-frontal views are learnt offline from manually annotated training data via Gaussian Process Regression. We then use this learner to synthesise non-frontal face images from any unseen frontal image. To demonstrate the utility of this approach, two frontal face recognition systems (the commonly used PCA and the recent Multi-Region Histograms) are augmented with synthesised non-frontal views for each person. This synthesis and augmentation approach is experimentally validated on the FERET dataset, showing a considerable improvement in recognition rates for 40 _ and 60 _ views, while maintaining high recognition rates for 15 _ and 25 _ views.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1338</field>
<field name="author">Marcus Hutter</field>
<field name="title">Discrete MDL Predicts in Total Variation</field>
<field name="keyword">minimum description length</field>
<field name="keyword"> countable model class</field>
<field name="keyword"> total variation distance</field>
<field name="keyword"> sequence prediction</field>
<field name="keyword"> discriminative learning</field>
<field name="keyword"> reinforcement learning.</field>
<field name="abstract">The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1339</field>
<field name="author">Paola M.V. Rancoita</field>
<field name="author">Marcus Hutter</field>
<field name="title">mBPCR: A Package for DNA Copy Number Profile Estimation</field>
<field name="keyword">Bayesian regression</field>
<field name="keyword"> exact polynomial algorithm</field>
<field name="keyword"> piecewise constant function</field>
<field name="keyword"> mBPCR</field>
<field name="keyword"> DNA copy number estimation</field>
<field name="keyword"> micro arrays</field>
<field name="keyword"> genomic aberrations</field>
<field name="keyword"> R package</field>
<field name="abstract">The algorithm mBPCR is a tool for estimating the profile of the log2ratio of copy number data. The procedure is a Bayesian piecewise constant regression and can be applied, generally, to estimate any piecewise constant function (like the log2ratio of the copy number data). The algorithm has been implemented in R and integrated into bioconductor, an open source software for bioinformatics. This document describes how to use the mBPCR bioconductor package in general and on several examples.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1340</field>
<field name="author">Marcus Hutter</field>
<field name="title">Feature Reinforcement Learning: Part I: Unstructured MDPs</field>
<field name="keyword">Reinforcement learning; Markov decision process; partial observability; feature learning; explore</field>
<field name="keyword">exploit; information rational agents</field>
<field name="abstract">General-purpose, intelligent, learning agents cycle through sequences of observations, actions, and rewards that are complex, uncertain, unknown, and non-Markovian. On the other hand, reinforcement learning is well-developed for small finite state Markov decision processes (MDPs). Up to now, extracting the right state representations out of bare observations, that is, reducing the general agent setup to the MDP framework, is an art that involves significant effort by designers. The primary goal of this work is to automate the reduction process and thereby significantly expand the scope of many existing reinforcement learning algorithms and the agents that employ them. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in Part II. The role of POMDPs is also considered there.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1341</field>
<field name="author">Marcus Hutter</field>
<field name="title">Open Problems in Universal Induction and Intelligence</field>
<field name="keyword">Kolmogorov complexity</field>
<field name="keyword"> information theory</field>
<field name="keyword"> sequential decision theory</field>
<field name="keyword"> reinforcement learning</field>
<field name="keyword"> artificial intelligence</field>
<field name="keyword"> universal Solomonoff induction</field>
<field name="keyword"> rational agents</field>
<field name="abstract">Specialized intelligent systems can be found everywhere: finger print, handwriting, speech, and face recognition, spam filtering, chess and other game programs, robots, et al. This decade the first presumably complete mathematical theory of artificial intelligence based on universal induction-prediction-decision-action has been proposed. This information-theoretic approach solidifies the foundations of inductive inference and artificial intelligence. Getting the foundations right usually marks a significant progress and maturing of a field. The theory provides a gold standard and guidance for researchers working on intelligent algorithms. The roots of universal induction have been laid exactly half-a-century ago and the roots of universal intelligence exactly one decade ago. So it is timely to take stock of what has been achieved and what remains to be done. Since there are already good recent surveys, I describe the state-of-the-art only in passing and refer the reader to the literature. This article concentrates on the open problems in universal induction and its extension to universal intelligence.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1342</field>
<field name="author">Marcus Hutter</field>
<field name="title">Bayesian Joint Estimation of CN and LOH Aberrations</field>
<field name="keyword">Bayesian regression</field>
<field name="keyword"> piecewise constant function</field>
<field name="keyword"> change point problem</field>
<field name="keyword"> DNA copy number estimation</field>
<field name="keyword"> LOH estimation</field>
<field name="abstract">SNP-microarrays are able to measure simultaneously both copy number and genotype at several single nucleotide polymorphism positions. Combining the two data, it is possible to better identify genomic aberrations. For this purpose, we propose a Bayesian piecewise constant regression which infers the type of aberration occurred, taking into account all the possible influence in the microarray detection of the genotype, resulting from an altered copy number level. Namely, we model the distributions of the detected genotype given a specific genomic alteration and we estimate the hyper-parameters used on public reference datasets.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1343</field>
<field name="author">Ke Zhang</field>
<field name="author">Marcus Hutter</field>
<field name="author">Warren Jin</field>
<field name="title">A New Local Distance-Based Outlier Detection Approach for Scattered Real-World Data</field>
<field name="keyword">local outlier; scattered data; k</field>
<field name="keyword">distance; KNN; LOF; LDOF</field>
<field name="abstract">Detecting outliers which are grossly different from or inconsistent with the remaining dataset is a major challenge in real-world KDD applications. Existing outlier detection methods are ineffective on scattered real-world datasets due to implicit data patterns and parameter setting issues. We define a novel ``Local Distance-based Outlier Factor'' (LDOF) to measure the outlier-ness of objects in scattered datasets which addresses these issues. LDOF uses the relative location of an object to its neighbours to determine the degree to which the object deviates from its neighbourhood. 



Properties of LDOF are theoretically analysed including LDOF's lower bound and its false-detection probability, as well as parameter settings. In order to facilitate parameter settings in real-world applications, we employ a top-n technique in our outlier detection approach, where only the objects with the highest LDOF values are regarded as outliers. Compared to conventional approaches (such as top-n KNN and top-n LOF), our method top-n LDOF is more effective at detecting outliers in scattered data. It is also easier to set parameters, since its performance is relatively stable over a large range of parameter values, as illustrated by experimental results on both real-world and synthetic datasets.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1344</field>
<field name="author">The Tung Nguyen</field>
<field name="author">Didier El Baz</field>
<field name="author">Pierre Spiteri</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Minh Chau</field>
<field name="title">High performance peer to peer distributed computing with application to numerical simulation</field>
<field name="keyword">communication protocol</field>
<field name="keyword"> self-adaptive protocol</field>
<field name="keyword"> micro-protocols</field>
<field name="keyword"> high performance computing</field>
<field name="keyword"> peer to peer computing</field>
<field name="keyword"> obstacle problem</field>
<field name="abstract">Computational experiments with P2PSAP, a self adaptive communication protocol for peer to peer distributed computing are presented and analyzed. P2PSAP can configure itself automatically in function of application requirements and topology changes by choosing the most appropriate communication mode between peers. P2PSAP is used in conjunction with an environment for high performance distributed computing that allows direct communication between peers.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1345</field>
<field name="author">Omer Gimenez</field>
<field name="author">Guillem Godoy</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Deciding Regularity of the Set of Instances of a Set of Terms with Regular Constraints is EXPTIME-Complete</field>
<field name="keyword">EXPTIME complexity</field>
<field name="keyword"> regularity</field>
<field name="keyword"> terms with variables</field>
<field name="keyword"> pattern matching</field>
<field name="keyword"> regular constraint</field>
<field name="abstract">Finite-state tree automata are a well studied formalism for

representing term languages. This paper studies the problem of

determining the regularity of the set of instances of a finite set of

terms with variables, where each variable is restricted to

instantiations of a regular set given by a tree automaton. The problem

was recently proved decidable, but with an unknown

complexity. Here, the exact complexity of the problem is determined

by proving EXPTIME-completeness. 

The main contribution is a

new, exponential time algorithm that performs various exponential

transformations on the involved terms and tree automata, and decides

regularity by analyzing formulas over inequality and height

predicates.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1346</field>
<field name="author">Steve Glass</field>
<field name="author">Marius Portmann</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">Securing Route and Path Integrity in Multi-Hop Wireless Networks</field>
<field name="keyword">wireles mesh networks</field>
<field name="keyword"> routing</field>
<field name="keyword"> security</field>
<field name="abstract">Wireless networks are becoming ubiquitous and can be found in domestic, commercial, industrial, military, and healthcare applications. In these application areas network security assumes an increasingly important role because security breaches may result in the loss of human lives. This chapter examines the problem of ensuring route integrity in multi-hop wireless networks. A failure of routing integrity can compromise the self-organizing and self-healing properties of the network leading to a network-wide loss of service. Particular attention is given to wireless mesh networks (WMNs) conforming to the IEEE 802.11s standard to provide concrete examples of both the security problems and their solutions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1347</field>
<field name="author">Fawad Nazir</field>
<field name="author">Helmut Prendinger</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Participatory Mobile Social Network Simulation Environment</field>
<field name="keyword"/>
<field name="abstract">In Mobile Social Networks(MSN) individuals with

similar interests or commonalities, connect to each other using

the mobile phones. Validation of protocols for these networks

relies almost exclusively on simulations. Thus a simulation using

a mobility model that captures the behavior of nodes in the real

world is needed. The current simulations techniques use random

models to generate dynamic MSN. However, the random models

are not suitable for MSN simulations. In this paper we use Second

Life(SL)1 as a simulation environment, which can support dynamics

in simulation models by allowing real users to participate

using their avatars. SL is a virtual world simulator accessible

via the Internet having more than five hundred thousand active

users. SL can capture dynamics of movement models as avatars

have different movement speed, different movement patterns and

different neighbors. Therefore, in this paper we propose the

Virtual Social Simulated Environment (VSSE). VSSE consists of

basic simulation using SL Bots (computer controlled SL agents)

and protocols to allow avatars to participate in the simulation.

Thus making it a participatory MSN simulation environment. We

present the design of our state-driven grid region, a prototype

implementation based on the daily mobility patterns and compare

our system with a similar real-world experiment. To the best

of our knowledge this is the first time that a participatory

MSN simulations environment has been proposed, which allows

anyone, including non-experts, to experiment and experience the

technology. Moreover, this approach allows to validate protocols

by providing a close to real life simulation environment for

researchers.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1348</field>
<field name="author">Marcus Hutter</field>
<field name="author">R.A. Servedio</field>
<field name="title">ALT '07 Special Issue</field>
<field name="keyword">algorithmic learning theory</field>
<field name="keyword"> special issue</field>
<field name="keyword"> preface</field>
<field name="abstract">This special issue contains expanded versions of papers that appeared in preliminary form in the proceedings of the 18th International Conference on Algorithmic Learning Theory (ALT 2007), which was held in Sendai, Japan during October 1--4, 2007. Algorithmic Learning Theory is a conference series which is dedicated to the theoretical study of the algorithmic aspects of learning. The best papers of the conference ALT 2007 were invited for this special issue and after a thorough reviewing process, most of them qualified for this Special Issue on Algorithmic Learning Theory of Theoretical Computer Science. The preface contains a short introduction to each of these papers.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1349</field>
<field name="author">Alberto Piatti</field>
<field name="author">Marco Zaffalon</field>
<field name="author">Fabio Trojani</field>
<field name="author">Marcus Hutter</field>
<field name="title">Limits of Learning about a Categorical Latent Variable under Prior Near-Ignorance</field>
<field name="keyword">Near</field>
<field name="keyword">ignorance set of priors; Latent variables; Imprecise Dirichlet model</field>
<field name="abstract">In this paper, we consider the coherent theory of (epistemic) uncertainty of Walley, in which beliefs are represented through sets of probability distributions, and we focus on the problem of modeling prior ignorance about a categorical random variable. In this setting, it is a known result that a state of prior ignorance is not compatible with learning. To overcome this problem, another state of beliefs, called near-ignorance, has been proposed. Near-ignorance resembles ignorance very closely, by satisfying some principles that can arguably be regarded as necessary in a state of ignorance, and allows learning to take place. What this paper does, is to provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is \emph{latent}. We argue that such a setting is by far the most common case in practice, and we provide, for the case of categorical latent variables (and general manifest variables) a condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied even in the most common statistical problems. We regard these results as a strong form of evidence against the possibility to adopt a condition of prior near-ignorance in real statistical problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1350</field>
<field name="author">Marcus Hutter</field>
<field name="title">Exact Non-Parametric Bayesian Inference on Infinite Trees</field>
<field name="keyword">Bayesian density estimation</field>
<field name="keyword"> exact linear time algorithm</field>
<field name="keyword"> non-parametric inference</field>
<field name="keyword"> adaptive infinite tree</field>
<field name="keyword"> Polya tree</field>
<field name="keyword"> scale invariance</field>
<field name="keyword"> consistency</field>
<field name="keyword"> asymptotics</field>
<field name="abstract">Given i.i.d. data from an unknown distribution, we consider the problem of predicting future items. An adaptive way to estimate the probability density is to recursively subdivide the domain to an appropriate data-dependent granularity. A Bayesian would assign a data-independent prior probability to ``subdivide'', which leads to a prior over infinite(ly many) trees. We derive an exact, fast, and simple inference algorithm for such a prior, for the data evidence, the predictive distribution, the effective model dimension, moments, and other quantities. We prove asymptotic convergence and consistency results, and illustrate the behavior of our model on some prototypical functions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1351</field>
<field name="author">Ben Goertzel</field>
<field name="author">Pascal Hitzler</field>
<field name="author">Marcus Hutter</field>
<field name="title">Artificial General Intelligence</field>
<field name="abstract">The Conference on Artificial General Intelligence is the only major conference series devoted wholly and specifically to the creation of AI systems possessing general intelligence at the human level and ultimately beyond. Its second installation, AGI-09, in Arlington, Virginia, March 6-9, 2009, attracted 67 paper submissions, which is a substantial increase from the previous year. Of these submissions, 33 (i.e., 49%) were accepted as full papers for presentation at the conference. Additional 13 papers were included as position papers. The program also included a keynote address by Jurgen Schmidhuber on The New AI, a post-conference workshop on The Future of AI, and a number of pre-conference tutorials on various topics related to AGI.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1352</field>
<field name="author">Marcus Hutter</field>
<field name="title">Feature Markov Decision Processes</field>
<field name="keyword">Reinforcement learning; Markov decision process; partial observability; feature learning; explore</field>
<field name="keyword">exploit</field>
<field name="abstract">General purpose intelligent learning agents cycle through (complex,non-MDP) sequences of observations, actions, and rewards. On the other hand, reinforcement learning is well-developed for small finite state Markov Decision Processes (MDPs). So far it is an art performed by human designers to extract the right state representation out of the bare observations, i.e. to reduce the agent setup to the MDP framework. Before we can think of mechanizing this search for suitable MDPs, we need a formal objective criterion. The main contribution of this article is to develop such a criterion. I also integrate the various parts into one learning algorithm. Extensions to more realistic dynamic Bayesian networks are developed in a companion article.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1353</field>
<field name="author">Marcus Hutter</field>
<field name="title">Feature Dynamic Bayesian Networks</field>
<field name="keyword">Reinforcement learning; dynamic Bayesian network; structure learning; feature learning; global vs. local reward; explore</field>
<field name="keyword">exploit</field>
<field name="abstract">Feature Markov Decision Processes (PhiMDPs) are well-suited for learning agents in general environments. Nevertheless, unstructured (Phi)MDPs are limited to relatively simple environments. Structured MDPs like Dynamic Bayesian Networks (DBNs) are used for large-scale real-world problems. In this article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost criterion that allows to automatically extract the most relevant features from the environment, leading to the 'best' DBN representation. I discuss all building blocks required for a complete general learning algorithm.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1354</field>
<field name="author">Marcus Hutter</field>
<field name="title">Practical Robust Estimators under the Imprecise Dirichlet Model</field>
<field name="keyword">Imprecise Dirichlet Model; exact</field>
<field name="keyword"> conservative</field>
<field name="keyword"> approximate</field>
<field name="keyword"> robust</field>
<field name="keyword"> credible interval estimates; entropy; mutual information</field>
<field name="abstract">Walley's Imprecise Dirichlet Model (IDM) for categorical i.i.d. data extends the classical Dirichlet model to a set of priors. It overcomes several fundamental problems which other approaches to uncertainty suffer from. Yet, to be useful in practice, one needs efficient ways for computing the imprecise=robust sets or intervals. The main objective of this work is to derive exact, conservative, and approximate, robust and credible interval estimates under the IDM for a large class of statistical estimators, including the entropy and mutual information</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1355</field>
<field name="author">Marcus Hutter</field>
<field name="title">Bayesian DNA Copy Number Analysis</field>
<field name="abstract">Background: Some diseases, like tumors, can be related to chromosomal aberrations, leading to changes of DNA copy number. The copy number of an aberrant genome can be represented as a piecewise constant function, since it can exhibit regions of deletions or gains. Instead, in a healthy cell the copy number is two because we inherit one copy of each chromosome from each our parents. Bayesian Piecewise Constant Regression (BPCR) is a Bayesian regression method for data that are noisy observations of a piecewise constant function. The method estimates the unknown segment number, the endpoints of the segments and the value of the segment levels of the underlying piecewise constant function. The Bayesian Regression Curve (BRC) estimates the same data with a smoothing curve. However, in the original formulation, some estimators failed to properly determine the corresponding parameters. For example, the boundary estimator did not take into account the dependency among the boundaries and succeeded in estimating more than one breakpoint at the same position, losing segments.

Results: We derived an improved version of the BPCR (called mBPCR) and BRC, changing the segment number estimator and the boundary estimator to enhance the fitting procedure. We also proposed an alternative estimator of the variance of the segment levels, which is useful in case of data with high noise. Using artificial data, we compared the original and the modified version of BPCR and BRC with other regression methods, showing that our improved version of BPCR generally outperformed all the others. Similar results were also observed on real data.

Conclusions: We propose an improved method for DNA copy number estimation, mBPCR, which performed very well compared to previously published algorithms. In particular, mBPCR was more powerful in the detection of the true position of the breakpoints and of small aberrations in very noisy data. Hence, from a biological point of view, our method can be very useful, for example, to find targets of genomic aberrations in clinical cancer samples.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1356</field>
<field name="author">Marcus Hutter</field>
<field name="title">Temporal Difference Updating without a Learning Rate</field>
<field name="keyword">reinforcement learning</field>
<field name="keyword"> temporal difference</field>
<field name="keyword"> eligibility trace</field>
<field name="keyword"> variational principle</field>
<field name="keyword"> learning rate</field>
<field name="abstract">We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1357</field>
<field name="author">Daniil Ryabko</field>
<field name="author">Marcus Hutter</field>
<field name="title">On the Possibility of Learning in Reactive Environments with Arbitrary Dependence</field>
<field name="keyword">Reinforcement learning</field>
<field name="keyword"> asymptotic average value</field>
<field name="keyword"> self-optimizing policies</field>
<field name="keyword"> (non) Markov decision processes</field>
<field name="abstract">We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1358</field>
<field name="author">Shane Legg</field>
<field name="author">Marcus Hutter</field>
<field name="title">Tests of Machine Intelligence</field>
<field name="keyword">Turing test and derivatives; Compression tests; Linguistic complexity; Multiple cognitive abilities; Competitive games; Psychometric tests; Smith's test; C</field>
<field name="keyword">test; Universal intelligence</field>
<field name="abstract">Although the definition and measurement of intelligence is clearly of fundamental importance to the field of artificial intelligence, no general survey of definitions and tests of machine intelligence exists. Indeed few researchers are even aware of alternatives to the Turing test and its many derivatives. In this paper we fill this gap by providing a short survey of the many tests of machine intelligence that have been proposed.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1359</field>
<field name="author">Shane Legg</field>
<field name="author">Marcus Hutter</field>
<field name="title">Universal Intelligence: A Definition of Machine intelligence</field>
<field name="keyword">AIXI</field>
<field name="keyword"> complexity theory</field>
<field name="keyword"> intelligence</field>
<field name="keyword"> theoretical foundations</field>
<field name="keyword"> Turing test</field>
<field name="keyword"> intelligence tests/measures/definitions</field>
<field name="abstract">A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1360</field>
<field name="author">Marcus Hutter</field>
<field name="title">Exact Bayesian Regression of Piecewise Constant Functions</field>
<field name="abstract">We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary locations, and levels. The derivation works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions are discussed.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1361</field>
<field name="author">Marcus Hutter</field>
<field name="author">Rocco A. Servedio</field>
<field name="author">Eiji Takimoto</field>
<field name="title">Algorithmic Learning Theory</field>
<field name="keyword">algorithmic learning theory</field>
<field name="keyword"> query models</field>
<field name="keyword"> online learning</field>
<field name="keyword"> inductive inference</field>
<field name="keyword"> boosting</field>
<field name="keyword"> kernel methods</field>
<field name="keyword"> complexity and learning</field>
<field name="keyword"> reinforcement learning</field>
<field name="keyword"> unsupervised learning</field>
<field name="keyword"> grammatical inference</field>
<field name="keyword"> algorithmic forecasting</field>
<field name="abstract">The LNAI series reports state-of-the-art results in artificial intelligence research, development, and education. This volume (LNAI 4754) contains research papers presented at the 18th International Conference on Algorithmic Learning Theory (ALT 2007), which was held in Sendai (Japan) during October 1-4, 2007. The main objective of the conference was to provide an interdisciplinary forum for high-quality talks with a strong theoretical background and scientific interchange in areas such as query models, online learning, inductive inference, boosting, kernel methods, complexity and learning, reinforcement learning, unsupervised learning, grammatical inference, and algorithmic forecasting. The conference was co-located with the 10th International Conference on Discovery Science (DS 2007). The volume includes 25 technical contributions that were selected from 50 submissions, and five invited talks presented to the audience of ALT and DS. Longer versions of the DS invited papers are available in the proceedings of DS 2007.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1362</field>
<field name="author">Marcus Hutter</field>
<field name="author">Rocco A. Servedio</field>
<field name="author">Eiji Takimoto</field>
<field name="title">Algorithmic Learning Theory 2007: Editors' Introduction</field>
<field name="abstract">Learning theory is an active research area that incorporates ideas, problems, and techniques from a wide range of disciplines including statistics, artificial intelligence, information theory, pattern recognition, and theoretical computer science. The research reported at the 18th International Conference on Algorithmic Learning Theory (ALT 2007) ranges over areas such as unsupervised learning, inductive inference, complexity and learning, boosting and reinforcement learning, query learning models, grammatical inference, online learning and defensive forecasting, and kernel methods. In this introduction we give an overview of the five invited talks and the regular contributions of ALT 2007.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1363</field>
<field name="author">Marcus Hutter</field>
<field name="title">On Universal Prediction and Bayesian Confirmation</field>
<field name="keyword">Sequence prediction</field>
<field name="keyword"> Bayes</field>
<field name="keyword"> Solomonoff prior</field>
<field name="keyword"> Kolmogorov complexity</field>
<field name="keyword"> Occam's razor</field>
<field name="keyword"> prediction bounds</field>
<field name="keyword"> model classes</field>
<field name="keyword"> philosophical issues</field>
<field name="keyword"> symmetry principle</field>
<field name="keyword"> confirmation theory</field>
<field name="keyword"> reparametrization invariance</field>
<field name="keyword"> old-evidence/updating problem</field>
<field name="keyword"> (non)computab</field>
<field name="abstract">The Bayesian framework is a well-studied and successful framework for inductive reasoning, which includes hypothesis testing and confirmation, parameter estimation, sequence prediction, classification, and regression. But standard statistical guidelines for choosing the model class and prior are not always available or fail, in particular in complex situations. Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Strong total and weak instantaneous bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1364</field>
<field name="author">Marcus Hutter</field>
<field name="author">Andrej A. Muchnik</field>
<field name="title">On Semimeasures Predicting Martin-Loef Random Sequence</field>
<field name="keyword">Sequence prediction; Algorithmic Information Theory; universal enumerable semimeasure; mixture distributions; posterior convergence; Martin</field>
<field name="keyword">Loef randomness; quasimeasures</field>
<field name="abstract">Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1365</field>
<field name="author">Marcus Hutter</field>
<field name="author">Shane Legg</field>
<field name="author">Paul M.B. Vitanyi</field>
<field name="title">Algorithmic Probability</field>
<field name="abstract">Algorithmic 'Solomonoff' Probability (AP) assigns to objects an a priori probability that is in some sense universal. This prior distribution has theoretical applications in a number of areas, including inductive inference theory and the time complexity analysis of algorithms. Its main drawback is that it is not computable and thus can only be approximated in practice.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1366</field>
<field name="author">Alberto Piatti</field>
<field name="author">Marco Zaffalon</field>
<field name="author">Fabio Trojani</field>
<field name="author">Marcus Hutter</field>
<field name="title">Learning about a Categorical Latent Variable under Prior Near-Ignorance</field>
<field name="keyword">Prior near-ignorance</field>
<field name="keyword"> latent and manifest variables</field>
<field name="keyword"> observational processes</field>
<field name="keyword"> vacuous beliefs</field>
<field name="keyword"> imprecise probabilities.</field>
<field name="abstract">It is well known that complete prior ignorance is not compatible with learning, at least in a coherent theory of (epistemic) uncertainty. What is less widely known, is that there is a state similar to full ignorance, that Walley calls near-ignorance, that permits learning to take place. In this paper we provide new and substantial evidence that also near-ignorance cannot be really regarded as a way out of the problem of starting statistical inference in conditions of very weak beliefs. The key to this result is focusing on a setting characterized by a variable of interest that is latent. We argue that such a setting is by far the most common case in practice, and we show, for the case of categorical latent variables (and general manifest variables) that there is a sufficient condition that, if satisfied, prevents learning to take place under prior near-ignorance. This condition is shown to be easily satisfied in the most common statistical problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1367</field>
<field name="author">Marcus Hutter</field>
<field name="title">Bayesian Regression of Piecewise Constant Functions</field>
<field name="keyword">Bayesian regression</field>
<field name="keyword"> exact polynomial algorithm</field>
<field name="keyword"> non-parametric inference</field>
<field name="keyword"> piecewise constant function</field>
<field name="keyword"> dynamic programming</field>
<field name="keyword"> change point problem</field>
<field name="abstract">We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We briefly mention the performance on synthetic and real-world examples. The full version of the paper contains detailed derivations, more motivation and discussion, the complete algorithm, the experiments, and various extensions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1368</field>
<field name="author">Daniil Ryabko</field>
<field name="author">Marcus Hutter</field>
<field name="title">On Sequence Prediction for Arbitrary Measures</field>
<field name="keyword">sequence prediction</field>
<field name="keyword"> local absolute continuity</field>
<field name="keyword"> non-stationary measures</field>
<field name="keyword"> average/expected criteria</field>
<field name="keyword"> absolute/KL divergence</field>
<field name="keyword"> mixtures of measures</field>
<field name="abstract">Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences. Consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense), if one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and generalize several different notions that are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1369</field>
<field name="author">Nicholas Fitzroy-Dale</field>
<field name="title">Benefits of Compiler Optimisation</field>
<field name="keyword"/>
<field name="abstract">Todd Proebsting s 1998 pronouncement that compiler optimisation re- search only results in marginal performance improvements was accom- panied by the gloomy suggestion that perhaps researchers should spend their time in areas more likely to yield large performance gains, such as research into programmer productivity. This paper revisits Proebsting s claims, and finds them to be optimistic.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1370</field>
<field name="author">Shane Legg</field>
<field name="author">Marcus Hutter</field>
<field name="title">A Collection of Definitions of Intelligence</field>
<field name="abstract">This chapter is a survey of a large number of informal definitions of 'intelligence' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1371</field>
<field name="author">Marcus Hutter</field>
<field name="title">The Loss Rank Principle for Model Selection</field>
<field name="keyword">Model selection</field>
<field name="keyword"> loss rank principle</field>
<field name="keyword"> non-parametric regression</field>
<field name="keyword"> classification general loss function</field>
<field name="keyword"> k nearest neighbors</field>
<field name="abstract">A key issue in statistics and machine learning is to automatically select the 'right' model complexity, e.g. the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle (LoRP) for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP only depends on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1372</field>
<field name="author">Marcus Hutter</field>
<field name="title">Algorithmic Information Theory: a brief non-technical guide to the field</field>
<field name="keyword">Algorithmic information theory</field>
<field name="keyword"> algorithmic 'Kolmogorov' complexity</field>
<field name="keyword"> algorithmic 'Solomonoff' probability</field>
<field name="keyword"> universal 'Levin' search</field>
<field name="keyword"> algorithmic 'Martin-Loef' randomness</field>
<field name="keyword"> applications</field>
<field name="keyword"> history</field>
<field name="keyword"> references</field>
<field name="keyword"> notation</field>
<field name="keyword"> nomenclature</field>
<field name="keyword"> map</field>
<field name="abstract">This article is a brief guide to the field of algorithmic information theory (AIT), its underlying philosophy, and the most important concepts. AIT arises by mixing information theory and computation theory to obtain an objective and absolute notion of information in an individual object, and in so doing gives rise to an objective and robust notion of randomness of individual objects. This is in contrast to classical information theory that is based on random variables and communication, and has no bearing on information and randomness of individual objects. After a brief overview, the major subfields, applications, history, and a map of the field are presented.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1373</field>
<field name="author">Alexey Chernov</field>
<field name="title">Algorithmic Complexity Bounds on Future Prediction Errors</field>
<field name="keyword">Kolmogorov complexity</field>
<field name="keyword"> posterior bounds</field>
<field name="keyword"> online sequential prediction</field>
<field name="keyword"> Solomonoff prior</field>
<field name="keyword"> monotone conditional complexity</field>
<field name="keyword"> total error</field>
<field name="keyword"> future loss</field>
<field name="keyword"> randomness deficiency</field>
<field name="abstract">We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor $M$ from the true distribution $mu$ by the algorithmic complexity of $mu$. Here we assume we are at a time $t&gt;1$ and already observed $x=x_1...x_t$. We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the randomness deficiency of $x$. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1374</field>
<field name="author">Marcus Hutter</field>
<field name="title">Universal Algorithmic Intelligence: A Mathematical Top-&gt;Down Approach</field>
<field name="keyword">Artificial intelligence</field>
<field name="keyword"> algorithmic probability</field>
<field name="keyword"> sequential decision theory</field>
<field name="keyword"> rational agents</field>
<field name="keyword"> value function</field>
<field name="keyword"> Solomonoff induction</field>
<field name="keyword"> Kolmogorov complexity</field>
<field name="keyword"> reinforcement learning</field>
<field name="keyword"> universal sequence prediction</field>
<field name="keyword"> strategic games</field>
<field name="keyword"> function minimization</field>
<field name="keyword"> supe</field>
<field name="abstract">Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI$tl$ that is still effectively more intelligent than any other time $t$ and length $l$ bounded agent. The computation time of AIXI$tl$ is of the order $t . 2^l$. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1375</field>
<field name="author">Marcus Hutter</field>
<field name="title">On Generalized Computable Universal Priors and their Convergence</field>
<field name="keyword">Sequence prediction; Algorithmic Information Theory; Solomonoff's prior; universal probability; mixture distributions; posterior convergence; computability concepts; Martin</field>
<field name="keyword">Loef randomness</field>
<field name="abstract">Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of the universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. The first part of the paper investigates the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: recursive, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not estimable, and to dominate all enumerable semimeasures. We present proofs for discrete and continuous semimeasures. The second part investigates more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues. In particular, we show that convergence fails (holds) on generalized-random sequences in gappy (dense) Bernoulli classes.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1376</field>
<field name="author">Marcus Hutter</field>
<field name="author">Shane Legg</field>
<field name="title">Fitness Uniform Optimization</field>
<field name="keyword">Evolutionary algorithms</field>
<field name="keyword"> fitness uniform selection scheme</field>
<field name="keyword"> fitness uniform deletion scheme</field>
<field name="keyword"> preserve diversity</field>
<field name="keyword"> local optima</field>
<field name="keyword"> evolution</field>
<field name="keyword"> universal similarity relation</field>
<field name="keyword"> correlated recombination</field>
<field name="keyword"> fitness tree model</field>
<field name="keyword"> traveling salesman</field>
<field name="keyword"> set covering</field>
<field name="keyword"> satisfi</field>
<field name="abstract">In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other hand. Motivated by a universal similarity relation on the individuals, we propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure toward sparsely populated fitness regions, not necessarily toward higher fitness, as is the case for all other selection schemes. We show analytically on a simple example that the new selection scheme can be much more effective than standard selection schemes. We also propose a new deletion scheme which achieves a similar result via deletion and show how such a scheme preserves genetic diversity more effectively than standard approaches. We compare the performance of the new schemes to tournament selection and random deletion on an artificial deceptive problem and a range of NP-hard problems: traveling salesman, set covering and satisfiability.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1377</field>
<field name="author">Marcus Hutter</field>
<field name="title">General Discounting versus Average Reward</field>
<field name="keyword">reinforcement learning</field>
<field name="keyword"> average value</field>
<field name="keyword"> discounted value</field>
<field name="keyword"> arbitrary environment</field>
<field name="keyword"> arbitrary discount sequence</field>
<field name="keyword"> effective horizon</field>
<field name="keyword"> increasing farsightedness</field>
<field name="keyword"> consistent behavior</field>
<field name="abstract">Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m-&gt;infinity and V for k-&gt;infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1378</field>
<field name="author">Daniil Ryabko</field>
<field name="author">Marcus Hutter</field>
<field name="title">Asymptotic Learnability of Reinforcement Problems with Arbitrary Dependence</field>
<field name="keyword">Reinforcement learning</field>
<field name="keyword"> asymptotic average value</field>
<field name="keyword"> self-optimizing policies</field>
<field name="keyword"> (non) Markov decision processes</field>
<field name="abstract">We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions, i.e. environments more general than (PO)MDPs. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1379</field>
<field name="author">Tania Xiao</field>
<field name="author">Penelope Sanderson</field>
<field name="author">Samantha Clayton</field>
<field name="author">Svetha Venkatesh</field>
<field name="title">Coordinating multiple artifacts to support articulation work in healthcare</field>
<field name="keyword">Computer-supported cooperative work</field>
<field name="keyword"> health informatics</field>
<field name="keyword"> requirements analysis</field>
<field name="keyword"> articulation work</field>
<field name="keyword"> cognitive artifacts</field>
<field name="abstract">We report a field study of ICU nurses in charge as they perform articulation work associated with staff allocations. We describe the various collaborative artifacts they use and identify three main factors that hindered their ability to work effectively. First, there was little integration of information offered by the artifacts and the nurse contemplated the relationships among the information without any cognitive aid. Second, some of the artifacts were not updated effectively, resulting in decisions being made based on unreliable information. Third, some important information was lost due to limitations of the artifact. We discuss the implications of these issues for the design of CSCW tools.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1380</field>
<field name="author">renato iannella</field>
<field name="title">Social Web Profiles</field>
<field name="keyword">Social Networks</field>
<field name="keyword"> Interoperability</field>
<field name="keyword"> Profiles</field>
<field name="keyword"> Privacy</field>
<field name="abstract">In the digital world many Web users manage different social and professional network accounts and utilise them in different ways depending on the digital context. For example, more friendly chat on FaceBook, more professional discussion on LinkedIn, and a bit more daring interactions on Hi5. Maintaining these multitude of online profiles is cumbersome and time consuming for the typical web user. This is also an impediment for new social networks to attract new members simply because of the effort involved in creating and maintaining "yet-another-profile" and reestablishing different aspects of your profile under a new context. The scalability and non-interoperability of social networks seriously fragments the global digital world.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1381</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Edwin Hancock</field>
<field name="title">Shape and Refractive Index Recovery from Single-View Polarisation Images</field>
<field name="keyword">polarization</field>
<field name="keyword"> shape recovery</field>
<field name="keyword"> refractive index</field>
<field name="keyword"> multispectral imagery</field>
<field name="abstract">In this paper, we propose an approach to the problem of simultaneous shape and refractive index recovery from multispectral polarisation imagery captured from a single viewpoint. The focus of this paper is on dielectric surfaces which diffusely polarise light transmitted from the dielectric body into the air. The diffuse polarisation of the reflection process is modelled using a Transmitted

Radiance Sinusoid curve and the Fresnel transmission theory. We provide a method of estimating the azimuth angle of surface normals from the spectral variation of the phase of polarisation. Moreover, to render the problem of simultaneous estimation of surface orientation and index of refraction well-posed, we enforce a generative model on the material dispersion equations for the index of refraction. This generative model, together with the Fresnel transmission ratio, permit the recovery of the index of refraction and the zenith angle simultaneously. We show results on shape recovery and rendering for real world and synthetic imagery.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1382</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Solution of the Dichromatic Model for Multispectral Photometric Invariance</field>
<field name="keyword">photometric invariance</field>
<field name="keyword"> multispectral imaging</field>
<field name="keyword"> dichromatic reflection model</field>
<field name="keyword"> reflectance</field>
<field name="abstract">In this paper, we address the problem of photometric invariance in multispectral imaging

making use of an optimisation approach based upon the dichromatic model. In this manner,

we cast the problem of recovering the spectra of the illuminant, the surface reflectance and

the shading and specular factors in a structural optimisation setting. Making use of the additional

information provided by multispectral imaging and the structure of image patches, we

recover the dichromatic parameters of the scene. To do this, we formulate a target cost function

combining the dichromatic error and the smoothness priors for the surfaces under study.

The dichromatic parameters are recovered through minimising this cost function in a coordinate

descent manner. The algorithm is quite general in nature, admitting the enforcement

of smoothness constraints and extending in a straightforward manner to trichromatic settings.

Moreover, the objective function is convex with respect to the subset of variables to be optimised

in each alternating step of the minimisation strategy. This gives rise to an optimal

closed-form solution for each of the iterations in our algorithm. We illustrate the effectiveness

of our method for purposes of illuminant spectrum recovery, skin recognition, material

clustering and specularity removal. We also compare our results to a number of alternatives.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1383</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Justin Zobel</field>
<field name="author">Stefan Pohl</field>
<field name="author">Falk Scholer</field>
<field name="title">The Challenge of High Recall in Biomedical Systematic Search</field>
<field name="keyword">Systematic Review</field>
<field name="keyword"> Boolean Retrieval</field>
<field name="keyword"> Ranked Retrieval</field>
<field name="abstract">Clinical systematic reviews are based on expert, laborious search of well-annotated literature. Boolean search on bibliographic databases, such as MEDLINE, continues to be the preferred discovery method, but the size of these databases, now approaching 20 million records, makes it impossible to fully trust these searching methods. We are investigating the trade-offs between Boolean and ranked retrieval. Our findings show that although Boolean search has limitations, it is not obvious that ranking is superior, and illustrate that a single query cannot be used to resolve an information need. Our experiments show that a combination of less complicated Boolean queries and ranked retrieval outperforms either of them individually, leading to possible time savings over the current process.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1384</field>
<field name="author">Timothy Armstrong</field>
<field name="author">Alistair Moffat</field>
<field name="author">William Webber</field>
<field name="author">Justin Zobel</field>
<field name="title">Has Adhoc Retrieval Improved Since 1994?</field>
<field name="abstract">Evaluation forums such as TREC allow systematic measurement and comparison of information retrieval techniques. The goal is consistent improvement, based on reliable comparison of the effectiveness of different approaches and systems. In this paper we report experiments to determine whether this goal has been achieved. We ran five publicly available search systems, in a total of seventeen different configurations, against nine TREC adhoc-style collections, spanning 1994 to 2005. These runsets were then used as a benchmark for reassessing the relative effectiveness of the original TREC runs for those collections. Surprisingly, there appears to have been no overall improvement in effectiveness for either median or top-end TREC submissions, even after allowing for several possible confounds. We therefore question whether the effectiveness of adhoc information retrieval has improved over the past decade and a half.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1385</field>
<field name="author">Yuye Zhang</field>
<field name="author">Alistair Moffat</field>
<field name="author">Laurence Park</field>
<field name="title">Click-Based Evidence for Decaying Weight Distributions in Search Effectiveness Metrics</field>
<field name="keyword">Effectiveness metric </field>
<field name="keyword"> Query log </field>
<field name="keyword"> Clickthrough </field>
<field name="keyword"> Rank</field>
<field name="keyword">biased precision </field>
<field name="keyword"> Average precision </field>
<field name="keyword"> Reciprocal rank </field>
<field name="keyword"> BPref</field>
<field name="abstract">Search effectiveness metrics are used to evaluate the quality of the answer lists returned by search services, usually based on a set of relevance judgments. One plausible way of calculating an effectiveness score for a system run is to compute the inner-product of the run s relevance vector and a utility vector, where the ith element in the utility vector represents the relative benefit obtained by the user of the system if they encounter a relevant document at depth i in the ranking. This paper uses such a framework to examine the user behavior patterns and hence utility weightings that can be inferred from a web query log. We describe a process for extrapolating user observations from query log clickthroughs, and employ this user model to measure the quality of effectiveness weighting distributions. Our results show that for measures with static distributions (that is, utility weighting schemes for which the weight vector is independent of the relevance vector), the geometric weighting model employed in the rank-biased precision effectiveness metric offers the closest fit to the user observation model. In addition, using past TREC data as to indicate likelihood of relevance, we also show that the distributions employed in the BPref and MRR metrics are the best fit out of the measures for which static distributions do not exist.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1386</field>
<field name="author">Bruce Moulton</field>
<field name="author">Leif Hanlen</field>
<field name="author">June Chen</field>
<field name="author">Graham Croucher</field>
<field name="author">Lukshi Mahendran</field>
<field name="author">Andrew Varis</field>
<field name="title">Body-Area-Network transmission power control using variable adaptive feedback periodicity</field>
<field name="abstract">We propose a class of adaptive power control protocol, where the period between each feedback transmission is adaptively varied to accommodate run-time variation in the quality of each channel. Initial analyses suggest that transmission control protocols with adaptive feedback periodicity can outperform other comparable schemes. For certain measured channels the period can increase to once every few minutes (thousands of packets) and still provides substantial power savings. Adaptive power control protocols also provide the potential to reduce intra-cell interference.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1387</field>
<field name="author">Suyu Kong</field>
<field name="author">M. K Bhuyan</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian C. Lovell</field>
<field name="title">Tracking of Persons for Video Surveillance of Unattended Environments</field>
<field name="keyword">visual surveillance system</field>
<field name="keyword"> efficiently tracking</field>
<field name="keyword"> blob matching with particle filtering</field>
<field name="abstract">This paper describes a visual surveillance system for remote monitoring of unattended environments. For the purpose of efficiently tracking multiple people in the presence of occlusions, we propose: (i) to combine blob matching with particle filtering, and (ii) to augment these tracking algorithms with a novel colour appearance model. The proposed system efficiently counteracts the shortcomings of the two algorithms by switching from one to the other during occlusions. Results on public datasets as well as real surveillance videos from a metropolitan railway station demonstrate the efficacy of the proposed system.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1388</field>
<field name="author">Steffan Thiel</field>
<field name="author">Ali Babar</field>
<field name="author">Goetz Botterweck</field>
<field name="author">Liam O'Brien</field>
<field name="title">Software Product Lines in Automotive Systems Engineering</field>
<field name="abstract">Product line approaches are well-known in many

manufacturing industries, such as consumer electronics,

medical systems and automotive [1]. In recent years,

approaches with a similar background have rapidly

emerged within Software Engineering, so called Software

Product Line (SPL) approaches [2], [3].

As automotive manufacturers and suppliers design and

implement complex applications, such as driver

assistance [4], they strive for mechanisms that allow

them to implement such functionality on integrated

platforms. This offers the opportunity to build a variety of

similar systems with a minimum of technical diversity and

thus allows for strategic reuse of components. This has

resulted in a growing interest in SPL approaches both in

the software engineering and the automotive systems

domain.

This paper discusses the increasing importance that SPL

approaches could play within the context of Automotive

Systems Engineering. To accomplish this, we first

provide an overview of the major challenges faced by

Automotive Systems Engineering [5]. We then present a

selection of SPL approaches, which could provide

solutions for the described challenges. To complement

this we make the case for empirical evaluation as a basis

for well-founded decisions and selection of techniques.

Finally, we present and in-depth discussion of how the

approaches and techniques outlined can be used to

address the identified challenges. The paper concludes

with an overview of open research questions and

expected benefits for the development of automotive

systems.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1389</field>
<field name="author">Werayut Saesue</field>
<field name="author">Chun Tung Chou</field>
<field name="author">Jian Zhang</field>
<field name="title">Video Quality Prediction in the Presence of MAC Contention and Wireless Channel Error</field>
<field name="keyword">Video quality</field>
<field name="keyword"> IEEE 802.11e</field>
<field name="keyword"> analytical performance models</field>
<field name="keyword"> Markov chain</field>
<field name="keyword"> H.264 videos</field>
<field name="abstract">This paper proposes an analytical model to predict the quality of video, expressed in terms of mean square error of the received video frames, in an IEEE 802.11e wireless network. The proposed model takes into account contention at the MAC layer, wireless channel error, queueing at the MAC layer, parameters of different 802.11e access categories (AC), and video characteristics of different H.264 data partitions. To the best of the authors' knowledge, this is the first model that takes these network and video characteristics into consideration to predict video quality in an IEEE 802.11e network. The proposed model consists of two components. The first component predicts the packet loss rate of each H.264 data partition (DP) by using a multi-dimensional discrete-time Markov chain (DTMC) coupled to a M/G/1 queue. The second component uses these packet loss rates and the video characteristics to predict the mean square error of each received video frames. We verify the accuracy of our analytical model by using discrete event simulation and real H.264 coded video sequences.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1390</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Himanshu Gupta</field>
<field name="author">Vinay Ribeiro</field>
<field name="title">A Longitudinal Study of Small Time Scaling Behavior of Internet Traffic</field>
<field name="abstract">We carry out a longitudinal study of evolution of small time- 

scaling behavior of Internet tra c on MAWI dataset spanning 8 years. 

MAWI dataset contains a number of anomalies which con ict in the 

correct identi cation of scaling behavior, and hence to mitigate these 

e ects, we use a sketch based procedure for robust estimation of Hurst 

exponent. We rst show the importance of robust estimation procedure 

while studying small time-scaling behavior of Internet tra c. We further 

study the evolution of following properties concerning the origins of small 

time-scaling behavior: (1) Scaling at IP level is independent of ow ar- 

rivals and (2) Dense ows are primary correlation-causing factor in small 

time-scales. Traditionally these properties have been shown to hold by 

using a semi-experiments based methodology. We next show that due to 

network anomalies, semi-experiments can result in misleading inferences. 

Hence we propose and motivate the use of robust semi-experiments i.e. 

a semi-experiment coupled with the use of a robust estimation procedure 

for inferring scaling behavior. By making use of robust semi-experiments 

we nd the above properties to be invariant across the entire MAWI 

dataset. Our other results consist in showing that dense ows are car- 

rying a larger fraction of aggregate tra c for recent traces and hence 

recent traces show larger short range correlations vis-a-vis earlier traces.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1391</field>
<field name="author">Youmna Borghol</field>
<field name="author">Sebastien Ardon</field>
<field name="author">Anirban Mahanti</field>
<field name="author">Niklas Carlsson</field>
<field name="title">Toward Efficient On-demand Streaming with BitTorrent</field>
<field name="keyword"/>
<field name="abstract">This paper considers the problem of adapting the BitTorrent

protocol for on-demand streaming.

BitTorrent is a popular peer-to-peer file sharing protocol

that efficiently accommodates a large number of requests

for file downloads. Two components of the protocol, namely

the Rarest-First piece selection policy and the Tit-for-Tat

algorithm for peer selection, are acknowledged to

contribute toward the protocol's efficiency with respect to

time to download files and its resilience to freeriders.

Rarest-First piece selection, however, does not augur well for

on-demand streaming. In this paper, we present a new

adaptive Window-based piece selection policy that

achieves a balance between the system scalability

provided by the Rarest-First algorithm and the necessity

of In-Order pieces for seamless media playback. We

also show that this simple modification to the piece selection

policy allows the system to be efficient with respect to

utilization of available upload capacity of participating

peers, and does not break the Tit-for-Tat incentive scheme

which provides resilience to freeriders.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1392</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">First and Second-Order Statistical Characterizations of the Dynamic Body-Area Propagation Channel of Various Bandwidths</field>
<field name="keyword">Body Area Networks</field>
<field name="keyword">channel modeling</field>
<field name="keyword">Akaike Information Criterion</field>
<field name="keyword">fading channels</field>
<field name="keyword">radio propagation</field>
<field name="keyword">wireless communication</field>
<field name="abstract">Comprehensive statistical characterizations of the dynamic

narrowband on-body area, and on-body to off-body area, channels are

presented. These characterizations are based on real-time

measurements of the time domain channel response at carrier

frequencies near the 900~MHz and 2400~MHz Industrial, Scientific and

Medical (ISM) bands, and at a carrier frequency near the 402~MHz

Medical Implant Communications (MICS) band. We consider varying

amounts of body movement, numerous transmit-receive pair (Tx-Rx)

locations on the human body, and various bandwidths. We also

consider long periods, i.e. hours of every-day activity

(predominantly indoor scenarios), for on-body channel

characterization. Various adult human test subjects are used. It is

shown, by applying the Akaike information criterion, that the

Weibull and Gamma distributions generally fit agglomerates of

received signal amplitude data, and that in various individual cases

the Lognormal distribution provides a good fit. We also characterize

fade duration and fade depth with direct matching to second-order

temporal statistics. These first and second-order characterizations

have important utility in the design and evaluation of body-area

communications systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1393</field>
<field name="author">Paul Bannerman</field>
<field name="title">Structuring Risk into Projects</field>
<field name="keyword">Risk management</field>
<field name="keyword"> organization structure</field>
<field name="keyword"> project governance</field>
<field name="keyword"> alignment</field>
<field name="abstract">Experience tells us that projects can and do fail for many reasons. However, it would be most unfortunate if the propensity to fail was structured into the design of a project before it even started. This paper extends recent research on structure-related risk which suggests that the organizational entities involved in projects, including the project itself, bring with them certain risks associated with their structural forms that can influence the performance and outcomes of projects. These risks can predispose a project to problems from the outset of its activities. The paper extends this research by considering how structure-related risks might arise and be managed in practice. Furthermore, since management of such risks tends to fall outside of the capabilities of the project and project manager, a new role for project governance is proposed in mediating and mitigating these risks. The propositions are illustrated with project examples and a case study. The arguments are presented within the application domain of software development projects but, conceptually, the principles can also be applied to other application domains such as engineering, construction and defense. The research is still in its infancy but early results point to a potentially significant source of risk that has previously been overlooked.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1394</field>
<field name="author">Mirco Gelain</field>
<field name="author">Maria Silvia Pini</field>
<field name="author">Francesca Rossi</field>
<field name="author">Brent Venable</field>
<field name="author">Toby Walsh</field>
<field name="title">Male optimality and uniqueness in stable matching problems with partial orders</field>
<field name="keyword">Stable marriage</field>
<field name="keyword"> preferences</field>
<field name="abstract">The stable marriage problem has a wide variety of practical

applications, including matching resident doctors to hospitals, and stu-

dents to schools. In the classical stable marriage problem, both men and

women express a strict order over the members of the other sex. Here we

consider a more realistic case, where both men and women can express

their preferences via partial orders, i.e., by allowing ties and incompa-

rability. This may be useful, for example, when preferences are elicited

via compact preference representations like soft constraint or CP-nets

that produce partial orders, as well as when preferences are obtained

via multi-criteria reasoning. We study male optimality and uniqueness

of stable marriages in this setting. Male optimality gives priority to one

gender over the other (for example, in matching residents to hospitals

in the US, priority is given to the residents). Uniqueness means that the

solution is optimal, since it is as good as possible for all the participat-

ing agents. Uniqueness of solution is also a barrier against manipulation.

We give an algorithm to &#12;nd stable marriages that are male optimal.

We also provide a su&#14;cient condition on the preferences that guarantees

that there is a male optimal stable marriage. Finally, we give another

su&#14;cient condition on the preferences (that is also necessary in some

special case), that occurs often in real-life scenarios, which guarantees

the uniqueness of a stable marriage.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1395</field>
<field name="author">Toby Walsh</field>
<field name="title">Manipulability of Single Transferable Vote</field>
<field name="keyword">Social choice</field>
<field name="keyword"> manipulation</field>
<field name="keyword"> computational complexity</field>
<field name="abstract">For many voting rules, it is NP-hard to compute a successful manipulation.

However, NP-hardness only bounds the worst-case complexity. Recent

theoretical results suggest that manipulation may often be easy in practice. We

study empirically the cost of manipulating the single transferable vote (STV) rule.

This was one of the first rules shown to be NP-hard to manipulate. It also appears

to be one of the harder rules to manipulate since it involves multiple rounds and

since, unlike many other rules, it is NP-hard for a single agent to manipulate without

weights on the votes or uncertainty about how the other agents have voted.

In almost every election in our experiments, it was easy to compute how a single

agent could manipulate the election or to prove that manipulation by a single

agent was impossible. It remains an interesting open question if manipulation by

a coalition of agents is hard to compute in practice.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1396</field>
<field name="author">Nathan Robinson</field>
<field name="author">Charles Gretton</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">Abdul Sattar</field>
<field name="title">SAT-Based Parallel Planning Using a Split Representation of Actions</field>
<field name="keyword"/>
<field name="abstract">Planning based on propositional SAT(isfiability) is a powerful approach to computing step-optimal plans given a parallel execution semantics. In this setting: (i) a solution plan must be minimal in the number of plan steps required, and (ii) non-conflicting actions can be executed instantaneously in parallel at

a plan step.



Underlying SAT-based approaches is the invocation of a decision procedure on a SAT encoding of a bounded version of the problem. A fundamental limitation of existing approaches is the size of these

encodings. This problem stems from the use of a direct representation of actions -- i.e. each action has a corresponding variable in the encoding.



A longtime goal in planning has been to mitigate this limitation by developing a more compact split -- also termed lifted -- representation of actions in SAT encodings of parallel step-optimal problems. This paper describes such a representation. In particular, each action and each parallel execution of actions is represented uniquely as a conjunct of variables. Here, each variable is derived from action pre and post-conditions. Because multiple actions share conditions, our encoding of the planning constraints is factored and relatively compact. We find experimentally that our encoding yields a much more efficient and scalable planning procedure over the state-of-the-art in a large set of planning benchmarks.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1397</field>
<field name="author">Ralf Huuck</field>
<field name="author">Gerwin Klein</field>
<field name="author">Schlich Bastian</field>
<field name="title">Proc. 4th International Workshop on System Software Verification (SSV09)</field>
<field name="abstract">This volume contains the proceedings of the 4th International Workshop on

Systems Software Verification (SSV) held in Aachen, Germany, June 22--24, 2009.



The purpose of SSV is to bring together researchers and developers from both academia and industry, who are facing real software and real problems to find real, applicable solutions. By "real'' we mean problems such as time-to-market or reliability that the industry is facing and is trying to fix in software that is deployed in the market place. A real solution is one that is applicable to the problem in industry and not one that only applies to an abstract, academic toy version of it. SSV discusses software analysis/development techniques and tools, and serves as a platform to discuss open problems and future challenges in dealing with existing and upcoming system code. 



Each paper has been examined by at least 3 reviewers and we would like to thank the members of the Programme Committee as well as the external reviewers for their detailed and thorough investigation of each contribution.



The workshop programme consisted of the presentation of the 10 full papers in this volume, the SSV'09 doctoral symposium, and the following three invited talks.



Kwangkeun Yi (Seoul National University, South Korea). On our Sparrow experience, an industrial-strength non-domain-specific static bug-finder for C



Thomas Santen (EMIC, Aachen, Germany). Verification of Real-World C Programs with VCC



Josh Berdine (Microsoft Research, Cambridge, UK). Automatic Verification of Heap Manipulation using Separation Logik





We would like to thank the the Programme Committee, the authors, and the workshop organizers for their contribution to the success of this 4th International Workshop on Systems Software Verification. Finally, we are grateful for the generous support we received from the European Microsoft Innovation Center (EMIC), Germany.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1398</field>
<field name="author">David Newman</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Lawrence Cavedon</field>
<field name="title">External Evaluation of Topic Models</field>
<field name="keyword">Topic Modeling</field>
<field name="keyword"> Evaluation</field>
<field name="keyword"> Document Similarity</field>
<field name="keyword"> Natural Language Processing</field>
<field name="keyword"> Information Retrieval</field>
<field name="abstract">Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.</field>
<field name="date">2009</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1399</field>
<field name="author">Yi Zhang</field>
<field name="author">Tim Baldwin</field>
<field name="author">Valia Kordoni</field>
<field name="author">David Martinez</field>
<field name="author">Jeremy Nicholson</field>
<field name="title">Chart Mining-based Lexical Acquisition with Precision Grammars</field>
<field name="abstract">In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars. The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition over low-density languages using low-coverage grammars. As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1400</field>
<field name="author">Keisuke Nakano</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Computing the Cost of Typechecking of Composition of Macro Tree Transducers</field>
<field name="abstract">Macro tree transducers are a classical formal model for structural-recursive tree transformation with accumulative

parameters. Recently they have been applied to model XML transformations and queries. Typechecking

a tree transformation means to check if all valid input trees are transformed into valid output trees,

for given regular tree languages of input and output trees. Typechecking macro tree transducers is generally

based on inverse type inference, because of the nice property that inverse transformations effectively preserve

regular tree languages. It is known that the time complexity of typechecking an n-fold composition

of macro tree transducers is non-elementary. The cost of typechecking can be reduced if transducers in the

composition have special properties, such as being deterministic or total, or having no accumulative parameters.

In this paper, the impact of such properties for the cost of typechecking is investigated. Improvements in

cost are achieved through application of composition and decomposition constructions for tree transducers.

Even though these constructions are well-known, they had not yet been analyzed with respect to the precise

sizes of the involved transducers. The results are directly applicable for typechecking XML transformations,

because type formalisms for XML are captured by the regular tree languages.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1401</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Ricky Robinson</field>
<field name="title">Modelling Weiser's "Sal" Scenario with CML</field>
<field name="abstract">Context modelling approaches differ in what concepts can be captured by application designers, in the expressive power of the context models, in the support they can provide for reasoning about context, and in the way they facilitate software engineering of context-aware applications. In this paper we respond to the challenge of CoMoRea'09 and show how the Context Modelling Language (CML) and the concepts associated with CML can be used to engineer applications that exhibit behaviour described in Mark Weiser's "Sal" scenario. The paper presents the CML based models for applications from the "Sal" scenario and evaluates the approach based on the criteria defined for the challenge including knowledge and software engineering effort, flexbility, usability, and support for uncertain information.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1402</field>
<field name="author">Priscilla Kan John</field>
<field name="author">Alban Grastien</field>
<field name="title">Synth se d'un diagnostiqueur distribu et pr cis</field>
<field name="keyword">Diagnosis</field>
<field name="keyword"> Discrete event system</field>
<field name="keyword"> Junction tree</field>
<field name="keyword"> Accuracy</field>
<field name="abstract">Diagnosis of large discrete event systems leads to combinatorial explosion. 

We propose to solve this issue by ignoring some connections on the system 

and using a junction tree-based approach. 

Removing connections reduces both the cost and the accuracy of the diagnosis. 

Therefore, we perform an off-line analysis 

to determine which connections can be safely ignored.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1403</field>
<field name="author">Aurelien Lemay</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Joachim Niehren</field>
<field name="title">A Learning Algorithm for Top-Down XML Transformations</field>
<field name="keyword">Tree transformation</field>
<field name="keyword"> top-down</field>
<field name="keyword"> transducer</field>
<field name="keyword"> learning algorithm</field>
<field name="keyword"> minimization</field>
<field name="keyword"> Myhill-Nerode equivalence</field>
<field name="abstract">A generalization from string to trees and

from languages to translations is given of the

classical result that any regular language can be learned

from examples: it is shown that

for any deterministic 

top-down tree transformation there exists a sample set of

polynomial size (with respect to the minimal transducer) which allows

to infer the translation.

Until now, only for string transducers and for simple relabeling

tree transducers, similar results had been known. 

Learning of deterministic top-down tree transducers (DTOPs)

is far more involved because

a DTOP can copy, delete, and permute its input subtrees.

Thus, complex dependencies of labeled input to output paths 

need to be maintained by the algorithm.

First, a Myhill-Nerode theorem is presented for DTOPs,

which is interesting on its own.

This theorem is then used to construct a learning algorithm for

DTOPs. Finally, it is shown how our result can be applied

to \XML transformations (e.g. XSLT programs). 

For this, a new DTD-based encoding of unranked trees by ranked ones is

presented. Over such encodings, DTOPs can realize

many practically interesting XML transformations

which cannot be realized on

first-child/next-sibling encodings.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1404</field>
<field name="author">Lixiang Xiong</field>
<field name="author">Lavy Libman</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Uncoordinated Cooperative Communications in Highly Dynamic Wireless Networks</field>
<field name="keyword">Cooperative communication</field>
<field name="keyword"> coordination</field>
<field name="keyword"> random networks</field>
<field name="abstract">Cooperative communication techniques offer significant performance benefits

over traditional methods that do not exploit the broadcast nature of wireless

transmissions. Such techniques generally require advance coordination among the

participating nodes to discover available neighbors and negotiate the

cooperation strategy. However, the associated discovery and negotiation

overheads may negate much of the cooperation benefit in mobile networks with

highly dynamic or unstable topologies (e.g. vehicular networks). This paper

discusses uncoordinated cooperation strategies, where each node

overhearing a packet decides independently whether to retransmit it,

without any coordination with the transmitter, intended receiver, or other

neighbors in the vicinity. We formulate and solve the problem of finding the

optimal uncoordinated retransmission probability at every location as a

function of only a priori statistical information about the local

environment, namely the node density and radio propagation model. We show that

the solution consists of an optimal cooperation region which we provide

a constructive method to compute explicitly. Our numerical evaluation

demonstrates that uncoordinated cooperation offers a low-overhead viable

alternative, especially in high-noise (or low-power) and high node density

scenarios.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1405</field>
<field name="author">Clinton Freeman</field>
<field name="title">User experience design during OpenSHAPA development</field>
<field name="keyword">Open-source</field>
<field name="keyword"> user experience design</field>
<field name="keyword"> control theory</field>
<field name="keyword"> openloop</field>
<field name="keyword"> closed-loop</field>
<field name="keyword"> exploratory sequential data analysis.</field>
<field name="abstract">During the development of an open-source exploratory sequential data analysis tool, broad control theory principles have guided user experience design. Closed-loop development practices guide the implementation discussion, which broadly comprises two parts. Firstly, there is a design discussion in which users propose and comment on user interface mock-ups. Secondly, there is a development discussion in which users actively participate in guiding development. The approach used in this case study effectively manages the dynamic nature of open-source software production and use.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1406</field>
<field name="author">Nathan Robinson</field>
<field name="author">Charles Gretton</field>
<field name="author">Duc Nghia Pham</field>
<field name="author">Abdul Sattar</field>
<field name="title">Cost-Optimal Planning using Weighted MaxSAT</field>
<field name="abstract">We consider the problem of computing optimal plans for propositional

planning problems with action costs. In the spirit of leveraging

advances in general-purpose automated reasoning for that setting, we

develop an approach that operates by solving a sequence of {\em

partial weighted MaxSAT} problems, each of which corresponds to

step-bounded variant of the problem at hand. Our approach is the

first SAT-based system in which a proof of cost optimality is obtained

using a MaxSAT procedure. It is also the first system of this kind to

incorporate an admissible planning heuristic. We perform a detailed

empirical evaluation of our work using benchmarks from a number of

International Planning Competitions.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1407</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">EMUNE: Architecture for Mobile Data Transfer Scheduling with Network Availability Predictions</field>
<field name="keyword">Network Interface Selection</field>
<field name="keyword"> Network Availability Prediction</field>
<field name="keyword"> Data Transfer Scheduling</field>
<field name="abstract">With the mobile communication market increasingly moving towards value-added services the network cost will need to be included in the service offering itself. This will lead service providers to optimize network usage based on real cost rather than the simplified network plans sold to consumers traditionally. Meanwhile, today's mobile devices are increasingly containing multiple radios, enabling users on the move to take advantage of the heterogeneous wireless network environment. In addition, we observe that many bandwidth intensive services such as video on demand and software updates are essentially non real-time and buffers in mobile devices are effectively unlimited. We therefore propose EMUNE, a new transfer service which leverages these aspects. It supports opportunistic bulk transfers in high bandwidth networks while adapting to device power concerns, application requirements and user preferences of cost and quality.



Our proposed architecture consists of an API, a transport service and two main functional units. The well defined API hides all internal complexities from a programmer and provides easy access to the functionalities. The prediction engine infers future network and bandwidth availability. The scheduling engine takes the output of the prediction engine as well as the power and monetary costs, application requirements and user preferences into account and determines which interface to use, when and for how long for all outstanding data transfer requests. The transport service accordingly executes the inferred data transfer schedule. The results from the implementation of EMUNE's and of the prediction and scheduling engines evaluated against real user data show the effectiveness of the proposed architecture for better utilization of multiple network interfaces in mobile devices.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1408</field>
<field name="author">Peter Chubb</field>
<field name="author">Yang Song</field>
<field name="title">Interrupts considered harmful</field>
<field name="abstract">Interrupts are a way for a piece of hardware to tell the operating system, `I want some attention, please'. Linux deals with them by stealing time from the currently running process and giving the stolen time to the device driver for the interrupting device.



While the interrupt handler is running, other interrupts are usually blocked. Device driver writers jump through hoops to ensure that interrupts are not held off too long --- and Linux provides a plethora of mechanisms to defer work after the device is told to stop interrupting. However, it is up to each individual device driver to `do the right thing' --- and not all do. In addition, there can be legitimate reasons for `interrupt storms' --- for example, a gigabit network adapter on a busy network --- that cause much time to be stolen from other processes.



This architecture has a number of problems. The main one is that other processes (both real time and interactive, but of course the problem is more severe for real-time processes) become sluggish.



One way used by other operating systems (and also in Linux with Ingo Molnar's PREEMPT:RT patch) is to make interrupt handlers first class threads, so that the time spent in them can be controlled relative to other processes' requirements. However, Ingo's patch does not go far enough --- the mechanisms for deferred work and for interrupt mitigation are still used by the drivers, resulting in poor performance, and, under heavy load, real time processes still miss their deadlines.



The User Level driver work I did (and reported in a previous LCA) suggests that restructuring drivers with a model where interrupts are just one event among many that that a driver copes with, can eliminate most of complexity of deferred work and interrupt processing, and can provide performance at least as good as the current Linux implementation --- and better than the PREEMPT:RT approach.



Currently, I have some preliminary results showing real-time processes missing their deadlines with PREEMPT:RT under heavy interrupt load; by the time of the conference I expect to have an GigE driver with the new model, and some hard benchmark results to prove my point.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1409</field>
<field name="author">Peter Chubb</field>
<field name="author">John Judge</field>
<field name="title">When the Arduino meets year 9</field>
<field name="abstract">During the last two years, NICTA, in conjunction with the University of Sydney, has extended the National Computer Science School to include an embedded systems component.



During the first trial at the end of 2008, we gave each participant an arduino, a breadboard and a handful of components. On-line exercises showed how to wire up a circuit each week, test it, and then gave programming exercises for that circuit. The aim was more to teach C and embedded programming, than to play with hardware; some of the students complained about the problems in wiring up the systems.



For this year, we designed and manufactured a board specifically for this course, based on the Arduino and using the same IDE. The board included a variety of peripherals (LEDs, potentiometers, temperature and inertial sensors, etc), and had a plug to allow it to interface directly with an AIBO.



In this talk, I'll run through what the students did with the course, and show some videos of what they did in the Challenge when they plugged the boards into the Aibo robots.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1410</field>
<field name="author">di yang</field>
<field name="author">Hongdong Li</field>
<field name="title">Learning Varying Dimension Radial Basis Functions</field>
<field name="abstract">This paper presents a method for learning Radial Basis

Functions (RBF) model with variable dimensions for aligning/

registrating images of deformable surface. Traditional

RBF-based approach, which is mainly based on a fixed dimension

parametric model, often suffers from severe parameter

over-fitting and complicated model selection (i.e.

select the number and locations of centers determination)

problems which lead to inaccurate estimation and unreliable

convergence. Our strategy for solving both the parameter

over-fitting and model selection problems is through the

use of a probabilistic Bayesian inference model to obtain a

posterior estimation of the alignment as well as the model

parameters simultaneously. To learn the parameters of the

Bayesian model, a reversible jump Markov Chain Monte

Carlo (RJMCMC) algorithm is employed, allowing us to

handle large deformation image registration. Our approach

is demonstrated successfully on real image sequences of different

deformation types, with results compared favorable

against other existing approaches.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1411</field>
<field name="author">renato iannella</field>
<field name="author">Harry Halpin</field>
<field name="author">Brian Suda</field>
<field name="author">Norman Walsh</field>
<field name="title">Representing vCard Objects in RDF</field>
<field name="keyword">vCard</field>
<field name="keyword"> RDF</field>
<field name="abstract">This submission specifies a Resource Description Framework (RDF) encoding of the vCard profile defined by RFC 2426 and to provide equivalent functionality to its standard format. The motivation is to enable the common and consistent description of people and organisations (using the existing semantics of vCard) and to encode these in RDF formats</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1412</field>
<field name="author">Haifeng Zhao</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="author">Jianfeng Lu</field>
<field name="author">Jing-yu Yang</field>
<field name="title">Graph Embedding via Riemannian Submersion Learning</field>
<field name="abstract">In this paper, we tackle the problem of embedding a set of relational structures into a metric space for purposes of matching and categorisation. To this end, we view the problem from a Riemannian perspective and make use of the concepts of charts on the manifold to define the embedding as a mixture of class-specific submersions. Formulated in this manner, the mixture weights are recovered using a probability density estimation on the embedded graph node coordinates. Further, we recover these class-specific submersions making use of an iterative trust-region method so as to minimise the L2 norm between the hard limit of the graph-vertex posterior probabilities and their estimated values. The method presented here is quite general in nature and allows tasks such as matching, categorisation and retrieval. We show results on graph matching, shape categorisation and digit classification on synthetic data, the MPEG-7 database and the MNIST dataset.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1413</field>
<field name="author">Jeffrey Zhang</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="title">On the Effective Energy Consumption in Wireless Sensor Networks</field>
<field name="abstract">We analytically characterize the energy consumption per successfully transmitted packet in end-to-end packet transmissions in a 

identically and independently distributed in a square area following a homogeneous Poisson process. It is assumed that a greedy forwarding protocol is used for routing. We obtain analytical results on the average number of hops between any two nodes if the transmission is successful and the average number of hops traversed by packets before being dropped if the transmission is unsuccessful. A transmission is unsuccessful if the packet sent from a source to a destination has to be dropped at an intermediate node because the greedy forwarding routing protocol is unable to find a next-hop node that is closer to the destination. Based on the above analysis, we derive the effective energy consumption per successfully transmitted packet. We show that there exists an optimum transmission range which minimizes the effective energy consumption and such optimum transmission range can be computed based on our analysis.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1414</field>
<field name="author">SehChun Ng</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Yang Yang</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Analysis of Access and Connectivity Probabilities in Infrastructure-Based Vehicular Relay Networks</field>
<field name="abstract">Coverage is an important problem in wireless networks.

Together with the access probability, which measures

how well an arbitrary user can access a wireless network, in

particular VANET, they are often used as major indicators

of the quality of the network. In this paper, we investigate

the coverage and access probability of the vehicular networks

with roadside infrastructure, i.e. base stations. Specifically, we

analyze the relation between these key parameters, i.e. the

coverage range of base stations, coverage range of vehicles,

vehicle density and distance between adjacent base stations, and

how these parameters interact with each other to collectively

determine the coverage and the access probability. We use the

connectivity probability, the probability that all nodes in the

network are connected to at least one base station within a

designated number of hops, as a measure of the coverage. We

derived close-form formulas for the connectivity probability and

the access probability for a 1D vehicular network bounded by

two adjacent base stations. The analytical results have been

validated by simulations. The results in the paper can be used

by network operators to design networks with specific service

coverage guarantees.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1415</field>
<field name="author">Mahmudul Hasan</field>
<field name="author">Mark Pickering</field>
<field name="author">Xiuping Jia</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="title">MULTI-SPECTRAL REMOTE SENSING IMAGE REGISTRATION VIA SPATIAL RELATIONSHIP ANALYSIS ON SIFT KEYPOINTS</field>
<field name="abstract">Multi-sensor image registration is a challenging task in

remote sensing. Considering the fact that multi-sensor

devices capture the images at wide range of frequencies and

at different time, multi-spectral image registration is

necessary for data fusion of the images. Several

conventional methods for image registration suffer from

poor performance due to their sensitivity to scale and

intensity variation. The scale invariant feature transform

(SIFT) is widely used for image registration and object

recognition to address these problems. However, directly

applying SIFT to remote sensing image registration often

results in a very large number of feature points but small

number of matching with a high false alarm rate. This is due

to the fact that spatial information is not considered during

the SIFT-based matching process. This paper proposes a

method to improve SIFT-based matching by taking

advantage of neighborhood information. The proposed

method generates more correct matching points as the

relative structure in different remote sensing images are

almost static.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1416</field>
<field name="author">SehChun Ng</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Yang Yang</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Analysis of Access and Connectivity Probabilities in Vehicular Relay Networks</field>
<field name="abstract">IEEE 802.11p and 1609 standards are currently under development to support Wireless Access in Vehicular

Environment (WAVE) and to deliver both safety and non-safety related applications and services to vehicles on

the road. For infrastructure-based vehicular relay networks, where both one-hop (direct access) and two-hop (via

a relay) communications between a vehicle and the infrastructure, i.e. a Base Station (BS), are supported, access

probability is an important measure of user satisfaction which indicates how well an arbitrary user/vehicle can access

the infrastructure via a one-hop or two-hop wireless path. From the system perspective, connectivity probability,

i.e. the probability that all the vehicles are connected to the infrastructure within two hops, indicates the service

coverage performance of a vehicular relay network. To improve user satisfaction and service coverage, we develop

in this paper an analytical model with a generic radio channel model to fully characterize the access probability

and connectivity probability performance in a vehicular relay network. Specifically, we derive close-form equations

for calculating these two probabilities, as well as an approximate equation of connectivity probability which is less

accurate but gives better intuitive understanding of the interactions among these performance impacting factors

than its exact counterpart. Our analytical results, validated by simulations, reveal the tradeoffs between key system

parameters, such as inter-BS distance (or BS density), vehicle density, transmission ranges of a BS and a vehicle, and

their collective impact on user access probability and service connectivity probability performance under different communication channel models. These results and new knowledge about vehicular relay networks will enable

network designers and operators to effectively improve network planning, deployment and resource management,

thus achieving high service coverage and quality.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1417</field>
<field name="author">Mahmudul Hasan</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Jun Zhou</field>
<field name="author">Xiuping Jia</field>
<field name="author">Mark Pickering</field>
<field name="title">REGISRATION OF HYPERSPECTRAL AND TRICHROMATIC IMAGES VIA CROSS CUMULATIVE RESIDUAL ENTROPY MINIMISATION</field>
<field name="abstract">In this paper we address the problem of image

registration between imagery acquired by trichromatic

sensors and hyperspectral imagers. We do this by presenting

a method aimed at registering a high-resolution trichromatic

image with lower resolution hyperspectral data. The method

presented here maps the hyperspectral image to a luminance

image so as to employ the cross cumulative residual entropy

for purposes of multimodal registration. As a result, we can

cast the problem in a minimisation setting in which

correspondences between an image pair can be recovered

using an iterative trust region. We illustrate the utility of our

approach by presenting registration results on a set of

surveillance image pairs comprised by high-oblique colour

and hyperspectral imagery.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1418</field>
<field name="author">Yang Yang</field>
<field name="author">Guoqiang Mao</field>
<field name="author">Brian Anderson</field>
<field name="author">Guoqiang Mao</field>
<field name="title">Relay Technologies for WiMAX and LTE-Advanced Mobile Systems</field>
<field name="abstract">Relay technologies have been actively studied

and considered in the standardization process of

next-generation mobile broadband communication

systems such as 3GPP LTE-Advanced,

IEEE 802.16j, and IEEE 802.16m. This article

first introduces and compares different relay

types in LTE-Advanced and WiMAX standards.

Simulation results show that relay technologies

can effectively improve service coverage and system

throughput. Three relay transmission

schemes are then summarized and evaluated in

terms of transmission efficiency under different

radio channel conditions. Finally, a centralized

pairing scheme and a distributed pairing scheme

are developed for effective relay selection. Simulation

results show that the proposed schemes

can maximize the number of served UE units

and the overall throughput of a cell in a realistic

multiple-RS-multiple-UE scenario.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1419</field>
<field name="author">Eddie Li</field>
<field name="author">Liam O'Brien</field>
<field name="author">Jacky Keung</field>
<field name="author">Xiwei (Sherry) Xu</field>
<field name="title">Effort-Oriented Classification Matrix of Web Service Composition</field>
<field name="keyword">service</field>
<field name="keyword">oriented architecture (SOA); classification matrix; Web service composition; context</field>
<field name="keyword">oriented; technology</field>
<field name="keyword">oriented</field>
<field name="abstract">Within the service-oriented computing domain, Web service composition is an effective realization to satisfy the rapid changing requirements of business. Therefore, the research into Web service composition has unfolded broadly. Since examining all of the related work in this area becomes a mission next to impossible, the classification of composition approaches can be used to facilitate multiple research tasks. However, the current attempts to classify Web service composition do not have clear objectives. Furthermore, the contexts and technologies of composition approaches are confused in the existing classifications. This paper proposes an effort-oriented classification matrix for Web service composition, which distinguishes between the context and technology dimension. The context dimension is aimed at analyzing the environment influence on the effort of Web service composition, while technology dimension is aimed at analyzing the technique influence. Consequently, besides the traditional classification benefits, this matrix can also be used to build the basis of cost estimation for Web service composition in future research.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1420</field>
<field name="author">yuhang Zhang</field>
<field name="author">Wang Lei</field>
<field name="author">Richard Hartley</field>
<field name="author">Hongdong Li</field>
<field name="title">Handling Significant Scale Difference for Object Retrieval in a Supermarket</field>
<field name="abstract">We propose an object retrieval application which

can retrieve user specified objects from a big supermarket.

Significant and unpredictable scale difference between the

query and the database image is the major obstacle encountered.

The widely used local invariant features show their

deficiency in such an occasion. To improve the situation, we

first design a new weighting scheme which can assess the

repeatability of local features against scale variance. Also,

another method which deals with scale difference through

retrieving a query under multiple scales is also developed. Our

methods have been tested on a real image database collected

from a local supermarket and outperform the existing local

invariant feature based image retrieval approaches. A new

spatial check method is also briefly discussed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1421</field>
<field name="author">Worapan Kusakunniran</field>
<field name="author">Qiang Wu</field>
<field name="author">Jian Zhang</field>
<field name="author">Hongdong Li</field>
<field name="title">Multi-view Gait Recognition based on Motion Regression using Multilayer Perceptron</field>
<field name="keyword">gait recognition</field>
<field name="keyword"> human identification</field>
<field name="keyword"> multi-view</field>
<field name="keyword"> MLP</field>
<field name="keyword"> VTM</field>
<field name="abstract">It has been shown that gait is an ef cient biometric feature for identifying a person at a distance. However, it is a challenging problem to obtain reliable gait feature when viewing angle changes because the body appearance can be different under the various viewing angles. In this paper, the problem above is formulated as a regression problem where a novel View Transformation Model (VTM) is constructed by adopting Multilayer Perceptron (MLP) as regression tool. It smoothly estimates gait feature under an unknown viewing angle based on motion information in a well selected Region of Interest (ROI) under other existing viewing angles. Thus, this proposal can normalize gait features under various viewing angles into a common viewing angle before gait similarity measurement is carried out. Encouraging experimental results have been obtained based on widelyadopted benchmark database.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1422</field>
<field name="author">William Billingsley</field>
<field name="author">Cindy Gallois</field>
<field name="author">Andrew Smith</field>
<field name="author">Marcus Watson</field>
<field name="title">COMLEX: Visualizing communication for research and saving lives</field>
<field name="keyword">Communication skills</field>
<field name="keyword"> Professional development tools</field>
<field name="keyword"> Medical simulation</field>
<field name="keyword"> Video review</field>
<field name="keyword"> Visualization</field>
<field name="abstract">One of the major causes of patient harm in hospital is poor communication. We are developing a video review and visualization platform to research and improve medics communication skills. It intended for use by experimenters, as a deployable training tool for medics, and also for forensic review of communication. It supports pluggable analysis modules and visualizations for research teams, and configurable workflow for educators and hospital administrators.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1423</field>
<field name="author">William Billingsley</field>
<field name="author">Cindy Gallois</field>
<field name="author">Andrew Smith</field>
<field name="author">Marcus Watson</field>
<field name="title">COMLEX: Visualizing communication for research and saving lives</field>
<field name="abstract">Poster accompanying Extended Abstract in submission QRL-3479</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1424</field>
<field name="author">Pengyi Yang</field>
<field name="author">Zili Zhang</field>
<field name="author">Bing Bing Zhou</field>
<field name="author">Albert Zomaya</field>
<field name="title">A Clustering Based Hybrid System for Biomarker Selection and Sample Classification of Mass Spectrometry Data</field>
<field name="keyword">Clustering</field>
<field name="keyword"> Mass spectrometry</field>
<field name="keyword"> Hybrid algorithm</field>
<field name="keyword"> Feature selection</field>
<field name="keyword"> Classification</field>
<field name="abstract">Mass spectrometry (MS)-based proteomics has been established as a standard way for

biomarker discovery and early detection of disease from the proteome level. However, the data

generated by MS technology are noisy, redundant, and most importantly with low sample-todimension

ratio. Therefore, successful analysis of MS data heavily relies on the informatic techniques

applied. In this paper, we briefly discuss some of the commonly used informatic algorithms applied to

mass-to-charge (m/z) feature selection, and propose a k-means clustering based hybrid algorithm

designed to address the high-dimension and high-correlation issues associated with this task. Our kmeans

clustering based hybrid algorithm incorporates the advantages of both filter- and wrapperbased

feature selection algorithms. A special iterative procedure is introduced to overcome the

instability of k-means clustering and genetic algorithm. By comparing the proposed hybrid system with

several popular m/z feature selection algorithms using 10 different classifiers, we show that the

proposed algorithm is able to select m/z features with lower correlation while achieving higher sample

classification accuracy. The m/z feature selection results also suggest that the proposed hybrid

algorithm is very stable despite its stochastic nature.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1425</field>
<field name="author">Cara Stitzlein</field>
<field name="author">Penelope Sanderson</field>
<field name="title">Clinical Information Flow and Variability in Practice in Healthcare</field>
<field name="keyword">eHealth</field>
<field name="keyword"> evaluation</field>
<field name="keyword"> information pathways</field>
<field name="keyword"> critical care</field>
<field name="abstract">We summarise ongoing research directed to the problem of prospectively evaluating the impact of eHealth technologies on work in the critical care context. We outline our approach and present a simplified model of clinical information pathways in a large Australian intensive care unit some months after the implementation of an Electronic Health Record (EHR). Observations and discussion with stakeholders make it clear that there is considerable variability across patients and caregivers in how the EHR is configured and used by caregivers, and therefore what the information pathways might be. Such variability in practice creates a challenge for prospectively evaluating the impact of further technical changes.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1426</field>
<field name="author">David Newman</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Lawrence Cavedon</field>
<field name="title">Topic Models to Interpret MeSH MEDLINE s Medical Subject Headings</field>
<field name="abstract">We show how topic models are useful for interpreting and understanding MeSH, the Medical Subject Headings applied to articles in MEDLINE. We show how our resampled author model captures some of the advantages of both the topic model and the author-topic model. We demonstrate how the topic modeling approach can provide an alternative and complementary view of the relationship between MeSH headings that could be informative and helpful for people searching MEDLINE.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1427</field>
<field name="author">Qinghua Lu</field>
<field name="author">Vladimir Tosic</field>
<field name="title">An Analysis of Patents from the Business-Driven IT Management Research Community</field>
<field name="keyword">business-driven IT management</field>
<field name="keyword"> literature survey</field>
<field name="keyword"> patent</field>
<field name="keyword"> patent survey</field>
<field name="keyword"> patent analysis</field>
<field name="abstract">Many important (particularly industry) solutions to research problems are included in patents, but not in academic publications. We studied patents invented by researchers associated with the annual IEEE/IFIP Business-Driven IT Management (BDIM) workshops and selected for in-depth analysis 4 granted patents and 19 pending patent applications most closely related to BDIM topics. These patents address diverse applications, most commonly resource allocation and contract management. Similarly, they have diverse official classifications, most often the International Patent Classification classes G06Q 10/00 and G06F 17/60 and the US Patent Classification class 705/7. We found that all analyzed patents were invented by either IBM or HP employees and determined the most prolific inventors, both those associated with the BDIM workshops and additional co-inventors. Our year-by-year analysis indicates some growth in interest in BDIM-related innovation. By examining citations by and citations of the studied 23 patents, we found additional BDIM-related patents. We also concluded that only 4 patents emphasize ease of operation, important for crossing the chasm between early and mainstream customers. Additional information and insights about the BDIM area from our study complement the past academic literature surveys and can guide future patent analyses.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1428</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Composite Binary Losses</field>
<field name="abstract">We study losses for binary classifi&#12;cation and class probability estimation and

extend the understanding of them from margin losses to general composite losses

which are the composition of a proper loss with a link function. We characterise

when margin losses can be proper composite losses, explicitly show how to determine

a symmetric loss in full from half of one of its partial losses, introduce an intrinsic

parametrisation of composite binary losses and give a complete characterisation of

the relationship between proper losses and "classi&#12;cation calibrated" losses. We

also consider the question of the "best" surrogate binary loss. We introduce a

precise notion of "best" and show there exist situations where two convex surrogate

losses are incommensurable. We provide a complete explicit characterisation of the

convexity of composite binary losses in terms of the link function and the weight

function associated with the proper loss which make up the composite loss. This

characterisation suggests new ways of "surrogate tuning". Finally, in an appendix

we present some new algorithm-independent results on the relationship between

properness, convexity and robustness to misclassifi&#12;cation noise for binary losses and

show that all convex proper losses are non-robust to misclassi&#12;fication noise.



Preprint: http://arxiv.org/pdf/0912.3301v1</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1429</field>
<field name="author">Leif Hanlen</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Smith</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Co-channel Interference in Body Area Networks with Indoor Measurements at 2.4GHz: Distance-to-interferer is a poor estimate of received interference power</field>
<field name="abstract">Inter-network interference is likely to be a significant source of difficulty for wireless body area networks. Movement, proximity of networks, the large number of nodes per network and the lack of central coordination make cellular approaches to interference modeling ineffective. 



We examine the interference power of multiple Body Area Networks (BANs) when people move randomly within an indoor office environment. The power-loss trend over 3m is overwhelmed by random variations in the signal power. Distance-to-interferer is a poor estimate of instantaneous received interference power, and an even less reliable estimate of instantaneous signal-to-interference ratio (SIR). We develop a lognormal statistical model for the signal-to-interference which incorporates the distance effect.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1430</field>
<field name="author">Tania Xiao</field>
<field name="author">Penelope Sanderson</field>
<field name="author">Samantha Clayton</field>
<field name="author">Svetha Venkatesh</field>
<field name="title">The ETTO principle and organisational strategies: A field study of ICU bed and staff management</field>
<field name="keyword">efficiency-thoroughness tradeoff</field>
<field name="keyword"> ETTO principle</field>
<field name="keyword"> healthcare</field>
<field name="keyword"> scheduling</field>
<field name="keyword"> nursing</field>
<field name="abstract">The aim of this paper is to identify the organisational strategies that nurses use to balance efficiency and thoroughness demands relating to bed and staff management in the ICU. We provide a summary of the ETTO principle, connecting it with key ideas in the organisational literature, and then describe the basics of the bed and staff management problem and of the study itself. Then we outline five key organisational strategies used by nurses to achieve success, highlighting the heuristics that guide nurses implementation of the strategies. Finally, we draw conclusions for how research on the ETTO principle at the organisational level might be further developed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1431</field>
<field name="author">David Shaw</field>
<field name="author">Nick Barnes</field>
<field name="title">Perspective Invariant Angle Ordering</field>
<field name="abstract">In this paper we present the geometric property

 of perspective invariant angle ordering; the order of angles between point features. We describe how this can be used to exploit the structure of the appearance of features on planar or

 near planar surfaces to improve precision for localisation and

 object recognition. We show test results on real-world images

 that show marked improvement over straight bag-of-features

 approaches.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1432</field>
<field name="author">Pengdong Xiao</field>
<field name="author">Nick Barnes</field>
<field name="author">Paulette Lieby</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Applying Sum and Max Product Algorithms of Belief Propagation to 3D Shape Matching and Registration</field>
<field name="abstract">3D shape matching based on meshed surfaces can be formulated as an energy function minimisation problem under a Markov random field (MRF) framework. However, to solve such a global optimisation problem is NP-hard. So research mainly focuses on approximation algorithms. One of the best known is belief propagation (BP), which has shown success in early vision and many other practical applications. In this paper, we investigate the application of both sum and techniques.

max product algorithms of belief propagation to 3D shape matching. We also apply the 3D shape matching results to a 3D registration problem.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1433</field>
<field name="author">Kevin Buchin</field>
<field name="author">Sergio Cabello</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Maarten L _ffler</field>
<field name="author">Jun Luo</field>
<field name="author">G _nter Rote</field>
<field name="author">Rodrigo Silveira</field>
<field name="author">Bettina Speckmann</field>
<field name="author">Thomas Wolle</field>
<field name="title">Finding the Most Relevant Fragments in Networks</field>
<field name="abstract">We study a point pattern detection problem on networks, motivated

by applications in geographical analysis, such as crime hotspot detection. Given a network $N$ (a

connected graph with non-negative edge lengths) together with a set of sites, which lie on the edges or vertices of $N$, we look for a

connected subnetwork $F$ of $N$ of small total length that contains

many sites. The edges of $F$ can form parts of the edges of $N$.



We consider different variants of this problem where $N$ is either a

general graph or restricted to a tree, and the subnetwork $F$ that we

are looking for is either a simple path, a path with

self-intersections at vertices, or a tree. We give polynomial-time

algorithms, NP-hardness and NP-completeness proofs, approximation

algorithms, and also fixed-parameter tractable algorithms.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1434</field>
<field name="author">Mohammad Ali Abam</field>
<field name="author">Mark de Berg</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">A Simple and Efficient Kinetic Spanner</field>
<field name="abstract">We present a kinetic data structure for maintaining a (1+e)-spanner of size O(n/e^2) for a set of n moving points in the plane. Assuming the trajectories of the points can be described by polynomials whose degrees are at most s, the number of events processed by our structure is O((n/e^2) L_s+2(n)), where L_s+2(n) denotes the maximum length of a (n,s+2)-Davenport-Schinzel sequence. Each event can be handled in O(1) time, plus O(log n) time to update the event queue.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1435</field>
<field name="author">Jason Jingshi Li</field>
<field name="author">Jinbo Huang</field>
<field name="author">Jochen Renz</field>
<field name="title">A Divide-and-Conquer Approach for Solving Interval Algebra Networks</field>
<field name="keyword">Spatial Reasoning</field>
<field name="keyword"> Temporal Reasoning</field>
<field name="keyword"> Efficient Algorithms</field>
<field name="keyword"> Artificial Intelligence</field>
<field name="keyword"> Interval Algebra</field>
<field name="abstract">Deciding consistency of constraint networks is a

fundamental problem in qualitative spatial and temporal

reasoning. In this paper we introduce a

divide-and-conquer method that recursively partitions

a given problem into smaller sub-problems in

deciding consistency. We identify a key theoretical

property of a qualitative calculus that ensures the

soundness and completeness of this method, and

show that it is satisfied by the Interval Algebra (IA)

and the Point Algebra (PA). We develop a new encoding

scheme for IA networks based on a combination

of our divide-and-conquer method with an

existing encoding of IA networks into SAT. We

empirically show that our new encoding scheme

scales to much larger problems and exhibits a consistent

and significant improvement in efficiency

over state-of-the-art solvers on the most difficult instances.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1436</field>
<field name="author">Owen Thomas</field>
<field name="author">Peter Sunehag</field>
<field name="author">Gideon Dror</field>
<field name="author">S Yun</field>
<field name="author">SuNam Kim</field>
<field name="author">Alex Smola</field>
<field name="author">Green Daniel</field>
<field name="author">Saunders Philo</field>
<field name="author">Matthew Robards</field>
<field name="author">C Yoo</field>
<field name="title">Wearable-sensor activity analysis using semi-Markov models with a grammar</field>
<field name="abstract">Detailed monitoring of training sessions of elite athletes is an important component of their training. In this paper we describe an application that performs a precise segmentation and labeling of swimming sessions. This allows a comprehensive break-down of the training session, including lap times, detailed statistics of strokes, and turns. To this end we use semi-Markov models (SMM), a formalism for labeling and segmenting sequential data, trained in a max-margin setting. To reduce the computational complexity of the task and at the same time enforce sensible output, we introduce a grammar into the SMM framework. Using the trained model on test swimming sessions of different swimmers provides highly accurate segmentation as well as perfect labeling of individual segments. The results are significantly better than those achieved by discriminative hidden Markov models.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1437</field>
<field name="author">xiaoqin chen</field>
<field name="author">Dhammika Jayalath</field>
<field name="author">Leif Hanlen</field>
<field name="title">Cooperative Routing for Wireless Networks with Multiple Shared Channels</field>
<field name="abstract">Transmission links in wireless networks suffer from in- 

dependent channel fading. Cooperative routing technology 

with diversity transmission can exploit the randomness of 

the wireless channel and efficiently improve the network 

performance by selecting multiple cooperative nodes to for- 

ward the data. This paper proposes a novel Multiple Shared 

Channels Cooperative (MSCC) routing protocol for wire- 

less networks. MSCC takes the advantage of cooperative 

transmission to achieve diversity gains. It also combines a 

clustering hierarchy with a bandwidth reuse scheme to miti- 

gate Co-Channel Interference (CCI). The simulation results 

show an improvement in the packet delivery ratio of net- 

works with MSCC.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1438</field>
<field name="author">Yin Kia Chiam</field>
<field name="author">Liming Zhu</field>
<field name="author">Mark Staples</field>
<field name="title">Systematic Selection of Quality Attribute Techniques</field>
<field name="keyword">Quality Attribute Techniques</field>
<field name="keyword"> Product Quality</field>
<field name="keyword"> Software Process Improvement</field>
<field name="keyword"> Selection Method</field>
<field name="abstract">Various techniques are used by development teams to investigate, evaluate, and control product quality risks throughout software development process. These ``Quality Attribute Techniques'' are used during all stages of the software development life cycle to ensure that acceptable levels of product qualities such as safety and performance are in place. In this paper, we propose a method to select from among the alternatives of these techniques. This method is based on Risk Management theory and the Analytic Hierarchy Process (AHP) approach. We apply our method to an example of real-world safety system presented in the literature. We identify advantages and limitations of the method, and discuss future research.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1439</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Thanassis Boulis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="author"/>
<field name="title">Sleeping channel measurements for body area networks</field>
<field name="abstract">300+ hours of sleeping BAN channel measurements with characterisation of data in terms of outages with respect to receiver sensitivity. Showing that channel outages are greater than 10% with -100 dBm receiver sensitivity</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1440</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author"/>
<field name="author">Thanassis Boulis</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">PHY Interference Statistics and MAC Simulations</field>
<field name="abstract">First order statistics of BAN signal and interference links. Castalia MAC simulations</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1441</field>
<field name="author">Patricia Bouyer</field>
<field name="author">Franck Cassez</field>
<field name="author">Francois Laroussinie</field>
<field name="title">Timed modal logics for real-time systems - specification, verification and control.</field>
<field name="keyword">Model checking</field>
<field name="keyword"> Timed automata</field>
<field name="keyword"> Timed modal logic</field>
<field name="keyword"> Timed control</field>
<field name="abstract">In this paper, a timed modal logic Lc is presented for the specification 

and verification of real-time systems. Several important results for Lc are discussed. 

First we address the model checking problem and we show that it is an EXPTIME-complete problem. Secondly we consider expressiveness and we explain how to 

express strong timed bisimilarity and how to build characteristic formulas for timed 

automata. We also propose a compositional algorithm for Lc model checking. Finally 

we consider several control problems for which Lcan be used to check controllability.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1442</field>
<field name="author">Franck Cassez</field>
<field name="author">Ron van der Meyden</field>
<field name="author">Chenyi Zhang</field>
<field name="title">The Complexity of Synchronous Notions of Information Flow Security</field>
<field name="abstract">The paper considers the complexity of verifying that a finite state system

satisfies a number of definitions of information flow security.

The systems model considered is one in which agents operate

synchronously with awareness of the global clock. This enables timing

based attacks to be captured, whereas previous work on this topic

has dealt primarily with asynchronous systems.

Versions of the notions of

nondeducibility on inputs, nondeducibility on strategies, and

an unwinding based notion are formulated for this model.

All three notions are shown to be decidable, and their computational

complexity is characterised.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1443</field>
<field name="author">Wee Lum Tan</field>
<field name="author">Konstanty Bialkowski</field>
<field name="author">Marius Portmann</field>
<field name="title">Evaluating Adjacent Channel Interference in IEEE 802.11 Networks</field>
<field name="keyword">IEEE802.11; multi</field>
<field name="keyword">channel wireless mesh network; adjacent channel interference; conducted testbed</field>
<field name="abstract">The performance of 802.11-based multi-channel wireless mesh networks is affected by the interference due to neighboring nodes operating on same or adjacent channels. In this paper, we have performed extensive measurements on our conducted testbed to evaluate the effects of adjacent channel interference (ACI) in 802.11 networks, under the exposed terminal and hidden terminal scenarios. By varying the path loss and the channel separation distance between two nodes, we investigate the effective attenuation needed in order to completely eliminate the ACI between the two nodes. Using node throughput as a metric, our results confirm that for low path loss between two 802.11 nodes, there still exists interference between the two nodes even though they are operating on non-overlapping channels. Our results also show that we require 37dB 45dB less attenuation to completely eliminate the ACI between two nodes operating on non-overlapping channels, compared to when both nodes are operating on the same channel. Our results are useful to network planners in terms of the placement of mesh nodes and the assignment of channels on the nodes in an 802.11-based multi-channel wireless mesh network.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1444</field>
<field name="author">Andres Sanin</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Improved Shadow Removal for Robust Person Tracking in Surveillance Scenarios</field>
<field name="abstract">Shadow detection and removal is an important step employed after foreground detection, in order to improve the segmentation of objects for tracking. Methods reported in the literature typically have a significant trade-off between the shadow detection rate (classifying true shadow areas as shadows) and the shadow discrimination rate (discrimination between shadow areas and foreground areas). We propose a method that is able to achieve good performance in both cases, leading to improved tracking in surveillance scenarios. Chromacity information is first used to create a mask of candidate shadow pixels, followed by employing gradient information to remove foreground pixels that were incorrectly included in the mask. Experiments on the CAVIAR dataset indicate that, for several tracking algorithms, the proposed method leads to considerable improvements in multiple object tracking precision and accuracy.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1445</field>
<field name="author">Yong Wong</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Sandra Mau</field>
<field name="author">Brian Lovell</field>
<field name="title">Dynamic Amelioration of Resolution Mismatches for Local Feature Based Identity Inference</field>
<field name="abstract">While existing face recognition systems based on local features are robust to issues such as misalignment, they can exhibit accuracy degradation when comparing two images of differing resolutions. This is common in surveillance environments where a gallery of high resolution gallery mugshots is compared to low resolution CCTV probe images, or where the size of a given image is not a reliable indicator of the underlying resolution (e.g. poor optics). To alleviate this degradation, we propose a compensation framework which dynamically chooses the most appropriate face recognition system for a given pair of image resolutions. This framework applies a novel resolution detection method which does not rely on the size of the input images, but instead exploits the sensitivity of local features to resolution using a probabilistic multi-region histogram approach. Experiments on a resolution-modified version of the "Labeled Faces in the Wild" dataset show that the proposed resolution detector front-end obtains a 99% average accuracy in selecting the most appropriate face recognition system, resulting in higher overall face discrimination accuracy (across several resolutions) compared to the individual baseline face recognition systems.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1446</field>
<field name="author">Philip Kilby</field>
<field name="author">Fraser Johnson</field>
<field name="title">RTA and NICTA: Innovating on Successful Traffic Management (Extended Abstract)</field>
<field name="keyword"/>
<field name="abstract">The Roads and Traffic Authority of NSW (RTA) and NICTA are engaged in a successful, ongoing research collaboration. RTA continues to develop the SCATS adaptive traffic control system working with NICTA to implement some of the innovations emerging from the RTA/NICTA Smart Transport and Roads initiative. SCATS is one of the most widely used and successful traffic control systems, with installations in 142 cities across 25 countries. NICTA is the Australian Government s Centre for Excellence for research into Information and Communications Technology. The collaboration is allowing RTA and NICTA to look at measures for improving traffic control in specific, difficult situations.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1447</field>
<field name="author">Zhidong Li</field>
<field name="author">Yang Wang</field>
<field name="author">Glenn Geers</field>
<field name="author">Jing Chen</field>
<field name="author">Jun Yang</field>
<field name="author">John Laird</field>
<field name="title">SALIENCY BASED JOINT TOPIC DISCOVERY FOR OBJECT CATEGORIZATION</field>
<field name="keyword">saliency</field>
<field name="keyword"> image categorization</field>
<field name="keyword"> topic model</field>
<field name="abstract">We present a novel approach of image categorization by integrating topic model with saliency detection. The method considers the layout of image by discriminating foreground objects from background scene with the saliency detection. Then the topic model is used to discover topics of both foreground part and background part. We show that our approach can category an image collection into classes in a complete unsupervised manner and achieve higher performance than traditional methods, especially for images with similar foreground or background topics.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1448</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Li Cheng</field>
<field name="author">Jian Zhang</field>
<field name="author">Li Wang</field>
<field name="author">Satoh Shinichi</field>
<field name="title">Weakly Supervised Action Recognition using Implicit Shape Models</field>
<field name="keyword">Action Recognition</field>
<field name="keyword"> Implicit Shape Model</field>
<field name="keyword"> Sparse Bayesian regression</field>
<field name="abstract">In this paper, we present a robust framework for action

recognition in video, that is able to perform competitively

against the state-of-the-art methods, yet does

not rely on sophisticated background subtraction preprocess

to remove background features. In particular,

we extend the Implicit Shape Modeling (ISM) of 

for object recognition to 3D to integrate local spatiotemporal

features, which are produced by a weakly supervised

Bayesian kernel filter. Experiments on benchmark

datasets (including KTH and Weizmann)

verifies the effectiveness of our approach.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1449</field>
<field name="author">Peter Chubb</field>
<field name="title">Decent MIDI from LilyPond</field>
<field name="abstract">LilyPond is primarily a means to produce beautifully typeset music scores; but it can also produce MIDI (musical instrument digital interface) output for 'proofhearing' the scores. Unfortunately, LilyPond's MIDI output is not very good: it obeys the notes and any explicit metronome and dynamic markings, but that's about it. 

So I (Peter Chubb) wrote some scheme code that interpreted some of the more commonly used marks in a musical score. The idea was to rewrite the LilyPond input before LilyPond interpreted it, so, for example, slurs and phrases were obeyed, and trills were fully expanded.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1450</field>
<field name="author">Sam Zhao</field>
<field name="author">Yongsheng Gao</field>
<field name="author">Terry Caelli</field>
<field name="title">High-Order Circular Derivative Pattern for Image Representation and Recognition</field>
<field name="keyword">Image representation</field>
<field name="keyword"> image recognition</field>
<field name="keyword"> micropattern representation</field>
<field name="keyword"> Circular Derivative Pattern (CDP).</field>
<field name="abstract">Micropattern based image representation and recognition, e.g. Local Binary Pattern (LBP), has been proved successful over the past few years due to its advantages of illumination tolerance and computational efficiency. However, LBP only encodes the first-order radial-directional derivatives of spatial images and is inadequate to completely describe the discriminative features for classification. This paper proposes a new Circular Derivative Pattern (CDP) which extracts high-order derivative information of images along circular directions. We argue that the high-order circular derivatives contain more detailed and more discriminative information than the first-order LBP in terms of recognition accuracy. Experimental evaluation through face recognition on the FERET database and insect classification on the NICTA Biosecurity Dataset demonstrated the effectiveness of the proposed method.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1451</field>
<field name="author">Sam Zhao</field>
<field name="author">Yongsheng Gao</field>
<field name="author">Baochang Zhang</field>
<field name="title">Performance Evaluation of Micropattern Representation on Gabor Features for Face Recognition</field>
<field name="keyword">Face recognition</field>
<field name="keyword"> performance evaluation</field>
<field name="keyword"> micropattern representation</field>
<field name="keyword"> Gabor feature.</field>
<field name="abstract">Face recognition using micropattern representation has recently received much attention in the computer vision and pattern recognition community. Previous researches demonstrated that micropattern representation based on Gabor features achieves better performance than its direct usage on gray-level images. This paper conducts a comparative performance evaluation of micropattern representations on four forms of Gabor features for face recognition. Three evaluation rules are proposed and observed for a fair comparison. To reduce the high feature dimensionality problem, uniform quantization is used to partition the spatial histograms. The experimental results reveal that: 1) micropattern representation based on Gabor magnitude features outperforms the other three representations, and the performances of the other three are comparable; and 2) micropattern representation based on the combination of Gabor magnitude and phase features performs the best.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1452</field>
<field name="author">Vikas Reddy</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Robust Foreground Object Segmentation via Adaptive Region-Based Background Modelling</field>
<field name="abstract">We propose a region-based foreground object segmentation method capable of dealing with image sequences containing noise, illumination variations and dynamic backgrounds (as often present in outdoor environments). The method utilises contextual spatial information through analysing each frame on an overlapping block-by-block basis and obtaining a low-dimensional texture descriptor for each block. Each descriptor is passed through an adaptive multi-stage classifier, comprised of a likelihood evaluation, an illumination invariant measure, and a temporal correlation check. The overlapping of blocks not only ensures smooth contours of the foreground objects but also effectively minimises the number of false positives in the generated foreground masks. The parameter settings are robust against wide variety of sequences and post-processing of foreground masks is not required. Experiments on the challenging I2R dataset show that the proposed method obtains considerably better results (both qualitatively and quantitatively) than methods based on Gaussian mixture models (GMMs), feature histograms, and normalised vector distances. On average, the proposed method achieves 36% more accurate foreground masks than the GMM based method.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1453</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Image Inpainting Based on Local Optimisation</field>
<field name="abstract">In this paper, we tackle the problem of image inpainting

which aims at removing objects from an image

or repairing damaged pictures by replacing the

missing regions using the information in the rest of the

scene. The image inpainting method proposed here

builds on an exemplar-based perspective so as to improve

the local consistency of the inpainted region. This

is done by selecting the optimal patch which maximises

the local consistency with respect to abutting candidate

patches. The similarity computation generates weights

based upon an edge prior and the structural differences

between inpainting exemplar candidates. This treatment

permits the generation of an inpainting sequence

based on a list of factors. The experiments show that

the proposed method delivers a margin of improvement

as compared to alternative methods.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1454</field>
<field name="author">Aaron Carroll</field>
<field name="author">Gernot Heiser</field>
<field name="title">An Analysis of Power Consumption in a Smartphone</field>
<field name="keyword">power management</field>
<field name="keyword"> DVFS</field>
<field name="keyword"> mobile phone</field>
<field name="abstract">Mobile consumer-electronics devices, especially phones, are powered from batteries which are limited in size and therefore capacity. This implies that managing energy well is paramount in such devices.



Good energy management requires a good understanding of where and how the energy is used. To this end we present a detailed analysis of the power consumption of a recent mobile phone, the Openmoko Neo Freerunner. We measure not only overall system power, but the exact breakdown of power consumption by the device's main hardware components. We present this power breakdown for micro-benchmarks as well as for a number of realistic usage scenarios. These results are validated by overall power measurements of two other devices: the HTC Dream and Google Nexus One.



We develop a power model of the Freerunner device and analyse the energy usage and battery lifetime under a number of usage patterns. We discuss the significance of the power drawn by various components, and identify the most promising areas to focus on for further improvements of power management. We also analyse the energy impact of dynamic voltage and frequency scaling of the device's application processor.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1455</field>
<field name="author">Abd-Krim Seghouane</field>
<field name="title">A Kullback-Leibler Methodology for Unconditional ML DOA Estimation in Unknown Nonuniform Noise</field>
<field name="abstract">Maximum likelihood (ML) direction-of arrival (DOA) estimation of

multiple narrowband sources in unknown nonunifrom white noise is

considered. A new iterative algorithm for stochastic ML DOA

estimation is presented. The stepwise concentration of the (LL)

log-likelihood function with respect to the signal and noise

nuisance parameters is derived by alternating minimization of the

Kullback-Leibler divergence between a model family of probability

distributions defined on the unconditional model and a desired

family of probability distributions constrained to be concentrated

on the observed data. The new algorithm presents the advantage to

provide closed form expressions for the signal and noise nuisance

parameter estimates which results in a substantial reduction of

the parameter space required for numerical optimization. The

proposed algorithm converges only after few iterations and its

effectiveness is confirmed in a simulation example.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1456</field>
<field name="author">Peng Wang</field>
<field name="author">Chunhua Shen</field>
<field name="author">Hong Zheng</field>
<field name="author">Ren Zhang</field>
<field name="title">Training a multi-exit cascade with linear asymmetric classification for efficient object detection</field>
<field name="keyword">Face detection</field>
<field name="keyword"> Boosting</field>
<field name="keyword"> Linear Asymmetric Classifier</field>
<field name="keyword"> Cascade Classifier</field>
<field name="abstract">Efficient visual object detection is of central interest in computer vision

and pattern recognition due to its wide ranges of applications. Viola and Jones'

detector has become a {\em de facto} framework \cite{Viola2001Robustrealtime}. 

In this work, we propose a new method to design a cascade of boosted

classifiers for fast object detection, 

which combines linear asymmetric classification (LAC) into the recent 

multi-exit cascade structure.

Therefore, the proposed method takes advantages of both LAC and the multi-exit cascade. Namely,

(1) the multi-exit cascade structure collects all the scores of prior nodes 

for decision making at the current node, 

which reduces the loss of decision information; 

(2) LAC considers the asymmetric nature of the node training.

We also show that the multi-exit cascade better meets the assumption of LAC learning

than the standard Viola-Jones' cascade, both theoretically and empirically. 

Experiments confirm that our method outperforms existing methods such as Viola and Jones

\cite{Viola2001Robustrealtime} 

and Wu \etal \cite{Wu2008FastAsymmetric} on the MIT+CMU test data set.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1457</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Sleeping channel measurements for BAN</field>
<field name="keyword"/>
<field name="abstract">300+ hours of sleeping BAN channel measurements with characterisation of data in terms of outages with respect to receiver sensitivity. Showing that channel outages are greater than 10% with -100 dBm receiver sensitivity.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1458</field>
<field name="author">Dino Miniutti</field>
<field name="author">David Smith</field>
<field name="author">Leif Hanlen</field>
<field name="author">Andrew Zhang</field>
<field name="author">David Rodda</field>
<field name="author">Ben Gilbert</field>
<field name="title">Further sleeping measurements for BAN</field>
<field name="keyword"/>
<field name="abstract">300+ hours of sleeping BAN channel measurements with characterisation of data in terms of outages with respect to receiver sensitivity. Showing that channel outages are greater than 10% with -100 dBm receiver sensitivity. Added appendix to document that responds to some concerns about the measurements that were raised at the November 2009 meeting. These measurements validate the measurements presented in November.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1459</field>
<field name="author">John Brondum</field>
<field name="author">Liming Zhu</field>
<field name="title">Towards an Architectural Viewpoint for Systems of Software Intensive Systems</field>
<field name="keyword">Software Architecture</field>
<field name="keyword"> Systems of Systems</field>
<field name="keyword"> Architectural Views</field>
<field name="abstract">An important part of architectural knowledge is the capture of the environment and external system relationships through contextual viewpoints[35]. However, the semantic definitions of software relationships[29][23][26] do not adequately capture inter-system level relationships[6], and offers no guidance on implicit or indirect relationships[35]. Yet the architect is tasked with the very responsibility of defining external relationships[11] leaving him/her either unaware of critical relationships or, to roll their own based on aggregations of code-level call/use structures. This leads to critical

gaps in the architectural documentation and communication problems within Systems of Software intensive Systems (S3) environments - if undetected can cause serious problems for development projects[3]. S3 environments may also restrict the sharing of architectural knowledge due to either legal or contractual constraints, or contain an overwhelming amount of documentation due to size and number of involved systems - either scenario further adds to the challenge of identifying and describing the relationships.



This paper presents a novel and light-weight S3 Architectural Viewpoint consisting of 1) an extensible taxonomy of system level relationships, 2) with a systematic, repeatable technique to detect both immediate and linked system level relationships. The goal is an architectural approach for the sharing and analysis of architectural knowledge relating to software system relationships in an S3 or ecosystem environment. The research is on-going and developed through the mining of existing software ecosystems and industry S3 architectures. Validation will be performed through case studies from industry collaborations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1460</field>
<field name="author">John Brondum</field>
<field name="title">Software Architecture for Systems of Software Intensive Systems (S3): The Concepts and Detection of Inter-System Relationships</field>
<field name="keyword">Software Architecture</field>
<field name="keyword"> Software Relationships</field>
<field name="keyword"> Systems-of- Systems</field>
<field name="abstract">Key to software architecture is the description of relationships

between software components [10] supported by commonly

understood semantic definitions [9][8]. However, the

definitions do not adequately capture the inter-system level

software relationships. This leaves software architects either

unaware of critical relationships or, to roll their own based

on aggregations of code-level call/use structures. This leads

to critical gaps in the architectural description and communication

problems within distributed development environments

- as poorly understood relationships can inadvertently

propagate changes and break system interoperability

[2]. The solution requires a description of new system level

relationships and a new systematic, repeatable technique to

detect both immediate and linked system level relationships.

The solution will be developed through the mining of existing

software ecosystems and industry systems of software

intensive systems (S3) architectures. Validation will be performed

through case studies from industry collaborations.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1461</field>
<field name="author">Marcus Hutter</field>
<field name="title">A Complete Theory of Everything (will be Subjective)</field>
<field name="keyword">artificial intelligence</field>
<field name="keyword"> ockham's razor</field>
<field name="keyword"> theory of everything</field>
<field name="abstract">The progression of theories suggested for our world, from ego- to geo- to helio-centric models to universe and multiverse theories and beyond, shows one tendency: The size of the described worlds increases, with humans being expelled from their center to ever more remote and random locations. If pushed too far, a potential theory of everything (ToE) is actually more a theory of nothing (ToN). Indeed such theories have already been developed. I show that including observer localization into such theories is necessary and su cient to avoid this problem. Ockham s razor is used to develop a quantitative recipe to identify ToEs and distinguish them from ToNs and theories in-between. This precisely shows what the problem is with some recently suggested universal ToEs. The suggested principle is extended to more practical (partial, approximate, probabilistic, parametric) world models (rather than ToEs). Finally, I provide a justi cation of Ockham s razor.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1462</field>
<field name="author">Joel Veness</field>
<field name="author">Kee Siong Ng</field>
<field name="author">Marcus Hutter</field>
<field name="author">David Silver</field>
<field name="title">A Monte Carlo AIXI Approximation</field>
<field name="keyword">artificial intelligence</field>
<field name="keyword"> ockham's razor</field>
<field name="keyword"> theory of everything</field>
<field name="abstract">This paper describes a computationally feasible approximation to the AIXI agent, a universal reinforcement learning agent for arbitrary environments. AIXI is scaled down in two key ways: First, the class of environment models is restricted to all prediction su x trees of a xed maximum depth. This allows a Bayesian mixture of environment models to be computed in time proportional to the logarithm of the size of the model class. Secondly, the nite-horizon expectimax search is approximated by an asymptotically convergent Monte Carlo Tree Search technique. This scaled down AIXI agent is empirically shown to be e ective on a wide class of toy problem domains, ranging from simple fully observable games to small POMDPs. We explore the limits of this approximate agent and propose a general heuristic framework for scaling this technique to much larger problems.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1463</field>
<field name="author">Jian Zhang</field>
<field name="author">Sakrapee (Paul) Paisitkriangkrai</field>
<field name="title">Scalabe Clip-based Near-duplicate Video Detection with Ordinal Measure</field>
<field name="abstract">Detection of duplicate or near-duplicate videos on large-scale

database plays an important role in video search. In this

paper, we analyze the problem of near-duplicates detection

and propose a practical and e&#11;ective solution for real-time

large-scale video retrieval. Unlike many existing approaches

which make use of video frames or key-frames, our solu-

tion is based on a more discriminative signature of video

clips. The feature used in this paper is an extension of or-

dinal measures which have proven to be robust to change

in brightness, compression formats and compression ratios.

For e&#14;cient retrieval, we propose to use Multi-Probe Local-

ity Sensitive Hashing (MPLSH) to index the video clips for

fast similarity search and high recall. MPLSH is able to &#12;lter

out a large number of dissimilar clips from video database.

To re&#12;ne the search process, we apply a slightly more expen-

sive clip-based signature matching between a pair of videos.

Experimental results on the data set of 12; 790 videos [25]

show that the proposed approach achieves at least 6:5% aver-

age precision improvement over the baseline color histogram

approach while satisfying real-time requirements. Further-

more, our approach is able to locate the frame o&#11;set of copy

segment in near-duplicate videos.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1464</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Falk Scholer</field>
<field name="author">Andrew Turpin</field>
<field name="title">Machine Transliteration Survey</field>
<field name="keyword">machine translation</field>
<field name="keyword"> transliteration</field>
<field name="keyword"> literature review</field>
<field name="abstract">Machine transliteration is the process of automatically transforming the

script of a word from a source language to a target language, while

preserving pronunciation. The development of algorithms specifically for

machine transliteration began over a decade ago based on the phonetics

of source and target languages, followed by approaches using statistical

and language-specific methods. In this survey, we review the key

methodologies introduced in the transliteration literature. The

approaches are categorized based on the resources and algorithms used,

and the effectiveness is compared.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1465</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Justin Zobel</field>
<field name="author">Falk Scholer</field>
<field name="title">Quantifying the Impact of Concept Recognition on Biomedical Information Retrieval</field>
<field name="abstract">In ad hoc querying of document collections, current approaches to

ranking primarily rely on identifying the documents that contain the

query terms. Methods such as query expansion, based on thesaural information or

automatic feedback, are used to add further terms, and can yield

significant though usually small gains in effectiveness.

Another approach to adding terms, which we investigate in this paper,

is to use natural language technology to annotate -- and thus

disambiguate -- key terms by the concept they represent.

Using biomedical research documents, we quantify the potential benefits

of tagging users' targeted concepts in queries and documents in

domain-specific information retrieval.

Our experiments, based on the TREC Genomics track data, both on passage

and full-text retrieval, found no evidence that current approaches

to concept recognition are of significant value for this task.

Moreover, the issues raised by these results suggest that it is

difficult for such disambiguation to be effective.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1466</field>
<field name="author">Baochang Zhang</field>
<field name="author">Yongsheng Gao</field>
<field name="author">Sam Zhao</field>
<field name="author">Jianzhuang Liu</field>
<field name="title">Local Derivative Pattern Versus Local Binary Pattern: Face Recognition With High-Order Local Pattern Descriptor</field>
<field name="keyword">Face recognition</field>
<field name="keyword"> Gabor feature</field>
<field name="keyword"> high-order local pattern</field>
<field name="keyword"> local binary pattern (LBP)</field>
<field name="keyword"> local derivative pattern (LDP).</field>
<field name="abstract">This paper proposes a novel high-order local pattern descriptor, Local Derivative Pattern (LDP), for face recognition. LDP is a general framework to encode directional pattern features based on local derivative variations. The nth-order LDP is proposed to encode the (n-1)th-order local derivative direction variations, which can capture more detailed information than the first-order local pattern used in Local Binary Pattern (LBP). Different from LBP encoding the relationship between the central point and its neighbors, the LDP templates extract high-order local information by encoding various distinctive spatial relationships contained in a given local region. Both gray-level images and Gabor feature images are used to evaluate the comparative performances of LDP and LBP. Extensive experimental results on FERET, CAS-PEAL, CMU-PIE, Extended Yale B and FRGC databases show that the high-order LDP consistently performs much better than LBP for both face identification and face verification under various conditions.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1467</field>
<field name="author">Scott Sanner</field>
<field name="author">Kristian Kersting</field>
<field name="title">Symbolic Dynamic Programming for First-order POMDPs</field>
<field name="keyword">Planning</field>
<field name="keyword"> Decision-making under Uncertainty</field>
<field name="abstract">Partially-observable Markov decision processes (POMDPs) provide a

powerful model for sequential decision-making problems with

partially-observed state and are known to have (approximately) optimal

dynamic programming solutions. Much work in recent years has focused

on improving the efficiency of these dynamic programming algorithms by

exploiting symmetries and factored or relational representations. In

this work, we show that it is also possible to exploit the full

expressive power of first-order quantification to achieve state,

action, \emph{and} observation abstraction in a closed-form, lifted

solution to relationally specified POMDPs. Among the advantages of

this approach are the ability to maintain compact value function

representations, abstract over the space of potentially optimal

actions, and automatically derive compact conditional policy trees

that minimally partition relational observation spaces according to

distinctions that have an impact on policy values. This is the first

lifted relational POMDP solution that can optimally accommodate

actions with a potentially infinite relational space of observation

outcomes.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1468</field>
<field name="author">Downey Carlton</field>
<field name="author">Scott Sanner</field>
<field name="title">Temporal Difference Bayesian Model Averaging: A Bayesian Perspective on Adapting Lambda</field>
<field name="keyword">MDPs</field>
<field name="keyword"> Reinforcement Learning</field>
<field name="abstract">Temporal difference (TD) algorithms are attractive for reinforcement

learning due to their ease-of-implementation and use of

``bootstrapped'' return estimates to make efficient use of sampled

data. In particular, TD($\lambda$) methods comprise a family of

reinforcement learning algorithms that often yield fast convergence by

averaging multiple estimators of the expected return. However,

TD($\lambda$) chooses a very specific way of averaging these

estimators based on the fixed parameter $\lambda$, which may not lead

to optimal convergence rates in all settings. In this paper, we

derive an automated Bayesian approach to setting $\lambda$ that we

call temporal difference Bayesian model averaging (TD-BMA).

Empirically, TD-BMA always performs as well and often \emph{much}

better than the best

fixed $\lambda$ for TD($\lambda$) (even when performance for different

values of $\lambda$ varies across problems) without requiring

that $\lambda$ or any analogous parameter be manually tuned.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1469</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Ivan Seskar</field>
<field name="title">OMF: A Control and Management Framework for Networking Testbeds</field>
<field name="keyword"/>
<field name="abstract">Networking testbeds are playing an increasingly important role in the development of new communication technologies. Testbeds are traditionally built for a particular project or to study a specific technology. An alternative approach is to federate existing testbeds to a) cater for experimenter needs which cannot be fulfilled by a single testbed, and b) provide a wider variety of environmental settings at different scales. These heterogenous settings allow the study of new approaches in environments similar to what one finds in the real world. This paper presents OMF, a control, measurement, and management framework for testbeds. It describes through some examples the versatility of OMF's current architecture and gives directions for federation of testbeds through OMF. In addition, this paper introduces a comprehensive experiment description language that allows an experimenter to describe resource requirements and their configurations, as well as experiment orchestration. Researchers would thus be able to reproduce their experiment on the same testbed or in a different environment with little changes. Along with the efficient support for large scale experiments, the use of testbeds and support for repeatable experiments will allow the networking field to build a culture of cross verification and therefore strengthen its scientific approach. 



 ----- This work was first accepted for publication and presented at the SOSP Workshop on Real Overlays and Distributed Systems (ROADS '09). It was then selected with other papers from SOSP Workshops to be re-published in the December 2009 issue of ACM Operating Systems Review journal (http://www.sigops.org/osr.html).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1470</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Ronnie Taib</field>
<field name="author">Fang Chen</field>
<field name="title">Cognitive Load Measurement through Multimodal Behaviour Patterns</field>
<field name="keyword">Cognitive load</field>
<field name="keyword"> multimodality</field>
<field name="keyword"> speech and gesture interfaces.</field>
<field name="abstract">Our research focuses on extending the accepted benefits of

multimodal computer interaction by using the paradigm to detect

fluctuations in cognitive load. The advantage of this approach

is that cognitive load can be determined implicitly by

monitoring variations in specific multimodal input features executed

during day to day tasks using a computer interface.

Such unobtrusive measures may help determine the user s cognitive

load in real-time, and achieve the ultimate goal of adapting

information selection and presentation in a dynamic computer

interface with reference to load. We identify some possible

correlations between the communicative structure of combined

speech and manual gesture input and high levels of cognitive

load. The results suggest that semantic multimodal communicative

structures are sensitive to cognitive load variations,

with multimodal communication becoming less redundant in

high load. The feasibility of using rates of redundant constructions

or complementary constructions in multimodal input as

an index of cognitive load is supported by the results of our

study.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1471</field>
<field name="author">Werayut Saesue</field>
<field name="author">Chun Tung Chou</field>
<field name="author">Jian Zhang</field>
<field name="title">CROSS-LAYER QOS-OPTIMIZED SCHEDULING FOR WIRELESS VIDEO STREAMING</field>
<field name="keyword">Cross layer</field>
<field name="keyword"> QoS</field>
<field name="keyword"> optimization</field>
<field name="keyword"> H.264</field>
<field name="keyword"> video</field>
<field name="keyword"> wireless network</field>
<field name="keyword"> 802.11e</field>
<field name="abstract">In this paper, we propose an adaptive cross layer technique that can optimally enhance the robustness and efficiency of QoS video transmission. The proposed optimization algorithm takes into account both limitations in wireless network and video streaming properties by incorporating two analytical models which are video distortion model and DTMC channel estimation model. The first model predicts the video quality in term of average PSNR of all decoded video frames. The second model can estimate the channel throughput and packet loss rates of each MAC layer s queue which is the input to the first model. The optimal scheduling parameters are selected by optimization module based on the information from the analytical models. The accuracy of our optimized scheduling parameters is verified through the extensive simulation results.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1472</field>
<field name="author">Jian Zhang</field>
<field name="author">Weihong Wang</field>
<field name="author">Chunhua Shen</field>
<field name="title">Improved Human Detection and Classification in Thermal Surveillance Systems</field>
<field name="keyword">Human Detection</field>
<field name="keyword"> thermal image</field>
<field name="keyword"> Shape Context</field>
<field name="keyword"> Adaboost supervised learning</field>
<field name="abstract">We present a new method for detecting human in widely varying thermal images. The method is based on the Shape Context Descriptor through Adaboost Cascade Framework. Compared to normal optical image, Thermal image camera offers an obvious advantage to nighttime surveillance and robustness to the light changes in daytime. Experiments show that such feature based descriptor approach provides significant improvement on edge detection and discriminated feature description for human detection in thermal images. In this paper, we have also evaluated our proposal method by comparing with rectangle feature based method (similar to Haar feature) using public dataset of thermal imagery [4].</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1473</field>
<field name="author">Michael Maher</field>
<field name="title">Contractibility and Contractible Approximations of Soft Global Constraints</field>
<field name="abstract">We investigate contractibility and contractible approximations of soft open constraints, that is, soft constraints in the sense of Petit et al

that are dynamic, or open, in the sense of Bart\'{a}k. We investigate two general classes of soft constraints based, respectively, on decompositions and edit-distance. We improve on several previous results and establish new results for these classes. While hard constraints seem to be amenable to contractible approximation, at least in the cases studied so far, we show that soft constraints are much less so.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1474</field>
<field name="author">Jun Yang</field>
<field name="author">Yang Wang</field>
<field name="author">Arcot Sowmya</field>
<field name="author">Zhidong Li</field>
<field name="title">VEHICLE DETECTION AND TRACKING FROM LOW-ANGLE CAMERAS</field>
<field name="keyword">Object detection</field>
<field name="keyword"> Tracking</field>
<field name="keyword"> Surveillance</field>
<field name="abstract">Vision-based vehicle detection is a critical task for traffic monitoring in modern Intelligent Traffic Systems (ITS). Due to the low-angle nature of most traffic surveillance cameras installed in the real world, vehicle detection in such case has to deal with one fundamental challenge occlusion, which renders most traditional vehicle detection methods ineffective. In this paper, instead of detecting the vehicle as a whole, we propose a vehicle detection algorithm based on windshield model matching. By detecting windshield directly, the algorithm achieves robustness to occlusion. Together with camera calibration and vehicle tracking, the system is able to provide reliable traffic state estimation. Experiments on real traffic videos demonstrate the better performance of our system compared to the state-of-the-art algorithm.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1475</field>
<field name="author">Glenn Geers</field>
<field name="author">Paul Tyler</field>
<field name="author">Chong-White Christian</field>
<field name="author">Johnson Fraser</field>
<field name="title">Roundabout Metering: Simulation and Reality</field>
<field name="keyword"/>
<field name="abstract">Simulations are widely used to judge the efficacy of proposed traffic control system implementations yet the performance of deployed systems with previous simulations is rarely documented. In this paper the simulated performance of a metered roundabout is compared to the measured performance achieved by the deployed system.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1476</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Li Cheng</field>
<field name="author">Jian Zhang</field>
<field name="author">Li Wang</field>
<field name="title">IMPLICIT MOTION-SHAPE MODEL: A GENERIC APPROACH FOR ACTION MATCHING</field>
<field name="keyword">Video Content Retrieval</field>
<field name="keyword"> Action Recognition</field>
<field name="keyword"> Implicit Motion-Shape Model</field>
<field name="keyword"> Motion History Image</field>
<field name="abstract">We develop a robust technique to search for videos having

similar motion patterns. Using a query video, we construct

a motion history image (MHI) of the main action taken inside

the search region. Dividing the MHI into concise spacetime

regions allows us to analyze the action as a dynamic 3D

structure of sparse motion patches. We adopt the idea of Generalized

Hough Transform to integrate statistics of all those

motion shapes into an Implicit Model, which describes the

dynamic characteristics of the query action. Motion segments

retrieved in the same way from video candidates are projected

onto the Hough hyperspace of the query model. Matching

scoring is then derived by running Parzen window density

estimation under different scales. Empirical results obtained

from KTH andWeizmann datasets have proven the efficiency

of this approach, returning highly accurate matches within acceptable

processing time. In addition, the nonparametric nature

of this modeling algorithm makes it highly generic and

adaptive to various applications in video search.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1477</field>
<field name="author">Yongbin Zheng</field>
<field name="author">Chunhua Shen</field>
<field name="title">Pedestrian Detection Using Center-symmetric Local Binary Patterns</field>
<field name="keyword">Pedestrian detection</field>
<field name="keyword"> center-symmetric local binary pattern</field>
<field name="keyword"> histogram of oriented gradients</field>
<field name="abstract">Detecting pedestrians in images plays a very important role in many computer vision applications such as video surveillance, smart cars and robotics. Feature extraction is the key for this task. Promising features should be discriminative, robust and easy to compute. This paper presents a novel and efficient feature, termed dense center-symmetric local binary patterns (CS-LBP), for pedestrian detection. The CS-LBP feature combines the desirable properties of the standard LBP, which can be viewed as texture-based features, and the gradient based feature. Moreover, CS-LBP is easy-to-implement and computationally efficient. Experiments on the INRIA pedestrian dataset show that the proposed feature outperforms the start-of-the-art Histograms of Oriented Gradients (HOG) feature. Our experiments also show that the combination of multiple features that focus on different low-level image statistics such as shape (e.g. HOG) and texture (e.g. the standard LBP) could improve the detection performance.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1478</field>
<field name="author">Guillaume Jourjon</field>
<field name="author">Thierry Rakotoarivelo</field>
<field name="author">Max Ott</field>
<field name="title">From Learning to Researching - Ease the shift through testbeds</field>
<field name="abstract">This papers presents an enhancement of a previously introduced e-learning platform. This update is based on the feedbacks from students and teachers who used it and the integration of a state of the art management framework. This enhancement allows three main improvements to the previously introduced networking platform. First, this allows an easier transition from basic to advanced student. Second, development new experiments is facilitated by the use of a robust and modular management framework. Third, this integration is taking part in the current federation and uni*cation process of testbeds around the world. Furthermore, in parallel to the enhancement of the e-learning platform, we integrated a new emulation module in the testbed management framework. This new module allows to extend the spectrum of possible topologies on a given testbed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1479</field>
<field name="author">Chunhua Shen</field>
<field name="author">Hanxi Li</field>
<field name="title">Robust real-time visual tracking with compressed sensing</field>
<field name="keyword">tracking</field>
<field name="keyword"> compressed sensing</field>
<field name="abstract">Robust real-time visual tracking is

 a challenging problem due to the presence of noise, occlusion and

 variation of illumination. Compressed sensing based tracker,

 {\em a.k.a.} $\ell_1$ minimization tracking, could

 overcome these difficulties to a remarkable extend. Nonetheless,

 the heavy computational complexity of linear programming causes a very

 poor

 efficiency of compressed sensing based trackers. We propose a

 modified compressed sensing tracker, which is termed Real-Time

 Compressed Sensing Tracking (RTCST), to strike the balance between

 efficiency and accuracy. Hash kernels yield a hash-matrix that is

 employed to reduce the dimensionality of observation. The

 optimization problem is solved via Orthogonal Matching Pursuit

 rather than standard interior-point approaches.

 With these two modifications, the proposed algorithm

 could achieve a real-time speed (which is up to $1000$ times

 faster than that of the $\ell_1$ minimization tracking in 

 \cite{Mei2009Robust})

 while still

 inherits promising robustness from the original compressed sensing

 based tracker. Experiments on video sequences show the excellence 

 of the proposed tracker.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1480</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="author">Edwin Hancock</field>
<field name="title">Robust Shape from Polarisation and Shading</field>
<field name="keyword">shape recovery</field>
<field name="keyword"> polarisation</field>
<field name="keyword"> robust statistics</field>
<field name="keyword"> hyperspectral imagery</field>
<field name="keyword"> illumination and reflectance modeling</field>
<field name="abstract">In this paper, we present an approach to robust estimation

of shape from single-view multi-spectral polarisation

images. The developed technique tackles the

problem of recovering the azimuth angle of surface normals

robust to image noise and a low degree of polarisation.

We note that the linear least-squares estimation

results in a considerable phase shift from the ground

truth in the presence of noise and light polarisation in

multispectral and hyperspectral imaging. This paper

discusses the utility of robust statistics to discount the

large error attributed to outliers and noise. Combining

this approach with Shape from Shading, we fully

recover the surface shape. We demonstrate the effectiveness

of the robust estimator compared to the linear

least-squares estimator through shape recovery experiments

on both synthetic and real images.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1481</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">Hyperspectral Imaging for Skin Recognition and Biometrics</field>
<field name="keyword">multi-spectral imaging</field>
<field name="keyword"> hyper-spectral imaging</field>
<field name="keyword"> biometrics</field>
<field name="keyword"> reflectance</field>
<field name="keyword"> skin recognition</field>
<field name="keyword"> NURBS</field>
<field name="abstract">In this paper, we present a system for automatic spectral signature

acquisition and recognition of skin from hyperspectral

face imagery. In the acquisition step, hyperspectral cameras

are used to capture multispectral or hyperspectral images of

faces for skin recognition. The acquired signature may either

be stored in a database for future testing or be used for

purposes of identification. In the recognition step, the system

accounts for variations in the illumination by recovering

the light power spectrum in the scene and obtains the

scene reflectance by normalising the input image radiance

accordingly. Incorporated into this system is a spline-based

compact descriptor of spectral reflectance for recognition purposes.

We have employed this system as a profiling tool to

classify a real-world multispectral face image database into

separate ethnic groups.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1482</field>
<field name="author">Ralf Huuck</field>
<field name="author">Brauer Joerg</field>
<field name="author">Schlich Bastian</field>
<field name="title">Interprocedural Pointer Analysis in Goanna</field>
<field name="abstract">Goanna is an industrial-strength static analysis tool used in academia and industry alike to find bugs in C/C++ programs. Unlike existing approaches, Goanna uses the off-the-shelf model checker NuSMV as its core analysis engine on a syntactic flow-sensitive program abstraction. The CTL-based model checking approach enables a high degree of flexibility in writing checks and scales to large code bases. In this paper, a new approach to pointer analysis for C is described. It is detailed how this technique is integrated into the model checking approach in order to perform interprocedural analysis. The performance and precision of this approach are demonstrated using a case study.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1483</field>
<field name="author">Gil Chamiel</field>
<field name="author">Maurice Pagnucco</field>
<field name="title">Ontology Guided Dynamic Preference Elicitation</field>
<field name="abstract">A challenge for preference based recommender systems is to elicit user preferences in an accurate and efficient manner. Eliciting preferences from the user in the form of a query that is then used to filter items from a database can result in a coarse recommendation with numerous results returned. The problem lies in the user's knowledge concerning the items among which they are searching. Unless the user is a domain expert, their preferences are likely to be expressed in a vague manner and so vague results (in the form of irrelevant alternatives) are returned. On the other hand, the advent of the world wide web has delivered an abundance of data at our fingertips. Information gathered from the web, reduced to structured ontologies, can prove useful in focussing preference elicitation mechanisms.



In this paper we present a preference elicitation process which allows users to communicate their preferences in a simple manner, through examples presented to them. The system then makes use of an ontology model, based on expert information and social web resources. It elicits the user's preferences guided by this ontology in an interactive and dynamic manner. We show that this leads to more effective recommendations.



We evaluate our work through empirical experiments and discuss the results in terms of preference elicitation coverage as well as the prediction accuracy of the preference model.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1484</field>
<field name="author">Walid Dabbous</field>
<field name="author">Max Ott</field>
<field name="title">Overview of the ROADS'09 workshop</field>
<field name="abstract">This report presents a brief overview of the organization and the program of the fourth Real Overlays and Distributed Systems Workshop ROADS'09 co-located with the 22nd SOSP conference at Big Sky, Montana.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1485</field>
<field name="author">Dino Lopez</field>
<field name="author">Emmanuel Lochin</field>
<field name="author">Golam Sarwar</field>
<field name="author">Roksana Boreli</field>
<field name="title">Understanding the Impact of TFRC Feedbacks Frequency over Long Delay Links</field>
<field name="keyword">TFRC</field>
<field name="keyword"> DCCP</field>
<field name="keyword"> transport</field>
<field name="abstract">TFRC is a transport protocol specifically designed to carry multimedia streams. TFRC does not enable a reliable and in order data delivery services. However TFRC implements a congestion control algorithm which is friendly with TCP. This congestion control relies in a feedback mechanism allowing receivers to communicate to the senders an experienced drop rate. Although the current TFRC RFC states that there is little gain from sending a large number of feedback messages per RTT, recent studies have shown that in long-delay contexts, such as satellite-based networks, the performance of TFRC can be improved by increasing the feedback frequency. Nevertheless, currently it is not clear how and why this increase may improve the performance of TFRC. Therefore, in this paper, we aim at understanding the impact that multiple feedback per RTT may have (i) on the key parameters of TFRC (RTT and error rate) and (ii) on the network parameters (reactiveness, fairness and link utilization). We also provide a detailed description of the micromechanisms at the origin of the improvements of the TFRC behavior when multiple feedback per RTT are delivered, and determine the context where such feedback frequencies should be applied.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1486</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Protocol Support for Bulk Transfer Architecture</field>
<field name="keyword">Mobility Protocols</field>
<field name="keyword"> Network Interface Selection</field>
<field name="keyword">Data Transfer Scheduling.</field>
<field name="abstract">Today's mobile devices are increasingly containing multiple radios, enabling users on the move to take advantage of the heterogeneous wireless network environment. In addition, we observe that many bandwidth intensive services such as video on demand and software updates are essentially non real-time and buffers in mobile devices are effectively unlimited. We therefore proposed EMUNE, a new transfer service architecture in our previous work which leverages these aspects and supports opportunistic bulk transfers in high bandwidth networks.



EMUNE uses multiple wireless network interfaces according to an optimal transfer schedule derived based on device power concerns, application requirements, user preferences of cost and quality and future network availability. In this paper, we explore the usability of network/transport protocols to achieve the use of multiple network interfaces with required functionalities including flow mobility and striping in mobile environments and propose a MONAMI + R$^2$CP hybrid approach for the architecture.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1487</field>
<field name="author">Tofazzal Hossain</field>
<field name="author">David Smith</field>
<field name="title">Data and Channel Aided Symbol Timing Estimation for Cooperative Communications</field>
<field name="keyword">data-and-channel-aided</field>
<field name="keyword"> timing estimation</field>
<field name="keyword"> cooperative communicaiton</field>
<field name="abstract">A data and channel aided maximum likelihood (DCA-ML) symbol timing estimator for cooperative communications

operating in flat fading channels is presented. The performance of the estimator is evaluated in terms of bit-error-rate (BER) and estimator variance. The BER performance is evaluated in various fading conditions and compared to a non-data-aided near maximum likelihood (NDA-NML) timing estimator and a correlation based data-aided maximum likelihood (DA-ML) timing estimator. For more severe fading the DCA-ML estimator performs better than the NDA-NML estimator and the DA-ML estimator. Three node cooperative communications, detect-and-forward relaying and various combining techniques are considered. The performance gains of the DCA-ML estimator over that of the DA-ML estimator become more significant in cooperative transmissions than single-link node-to-node transmission when operating in Weibull fading and Lognormal fading conditions.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1488</field>
<field name="author">Bathiya Senanayake</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">Iterative Timing Recovery for IDMA Receivers Operating Under Severe Timing Drift</field>
<field name="keyword">IDMA</field>
<field name="keyword"> synchronisation</field>
<field name="keyword"> uplink</field>
<field name="keyword"> timing recovery</field>
<field name="keyword"> turbo synchronisation</field>
<field name="abstract">Recently Interleaved Division Multiple Access

(IDMA) has been proposed as an enhancement for 3GPP wireless

broadband standards. Timing synchronisation in an IDMA

system must take place in conjunction with signal detection

and decoding. This paper compares an iterative timing recovery

technique with a conventional timing recovery approach for

IDMA systems in the uplink. Under high multiple access interference

and very severe timing drift conventional timing recovery

techniques have been shown to result in poor performance. In

conventional multi-user DS/CDMA systems the pilot channel is

used exclusively for single-shot timing synchronisation. In this

paper we propose an iterative timing synchronisation algorithm

which uses soft data channel to aid the synchronisation process.

MMSE combining is used to determine the optimal timing error

discriminator.Decoded information from the previous iteration

is used to cancel interference from the co-channel users before

timing recovery is performed on each iteration. Under severe

drift scenarios, our results show that the new algorithm developed

reduces the timing error variance by a factor of 15 times and,

consequently results in a 2dB gain in bit-error rate at high SNR

as compared with conventional techniques.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1489</field>
<field name="author">Petar Rajkovic</field>
<field name="author">Dragan Jankovic</field>
<field name="author">Tatjana Stankovic</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Software Tools for Rapid Development and Customization of Medical Information Systems</field>
<field name="keyword">electronic health record (EHR)</field>
<field name="keyword"> medical information system (MIS)</field>
<field name="keyword"> code generation</field>
<field name="keyword"> rapid application development</field>
<field name="abstract">We present data modeling and code generation tools for easier and faster development and customization of electronic health record (EHR) based medical information systems (MIS). In development of MIS, it is usually necessary to create a large number of different, but somewhat similar, graphical user interface (GUI) forms and database tables corresponding to different medical data. This process can be inefficient and time consuming. Our software tools enable power users to define meta data models, from which the tools automatically generate database tables, data object model classes and several common components (input forms, selection components, components for tracking measured values, reporting profiles, access control configurations) for EHR-based MIS. While we first developed these tools for clinical and ambulatory MIS in Serbia, they can be used in other countries, due to the built-in flexibility and multi-language support.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1490</field>
<field name="author">Udo Kannengiesser</field>
<field name="author">John S Gero</field>
<field name="title">A Framework for Constructive Design Rationale</field>
<field name="abstract">This paper proposes a framework for describing design rationale as a constructive notion rather than a fixed record of design reasoning. The framework is based on two views: an instance-based view of design rationale as an ordered set of decisions, and a state-space view of design rationale as a space of solution alternatives. The two views are connected with each other using the function-behaviour-structure (FBS) ontology. Constructive design rationale is defined and categorised based on reformulations of the function, behaviour or structure of the rationale. The drivers of the different reformulations are represented in the situated FBS framework.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1491</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Muhammad Ali Babar</field>
<field name="title">On Searching Relevant Studies in Software Engineering</field>
<field name="keyword">Search strategy</field>
<field name="keyword"> quasi-gold standard</field>
<field name="keyword"> systematic literature review</field>
<field name="keyword"> evidence-based software engineering</field>
<field name="abstract">BACKGROUND: Systematic Literature Review (SLR) has become an important research methodology in software engineering since 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is quite time consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need of a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.



OBJECTIVE: The main objective of the research reported in this paper is to improve the search step of doing SLRs in SE by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.



OUTCOMES: We have systematically selected and analytically studied a large number of papers to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLR, we have devised a systematic approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of `quasi-gold standard', which consists of collection of known studies and corresponding `quasi-sensitivity' into the search process for evaluating search performance. We report the case study and its finding to demonstrate that the approach is able to improve the rigor of search process in an SLR, and can serves as the supplements to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using several case studies with varying topics in software engineering.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1492</field>
<field name="author">Lianping Chen</field>
<field name="author">Muhammad Ali Babar</field>
<field name="author">He (Jason) Zhang</field>
<field name="title">Towards an Evidence-Based Understanding of Electronic Data Sources</field>
<field name="keyword">Evidence-Based Software Engineering</field>
<field name="keyword"> systematic literature review; systematic mapping study; primary study selection</field>
<field name="keyword"> search engine selection</field>
<field name="keyword"> database selection</field>
<field name="abstract">Systematic Literature Reviews and Systematic Mapping Studies are relatively new forms of secondary studies in software engineering. Identifying relevant papers from various Electronic Data Sources (EDS) is one of the key activities of conducting these kinds of studies. Hence, the selection of EDS for searching the potentially relevant papers is an important decision, which can affect a study s coverage of relevant papers. Researchers usually select EDS mainly based on personal knowledge, experience, and preferences and/or recommendations by other researchers. We believe that building an evidence-based understanding of EDS can enable researchers to make more informed decisions about the selection of EDS. This paper reports our initial effort towards this end. We propose an initial set of metrics for characterizing the EDS from the perspective of the needs of secondary studies. We explain the usage and benefits of the proposed metrics using the data gathered from two secondary studies. We also tried to synthesize the data from the two studies and that from literature to provide initial evidence-based heuristics for EDS selection.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1493</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Kitchenham Barbara</field>
<field name="author">Dietmar Pfahl</field>
<field name="title">Software Process Simulation Modeling: An Extended Systematic Review</field>
<field name="abstract">Software Process Simulation Modeling (SPSM) research has increased in the past two decades, especially since the first ProSim Workshop held in 1998. Our research aims to systematically assess how SPSM has evolved during the past 10 years in particular whether the purposes for SPSM, the simulation paradigms, tools, research topics, and the model scopes and outputs have changed. We performed a systematic literature review of the SPSM research in two subsequent stages, and identified 156 relevant studies in four categories. This paper reports the review process of the second stage and the preliminary results by aggregating studies from the two stages. Although the load of SPSM studies was dominated in ProSim/ICSP community, the outside research presented more diversity in some aspects. We also perceived an immediate need for refining and updating the reasons and the classification scheme for SPSM introduced by KMR.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1494</field>
<field name="author">Marcus Hutter</field>
<field name="author">W. Merkle</field>
<field name="author">P.M.B. Vitanyi</field>
<field name="title">Kolmogorov Complexity and Applications</field>
<field name="keyword">Information theory</field>
<field name="keyword"> Kolmogorov Complexity</field>
<field name="keyword"> effective randomness</field>
<field name="keyword"> algorithmic probability</field>
<field name="keyword"> recursion theory</field>
<field name="keyword"> computational complexity</field>
<field name="keyword"> machine learning</field>
<field name="abstract">From 29.01.06 to 03.02.06, the Dagstuhl Seminar 06051 `Kolmogorov Complexity and Applications' was held in the International Conference and Research Center (IBFI), Schloss Dagstuhl. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this proceedings. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1495</field>
<field name="author">Shengbo Guo</field>
<field name="author">Scott Sanner</field>
<field name="title">Probabilistic Latent Maximal Marginal Relevance</field>
<field name="keyword">graphical models</field>
<field name="keyword"> maximal marginal relevance</field>
<field name="abstract">Diversification has been heavily motivated in the literature as an

objective criterion for result sets in search and recommender systems.

Certainly, diversification is important in systems that must hedge

their recommendations w.r.t. uncertainty over ambiguous queries.

However, in this paper, we propose that diversification

should not be viewed as a direct optimization objective, but rather as

a property that naturally arises in many settings from maximizing the accumulated

relevance of the recommendations w.r.t. a reasonable model in a latent

variable setting. To this end, we present the probabilistic maximal marginal relevance to learn appropriate

latent variable models, and maximize the accumulated relevance w.r.t. ambiguous queries

than maximizing surrogate diversification objectives like those often found in the

literature.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1496</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Gordon Plotkin</field>
<field name="title">On CSP and the Algebraic Theory of Effects</field>
<field name="keyword">Concurrency</field>
<field name="keyword"> functional programming</field>
<field name="keyword"> computational lambda-calculus</field>
<field name="keyword"> free algebras</field>
<field name="keyword"> (de)constructors</field>
<field name="keyword"> stable failures model</field>
<field name="abstract">We consider CSP from the point of view of the algebraic

theory of effects, which classifies operations as effect

constructors or effect deconstructors; it also

provides a link with functional programming, being a refinement of

Moggi's seminal monadic point of view. Constructors form the

signature of a natural algebraic theory whose free algebra functor

is Moggi's monad; we illustrate this by characterising free and

initial algebras in terms of two versions of the stable failures

model of CSP, one more general than the other. Deconstructors are

dealt with as homomorphisms to (possibly non-free) algebras.

 One can view CSP's action and choice operations as

constructors and the rest, such as concealment and concurrency, as

deconstructors. Carrying this programme out results in taking

deterministic external choice as constructor rather than general

external choice. However, binary deconstructors, such as the CSP

concurrency operation, provide unresolved difficulties. We conclude

by presenting a combination of CSP with Moggi's computational

lambda-calculus, in which the operations, including concurrency,

are polymorphic. While the paper mainly concerns CSP, it ought to be

possible to carry over similar ideas to other process calculi.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1497</field>
<field name="author">Bojan Djordjevic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="title">Detecting Areas Visited Regularly</field>
<field name="keyword">computational geometry</field>
<field name="keyword"> data mining</field>
<field name="abstract">We are given a trajectory T and an area A. T might intersect A several times,

and our aim is to detect whether T visits A with some regularity,

e.g. what is the longest time span that a GPS-GSM equipped elephant visited a specific lake on a daily (weekly or yearly) basis,

where the elephant has to visit the lake most of the days (weeks or years), but not necessarily on every day (week or year). We call this a regular pattern with period of one day (week or year, respectively).



We consider the most general version of the problem defined in [8], the case where we are not given the period length of the regular pattern but have to find the longest regular pattern over all possible period lengths. We give an exact algorithm with O(n^3.5 log^3 n) running time and an approximate algorithm with O(1/\eps n^3 \log^2 n) running time.



We also consider the problem of finding a region that is visited regularly if one is not given. We give exact and approximate algorithms for this problem when the period length is fixed.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1498</field>
<field name="author">Julian McAuley</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Exploiting Within-Clique Factorizations in Junction-Tree Algorithms</field>
<field name="abstract">We show that the expected computational complexity of the Junction-Tree Algorithm for maximum a posteriori inference in graphical models can be improved. Our results apply whenever the potentials over maximal cliques of the triangulated graph are factored over subcliques. This is common in many real applications, as we illustrate with several examples. The new algorithms are easily implemented, and experiments show substantial speed-ups over the classical Junction-Tree Algorithm. This enlarges the class of models for which exact inference is efficient.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1499</field>
<field name="author">Mark Reid</field>
<field name="author">Bob Williamson</field>
<field name="title">Convexity of Proper Composite Binary Losses</field>
<field name="keyword">convexity</field>
<field name="keyword"> proper loss</field>
<field name="keyword"> probability estimation</field>
<field name="abstract">A composite loss assigns a penalty to a real-valued prediction by associating the prediction with a probability via a link function then applying a class probability estimation (CPE) loss. If the risk for a composite loss is always minimised by predicting the value associated with the true class probability the composite loss is proper. We provide a novel, explicit and complete characterisation of the convexity of proper composite losses in terms 

of its link and the weight function associated with its proper CPE loss.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1500</field>
<field name="author">Pranam Janney</field>
<field name="author">Glenn Geers</field>
<field name="title">IFLT based real-time framework for image-matching</field>
<field name="keyword">image</field>
<field name="keyword">matching</field>
<field name="keyword">IFLT</field>
<field name="keyword"> SIFT</field>
<field name="keyword"> texture</field>
<field name="keyword"> real-time</field>
<field name="abstract">In this paper we show that the features generated by the recently presented Invariant Features of Local

Textures (IFLT) technique can be used in a SIFT like framework to deliver real-time pointwise image matching

with performance comparable to existing state-of-the- art image matching systems. The proposed framework

is also capable of saving considerable amount of computation time.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1501</field>
<field name="author">Novi Quadrianto</field>
<field name="author">Kristian Kersting</field>
<field name="author">Tinne Tuytelaars</field>
<field name="author">Wray Buntine</field>
<field name="title">Beyond 2D-Grids: A Dependence Maximization View on Image Browsing</field>
<field name="abstract">Ideally, one would like to perform image search using an intuitive and friendly approach. Many existing image search engines, however, present users with sets of images arranged in some default order on the screen, typically the relevance to a query, only. While this certainly has its advantages, arguably, a more flexible and intuitive way would be to sort images into arbitrary structures such as grids, hierarchies, or spheres so that images that are visually or semantically alike are placed together. This paper focuses on designing such a navigation system for image browsers. This is a challenging task because arbitrary layout structure makes it difficult -- if not impossible -- to compute cross-similarities between images and structure coordinates, the main ingredient of traditional layouting approaches. For this reason, we resort to a recently developed machine learning technique: kernelized sorting. It is a general technique for matching pairs of objects from different domains without requiring cross-domain similarity measures and hence elegantly allows sorting images into arbitrary structures. Moreover, we extend it so that some images can be preselected for instance forming the tip of the hierarchy allowing to subsequently navigate through the search results in the lower levels in an intuitive way.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1502</field>
<field name="author">M.A.Hakim Newton Newton</field>
<field name="author">John Levine</field>
<field name="title">Implicit Learning of Compiled Macro-Actions for Planning</field>
<field name="abstract">We build a comprehensive macro-learning system and contribute in three different dimensions that have previously not been addressed adequately. Firstly, we learn macro-sets considering implicitly the interactions between constituent macros. Secondly, we effectively learn macros that are not found in given example plans. Lastly, we improve or reduce degradation of plan-length when macros are used; note, our main objective is to achieve fast planning. Our macro-learning system significantly outperforms a very recent macro-learning method both in solution speed and plan length.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1503</field>
<field name="author">Jasmine Croll</field>
<field name="author">Penelope Sanderson</field>
<field name="title">Workshop for HIC2010: The Procurement Process for Health Information Systems</field>
<field name="keyword"/>
<field name="abstract">Abstract/Outline of workshop

Increasingly significant investments are being made in the roll out of the electronic medical record. The progress of the National Program for Information Technology (NPfIT) in the United Kingdom with its spiralling costs of 12.7 billion (to date) is an example of the huge costs involved in the implementation of HIS. Procurement is vital to ensure that the right system is implemented at the right cost for the right purpose. 

Procurement is based on a long tradition of established processes of the supply chain. Much information exists on how to procure goods and services. Clearly defined steps from information gathering, requests for quotations, negotiation, implementation, and renewal are available on government websites. The Australian Government has the Commonwealth Procurement Guidelines (CPG) with transparent and clear processes. The Guidelines to ICT procurement (GITC) is another initiative by the Australian Government. Health ICT systems procurement follows the GITC. 

No standard or protocol exists specifically for the procurement of health ICT. The question arises whether the above guidelines for generic ICT systems procurement are also applicable to health ICT systems. Heath is complex and fraught with potential problems. Errors that occur in financial or retail ICT can result in job losses but errors in health ICT can end in irreversible breaches; death. Health ICT involves disparate systems, applications and methods. Interoperability is crucial for the successful implementation of a health information system. Yet it is this lack of interoperability that is the everyday reality in healthcare; within single organisations like a hospital, between different healthcare providers such as hospitals and general practitioners, within regional and national health systems, and not least in international healthcare.

This workshop will give attendees the opportunity to review and discuss the state of procurement of health information systems in Australia and overseas. Invited workshop presenters and selected attendees will identify and address the needs and requirements of people involved in the choice and purchase of health information systems. Our particular focus is whether end-user needs are adequately represented in the procurement process.

Anecdotal evidence indicates that some HIS procurement processes result in upgrades to legacy systems that do not specifically meet the requirements of the organisation or the users. Vendors can be persuasive about their products, but the reasons they provide and examples they use to justify purchase may not be relevant for the organisation concerned. For example, a system may be very successful in one workplace but may fail in another due to differences in organisational routines, work practices and the clinicians concerned. 

A question for the workshop is whether a standardised protocol of Health Information System Procurement could facilitate the purchase of HIS that fit their users needs. A topic for discussion is whether such a protocol is feasible, and whether it could reduce costs and increase the efficiency, safety and quality of health outcomes.

The invited workshop presenters have experience that ranges from procurement of state enterprise level systems to individual purpose built systems. 

Planned duration (90 min or 3 hour duration)	

Three hours duration, divided into presentation, breakout discussions, and plenary summary discussions.

Target audience

Procurers of health information systems, health service/information managers and academics/researchers will be the target audience. Twenty workshop members will be selected on the basis of a one- to two-page (maximum) position paper that will be circulated to other selected participants. A special issue of a journal could be one outcome of the workshop.

Content level / Assumed knowledge and experience of participants

Intermediate to experienced in procurement of health information systems.

Maximum number of participants

20 participants.

Short biographies (&lt;50 words) of invited presenters

Mark Barnet is a founding partner of iCare Solutions which now dominates the Aged Care industry. He now runs an advisory business focused on assisting Aged and Community Care providers select their technology mix. Mark is also the founder of the IT: informer an important technology resource.

Elaine Lacey is the Assistant Manager in Queensland Health. She manages high value, high risk ICT procurement and supports the unit Manager in mentoring and supervising staff. Her employment background covers a range of industries in private and public organisations in business and contract management. 

Katerina Andronis is the Director in Health Practice for Deloitte with more than 25 years experience in information systems. She has expertise in all aspects of Health IS. Katerina is also an industry advisor and a member of the HISA board.

Short biographies (&lt;50 words) of workshop organisers

Jasmine Croll is a researcher with NICTA working on the prospective evaluation of ICT in health. Her areas of expertise include clinician acceptance of technology and usability. Jasmine has many years of experience as a researcher and educator.

Penelope Sanderson is Project Leader of NICTA s Cognitive and Organisational Systems Engineering project, and she also leads the Prospective ICT Evaluation project. She is Professor of Cognitive Engineering and Human Factors at The University of Queensland with appointments in ITEE, Psychology, and Medicine.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1504</field>
<field name="author">Jasmine Croll</field>
<field name="author">Penelope Sanderson</field>
<field name="title">Procurement of Health Information Systems; A review of current practice</field>
<field name="keyword"/>
<field name="abstract">Abstract: Managing changes in health outcomes depends upon using information and communication technology (ICT) effectively. The increasing life expectancy of the population and the increasing range of medical interventions and treatments available are putting pressure on current healthcare practices. Health Information Systems (HIS) are now needed at all levels of healthcare, from administration to supporting clinicians in decision making. The procurement of HIS becomes crucial to ensure the right system is purchased and implemented. Procurement itself is a well-established area with a long history and tradition going back centuries. In some jurisdictions, guidelines exist for government procurement for systems ranging from devices to ICT systems. The question arises whether the guidelines can be used effectively for HIS procurement, where other issues may come in to play regarding the choice and selection of systems. 

This paper reviews literature on the current state of procurement and examines the following five issues: (1) the definition of procurement and its phases, (2) how the procurement process is carried out in the United States, the European Union and Australia, (3) ICT procurement, and (4) HIS procurement.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1505</field>
<field name="author">Matt Thompson</field>
<field name="author">Jason Tangen</field>
<field name="author">Renee Treloar</field>
<field name="author">Kathleen J. Ivison</field>
<field name="title">Humans matching fingerprints: Sequence and Size</field>
<field name="keyword">Cognition</field>
<field name="keyword"> Perception</field>
<field name="keyword"> Decision Making</field>
<field name="keyword"> Fingerprints</field>
<field name="keyword"> Expertise</field>
<field name="abstract">Television shows like CSI can give the impression that matching crime-scene fingerprints is fully automated. But it is actually humans (fingerprint experts) who ultimately decide whether a crime-scene print belongs to a suspect or not. Despite this fact, there have been no published, peer-reviewed studies directly examining the extent to which experts can correctly match fingerprints to one another. In two experiments presented here we aim to determine the factors affecting accuracy using non-expert participants and test (1) whether the advantage found for the sequential presentation of faces applies to prints and (2) whether the amount of information in a print matters.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1506</field>
<field name="author">Morgan Tear</field>
<field name="author">Matt Thompson</field>
<field name="author">Jason Tangen</field>
<field name="title">The importance of ground truth: An open-source biometric</field>
<field name="keyword">Biometric</field>
<field name="keyword"> repository</field>
<field name="keyword"> database</field>
<field name="keyword"> forensic</field>
<field name="keyword"> ground truth</field>
<field name="abstract">Advances in forensic technologies and procedures are designed to produce more ef cient policing for 

safer nations, but little is understood about how humans can interact with them effectively. Speci cally, research into the judgement and decision-making processes that forensic examiners undertake when evaluating evidence can highlight how best to implement new technology and procedures. For this research experimenters need access to two types of stimuli; that where the origin of the stimuli is fact, and also that which approximates the uncertainty of real-world forensic stimuli. We discuss the development of an open-source biometric repository where ground truth of stimuli is built into the system. We intend for this repository to be a tool for research into forensic judgement and decision-making. Our repository contains ngerprints and palm-prints, shoe-prints, faces, handwriting, voices, and irises.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1507</field>
<field name="author">Fawad Nazir</field>
<field name="author">Ken-ichi Kawarabayashi</field>
<field name="title">Message Duplication Reduction in Dense Mobile Social Networks</field>
<field name="keyword"/>
<field name="abstract">Abstract In this paper we take a special case of Mobile Social

Networks (MSNs) in which individuals with similar interests or

commonalities connect to each other in close proximity using the

mobile phones (i.e. using Bluetooth or Ad-hoc mode of wireless).

This special case is assumed in this paper in-order to filter

out enormous of data that has potential to be exchanged. We

assume that people only want to exchange profile, exchange data,

get opinions, trust information, take recommendations, event

notification, news etc from the people they meet (i.e. encounter/reencounter)

regularly. These type of MSN s are similar to Disruption

Tolerant Networks (DTNs) expect that in MSN s the

assumption is that individuals follow predictable working day

movement model [1]. The current message forwarding protocols

for dense DTN/MSN can experience 94% of duplicate messages in

the worst-case because the messages are replicated to be routed

over multiple delivery paths in-order to optimize the probability

of successful message delivery. These approaches do (statistically)

guarantee delivery probability but impose overheads on bandwidth,

energy and memory. Therefore, in this paper we propose

a duplication reduction algorithm for message delivery in dense

MSN by exploiting the predictability property of MSN s. We

model our problem as an online graph and solve the problem

of message duplication reduction by an algorithm on the wellknown

spanning tree problem in an online graph. Furthermore,

we divide our problem into three optimisation problems and

prioritise them as minimize message delivery time, minimize

message duplication and minimize message storage space. We

verify our approach by conducting two experiments with 15

mobile users carrying imate Kjam, HTC tytn and Dopod phone

for 180 days and 15 users playing a game called Discrete

Encounters for 60 hours. Discrete Encounters is a game with

a purpose to model mobile social networks. The heuristics and

initial study presented in this paper achieve message duplication

reduction of 71:8% in the best-case (i.e. selecting social users

as data disseminators and many to many ([*,*]) data delivery)

and 14:8% in the worst case (i.e. selecting random users as

data disseminators and one to one ([1,1]) data delivery) while

guaranteeing a given delay or delivery probability and lowering

system wide traffic flooding.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1508</field>
<field name="author">Haifeng Zhao</field>
<field name="author">Jun Zhou</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Structured Learning Approach to Attributed Graph Embedding</field>
<field name="abstract">n this paper, we describe the use of concepts from structural and statistical pattern recognition for

recovering a mapping which can be viewed as an operator on the graph attribute-set.

This mapping can be used to embed graphs into spaces where tasks such as

categorisation and relational matching can be effected.

We depart from concepts in graph theory to

introduce mappings as operators over graph spaces. This treatment leads to

the recovery of a mapping based upon the graph attributes which is

related to the edge-space of the graphs under study. As a result, this mapping

is a linear operator over the attribute set which is associated with the graph topology.

To do so, we employ an optimisation approach whose cost function is

related to the target function used in discrete Markov Random Field approaches.

Thus, the proposed method provides a link between concepts in graph

theory, statistical inference and linear operators. We illustrate the utility of

the recovered embedding for shape matching and categorisation on

MPEG7 CE-Shape-1 dataset. We also compare our results to those yielded by alternatives.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1509</field>
<field name="author">Yan Shvartzshnaider</field>
<field name="title">Global Semantic Graph as an Alternative Information and Collaboration Infrastructure</field>
<field name="abstract">We propose the development of a Global Semantic Graph (GSG) as the foundation for future information and collaboration-centric applications and services. It would provide a single abstraction for storing, processing and communicating information based on globally interlinked semantic resources. The GSG adopts approaches and methods from the Semantic Web and thus facilitates a better information sharing abstraction.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1510</field>
<field name="author">Matt Thompson</field>
<field name="author">Jason Tangen</field>
<field name="author">Kathleen J. Ivison</field>
<field name="author">Renee Treloar</field>
<field name="title">Expertise in matching fingerprints and faces</field>
<field name="keyword">Cognition</field>
<field name="keyword"> Perception</field>
<field name="keyword"> Decision Making</field>
<field name="keyword"> Fingerprints</field>
<field name="keyword"> Expertise</field>
<field name="abstract">The TV show CSI gives viewers the impression that matching crime-scene fingerprints is fully automated. But it s actually humans (fingerprint experts) who ultimately decide whether a crime-scene print belongs to a suspect or not. Despite this fact, there have been no published, peer-reviewed studies directly examining the extent to which experts can correctly match fingerprints to one another. In two experiments presented here we aim to determine the factors affecting accuracy using non-expert participants and test (1) whether the advantage found for the sequential presentation of faces applies to prints and (2) whether the amount of information in a print matters.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1511</field>
<field name="author">Ihor Kuz</field>
<field name="author">Gerwin Klein</field>
<field name="author">Corey Lewis</field>
<field name="author">Adam Christopher Walker</field>
<field name="title">capDL: A Language for Describing Capability-Based Systems</field>
<field name="keyword">OS</field>
<field name="keyword"> security</field>
<field name="keyword"> capabilities</field>
<field name="keyword"> language</field>
<field name="abstract">Capabilities provide an access control model that can be used to construct systems where safety of protection can be precisely determined. However, in order to be certain of the security provided by such systems it is necessary to verify that their capability distributions do in fact fulfil requirements relating to isolation and information flow, and that there is a direct connection to the actual capability distribution in the system. We claim that, in order to do this effectively, systems need to have explicit descriptions of their capability distributions. In this paper we present the capDL capability distribution language for the capability-based seL4 microkernel. We present the key language features and their motivations, and provide a small example to illustrate the language syntax and semantics.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1512</field>
<field name="author">Andrey Kan</field>
<field name="author">James Bailey</field>
<field name="author">Chris Leckie</field>
<field name="author">John Markham</field>
<field name="author">Mark Dowling</field>
<field name="author">Rajib Chakravorty</field>
<field name="title">Automated and semi-automated cell tracking: addressing portability challenges</field>
<field name="keyword">Cell tracking</field>
<field name="keyword"> gating distance</field>
<field name="keyword"> frame selection</field>
<field name="keyword"> interframe assignment</field>
<field name="keyword"> semi-automated tracking</field>
<field name="abstract">Abstract Cell tracking is a key task in the high-throughput quantitative study of important biological processes, such as immune system regulation and neurogenesis. Variability in cell density and dynamics in different videos, hampers portability of existing trackers across videos. We address these potability challenges in order to develop a portable cell tracking algorithm. Our algorithm can handle noise in cell segmentation as well as divisions and deaths of cells. We also propose a parameter-free variation of our tracker. In the tracker, we employ a novel method for recovering the distribution of cell displacements. Further, we present a mathematically justified procedure for determining the gating distance in relation to tracking performance. For the range of real videos tested, our tracker correctly recovers on average $96\%$ of cell moves, and outperforms an advanced probabilistic tracker when the cell detection quality is high. The scalability of our tracker was tested on synthetic videos with up to $200$ cells per frame. For more challenging tracking conditions, we propose a novel semi-automated framework that can increase the ratio of correctly recovered tracks by $12\%$, through selective manual inspection of only $10\%$ of all frames in a video.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1513</field>
<field name="author">Leonid Ryzhyk</field>
<field name="author">Yanjin Zhu</field>
<field name="author">Gernot Heiser</field>
<field name="title">The Case for Active Device Drivers</field>
<field name="keyword">device drivers</field>
<field name="keyword"> reliability</field>
<field name="abstract">We revisit the device-driver architecture supported by the majority of operating systems, where a driver is a passive object that does not have its own thread of control and is only activated when an external thread invokes one of its entry points. This architecture complicates driver development and induces errors in 

two ways. First, since multiple threads can invoke the driver concurrently, it must take care to synchronise the invocations to avoid race conditions. Second, since every invocation occurs in the context of its own thread, the driver cannot rely on programming-language constructs to maintain its control flow. 



To address these issues, we propose a device-driver architecture where each driver has its own thread of control and communicates with other threads in the system via message passing. We show how this architecture addresses both of the above problems. Unlike previous message-based driver frameworks, it does not require any special language support and can be implemented inside an existing operating system as a kernel extension. We present our Linux-based implementation in progress and report on preliminary performance results.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1514</field>
<field name="author">Nicholas Fitzroy-Dale</field>
<field name="author">Ihor Kuz</field>
<field name="author">Gernot Heiser</field>
<field name="title">Architecture optimisation with Currawong</field>
<field name="abstract">We describe Currawong, a tool to perform system soft- ware architecture optimisation. Currawong is an extensible, multi-language tool which applies optimisations across API boundaries. Unlike many existing optimisation tools, Curra- wong does not require source code to perform optimisations. We show, through examples written for the popular Android smartphone platform, that Currawong is capable of signifi- cant performance improvement to existing applications.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1515</field>
<field name="author">Paul Brebner</field>
<field name="author">Anna Liu</field>
<field name="title">Modeling Cloud Cost and Performance</field>
<field name="keyword">Cloud performance</field>
<field name="keyword"> scalability</field>
<field name="keyword"> cost</field>
<field name="keyword"> limits</field>
<field name="keyword"> quotas</field>
<field name="keyword"> service-oriented performance modeling (SOPM)</field>
<field name="keyword"> Amazon EC2</field>
<field name="keyword"> Google AppEngine</field>
<field name="keyword"> Microsoft Azure.</field>
<field name="abstract">Architecting applications for the Cloud is challenging due to significant differences between Cloud infrastructures, and Cloud performance and scalability limitations. Clouds claim to offer benefits in terms of cost and elasticity under some situations, but may introduce significant risks which need to be managed. Building workable cloud applications therefore requires in-depth insight into the architectural and performance characteristics of each cloud offering, and the ability to reason about tradeoffs and alternatives of application designs and deployments. 

NICTA has developed a Service Oriented Performance Modeling technology for modeling the performance and scalability of Service Oriented applications architected for a variety of platforms. Using a suite of cloud testing application we conducted in-depth empirical evaluations of a variety of real cloud infrastructures, including Google App Engine, Amazon EC2, and Microsoft Azure. The insights from these experimental evaluations, and other public/published data, were combined with the modeling technology to predict the resource requirements and cost, performance, and limits of a realistic application for different deployment scenarios. We conclude with some architectural lessons learnt.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1516</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Ronnie Taib</field>
<field name="author">Fang Chen</field>
<field name="title">Geometric Features of Pen Input as a Proxy for Cognitive Load</field>
<field name="abstract">High cognitive load induced by complex, time-pressured tasks have been found to impact performance and multimodal behaviour. Monitoring specific interaction features can potentially detect cognitive load fluctuations. In this paper, we present a study that induced controlled levels of load and collected speech and pen inputs. In our within subjects design, eight participants completed four individual sessions, each with low, medium and high load tasks. We hypothesised that as cognitive load increased, the pen-shapes would degenerate from the standard form of the intended shape the baseline for that user. Though subjective ratings increase as cognitive load increases (Friedman's X2(4,2)=8.00, p&lt;0.02), while the performance scores show no significant linear trend as cognitive load increases (F(1,7)=0.032, p=.622), posing an interesting discussion about the correlation of performance and cognitive load. 

 The analysis of a series of 11 geometric features extracted from the interactive pen input trajectories using a measure called the Malahanobis distance (MDIST), reveals significant degeneration of shapes from the standard form, increasing linearly as cognitive load increases (F(1,5)=6.96, p=.046). The limited capacity of working memory is suggested as a reason why the quality and performance of a subject might degrade as tasks become more complex, including signal level changes in speech and writing. The advantage of this approach is that it does not analyse the semantic content of the pen input, but rather the quality of the signal production at a geometric level. The results show that use of geometric features as an index of cognitive load is viable in this scenario, and may be implemented as automated pen input analysis system.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1517</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Ronnie Taib</field>
<field name="author">Fang Chen</field>
<field name="title">Scratchpad Usage as an Indicator for Cognitive Load</field>
<field name="abstract">A digital scratchpad was used by 8 participants in a study aimed at eliciting natural interactive behaviour under 3 increasing levels of cognitive load, and over 4 sessions, such that participant s expertise increased. The scratchpad was used to jot down numbers, solve arithmetic calculations, and diagram possible solutions and captured digitally with a tablet monitor. Note-taking while problem solving can serve as mnemonic support and visual aid for high cognitive load tasks, as such this could translate into relatively more extensive usage of the scratchpad, as load increases. We hypothesized various aspects of the use of the scratchpad (such as increased markings or the frequency of complex markings such as drawings or symbolic marks) would offer insights into the cognitive load of the subjects completing the tasks, thus serving as a proxy for cognitive load that be assessed automatically. We expected that use of the scratchpad would increase as subjects attempted more complex tasks, and similarly decrease as subjects gained expertise. The results show that subjects used the scratchpad more extensively as cognitive load increased (F(1,7)=22.98, p=.002); however, they did not change the amount of scratchpad usage as their expertise increased (F(3,18)=.328, p=.81). Secondly, we expected the type of scratchpad markings would change in more difficult tasks as participants began to use different types of markings, primarily diagramming and organizational marks. Results show subjects changed the type of marking they used as cognitive load increased: with relatively greater quantity of symbolic markings (Friedman s X2(4,2) = 8.0, p=.018) and a greater quantity of diagrams (Friedman s X2(4,2) = 8.0, p=.02). We suggest the increased use of the scratchpad (both as a visual aid and motor-related activity) serves to cognitively and perceptually integrate and structure task related information further engaging working memory and hence used as a tool to self-manage high cognitive load.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1518</field>
<field name="author">Cong Phuoc Huynh</field>
<field name="author">Antonio Robles-Kelly</field>
<field name="title">A Probabilistic Approach to Spectral Unmixing</field>
<field name="keyword">Image Understanding</field>
<field name="keyword"> Maximum Entropy Principle</field>
<field name="keyword"> Deterministic Annealing</field>
<field name="keyword"> Soft clustering</field>
<field name="keyword"> Spectral Unmixing</field>
<field name="abstract">In this paper, we present a statistical approach to spectral unmixing

with unknown endmember spectra and unknown illuminant power spectrum. The

method presented here is quite general in nature, being applicable to settings

in which sub-pixel information is required. The method is formulated as a simultaneous

process of illuminant power spectrum prediction and basis material

reflectance decomposition via a statistical approach based upon deterministic annealing

and the maximum entropy principle. As a result, the method presented

here is related to soft clustering tasks with a strategy for avoiding local minima.

Furthermore, the final endmembers depend on the similarity between pixel reflectance

spectra. Hence, the method does not require a preset number of material

clusters or spectral signatures as input. We show the utility of our method on

trichromatic and hyperspectral imagery and compare our results to those yielded

by alternatives elsewhere in the literature.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1519</field>
<field name="author">Bipin Gopalakrishna Pillai</field>
<field name="author">Robert Ayre</field>
<field name="author">Kerry Hinton</field>
<field name="author">Li Jiang</field>
<field name="author">Rod Tucker</field>
<field name="author">An Tran</field>
<field name="title">Automated Path Identification for Node Aggregation in Backhaul Networks</field>
<field name="keyword">national broadband network</field>
<field name="keyword"> network design</field>
<field name="keyword"> automation</field>
<field name="abstract">We propose a novel technique that uses information on node connectivity from geographical information systems for identifying paths for aggregating ethernet nodes and hence, is more realistic than node clustering based on position coordinates.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1520</field>
<field name="author">Udo Kannengiesser</field>
<field name="title">Towards a Methodology for Flexible Process Specification</field>
<field name="abstract">This paper proposes the basics of a methodology for specifying process flexibility based on a view of processes as design objects. It is represented using the function-behaviour-structure (FBS) ontology of designing. The paper shows how the FBS ontology allows extending and generalising recent work on flexibility in engineering design, and how it allows applying this work to processes. The resulting framework provides a comprehensive account of process flexibility that subsumes existing approaches. Finally, the paper presents a method for flexible process specification, illustrated using examples of a property valuation process in the Australian lending industry.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1521</field>
<field name="author">Shanika Kuruppu</field>
<field name="author">S. Puglisi</field>
<field name="author">Justin Zobel</field>
<field name="title">Relative Lempel-Ziv Parsing for Storing and Accessing Similar Genomes.</field>
<field name="abstract">This report is a description of the relative LZ-based compression algorithm RLZ that compresses a set of similar sequences against a selected reference sequence in a space efficient manner while providing fast access to the sequence collection.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1522</field>
<field name="author">Jonathan Guerin</field>
<field name="author">Marius Portmann</field>
<field name="author">Konstanty Bialkowski</field>
<field name="author">Wee Lum Tan</field>
<field name="author">Steve Glass</field>
<field name="title">Low-Cost Wireless Link Capacity Estimation</field>
<field name="keyword">metrics</field>
<field name="keyword"> link quality</field>
<field name="keyword"> wireless mesh network</field>
<field name="abstract">Wireless link quality estimations are essential for the optimal operation of various network functions like routing and rate adaptation. In this paper, we present a new link quality metric named Effective Link Capacity (ELC), which predicts link capacity by utilizing information such as the packet delivery ratio (PDR) and the transmission count of data packets (TXC). ELC requires no active probing and hence incurs zero overhead, while using only locally-available information from the transmitting node. Using our conducted testbed, we evaluate the accuracy of ELC on both single link and hidden terminal scenarios, with different configurations of packet sizes, link rates and offered loads. We also corroborate our conducted testbed findings with an evaluation of ELC for varying amounts of offered load on a live wireless data link in an office environment and find that ELC is highly accurate, with a maximum Root Mean Squared Error (RMSE) of 4.43%. We also compare ELC against a known bandwidth estimation tool, PathChirp, and find that ELC s RMSE of 0.68% far outperforms PathChirp s RMSE of 18.66%.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1523</field>
<field name="author">Guido Governatori</field>
<field name="author">Renato Iannella</field>
<field name="title">A Modelling and Reasoning Framework for Social Networks Policies</field>
<field name="keyword">social networks</field>
<field name="keyword"> open digital right language</field>
<field name="keyword"> policy languages</field>
<field name="abstract">Policy languages (such as privacy and rights) have had little impact on the wider community. Now that Social Networks have taken off, the need to revisit Policy languages and realign them towards Social Networks requirements has become more apparent. One such language is explored as to its applicability to the Social Networks masses. We also argue that policy languages alone are not sufficient and thus they should be paired with reasoning mechanisms to provide precise and unambiguous execution models of the policies. To this end we propose a computationally oriented model to represent, reason with and execute policies for Social Networks.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1524</field>
<field name="author">Jeff Li</field>
<field name="title">Blind SNR estimation of OFDM signals</field>
<field name="keyword">60 GHz</field>
<field name="keyword"> OFDM</field>
<field name="keyword"> SNR estimation</field>
<field name="abstract">A low-complexity blind algorithm is presented for

estimating the Signal-to-Noise ratio (SNR) of an Orthogonal

Frequency Division Multiplexing (OFDM) signal transmitted

over a frequency-selective channel with colored or white noise.

The algorithm only uses the second moment statistics in the

frequency domain without requiring training symbols or pilot

subcarriers.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1525</field>
<field name="author">Vida Dujmovic</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Pat Morin</field>
<field name="author">Thomas Wolle</field>
<field name="title">Notes on Large Angle Crossing Graphs</field>
<field name="keyword">graph drawing</field>
<field name="keyword"> edge crossings</field>
<field name="keyword"> upper and lower bounds</field>
<field name="abstract">A graph $G$ is an $\alpha$ angle crossing ($\alpha$AC) graph if

every pair of crossing edges in $G$ intersect at an angle of at least

$\alpha$. The concept of right angle crossing (RAC) graphs ($\alpha=\pi/2$)

was recently introduced by Didimo \etal\ \cite{del-dgrac-09}. It was

shown that any RAC graph with $n$ vertices has at most $4n-10$ edges

and that there are infinitely many values of $n$ for which there exists a RAC

graph with $n$ vertices and $4n-10$ edges. In this paper, we give upper

and lower bounds for the number of edges in $\alpha$AC graphs for all

$0 &lt; \alpha &lt; \pi/2$.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1526</field>
<field name="author">Hai-Ning Liang</field>
<field name="title">Overview of the health informatics research field: A bibliometric approach</field>
<field name="keyword">health informatics; medical informatics; e</field>
<field name="keyword">health; bibliometrics; literature review; research field; citation and co</field>
<field name="keyword">citation analysis</field>
<field name="abstract">Health informatics is a relatively new research area. Over the last decade or so, research in health information has been growing at a very rapid rate, as evidenced by the large number of publications. While this growth has been beneficial to the field, it has also made understanding the scope of the field more difficult. Consequently, it is difficult to answer questions such as how the research field has evolved over time, what the landmark publications are, what impact these publications have had, and who are the most prolific and high impact researchers. The purpose of this paper is to provide an overview of the research field of health informatics and, in doing so, attempt to answer some of the questions just mentioned. To this end, we make use of two bibliometric tools: HistCite and CiteSpace II. Because these tools offer complementary bibliometric methods, their use together provides results that are more robust. In this paper, we report some general findings of our bibliometric analysis using these two tools.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1527</field>
<field name="author">Tania Xiao</field>
<field name="author">Wendy Broxham</field>
<field name="author">Cara Stitzlein</field>
<field name="author">Jasmine Croll</field>
<field name="author">Penelope Sanderson</field>
<field name="title">Two human-centred approaches to health informatics: Cognitive systems engineering and usability</field>
<field name="abstract">There is growing recognition among many healthcare researchers that a human-centred approach to the design and evaluation of health information systems is vital for the success of such systems in healthcare. In this paper, we survey the work of two human-centred research communities that have been active in the area of health information systems research but that have not been adequately discussed in past comparative reviews. They are cognitive systems engineering and usability. We briefly consider the origins and contributions of the two research communities and then discuss the similarities and differences between them on several topics relevant to health information systems. Our objective is to clarify the distinction between the two communities and to help future researchers make more informed decisions about the approaches and methods that will meet their needs.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1528</field>
<field name="author">Juan Li</field>
<field name="author">Liming Zhu</field>
<field name="author">Ross Jeffery</field>
<field name="author">Jenny Liu</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Qing Wang</field>
<field name="author">He (Jason) Zhang</field>
<field name="title">An Initial Evaluation of Requirements Dependency Types in Change Propagation Analysis</field>
<field name="abstract">Change propagation analysis helps predict the parts of the software that may be affected if a change is made. Existing research on change propagation focuses on design and code level changes. However, as software evolves, the requirements that drive these changes also have intricate dependencies. Understanding the effect of these requirement dependencies on change prorogation is useful but not trivial. More than twenty requirements dependency types have been identified in the literature, however there still lacks an evaluation of the applicability of these dependency types in requirements and change propagation analysis. We conducted a case study in a real-world industry project to investigate whether requirements have dependencies and whether these dependencies are useful for change propagation analysis. Moreover we provide a group of specific dependency types for change propagation analysis based on our empirical findings. This case study evaluates two representative dependency models covering twenty five types of dependencies. Our initial evaluation has found that five dependency types are particularly useful in change propagation analysis and practitioners with different backgrounds have various viewpoints on change propagation. Thus change impact analysis should involve a wide range of stakeholders including project managers, requirements engineers, designers and developers. Our case study provides insights into requirements dependencies and their effects on change propagation analysis for both research and practice.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1529</field>
<field name="author">David Newman</field>
<field name="author">David Newman</field>
<field name="author">Ian Porteous</field>
<field name="author">Max Welling</field>
<field name="author">Padhraic Smyth</field>
<field name="author">Scott Triglia</field>
<field name="title">Machine Learning on Very Large Data Sets: Distributed Gibbs Sampling for Latent Variable Models</field>
<field name="abstract">We begin with a brief review of Bayesian latent variable models 

such as Latent Dirichlet Allocation and Hierarchical Dirichlet Processes. 

We then extend the discussion to the more general case of directed graphical 

models (or Bayesian networks), focusing on hidden Markov models as a concrete 

example. We discuss a number of parallel and distributed algorithms for

learning these models and show that these algorithms can achieve substantial 

computational speedups without sacrificing model quality.



This chapter also includes a discussion of practical guidelines for implementing 

our algorithms. In particular, we discuss how these techniques can be applied 

within various parallel computing infrastructures. We also compare the relative 

merits of various inference techniques, such as collapsed Gibbs sampling and

collapsed variational inference. Finally, we show that our parallel and 

distributed algorithms can be combined with other statistical acceleration 

methods to achieve compounded computational gains.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1530</field>
<field name="author">David Newman</field>
<field name="author">Youn Noh</field>
<field name="author">Edmund Talley</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Tim Baldwin</field>
<field name="title">Evaluating Topic Models for Digital Libraries</field>
<field name="abstract">Topic models could have a huge impact on improving the ways users

 find and discover content in digital libraries and search

 interfaces, through their ability to automatically learn and apply

 subject tags to each and every item in a collection, and their

 ability to dynamically create virtual collections on the fly.

 However, much remains to be done to tap this potential, and

 empirically evaluate the true value of a given topic model to

 humans. In this work, we sketch out some sub-tasks that we suggest

 pave the way towards this goal, and present methods for assessing

 the coherence and interpretability of topics learned by topic

 models. Our large-scale user study includes over 70 human subjects

 evaluating and scoring almost 500 topics learned from collections

 from a wide range of genres and domains. We show how a scoring

 model -- based on pointwise mutual information of word-pairs using

 Wikipedia, Google and MEDLINE as external data sources -- performs

 well at predicting human scores. This automated scoring of topics

 is an important first step to integrating topic modeling into

 digital libraries.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1531</field>
<field name="author">Jacky Keung</field>
<field name="author">Adam Brady</field>
<field name="author">Tim Menzies</field>
<field name="author">Oussama El-Rawas</field>
<field name="author">Ekrem Kocaguneli</field>
<field name="title">Case-Based Reasoning for Reducing Software Development Effort</field>
<field name="keyword">Software Cost Estimation</field>
<field name="keyword"> Analogy</field>
<field name="keyword"> k-NN</field>
<field name="abstract">How can we best find project changes that most improve project estimates? Prior solutions to this problem required the use of stan- dard software process models that may not be relevant to some new project. Also, those prior solutions suffered from limited verifica- tion (the only way to assess the results of those studies was to run the recommendations back through the standard process models).

Combining case-based reasoning and contrast set learning, the W system requires no underlying model. Hence, it is widely appli- cable (since there is no need for data to conform to some software process models). Also, W s results can be verified (using hold- out sets). For example, in the experiments reported here, W found changes to projects that greatly reduced estimate median and vari- ance by up to 95% and 83% (respectively).</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1532</field>
<field name="author">Jacky Keung</field>
<field name="author">Ekrem Kocaguneli</field>
<field name="author">Gregory Gay</field>
<field name="author">Tim Menzies</field>
<field name="author">Ye Yang</field>
<field name="title">When to Use Data from Other Projects for Effort Estimation</field>
<field name="keyword">Software effort estimation</field>
<field name="keyword"> cross</field>
<field name="keyword"> within</field>
<field name="keyword"> data mining</field>
<field name="abstract">Collecting the data required for quality prediction within a development team is time-consuming and expensive. It is tempting to make predictions using data that crosses from other projects or even other companies.

The literature is contradictory on the value of effort esti- mates based on such cross data. We hypothesis that data irrelevancies are the root cause of unreliable cross effort es- timation. To test that, this study takes three groupings of three projects, and trains effort estimators for project X using either local data (from within the same project) or im- ported data (that crosses from other projects or companies). This is repeated with and without relevancy filtering.

Our results clearly show that without relevancy filtering, im- ported data performed significantly worse than using local data. However, with relevancy filtering, imported data per- formed no worse than local data. Therefore, we recommend the use of relevancy filtering whenever generating estimates using data from another project.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1533</field>
<field name="author">Ramana Kumar</field>
<field name="author">Michael Norrish</field>
<field name="title">(Nominal) Unification by Recursive Descent with Triangular Substitutions</field>
<field name="abstract">We mechanise termination and correctness for two unification algorithms, written in a recursive descent style. One computes unifiers for first order terms, the other for nominal terms (terms including alpha-equivalent binding structure). Both algorithms work with triangular substitutions in accumulator-passing style: taking a substitution as input, and returning an extension of that substitution on success.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1534</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">A Conceptually Rich Model of Business Process Compliance</field>
<field name="keyword">social networks</field>
<field name="keyword"> open digital right language</field>
<field name="keyword"> policy languages</field>
<field name="abstract">In this paper we extend the preliminary work developed elsewhere and

 investigate how to characterise many aspects of the compliance problem in

 business process modeling. We first define a formal and conceptually rich

 language able to represent, and reason about, chains of reparational

 obligations of various types. Second, we devise a mechanism for normalising

 a system of legal norms. Third, we specify a suitable language for business

 process modeling able to automate and optimise business procedures and to

 embed normative constraints. Fourth, we develop an algorithm for compliance

 checking and discuss some computational issues regarding the possibility of

 checking compliance runtime or of enforcing it at design time.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1535</field>
<field name="author">Matt Thompson</field>
<field name="author">Jason Tangen</field>
<field name="author">Duncan McCarthy</field>
<field name="author">Morgan Tear</field>
<field name="title">Ground truth: On certainty in forensic decision-making research</field>
<field name="keyword">biometric</field>
<field name="keyword"> database</field>
<field name="keyword"> judgement</field>
<field name="keyword"> decision making</field>
<field name="keyword"> forensic</field>
<field name="keyword"> biometric repository</field>
<field name="keyword"> quality assurance</field>
<field name="abstract">Optimal decision making in forensics and policing relies on humans and technology. But it is unclear how experts interface with technology to make their decisions. Our approach in determining how best to implement new technology and procedures is to understand the judgement and decision-making processes undertaken by forensic professionals when they are evaluating evidence. When investigating these processes, we use the very same materials that professionals use in their investigations and training. But when using these authentic materials for forensic research, it is impossible to be absolutely certain that a piece of crime-scene evidence actually originated from the claimed source. That is, the ground truth is incalculable. We are working to solve this problem by developing an open-source biometric repository to be accessed, free-of-charge, by forensic professionals and researchers worldwide. Several pieces of standardised biometric materials are taken from hundreds of participants where the ground truth of their origin is built into the system. The repository will contain fingerprints, palm-prints, shoe-prints, face photographs, video footage, handwriting samples, voice samples, and iris scans. Here we present results from experiments using these materials and show that the repository can be used as a valuable tool to support training and quality assurance.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1536</field>
<field name="author">Ke Jia</field>
<field name="author">Nianjun Liu</field>
<field name="author">Lei Wang</field>
<field name="author">Li Cheng</field>
<field name="title">Efficient Structured Support Vector Regression in Vision</field>
<field name="abstract">Support Vector Regression (SVR) has been a long standing problem in machine learning, and gains its popularity on various computer vision tasks.

%attracted much attention in recent years.In this paper, we propose a structured support vector regression framework by extending the max-margin

principle to incorporate spatial correlations among neighboring pixels. The objective function in our framework %naturally generalizes the multi-class discriminative algorithms and can be regarded as an alternative approach of them in many ways. considers both label information and pairwise feature, helping to achieve better cross-smoothing over neighboring nodes. With the bundle method, we effectively reduce the number of constraints and alleviate the adverse effect of outlier, leading to an efficient and robust learning algorithm. Moreover, we conduct a thorough analysis for the loss function used in structured regression and provide a general recipe for defining proper loss functions and deriving the corresponding solvers for finding the most violated constraint. Our framework naturally generalizes the multi-class discriminative algorithms and can be regarded as an alternative approach of them in many ways. We demonstrate that our method outperforms the state-of-the-art regression approaches on various testbeds of synthetic images and real-world scenes.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1537</field>
<field name="author">William Billingsley</field>
<field name="author">Cindy Gallois</field>
<field name="author">Andrew Smith</field>
<field name="author">Marcus Watson</field>
<field name="title">A system to make machine analysis of communication available and accessible to communications researchers and non-technologists.</field>
<field name="abstract">Analysing a conversation or a communication episode is a central activity in many fields of research, and it is also a key research area in many technology-based fields. Artificial Intelligence, Natural Language Processing, and Human-Computer Interaction research groups have invented many different techniques for analysing computable aspects of human expression, such as co-occurrence patterns of words, facial gestures, and measures of voice quality. At present, these tools are used piecemeal by communications researchers in their work. The problem is that each tool adds information about a single aspect of the conversation (e.g., lexical analysis works only on the transcript and does not take into account behaviours like pause durations or the vocal stress and other voice qualities of the speakers). In addition, the tool a researcher is using risks being the main way that researcher s questions are framed, and thus the kinds of analysis and results the research will find.

 

We have developed computer software that can bring together many different machine analysis tools, allowing their outputs to be visualised together. In this package, analysis tools are integrated as plug-ins. When a researcher wishes to study a communication episode (e.g., video with transcript), the software automatically processes it through the analysis plug-ins. The user can then open the viewer to interact with visualisations of the different analyses, together with the video and transcript, on a common timeline. The system supports combined analysis, or plug-ins and visualisations can use the output of other plug-ins in their own analysis. For example, a visualisation might identify the main themes in a transcript, based on co-occurrences of words, and then allow visualisation of their association with the level of vocal stress at each time period. The software is designed to be extensible, and new analysis plug-ins, visualisations, workflows, and configuration templates can be added.

 

In this paper, we demonstrate the software and discuss the development process, including challenges like the need to find common units of time that are meaningful to researchers. Our hope for the software is that it will make machine analysis of communication more readily accessible, and that it will enable a closer link between technology-based and behaviourally-based researchers into language and communication. We also hope to make communication analysis more accessible to non-research users in ways that will be meaningful to them. For example, doctors (who have only limited awareness of communication analysis techniques) want to understand more about their interactions with their patients, and this tool may allow more access to research findings and interventions for them.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1538</field>
<field name="author">William Billingsley</field>
<field name="author">Cindy Gallois</field>
<field name="author">Andrew Smith</field>
<field name="author">Timothy Marks</field>
<field name="author">Fernando Bernal</field>
<field name="author">Marcus Watson</field>
<field name="title">Towards a Diagnostic Toolbox for Medical Communicaton</field>
<field name="keyword">Communication</field>
<field name="keyword"> Visualisation</field>
<field name="keyword"> Patient History Taking</field>
<field name="keyword"> Natural Language Processing</field>
<field name="keyword"> Artificial Intelligence</field>
<field name="abstract">Poor communication is a major cause of adverse patient events in hospitals. Although sophisticated simulators are in use for performing medical operations, there is comparatively little technology support being used for improving communication skills including patient history taking. Artificial Intelligence and Natural Language Processing researchers have developed sophisticated algorithms for analysing conversations. We are experimentally developing software that can visualise the combined output of these algorithms, as a diagnostic toolkit for medical communication.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1539</field>
<field name="author">Natalie Ruiz</field>
<field name="author">Guang Liu</field>
<field name="author">Bo Yin</field>
<field name="author">Damian Farrow</field>
<field name="author">Fang Chen</field>
<field name="title">Teaching Athletes Cognitive Skills: Cognitive Load in Speech Input</field>
<field name="keyword">Cognitive load</field>
<field name="keyword"> multimodality</field>
<field name="keyword"> speech and pen interfaces.</field>
<field name="abstract">complete cognitive skills training using targeted sports-specific software applications. When cognitive load is very high, the quality of performance can be negatively affected and learning can be inhibited. The aim of this study is to verify whether cognitive load can be inferred directly from speech signal changes collected using one such training application. We expect that the quality of the communicative signals during interaction will change as cognitive load increases. Ten recreational basketball players completed training requiring them to recall aloud the positions of increasing numbers of team players, and draw symbols to represent those players onto a court schematic on a digital surface. This paper focuses on the analysis of the speech data only, testing whether the speech signal changes due to high cognitive load. We describe the techniques used to build the speech load models and present the classification results. Using only automated speech signal analysis, we can identify subjects experiencing low or high load with an accuracy of 92.3%. We envisage it is possible to discern broad level cognitive load ranges through speech signal changes and may provide the opportunity to tailor the training application in more appropriate ways for each learner in real time.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1540</field>
<field name="author">Xu Bai</field>
<field name="author">Huang Liguo</field>
<field name="author">He (Jason) Zhang</field>
<field name="title">On Scoping Stakeholders and Artifacts in Software Process</field>
<field name="abstract">Stakeholders and artifacts have been applied in software engineering domain for decades, but they are rarely considered in software process modeling and simulation. Inspired by the Workshop of Modeling Systems and Software Engineering Processes in 2008 at University of Southern California and our previous studies on integrating stakeholders perspectives into software process modeling, we undertook a study on the application of these entities in software engineering, through both a systematic literature review and a complementary online survey within software process research and practice communities. Our results reveal that the portion of studies on process stakeholders and process artifacts in software engineering is unexpectedly small, and there lacks consistent understanding of process stakeholder roles in software process engineering. By further analysis of stakeholder roles and artifact types based on our results, we define the stakeholder and artifact in the lieu of software process engineering, providing clear criteria to differentiate stakeholder and artifact in different application scopes.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1541</field>
<field name="author">Xu Bai</field>
<field name="author">Huang Liguo</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Koolmanojwong Supannika</field>
<field name="title">Hybrid Modeling and Simulation for Trustworthy Software Process Management: A Stakeholder-Oriented Approach</field>
<field name="keyword">Trustworthy Process Management</field>
<field name="keyword"> process modeling and simulation</field>
<field name="keyword"> stakeholder-oriented process modeling</field>
<field name="keyword"> Process Modeling Stakeholders</field>
<field name="keyword"> Process Modeling Language</field>
<field name="keyword"> hybrid process simulation</field>
<field name="abstract">Process Management Model (PMM) and Process Simulation Model (PSM) are the critical infrastructural components of the Trust-worthy Process Management Framework (TPMF), which involves a large and heterogeneous group of stakeholders in process modeling and simulation to improve process trustworthiness. Process Modeling Stakeholders (PMS) have di&#11;erent levels of dependency on various process modeling and simulation techniques. They may also possess di&#11;erent perspectives or concerns in modeling. To support trustworthy process management, this paper integrates the stakeholder-oriented approach and hybrid simulation technique into software process modeling at three levels of abstraction (i.e., activity, sub-process and system). The hybrid process simulation combines microlevel discrete process models with the macro-level continuous process models to capture process dynamics. In particular, the stakeholder-oriented approach addresses the various perspectives of PMS during process modeling and simulation. Finally a case study with a realistic process model demonstrates that this approach incrementally integrates stakeholders modeling

concerns through hybrid simulation, which is difficult to achieve using discrete or continuous modeling/simulation techniques independently.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1542</field>
<field name="author">Gernot Heiser</field>
<field name="author">Ben Leslie</field>
<field name="title">The OKL4 Microvisor: Convergence Point of Microkernels and Hypervisors</field>
<field name="keyword">microkernels</field>
<field name="keyword"> hypervisors</field>
<field name="keyword"> virtual machines</field>
<field name="keyword"> operating systems</field>
<field name="keyword"> trusted computing base</field>
<field name="abstract">We argue that recent hypervisor-vs-microkernel discussions completely miss the point. Fundamentally, the two classes of systems have much in common, and provide similar abstractions. We assert that the requirements for both types of systems can be met with a single set of abstractions, a single design, and a single implementation. We present partial proof of the existence of this convergence point, in the guise of the OKL4 microvisor, an industrial-strength system designed as a highly-efficient hypervisor for use in embedded systems. It is also a third-generation microkernel that aims to support the construction of similarly componentised systems as classical microkernels. Benchmarks show that the microvisor s performance is highly competitive.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1543</field>
<field name="author">Minh Pham</field>
<field name="title">River Modeling Using Support Vector Machines</field>
<field name="keyword">Hydrological modelling</field>
<field name="keyword"> River Basin modelling</field>
<field name="keyword"> Support Vector Machines</field>
<field name="abstract">This paper presents an analysis of the real data of large-scale river systems and proposes a computationally tractable mathematical model which can deal with high complexity and inherent nonlinearly in the systems. In addition, the paper presents a systematic implementation of the models based on the Support Vector Machine framework. In contrast to existing methodologies, the approach proposed herein can readily incorporate any relevant inputs such as rainfall, snowpack depth, soil moisture, evapotranspiration, and other environmental parameters without incurring further computational complexity. The models can be used for reliable short-term and long-term water level forecasts. Rainfalls data collected from National Alpine Park and hydrological data collected from the Mitta-Mitta-River in Australia is used to demonstrate the effectiveness of this proposed approach.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1544</field>
<field name="author">Upendra Rathnayake</field>
<field name="author">Lars (Henrik) Petander</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Protocol Support for Bulk Transfer Architecture</field>
<field name="keyword">Mobility Protocols</field>
<field name="keyword"> Network Interface Selection</field>
<field name="keyword"> Data Transfer Scheduling</field>
<field name="abstract">Today s mobile devices are increasingly containing

multiple radios, enabling users on the move to take advantage of

the heterogeneous wireless network environment. In addition, we

observe that many bandwidth intensive services such as podcasts,

software updates etc are essentially non-real-time and buffers in

mobile devices are effectively unlimited. We therefore proposed

EMUNE, a new transfer service architecture in our previous

work which leverages these aspects and supports opportunistic

bulk transfers in high bandwidth networks.

EMUNE uses multiple wireless network interfaces according

to an optimal transfer schedule derived based on device power

concerns, application requirements, user preferences of cost

and quality and future network availability. In this paper, we

explore the usability of network/transport protocols to achieve the

use of multiple network interfaces with required functionalities

including flow mobility and striping in mobile environments and

propose a MONAMI [1] + R2CP [3] hybrid approach for the

architecture.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1545</field>
<field name="author">Arun Konagurthu</field>
<field name="author">Cyril F. Reboul</field>
<field name="author">James W. Schmidberger</field>
<field name="author">James Irving</field>
<field name="author">Arthur M. Lesk</field>
<field name="author">Peter Stuckey</field>
<field name="author">James C. Whisstock</field>
<field name="author">Ashley Buckle</field>
<field name="title">MUSTANG-MR Structural Sieving Server: Applications in Protein Structural Analysis and Crystallography</field>
<field name="keyword">Protein structure alignment</field>
<field name="keyword"> X-Ray Crystallography</field>
<field name="keyword"> Molecular replacement</field>
<field name="abstract">Background: A central tenet of structural biology is that related proteins of common function share structural similarity. This has key practical consequences for the derivation and analysis of protein structures, and is exploited by the process of molecular sieving whereby a common core is progressively distilled from a comparison of two or more protein structures. This paper reports a novel web server for sieving of protein structures, based on the multiple structural alignment program MUSTANG. 



Methodology/Principal Findings: Sieved models are generated from MUSTANG-generated multiple alignment and superpositions by iteratively filtering out noisy residue-residue correspondences, until the resultant correspondences in the models are optimally superposable under a threshold of RMSD. This residue-level sieving is also accompanied by iterative elimination of the poorly fitting structures from the input ensemble. Therefore, by varying the thresholds of RMSD and the cardinality of the ensemble, multiple sieved models are generated for a given multiple alignment and superposition from MUSTANG. To aid the identification of structurally conserved regions of functional importance in an ensemble of protein structures, Lesk-Hubbard graphs are generated, plotting the number of residue correspondences in a superposition as a function of its corresponding RMSD. The conserved core (or typically active site) shows a linear trend, which becomes exponential as divergent parts of the structure are included into the superposition. 



Conclusions: The application addresses two fundamental problems in structural biology: First, the identification of common substructures among structurally related proteins - an important problem in characterization and prediction of function; second, generation of sieved models with demonstrated uses in protein crystallographic structure determination using the technique of Molecular Replacement.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1546</field>
<field name="author">Arun Konagurthu</field>
<field name="author">Arthur M. Lesk</field>
<field name="title">Cataloging Topologies of Protein Folding Patterns</field>
<field name="keyword">protein folding pattern</field>
<field name="keyword"> computational biology</field>
<field name="abstract">Comparing and classifying protein folding patterns allows organizing the known structures, structure search and retrieval, and investigation of general principles of protein architecture. We have been developing a concise tableau representation of protein folding patterns, based on the order and contact patterns of elements of secondary structure: helices and strands of sheet (Lesk, [1995]; Kamat and Lesk, [2007]; Konagurthu et al., [2008]). The tableaux provide a database, derived from the world-wide protein data bank, mineable in studies of protein architecture, including: (i) determination of statistical properties of secondary structure contacts in an unbiased set of protein domains, (ii) investigations of the range of, and relationships among, protein topologies, (iii) investigation of the relationship between local structure of proteins and the complete folding topology, (iv) potential for fold identification from amino acid sequence, and (v) the basis for a complete enumeration of possible protein folding patterns, which can be compared with the corpus of known structures.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1547</field>
<field name="author">Lei Zhang</field>
<field name="author">James Bailey</field>
<field name="author">Arun Konagurthu</field>
<field name="author">Rao Kotagiri</field>
<field name="title">A fast indexing approach for protein structure comparison</field>
<field name="keyword"/>
<field name="abstract">Background

Protein structure comparison is a fundamental task in structural biology. While the number of known protein structures has grown rapidly over the last decade, searching a large database of protein structures is still relatively slow using existing methods. There is a need for new techniques which can rapidly compare protein structures, whilst maintaining high matching accuracy.



Results

We have developed IR Tableau, a fast protein comparison algorithm, which leverages the tableau representation to compare protein tertiary structures. IR tableau compares tableaux using information retrieval style feature indexing techniques. Experimental analysis on the ASTRAL SCOP protein structural domain database demonstrates that IR Tableau achieves two orders of magnitude speedup over the search times of existing methods, while producing search results of comparable accuracy.



Conclusion

We show that it is possible to obtain very significant speedups for the protein structure comparison problem, by employing an information retrieval style approach for indexing proteins. The comparison accuracy achieved is also strong, thus opening the way for large scale processing of very large protein structure databases.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1548</field>
<field name="author">Ming Zhao</field>
<field name="author">Mark Reed</field>
<field name="author">Zhenning Shi</field>
<field name="title">On Uplink Interference Scenarios in Two-Tier Macro and Femto Co-existing UMTS Networks</field>
<field name="keyword">Femtocell</field>
<field name="keyword"> macrocell</field>
<field name="keyword"> home NodeB</field>
<field name="keyword"> HUE and interference management</field>
<field name="abstract">In this paper, a two-tier UMTS network is considered where a large number of randomly deployed Wideband Code Division Multiple Access (WCDMA) femtocells are laid under macrocells where the spectrum is shared. The co-channel interference between the cells may be a potential limiting factor for the system. We study the uplink of this hybrid network, and identify the critical scenarios that give rise to substantial interference. The mechanism for generating the nterference is analyzed and guidelines for interference mitigation are provided. The impacts of the cross-tier interference, especially caused by increased numbers of users and higher data rates is evaluated in the multi-cell simulation environment in terms of the noise rise at the base stations, the cell throughput and the user transmit power consumption.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1549</field>
<field name="author">Zhenning Shi</field>
<field name="author">Ming Zhao</field>
<field name="author">Jackson Wang</field>
<field name="author">Mark Reed</field>
<field name="title">On the Uplink Capacity and Coverage of Relay-Assisted UMTS Cellular Network with Multiuser Detection</field>
<field name="abstract">Capacity and Coverage are the two major concerns for the wireless operators to deploy cellular networks. This paper investigates the uplink capacity and coverage of relayassisted UMTS network when multiuser detection techniques are employed at both macrocell base station (BS) and relay station (RS). A hierarchical geometric system model is used where the fixed relay stations are deployed within the macrocell. We are particularly interested in the cases where macro BSs and RSs have different capabilities in mitigating intracell and intercell interferences. Simulation results show that the capacity can be tripled and the cell range can be extended when MUD can be employed at base stations and relay stations. Furthermore, we studied the practical scenario where the users in the relay region are partially served by the relay station. This case is meaningful when there are not enough relay resources or users are restricted due to limited handover capability. We then proposed a partial relay system to enhance the cell range while maintaining a reasonable cell load.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1550</field>
<field name="author">Zhenning Shi</field>
<field name="author">Jackson Wang</field>
<field name="author">Ming Zhao</field>
<field name="author">Mark Reed</field>
<field name="title">An Uplink Analytical Model for Two-Tiered 3G Femtocell Networks</field>
<field name="keyword">Femtocell</field>
<field name="keyword"> Analytical Model</field>
<field name="keyword"> Interference</field>
<field name="keyword"> Two-tier Network</field>
<field name="abstract">This paper proposes an analytical model to investigate the impact of interference on the uplink capacity and coverage in a WCDMA network where macrocell and femtocells co-exist. Geometric modeling for the hierarchical system is used where the randomly deployed femtocells are within the planned macrocells' topology. The interference effects among femtocells and between femtocells and macrocells are studied analytically to quantify the system capacity and coverage based on the practical target signal-to-interference ratio (SIR). Interference level splitting results show that the macrocell attached User Equipment (UE) to Home Node B (HNB) interference has severe impact on the capacity and coverage of femtocell network. Further study suggests that advanced receivers which cancel interference at the femtocell could minimize the effect brought by different interferences in a cost-effective manner.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1551</field>
<field name="author">Gerwin Klein</field>
<field name="author">June Andronick</field>
<field name="author">Kevin Elphinstone</field>
<field name="author">Gernot Heiser</field>
<field name="author">David Cock</field>
<field name="author">Philip Derrin</field>
<field name="author">Dhammika Elkaduwe</field>
<field name="author">Kai Engelhardt</field>
<field name="author">Rafal Kolanski</field>
<field name="author">Michael Norrish</field>
<field name="author">Thomas Sewell</field>
<field name="author">Harvey Tuch</field>
<field name="author">Simon Winwood</field>
<field name="title">seL4: Formal Verification of an Operating-System Kernel</field>
<field name="keyword">seL4</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> verification</field>
<field name="abstract">We report on the formal, machine-checked

verification of the seL4 microkernel from an abstract

specification down to its C implementation. We assume

correctness of compiler, assembly code, hardware, and boot code.



seL4 is a third-generation microkernel of L4 provenance,

comprising 8,700 lines of C and 600 lines of assembler. Its

performance is comparable to other high-performance L4 kernels.

 

We prove that the implementation always

strictly follows our high-level abstract specification of kernel

behaviour. This encompasses traditional design and implementation 

safety properties such as that the kernel will never crash, and 

it will never perform an unsafe operation. 

It also implies much more: we can predict precisely how the kernel

will behave in every possible situation.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1552</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">Stefan Pohl</field>
<field name="author">Falk Scholer</field>
<field name="author">Lawrence Cavedon</field>
<field name="author">Justin Zobel</field>
<field name="title">Boolean versus Ranked Querying for Biomedical Systematic Reviews</field>
<field name="keyword">Boolean retrieval</field>
<field name="keyword"> Ranked Retrieval</field>
<field name="keyword"> Biomedical Systematic reviewing</field>
<field name="keyword"> Complex Boolean Queries</field>
<field name="abstract">Background

The process of constructing a systematic review, a document that compiles the published evidence pertaining to a specified medical topic, is intensely time-consuming, often taking a team of researchers over a year, with the identification of relevant published research comprising a substantial portion of the effort. The standard paradigm for this information-seeking task is to use Boolean search; however, this leaves the user(s) the requirement of examining every returned result. Further, our experience is that effective Boolean queries for this specific task are extremely difficult to formulate and typically require multiple iterations of refinement before being finalized.



Methods

We explore the effectiveness of using ranked retrieval as compared to Boolean querying for the purpose of constructing a systematic review. We conduct a series of experiments involving ranked retrieval, using queries defined methodologically, in an effort to understand the practicalities of incorporating ranked retrieval into the systematic search task.



Results

Our results show that ranked retrieval by itself is not viable for this search task requiring high recall. However, we describe a refinement of the standard Boolean search process and show that ranking within a Boolean result set can improve the overall search performance by providing early indication of the quality of the results, thereby speeding up the iterative query-refinement process.



Conclusions

Outcomes of experiments suggest that an interactive query-development process using a hybrid ranked and Boolean retrieval system has the potential for significant time-savings over the current search process in the systematic reviewing.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1553</field>
<field name="author">Sebastian Maneth</field>
<field name="author">Kim Nguyen</field>
<field name="title">XPath Whole Query Optimization</field>
<field name="abstract">Previous work reports about SXSI, a fast XPath engine which executes

tree automata over compressed XML indexes. Here, reasons

are investigated why SXSI is so fast. It is shown that tree automata

can be used as a general framework for fine grained XML query

optimization. We define the relevant nodes of a query as those

nodes that a minimal automaton must touch in order to answer the

query. This notion allows to skip many subtrees during execution,

and, with the help of particular tree indexes, even allows to skip

internal nodes of the tree. We efficiently approximate runs over

relevant nodes by means of on-the-fly removal of alternation and

non-determinism of (alternating) tree automata.

We also introduce many implementation techniques which allows

us to efficiently evaluate tree automata, even in the absence of

special indexes. Through extensive experiments, we demonstrate

the impacts of the different optimization techniques.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1554</field>
<field name="author">Sylvia Pott</field>
<field name="author">Helmut Seidl</field>
<field name="author">Sebastian Maneth</field>
<field name="title">Minimization of Deterministic Bottom-up Tree Transducers</field>
<field name="abstract">A unique minimal transducer can be constructed for every deterministic

bottom-up tree transducer. The construction is based on a sequence of normalizing

transformations which, among others, guarantee that non-trivial output

is produced as early as possible. For a deterministic bottom-up tree transducer

where every state produces either none or infinitely many outputs, the corresponding

unique minimal transducer can be constructed in polynomial time.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1555</field>
<field name="author">Chunhua Shen</field>
<field name="author">Peng Wang</field>
<field name="author">Hanxi Li</field>
<field name="title">LACBoost and FisherBoost: Optimally Building Cascade Classifiers</field>
<field name="keyword">Object detection</field>
<field name="keyword"> Boosting</field>
<field name="keyword"> Fisher Linear Discriminant Analysis</field>
<field name="keyword"> Linear asymmetric classifier.</field>
<field name="abstract">Object detection is one of the key tasks in computer vision.

The cascade framework of Viola and Jones has become the de facto

standard. A classi&#12;er in each node of the cascade is required to achieve

extremely high detection rates, instead of low overall classi&#12;cation error.

Although there are a few reported methods addressing this requirement

in the context of object detection, there is no a principled feature selection

method that explicitly takes into account this asymmetric node

learning objective. We provide such a boosting algorithm in this work.

It is inspired by the linear asymmetric classi&#12;er (LAC) of [1] in that our

boosting algorithm optimizes a similar cost function. The new totallycorrective

boosting algorithm is implemented by the column generation

technique in convex optimization. Experimental results on face detection

suggest that our proposed boosting algorithms can improve the state-ofthe-

art methods in detection performance.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1556</field>
<field name="author">Julian McAuley</field>
<field name="author">Teofilo de Campos</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Unified Graph Matching in Euclidean Spaces</field>
<field name="abstract">Graph matching is a classical problem in pattern recognition with many applications, particularly when the graphs are embedded in Euclidean spaces, as is often the case for computer vision. There are several variants of the matching problem, concerned with isometries, isomorphisms, homeomorphisms, and node attributes; di&#11;erent approaches exist for each variant. We show how structured estimation methods from machine learning can be used to combine such variants into a single version of graph matching. In this paradigm, the extent to which our datasets reveal isometries, isomorphisms, homeomorphisms, and other properties is automatically accounted for in the learning process so that any such specific qualification of graph matching loses meaning. We present experiments with real computer vision data showing the leverage of this unified formulation.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1557</field>
<field name="author">Qinfeng Shi</field>
<field name="author">Hanxi Li</field>
<field name="author">Chunhua Shen</field>
<field name="title">Rapid Face Recognition Using Hashing</field>
<field name="keyword">face recogintion</field>
<field name="keyword"> hashing kernel</field>
<field name="keyword"> Orthogonal Matching Pursuit.</field>
<field name="abstract">We propose hashing to facilitate face recognition, which

is up to 150 times faster than the random `1 approach [18]

on YaleB dataset with competitive recognition accuracy. We

show with hashing, the sparse representation can be recovered

with high probability because hashing preserves the restrictive

isometry property. Moreover we give a recognition

rate analysis on the hashing approach. Experiments show

a very competitive recognition rate and significant speedup

compared with the state-of-the-art.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1558</field>
<field name="author">Julian McAuley</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Alex Smola</field>
<field name="author">Matthias Franz</field>
<field name="title">Learning high-order MRF priors of color images</field>
<field name="abstract">In this paper, we use large neighborhood Markov random fields to learn rich prior models of color images. Our approach extends the monochromatic Fields of Experts model (Roth &amp; Black, 2005a) to color images. In the Fields of Experts model, the curse of dimensionality due to very large clique sizes is circumvented by parameterizing the potential functions according to a product of experts. We introduce simplifications to the original approach by Roth and Black which allow us to cope with the increased clique size (typically 3x3x3 or 5x5x3 pixels) of color images. Experimental results are presented for image denoising which evidence improvements over state-of-the-art monochromatic image priors.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1559</field>
<field name="author">Tiberio Caetano</field>
<field name="author">Julian McAuley</field>
<field name="title">Faster graphical models for point-pattern matching</field>
<field name="abstract">It has been shown that isometric matching problems can be solved exactly in polynomial time, by means of a Junction Tree with small maximal clique size. Recently, an iterative algorithm was presented which converges to the same solution an order of magnitude faster. Here, we build on both of these ideas to produce an algorithm with the same asymptotic running time as the iterative solution, but which requires only a single iteration of belief propagation. Thus our algorithm is much faster in practice, while maintaining similar error rates.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1560</field>
<field name="author">Julian McAuley</field>
<field name="author">Luciano da Fontoura Costa</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Rich-club phenomenon across complex network hierarchies</field>
<field name="abstract">The "rich-club phenomenon" in complex networks is characterized when nodes of higher degree are more interconnected than nodes with lower degree. The presence of this phenomenon may indicate several interesting high-level network properties, such as tolerance to hub failures. Here, the authors investigate the existence of this phenomenon across the hierarchies of several real-world networks. Their simulations reveal that the presence or absence of this phenomenon in a network does not imply its presence or absence in the network s successive hierarchies, and that this behavior is even nonmonotonic in some cases.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1561</field>
<field name="author">Mei He</field>
<field name="author">He (Jason) Zhang</field>
<field name="title">Understanding the Influential Factors to Development Effort in Chinese Software Industry</field>
<field name="abstract">A good understanding of the influencing factors to software development effort and further precise effort estimate are undoubtedly crucial to any cost-effective and controllable software development projects. In most effort estimation researches, a large dataset is always a necessary basis of estimation modeling, model calibration and method validation. Among them, different attributes and characteristics of project data will to a large extent affect the applicable scope of particular research result. This research aims to identify the factors that significantly influence software development effort, and to investigate how the influence works in Chinese software industry. In this study, six factors and their relationships to development effort are analyzed, prioritized and discussed based upon the dataset recording 999 projects from 140 software organizations in China. In terms of our analysis and findings, some suggestions for effort estimation and control can be extracted for software practitioners to assist them in coping with various types of software project.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1562</field>
<field name="author">Andre Almeida</field>
<field name="author">Mark Sheahan</field>
<field name="author"/>
<field name="author">Julie LeMare</field>
<field name="author">Roman Auvray</field>
<field name="author">Kim Song Dang</field>
<field name="author">John Sebastian</field>
<field name="author">Jean Geoffroy</field>
<field name="author">Jay Katupitiya</field>
<field name="author">Paul Santus</field>
<field name="author">Andrei Skougarevsky</field>
<field name="author">John Smith</field>
<field name="author">Joe Wolfe</field>
<field name="title">Clarinet parameter cartography: automatic mapping of the sound produced as a function of blowing pressure and reed force</field>
<field name="keyword">Clarinet</field>
<field name="keyword"> Music</field>
<field name="keyword"> Acoustics</field>
<field name="keyword"> Automated Clarinet</field>
<field name="abstract">In simple models of a single-reed instrument mouthpiece, important control parameters include the air pressure in the mouth, the force applied by the lip on the reed, the position at which it is applied and the damping of the reed. In these simple models, position and damping are usually considered constant while pressure and force are regarded as the key control parameters. Pressure in the mouth is easy to measure during human performance. The lip force is harder to relate to the gesture of the musician because the range of forces applied by a player depends on several factors including the reed stiffness and profile, and the distribution of force on the reed. When the instrument is played by a mechanical device, greater independence and control of these parameters is possible. This study uses an automated clarinet playing system developed during a series of student projects involving NICTA and UNSW (hence the long author list). The mouth pressure is controlled, and two further parameters control the lip force and its position of application. The precision and short-term stability of this control allow a systematic study of the pitch, timbre and starting transients of the clarinet for a wide range of these three parameters and, in principle, up to 215 fingerings. This allows the mapping, in fingering, pressure and lip parameter space, of the regions that produce the intended note, poorly tuned notes, notes in another register, slowly starting notes, squeaks or no sound at all. Maps measured with different protocols are here compared with the predictions of theoretical models.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1563</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Pat Morin</field>
<field name="title">Planar Visibility: Testing and Counting</field>
<field name="keyword">algorithms</field>
<field name="keyword"> visibility</field>
<field name="keyword"> data structures</field>
<field name="abstract">In this paper we consider query versions of visibility testing and visibility counting. Let $S$ be a set of $n$ disjoint line segments in $\R^2$ and let $s$ be an element of $S$. Visibility testing is to preprocess $S$ so that we can quickly determine if $s$ is visible from a query point $q$. Visibility counting involves preprocessing $S$ so that one can quickly estimate the number of segments in $S$ visible from a query point $q$.

We present several data structures for the two query problems. The structures build upon a result by O'Rourke and Suri (1984) who showed that the subset, $V_S(s)$, of $\R^2$ that is weakly visible from a segment $s$ can be represented as the union of a set, $C_S(s)$, of $O(n^2)$ triangles, even though the complexity of $V_S(s)$ can be $\Omega(n^4)$. We define a variant of their covering, give efficient output-sensitive algorithms for computing it, and prove additional properties needed to obtain approximation bounds. Some of our bounds rely on a new combinatorial result that relates the number of segments of $S$ visible from a point $p$ to the number of triangles in $\bigcup_{s\in S} C_S(s)$ that contain $p$.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1564</field>
<field name="author">Adrian Bishop</field>
<field name="author">Baris Fidan</field>
<field name="author">Brian Anderson</field>
<field name="author">Kutluyil Dogancay</field>
<field name="author">Pubudu Pathirana</field>
<field name="title">Optimality Analysis of Sensor-Target Localization Geometries</field>
<field name="abstract">The problem of target localization involves estimating the position of a target from multiple and typically noisy measurements of the target position. It is well known that the relative sensor-target geometry can significantly affect the performance of any particular localization algorithm. The localization performance can be explicitly characterized by certain measures, for example, by the Cramer-Rao lower bound (which is equal to the inverse Fisher information matrix) on the estimator variance. In addition, the Cramer-Rao lower bound is commonly used to generate a so-called uncertainty ellipse which characterizes the spatial variance distribution of an efficient estimate, i.e. an estimate which achieves the lower bound. The aim of this work is to identify those relative sensor-target geometries which result in a measure of the uncertainty ellipse being minimized. Deeming such sensor-target geometries to be optimal with respect to the chosen measure, the optimal sensor-target geometries for range-only, time-of-arrival-based and bearing-only localization are identified and studied in this work. The optimal geometries for an arbitrary number of sensors are identified and it is shown that an optimal sensor-target configuration is not, in general, unique. The importance of understanding the influence of the sensor-target geometry on the potential localization performance is highlighted via formal analytical results and a number of illustrative examples.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1565</field>
<field name="author">Helen Allvin</field>
<field name="author">Elin Carlsson</field>
<field name="author">Hercules Dalianis</field>
<field name="author">Riitta Danielsson-Ojala</field>
<field name="author">Vidas Daudaravicius</field>
<field name="author">Martin Hassel</field>
<field name="author">Dimitrios Kokkinakis</field>
<field name="author">Helja Lundgren-Laine</field>
<field name="author">Gunnar Nilsson</field>
<field name="author">Oystein Nytro</field>
<field name="author">Sanna Salantera</field>
<field name="author">Maria Skeppstedt</field>
<field name="author">Hanna Suominen</field>
<field name="author">Sumithra Velupillai</field>
<field name="title">Characteristics and Analysis of Finnish and Swedish Clinical Intensive Care Nursing Narratives</field>
<field name="keyword">ehealth</field>
<field name="abstract">We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothesis is that there are similarities that are important and interesting from a language technology point of view. This may have implications when building tools to support producing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nursing narratives. Our findings are that ICU nursing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to developing language technology tools.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1566</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">Rules and Norms: Requirements for Rule Interchange Languages in the Legal Domain</field>
<field name="keyword">norm dynamics</field>
<field name="keyword"> defeasible logic</field>
<field name="keyword"> temporal defeasible logic</field>
<field name="keyword"> annulment</field>
<field name="keyword"> abrogation</field>
<field name="abstract">In this survey paper we summarize the requirements for rule interchange languages for applications in the legal domain and use these requirements to evaluate RuleML, SBVR, SWRL and RIF. We also present the Legal Knowledge Interchange Format (LKIF), a new rule interchange format developed specifically for applications in the legal domain.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1567</field>
<field name="author">Guido Governatori</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Leendert van der Torre</field>
<field name="title">Lex minus dixit quam voluit, lex magis dixit quam voluit: A formal study on legal compliance and interpretation</field>
<field name="keyword">legal interpretation</field>
<field name="keyword"> extensive interpretation</field>
<field name="keyword"> restrictive interpretation</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">This paper argues in favour of the necessity of dynamically restricting and expanding the applicability of norms regulating computer systems like multiagent systems, in situations where the compliance to the norm does not achieve the purpose of the norm. We propose a logical framework which distinguishes between constitutive and regulative norms and captures the norm change power and at the same time the limitations of the judicial system in dynamically revising the set of constitutive rules defining the concepts on which the applicability of norms is based. In particular, the framework is used to reconstruct some interpretive arguments described in legal theory such as those corresponding to the Roman maxims \emph{lex minus dixit quam voluit} and \emph{lex magis dixit quam voluit}. The logical framework is based on an extension of defeasible logic.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1568</field>
<field name="author">Guido Boella</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Leendert van der Torre</field>
<field name="title">A Logical Understanding of Legal Interpretation</field>
<field name="keyword">legal interpretation</field>
<field name="keyword"> extensive interpretation</field>
<field name="keyword"> restrictive interpretation</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">{If compliance with a norm does not achieve its

 purpose, then its applicability must dynamically be

 restricted or expanded. Legal interpretation is a

 mechanism from law allowing norms to be adapted to

 unforeseen situations. We model this mechanism for

 norms regulating computer systems by representing

 the purpose of norms by social goals and by revising

 the constitutive rules defining the applicability of

 norms. We illustrate the interpretation mechanism

 by examples.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1569</field>
<field name="author">SuNam Kim</field>
<field name="author">Tim Baldwin</field>
<field name="author">Min-Yen Kan</field>
<field name="title">An Unsupervised Approach to Domain-Specific Term Extraction</field>
<field name="abstract">Domain-specific terms provide vital semantic information for natural language processing (NLP) tasks and applications, but remain a largely untapped resource in the field. In this paper, we propose an unsupervised method to extract domain-specific terms from the Reuters document collection using term frequency and inverse document frequency.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1570</field>
<field name="author">Bathiya Senanayake</field>
<field name="author">Mark Reed</field>
<field name="title">Multi-Dimensional EXIT Analysis for Iterative Multi-User Detection with Unequal Power Allocation</field>
<field name="keyword">Iterative multi-user detection</field>
<field name="keyword"> Extrinsic Information transfer chart</field>
<field name="keyword"> multi-user systems</field>
<field name="keyword"> Multiple access channel</field>
<field name="keyword"> power optimization</field>
<field name="keyword"> capacity optimization</field>
<field name="keyword"> convergence</field>
<field name="abstract">A multi-dimensional mutual information transfer

characteristics of multi-power level multi-user detectors is proposed

as a tool to better understand the convergence behavior of

iterative decoding schemes. We develop a K dimensional EXIT

chart in order to analyze a system with K power levels. We state

a conjecture that predicts the convergence point of the system.

We show through simulation our analysis closely matches the

simulation results. The tools discussed here provides a new insight

to receiver performance analysis and can be used to design near

capacity achieving multi-user systems.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1571</field>
<field name="author">Nick Barnes</field>
<field name="author">Gareth Loy</field>
<field name="author">David Shaw</field>
<field name="title">The Regular Polygon Detector</field>
<field name="abstract">This paper describes a robust regular polygon detector. Given image edges, we derive the a posteriori probability for a mixture of regular polygons, and thus the probability density function for the appearance of a set of regular polygons. Likely regular polygons can be isolated quickly by discretising and collapsing the search space into three dimensions. We derive a complete formulation for efficiently recovering the remaining dimensions using maximum likelihood at the locations of the most likely polygons. Results show robustness to noise, the ability to find and differentiate different shape types, and to perform real-time sign detection for driver assistance.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1572</field>
<field name="author">David Newman</field>
<field name="author">Tim Baldwin</field>
<field name="author">Lawrence Cavedon</field>
<field name="author">Eric Huang</field>
<field name="author">Sarvnaz Karimi</field>
<field name="author">David Martinez</field>
<field name="author">Falk Scholer</field>
<field name="author">Justin Zobel</field>
<field name="title">Visualizing search results and document collections using topic maps</field>
<field name="abstract">This paper explores visualizations of document collections, which we call topic maps. Our topic maps are based on

a topic model of the document collection, where the topic model is used to determine the semantic content of each

document. Using two collections of search results, we show how topic maps reveal the semantic structure of a collection

and visually communicate the diversity of content in the collection. We describe techniques for assessing the validity

and accuracy of topic maps, and discuss the challenge of producing useful two-dimensional maps of documents.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1573</field>
<field name="author">Thanassis Boulis</field>
<field name="author">Yuri Tselishchev</field>
<field name="title">Contention vs. Polling: A Study in Body Area Networks MAC Design</field>
<field name="keyword">Medium Access Control (MAC)</field>
<field name="keyword"> Body Area Networks</field>
<field name="keyword"> Body Sensor Networks</field>
<field name="keyword"> contention</field>
<field name="keyword"> polling</field>
<field name="abstract">Medium Access Control design for Body Area Networks is challenging due to the uniqueness of the wireless channel characteristics (highly variable in time) and the need for ultra low power while maintaining reasonable performance. Using our temporal BAN channel models we study trade-offs created by a mix of two MAC techniques: i) contention-based access, and ii) polling-based access. By using these techniques at different proportions we see how performance and energy consumption vary. The results reveal design trade-offs in the packet delivery vs. latency vs. consumed energy space. Moreover, we show how optimal points for packet delivery vary depending on the traffic. Finally, explaining the results offers important insight into the behaviour of these techniques under BAN conditions as well as more general issues with medium access for BAN. This insight translates to concrete design suggestions to build more efficient MAC protocols.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1574</field>
<field name="author">(Hans) Joachim Gudmundsson</field>
<field name="author">Thomas Wolle</field>
<field name="title">Towards Automated Football Analysis: Algorithms and Data Structures</field>
<field name="keyword">team sport</field>
<field name="keyword"> position data</field>
<field name="keyword"> trajectory analysis</field>
<field name="keyword"> event detection</field>
<field name="keyword"> clustering</field>
<field name="abstract">Analysing a football match is without doubt an important task for

coaches, clubs and players;

and with current technologies more and more match data is collected.

For instance, many companies offer the ability to track the position of

each player and the ball with high accuracy and high resolution.

Analysing this position data can be very useful.

Nowadays, some companies offer products that include simple analyses,

such as statistics and basic queries.

It is, however, a non-trivial task to perform a more advanced analysis.

In our research, we assume that we are given only the position data of

all players and the ball with high accuracy and high resolution.

In this paper we present two tools.



Our first tool automatically extract (from the position data) a list

of certain events that happened during the football match. These

events include kick-offs, corner kicks, passes etc.

In experiments we could observe that our method is very fast and

reaches a high level of correctness.

We also learned that errors in the event detection are hard to avoid

completely, when looking at only the position data.



Our second tool aims at analysing a single player's trajectory (the

sequence of all positions during a game).

More precisely, we look for movements of a player that are repeated often

(so called subtrajectory clusters).

For example a left wing attacker runs from the centre-line along the

left side of the field towards the opponent's goal.

And this attacker might repeat this type of movement very often during

a game (or perhaps multiple games).

Our goal is to detect this kind of frequent movements automatically.

Our method reliably identifies subtrajectory clusters,

which then could be used for further analysis.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1575</field>
<field name="author">Thanassis Boulis</field>
<field name="title">A practical guide to MAC implementation and general MAC design guidelines for Body Area Networks</field>
<field name="keyword">tuturial</field>
<field name="keyword"> MAC</field>
<field name="keyword"> Body Area Networks</field>
<field name="keyword"> Body Sensor Networks</field>
<field name="keyword"> Wireless low-power Networks</field>
<field name="abstract">The tutorial will expose attendees to issues with MAC implementation from a specifications document, and to generic design issues with BAN MAC. It will show how implementation and design are tightly linked in practice, despite the theoretical ideal of clean separation. It will offer practical advice on MAC implementation; it will examine common pitfalls with MAC design, and propose ways to guard against them. Modelling and various abstractions will be studied, as well as different ways to implement the event-driven state machines that MACs are. This will be a hands-on tutorial: writing snippets of code, testing simple MAC ideas in simulation during the tutorial, making things break and fail! The Castalia Simulator will be used for all simulation needs, offering a platform tailored to BAN MAC evaluation needs. No prior knowledge of Castalia is required.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1576</field>
<field name="author">Vikas Reddy</field>
<field name="author">Andres Sanin</field>
<field name="author">Conrad Sanderson</field>
<field name="author">Brian Lovell</field>
<field name="title">Adaptive Patch-Based Background Modelling for Improved Foreground Object Segmentation and Tracking</field>
<field name="abstract">A robust foreground object segmentation technique is proposed, capable of dealing with image sequences containing noise, illumination variations and dynamic backgrounds. The method employs contextual spatial information by analysing each image on an overlapping patch-by-patch basis and obtaining a low-dimensional texture descriptor for each patch. Each descriptor is passed through an adaptive multi-stage classifier, comprised of a likelihood evaluation, an illumination invariant measure, and a temporal correlation check. A probabilistic foreground mask generation approach integrates the classification decisions by exploiting the overlapping of patches, ensuring smooth contours of the foreground objects as well as effectively minimising the number of errors. The parameter settings are robust against wide variety of sequences and post-processing of foreground masks is not required. Experiments on the difficult Wallflower and I2R datasets show that the proposed method obtains considerably better results (both qualitatively and quantitatively) than methods based on Gaussian mixture models, feature histograms, and normalised vector distances. Further experiments on the CAVIAR dataset (using several tracking algorithms) indicate that the proposed method leads to considerable improvements in object tracking accuracy.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1577</field>
<field name="author">Steve Glass</field>
<field name="author">Marius Portmann</field>
<field name="author">Vallipuram Muthukkumarasamy</field>
<field name="title">The Insecurity of Time-of-Arrival Distance Ranging in IEEE 802.11 Wireless Networks</field>
<field name="keyword">Computer network security</field>
<field name="keyword"> Wireless LAN</field>
<field name="keyword"> Delay estimation</field>
<field name="keyword"> Distance measurement</field>
<field name="keyword"> Position measurement</field>
<field name="abstract">Two-way Time-of-Arrival (TOA) distance-ranging is well-suited for use in IEEE 802.11 MANETs and wireless mesh networks because it is simple, efficient and does not require precise time synchronization between network stations. Despite its utility we show that this distance-ranging procedure is completely insecure and demonstrate how it can be subverted by a simple but highly effective attack. This attack allows the adversary comprehensive and fine-grained control over the distance reported by the procedure. Such adversaries can appear to be either much further away or much closer than they are in reality. We demonstrate the attack experimentally and also show how it can be implemented using ordinary wireless network interfaces. Finally, the necessary and sufficient conditions for the secure use of two-way TOA distance-ranging procedure in IEEE 802.11 wireless networks are identified.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1578</field>
<field name="author">Himanshu Gupta</field>
<field name="author">Vinay Ribeiro</field>
<field name="author">Anirban Mahanti</field>
<field name="title">A Case for Robust Semi-Experiments</field>
<field name="abstract">A semi-experimental methodology has been widely used to investigate Internet traf c scaling behavior and properties. A semi-experimental analysis of a packet trace involves modifying a speci c aspect of the packet arrival process. One then compares the scaling behavior before and after the modi cation and draws conclusions regarding the role played by the modified aspect of the packet arrival process.



In this paper we investigate the impact, the presence of anoma-lies in Internet traf c can have on the conclusions derived by the semi-experimental methodology. Our analysis on anomalousMAWI traces shows that the semi-experimental methodology often results in misleading inferences. To mitigate the impact of anomalies, we couple the semi-experiments with a recentlyproposed sketch-based procedure for robust estimation of scaling behavior. We call such semi-experiments as robust semi-experiments . We revisit few well-known semi-experiments and illustrate the differences in the conclusions derived by the semi-experimental methodology vis-a-vis robust semi-experimental methodology. Our analysis shows that the conclusions derivedby the robust semi-experimental methodology are consistent with prior studies.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1579</field>
<field name="author">Peizhao Hu</field>
<field name="author">Suan Khai Chong</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Shonali Krishnaswamy</field>
<field name="title">Context-aware and resource efficient sensing infrastructure for context-aware applications</field>
<field name="abstract">Middleware for wireless sensor networks and

middleware for context-aware applications both provide information

abstraction and programming support for gathering,

pre-processing, and managing sensor data. However the former

mostly concentrates on optimising the operations of the resource

constrained hardware and simplifying access to the raw

sensor data while the latter focuses on gathering sensor data,

pre-processing it to the abstract context information required

by the applications and providing reasoning on this data.

In this paper, we explore the idea of enhancing middleware

for context-aware applications with solutions from sensor

networks middleware to allow resource efficient and contextaware

management of sensing infrastructure. The decisions on

which sensor data needs to be delivered to the middleware

for evaluation are based on current contextual situations. The

approach allows to trade the level of confidence in context

information for resource efficiency in context provisioning

without a detrimental effect on the functionality of contextaware

applications.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1580</field>
<field name="author">Peizhao Hu</field>
<field name="author">Wee Lum Tan</field>
<field name="author">Ryan Wishart</field>
<field name="author">Marius Portmann</field>
<field name="author">Jadwiga Indulska</field>
<field name="title">MeshVision: An Adaptive Wireless Mesh Network Video Surveillance System</field>
<field name="abstract">The major surveillance camera manufacturers have begun

incorporating wireless networking functionality into their products to

enable wireless access. However, the video feeds from such cameras can

only be accessed within the transmission range of the cameras. These

cameras must be connected to backbone infrastructure in order to ac-

cess them from more than one hop away. This network infrastructure

is both time-consuming and expensive to install, making it impractical

in many rapid deployment situations (for example to provide temporary

surveillance at a crime scene). To overcome this problem, we propose the

MeshVision system that incorporates wireless mesh network function-

ality directly into the cameras. Video streams can be pulled from any

camera within a network of MeshVision cameras, irrespective of how

many hops away that camera is. To manage the trade-o&#11; between video

stream quality and the number of video streams that could be concur-

rently accessed over the network, MeshVision uses a Bandwidth Adapta-

tion Mechanism. This mechanism monitors the wireless network looking

for drops in link quality or signs of congestion and adjusts the quality of

existing video streams in order to reduce that congestion. A signi&#12;cant

bene&#12;t of the approach is that it is low cost, requiring only a software

upgrade of the cameras.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1581</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="title">On the complexity of Temporal Defeasible Logic</field>
<field name="keyword">defeasible logic</field>
<field name="keyword"> temporal defeasible logic</field>
<field name="keyword"> complexity</field>
<field name="abstract">In this paper we investigate the complexity of temporal defeasible

logic, and we propose an efficient algorithm to compute the extension

of a temporalised defeasible theory. We motivate the logic

showing how it can be used to model deadlines.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1582</field>
<field name="author">Guido Governatori</field>
<field name="author">Francesco Olivieri</field>
<field name="author">Simone Scannapieco</field>
<field name="author">Matteo Cristani</field>
<field name="title">Superiority Based Revision of Defeasible Theories</field>
<field name="keyword">defeasible logic</field>
<field name="keyword"> preference revision</field>
<field name="abstract">We propose a systematic investigation on how to

modify a preference relation in a defeasible logic theory to

change the conclusions of the theory

itself. We argue that the approach we adopt is applicable to

legal reasoning, where users, in general,

cannot change facts and rules, but can propose their preferences

about the relative strength of the rules.



We provide a comprehensive study of the possible combinatorial cases

and we identify and analyse the cases where the revision process is

successful.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1583</field>
<field name="author">Guido Boella</field>
<field name="author">Guido Governatori</field>
<field name="author">Antonino Rotolo</field>
<field name="author">Leendert van der Torre</field>
<field name="title">A formal study on legal compliance and interpretation</field>
<field name="keyword">legal interpretation</field>
<field name="keyword"> extensive interpretation</field>
<field name="keyword"> restrictive interpretation</field>
<field name="keyword"> belief revision</field>
<field name="keyword"> defeasible logic</field>
<field name="abstract">If compliance with a norm does not achieve its

 purpose, then its applicability must dynamically be

 restricted or expanded. Legal interpretation is a

 mechanism from law allowing norms to be adapted to

 unforeseen situations. We model this mechanism for

 norms regulating computer systems by representing

 the purpose of norms by social goals and by revising

 the constitutive rules defining the applicability of

 norms. We illustrate the interpretation mechanism

 by examples.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1584</field>
<field name="author">Konstanty Bialkowski</field>
<field name="author">Marius Portmann</field>
<field name="title">Design of testbed for Wireless Mesh Networks</field>
<field name="keyword">Testbed</field>
<field name="keyword"> Wireless Mesh</field>
<field name="abstract">This paper reports on the design and performance of a testbed for evaluating 802.11

wireless devices in the 2.45 GHz and 5 GHz frequency bands. This instrument

differs from simple channel emulators which model a wireless channel between two

nodes. The device uses electronically controlled wireless channels, and as a result

is able to provide accurate and repeatable wireless environments for the testing

of wireless metrics and routing protocols. These environments include physical

scenarios with various grades of path loss, multi-path fading and interference. The

testbed is currently configured to support up to twelve variable links, or five nodes

in symmetric link configuration.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1585</field>
<field name="author">Jun Yang</field>
<field name="author">Yang Wang</field>
<field name="author">Arcot Sowmya</field>
<field name="author">Matt (Bang) Zhang</field>
<field name="author">Jie Xu</field>
<field name="author">Zhidong Li</field>
<field name="title">Affinity Propagation Feature Clustering with Application to Vehicle Detection and Tracking in Road Traffic Surveillance</field>
<field name="keyword">feature clustering</field>
<field name="keyword"> vehicle tracking</field>
<field name="abstract">In this paper, we investigate the applicability of the newly proposed data clustering method, affinity propagation, in feature points clustering and the task of vehicle detection and tracking in road traffic surveillance. We propose a model-based temporal association scheme and novel preprocessing and postprocessing operations which together with affinity propagation make a quite successful method for the given task. Our experiments demonstrate the effectiveness and efficiency of our method and its superiority over the state-of-the-art algorithm.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1586</field>
<field name="author">Wayes Tushar</field>
<field name="author">David Smith</field>
<field name="title">Distributed Transmit Beamforming Based on A 3-Bit Feedback System</field>
<field name="abstract">A distributed transmit beamforming technique is developed for both a static and a time-varying channel in a wireless sensor network. This algorithm is based on an iterative procedure that synchronizes the transmitters to send a common message signal coherently to the receiver, using a 3-bit feedback in each timeslot. Results show that the received power increases quadratically with number of transmitters, and there is a substantially improved performance over a 1-bit feedback system

and system without feedback.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1587</field>
<field name="author">Eddie Li</field>
<field name="author">Liam O'Brien</field>
<field name="author">He (Jason) Zhang</field>
<field name="author">Jacky Keung</field>
<field name="title">Toward SOA Implementation Complexity Measurement Enlightened by Organization Theory</field>
<field name="keyword">Service-Oriented Architecture (SOA)</field>
<field name="keyword"> Organization Theory</field>
<field name="keyword"> Organizational Complexity</field>
<field name="keyword"> SOA Implementation Complexity</field>
<field name="keyword"> Measurement</field>
<field name="abstract">When implementing information infrastructures to support business, the Service-Oriented Architecture (SOA) based systems are inevitably more complex than the traditional architecture based systems. Therefore, it is essential to measure the implementation complexity to avoid the development un-worthy of adopting SOA. However, there are few works that can cover the full scope of complexity measurement for SOA implementation. Through organizationally comprehending SOA and borrowing existing work related to organizational complexity, this paper proposes a framework for measuring the complexity of SOA implementation, which comprises four dimensions: Structure, Environment, Business and Resource. This framework can then instruct the future work of proposing metrics for integrally and quantitatively calculating the complexity when implementing SOA.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1588</field>
<field name="author">Tofazzal Hossain</field>
<field name="author">David Smith</field>
<field name="title">Cooperative Communications: Synchronization in Fast Flat-Fading Channels with Various Signal Constellations</field>
<field name="abstract">A non-data-aided near maximum likelihood (NDA-NML) symbol timing estimator is applied to cooperative communications

in fast flat-fading conditions. The performance of a three-node detect-and-forward cooperative communication system

is evaluated operating in fast Rayleigh fading channels using various signal constellations with and without synchronization. Timing errors have significant effect on performance. The lower complexity NDA-NML estimator performs well for larger signal constellations in fast fading, when compared to a data aided maximum likelihood (DA-ML) estimator.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1589</field>
<field name="author">Wei Yin</field>
<field name="author">Konstanty Bialkowski</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Peizhao Hu</field>
<field name="title">Evaluations of MadWifi MAC Layer Rate Control Mechanisms</field>
<field name="abstract">The 802.11 standards specify several transmission rates that can be used at the MAC layer protocol to adapt the transmission rate to channel conditions. Such dynamic adaptations can improve per-hop performance in Wireless Networks and therefore can have impact on the Quality of Service provided for communicating applications. In this paper we present a comprehensive evaluation of the performance of four rate control mechanisms used by the MadWifi driver in Linux: Onoe, AMRR, SampleRate and minstrel. The evaluation of these four rate control mechanisms was carried out in our platform for controllable and repeatable experiments.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1590</field>
<field name="author">Gerwin Klein</field>
<field name="title">A Formally Verified OS Kernel. Now What?</field>
<field name="keyword">Isabelle/HOL</field>
<field name="keyword"> seL4</field>
<field name="abstract">Last year, the L4.verified project produced a formal, machine-checked Isabelle/HOL proof that the C code of the seL4 OS microkernel correctly implements its abstract implementation. In my presentation I will summarise the proof together with its main implications and assumptions, I will describe in which kinds of systems this formally verified kernel can be used for gaining assurance on overall system security, and I will explore further future research directions that open up with a formally verified OS kernel.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1591</field>
<field name="author">Tuan Hue Thi</field>
<field name="author">Jian Zhang</field>
<field name="author">Li Cheng</field>
<field name="author">Li Wang</field>
<field name="author">Shinichi Satoh</field>
<field name="title">Human Action Recognition and Localization in Video using Structured Learning of Local Space-Time Features</field>
<field name="keyword">Action Recognition and Localization</field>
<field name="keyword"> Structured learning</field>
<field name="abstract">This paper presents a unified framework for human action classification and localization in video using structured learning of local space-time features. Each human action class is represented by a set of its own compact set of local patches. In our approach, we first use a discriminative hierarchical Bayesian classifier to select those spacetime interest points that are constructive for each particular action. Those concise local features are then passed to a

Support Vector Machine with Principle Component Analysis projection for the classification task. Meanwhile, the action localization is done using Dynamic Conditional Random Fields developed to incorporate the spatial and temporal structure constraints of superpixels extracted around those features. Each superpixel in the video is defined by the shape and motion information of its corresponding feature region. Compelling results obtained from experiments on KTH [25], Weizmann [1], HOHA [16] and TRECVid [26] datasets have proven the efficiency and robustness of our framework for the task of human action recognition and localization in video.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1592</field>
<field name="author">Aditi Barthwal</field>
<field name="author">Michael Norrish</field>
<field name="title">Mechanisation of PDA and Grammar Equivalence for Context-Free Languages</field>
<field name="keyword">language theory</field>
<field name="keyword"> interactive theorem proving</field>
<field name="abstract">We provide a formalisation of the theory of pushdown automata (PDAs) using the HOL4 theorem prover. It illustrates how provers such as HOL can be used for mechanising complicated proofs, but also how intensive such a process can turn out to be. The proofs blow up in size in way difficult to predict from examining original textbook presentations. Even a meticulous text proof has intuitive leaps that need to be identified and formalised.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1593</field>
<field name="author">Michael von Tessin</field>
<field name="title">Towards High-Assurance Multiprocessor Virtualisation</field>
<field name="keyword">formal verification</field>
<field name="keyword"> multiprocessor</field>
<field name="keyword"> microkernel</field>
<field name="keyword"> virtualisation</field>
<field name="keyword"> seL4</field>
<field name="keyword"> Isabelle/HOL</field>
<field name="abstract">Virtualisation is increasingly being used in security-critical systems to provide isolation between system components. Being the foundation of any virtualised system, hypervisors need to provide a high degree of assurance with regards to correctness and isolation. Microkernels, such as seL4, can be used as hypervisors. Functional correctness of seL4's uniprocessor C implementation has been formally verified. The framework employed to verify seL4 is tailored to facilitate reasoning about sequential programs. However, we want to be able to use the full power of multiprocessor/multicore systems, and at the same time, leverage the high assurance seL4 already gives us for uniprocessors.



This work-in-progress paper explores possible multiprocessor designs of seL4 and their amenability to verification. For the chosen design, it contributes a formal multiprocessor execution model to lift seL4's uniprocessor model and proofs into a multiprocessor context using only minor modifications. The theorems proving the validity of the lift operation are machine-checked in Isabelle/HOL and walked-through in the paper.</field>
<field name="date">2015</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1594</field>
<field name="author">Karl Michael Goeschka</field>
<field name="author">Hye-young Paik</field>
<field name="author">Vladimir Tosic</field>
<field name="title">Engineering Middleware for Service-Oriented Computing: Editorial Note</field>
<field name="keyword">Service-oriented computing</field>
<field name="keyword"> Web service</field>
<field name="keyword"> middleware</field>
<field name="keyword"> Service level agreement</field>
<field name="keyword"> Ontology</field>
<field name="keyword"> Grid computing</field>
<field name="keyword"> Cloud computing</field>
<field name="keyword"> Mobile computing</field>
<field name="keyword"> Fault tolerance.</field>
<field name="abstract">This special issue of the International Journal of Systems and Service-Oriented Engineering (IJSSOE) contains five peer-reviewed research papers with recent advances on middleware for service-oriented computing. In the editorial, we first explain importance of middleware for service-oriented computing. Then, we give a brief overview of the review process and the papers accepted for the special issue. Finally, we discuss some open challenges for future research, with an emphasis on increasingly popular mobile service-oriented systems and cloud computing. The editorial note is followed by lists of review board members and additional reviewers for this special issue.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1595</field>
<field name="author">Ahmad Ali Iqbal</field>
<field name="author">Max Ott</field>
<field name="author">Aruna Seneviratne</field>
<field name="title">Resource Selection from Distributed Semantic Web Stores</field>
<field name="keyword">resource selection</field>
<field name="keyword"> RDF</field>
<field name="keyword"> distributed</field>
<field name="keyword"> information stores</field>
<field name="abstract">Semantic web is gaining popularity as the candidate for next generation World Wide Web. Distribution of the data across number of physical information stores and proliferation of semantic web data brings variety of non-trivial challenges. One of such challenge is to identify information stores for a given query. This paper presents a framework to address this problem probabilistically and by exchanging the summaries of actual contents. Experimental evaluation shows promising results with high discovery for probabilistic approach and lesser response time for pre-processed summary exchanges.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1596</field>
<field name="author">Leif Hanlen</field>
<field name="author">Vasanta Chaganti</field>
<field name="author">Ben Gilbert</field>
<field name="author">David Rodda</field>
<field name="author">Tharaka Lamahewa</field>
<field name="author">David Smith</field>
<field name="title">Open-source Testbed for Body Area Networks: 200 sample/sec, 12 hrs Continuous Measurement.</field>
<field name="abstract">We present the design criteria and specifications of a novel Open-Source hardware channel sounder and Open-Source data sets for measurements of the Body Area Channel at the 2400MHz ISM band and 2360MHz band. We outline a need for open hardware and measurement data to facilitate robust standardization of the new Body Area Networks. We demonstrate typical analyses on a public data set, with reference to previous works, and show how complex network topologies may be simulated through simple real measurements using reciprocity.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1597</field>
<field name="author">Shivanajay Marwaha</field>
<field name="author">Jadwiga Indulska</field>
<field name="author">Marius Portmann</field>
<field name="title">Quality of Service Provisioning in Wireless Mobile Ad Hoc Networks: Current State of the Art</field>
<field name="abstract">Wireless networks such as Bluetooth, WLAN and WiMax have transformed the way we access information and communicate seamlessly whether we are at home, in the office, or on the move on a train, bus or even aircraft. As mobile and embedded computing devices become more omnipresent, it will become increasingly difficult to interconnect them via wires and single-hop wireless links limited by radio transmission range. This has given rise to mobile ad hoc networks (MANET) where far away nodes communicate by requesting intermediate nodes to relay their information in order to reach the destination. MANETs self-organize, self-configure and self-heal themselves. MANETs are being used in many applications ranging from emergency response situations to wireless vehicular ad hoc networks. Many applications of MANETs such as Emergency Response and First Responders have strict Quality of Service (QoS) requirements for their communications systems, making MANET QoS provisioning mechanisms very crucial for supporting multimedia communications such as real-time audio and video. However, QoS provisioning in MANETs is quite tough in comparison to QoS provisioning in wireline IP networks. This is due to numerous reasons such as the dynamic network topology, unpredictable communication medium and limited battery power of mobile devices forming the network. This chapter describes the challenges and the current state of the art of QoS protocols and mechanisms in MANETs.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1598</field>
<field name="author">Paul Zhang</field>
<field name="author">Yongsheng Gao</field>
<field name="author">Terry Caelli</field>
<field name="title">Primitive-based 3D Structure Inference from a Single 2D Image for Insect Modeling: Towards an Electronic Field Guide for Insect Identification</field>
<field name="keyword">3D model</field>
<field name="keyword"> insect description</field>
<field name="keyword"> structure inference</field>
<field name="keyword"> 3D reconstruction</field>
<field name="abstract">3D insect models are useful to overcome viewing angle variations and self-occlusions in computer-assisted insect

taxonomy for electronic field guides. The acquisition of 3D information is, however, unreliable due to the flexibility and small size of the insect bodies. This paper explores how to infer 3D insect models from a single 2D insect image, which will assist both insect description and identification. The 3D structure of the insect body is modeled from two geometric primitives, generalized cylinders and deformable ellipsoids. The primitives are fitted and warped based on both edge and medial axis constraints of the 2D image. Individualized 3D models are then built to approximate the insect structure. The proposed approach results in seemingly useful 3D insect models capable of representing the major morphological characteristics for a variety of insects with different body types. This method could be a helpful assistance for computer-assisted insect taxonomy and insect identification by entomologists and the public.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1599</field>
<field name="author">Yuxin Deng</field>
<field name="author">Robert van Glabbeek</field>
<field name="author">Matthew Hennessy</field>
<field name="author">Carroll Morgan</field>
<field name="title">Real Reward Testing for Probabilistic Processes (Extended Abstract)</field>
<field name="keyword">Probabilistic processes</field>
<field name="keyword"> nondeterminism</field>
<field name="keyword"> transition systems</field>
<field name="keyword"> testing equivalences</field>
<field name="keyword"> reward testing</field>
<field name="keyword"> failure simulation</field>
<field name="keyword"> divergence.</field>
<field name="abstract">We introduce a notion of real-valued reward testing for probabilistic

processes by extending the traditional nonnegative reward testing with

negative rewards. In this testing framework, the may and must

preorders turn out to be inverses. We show that for convergent

processes with finitely many states and transitions, but not in the

presence of divergence, the real reward must testing preorder

coincides with the nonnegative reward must testing preorder. To prove

this coincidence we characterise the usual resolution-based testing in

terms of the weak transitions of processes, without involving

policies, adversaries, schedulers, resolutions, or similar structures

that are external to the process under investigation. This requires

establishing the continuity of our function for calculating testing

outcomes.</field>
<field name="date">2012</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1600</field>
<field name="author">Yan Shvartzshnaider</field>
<field name="author">Max Ott</field>
<field name="author">Levy David</field>
<field name="title">Publish/Subscribe On Top Of DHT Using RETE algorithm</field>
<field name="keyword">publish/subscribe system</field>
<field name="keyword"> distributed pattern matching</field>
<field name="keyword"> global semantic graph</field>
<field name="keyword"> Rete algorithm</field>
<field name="abstract">This paper discusses the construction of a Global Semantic Graph (GSG) [1] to support future information- and collaboration-centric applications and services. The GSG is a publish/subscribe (pub/sub)

based architecture that supports publication of tuples and subscriptions with standing graph queries. We believe that an implementation of an efficient pattern matching algorithm such as Rete [2] on top of a distributed environment might serve as a possible substrate for GSG s pub/sub facility. Rete operates on loosely coupled alpha, beta and join nodes and therefore has been chosen by us for implementation in a distributed setting. In this paper, we propose a way to perform Rete s pattern matching over a DHT-based Structured P2P network to provide a scalable content-

based publish/subscribe service.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1601</field>
<field name="author">Wray Buntine</field>
<field name="author">Marko Grobelnik</field>
<field name="author">Dunja Mladenic</field>
<field name="author">John Shawe-Taylor</field>
<field name="title">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2009, Proceedings Part I</field>
<field name="abstract">The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) will take place in Bled, Slovenia, from September 7th to 11th, 2009. This event builds upon a very successful series of 19 ECML and 12 PKDD conferences, which have been jointly organized for the past eight years. It has become the major European scientific event in these fields and in 2009 it will comprise presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1602</field>
<field name="author">Wray Buntine</field>
<field name="author">Marko Grobelnik</field>
<field name="author">Dunja Mladenic</field>
<field name="author">John Shawe-Taylor</field>
<field name="title">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2009, Proceedings Part II</field>
<field name="abstract">The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) will take place in Bled, Slovenia, from September 7th to 11th, 2009. This event builds upon a very successful series of 19 ECML and 12 PKDD conferences, which have been jointly organized for the past eight years. It has become the major European scientific event in these fields and in 2009 it will comprise presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.</field>
<field name="date">2010</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1603</field>
<field name="author">Julian McAuley</field>
<field name="author">Tiberio Caetano</field>
<field name="title">Exploiting data-independence for fast belief-propagation</field>
<field name="abstract">Maximum a posteriori (MAP) inference in graphical models requires that we maximize the sum of two terms: a data-dependent term, encoding the conditional likelihood of a certain labeling given an observation, and a data-independent term, encoding some prior on labelings. Often, the data-dependent factors contain fewer latent variables than the data-independent factors -- for instance, many grid and tree-structured models consist of only first-order conditionals despite having pairwise priors. In this paper, we note that MAP-inference in any such graphical model can be made substantially faster by appropriately preprocessing its data-independent terms. Our main result is to show that message-passing in any such pairwise model has an expected-case exponent of only $1.5$ on the number of states per node, leading to significantly faster algorithms than the standard quadratic time solution.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1604</field>
<field name="author">Suronapee Phoomvuthisarn</field>
<field name="author">Jenny Liu</field>
<field name="author">Liming Zhu</field>
<field name="title">An Architectural Approach to Composing Reputation-based Distributed Services</field>
<field name="keyword">service oriented computing</field>
<field name="keyword"> distribtued computing</field>
<field name="keyword"> software architecture</field>
<field name="keyword"> trust</field>
<field name="keyword"> auction mechanism</field>
<field name="abstract">In distributed environments, a Reputation-Based Trust (RBT) model with embedded incentive mechanisms provides an accurate quantitative measurement for services choosing their partners based on fair ratings accumulated from users. Such mechanisms stimulate services to offer ratings truthfully, otherwise they lose their gains or even receive penalties. However, leveraging such mechanisms in distributed environments is a challenging task by its centralized nature. In this paper, we propose a new architecture development that combines relevant architectural components to make trust systems highly scalable with the auction mechanisms capability to prevent lie. In this architecture we define an auction-based trust negotiation protocol that guides the interactions of distributed services and realize it in the distributed trust framework. The contribution of our architecture is that it scales efficiently for enormous services interacting with the system. The resulting architecture also produces accurate results to achieve protection against untruthful incentives, especially when a majority of ratings are unfair. An example on a supply chain scenario is devised with empirical evidence collected.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1605</field>
<field name="author">Juan Li</field>
<field name="author">Ross Jeffery</field>
<field name="author">Kam Hay Fung</field>
<field name="author">Liming Zhu</field>
<field name="author">Qing Wang</field>
<field name="author">Xiwei (Sherry) Xu</field>
<field name="title">A Business Process-driven Approach for Requirements Dependency Analysis</field>
<field name="abstract">Dependencies among software artifacts are very useful information for various software maintenance activities such as change impact analysis and maintenance effort estimation. In the past, the focus on artifact dependencies has been at the design and code level rather than at the requirements level due to the difficulties in identifying dependencies in a text-based requirements specification. We observe that difficulties reside in the disconnection among itemized requirements and the lack of a more systematic approach to write text-based requirements. Business process models are increasingly used as an important part of a requirements specification. In this paper, we present a business process-driven approach for requirements dependency analysis. A mapping between some generic business process patterns (derived from workflow patterns) and dependency types is proposed to aid dependency identification and change impact analysis. We evaluated our approach in a real-world project. The results show that practitioners with the help of our approach can systematically and manually discover more dependencies than people using text-based requirements only. Many of these additional dependencies are very useful for change impact analysis but highly difficult to spot from the text-based requirements.</field>
<field name="date">2013</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1606</field>
<field name="author">Brian Lam</field>
<field name="title">SPINdle - User Guide</field>
<field name="keyword">Defeasible Logic</field>
<field name="keyword"> SPINdle</field>
<field name="abstract">The main goal of this guide is to provide an introduction to the defeasible logics reasoner SPINdle . It teaches researchers how to carry out research using SPINdle and application developers how to embed SPINdle into their applications. In addition, it also shows readers how to make proper use of available features. It does not attempt to cover the theoretical aspect of defeasible logic in detail. Readers interested in this subject can refer to (Nute 1994; Antoniou et al. 2001) for details.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
<doc>
<field name="id">1607</field>
<field name="author">Adrian Bishop</field>
<field name="title">Gaussian-Sum-Based Probability Hypothesis Density Filtering with Delayed and Out-of-Sequence Measurements</field>
<field name="abstract">The problem of multiple-sensor-based multiple-object tracking is studied for adverse environments involving clutter (false positives), missing measurements (false negatives) and random target births and deaths (a priori unknown target numbers). Various (potentially spatially separated) sensors are assumed to generate signals which are sent to the estimator via parallel channels which incur independent delays. These signals may arrive out of order, be corrupted or even lost. In addition, there may be periods when the estimator receives no information. A closed-form, recursive solution to the considered problem is detailed that generalizes the Gaussian-mixture probability hypothesis density (GM-PHD) filter previously detailed in the literature. This generalization allows the GM-PHD framework to be applied in more realistic network scenarios involving not only transmission delays but rather more general irregular measurement sequences where particular measurements from some sensors can arrive out of order with respect to the generating sensor and also with respect to the signals generated by the other sensors in the network.</field>
<field name="date">2011</field>
<field name="publisher">Data61</field>
</doc>
</add>
