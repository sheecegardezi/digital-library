paper_title,venue,year,author_names,bibtex,abstract,pdf_url,google_search_url,source_page_url
Real-time communications for the web,Communications Magazine IEEE vol. 51 (2013) pp. 20-26,2013,Cullen Jenngins Ted Hardie Magnus Westerlund,@article{41400 title = {Real-time communications for the web} author = {Cullen Jenngins and Ted Hardie and Magnus Westerlund} year = 2013 URL = {http://ieeexplore.ieee.org/application/enterprise/entconfirmation.jsp?arnumber=6495756&icp=false} journal = {Communications Magazine IEEE} pages = {20-26} volume = {51} },This article provides an overview of the work that W3C and IETF are doing toward defining a framework protocols and application programming interfaces that will provide real-time interactive voice video and data in web browsers and other applications. The article explains how media and data will flow in a peer-to-peer style directly between two web browsers. This explains the protocols used to transport and secure the encrypted media traverse NATs and firewalls negotiate media capabilities and provide identity for the media.,http://ieeexplore.ieee.org/application/enterprise/entconfirmation.jsp?arnumber=6495756&icp=false,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Real-time+communications+for+the+web+Jenngins+Hardie+Westerlund,http://research.google.com/pubs/pub41400.html
Local Collaborative Ranking,International World Wide Web Conference (2014),2014,Joonseok Lee Samy Bengio Seungyeon Kim Guy Lebanon Yoram Singer,@inproceedings{42242 title = {Local Collaborative Ranking} author = {Joonseok Lee and Samy Bengio and Seungyeon Kim and Guy Lebanon and Yoram Singer} year = 2014 booktitle = {International World Wide Web Conference} },Personalized recommendation systems are used in a wide variety of applications such as electronic commerce social networks web search and more. Collaborative filtering approaches to recommendation systems typically assume that the rating matrix (e.g. movie ratings by viewers) is low-rank. In this paper we examine an alternative approach in which the rating matrix is \emph{locally low-rank}. Concretely we assume that the rating matrix is low-rank within certain neighborhoods of the metric space defined by (user item) pairs. We combine a recent approach for local low-rank approximation based on the Frobenius norm with a general empirical risk minimization for ranking losses. Our experiments indicate that the combination of a mixture of local low-rank matrices each of which was trained to minimize a ranking loss outperforms many of the currently used state-of-the-art recommendation systems. Moreover our method is easy to parallelize making it a viable approach for large scale real-world rank-based recommendation systems.,http://research.google.com/pubs/archive/42242.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Local+Collaborative+Ranking+Lee+Bengio+Kim+Lebanon+Singer,http://research.google.com/pubs/pub42242.html
Weave: Scripting Cross-Device Wearable Interaction,CHI 2015: ACM Conference on Human Factors in Computing Systems ACM pp. 3923-3932,2015,Pei-Yu (Peggy) Chi Yang Li,@inproceedings{44264 title = {Weave: Scripting Cross-Device Wearable Interaction} author = {Pei-Yu (Peggy) Chi and Yang Li} year = 2015 booktitle = {CHI 2015: ACM Conference on Human Factors in Computing Systems} pages = {3923-3932} },Provides a set of high-level APIs based on JavaScript and integrated tool support for developers to easily distribute UI output and combine user input and sensing events across devices for cross-device interaction.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Weave:+Scripting+Cross-Device+Wearable+Interaction+Chi+Li,http://research.google.com/pubs/pub44264.html
Temporal/Spatial Calendar Events and Triggers,Defensive Publications Series Technical Disclosure Commons (2015),2015,Daniel V. Klein Dean Jackson,@incollection{43422 title = {Temporal/Spatial Calendar Events and Triggers} author = {Daniel V. Klein and Dean Jackson} year = 2015 URL = {http://www.tdcommons.org/dpubs_series/31/} booktitle = {Defensive Publications Series} },Spatial and/or temporal triggers may be established so that when actuated one or more notifications such as reminders may be provided to one or more users. These triggers may be established manually e.g. by a user operating a user interface automatically e.g. by scraping calendar and/or email data to ascertain and/or predict various aspects of upcoming appointments such as start times duration date location and so forth or a combination of the two. Spatial triggers may be actuated based on a determination that a user is or will be at a particular location. Temporal triggers may be actuated at particular points in time e.g. at the scheduled time of an event or at some predetermined time interval before or after the event. Using one or more triggers it is possible to provide notifications to a user at some predetermined time interval prior to a scheduled event so that the user has sufficient time to make appropriate arrangements such as buying tickets making a reservation scheduling a rendezvous with a friend and so forth. A calendar system may also be interfaced with to manually or automatically establish triggers.,http://www.tdcommons.org/dpubs_series/31/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Temporal/Spatial+Calendar+Events+and+Triggers+Klein+Jackson,http://research.google.com/pubs/pub43422.html
Inferring the Network Latency Requirements of Cloud Tenants,15th Workshop on Hot Topics in Operating Systems (HotOS XV) USENIX Association (2015),2015,Jeffrey C Mogul Ramana Rao Kompella,@inproceedings{43867 title = {Inferring the Network Latency Requirements of Cloud Tenants}} author = {Jeffrey C Mogul and Ramana Rao Kompella} year = 2015 URL = {https://www.usenix.org/conference/hotos15/workshop-program/presentation/mogul} booktitle = {15th Workshop on Hot Topics in Operating Systems (HotOS XV)} },Cloud IaaS and PaaS tenants rely on cloud providers to provide network infrastructures that make the appropriate tradeoff between cost and performance. This can include mechanisms to help customers understand the performance requirements of their applications. Previous research (e.g. Proteus and Cicada) has shown how to do this for network-bandwidth demands but cloud tenants may also need to meet latency objectives which in turn may depend on reliable limits on network latency and its variance within the cloud providers infrastructure. On the other hand if network latency is sufficient for an application further decreases in latency might add cost without any benefit. Therefore both tenant and provider have an interest in knowing what network latency is good enough for a given application. This paper explores several options for a cloud provider to infer a tenants network-latency demands with varying tradeoffs between requirements for tenant participation accuracy of inference and instrumentation overhead. In particular we explore the feasibility of a hypervisor-only mechanism which would work without any modifications to tenant code even in IaaS clouds.,http://research.google.com/pubs/archive/43867.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Inferring+the+Network+Latency+Requirements+of+Cloud+Tenants%7D+Mogul+Kompella,http://research.google.com/pubs/pub43867.html
A Pole-Zero Filter Cascade Provides Good Fits to Human Masking Data and to Basilar Membrane and Neural Data,Mechanics of Hearing (2011),2011,Richard F. Lyon,@inproceedings{37210 title = {A Pole-Zero Filter Cascade Provides Good Fits to Human Masking Data and to Basilar Membrane and Neural Data} author = {Richard F. Lyon} year = 2011 booktitle = {Mechanics of Hearing} },A cascade of two-pole–two-zero filters with level-dependent pole and zero dampings with few parameters can provide a good match to human psychophysical and physiological data. The model has been fitted to data on detection threshold for tones in notched-noise masking including bandwidth and filter shape changes over a wide range of levels and has been shown to provide better fits with fewer parameters compared to other auditory filter models such as gammachirps. Originally motivated as an efficient machine implementation of auditory filtering related to the WKB analysis method of cochlear wave propagation such filter cascades also provide good fits to mechanical basilar membrane data and to auditory nerve data including linear low-frequency tail response level-dependent peak gain sharp tuning curves nonlinear compression curves level-independent zero-crossing times in the impulse response realistic instantaneous frequency glides and appropriate level-dependent group delay even with minimum-phase response. As part of exploring different level-dependent parameterizations of such filter cascades we have identified a simple sufficient condition for stable zero-crossing times based on the shifting property of the Laplace transform: simply move all the $s$-domain poles and zeros by equal amounts in the real-$s$ direction. Such pole-zero filter cascades are efficient front ends for machine hearing applications such as music information retrieval content identification speech recognition and sound indexing.,http://research.google.com/pubs/archive/37210.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Pole-Zero+Filter+Cascade+Provides+Good+Fits+to+Human+Masking+Data+and+to+Basilar+Membrane+and+Neural+Data+Lyon,http://research.google.com/pubs/pub37210.html
Multi-component Models for Object Detection,European Conference on Computer Vision Springer (2012) Volume 4 445-458,2012,Chunhui Gu Pablo Arbelaez Yuanqing Lin Kai Yu Jitendra Malik,@inproceedings{40665 title = {Multi-component Models for Object Detection} author = {Chunhui Gu and Pablo Arbelaez and Yuanqing Lin and Kai Yu and Jitendra Malik} year = 2012 URL = {http://link.springer.com/chapter/10.1007%2F978-3-642-33765-9_32} booktitle = {European Conference on Computer Vision} pages = {Volume 4 445--458} },In this paper we propose a multi-component approach for object detection. Rather than attempting to represent an object category with a monolithic model or pre-defining a reduced set of aspects we form visual clusters from the data that are tight in appearance and configuration spaces. We train individual classifiers for each component and then learn a second classifier that operates at the category level by aggregating responses from multiple components. In order to reduce computation cost during detection we adopt the idea of object window selection and our segmentation-based selection mechanism produces fewer than 500 windows per image while preserving high object recall. When compared to the leading methods in the challenging VOC PASCAL 2010 dataset our multi-component approach obtains highly competitive results. Furthermore unlike monolithic detection methods our approach allows the transfer of finer-grained semantic information from the components such as keypoint location and segmentation masks.,http://research.google.com/pubs/archive/40665.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multi-component+Models+for+Object+Detection+Gu+Arbelaez+Lin+Yu+Malik,http://research.google.com/pubs/pub40665.html
The Future of Computing Performance: Game Over or Next Level?,The National Academies Press (2011) pp. 200,2011,Samuel H. Fuller Luiz André Barroso Robert P. Colwell William J. Dally Dan Dobberpuhl Pradeep Dubey Mark D. Hill Mark Horowitz David Kirk Monica Lam Kathryn S. McKinley Charles Moore Katherine Yelick,none,"The end of dramatic exponential growth in single-processor performance marks the end of the dominance of the single microprocessor in computing. The era of sequential computing must give way to a new era in which parallelism is at the forefront. Although important scientific and engineering challenges lie ahead this is an opportune time for innovation in programming systems and computing architectures. We have already begun to see diversity in computer designs to optimize for such considerations as power and throughput. The next generation of discoveries is likely to require advances at both the hardware and software levels of computing systems. There is no guarantee that we can make parallel computing as common and easy to use as yesterday's sequential single-processor computer systems but unless we aggressively pursue efforts suggested by the recommendations in this book it will be ""game over"" for growth in computing performance. If parallel programming and related software efforts fail to become widespread the development of exciting new applications that drive the computer industry will stall; if such innovation stalls many other parts of the economy will follow suit. The Future of Computing Performance describes the factors that have led to the future limitations on growth for single processors that are based on complementary metal oxide semiconductor (CMOS) technology. It explores challenges inherent in parallel computing and architecture including ever-increasing power consumption and the escalated requirements for heat dissipation. The book delineates a research practice and education agenda to help overcome these challenges. The Future of Computing Performance will guide researchers manufacturers and information technology professionals in the right direction for sustainable growth in computer performance so that we may all enjoy the next level of benefits to society.",http://www.nap.edu/catalog.php?record_id=12980,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Future+of+Computing+Performance:+Game+Over+or+Next+Level%3F+Fuller+Barroso+Colwell+Dally+Dobberpuhl+Dubey+Hill+Horowitz+Kirk+Lam+McKinley+Moore+Yelick,http://research.google.com/pubs/pub40496.html
Deep boosting,Proceedings of the Thirty-First International Conference on Machine Learning (ICML 2014),2014,Corinna Cortes Mehryar Mohri Umar Syed,@inproceedings{42856 title = {Deep boosting} author = {Corinna Cortes and Mehryar Mohri and Umar Syed} year = 2014 booktitle = {Proceedings of the Thirty-First International Conference on Machine Learning (ICML 2014)} },We present a new ensemble learning algorithm DeepBoost which can use as base classifiers a hypothesis set containing deep decision trees or members of other rich or complex families and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a capacity-conscious criterion for the selection of the hypotheses. We give new data- dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm including the details of its derivation and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L1-regularized variants.,http://research.google.com/pubs/archive/42856.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deep+boosting+Cortes+Mohri+Syed,http://research.google.com/pubs/pub42856.html
Incremental Clicks: The Impact of Search Advertising,Journal of Advertising Research vol. 51 no. 4 (2011) pp. 643-647,2011,David X. Chan Yuan Yuan Jim Koehler Deepak Kumar,@article{38334 title = {Incremental Clicks: The Impact of Search Advertising} author = {David X. Chan and Yuan Yuan and Jim Koehler and Deepak Kumar} year = 2011 URL = {http://dx.doi.org/10.2501/JAR -51-4-643-647} journal = {Journal of Advertising Research} pages = {643--647} volume = {51 no. 4} },In this research the authors examined how the number of organic clicks changed when search ads were present and when search ad campaigns were turned off. The authors developed a statistical model to estimate the fraction of total clicks that could be attributed to search advertising. A meta-analysis of several hundred of these studies revealed that more than 89 percent of the ads clicks were incremental in the sense that those visits to the advertiser's site would not have occurred without the ad campaigns.,http://dx.doi.org/10.2501/JAR%20-51-4-643-647,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Incremental+Clicks:+The+Impact+of+Search+Advertising+Chan+Yuan+Koehler+Kumar,http://research.google.com/pubs/pub38334.html
Age-based Packet Arbitration in Large k-ary n-cubes,SC (2007),2007,Dennis Abts Deborah Weisser,@inproceedings{33386 title = {Age-based Packet Arbitration in Large k-ary n-cubes} author = {Dennis Abts and Deborah Weisser} year = 2007 URL = {http://sc07.supercomputing.org/schedule/event_detail.php?evid=11059} note = {sc07.supercomputing.org} booktitle = {SC} },As applications scale to increasingly large processor counts the interconnection network is frequently the limiting factor in application performance. In order to achieve application scalability the interconnect must maintain high bandwidth while minimizing variation in packet latency. As the offered load in the network increases with growing problem sizes and processor counts so does the expected maximum packet latency in the network directly impacting performance of applications with any synchronized communication. Age-based packet arbitration reduces the variance in packet latency as well as average latency. This paper describes the Cray XT router packet aging algorithm which allows globally fair arbitration by incorporating age in the packet output arbitration. We describe the parameters of the aging algorithm and how to arrive at appropriate settings. We show that an efficient aging algorithm reduces both the average packet latency and the variance in packet latency on communication-intensive benchmarks.,http://research.google.com/pubs/archive/33386.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Age-based+Packet+Arbitration+in+Large+k-ary+n-cubes+Abts+Weisser,http://research.google.com/pubs/pub33386.html
All Smiles : Automatic Photo Enhancement by Facial Expression Analysis,Conference for Visual Media Production (CVMP 2012) [Best Paper],2012,Rajvi Shah Vivek Kwatra,@inproceedings{41101 title = {All Smiles : Automatic Photo Enhancement by Facial Expression Analysis} author = {Rajvi Shah and Vivek Kwatra} year = 2012 URL = {https://sites.google.com/site/allsmilespaper/} booktitle = {Conference for Visual Media Production (CVMP 2012) [Best Paper]} },We propose a framework for automatic enhancement of group photographs by facial expression analysis. We are motivated by the observation that group photographs are seldom perfect. Subjects may have inadvertently closed their eyes may be looking away or may not be smiling at that moment. Given a set of photographs of the same group of people our algorithm uses facial analysis to determine a goodness score for each face instance in those photos. This scoring function is based on classifiers for facial expressions such as smiles and eye-closure trained over a large set of annotated photos. Given these scores a best composite for the set is synthesized by (a) selecting the photo with the best overall score and (b) replacing any low-scoring faces in that photo with high-scoring faces of the same person from other photos using alignment and seamless composition.,http://research.google.com/pubs/archive/41101.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=All+Smiles+:+Automatic+Photo+Enhancement+by+Facial+Expression+Analysis+Shah+Kwatra,http://research.google.com/pubs/pub41101.html
CSRIC III WORKING GROUP 4 Network Security Best Practices FINAL Report – BGP Security Best Practice,FCC (2013),2013,Jason Schiller Rodney Joffe Rod Rasmussen Mark Adams Steve Bellovin Donna Bethea-Murphy Rodney Buie Kevin Cox John Crain Michael Currie Dale Drew Chris Garner Joseph Gersch Jose A. Gonzalez Kevin Graves Chris Joul Tom Haynes Mazen Khaddam Ron Mathis Danny McPherson Doug Montgomery Heather Schiller Tony Tauber Marvin Simpson Ron Roman Elman Reyes Victor Oppleman Chris Oberg Russ White Paul Vixie Bob Wright,@misc{41862 title = {CSRIC III WORKING GROUP 4 Network Security Best Practices FINAL Report – BGP Security Best Practice} author = {Jason Schiller and Rodney Joffe and Rod Rasmussen and Mark Adams and Steve Bellovin and Donna Bethea-Murphy and Rodney Buie and Kevin Cox and John Crain and Michael Currie and Dale Drew and Chris Garner and Joseph Gersch and Jose A. Gonzalez and Kevin Graves and Chris Joul and Tom Haynes and Mazen Khaddam and Ron Mathis and Danny McPherson and Doug Montgomery and Heather Schiller and Tony Tauber and Marvin Simpson and Ron Roman and Elman Reyes and Victor Oppleman and Chris Oberg and Russ White and Paul Vixie and Bob Wright} year = 2013 URL = {http://transition.fcc.gov/bureaus/pshs/advisory/csric3/CSRIC_III_WG4_Report_March_%202013.pdf} },Discussion of best current practices for securing intra-domain routing prior to the wide spread adoption of BGPsec.,http://research.google.com/pubs/archive/41862.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=CSRIC+III+WORKING+GROUP+4++Network+Security+Best+Practices++++FINAL+Report+%E2%80%93+BGP+Security+Best+Practice+Schiller+Joffe+Rasmussen+Adams+Bellovin+Bethea-Murphy+Buie+Cox+Crain+Currie+Drew+Garner+Gersch+Gonzalez+Graves+Joul+Haynes+Khaddam+Mathis+McPherson+Montgomery+Schiller+Tauber+Simpson+Roman+Reyes+Oppleman+Oberg+White+Vixie+Wright,http://research.google.com/pubs/pub41862.html
Online Effects of Offline Ads,AdKDD08 (in the ACM digital library) ACM (2008) pp. 10-17,2008,Diane Lambert Daryl Pregibon,@inproceedings{34436 title = {Online Effects of Offline Ads} author = {Diane Lambert and Daryl Pregibon} year = 2008 URL = {http://research.microsoft.com/users/mbilenko/kdd_old/workshops/ADKDD.pdf} booktitle = {AdKDD08 (in the ACM digital library)} pages = {10-17} },We propose a methodology for assessing how ad campaigns in offline media such as print audio and TV affect online interest in the advertiser's brand. Online interest can be measured by daily counts of the number of search queries that contain brand related keywords by the number of visitors to the advertiser's web pages by the number of pageviews at the advertiser's websites or by the total duration of visits to the advertiser's website. An increase in outcomes like these in designated market areas (DMAs) where the offline ad appeared suggests heightened interest in the advertised product as long as there would have been no such increase if the ad had not appeared. We propose a regression analysis to estimate the incremental value of the ad campaign beyond the baseline interest that would have been seen if the campaign had not been shown. A small print ad campaign illustrates the method.,http://research.google.com/pubs/archive/34436.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Effects+of+Offline+Ads+Lambert+Pregibon,http://research.google.com/pubs/pub34436.html
A Computational Approach for Obstruction-Free Photography,ACM Transactions on Graphics vol. 34 no. 4 (Proc. SIGGRAPH) (2015),2015,Tianfan Xue Michael Rubinstein Ce Liu William T. Freeman,@article{43884 title = {A Computational Approach for Obstruction-Free Photography} author = {Tianfan Xue and Michael Rubinstein and Ce Liu and William T. Freeman} year = 2015 URL = {https://sites.google.com/site/obstructionfreephotography/} journal = {ACM Transactions on Graphics} volume = {34 no. 4 (Proc. SIGGRAPH)} },We present a unified computational approach for taking photos through reflecting or occluding elements such as windows and fences. Rather than capturing a single image we instruct the user to take a short image sequence while slightly moving the camera. Differences that often exist in the relative position of the background and the obstructing elements from the camera allow us to separate them based on their motions and to recover the desired background scene as if the visual obstructions were not there. We show results on controlled experiments and many real and practical scenarios including shooting through reflections fences and raindrop-covered windows.,https://sites.google.com/site/obstructionfreephotography/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Computational+Approach+for+Obstruction-Free+Photography+Xue+Rubinstein+Liu+Freeman,http://research.google.com/pubs/pub43884.html
The Shoebox and the Safe: When Once-Personal Information Changes Hands,Proceedings of the 5th International Workshop on Personal Information Management at CSCW 2012,2012,Manas Tungare,@inproceedings{40349 title = {The Shoebox and the Safe: When Once-Personal Information Changes Hands} author = {Manas Tungare} year = 2012 URL = {http://manas.tungare.name/publications/tungare_2012_shoebox} booktitle = {Proceedings of the 5th International Workshop on Personal Information Management at CSCW 2012} },This paper presents several examples where one user’s personal information is accessed by another without the consent of the owner or without the capability of the owner to consent to such sharing. While intentional sharing of information at home as well as at work has been studied in detail there is extremely limited understanding about the practices dimensions and models of unintentional sharing. Laws and policies that were developed with paper and other nondigital archives in mind are being found to be inadequate for addressing the challenges that digital personal information brings. Worse those laws are being enforced in inconsistent ways prompting lawsuits. Posthumously shared information brings up questions that have not been addressed before. This paper starts by noting examples of posthumous sharing and sharing without consent proposes models and dimensions for understanding it and concludes by proposing research questions that need to be addressed by the wider PIM community.,http://research.google.com/pubs/archive/40349.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Shoebox+and+the+Safe:+When+Once-Personal+Information+Changes+Hands+Tungare,http://research.google.com/pubs/pub40349.html
User browsing models: relevance versus examination,Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining ACM Washington DC (2010) pp. 223-232,2010,Ramakrishnan Srikant Sugato Basu Ni Wang Daryl Pregibon,@inproceedings{36588 title = {User browsing models: relevance versus examination} author = {Ramakrishnan Srikant and Sugato Basu and Ni Wang and Daryl Pregibon} year = 2010 URL = {http://doi.acm.org/10.1145/1835804.1835835} booktitle = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining} pages = {223-232} address = {Washington DC} },There has been considerable work on user browsing models for search engine results both organic and sponsored. The click-through rate (CTR) of a result is the product of the probability of examination (will the user look at the result) times the perceived relevance of the result (probability of a click given examination). Past papers have assumed that when the CTR of a result varies based on the pattern of clicks in prior positions this variation is solely due to changes in the probability of examination. We show that for sponsored search results a substantial portion of the change in CTR when conditioned on prior clicks is in fact due to a change in the relevance of results for that query instance not just due to a change in the probability of examination. We then propose three new user browsing models which attribute CTR changes solely to changes in relevance solely to changes in examination (with an enhanced model of user behavior) or to both changes in relevance and examination. The model that attributes all the CTR change to relevance yields substantially better predictors of CTR than models that attribute all the change to examination and does only slightly worse than the model that attributes CTR change to both relevance and examination. For predicting relevance the model that attributes all the CTR change to relevance again does better than the model that attributes the change to examination. Surprisingly we also find that one model might do better than another in predicting CTR but worse in predicting relevance. Thus it is essential to evaluate user browsing models with respect to accuracy in predicting relevance not just CTR.,http://doi.acm.org/10.1145/1835804.1835835,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=User+browsing+models:+relevance+versus+examination+Srikant+Basu+Wang+Pregibon,http://research.google.com/pubs/pub36588.html
A Fault Detection and Protection Scheme for Three-Level DC–DC Converters Based on Monitoring Flying Capacitor Voltage,IEEE Transactions on Power Electronics vol. 27 (2012) pp. 685-697,2012,Honggang Sheng Fred Wang C.W. Tipton,@article{39976 title = {A Fault Detection and Protection Scheme for Three-Level DC–DC Converters Based on Monitoring Flying Capacitor Voltage} author = {Honggang Sheng and Fred Wang and C.W. Tipton} year = 2012 URL = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5942191} journal = {IEEE Transactions on Power Electronics} pages = {685-697} volume = {27} },Fault detection and protection is an important design aspect for any power converter especially in high-power high-voltage applications where cost of failure can be high. The three-level dc-dc converter and its varied derivatives are attractive topologies in high-voltage high-power converter applications. The protection method can not only prevent the system failure against unbalanced voltage stresses on the switches but also provide a remedy for the system as faults occur and save the remaining components. The three-level converter is subject to voltage unbalance in certain abnormal conditions which can result in switch overvoltage and system failure. The reasons for the unbalanced voltage stresses are fully investigated and categorized. The solutions to each abnormal condition are introduced. In addition to the voltage unbalance the three-level converters can be protected against multiple faults by the proposed protection method through monitoring the flying capacitor voltage. Phenomena associated with each fault are thoroughly analyzed and summarized. The protection circuit is simple and can be easily implemented while it can effectively protect the three-level converters and its derivatives which has been verified by the experiment with a three-level parallel resonant converter.,http://research.google.com/pubs/archive/39976.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Fault+Detection+and+Protection+Scheme+for+Three-Level+DC%E2%80%93DC+Converters+Based+on+Monitoring+Flying+Capacitor+Voltage+Sheng+Wang+Tipton,http://research.google.com/pubs/pub39976.html
Dagstuhl Seminar 09141: Web Application Security (Abstracts collection),Dagstuhl Seminar Proceedings Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik Germany Dagstuhl Germany (2010),2010,Dan Boneh Úlfar Erlingsson Martin Johns Benjamin Livshits,@inbook{37841 title = {Dagstuhl Seminar 09141: Web Application Security (Abstracts collection)} author = {Dan Boneh and Úlfar Erlingsson and Martin Johns and Benjamin Livshits} year = 2010 URL = {http://drops.dagstuhl.de/opus/volltexte/2010/2726} booktitle = {Dagstuhl Seminar Proceedings} address = {Dagstuhl Germany} },From 29th March to 3rd April 2009 the Dagstuhl Seminar 09141 Web Application Security was held in Schloss Dagstuhl -- Leibniz Center for Informatics. During the seminar several participants presented their current research and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar are put together in this paper. Links to full papers (if available) are provided in the corresponding seminar summary document.,http://drops.dagstuhl.de/opus/volltexte/2010/2726,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dagstuhl+Seminar+09141:+Web+Application+Security+(Abstracts+collection)+Boneh+Erlingsson+Johns+Livshits,http://research.google.com/pubs/pub37841.html
The Tail at Scale,Communications of the ACM vol. 56 (2013) pp. 74-80,2013,Jeffrey Dean Luiz André Barroso,@article{40801 title = {The Tail at Scale} author = {Jeffrey Dean and Luiz André Barroso} year = 2013 URL = {http://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext} journal = {Communications of the ACM} pages = {74-80} volume = {56} },Systems that respond to user actions very quickly (within 100 milliseconds) feel more fluid and natural to users than those that take longer [Card et al 1991]. Improvements in Internet connectivity and the rise of warehouse-scale computing systems [Barroso & Hoelzle 2009] have enabled Web services that provide fluid responsiveness while consulting multi-terabyte datasets that span thousands of servers. For example the Google search system now updates query results interactively as the user types predicting the most likely query based on the prefix typed so far performing the search and showing the results within a few tens of milliseconds. Emerging augmented reality devices such as the Google Glass prototype will need associated Web services with even greater computational needs while guaranteeing seamless interactivity. It is challenging to keep the tail of the latency distribution low for interactive services as the size and complexity of the system scales up or as overall utilization increases. Temporary high latency episodes which are unimportant in moderate size systems may come to dominate overall service performance at large scale. Just as fault-tolerant computing aims to create a reliable whole out of less reliable parts we suggest that large online services need to create a predictably responsive whole out of less predictable parts. We refer to such systems as latency tail-tolerant or tail-tolerant for brevity. This article outlines some of the common causes of high latency episodes in large online services and describes techniques that reduce their severity or mitigate their impact in whole system performance. In many cases tail-tolerant techniques can take advantage of resources already deployed to achieve fault-tolerance resulting in low additional overheads. We show that these techniques allow system utilization to be driven higher without lengthening the latency tail avoiding wasteful over-provisioning.,http://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Tail+at+Scale+Dean+Barroso,http://research.google.com/pubs/pub40801.html
3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding,Proceedings of the International Conference on Computer Vision (ICCV) (2013) (to appear),2013,Scott Satkin Martial Hebert,@inproceedings{41643 title = {3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding} author = {Scott Satkin and Martial Hebert} year = 2013 booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)} },We present a new algorithm 3DNN (3D Nearest-Neighbor) which is capable of matching an image with 3D data independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints we free ourselves from the need to have training examples captured from all possible viewpoints. Thus we are able to achieve comparable results using orders of magnitude less data and recognize objects from never-before-seen viewpoints. In this work we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image we develop a scene matching approach which is truly 100% viewpoint invariant yielding state-of-the-art performance on challenging data.,http://research.google.com/pubs/archive/41643.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=3DNN:+Viewpoint+Invariant+3D+Geometry+Matching+for+Scene+Understanding+Satkin+Hebert,http://research.google.com/pubs/pub41643.html
JavaSpaces NetBeans: a linda workbench for distributed programming course,Proceedings of the fifteenth annual conference on Innovation and technology in computer science education ACM New York NY USA (2010) pp. 23-27,2010,Magdalena Dukielska Jacek Sroka,@inproceedings{36661 title = {JavaSpaces NetBeans: a linda workbench for distributed programming course} author = {Magdalena Dukielska and Jacek Sroka} year = 2010 note = {JSN project won Netbeans Innovators Grant in April 2008} booktitle = {Proceedings of the fifteenth annual conference on Innovation and technology in computer science education} pages = {23-27} address = {New York NY USA} },In this paper we introduce the JavaSpaces NetBeans IDE (JSN) which integrates the JavaSpaces technology an implementation of Linda principles in Java with the NetBeans IDE. JSN is a didactic tool for practical assignments during distributed programming courses. It hides advanced aspects of JavaSpaces configuration and lets students focus on interprocess coordination. An important component of JSN is a distributed debugger which can help to make concurrent programming classes easier to understand and more compelling.,http://research.google.com/pubs/archive/36661.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=JavaSpaces+NetBeans:+a+linda+workbench+for+distributed+programming+course+Dukielska+Sroka,http://research.google.com/pubs/pub36661.html
Affinity Weighted Embedding,International Conference on Machine Learning (2014),2014,Jason Weston Ron Weiss Hector Yee,@inproceedings{42557 title = {Affinity Weighted Embedding} author = {Jason Weston and Ron Weiss and Hector Yee} year = 2014 URL = {http://jmlr.org/proceedings/papers/v32/weston14.pdf} booktitle = {International Conference on Machine Learning} },Supervised linear embedding models like Wsabie (Weston et al. 2011) and supervised semantic indexing (Bai et al. 2010) have proven successful at ranking recommendation and annotation tasks. However despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family and show its usefulness on several datasets.,http://research.google.com/pubs/archive/42557.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Affinity+Weighted+Embedding+Weston+Weiss+Yee,http://research.google.com/pubs/pub42557.html
Idest: Learning a Distributed Representation for Event Patterns,Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL'15) pp. 1140-1149,2015,Sebastian Krause Enrique Alfonseca Katja Filippova Daniele Pighin,@inproceedings{43851 title = {Idest: Learning a Distributed Representation for Event Patterns} author = {Sebastian Krause and Enrique Alfonseca and Katja Filippova and Daniele Pighin} year = 2015 booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL'15)} pages = {1140--1149} },This paper describes IDEST a new method for learning paraphrases of event patterns. It is based on a new neural network architecture that only relies on the weak supervision signal that comes from the news published on the same day and mention the same real-world entities. It can generalize across extractions from different dates to produce a robust paraphrase model for event patterns that can also capture meaningful representations for rare patterns. We compare it with two state-of-the-art systems and show that it can attain comparable quality when trained on a small dataset. Its generalization capabilities also allow it to leverage much more data leading to substantial quality improvements.,http://research.google.com/pubs/archive/43851.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Idest:+Learning+a+Distributed+Representation+for+Event+Patterns+Krause+Alfonseca+Filippova+Pighin,http://research.google.com/pubs/pub43851.html
Large-scale cluster management at Google with Borg,Proceedings of the European Conference on Computer Systems (EuroSys) ACM Bordeaux France (2015),2015,Abhishek Verma Luis Pedrosa Madhukar R. Korupolu David Oppenheimer Eric Tune John Wilkes,@inproceedings{43438 title = {Large-scale cluster management at {Google} with {Borg}} author = {Abhishek Verma and Luis Pedrosa and Madhukar R. Korupolu and David Oppenheimer and Eric Tune and John Wilkes} year = 2015 booktitle = {Proceedings of the European Conference on Computer Systems (EuroSys)} address = {Bordeaux France} },Google's Borg system is a cluster manager that runs hundreds of thousands of jobs from many thousands of different applications across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control efficient task-packing over-commitment and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language name service integration real-time job monitoring and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features important design decisions a quantitative analysis of some of its policy decisions and a qualitative examination of lessons learned from a decade of operational experience with it.,http://research.google.com/pubs/archive/43438.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+cluster+management+at+%7BGoogle%7D+with+%7BBorg%7D+Verma+Pedrosa+Korupolu+Oppenheimer+Tune+Wilkes,http://research.google.com/pubs/pub43438.html
Google hostload prediction based on Bayesian model with optimized feature combination,Journal Parallel and Distributed Computing (2014),2014,Sheng Dia Derrick Kondo Walfredo Cirne,@article{42299 title = {Google hostload prediction based on Bayesian model with optimized feature combination} author = {Sheng Dia and Derrick Kondo and Walfredo Cirne} year = 2014 URL = {http://www.sciencedirect.com/science/article/pii/S0743731513002128} journal = {Journal Parallel and Distributed Computing} },We design a novel prediction method with Bayes model to predict a load fluctuation pattern over a long-term interval in the context of Google data centers. We exploit a set of features that capture the expectation trend stability and patterns of recent host loads. We also investigate the correlations among these features and explore the most effective combinations of features with various training periods. All of the prediction methods are evaluated using Google trace with 10000+heterogeneous hosts. Experiments show that our Bayes method improves the long-term load prediction accuracy by 5.6%–50% compared to other state-of-the-art methods based on moving average auto-regression and/or noise filters. Mean squared error of pattern prediction with Bayes method can be approximately limited in [10_8 10_5 ]. Through a load balancing scenario we confirm the precision of pattern prediction in finding a set of idlest/busiest hosts from among 10000+ hosts can be improved by about 7% on average.,http://www.sciencedirect.com/science/article/pii/S0743731513002128,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+hostload+prediction+based+on+Bayesian+model+with+optimized+feature+combination+Dia+Kondo+Cirne,http://research.google.com/pubs/pub42299.html
Large-scale Privacy Protection in Google Street View,IEEE International Conference on Computer Vision (2009),2009,Andrea Frome German Cheung Ahmad Abdulkader Marco Zennaro Bo Wu Alessandro Bissacco Hartwig Adam Hartmut Neven Luc Vincent,@inproceedings{35481 title = {Large-scale Privacy Protection in Google Street View} author = {Andrea Frome and German Cheung and Ahmad Abdulkader and Marco Zennaro and Bo Wu and Alessandro Bissacco and Hartwig Adam and Hartmut Neven and Luc Vincent} year = 2009 URL = {http://research.google.com/archive/papers/cbprivacy_iccv09.pdf} booktitle = {IEEE International Conference on Computer Vision} },"The last two years have witnessed the introduction and rapid expansion of products based upon large systematically-gathered street-level image collections such as Google Street View EveryScape and Mapjack. In the process of gathering images of public spaces these projects also capture license plates faces and other information considered sensitive from a privacy standpoint. In this work we present a system that addresses the challenge of automatically detecting and blurring faces and license plates for the purpose of privacy protection in Google Street View. Though some in the field would claim face detection is ""solved"" we show that state-of-the-art face detectors alone are not sufficient to achieve the recall desired for large-scale privacy protection. In this paper we present a system that combines a standard sliding-window detector tuned for a high recall low-precision operating point with a fast post-processing stage that is able to remove additional false positives by incorporating domain-specific information not available to the sliding-window detector. Using a completely automatic system we are able to sufficiently blur more than 89% of faces and 94-96% of license plates in evaluation sets sampled from Google Street View imagery. The full paper will appear from IEEE.",http://research.google.com/pubs/archive/35481.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+Privacy+Protection+in+Google+Street+View+Frome+Cheung+Abdulkader+Zennaro+Wu+Bissacco+Adam+Neven+Vincent,http://research.google.com/pubs/pub35481.html
Convolutional Color Constancy,ICCV (2015) (to appear),2015,Jonathan T Barron,@inproceedings{44004 title = {Convolutional Color Constancy} author = {Jonathan T Barron} year = 2015 booktitle = {ICCV} },Color constancy is the problem of inferring the color of the light that illuminated a scene usually so that the illumination color can be removed. Because this problem is underconstrained it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images our model is able to improve performance on standard benchmarks by nearly 40%.,http://research.google.com/pubs/archive/44004.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Convolutional+Color+Constancy+Barron,http://research.google.com/pubs/pub44004.html
Preemptive Policy Experimentation,Econometrica vol. 82 (2014) pp. 1509-1528,2014,Steven Callander Patrick Hummel,@article{42806 title = {Preemptive Policy Experimentation} author = {Steven Callander and Patrick Hummel} year = 2014 URL = {https://www.econometricsociety.org/publications/econometrica/2014/07/01/preemptive-policy-experimentation} journal = {Econometrica} pages = {1509-1528} volume = {82} },We develop a model of experimentation and learning in policymaking when control of power is temporary. We demonstrate how an early office holder who would otherwise not experiment is nonetheless induced to experiment when his hold on power is temporary. This preemptive policy experiment is profitable for the early office holder as it reveals information about the policy mapping to his successor information that shapes future policy choices. Thus policy choices today can cast a long shadow over future choices purely through information transmission and absent any formal institutional constraints or real state variables. The model we develop utilizes a recent innovation that represents the policy mapping as the realized path of a Brownian motion. We provide a precise characterization of when preemptive experimentation emerges in equilibrium and the form it takes. We apply the model to several well known episodes of policymaking reinterpreting the policy choices as preemptive experiments.,https://www.econometricsociety.org/publications/econometrica/2014/07/01/preemptive-policy-experimentation,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Preemptive+Policy+Experimentation+Callander+Hummel,http://research.google.com/pubs/pub42806.html
Answer typing for information retrieval,Proceeding of the 18th ACM conference on Information and knowledge management (CIKM) ACM Hong Kong (2009) pp. 1955-1958,2009,Christopher Pinchak Davood Rafiei Dekang Lin,@inproceedings{36029 title = {Answer typing for information retrieval} author = {Christopher Pinchak and Davood Rafiei and Dekang Lin} year = 2009 URL = {http://doi.acm.org/10.1145/1645953.1646274} booktitle = {Proceeding of the 18th ACM conference on Information and knowledge management (CIKM)} pages = {1955-1958} address = {Hong Kong} },Answer typing is commonly thought of as finding appropriate responses to given questions. We extend the notion of answer typing to information retrieval to ensure results contain plausible answers to queries. Identification of a large class of applicable queries is performed using a discriminative classifier and discriminative preference ranking methods are employed for the selection of type-appropriate terms. Experimental results show that type-appropriate terms identified by the model are superior to terms most commonly associated with the query providing strong evidence that answer typing techniques can find meaningful and appropriate terms. Further experiments show that snippets containing correct answers are ranked higher by our model than by the baseline Google search engine in those instances in which a query does indeed seek a short answer.,http://doi.acm.org/10.1145/1645953.1646274,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Answer+typing+for+information+retrieval+Pinchak+Rafiei+Lin,http://research.google.com/pubs/pub36029.html
Using a Market Economy to Provision Compute Resources Across Planet-wide Clusters,Proceedings for the International Parallel and Distributed Processing Symposium 2009 IEEE pp. 1-8,2009,Murray Stokely Jim Winget Ed Keyes Carrie Grimes Benjamin Yolken,@inproceedings{35115 title = {Using a Market Economy to Provision Compute Resources Across Planet-wide Clusters} author = {Murray Stokely and Jim Winget and Ed Keyes and Carrie Grimes and Benjamin Yolken} year = 2009 URL = {http://www.stokely.org/papers/google-cluster-auctions.pdf} booktitle = {Proceedings for the International Parallel and Distributed Processing Symposium 2009} pages = {1--8} },We present a practical market-based solution to the resource provisioning problem in a set of heterogeneous resource clusters. We focus on provisioning rather than immediate scheduling decisions to allow users to change long-term job specifications based on market feedback. Users enter bids to purchase quotas or bundles of resources for long-term use. These requests are mapped into a simulated clock auction which determines uniform fair resource prices that balance supply and demand. The reserve prices for resources sold by the operator in this auction are set based on current utilization thus guiding the users as they set their bids towards under-utilized resources. By running these auctions at regular time intervals prices fluctuate like those in a real-world economy and provide motivation for users to engineer systems that can best take advantage of available resources. These ideas were implemented in an experimental resource market at Google. Our preliminary results demonstrate an efficient transition of users from more congested resource pools to less congested resources. The disparate engineering costs for users to reconfigure their jobs to run on less expensive resource pools was evidenced by the large price premiums some users were willing to pay for more expensive resources. The final resource allocations illustrated how this framework can lead to significant beneficial changes in user behavior reducing the excessive shortages and surpluses of more traditional allocation methods.,http://research.google.com/pubs/archive/35115.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+a+Market+Economy+to+Provision+Compute+Resources+Across+Planet-wide++Clusters+Stokely+Winget+Keyes+Grimes+Yolken,http://research.google.com/pubs/pub35115.html
Auto-Directed Video Stabilization with Robust L1 Optimal Camera Paths,IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2011),2011,Matthias Grundmann Vivek Kwatra Irfan Essa,@inproceedings{37041 title = {Auto-Directed Video Stabilization with Robust L1 Optimal Camera Paths} author = {Matthias Grundmann and Vivek Kwatra and Irfan Essa} year = 2011 URL = {http://cpl.cc.gatech.edu/projects/videostabilization/} booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2011)} },We present a novel algorithm for automatically applying constrainable L1-optimal camera paths to generate stabilized videos by removing undesired motions. Our goal is to compute camera paths that are composed of constant linear and parabolic segments mimicking the camera motions employed by professional cinematographers. To this end our algorithm is based on a linear programming framework to minimize the first second and third derivatives of the resulting camera path. Our method allows for video stabilization beyond the conventional filtering of camera paths that only suppresses high frequency jitter. We incorporate additional constraints on the path of the camera directly in our algorithm allowing for stabilized and retargeted videos. Our approach accomplishes this without the need of user interaction or costly 3D reconstruction of the scene and works as a post-process for videos from any camera or from an online source.,http://research.google.com/pubs/archive/37041.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Auto-Directed+Video+Stabilization+with+Robust+L1+Optimal+Camera+Paths+Grundmann+Kwatra+Essa,http://research.google.com/pubs/pub37041.html
Transfer Learning In MIR: Sharing Learned Latent Representations For Music Audio Classification And Similarity,14th International Conference on Music Information Retrieval (ISMIR '13) (2013),2013,Philippe Hamel Matthew E. P. Davies Kazuyoshi Yoshii Masataka Goto,@inproceedings{41530 title = {Transfer Learning In MIR: Sharing Learned Latent Representations For Music Audio Classification And Similarity} author = {Philippe Hamel and Matthew E. P. Davies and Kazuyoshi Yoshii and Masataka Goto} year = 2013 booktitle = {14th International Conference on Music Information Retrieval (ISMIR '13)} },This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore these datasets often contain relatively few songs. Thus there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity.,http://research.google.com/pubs/archive/41530.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Transfer+Learning+In+MIR:+Sharing+Learned+Latent+Representations+For+Music+Audio+Classification+And+Similarity+Hamel+Davies+Yoshii+Goto,http://research.google.com/pubs/pub41530.html
Security-Aware SoC Test Access Mechanisms,Proceedings of the 2011 IEEE VLSI Test Symposium,2011,Kurt Rosenfeld,@inproceedings{37397 title = {Security-Aware SoC Test Access Mechanisms} author = {Kurt Rosenfeld} year = 2011 booktitle = {Proceedings of the 2011 IEEE VLSI Test Symposium} },Test access mechanisms are critical components in digital systems. They affect not only production and operational economics but also system security. We propose a security enhancement for system-on-chip (SoC) test access that addresses the threat posed by untrustworthy cores. The scheme maintains the economy of shared wiring (bus or daisy-chain) while achieving most of the security benefits of star-topology test access wiring. Using the proposed scheme the tester is able to establish distinct cryptographic session keys with each of the cores significantly reducing the exposure in cases where one or more of the cores contains malicious or otherwise untrustworthy logic. The proposed scheme is out of the functional path and does not affect functional timing or power consumption.,http://research.google.com/pubs/archive/37397.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Security-Aware+SoC+Test+Access+Mechanisms+Rosenfeld,http://research.google.com/pubs/pub37397.html
Position Auctions with Externalities,Proceedings of the 10th Conference on Web and Internet Economics (WINE) (2014) pp. 417-422,2014,Patrick Hummel Preston McAfee,@inproceedings{43219 title = {Position Auctions with Externalities} author = {Patrick Hummel and Preston McAfee} year = 2014 URL = {http://rd.springer.com/chapter/10.1007/978-3-319-13129-0_33} booktitle = {Proceedings of the 10th Conference on Web and Internet Economics (WINE)} pages = {417-422} },This paper presents models for predicted click-through rates in position auctions that take into account the externalities ads shown in other positions may impose on the probability that an ad in a particular position receives a click. We present a general axiomatic methodology for how click probabilities are affected by the qualities of the ads in the other positions and illustrate that using these axioms will increase revenue as long as higher quality ads tend to be ranked ahead of lower quality ads. We also present appropriate algorithms for selecting the optimal allocation of ads when predicted click-through rates are governed by a natural special case of this axiomatic model of externalities.,http://rd.springer.com/chapter/10.1007/978-3-319-13129-0_33,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Position+Auctions+with+Externalities+Hummel+McAfee,http://research.google.com/pubs/pub43219.html
Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings,European Conference on Machine Learning (2010),2010,Jason Weston Samy Bengio Nicolas Usunier,@inproceedings{35780 title = {Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings} author = {Jason Weston and Samy Bengio and Nicolas Usunier} year = 2010 URL = {http://www.kyb.mpg.de/bs/people/weston/papers/wsabie-ecml.pdf} booktitle = {European Conference on Machine Learning} },Image annotation datasets are becoming larger and larger with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and in comparison to them is faster and consumes less memory. We also demonstrate how our method learns an interpretable model where annotations with alternate spellings or even languages are close in the embedding space. Hence even when our model does not predict the exact annotation given by a human labeler it often predicts similar annotations a fact that we try to quantify by measuring the newly introduced ``sibling'' precision metric where our method also obtains excellent results.,http://research.google.com/pubs/archive/35780.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Image+Annotation:+Learning+to+Rank+with+Joint+Word-Image+Embeddings+Weston+Bengio+Usunier,http://research.google.com/pubs/pub35780.html
Speech and Natural Language: Where Are We Now And Where Are We Headed?,Mobile Voice Conference San Francisco (2013),2013,Ciprian Chelba,@misc{41117 title = {Speech and Natural Language: Where Are We Now And Where Are We Headed?} author = {Ciprian Chelba} year = 2013 },Slides from a presentation on invited panel at the Mobile Voice Conference 2013 San Francisco.,http://research.google.com/pubs/archive/41117.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Speech+and+Natural+Language:+Where+Are+We+Now+And+Where+Are+We+Headed%3F+Chelba,http://research.google.com/pubs/pub41117.html
Price Competition in Online Combinatorial Markets,Proceedings of the 23st World Wide Web Conference 2014,2014,Moshe Babaioff Renato Paes Leme Noam Nisan,@inproceedings{42258 title = {Price Competition in Online Combinatorial Markets} author = {Moshe Babaioff and Renato Paes Leme and Noam Nisan} year = 2014 URL = {http://arxiv.org/abs/1401.1559} booktitle = {Proceedings of the 23st World Wide Web Conference 2014} },"We consider a single buyer with a combinatorial preference that would like to purchase related products and services from different vendors where each vendor supplies exactly one product. We study the general case where subsets of products can be substitutes as well as complementary and analyze the game that is induced on the vendors where a vendor's strategy is the price that he asks for his product. This model generalizes both Bertrand competition (where vendors are perfect substitutes) and Nash bargaining (where they are perfect complements) and captures a wide variety of scenarios that can appear in complex crowd sourcing or in automatic pricing of related products. We study the equilibria of such games and show that a pure efficient equilibrium always exists. In the case of submodular buyer preferences we fully characterize the set of pure Nash equilibria essentially showing uniqueness. For the even more restricted ""substitutes"" buyer preferences we also prove uniqueness over {\em mixed} equilibria. Finally we begin the exploration of natural generalizations of our setting such as when services have costs when there are multiple buyers or uncertainty about the the buyer's valuation and when a single vendor supplies multiple products.",http://arxiv.org/abs/1401.1559,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Price+Competition+in+Online+Combinatorial+Markets+Babaioff+Paes+Leme+Nisan,http://research.google.com/pubs/pub42258.html
Using Web Search Query Data to Monitor Dengue Epidemics: A New Model for Neglected Tropical Disease Surveillance,PLoS Neglected Tropical Diseases vol. 5 Issue 5 (2011),2011,Emily H. Chan Vikram Sahai Corrie Conrad John S. Brownstein,@article{37209 title = {Using Web Search Query Data to Monitor Dengue Epidemics: A New Model for Neglected Tropical Disease Surveillance} author = {Emily H. Chan and Vikram Sahai and Corrie Conrad and John S. Brownstein} year = 2011 URL = {http://www.plosntds.org/article/info%3Adoi%2F10.1371%2Fjournal.pntd.0001206} journal = {PLoS Neglected Tropical Diseases} volume = {5 Issue 5} },Background A variety of obstacles including bureaucracy and lack of resources have interfered with timely detection and reporting of dengue cases in many endemic countries. Surveillance efforts have turned to modern data sources such as Internet search queries which have been shown to be effective for monitoring influenza-like illnesses. However few have evaluated the utility of web search query data for other diseases especially those of high morbidity and mortality or where a vaccine may not exist. In this study we aimed to assess whether web search queries are a viable data source for the early detection and monitoring of dengue epidemics. Methodology/Principal Findings Bolivia Brazil India Indonesia and Singapore were chosen for analysis based on available data and adequate search volume. For each country a univariate linear model was then built by fitting a time series of the fraction of Google search query volume for specific dengue-related queries from that country against a time series of official dengue case counts for a time-frame within 2003–2010. The specific combination of queries used was chosen to maximize model fit. Spurious spikes in the data were also removed prior to model fitting. The final models fit using a training subset of the data were cross-validated against both the overall dataset and a holdout subset of the data. All models were found to fit the data quite well with validation correlations ranging from 0.82 to 0.99. Conclusions/Significance Web search query data were found to be capable of tracking dengue activity in Bolivia Brazil India Indonesia and Singapore. Whereas traditional dengue data from official sources are often not available until after some substantial delay web search query data are available in near real-time. These data represent valuable complement to assist with traditional dengue surveillance.,http://research.google.com/pubs/archive/37209.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Web+Search+Query+Data+to+Monitor+Dengue+Epidemics:+A+New+Model+for+Neglected+Tropical+Disease+Surveillance+Chan+Sahai+Conrad+Brownstein,http://research.google.com/pubs/pub37209.html
Google Fusion Tables: Data Management Integration and Collaboration in the Cloud,Proceedings of the ACM Symposium on Cloud Computing (SOCC) (2010),2010,Hector Gonzalez Alon Halevy Christian Jensen Anno Langen Jayant Madhavan Rebecca Shapley Warren Shen,@inproceedings{36256 title = {Google Fusion Tables: Data Management Integration and Collaboration in the Cloud} author = {Hector Gonzalez and Alon Halevy and Christian Jensen and Anno Langen and Jayant Madhavan and Rebecca Shapley and Warren Shen} year = 2010 URL = {http://www.cs.washington.edu/homes/alon/files/socc10.pdf} booktitle = {Proceedings of the ACM Symposium on Cloud Computing (SOCC)} },Google Fusion Tables is a cloud-based service for data management and integration. Fusion Tables enables users to upload tabular data les (spreadsheets CSV KML) currently of up to 100MB. The system provides several ways of visualizing the data (e.g. charts maps and timelines) and the ability to filter and aggregate the data. It supports the integration of data from multiple sources by performing joins across tables that may belong to dierent users. Users can keep the data private share it with a select set of collaborators or make it public and thus crawlable by search engines. The discussion feature of Fusion Tables allows collaborators to conduct detailed discussions of the data at the level of tables and individual rows columns and cells. This paper describes the inner workings of Fusion Tables including the storage of data in the system and the tight integration with the Google Maps infrastructure.,http://www.cs.washington.edu/homes/alon/files/socc10.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+Fusion+Tables:+Data+Management+Integration+and+Collaboration+in+the+Cloud+Gonzalez+Halevy+Jensen+Langen+Madhavan+Shapley+Shen,http://research.google.com/pubs/pub36256.html
F1: A Distributed SQL Database That Scales,VLDB (2013),2013,Jeff Shute Radek Vingralek Bart Samwel Ben Handy Chad Whipkey Eric Rollins Mircea Oancea Kyle Littleﬁeld David Menestrina Stephan Ellner John Cieslewicz Ian Rae Traian Stancescu Himani Apte,@inproceedings{41344 title = {F1: A Distributed SQL Database That Scales} author = {Jeff Shute and Radek Vingralek and Bart Samwel and Ben Handy and Chad Whipkey and Eric Rollins and Mircea Oancea and Kyle Littleﬁeld and David Menestrina and Stephan Ellner and John Cieslewicz and Ian Rae and Traian Stancescu and Himani Apte} year = 2013 booktitle = {VLDB} },F1 is a distributed relational database system built at Google to support the AdWords business. F1 is a hybrid database that combines high availability the scalability of NoSQL systems like Bigtable and the consistency and usability of traditional SQL databases. F1 is built on Spanner which provides synchronous cross-datacenter replication and strong consistency. Synchronous replication implies higher commit latency but we mitigate that latency by using a hierarchical schema model with structured data types and through smart application design. F1 also includes a fully functional distributed SQL query engine and automatic change tracking and publishing.,http://research.google.com/pubs/archive/41344.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=F1:+A+Distributed+SQL+Database+That+Scales+Shute+Vingralek+Samwel+Handy+Whipkey+Rollins+Oancea+Littlefield+Menestrina+Ellner+Cieslewicz+Rae+Stancescu+Apte,http://research.google.com/pubs/pub41344.html
A Game-Theoretic Analysis of Rank-Order Mechanisms for User-Generated Content,Journal of Economic Theory vol. 154 (2014) pp. 349-374,2014,Arpita Ghosh Patrick Hummel,@article{43156 title = {A Game-Theoretic Analysis of Rank-Order Mechanisms for User-Generated Content} author = {Arpita Ghosh and Patrick Hummel} year = 2014 URL = {http://www.sciencedirect.com/science/article/pii/S0022053114001306} journal = {Journal of Economic Theory} pages = {349-374} volume = {154} },We investigate the widely-used rank-order mechanism for displaying user-generated content where contributions are displayed on a webpage in decreasing order of their ratings in a game-theoretic model where strategic contributors benefit from attention and have a cost to quality. We show that the lowest quality elicited by this rank-order mechanism in any mixed-strategy equilibrium becomes optimal as the available attention diverges. Additionally these equilibrium qualities are higher with probability tending to 1 in the limit of diverging attention than those elicited by a more equitable proportional mechanism which distributes attention in proportion to the positive ratings a contribution receives but the proportional mechanism elicits a greater number of contributions than the rank-order mechanism.,http://www.sciencedirect.com/science/article/pii/S0022053114001306,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Game-Theoretic+Analysis+of+Rank-Order+Mechanisms+for+User-Generated+Content+Ghosh+Hummel,http://research.google.com/pubs/pub43156.html
Chale How Much it Cost to Browse? Results from a Mobile Data Price Transparency Trial in Ghana,Proceedings of the Sixth International Conference on Information and Communication Technologies and Development: Full Papers - Volume 1 (ICTD '13) ACM New York NY USA (2013) pp. 13-23,2013,Nithya Sambasivan Paul Lee Greg Hecht Paul M. Aoki Maria-Ines Carrera Jenny Chen David Pablo Cohn Pete Kruskall Everett Wetchler Michael Youssefmir Astrid Twenebowa Larssen,@inproceedings{41878 title = {Chale How Much it Cost to Browse? Results from a Mobile Data Price Transparency Trial in Ghana} author = {Nithya Sambasivan and Paul Lee and Greg Hecht and Paul M. Aoki and Maria-Ines Carrera and Jenny Chen and David Pablo Cohn and Pete Kruskall and Everett Wetchler and Michael Youssefmir and Astrid Twenebowa Larssen} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2516607&bnc=1} booktitle = {Proceedings of the Sixth International Conference on Information and Communication Technologies and Development: Full Papers - Volume 1 (ICTD '13)} pages = {13-23} address = {New York NY USA} },Mobile data usage is on the rise globally. In emerging regions mobile data is particularly expensive and suffers from the lack of price and data usage transparency needed to make informed decisions about Internet use. To measure and address this problem we designed SmartBrowse an Internet proxy system that shows mobile data usage information and provides controls to avoid overspending. In this paper we discuss the results of a 10-week study with SmartBrowse involving 299 participants in Ghana. Half the users were given SmartBrowse and the other half was given a regular Internet experience. Our findings suggest that compared with the control group using SmartBrowse led to a significant reduction in Internet credit spend and increased online activity among SmartBrowse users while providing the same or better mobile Internet user experience. Additionally SmartBrowse users who were prior mobile data non-users increased their webpage views while spending less money than control users. Our discussion contributes to the understanding of how forward-looking ICTD research in the wild can empower mobile data users in this case through increased price transparency.,http://research.google.com/pubs/archive/41878.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Chale+How+Much+it+Cost+to+Browse%3F+Results+from+a+Mobile+Data+Price+Transparency+Trial+in+Ghana+Sambasivan+Lee+Hecht+Aoki+Carrera+Chen+Cohn+Kruskall+Wetchler+Youssefmir+Larssen,http://research.google.com/pubs/pub41878.html
Profile CBC: Using Conjoint Analysis for Consumer Profiles,Proceedings of the 18th Sawtooth Software Conference Orlando FL (2015) pp. 1-12,2015,Chris Chapman Kate Krontiris John Webb,@inproceedings{44167 title = {Profile CBC: Using Conjoint Analysis for Consumer Profiles} author = {Chris Chapman and Kate Krontiris and John Webb} year = 2015 booktitle = {Proceedings of the 18th Sawtooth Software Conference} pages = {1-12} address = {Orlando FL} },We investigate the usage of choice-_based conjoint analysis (CBC) for sizing consumer profiles for a technology product area. Traditionally technology research has often relied upon qualitative personas approaches that are difficult to assess quantitatively. We demonstrate that Profile CBC is able to find consumer profiles from tradeoffs of attributes derived from qualitative research and yields replicable specifically sized groups that are well_ differentiated on both intra_-method and extra-_method variables. We conclude that Profile CBC is a potentially useful addition to analysts' tools for investigating consumer profiles.,http://research.google.com/pubs/archive/44167.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Profile+CBC:+Using+Conjoint+Analysis+for++Consumer+Profiles+Chapman+Krontiris+Webb,http://research.google.com/pubs/pub44167.html
Building high-level features using large scale unsupervised learning,International Conference in Machine Learning (2012),2012,Quoc Le Marc'Aurelio Ranzato Rajat Monga Matthieu Devin Kai Chen Greg Corrado Jeff Dean Andrew Ng,@inproceedings{38115 title = {Building high-level features using large scale unsupervised learning} author = {Quoc Le and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Greg Corrado and Jeff Dean and Andrew Ng} year = 2012 booktitle = {International Conference in Machine Learning} },We consider the problem of building highlevel class-speciﬁc feature detectors from only unlabeled data. For example is it possible to learn a face detector using only unlabeled images? To answer this we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1000 machines (16000 cores) for three days. Contrary to what appears to be a widely-held intuition our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also ﬁnd that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features we trained our network to obtain 15.8% accuracy in recognizing 20000 object categories from ImageNet a leap of 70% relative improvement over the previous state-of-the-art.,http://research.google.com/pubs/archive/38115.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Building+high-level+features+using+large+scale+unsupervised+learning+Le+Ranzato+Monga+Devin+Chen+Corrado+Dean+Ng,http://research.google.com/pubs/pub38115.html
Direct construction of compact context-dependency transducers from data,Computer Speech & Language (2013) (to appear),2013,David Rybach Michael Riley Chris Alberti,@article{41450 title = {Direct construction of compact context-dependency transducers from data} author = {David Rybach and Michael Riley and Chris Alberti} year = 2013 URL = {http://dx.doi.org/10.1016/j.csl.2013.04.006} journal = {Computer Speech & Language} },This paper describes a new method for building compact context-dependency transducers for finite-state transducer-based ASR decoders. Instead of the conventional phonetic decision tree growing followed by FST compilation this approach incorporates the phonetic context splitting directly into the transducer construction. The objective function of the split optimization is augmented with a regularization term that measures the number of transducer states introduced by a split. We give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy. This permits using context sizes and features that might otherwise be unmanageable.,http://dx.doi.org/10.1016/j.csl.2013.04.006,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Direct+construction+of+compact+context-dependency+transducers+from+data+Rybach+Riley+Alberti,http://research.google.com/pubs/pub41450.html
Model Recommendation for Action Recognition,IEEE International Conference on Computer Vision and Pattern Recognition (CVPR'12) (2012),2012,Pyry Matikainen Rahul Sukthankar Martial Hebert,@inproceedings{38093 title = {Model Recommendation for Action Recognition} author = {Pyry Matikainen and Rahul Sukthankar and Martial Hebert} year = 2012 booktitle = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR'12)} },Simply choosing one model out of a large set of possibilities for a given vision task is a surprisingly difficult problem especially if there is limited evaluation data with which to distinguish among models such as when choosing the best ``walk'' action classifier from a large pool of classifiers tuned for different viewing angles lighting conditions and background clutter. In this paper we suggest that this problem of selecting a good model can be recast as a recommendation problem where the goal is to recommend a good model for a particular task based on how well a limited probe set of models appears to perform. Through this conceptual remapping we can bring to bear all the collaborative filtering techniques developed for consumer recommender systems (e.g. Netflix Amazon.com). We test this hypothesis on action recognition and find that even when every model has been directly rated on a training set recommendation finds better selections for the corresponding test set than the best performers on the training set.,http://research.google.com/pubs/archive/38093.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Model+Recommendation+for+Action+Recognition+Matikainen+Sukthankar+Hebert,http://research.google.com/pubs/pub38093.html
RFC7342 - Practices for Scaling ARP and Neighbor Discovery (ND) in Large Data Centers,IETF RFC Internet Engineering Task Force (2014),2014,Warren Kumari,@incollection{42919 title = {RFC7342 - Practices for Scaling ARP and Neighbor Discovery (ND) in Large Data Centers} author = {Warren Kumari} year = 2014 URL = {http://tools.ietf.org/html/rfc7342} booktitle = {IETF RFC} },This memo documents some operational practices that allow ARP and Neighbor Discovery (ND) to scale in data center environments.,http://tools.ietf.org/html/rfc7342,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7342+-+Practices+for+Scaling+ARP+and+Neighbor+Discovery+(ND)+in+Large+Data+Centers+Kumari,http://research.google.com/pubs/pub42919.html
Adapting Software Fault Isolation to Contemporary CPU Architectures,19th USENIX Security Symposium USENIX (2010) pp. 1-11,2010,David Sehr Robert Muth Cliff L. Biffle Victor Khimenko Egor Pasko Bennet Yee Karl Schimpf Brad Chen,@inproceedings{35649 title = {Adapting Software Fault Isolation to Contemporary CPU Architectures} author = {David Sehr and Robert Muth and Cliff L. Biffle and Victor Khimenko and Egor Pasko and Bennet Yee and Karl Schimpf and Brad Chen} year = 2010 URL = {http://code.google.com/p/nativeclient/} booktitle = {19th USENIX Security Symposium} pages = {1-11} },Software Fault Isolation (SFI) is an effective approach to sandboxing binary code of questionable provenance an interesting use case for native plugins in a Web browser. We present software fault isolation schemes for ARM and x86-64 that provide control-flow and memory integrity with average performance overhead of under 5% on ARM and 7% on x86-64. We believe these are the best known SFI implementations for these architectures with significantly lower overhead than previous systems for similar architectures. Our experience suggests that these SFI implementations benefit from instruction-level parallelism and have particularly small impact for workloads that are data memory-bound both properties that tend to reduce the impact of our SFI systems for future CPU implementations.,http://research.google.com/pubs/archive/35649.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adapting+Software+Fault+Isolation+to+Contemporary+CPU+Architectures+Sehr+Muth+Biffle+Khimenko+Pasko+Yee+Schimpf+Chen,http://research.google.com/pubs/pub35649.html
Finding Related Tables,SIGMOD (2012),2012,Anish Das Sarma Lujun Fang Nitin Gupta Alon Y. Halevy Hongrae Lee Fei Wu Reynold Xin Cong Yu,@inproceedings{38124 title = {Finding Related Tables} author = {Anish Das Sarma and Lujun Fang and Nitin Gupta and Alon Y. Halevy and Hongrae Lee and Fei Wu and Reynold Xin and Cong Yu} year = 2012 URL = {http://i.stanford.edu/~anishds/publications/sigmod12/modi255i-dassarma.pdf} booktitle = {SIGMOD} },We consider the problem of finding related tables in a large corpus of heterogenous tables. Detecting related tables provides users a powerful tool for enhancing their tables with additional data and enables effective reuse of available public data. Our first contribution is a framework that captures several types of relatedness including tables that are candidates for joins and tables that are candidates for union. Our second contribution is a set of algorithms for detecting related tables that can be either unioned or joined. We describe a set of experiments that demonstrate that our algorithms produce highly related tables. We also show that we can often improve the results of table search by pulling up tables that are ranked much lower based on their relatedness to top-ranked tables. Finally we describe how to scale up our algorithms and show the results of running it on a corpus of over a million tables extracted from Wikipedia.,http://research.google.com/pubs/archive/38124.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Finding+Related+Tables+Das+Sarma+Fang+Gupta+Halevy+Lee+Wu+Xin+Yu,http://research.google.com/pubs/pub38124.html
CPI^2: CPU performance isolation for shared compute clusters,SIGOPS European Conference on Computer Systems (EuroSys) ACM Prague Czech Republic (2013) pp. 379-391,2013,Xiao Zhang Eric Tune Robert Hagmann Rohit Jnagal Vrigo Gokhale John Wilkes,@inproceedings{40737 title = {{CPI^2}: {CPU} performance isolation for shared compute clusters} author = {Xiao Zhang and Eric Tune and Robert Hagmann and Rohit Jnagal and Vrigo Gokhale and John Wilkes} year = 2013 URL = {http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Zhang_2.pdf} booktitle = {SIGOPS European Conference on Computer Systems (EuroSys)} pages = {379--391} address = {Prague Czech Republic} },Performance isolation is a key challenge in cloud computing. Unfortunately Linux has few defenses against performance interference in shared resources such as processor caches and memory buses so applications in a cloud can experience unpredictable performance caused by other program's behavior. Our solution CPI2 uses cycles-per-instruction (CPI) data obtained by hardware performance counters to identify problems select the likely perpetrators and then optionally throttle them so that the victims can return to their expected behavior. It automatically learns normal and anomalous behaviors by aggregating data from multiple tasks in the same job. We have rolled out CPI2 to all of Google's shared compute clusters. The paper presents the analysis that lead us to that outcome including both case studies and a large-scale evaluation of its ability to solve real production issues.,http://research.google.com/pubs/archive/40737.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%7BCPI%5E2%7D:+%7BCPU%7D+performance+isolation+for+shared+compute+clusters+Zhang+Tune+Hagmann+Jnagal+Gokhale+Wilkes,http://research.google.com/pubs/pub40737.html
Parallelizing Support Vector Machines on Distributed Computers,Neural Information Processing Systems (NIPS) (2007),2007,Edward Y. Chang Kaihua Zhu Hao Wang Hongjie Bai Jian Li Zhihuan Qiu Hang Cui,@inproceedings{34638 title = {Parallelizing Support Vector Machines on Distributed Computers} author = {Edward Y. Chang and Kaihua Zhu and Hao Wang and Hongjie Bai and Jian Li and Zhihuan Qiu and Hang Cui} year = 2007 URL = {http://books.nips.cc/papers/files/nips20/NIPS2007_0435.pdf} booktitle = {Neural Information Processing Systems (NIPS)} },We also open-source this work at: http://code.google.com/p/psvm/.,http://research.google.com/pubs/archive/34638.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Parallelizing+Support+Vector+Machines+on+Distributed+Computers+Chang+Zhu+Wang+Bai+Li+Qiu+Cui,http://research.google.com/pubs/pub34638.html
Long-term SLOs for reclaimed cloud computing resources,ACM Symposium on Cloud Computing (SoCC) ACM Seattle WA USA (2014) 20:1-20:13,2014,Marcus Carvalho Walfredo Cirne Franciso Brasileiro John Wilkes,@inproceedings{43017 title = {Long-term {SLOs} for reclaimed cloud computing resources} author = {Marcus Carvalho and Walfredo Cirne and Franciso Brasileiro and John Wilkes} year = 2014 URL = {http://dl.acm.org/citation.cfm?id=2670999} booktitle = {ACM Symposium on Cloud Computing (SoCC)} pages = {20:1--20:13} address = {Seattle WA USA} },The elasticity promised by cloud computing does not come for free. Providers need to reserve resources to allow users to scale on demand and cope with workload variations which results in low utilization. The current response to this low utilization is to re-sell unused resources with no Service Level Objectives (SLOs) for availability. In this paper we show how to make some of these reclaimable resources more valuable by providing strong long-term availability SLOs for them. These SLOs are based on forecasts of how many resources will remain unused during multi-month periods so users can do capacity planning for their long-running services. By using confidence levels for the predictions we give service providers control over the risk of violating the availability SLOs and allow them trade increased risk for more resources to make available. We evaluated our approach using 45 months of workload data from 6 production clusters at Google and show that 6--17% of the resources can be re-offered with a long-term availability of 98.9% or better. A conservative analysis shows that doing so may increase the profitability of selling reclaimed resources by 22--60%.,http://research.google.com/pubs/archive/43017.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Long-term+%7BSLOs%7D+for+reclaimed+cloud+computing+resources+Carvalho+Cirne+Brasileiro+Wilkes,http://research.google.com/pubs/pub43017.html
Visual and Semantic Similarity in ImageNet,IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2011) pp. 1777-1784,2011,Thomas Deselaers Vittorio Ferrari,@inproceedings{37065 title = {Visual and Semantic Similarity in ImageNet} author = {Thomas Deselaers and Vittorio Ferrari} year = 2011 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)} pages = {1777--1784} },Many computer vision approaches take for granted positive answers to questions such as “Are semantic categories visually separable?” and “Is visual similarity correlated to semantic similarity?” In this paper we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.,http://research.google.com/pubs/archive/37065.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Visual+and+Semantic+Similarity+in+ImageNet+Deselaers+Ferrari,http://research.google.com/pubs/pub37065.html
K2Q: Generating Natural Language Questions from Keywords with User Refinements,Proceedings of the 5th International Joint Conference on Natural Language Processing ACL (2011) 947–955,2011,Zhicheng Zheng Xiance Si Edward Y. Chang Xiaoyan Zhu,@inproceedings{37566 title = {K2Q: Generating Natural Language Questions from Keywords with User Refinements} author = {Zhicheng Zheng and Xiance Si and Edward Y. Chang and Xiaoyan Zhu} year = 2011 URL = {http://aclweb.org/anthology-new/I/I11/I11-1106.pdf} booktitle = {Proceedings of the 5th International Joint Conference on Natural Language Processing} pages = {947–955} },Garbage in and garbage out. A Q&A system must receive a well formulated question that matches the user’s intent or she has no chance to receive satisfactory answers. In this paper we propose a keywords to questions (K2Q) system to assist a user to articulate and refine questions. K2Q generates candidate questions and refinement words from a set of input keywords. After specifying some initial keywords a user receives a list of candidate questions as well as a list of refinement words. The user can then select a satisfactory question or select a refinement word to generate a new list of candidate questions and refinement words. We propose a User Inquiry Intent (UII) model to de- scribe the joint generation process of keywords and questions for ranking questions suggesting refinement words and generating questions that may not have previously appeared. Empirical study shows UII to be useful and effective for the K2Q task.,http://research.google.com/pubs/archive/37566.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=K2Q:+Generating+Natural+Language+Questions+from+Keywords+with+User+Refinements+Zheng+Si+Chang+Zhu,http://research.google.com/pubs/pub37566.html
Reducing the Sampling Complexity of Topic Models,ACM Conference on Knowledge Discovery and Data Mining (KDD) (2014),2014,Aaron Li Amr Ahmed Sujith Ravi Alexander J Smola,@inproceedings{42500 title = {Reducing the Sampling Complexity of Topic Models} author = {Aaron Li and Amr Ahmed and Sujith Ravi and Alexander J Smola} year = 2014 URL = {http://www.sravi.org/pubs/fastlda-kdd2014.pdf} booktitle = {ACM Conference on Knowledge Discovery and Data Mining (KDD)} },Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity with the increase in data requiring O(k) operations per word for k latent states such as topics. In this paper we propose an algorithm which requires only O(kd) operations per word where kd is the number of actually instantiated topics in the document. For large document collections and structured hierarchical models kd _ k thus yielding an order of magnitude speedup. Our method is general and it applies to a wide variety of statistical models. At its core is the idea that dense rapidly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step judicious use of sparsity and amortized preprocessing via the alias method.,http://www.sravi.org/pubs/fastlda-kdd2014.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+the+Sampling+Complexity+of+Topic+Models+Li+Ahmed+Ravi+Smola,http://research.google.com/pubs/pub42500.html
R for Marketing Research and Analytics,Springer New York (2015),2015,Chris Chapman Elea McDonnell Feit,@book{43342 title = {R for Marketing Research and Analytics} author = {Chris Chapman and Elea McDonnell Feit} year = 2015 URL = {http://www.springer.com/statistics/business%2C+economics+%26+finance/book/978-3-319-14435-1} address = {New York} },This book is a complete introduction to the power of R for marketing research practitioners. The text describes statistical models from a conceptual point of view with a minimal amount of mathematics presuming only an introductory knowledge of statistics. Hands-on chapters accelerate the learning curve by asking readers to interact with R from the beginning. Core topics include the R language basic statistics linear modeling and data visualization which is presented throughout as an integral part of analysis. Later chapters cover more advanced topics yet are intended to be approachable for all analysts. These sections examine logistic regression customer segmentation hierarchical linear modeling market basket analysis structural equation modeling and conjoint analysis in R. The text uniquely presents Bayesian models with a minimally complex approach demonstrating and explaining Bayesian methods alongside traditional analyses for analysis of variance linear models and metric and choice-based conjoint analysis. With its emphasis on data visualization model assessment and development of statistical intuition this book provides guidance for any analyst looking to develop or improve skills in R for marketing applications.,http://www.springer.com/statistics/business%2C+economics+%26+finance/book/978-3-319-14435-1,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=R+for+Marketing+Research+and+Analytics+Chapman+Feit,http://research.google.com/pubs/pub43342.html
Evaluating IPv6 adoption in the Internet,PAM 2010 Springer,2010,Lorenzo Colitti Steinar H. Gunderson Erik Kline Tiziana Refice,@inproceedings{36240 title = {Evaluating IPv6 adoption in the Internet} author = {Lorenzo Colitti and Steinar H. Gunderson and Erik Kline and Tiziana Refice} year = 2010 URL = {http://www.pam2010.ethz.ch/papers/full-length/15.pdf} booktitle = {PAM 2010} },As IPv4 address space approaches exhaustion large networks are deploying IPv6 or preparing for deployment. However there is little data available about the quantity and quality of IPv6 connectivity. We describe a methodology to measure IPv6 adoption from the perspective of a Web site operator and to evaluate the impact that adding IPv6 to a Web site will have on its users. We apply our methodology to the Google Web site and present results collected over the last year. Our data show that IPv6 adoption while growing significantly is still low varies considerably by country and is heavily influenced by a small number of large deployments. We find that native IPv6 latency is comparable to IPv4 and provide statistics on IPv6 transition mechanisms used.,http://research.google.com/pubs/archive/36240.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evaluating+IPv6+adoption+in+the+Internet+Colitti+Gunderson+Kline+Refice,http://research.google.com/pubs/pub36240.html
STAGGER: Periodicity Mining of Data Streams Using Expanding Sliding Windows,Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 2006) IEEE Computer Society pp. 188-199,2006,Mohamed G. Elfeky Walid G. Aref Ahmed K. Elmagarmid,@inproceedings{36677 title = {STAGGER: Periodicity Mining of Data Streams Using Expanding Sliding Windows} author = {Mohamed G. Elfeky and Walid G. Aref and Ahmed K. Elmagarmid} year = 2006 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/ICDM.2006.153} booktitle = {Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 2006)} pages = {188-199} },Sensor devices are becoming ubiquitous especially in measurement and monitoring applications. Because of the real-time append-only and semi-infinite natures of the generated sensor data streams an online incremental approach is a necessity for mining stream data types. In this paper we propose STAGGER: a one-pass online and incremental algorithm for mining periodic patterns in data streams. STAGGER does not require that the user pre-specify the periodicity rate of the data. Instead STAGGER discovers the potential periodicity rates. STAGGER maintains multiple expanding sliding windows staggered over the stream where computations are shared among the multiple overlapping windows. Small-length sliding windows are imperative for early and real-time output yet are limited to discover short periodicity rates. As streamed data arrives continuously the sliding windows expand in length in order to cover the whole stream. Larger-length sliding windows are able to discover longer periodicity rates. STAGGER incrementally maintains a tree-like data structure for the frequent periodic patterns of each discovered potential periodicity rate. In contrast to the Fourier/Wavelet-based approaches used for discovering periodicity rates STAGGER not only discovers a wider more accurate set of periodicities but also discovers the periodic patterns themselves. In fact experimental results with real and synthetic data sets show that STAGGER outperforms Fourier/Wavelet-based approaches by an order of magnitude in terms of the accuracy of the discovered periodicity rates. Moreover realdata experiments demonstrate the practicality of the discovered periodic patterns.,http://research.google.com/pubs/archive/36677.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=STAGGER:+Periodicity+Mining+of+Data+Streams+Using+Expanding+Sliding+Windows+Elfeky+Aref+Elmagarmid,http://research.google.com/pubs/pub36677.html
Experiences Using Static Analysis to Find Bugs,IEEE Software vol. 25 (2008) pp. 22-29,2008,Nathaniel Ayewah David Hovemeyer J. David Morgenthaler John Penix William Pugh,@article{34339 title = {Experiences Using Static Analysis to Find Bugs} author = {Nathaniel Ayewah and David Hovemeyer and J. David Morgenthaler and John Penix and William Pugh} year = 2008 note = {Special issue on software development tools September/October (25:5)} journal = {IEEE Software} pages = {22--29} volume = {25} },Static analysis examines code in the absence of input data and without running the code and can detect potential security violations (e.g. SQL injection) runtime errors (e.g. dereferencing a null pointer) and logical inconsistencies (e.g. a conditional test that cannot possibly be true). While there is a rich body of literature on algorithms and analytical frameworks used by such tools reports describing experiences with such tools in industry are much harder to come by. In this paper we describe FindBugs an open source static analysis tool for Java and experience using it in production settings. FindBugs does not push the envelope in terms of the sophistication of its analysis techniques. Rather it is designed to evaluate what kinds of defects can be effectively detected with relatively simple techniques and to help us understand how such tools can be incorporated into the software development process. FindBugs has become very popular downloaded more than 500000 times and used by many major companies and software projects. We report on experience running FindBugs against Sun’s JDK implementation using Findbugs at Google where it has been used for more than a year and a half and incorporated into their standard development process and preliminary results from a survey of FindBugs users.,http://research.google.com/pubs/archive/34339.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Experiences+Using+Static+Analysis+to+Find+Bugs+Ayewah+Hovemeyer+Morgenthaler+Penix+Pugh,http://research.google.com/pubs/pub34339.html
Kubernetes - Scheduling the Future at Cloud Scale,O'Reilly and Associates 1005 Gravenstein Highway North Sebastopol CA 95472 All,2015,David K. Rensin,@book{43826 title = {Kubernetes - Scheduling the Future at Cloud Scale} author = {David K. Rensin} year = 2015 URL = {http://www.oreilly.com/webops-perf/free/kubernetes.csp} booktitle = {OSCON 2015} pages = {All} address = {1005 Gravenstein Highway North Sebastopol CA 95472} },Containers are taking over the world but they aren’t full VMs and present special challenges to people build web-scale services. They need a lot of orchestration to run efficiently and resiliently. Their execution needs to be scheduled and managed. When they die (and they do) they need to be seamlessly replaced and re-balanced. An introductory mini-book designed to explain Kubernetes to IT managers CIOs and the otherwise cloud-curious.,http://www.oreilly.com/webops-perf/free/kubernetes.csp,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Kubernetes+-+Scheduling+the+Future+at+Cloud+Scale+Rensin,http://research.google.com/pubs/pub43826.html
Delegating Responsibility in Digital Systems: Horton's,2nd USENIX Workshop on Hot Topics in Security USENIX (2007) pp. 5,2007,Mark S. Miller Jed Donnelley Alan H. Karp,@inproceedings{33037 title = {Delegating Responsibility in Digital Systems: Horton's} author = {Mark S. Miller and Jed Donnelley and Alan H. Karp} year = 2007 URL = {http://erights.org/elib/capability/horton/} booktitle = {2nd USENIX Workshop on Hot Topics in Security} pages = {5} },Programs do good things but also do bad making software security more than a fad. The authority of programs we do need to tame. But bad things still happen. Who do we blame? From the very beginnings of access control: Should we be safe by construction or should we patrol? Horton shows how in an elegant way we can simply do both and so save the day. with apologies to Dr. Seuss,http://research.google.com/pubs/archive/33037.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Delegating+Responsibility+in+Digital+Systems:+Horton's+Miller+Donnelley+Karp,http://research.google.com/pubs/pub33037.html
The Snap Framework: A Web Toolkit for Haskell,IEEE Internet Computing vol. 15 (2011) pp. 84-87,2011,Gregory Collins Doug Beardsley,@article{37267 title = {The Snap Framework: A Web Toolkit for Haskell} author = {Gregory Collins and Doug Beardsley} year = 2011 URL = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.21} journal = {IEEE Internet Computing} pages = {84-87} volume = {15} },The authors discuss Web development in the Haskell programming language. They look at Snap a simple expressive Web development framework with an integrated high-performance HTTP server.,http://research.google.com/pubs/archive/37267.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Snap+Framework:+A+Web+Toolkit+for+Haskell+Collins+Beardsley,http://research.google.com/pubs/pub37267.html
Confidentiality in the Face of Pervasive Surveillance: A Threat Model and Problem Statement,IETF RFCs Internet Engineering Task Force (2015) pp. 24,2015,Richard Barnes Bruce Schneier Cullen Jennings Ted Hardie Brian Trammel Christian Huitema Daniel Borkman,@incollection{44668 title = {Confidentiality in the Face of Pervasive Surveillance: A Threat Model and Problem Statement} author = {Richard Barnes and Bruce Schneier and Cullen Jennings and Ted Hardie and Brian Trammel and Christian Huitema and Daniel Borkman} year = 2015 URL = {https://www.rfc-editor.org/rfc/pdfrfc/rfc7624.txt.pdf} booktitle = {IETF RFCs} pages = {24} },Since the initial revelations of pervasive surveillance in 2013 several classes of attacks on Internet communications have been discovered. In this document we develop a threat model that describes these attacks on Internet confidentiality. We assume an attacker that is interested in undetected indiscriminate eavesdropping. The threat model is based on published verified attacks.,http://research.google.com/pubs/archive/44668.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Confidentiality+in+the+Face+of+Pervasive+Surveillance:+A+Threat+Model+and+Problem+Statement+Barnes+Schneier+Jennings+Hardie+Trammel+Huitema+Borkman,http://research.google.com/pubs/pub44668.html
FlumeJava: Easy Efficient Data-Parallel Pipelines,ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI) ACM New York NY 2010 2 Penn Plaza Suite 701 New York NY 10121-0701 (2010) pp. 363-375,2010,Craig Chambers Ashish Raniwala Frances Perry Stephen Adams Robert Henry Robert Bradshaw Nathan,@inproceedings{35650 title = {FlumeJava: Easy Efficient Data-Parallel Pipelines} author = {Craig Chambers and Ashish Raniwala and Frances Perry and Stephen Adams and Robert Henry and Robert Bradshaw and Nathan} year = 2010 URL = {http://dl.acm.org/citation.cfm?id=1806638} booktitle = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)} pages = {363-375} address = {2 Penn Plaza Suite 701 New York NY 10121-0701} },MapReduce and similar systems significantly ease the task of writing data-parallel code. However many real-world computations require a pipeline of MapReduces and programming and managing such pipelines can be difficult. We present FlumeJava a Java library that makes it easy to develop test and run efficient dataparallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple high-level uniform abstraction over different data representations and execution strategies. To enable parallel operations to run effi- ciently FlumeJava defers their evaluation instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed FlumeJava first optimizes the execution plan and then executes the optimized operations on appropriate underlying primitives (e.g. MapReduces). The combination of high-level abstractions for parallel data and computation deferred evaluation and optimization and efficient parallel primitives yields an easy-to-use system that approaches the effi- ciency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google. Categories and Subject Descriptors D.1.3 [Concurrent Programming]: Parallel Programming General Terms Algorithms Languages Performance Keywords data-parallel programming MapReduce Java,http://research.google.com/pubs/archive/35650.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=FlumeJava:+Easy+Efficient+Data-Parallel+Pipelines+Chambers+Raniwala+Perry+Adams+Henry+Bradshaw+Weizenbaum,http://research.google.com/pubs/pub35650.html
Fastfood - Approximating Kernel Expansions in Loglinear Time,30th International Conference on Machine Learning (ICML) Omnipress (2013),2013,Quoc Le Tamas Sarlos Alex Smola,@inproceedings{41466 title = {Fastfood - Approximating Kernel Expansions in Loglinear Time} author = {Quoc Le and Tamas Sarlos and Alex Smola} year = 2013 URL = {http://jmlr.org/proceedings/papers/v28/le13.html} booktitle = {30th International Conference on Machine Learning (ICML)} },Fast nonlinear function classes are crucial for nonparametric estimation such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.,http://jmlr.org/proceedings/papers/v28/le13.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fastfood+-+Approximating+Kernel+Expansions+in+Loglinear+Time+Le+Sarlos+Smola,http://research.google.com/pubs/pub41466.html
MAC Reforgeability,Fast Software Encryption Springer (2009) pp. 345-362,2009,John Black Martin Cochran,@inproceedings{36378 title = {MAC Reforgeability} author = {John Black and Martin Cochran} year = 2009 booktitle = {Fast Software Encryption} pages = {345-362} },"Message Authentication Codes (MACs) are core algorithms deployed in virtually every security protocol in common usage. In these protocols the integrity and authenticity of messages rely entirely on the security of the MAC; we examine cases in which this security is lost. In this paper we examine the notion of ""reforgeability"" for MACs and motivate its utility in the context of {power bandwidth CPU}-constrained computing environments. We first give a definition for this new notion then examine some of the most widely-used and well-known MACs under our definition in a variety of adversarial settings finding in nearly all cases a failure to meet the new notion. We examine simple counter-measures to increase resistance to reforgeability using state and truncating the tag length but find that both are not simultaneously applicable to modern MACs. In response we give a tight security reduction for a new MAC WMAC which we argue is the ""best fit"" for resource-limited devices.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MAC+Reforgeability+Black+Cochran,http://research.google.com/pubs/pub36378.html
AddressSanitizer: A Fast Address Sanity Checker,USENIX ATC 2012,2012,Konstantin Serebryany Derek Bruening Alexander Potapenko Dmitry Vyukov,@inproceedings{37752 title = {AddressSanitizer: A Fast Address Sanity Checker} author = {Konstantin Serebryany and Derek Bruening and Alexander Potapenko and Dmitry Vyukov} year = 2012 URL = {https://www.usenix.org/conference/usenixfederatedconferencesweek/addresssanitizer-fast-address-sanity-checker} booktitle = {USENIX ATC 2012} },Memory access bugs including buffer overﬂows and uses of freed heap memory remain a serious problem for programming languages like C and C++. Many memory error detectors exist but most of them are either slow or detect a limited set of bugs or both. This paper presents AddressSanitizer a new memory error detector. Our tool finds out-of-bounds accesses to heap stack and global objects as well as use-after-free bugs. It employs a specialized memory allocator and code instrumentation that is simple enough to be implemented in any compiler binary translation system or even in hardware. AddressSanitizer achieves efficiency without sacriﬁcing comprehensiveness. Its average slowdown is just 73% yet it accurately detects bugs at the point of occurrence. It has found over 300 previously unknown bugs in the Chromium browser and many bugs in other software.,http://research.google.com/pubs/archive/37752.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=AddressSanitizer:+A+Fast+Address+Sanity+Checker+Serebryany+Bruening+Potapenko+Vyukov,http://research.google.com/pubs/pub37752.html
A Linear-Time Transition System for Crossing Interval Trees,NAACL (2015) 662–-671,2015,Emily Pitler Ryan McDonald,@inproceedings{43801 title = {A Linear-Time Transition System for Crossing Interval Trees} author = {Emily Pitler and Ryan McDonald} year = 2015 booktitle = {NAACL} pages = {662–-671} },We define a restricted class of non-projective trees that 1) covers many natural language sentences; and 2) can be parsed exactly with a generalization of the popular arc-eager system for projective trees (Nivre 2003). Crucially this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system an unconstrained non-projective transition system with a worst-case quadratic runtime (Nivre 2009).,http://research.google.com/pubs/archive/43801.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Linear-Time+Transition+System+for+Crossing+Interval+Trees+Pitler+McDonald,http://research.google.com/pubs/pub43801.html
Access and Analyze Broadband Measurements Collected using M-Lab,Summer 2010 ESCC/Internet2 Joint Techs,2010,Tiziana Refice,@incollection{36910 title = {Access and Analyze Broadband Measurements Collected using M-Lab} author = {Tiziana Refice} year = 2010 booktitle = {Summer 2010 ESCC/Internet2 Joint Techs} },Measurement Lab (M-Lab) is an open distributed server platform for researchers to deploy Internet measurement tools. Everybody can use M-Lab's tools to measure their own broadband connection performance. The M-Lab servers collect logs of all the users' tests and make them publicly available. As of July 2010 users have run millions of tests that have generated many terabytes of measurement data. This talk will present the public repositories of M-Lab data and will explain how to analyze M-Lab data using Google's BigQuery. BigQuery stores M-Lab's measurements logs in a table with more than 60 billions of rows. It takes less than 1 minute to run a query against the whole dataset.,http://research.google.com/pubs/archive/36910.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Access+and+Analyze+Broadband+Measurements+Collected+using+M-Lab+Refice,http://research.google.com/pubs/pub36910.html
Domain Adaptation with Coupled Subspaces,Artificial Intelligence and Statistics (2011),2011,John Blitzer Sham Kakade Dean Foster,@inproceedings{37592 title = {Domain Adaptation with Coupled Subspaces} author = {John Blitzer and Sham Kakade and Dean Foster} year = 2011 URL = {http://john.blitzer.com/papers/aistats2011.pdf} booktitle = {Artificial Intelligence and Statistics} },Domain adaptation algorithms address a key issue in applied machine learning: How can we train a system under a source distribution but achieve high performance under a different target distribution? We tackle this question for divergent distributions where crucial predictive target features may not even have support under the source distribution. In this setting the key intuition is that that if we can link target-speciﬁc features to source features we can learn effectively using only source labeled data. We formalize this intuition as well as the assumptions under which such coupled learning is possible. This allows us to give ﬁnite sample target error bounds (using only source training data) and an algorithm which performs at the state-of-the-art on two natural language processing adaptation tasks which are characterized by novel target features.,http://research.google.com/pubs/archive/37592.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Domain+Adaptation+with+Coupled+Subspaces+Blitzer+Kakade+Foster,http://research.google.com/pubs/pub37592.html
F-ing modules,Journal of Functional Programming vol. 24 (5) (2014),2014,Andreas Rossberg Claudio Russo Derek Dreyer,@article{43981 title = {F-ing modules} author = {Andreas Rossberg and Claudio Russo and Derek Dreyer} year = 2014 URL = {https://www.mpi-sws.org/~rossberg/f-ing/} journal = {Journal of Functional Programming} volume = {24 (5)} },"ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately they also have a reputation for being ""complex"" and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable given the many non-standard methodologies that have been developed in the process of studying modules we aim here to demonstrate that it is undeserved. To do so we give a very simple elaboration semantics for a full-featured higher-order ML-like module language. Our elaboration defines the meaning of module expressions by a straightforward compositional translation into vanilla System F_ (the higher-order polymorphic _-calculus) under plain F_ typing environments. We thereby show that ML modules are merely a particular mode of use of System F_. We start out with a module language that supports the usual second-class modules with Standard ML-style generative functors and includes local module definitions. To demonstrate the versatility of our approach we further extend the language with the ability to package modules as first-class values — a very simple extension as it turns out — and a novel treatment of OCaml-style applicative functors. Unlike previous work combining both generative and applicative functors we do not require two distinct forms of functor or sealing expressions. Instead whether a functor is applicative or not depends only on the computational purity of its body — in fact we argue that applicative/generative is rather incidental terminology for what is best understood as pure vs. impure functors. This approach results in a semantics that we feel is simpler and more natural and moreover prohibits breaches of data abstraction that are possible under earlier semantics for applicative functors. We also revive (in refined form) the long-lost notion of structure sharing from SML'90. Although previous work on module type systems has disparaged structure sharing as type-theoretically questionable we observe that (1) some variant of it is in fact necessary in order to provide a proper treatment of abstraction in the presence of applicative functors and (2) it is straightforward to account for using ``phantom types''. Based on this we can even justify the (previously poorly understood) ""where module"" operator for signatures and the related notion of manifest module specifications. Altogether we describe a comprehensive unified and yet simple semantics of a full-blown module language that — with the main exception of cross-module recursion — covers almost all interesting features that can be found in either the literature or in practical implementations of ML modules. We prove the language sound and its type checking decidable.",http://research.google.com/pubs/archive/43981.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=F-ing+modules+Rossberg+Russo+Dreyer,http://research.google.com/pubs/pub43981.html
Why does Unsupervised Pre-training Help Deep Learning?,Journal of Machine Learning Research (2010) pp. 625-660,2010,Dumitru Erhan Yoshua Bengio Aaron Courville Pierre-Antoine Manzagol Pascal Vincent Samy Bengio,@article{35536 title = {Why does Unsupervised Pre-training Help Deep Learning?} author = {Dumitru Erhan and Yoshua Bengio and Aaron Courville and Pierre-Antoine Manzagol and Pascal Vincent and Samy Bengio} year = 2010 URL = {http://jmlr.csail.mit.edu/papers/v11/erhan10a.html} journal = {Journal of Machine Learning Research} pages = {625--660} },Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results obtained in several areas mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth model capacity and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.,http://research.google.com/pubs/archive/35536.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+does+Unsupervised+Pre-training+Help+Deep+Learning%3F+Erhan+Bengio+Courville+Manzagol+Vincent+Bengio,http://research.google.com/pubs/pub35536.html
100GbE and Beyond for Warehouse Scale Computing,OptoeElectronics and Communications Conference (OECC) Technical Digest (2010) pp. 106-107,2010,Bikash Koley Vijay Vusirikala Cedric Lam Vijay Gill,@inproceedings{36863 title = {100GbE and Beyond for Warehouse Scale Computing} author = {Bikash Koley and Vijay Vusirikala and Cedric Lam and Vijay Gill} year = 2010 booktitle = {OptoeElectronics and Communications Conference (OECC) Technical Digest} pages = {106-107} },As computation and storage continues to move from desktops to large internet services computing platforms running such services are transforming into warehouse-scale computers. 100 Gigabit Ethernet and beyond will be instrumental in scaling the interconnection within and between these ubiquitous warehouse-scale computing infrastructures. In this paper we describe the drivers for such interfaces and some methods of scaling Ethernet interfaces to speeds beyond 100GbE.,http://research.google.com/pubs/archive/36863.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=100GbE+and+Beyond+for+Warehouse+Scale+Computing+Koley+Vusirikala+Lam+Gill,http://research.google.com/pubs/pub36863.html
Detecting Adversarial Advertisements in the Wild,Proceedings of the 17th ACM SIGKDD International Conference on Data Mining and Knowledge Discovery KDD (2011),2011,D. Sculley Matthew Eric Otey Michael Pohl Bridget Spitznagel John Hainsworth Yunkai Zhou,@inproceedings{37195 title = {Detecting Adversarial Advertisements in the Wild} author = {D. Sculley and Matthew Eric Otey and Michael Pohl and Bridget Spitznagel and John Hainsworth and Yunkai Zhou} year = 2011 URL = {http://www.eecs.tufts.edu/~dsculley/papers/adversarial-ads.pdf} booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Data Mining and Knowledge Discovery} },In a large online advertising system adversaries may attempt to proﬁt from the creation of low quality or harmful advertisements. In this paper we present a large scale data mining e_ort that detects and blocks such adversarial advertisements for the beneﬁt and safety of our users. Because both false positives and false negatives have high cost our deployed system uses a tiered strategy combining automated and semi-automated methods to ensure reliable classiﬁcation. We also employ strategies to address the challenges of learning from highly skewed data at scale allocating the effort of human experts leveraging domain expert knowledge and independently assessing the e_ectiveness of our system.,http://research.google.com/pubs/archive/37195.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Detecting+Adversarial+Advertisements+in+the+Wild+Sculley+Otey+Pohl+Spitznagel+Hainsworth+Zhou,http://research.google.com/pubs/pub37195.html
Upper and Lower Bounds on the Cost of a Map-Reduce Computation,Arxiv (2012),2012,Foto Afrati Anish Das Sarma Semih Salihoglu Jeffrey Ullman,@techreport{40413 title = {Upper and Lower Bounds on the Cost of a Map-Reduce Computation} author = {Foto Afrati and Anish Das Sarma and Semih Salihoglu and Jeffrey Ullman} year = 2012 URL = {http://arxiv.org/abs/1206.4377} institution = {Arxiv} },"In this paper we study the tradeoff between parallelism and communication cost in a map-reduce computation. For any problem that is not ""embarrassingly parallel"" the finer we partition the work of the reducers so that more parallelism can be extracted the greater will be the total communication between mappers and reducers. We introduce a model of problems that can be solved in a single round of map-reduce computation. This model enables a generic recipe for discovering lower bounds on communication cost as a function of the maximum number of inputs that can be assigned to one reducer. We use the model to analyze the tradeoff for three problems: finding pairs of strings at Hamming distance $d$ finding triangles and other patterns in a larger graph and matrix multiplication. For finding strings of Hamming distance 1 we have upper and lower bounds that match exactly. For triangles and many other graphs we have upper and lower bounds that are the same to within a constant factor. For the problem of matrix multiplication we have matching upper and lower bounds for one-round map-reduce algorithms. We are also able to explore two-round map-reduce algorithms for matrix multiplication and show that these never have more communication for a given reducer size than the best one-round algorithm and often have significantly less.",http://arxiv.org/abs/1206.4377,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Upper+and+Lower+Bounds+on+the+Cost+of+a+Map-Reduce+Computation+Afrati+Das+Sarma+Salihoglu+Ullman,http://research.google.com/pubs/pub40413.html
Resource-bounded multicore emulation using Beefarm,Microprocessors and Microsystems (2012),2012,Oriol Arcas Nehir Sonmez Gokhan Sayilar Satnam Singh Osman S. Unsal Adrian Cristal Ibrahim Hur Mateo Valero,@article{40366 title = {Resource-bounded multicore emulation using Beefarm} author = {Oriol Arcas and Nehir Sonmez and Gokhan Sayilar and Satnam Singh and Osman S. Unsal and Adrian Cristal and Ibrahim Hur and Mateo Valero} year = 2012 URL = {http://www.sciencedirect.com/science/article/pii/S0141933112000920} journal = {Microprocessors and Microsystems} },In this article we present the Beefarm infrastructure for FPGA-based multiprocessor emulation a popular research topic of the last few years both in FPGA and computer architecture communities. We explain how we modify and extend a MIPS-based open-source soft core we discuss various design tradeoffs to make efficient use of the bounded resources available on chip and we demonstrate superior scalability compared to traditional software instruction set simulators through experimental results running Software Transactional Memory (STM) benchmarks. Based on our experience we comment on the pros and cons and the future trends of using hardware-based emulation for multicore research.,http://www.sciencedirect.com/science/article/pii/S0141933112000920,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Resource-bounded+multicore+emulation+using+Beefarm+Arcas+Sonmez+Sayilar+Singh+Unsal+Cristal+Hur+Valero,http://research.google.com/pubs/pub40366.html
DeViSE: A Deep Visual-Semantic Embedding Model,Neural Information Processing Systems (NIPS) (2013),2013,Andrea Frome Greg Corrado Jonathon Shlens Samy Bengio Jeffrey Dean Marc’Aurelio Ranzato Tomas Mikolov,@inproceedings{41869 title = {DeViSE: A Deep Visual-Semantic Embedding Model} author = {Andrea Frome and Greg Corrado and Jonathon Shlens and Samy Bengio and Jeffrey Dean and Marc’Aurelio Ranzato and Tomas Mikolov} year = 2013 booktitle = {Neural Information Processing Systems (NIPS)} },Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.,http://research.google.com/pubs/archive/41869.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=DeViSE:+A+Deep+Visual-Semantic+Embedding+Model+Frome+Corrado+Shlens+Bengio+Dean+Ranzato+Mikolov,http://research.google.com/pubs/pub41869.html
Accuracy of Contemporary Parametric Software Estimation Models: A Comparative Analysis,Proceeding of the 39th Euromicro Conference Series on Software Engineering and Advanced Applications IEEE Santander Spain (2013) pp. 313-316,2013,Derya Toka,@inproceedings{41441 title = {Accuracy of Contemporary Parametric Software Estimation Models: A Comparative Analysis} author = {Derya Toka} year = 2013 booktitle = {Proceeding of the 39th Euromicro Conference Series on Software Engineering and Advanced Applications} pages = {313-316} address = {Santander Spain} },Predicting the effort duration and cost required to develop and maintain a software system is crucial in IT project management. Although an accurate estimation is invaluable for the success of an IT development project it often proves difficult to attain. This paper presents an empirical evaluation of four parametric software estimation models namely COCOMO II SEER-SEM SLIM and TruePlanning in terms of their project effort and duration prediction accuracy. Using real project data from 51 software development projects we evaluated the capabilities of the models by comparing the predictions with the actual effort and duration values. The study showed that the estimation capabilities of the models investigated are on a par in accuracy while there is still significant room for improvement in order to better address the prediction challenges faced in practice.,http://research.google.com/pubs/archive/41441.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Accuracy+of+Contemporary+Parametric+Software++Estimation+Models:+A+Comparative+Analysis+Toka,http://research.google.com/pubs/pub41441.html
Planet-Planet Scattering in Planetesimal Disks,Astrophysical Journal Letters vol. 699 (2009) L88-L92,2009,Sean N. Raymond Philip J. Armitage Noel Gorelick,@article{42928 title = {Planet-Planet Scattering in Planetesimal Disks} author = {Sean N. Raymond and Philip J. Armitage and Noel Gorelick} year = 2009 journal = {Astrophysical Journal Letters} pages = {L88-L92} volume = {699} },We study the final architecture of planetary systems that evolve under the combined effects of planet-planet and planetesimal scattering. Using N-body simulations we investigate the dynamics of marginally unstable systems of gas and ice giants both in isolation and when the planets form interior to a planetesimal belt. The unstable isolated systems evolve under planet-planet scattering to yield an eccentricity distribution that matches that observed for extrasolar planets. When planetesimals are included the outcome depends upon the total mass of the planets. For M tot gsim 1 MJ the final eccentricity distribution remains broad whereas for M tot lsim 1 MJ a combination of divergent orbital evolution and recircularization of scattered planets results in a preponderance of nearly circular final orbits. We also study the fate of marginally stable multiple planet systems in the presence of planetesimal disks and find that for high planet masses the majority of such systems evolve into resonance. A significant fraction leads to resonant chains that are planetary analogs of Jupiter's Galilean satellites. We predict that a transition from eccentric to near-circular orbits will be observed once extrasolar planet surveys detect sub-Jovian mass planets at orbital radii of a sime 5-10 AU.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Planet-Planet+Scattering+in+Planetesimal+Disks+Raymond+Armitage+Gorelick,http://research.google.com/pubs/pub42928.html
General and nested Wiberg minimization: L2 and maximum likelihood,European Conference on Computer Vision Springer (2012),2012,Dennis Strelow,@inproceedings{38310 title = {General and nested Wiberg minimization: L2 and maximum likelihood} author = {Dennis Strelow} year = 2012 URL = {http://www.dennis-strelow.com/papers/documents/strelow_eccv12.pdf} booktitle = {European Conference on Computer Vision} },Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U linearizing V (U) about U and iteratively minimizing jjY UV (U)jj2 with respect to U only. This approach factors the matrix while eectively removing V from the minimization. We generalize the Wiberg approach beyond factorization to minimize an arbitrary function that is nonlinear in each of two sets of variables. In this paper we focus on the case of L2 minimization and maximum likelihood estimation (MLE) presenting an L2 Wiberg bundle adjustment algorithm and a Wiberg MLE algorithm for Poisson matrix factorization. We also show that one Wiberg minimization can be nested inside another eectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L2 projective bundle adjustment solving for camera matrices points and projective depths.,http://research.google.com/pubs/archive/38310.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=General+and+nested+Wiberg+minimization:+L2+and+maximum+likelihood+Strelow,http://research.google.com/pubs/pub38310.html
A critical review of studies investigating the quality of data obtained with online panels based on probability and nonprobability samples,Online Panel Research: A Data Quality Perspective Wiley (2014) pp. 23-53,2014,Mario Callegaro Ana Villar David S. Yeager Jon A. Krosnick,@inbook{42494 title = {A critical review of studies investigating the quality of data obtained with online panels based on probability and nonprobability samples} author = {Mario Callegaro and Ana Villar and David S. Yeager and Jon A. Krosnick} year = 2014 URL = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html} booktitle = {Online Panel Research: A Data Quality Perspective} pages = {23-53} },his chapter provides an overview of studies comparing the quality of data collected by online survey panels by looking at three criteria: (1) comparisons of point estimates from online panels to high-quality established population benchmarks; (2) comparisons of the relationship among variables; and (3) the reproducibility of results for online survey panels conducted on probability samples to panels conducted on nonprobability samples. When looking at point estimates all online survey panels differed to some extent from the population benchmarks. However the largest comparison studies suggest that point estimates from online panels of nonprobability samples have higher differences as compared to benchmarks than online panels of probability samples. This finding is consistent across time and across studies conducted in different countries. Moreover post-stratification weighting strategies helped little and in an inconsistent way to reduce such differences for data coming from online panels of nonprobability samples whereas these strategies did bring estimates from online panels of probability samples consistently closer to the benchmarks. When comparing relationships among variables it was found that researchers would reach different conclusions when using online panels of nonprobability samples versus panels of probability samples. When looking at reproducibility of results the limited evidence found suggests that there are no substantial differences in replication and effect size across probability and nonprobability samples for question wording experiments and when comparing students samples to other samples. It is worth noting that in pre-election polls an area where abundant prior knowledge exists online panels of nonprobability samples have consistently performed as well and in some cases better than polls based on probability samples in predicting election winners.,http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+critical+review+of+studies+investigating+the+quality+of+data+obtained+with+online+panels+based+on+probability+and+nonprobability+samples+Callegaro+Villar+Yeager+Krosnick,http://research.google.com/pubs/pub42494.html
Neural Networks and Neuroscience-Inspired Computer Vision,Current Biology vol. 24 (2014) pp. 921-929,2014,David Cox Tom Dean,@article{43140 title = {Neural Networks and Neuroscience-Inspired Computer Vision} author = {David Cox and Tom Dean} year = 2014 note = {Computational Neuroscience} journal = {Current Biology} pages = {921-929} volume = {24} },Brains are at a fundamental level biological computing machines. They transform a torrent of complex and ambiguous sensory information into coherent thought and action allowing an organism to perceive and model its environment synthesize and make decisions from disparate streams of information and adapt to a changing environment. Against this backdrop it is perhaps not surprising that computer science the science of building artificial computational systems has long looked to biology for inspiration. However while the opportunities for cross-pollination between neuroscience and computer science are great the road to achieving brain-like algorithms has been long and rocky. Here we review the historical connections between neuroscience and computer science and we look forward to a new era of potential collaboration enabled by recent rapid advances in both biologically-inspired computer vision and in experimental neuroscience methods. In particular we explore where neuroscience-inspired algorithms have succeeded where they still fail and we identify areas where deeper connections are likely to be fruitful.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Neural+Networks+and+Neuroscience-Inspired+Computer+Vision+Cox+Dean,http://research.google.com/pubs/pub43140.html
If You Put All The Pieces Together... - Attitudes Towards Data Combination and Sharing Across Services and Companies,ACM CHI (2016) (to appear),2016,Igor Bilogrevic Martin Ortlieb,"@inproceedings{44643 title = {""If You Put All The Pieces Together..."" - Attitudes Towards Data Combination and Sharing Across Services and Companies} author = {Igor Bilogrevic and Martin Ortlieb} year = 2016 booktitle = {ACM CHI} }",Online services often rely on processing users’ data which can be either provided directly by the users or combined from other services. Although users are aware of the latter it is unclear whether they are comfortable with such data combination whether they view it as beneficial for them or the extent to which they believe that their privacy is exposed. Through an online survey (N=918) and follow-up interviews (N=14) we show that (1) comfort is highly dependent on the type of data type of service and on the existence of a direct relationship with a company (2) users have a highly different opinion about the presence of benefits for them irrespectively of the context and (3) users perceive the combination of online data as more identifying than data related to offline and physical behavior (such as location). Finally we discuss several strategies for companies to improve upon these issues,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%22If+You+Put+All+The+Pieces+Together...%22+-+Attitudes+Towards+Data+Combination+and+Sharing+Across+Services+and+Companies+Bilogrevic+Ortlieb,http://research.google.com/pubs/pub44643.html
RFC7304 - A Method for Mitigating Namespace Collisions,IETF RFCs Internet Engineering Task Force (2014),2014,Warren Kumari,@incollection{42897 title = {RFC7304 - A Method for Mitigating Namespace Collisions} author = {Warren Kumari} year = 2014 URL = {http://tools.ietf.org/pdf/rfc7304.pdf} booktitle = {IETF RFCs} },This document outlines a possible but not recommended method to mitigate the effect of collisions in the DNS namespace by providing a means for end users to disambiguate the conflict.,http://research.google.com/pubs/archive/42897.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7304+-+A+Method+for+Mitigating+Namespace+Collisions+Kumari,http://research.google.com/pubs/pub42897.html
Learning Prices for Repeated Auctions with Strategic Buyers,Neural Information Processing Systems (2013),2013,Kareem Amin Afshin Rostamizadeh Umar Syed,@inproceedings{41394 title = {Learning Prices for Repeated Auctions with Strategic Buyers} author = {Kareem Amin and Afshin Rostamizadeh and Umar Syed} year = 2013 booktitle = {Neural Information Processing Systems} },Inspired by real-time ad exchanges for online display advertising we consider the problem of inferring a buyer’s value for a good when the buyer is repeatedly interacting with the seller through a posted-price mechanism. We model the buyer as a strategic agent interested in maximizing her long-term surplus and are interested in optimizing seller revenue. We show conditions under which the seller cannot hope to gain an advantage by learning the buyer’s value – i.e. the buyer can always manipulate the exchange to hide her value. This result is accompanied by a seller algorithm that is able to achieve no-regret when the buyer is unable to incur the short-term costs of such manipulation.,http://research.google.com/pubs/archive/41394.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Prices+for+Repeated+Auctions+with+Strategic+Buyers+Amin+Rostamizadeh+Syed,http://research.google.com/pubs/pub41394.html
RFC 7413 - TCP Fast Open,Internet Engineering Task Force (IETF) (2014),2014,Yuchung Cheng Jerry Chu Sivasankar Radhakrishnan Arvind Jain,@misc{43269 title = {RFC 7413 - TCP Fast Open} author = {Yuchung Cheng and Jerry Chu and Sivasankar Radhakrishnan and Arvind Jain} year = 2014 URL = {http://tools.ietf.org/html/rfc7413} },This document describes an experimental TCP mechanism called TCP Fast Open (TFO). TFO allows data to be carried in the SYN and SYN-ACK packets and consumed by the receiving end during the initial connection handshake and saves up to one full round-trip time (RTT) compared to the standard TCP which requires a three-way handshake (3WHS) to complete before data can be exchanged. However TFO deviates from the standard TCP semantics since the data in the SYN could be replayed to an application in some rare circumstances.Applications should not use TFO unless they can tolerate this issue as detailed in the Applicability section.,http://research.google.com/pubs/archive/43269.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC+7413+-+TCP+Fast+Open+Cheng+Chu+Radhakrishnan+Jain,http://research.google.com/pubs/pub43269.html
Who’s calling? The impact of Caller ID on telephone survey response,Field Methods vol. 22 (2010) pp. 175-191,2010,Mario Callegaro Allan L. McCutcheon Jack Ludwig,@article{42217 title = {Who’s calling? The impact of Caller ID on telephone survey response} author = {Mario Callegaro and Allan L. McCutcheon and Jack Ludwig} year = 2010 URL = {http://fmx.sagepub.com/content/22/2/175.abstract} journal = {Field Methods} pages = {175-191} volume = {22} },The Gallup Organization conducted a caller ID randomized study with a pre-and postexperimental design to test the impact of different caller ID displays (names) on the response contact and cooperation rates for telephone surveys. This research focuses on the impact of caller ID listing on the frequency of final dialing dispositions. The authors find initial evidence for the hypothesis that the caller ID transmission works as a sort of “condensed survey research organization business card” that can trigger brand awareness thus legitimating the survey and diminishing suspicions of collector or telemarketing calls.,http://fmx.sagepub.com/content/22/2/175.abstract,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Who%E2%80%99s+calling%3F+The+impact+of+Caller+ID+on+telephone+survey+response+Callegaro+McCutcheon+Ludwig,http://research.google.com/pubs/pub42217.html
Rolling Up Random Variables in Data Cubes,Joint Statistical Meetings American Statistical Association 732 North Washington Street Alexandria VA 22314-1943 (2013),2013,Phillip M. Yelland,@inproceedings{41652 title = {Rolling Up Random Variables in Data Cubes} author = {Phillip M. Yelland} year = 2013 booktitle = {Joint Statistical Meetings} address = {732 North Washington Street Alexandria VA 22314-1943} },Data cubes first developed in the context of on-line analytic processing (OLAP) applications for databases have become increasingly widespread as a means of structuring data aggregations in other contexts. For example increasing levels of aggregation in a data cube can be used to impose a hierarchical structure---often referred to as roll-ups---on sets of cross-categorized values producing a summary description that takes advantage of commonalities within the cube categories. In this paper we describe a novel technique for realizing such a hierarchical structure in a data cube containing discrete random variables. Using a generalization of an approach due to Chow and Liu this technique construes roll-ups as parsimonious approximations to the joint distribution of the variables in terms of the aggregation structure of the cube. The technique is illustrated using a real-life application that involves monitoring and reporting anomalies in Web traffic streams over time.,http://research.google.com/pubs/archive/41652.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Rolling+Up+Random+Variables+in+Data+Cubes+Yelland,http://research.google.com/pubs/pub41652.html
Multi-armed bandit experiments in the online service economy,Applied Stochastic Models in Business and Industry vol. 31 (2015) pp. 37-49,2015,Steven L. Scott,@article{42550 title = {Multi-armed bandit experiments in the online service economy} author = {Steven L. Scott} year = 2015 URL = {http://onlinelibrary.wiley.com/doi/10.1002/asmb.2104/abstract} note = {Special issue on actual impact and future perspectives on stochastic modelling in business and industry} journal = {Applied Stochastic Models in Business and Industry} pages = {37--49} volume = {31} },The modern service economy is substantively different from the agricultural and manufacturing economies that preceded it. In particular the cost of experimenting is dominated by opportunity cost rather than the cost of obtaining experimental units. The different economics require a new class of experiments in which stochastic models play an important role. This article briefly summarizes mulit-armed bandit experiments where the experimental design is modified as the experiment progresses to make the experiment as inexpensive as possible.,http://research.google.com/pubs/archive/42550.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multi-armed+bandit+experiments+in+the+online+service+economy+Scott,http://research.google.com/pubs/pub42550.html
Learning Invariant Features Using Inertial Priors,Annals of Mathematics and Artificial Intelligence vol. 47 (2006) pp. 223-250,2006,Thomas Dean,@article{34743 title = {Learning Invariant Features Using Inertial Priors} author = {Thomas Dean} year = 2006 journal = {Annals of Mathematics and Artificial Intelligence} pages = {223-250} volume = {47} },We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.,http://research.google.com/pubs/archive/34743.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Invariant+Features+Using+Inertial+Priors+Dean,http://research.google.com/pubs/pub34743.html
WHAD: Wikipedia historical attributes data,Language Resources and Evaluation (2013) pp. 28,2013,Enrique Alfonseca Guillermo Garrido Jean-Yves Delort Anselmo Peñas,@article{41184 title = {WHAD: Wikipedia historical attributes data} author = {Enrique Alfonseca and Guillermo Garrido and Jean-Yves Delort and Anselmo Peñas} year = 2013 URL = {http://rd.springer.com/content/pdf/10.1007%2Fs10579-013-9232-5.pdf} journal = {Language Resources and Evaluation} pages = {28} },This paper describes the generation of temporally anchored infobox attribute data from the Wikipedia history of revisions. By mining (attribute value) pairs from the revision history of the English Wikipedia we are able to collect a comprehensive knowledge base that contains data on how attributes change over time. When dealing with the Wikipedia edit history vandalic and erroneous edits are a concern for data quality. We present a study of vandalism identiﬁcation in Wikipedia edits that uses only features from the infoboxes and show that we can obtain on this dataset an accuracy comparable to a state-of-the-art vandalism identiﬁcation method that is based on the whole article. Finally we discuss different characteristics of the extracted dataset which we make available for further study.,http://research.google.com/pubs/archive/41184.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=WHAD:+Wikipedia+historical+attributes+data+Alfonseca+Garrido+Delort+Pe%C3%B1as,http://research.google.com/pubs/pub41184.html
Robust Estimation of Reverberation Time Using Polynomial Roots,AES 60th Conference on Dereverberation and Reverberation of Audio Music and Speech Google Ireland Ltd. (2016) (to appear),2016,Ian Kelly Francis Boland Jan Skoglund,@inproceedings{43989 title = {Robust Estimation of Reverberation Time Using Polynomial Roots} author = {Ian Kelly and Francis Boland and Jan Skoglund} year = 2016 booktitle = {AES 60th Conference on Dereverberation and Reverberation of Audio Music and Speech} address = {Google Ireland Ltd.} },This paper further investigates previous findings that coefficients of acoustic responses can be modelled as random polynomials with certain constraints applied. In the case of room impulse responses the median value of their clustered roots has been shown to be directly related to the reverberation time of the room. In this paper we examine the frequency dependency of reverberation time and we also demonstrate the method’s robustness to truncation of impulse responses.,http://research.google.com/pubs/archive/43989.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Robust+Estimation+of+Reverberation+Time+Using+Polynomial+Roots+Kelly+Boland+Skoglund,http://research.google.com/pubs/pub43989.html
Real-Time Grasp Detection Using Convolutional Neural Networks,International Conference on Robotics and Automation (ICRA) IEEE (2015),2015,Joseph Redmon Anelia Angelova,@inproceedings{43875 title = {Real-Time Grasp Detection Using Convolutional Neural Networks} author = {Joseph Redmon and Anelia Angelova} year = 2015 URL = {http://www.vision.caltech.edu/anelia/publications/RedmonAngelova15GraspDetection.pdf} booktitle = {International Conference on Robotics and Automation (ICRA)} },We present an accurate real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of- the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better especially on objects that can be grasped in a variety of ways.,http://research.google.com/pubs/archive/43875.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Real-Time+Grasp+Detection+Using+Convolutional+Neural+Networks+Redmon+Angelova,http://research.google.com/pubs/pub43875.html
A Comparison of Features for Automatic Readability Assessment,23rd International Conference on Computational Linguistics (COLING 2010) Poster Volume pp. 276-284,2010,Lijun Feng Martin Jansche Matt Huenerfauth Noémie Elhadad,@inproceedings{36449 title = {A Comparison of Features for Automatic Readability Assessment} author = {Lijun Feng and Martin Jansche and Matt Huenerfauth and Noémie Elhadad} year = 2010 URL = {http://www.aclweb.org/anthology/C10-2032} booktitle = {23rd International Conference on Computational Linguistics (COLING 2010) Poster Volume} pages = {276--284} },Several sets of explanatory variables – including shallow language modeling POS syntactic and discourse features – are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students. We find that features based on in-domain language models have the highest predictive power. Entity-density (a discourse feature) and POS-features in particular nouns are individually very useful but highly correlated. Average sentence length (a shallow feature) is more useful – and less expensive to compute – than individual syntactic features. A judicious combination of features examined here results in a significant improvement over the state of the art.,http://research.google.com/pubs/archive/36449.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Comparison+of+Features+for+Automatic+Readability+Assessment+Feng+Jansche+Huenerfauth+Elhadad,http://research.google.com/pubs/pub36449.html
Remote Medical Monitoring Through Vehicular Ad Hoc Network,IEEE 68th Vehicular Technology Conference (VTC) 2008. IEEE pp. 1-5,2008,Hyduke Noshadi Eugenio Giordano Hagop Hagopian Giovanni Pau Mario Gerla Majid Sarrafzadeh,@inproceedings{41370 title = {Remote Medical Monitoring Through Vehicular Ad Hoc Network} author = {Hyduke Noshadi and Eugenio Giordano and Hagop Hagopian and Giovanni Pau and Mario Gerla and Majid Sarrafzadeh} year = 2008 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4657288} booktitle = {IEEE 68th Vehicular Technology Conference (VTC) 2008.} pages = {1-5} },"Several diseases and medical conditions require constant monitoring of physiological signals and vital signs on daily bases such as diabetics hypertension and etc. In order to make these patients capable of living their daily life it is necessary to provide a platform and infrastructure that allows the constant collection of physiological data even when the patient is not inside of the coverage area. The data must be rapidly ""transported"" to care givers or to the designated medical enterprise. The problem is particularly severe in case of emergencies (e.g. natural disasters or hostile attacks) when the communications infrastructure (e.g. cellular telephony WiFi public access etc) has failed or is totally congested. In this paper we present an evaluation of of the vehicular ad-hoc networks (VANET) as an alternate method of collecting patient pre-recorded physiological data and at the same time reconfiguring patient medical wearable body vests to select the data specifically requested by the physicians. Another important use of vehicular collection of medical data from body vests is prompted by the need to correlate pedestrian reaction to vehicular traffic hazards such as chemical and noise pollution and traffic congestion. The vehicles collect noise chemical and traffic samples and can directly correlate with the ""stress level"" of volunteers.",http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4657288,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Remote+Medical+Monitoring+Through+Vehicular+Ad+Hoc+Network+Noshadi+Giordano+Hagopian+Pau+Gerla+Sarrafzadeh,http://research.google.com/pubs/pub41370.html
Automatic Speech and Speaker Recognition: Large Margin and Kernel Methods,Wiley (2009),2009,Joseph Keshet Samy Bengio,@book{34557 title = {Automatic Speech and Speaker Recognition: Large Margin and Kernel Methods} author = {Joseph Keshet and Samy Bengio} year = 2009 URL = {http://www.wiley.com/remtitle.cgi?isbn=9780470696835} },This is the first book dedicated to uniting research related to speech and speaker recognition based on the recent advances in large margin and kernel methods. The first part of the book presents theoretical and practical foundations of large margin and kernel methods from support vector machines to large margin methods for structured learning. The second part of the book is dedicated to acoustic modeling of continuous speech recognizers where the grounds for practical large margin sequence learning are set. The third part introduces large margin methods for discriminative language modeling. The last part of the book is dedicated to the application of keyword spotting speaker verification and spectral clustering. The book is an important reference to researchers and practitioners in the field of modern speech and speaker recognition. The purpose of the book is twofold; first to set the theoretical foundation of large margin and kernel methods relevant to speech recognition domain; second to propose a practical guide on implementation of these methods to the speech recognition domain. The reader is presumed to have basic knowledge of large margin and kernel methods and of basic algorithms in speech and speaker recognition.,http://research.google.com/pubs/archive/34557.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatic+Speech+and+Speaker+Recognition:+Large+Margin+and+Kernel+Methods+Keshet+Bengio,http://research.google.com/pubs/pub34557.html
Improving the speed of neural networks on CPUs,Deep Learning and Unsupervised Feature Learning Workshop NIPS 2011,2011,Vincent Vanhoucke Andrew Senior Mark Z. Mao,@inproceedings{37631 title = {Improving the speed of neural networks on CPUs} author = {Vincent Vanhoucke and Andrew Senior and Mark Z. Mao} year = 2011 booktitle = {Deep Learning and Unsupervised Feature Learning Workshop NIPS 2011} },Recent advances in deep learning have made the use of large deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden even for modern CPUs. For this reason GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout batching of the computation the use of SSE2 instructions and particularly leverage SSSE3 and SSE4 ﬁxed-point instructions which provide a 3X improvement over an optimized ﬂoating-point baseline. We use speech recognition as an example task and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10X speedup over an unoptimized baseline and a 4X speedup over an aggressively optimized ﬂoating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.,http://research.google.com/pubs/archive/37631.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+the+speed+of+neural+networks+on+CPUs+Vanhoucke+Senior+Mao,http://research.google.com/pubs/pub37631.html
A Language-Based Approach to Secure Quorum Replication,Proceedings of the Ninth Workshop on Programming Languages and Analysis for Security (2014) pp. 27-39,2014,Lantian Zheng Andrew C. Myers,@inproceedings{42903 title = {A Language-Based Approach to Secure Quorum Replication} author = {Lantian Zheng and Andrew C. Myers} year = 2014 booktitle = {Proceedings of the Ninth Workshop on Programming Languages and Analysis for Security} pages = {27-39} },Quorum replication is an important technique for building distributed systems because it can simultaneously improve both the integrity and availability of computation and storage. Information flow control is a well-known method for enforcing the confidentiality and integrity of information. This paper demonstrates that these two techniques can be integrated to simultaneously enforce all three major security properties: confidentiality integrity and availability. It presents a security-typed language with explicit language constructs for supporting secure quorum replication. The dependency analysis performed by the type system of the language provides a way to formally verify the end-to-end security assurance of complex replication schemes. We also contribute a new multilevel timestamp mechanism for synchronizing code and data replicas while controlling the side channels such mechanisms introduce.,http://research.google.com/pubs/archive/42903.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Language-Based+Approach+to+Secure+Quorum+Replication+Zheng+Myers,http://research.google.com/pubs/pub42903.html
Named Entity Transcription with Pair n-Gram Models,2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009) ACL-IJCNLP 2009 pp. 32-35,2009,Martin Jansche Richard Sproat,@inproceedings{35254 title = {Named Entity Transcription with Pair n-Gram Models} author = {Martin Jansche and Richard Sproat} year = 2009 URL = {http://aclweb.org/anthology/W09-3505} booktitle = {2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009) ACL-IJCNLP 2009} pages = {32--35} },We submitted results for each of the eight shared tasks. Except for Japanese name kanji restoration which uses a noisy channel model our Standard Run submissions were produced by generative long-range pair ngram models which we mostly augmented with publicly available data (either from LDC datasets or mined from Wikipedia) for the Non-Standard Runs.,http://research.google.com/pubs/archive/35254.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Named+Entity+Transcription+with+Pair+n-Gram+Models+Jansche+Sproat,http://research.google.com/pubs/pub35254.html
Star Quality: Aggregating Reviews to Rank Products and Merchants,Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM) AAAI (2010),2010,Mary McGlohon Natalie Glance Zach Reiter,@inproceedings{36265 title = {Star Quality: Aggregating Reviews to Rank Products and Merchants} author = {Mary McGlohon and Natalie Glance and Zach Reiter} year = 2010 booktitle = {Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM)} },Given a set of reviews of products or merchants from a wide range of authors and several reviews websites how can we measure the true quality of the product or merchant? How do we remove the bias of individual au- thors or sources? How do we compare reviews obtained from different websites where ratings may be on differ- ent scales (1-5 stars A/B/C etc.)? How do we filter out unreliable reviews to use only the ones with “star qual- ity”? Taking into account these considerations we an- alyze data sets from a variety of different reviews sites (the first paper to our knowledge to do this). These data sets include 8 million product reviews and 1.5 million merchant reviews. We explore statistic- and heuristic- based models for estimating the true quality of a prod- uct or merchant and compare the performance of these estimators on the task of ranking pairs of objects. We also apply the same models to the task of using Netflix ratings data to rank pairs of movies and discover that the performance of the different models is surprisingly similar on this data set.,http://research.google.com/pubs/archive/36265.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Star+Quality:+Aggregating+Reviews+to+Rank+Products+and+Merchants+McGlohon+Glance+Reiter,http://research.google.com/pubs/pub36265.html
Omega: flexible scalable schedulers for large compute clusters,SIGOPS European Conference on Computer Systems (EuroSys) ACM Prague Czech Republic (2013) pp. 351-364,2013,Malte Schwarzkopf Andy Konwinski Michael Abd-El-Malek John Wilkes,@inproceedings{41684 title = {Omega: flexible scalable schedulers for large compute clusters} author = {Malte Schwarzkopf and Andy Konwinski and Michael Abd-El-Malek and John Wilkes} year = 2013 URL = {http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf} booktitle = {SIGOPS European Conference on Computer Systems (EuroSys)} pages = {351--364} address = {Prague Czech Republic} },Increasing scale and the need for rapid response to changing requirements are hard to meet with current monolithic cluster scheduler architectures. This restricts the rate at which new features can be deployed decreases efficiency and utilization and will eventually limit cluster growth. We present a novel approach to address these needs using parallelism shared state and lock-free optimistic concurrency control. We compare this approach to existing cluster scheduler designs evaluate how much interference between schedulers occurs and how much it matters in practice present some techniques to alleviate it and finally discuss a use case highlighting the advantages of our approach -- all driven by real-life Google production workloads.,http://research.google.com/pubs/archive/41684.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Omega:+flexible+scalable+schedulers+for+large+compute+clusters+Schwarzkopf+Konwinski+Abd-El-Malek+Wilkes,http://research.google.com/pubs/pub41684.html
Reasoning about Risk and Trust in an Open World,Victoria University of Wellington (2015),2015,Sophia Drossopoulou James Noble Toby Murray Mark S. Miller,@techreport{44272 title = {Reasoning about Risk and Trust in an Open World} author = {Sophia Drossopoulou and James Noble and Toby Murray and Mark S. Miller} year = 2015 URL = {http://ecs.victoria.ac.nz/foswiki/pub/Main/TechnicalReportSeries/ECSTR15-08.pdf} institution = {Victoria University of Wellington} },Contemporary open systems use components developed by different parties linked together dynamically in unforeseen constellations. Code needs to live up to strict security requirements and ensure the correct functioning of its objects even when they collaborate with external potentially malicious objects. In this paper we propose special specification predicates that model risk and trust in open systems. We specify Miller Van Cutsem and Tulloh’s escrow exchange example and discuss the meaning of such a specification. We propose a novel Hoare logic based on four-tuples including an invariant describing properties preserved by the execution of a statement as well as a post-condition describing the state after execution. We model specification and programing languages based on the Hoare logic prove soundness and prove the key steps of the Escrow protocol.,http://research.google.com/pubs/archive/44272.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reasoning+about+Risk+and+Trust+in+an+Open+World+Drossopoulou+Noble+Murray+Miller,http://research.google.com/pubs/pub44272.html
Distributed Electronic Rights in JavaScript,ESOP'13 22nd European Symposium on Programming Springer (2013),2013,Mark S. Miller Tom Van Cutsem Bill Tulloh,@inproceedings{40673 title = {Distributed Electronic Rights in JavaScript} author = {Mark S. Miller and Tom Van Cutsem and Bill Tulloh} year = 2013 booktitle = {ESOP'13 22nd European Symposium on Programming} },Contracts enable mutually suspicious parties to cooperate safely through the exchange of rights. Smart contracts are programs whose behavior enforces the terms of the contract. This paper shows how such contracts can be specified elegantly and executed safely given an appropriate distributed secure persistent and ubiquitous computational fabric. JavaScript provides the ubiquity but must be significantly extended to deal with the other aspects. The first part of this paper is a progress report on our efforts to turn JavaScript into this fabric. To demonstrate the suitability of this design we describe an escrow exchange contract implemented in 42 lines of JavaScript code.,http://research.google.com/pubs/archive/40673.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Electronic+Rights+in+JavaScript+Miller+Cutsem+Tulloh,http://research.google.com/pubs/pub40673.html
Sound Ranking Using Auditory Sparse-Code Representations,ICML 2009 Workshop on Sparse Method for Music Audio,2009,Martin Rehn Richard F. Lyon Samy Bengio Thomas C. Walters Gal Chechik,@inproceedings{35269 title = {Sound Ranking Using Auditory Sparse-Code Representations} author = {Martin Rehn and Richard F. Lyon and Samy Bengio and Thomas C. Walters and Gal Chechik} year = 2009 booktitle = {ICML 2009 Workshop on Sparse Method for Music Audio} },The task of ranking sounds from text queries is a good test application for machine-hearing techniques and particularly for comparison and evaluation of alternative sound representations in a large-scale setting. We have adapted a machine-vision system ``passive-aggressive model for image retrieval'' (PAMIR) which efficiently learns using a ranking-based cost function a linear mapping from a very large sparse feature space to a large query-term space. Using this system allows us to focus on comparison of different auditory front ends and different ways of extracting sparse features from high-dimensional auditory images. In addition to two main auditory-image models we also include and compare a family of more conventional MFCC front ends. The experimental results show a significant advantage for the auditory models over vector-quantized MFCCs. The two auditory models tested use the adaptive pole-zero filter cascade (PZFC) auditory filterbank and sparse-code feature extraction from stabilized auditory images via multiple vector quantizers. The models differ in their implementation of the strobed temporal integration used to generate the stabilized image. Using ranking precision-at-top-k performance measures the best results are about 70% top-1 precision and 35% average precision using a test corpus of thousands of sound files and a query vocabulary of hundreds of words.,http://research.google.com/pubs/archive/35269.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sound+Ranking+Using+Auditory+Sparse-Code+Representations+Rehn+Lyon+Bengio+Walters+Chechik,http://research.google.com/pubs/pub35269.html
Lockdown: Towards a Safe and Practical Architecture for Security Applications on Commodity Platforms,TRUST 2012 Lecture Notes in Computer Science pp. 21,2012,Amit Vasudevan Bryan Parno Ning Qu Virgil D. Gligor Adrian Perrig,@inproceedings{39968 title = {Lockdown: Towards a Safe and Practical Architecture for Security Applications on Commodity Platforms} author = {Amit Vasudevan and Bryan Parno and Ning Qu and Virgil D. Gligor and Adrian Perrig} year = 2012 URL = {http://www.springerlink.com/content/r6m56j12vh502715/} booktitle = {TRUST 2012} pages = {21} },We investigate a new point in the design space of red/green systems [1930] which provide the user with a highly-protected yet also highly-constrained trusted (“green”) environment for performing security-sensitive transactions as well as a high-performance general-purpose environment for all other (non-security-sensitive or “red”) applications. Through the design and implementation of the Lockdown architecture we evaluate whether partitioning rather than virtualizing resources and devices can lead to better security or performance for red/green systems. We also design a simple external interface to allow the user to securely learn which environment is active and easily switch between them. We find that partitioning offers a new tradeoff between security performance and usability. On the one hand partitioning can improve the security of the “green” environment and the performance of the “red” environment (as compared with a virtualized solution). On the other hand with current systems partitioning makes switching between environments quite slow (13-31 seconds) which may prove intolerable to users.,http://research.google.com/pubs/archive/39968.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Lockdown:+Towards+a+Safe+and+Practical+Architecture+for+Security+Applications+on+Commodity+Platforms+Vasudevan+Parno+Qu+Gligor+Perrig,http://research.google.com/pubs/pub39968.html
Advertising and Traffic: Learning from online video data,Audience Measurement 6.0 New York NY (2011),2011,Dan Zigmond,@inproceedings{37146 title = {Advertising and Traffic: Learning from online video data} author = {Dan Zigmond} year = 2011 booktitle = {Audience Measurement 6.0} address = {New York NY} },Online media portals like Google’s YouTube are generating unprecedented volumes of data on usage patterns and viewing behavior. Learn about improving online advertising by understanding how ads impact online traffic.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Advertising+and+Traffic:+Learning+from+online+video+data+Zigmond,http://research.google.com/pubs/pub37146.html
Field verification of 40G DPSK upgrade in a legacy 10G network,Optical Fiber Communication IEEE (2010) NTuC2,2010,Valey Kamalov Bikash Koley Xiaoxue Zhao Cedric F. Lam,@inproceedings{36935 title = {Field verification of 40G DPSK upgrade in a legacy 10G network} author = {Valey Kamalov and Bikash Koley and Xiaoxue Zhao and Cedric F. Lam} year = 2010 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5465232} booktitle = {Optical Fiber Communication} pages = {NTuC2} },We report verification of 1200 km field upgrade of 10 G NRZ wavelengths with 40 G DPSK channels. Non symmetric dispersion map results in pronounced intra-channel nonlinear effect which could be significantly reduced by dispersion pre-compensation,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5465232,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Field+verification+of+40G+DPSK+upgrade+in+a+legacy+10G+network+Kamalov+Koley+Zhao+Lam,http://research.google.com/pubs/pub36935.html
Generalization Bounds for Learning Kernels,Proceedings of the 27th Annual International Conference on Machine Learning (ICML 2010),2010,Corinna Cortes Mehryar Mohri Afshin Rostamizadeh,@inproceedings{36467 title = {Generalization Bounds for Learning Kernels} author = {Corinna Cortes and Mehryar Mohri and Afshin Rostamizadeh} year = 2010 URL = {http://www.cs.nyu.edu/~mohri/pub/lk.pdf} booktitle = {Proceedings of the 27th Annual International Conference on Machine Learning (ICML 2010)} },This paper presents several novel generalization bounds for the problem of learning kernels based on a combinatorial analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels using L1 regularization admits only a √log p dependency on the number of kernels which is tight and considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a non-negative combination of p base kernels with an L2 regularization whose dependency on p is also tight and only in p^(1/4). We present similar results for Lq regularization with other values of q and outline the relevance of our proof techniques to the analysis of the complexity of the class of linear functions. Experiments with a large number of kernels further validate the behavior of the generalization error as a function of p predicted by our bounds.,http://research.google.com/pubs/archive/36467.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Generalization+Bounds+for+Learning+Kernels+Cortes+Mohri+Rostamizadeh,http://research.google.com/pubs/pub36467.html
Achieving Predictable Performance through Better Memory Controller Placement in Many-Core CMPs,Proceedings of the International Symposium on Computer Architecture ACM (2009),2009,Dennis Abts Natalie Engright Jerger John Kim Dan Gibson Mikko Lipasti,@inproceedings{35156 title = {Achieving Predictable Performance through Better Memory Controller Placement in Many-Core CMPs} author = {Dennis Abts and Natalie Engright Jerger and John Kim and Dan Gibson and Mikko Lipasti} year = 2009 booktitle = {Proceedings of the International Symposium on Computer Architecture} },In the near term Moore's law will continue to provide an increasing number of transistors and therefore an increasing number of on-chip cores. Limited pin bandwidth prevents the integration of a large number of memory controllers on-chip. With many cores and few memory controllers where to locate the memory controllers in the on-chip interconnection fabric becomes an important and as yet unexplored question. In this paper we show how the location of the memory controllers can reduce contention (hot spots) in the on-chip fabric as well as lower the variance in reference latency which provides for predictable performance of memory-intensive applications regardless of the processing core on which a thread is scheduled. We explore the design space of on-chip fabrics to find optimal memory controller placement relative to different topologies (i.e. mesh and torus) routing algorithms and workloads.,http://research.google.com/pubs/archive/35156.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Achieving+Predictable+Performance+through+Better+Memory+Controller+Placement+in+Many-Core+CMPs+Abts+Jerger+Kim+Gibson+Lipasti,http://research.google.com/pubs/pub35156.html
Computing weak consistency in polynomial time,Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing ACM New York NY USA pp. 395-404,2015,Wojciech Golab Xiaozhou (Steve) Li Alejandro López-Ortiz Naomi Nishimura,@inproceedings{43876 title = {Computing weak consistency in polynomial time} author = {Wojciech Golab and Xiaozhou (Steve) Li and Alejandro López-Ortiz and Naomi Nishimura} year = 2015 URL = {http://dl.acm.org/citation.cfm?id=2767407} booktitle = {Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing} pages = {395--404} address = {New York NY USA} },The k-atomicity property can be used to describe the consistency of data operations in large distributed storage systems. The weak consistency guarantees offered by such systems are seen as a necessary compromise in view of Brewer's CAP principle. The k-atomicity property requires that every read operation obtains a value that is at most k updates (writes) old and becomes a useful way to quantify weak consistency if k is treated as a variable that can be computed from a history of operations. Specifically the value of k quantifies how far the history deviates from Lamport's atomicity property for read/write registers. We address the problem of computing k indirectly by solving the k-atomicity verification problem (k-AV): given a history of read/write operations and a positive integer k decide whether the history is k-atomic. Gibbons and Korach showed that in general this problem is NP-complete when k = 1 and hence not solvable in polynomial time unless P = NP. In this paper we present two algorithms that solve the k-AV problem for any k >= 2 in special cases. Similarly to known solutions for k = 1 and k = 2 both algorithms assume that all the values written to a given object are distinct. The first algorithm places an additional restriction on the structure of the input history and solves k-AV in O(n^2 + n (k log k) time. The second algorithm does not place any additional restrictions on the input but is efficient only when k is small and when concurrency among write operations is limited. Its time complexity is O(n^2) if both k and our particular measure of write concurrency are bounded by constants.,http://dl.acm.org/citation.cfm?id=2767407,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Computing+weak+consistency+in+polynomial+time+Golab+Li+L%C3%B3pez-Ortiz+Nishimura,http://research.google.com/pubs/pub43876.html
High-Resolution Global Maps of 21st-Century Forest Cover Change,Science vol. 342 (2013) pp. 850-853,2013,Rebecca Moore Matt Hancher David Thau,@article{42119 title = {High-Resolution Global Maps of 21st-Century Forest Cover Change} author = {Rebecca Moore and Matt Hancher and David Thau} year = 2013 URL = {http://www.sciencemag.org/content/342/6160/850} note = {See Materials and Methods section for detailed description of Google Earth Engine contribution to this study. In Supplemental Materials p2-3 included in PDF.} journal = {Science} pages = {850-853} volume = {342} },Quantification of global forest change has been lacking despite the recognized importance of forest ecosystem services. In this study Earth observation satellite data were used to map global forest loss (2.3 million square kilometers) and gain (0.8 million square kilometers) from 2000 to 2012 at a spatial resolution of 30 meters. The tropics were the only climate domain to exhibit a trend with forest loss increasing by 2101 square kilometers per year. Brazil’s well-documented reduction in deforestation was offset by increasing forest loss in Indonesia Malaysia Paraguay Bolivia Zambia Angola and elsewhere. Intensive forestry practiced within subtropical forests resulted in the highest rates of forest change globally. Boreal forest loss due largely to fire and forestry was second to that in the tropics in absolute and proportional terms. These results depict a globally consistent and locally relevant record of forest change.,http://research.google.com/pubs/archive/42119.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=High-Resolution+Global+Maps+of+21st-Century+Forest+Cover+Change+Moore+Hancher+Thau,http://research.google.com/pubs/pub42119.html
Source-Side Classifier Preordering for Machine Translation,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP '13) (2013),2013,Uri Lerner Slav Petrov,@inproceedings{41651 title = {Source-Side Classifier Preordering for Machine Translation} author = {Uri Lerner and Slav Petrov} year = 2013 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP '13)} },We present a simple and novel classifier-based preordering approach. Unlike existing preordering models we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree while utilizing a discriminative model with a rich set of features including lexical features. We present extensive experiments on 22 language pairs including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.,http://research.google.com/pubs/archive/41651.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Source-Side+Classifier+Preordering+for+Machine+Translation+Lerner+Petrov,http://research.google.com/pubs/pub41651.html
Diversity maximization under matroid constraints,KDD ACM SIGKDD (2013) pp. 32-40,2013,Zeinab Abbassi Vahab Mirrokni Mayur Thakur,@inproceedings{41408 title = {Diversity maximization under matroid constraints} author = {Zeinab Abbassi and Vahab Mirrokni and Mayur Thakur} year = 2013 URL = {http://academiccommons.columbia.edu/download/fedora_content/download/ac:154865/CONTENT/cucs-019-12.pdf} booktitle = {KDD} pages = {32-40} },Aggregator websites typically present documents in the form of representative clusters. In order for users to get a broader perspective it is important to deliver a diversiﬁed set of representative documents in those clusters. One approach to diversiﬁcation is to maximize the average dissimilarity among documents. Another way to capture diversity is to avoid showing several documents from the same category (e.g. from the same news channel). We combine the above two diversiﬁcation concepts by modeling the latter approach as a (partition) matroid constraint and study diversity maximization problems under matroid constraints. We present the ﬁrst constant-factor approximation algorithm for this problem using a new technique. Our local search 0:5-approximation algorithm is also the ﬁrst constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints uses the existence of a family of Latin squares which may also be of independent interest. In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool we develop greedy clustering algorithms that maximize weighted coverage of a predeﬁned set of topics. Our algorithms are based on computing a set of cluster centers where clusters are formed around them. We show the better performance of our algorithms for diversity and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts. Finally we perform a user study validating our algorithms and diversity metrics.,http://academiccommons.columbia.edu/download/fedora_content/download/ac:154865/CONTENT/cucs-019-12.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Diversity+maximization+under+matroid+constraints+Abbassi+Mirrokni+Thakur,http://research.google.com/pubs/pub41408.html
The Mobile Web in Developing Countries,W3C Workshop on the Mobile Web in Developing Countries W3C W3C (2006),2006,Ravi Jain,@inproceedings{34631 title = {The Mobile Web in Developing Countries} author = {Ravi Jain} year = 2006 URL = {http://www.w3.org/2006/07/MWI-EC/PC/rj-mobileweb-developingcountries.pdf} booktitle = {W3C Workshop on the Mobile Web in Developing Countries} address = {W3C} },The mobile web in developing countries has received increasing attention within the last few years both as a potential means of bridging the digital divide as well as a lucrative market opportunity. However while the realized gains so far as well as the potential are indeed tremendous significant challenges remain to be overcome. Mobile data usage particularly for advanced data applications faces difficulties that are different from those posed by the initial expansion of voice services. The needs and environments of developing countries are very diverse with as many significant differences perhaps as similarities making it difficult to replicate country-specific solutions. In addition while one traditional migration route of functionality – from the desktop to the handheld – may be viable in the industrialized world it is not clear that this is the likely best approach in developing countries. What does seem clear is that there is a definite and significant need for further research examining the characteristics and challenges of the mobile web in developing countries at all layers ranging from applications to networking. We sketch examples of such research issues and mention specific roles the W3C could potentially play. This brief position paper presents these hypotheses with the goal of stimulating discussion at the workshop.,http://research.google.com/pubs/archive/34631.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Mobile+Web+in+Developing+Countries+Jain,http://research.google.com/pubs/pub34631.html
History Dependent Domain Adaptation,Domain Adaptation Workshop at NIPS '11 (2011),2011,Allen Lavoie Matthew Eric Otey Nathan Ratliff,@inproceedings{42452 title = {History Dependent Domain Adaptation} author = {Allen Lavoie and Matthew Eric Otey and Nathan Ratliff} year = 2011 booktitle = {Domain Adaptation Workshop at NIPS '11} },We study a novel variant of the domain adaptation problem in which the loss function on test data changes due to dependencies on prior predictions. One important instance of this problem area occurs in settings where it is more costly to make a new error than to repeat a previous error. We propose several methods for learning effectively in this setting and test them empirically on the real-world tasks of malicious URL classiﬁcation and adversarial advertisement detection.,http://research.google.com/pubs/archive/42452.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=History+Dependent+Domain+Adaptation+Lavoie+Otey+Ratliff,http://research.google.com/pubs/pub42452.html
Approximating the Effects of Installed Traffic Lights: A Behaviorist Approach Based on Travel Tracks,International Conference on Intelligent Transportation Systems (2015),2015,Shumeet Baluja Michele Covell Rahul Sukthankar,@inproceedings{43988 title = {Approximating the Effects of Installed Traffic Lights: A Behaviorist Approach Based on Travel Tracks} author = {Shumeet Baluja and Michele Covell and Rahul Sukthankar} year = 2015 URL = {http://www.esprockets.com/papers/itsc-discovery.pdf} booktitle = {International Conference on Intelligent Transportation Systems} },Decades of research have been directed towards improving the timing of existing traffic lights. In many parts of the world where this research has been conducted detailed maps of the streets and the precise locations of the traffic lights are publicly available. Continued timing research has recently been further spurred by the increasing ubiquity of personal cell-phone based GPS systems. Through their use an enormous amount of travel tracks have been amassed — thus providing an easy source of real traffic data. Nonetheless one fundamental piece of information remains absent that limits the quantification of the benefits of new approaches: the existing traffic light schedules and traffic light response behaviors. Unfortunately deployed traffic light schedules are often not known. Rarely are they kept in a central database and even when they are they are often not easily obtainable. The alternative manual inspection of a system of multiple traffic lights may be prohibitively expensive and time-consuming for many experimenters. Without the existing light schedules it is difficult to ascertain the real-improvements that new traffic light algorithms and approaches will have — especially on traffic patterns that have not yet been encountered in the collected data. To alleviate this problem we present an approach to estimating existing traffic light schedules based on collected GPS-travel tracks. We present numerous ways to test the results and comprehensively demonstrate them on both synthetic and real data. One of the many uses beyond studying the effects of existing lights in previously unencountered traffic flow environments is to serve as a realistic baseline for light timing and schedule optimization studies.,http://www.esprockets.com/papers/itsc-discovery.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Approximating+the+Effects+of+Installed+Traffic+Lights:+A+Behaviorist+Approach+Based+on+Travel+Tracks+Baluja+Covell+Sukthankar,http://research.google.com/pubs/pub43988.html
Towards A Unified Modeling and Verification of Network and System Security Configuration,5th Symposium on Configuration Analytics and Automation (SafeConfig 2012),2012,Mohammed Noraden Alsaleh Ehab Al-Shaer Adel El-Atawy,@inproceedings{40606 title = {Towards A Unified Modeling and Verification of Network and System Security Configuration} author = {Mohammed Noraden Alsaleh and Ehab Al-Shaer and Adel El-Atawy} year = 2012 booktitle = {5th Symposium on Configuration Analytics and Automation (SafeConfig 2012)} },Systems and networks access control configuration are usually analyzed independently although they are logically combined to define the the end-to-end security property. While systems and applications security policies define access control based on user identity or group request type and the requested resource network security policies uses flow information such as host and service addresses for source and destination to define access control. Therefore both network and systems access control have to be configured consistently in order enforce end-to-end security policies. Many previous research attempt to verify either side separately but it does not provide a unified approach to automatically validate the logical consistency between both of them. Thus using existing techniques requires error-prone manual and ad-hoc analysis to validate this link. In this paper we introduce a cross-layer modeling and verification system that can analyzes the configurations and policies across both application and network components as a single unit. It combines policies from different devices as firewalls NAT routers and IPSec gateways as well as basic RBAC-based policies of higher service layers. This will allow analyzing for example firewall polices in the context of application access control and vice versa. Thus by incorporating policies across the network and over multiple layers we provide a true end-to-end configuration verification tool. Our model represents the system as a state machine where packet header service request and location determine the state and transitions that conform with the configurations device operations and packet values are established. We encode the model as Boolean functions using binary decision diagrams (BDDs). We used an extended version of computational tree logic (CTL) to provide more useful operators and then use it with symbolic model checking to prove or find counter examples to needed properties. The tool is implemented and we gave special consideration to efficiency and scalability. Our extensive evaluation study shows acceptable computation and space requirements with large number of nodes and configuration sizes.,http://research.google.com/pubs/archive/40606.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Towards+A+Unified+Modeling+and+Verification+of+Network+and+System+Security+Configuration+Alsaleh+Al-Shaer+El-Atawy,http://research.google.com/pubs/pub40606.html
Compact Hyperplane Hashing with Bilinear Functions,International Conference on Machine Learning (ICML) (2012),2012,Wei Liu Jun Wang Yadong Mu Sanjiv Kumar Shih-Fu Chang,@inproceedings{38141 title = {Compact Hyperplane Hashing with Bilinear Functions} author = {Wei Liu and Jun Wang and Yadong Mu and Sanjiv Kumar and Shih-Fu Chang} year = 2012 URL = {http://www.sanjivk.com/ICML12_hyperplane_hash.pdf} booktitle = {International Conference on Machine Learning (ICML)} },Hyperplane hashing aims at rapidly searching nearest points to a hyperplane and has shown practical impact in scaling up active learning with SVMs. Unfortunately the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach.,http://research.google.com/pubs/archive/38141.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Compact+Hyperplane+Hashing+with+Bilinear+Functions+Liu+Wang+Mu+Kumar+Chang,http://research.google.com/pubs/pub38141.html
Spatiotemporal Assignment of Energy Harvesters on a Self-Sustaining Medical Shoe,2012 IEEE Sensors IEEE pp. 1-4,2012,James Wendt Vishwa Goudar Hyduke Noshadi Miodrag Potkonjak,@inproceedings{41352 title = {Spatiotemporal Assignment of Energy Harvesters on a Self-Sustaining Medical Shoe} author = {James Wendt and Vishwa Goudar and Hyduke Noshadi and Miodrag Potkonjak} year = 2012 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6411353} booktitle = {2012 IEEE Sensors} pages = {1-4} },We present a new method for spatiotemporal assignment and scheduling of energy harvesters on a medical shoe tasked with measuring gait diagnostics. While prior work exists on the application of dielectric elastomers (DEs) for energy scavenging on shoes current literature does not address the issues of placement and timing of these harvesters nor does it address integration into existing sensing systems. We solve these issues and present a self-sustaining medical shoe that harvests energy from human ambulation while simultaneously measuring gait characteristics most relevant to medical diagnosis.,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6411353,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Spatiotemporal+Assignment+of+Energy+Harvesters+on+a+Self-Sustaining+Medical+Shoe+Wendt+Goudar+Noshadi+Potkonjak,http://research.google.com/pubs/pub41352.html
A unified format for traces of peer-to-peer systems,LSAP '09: Proceedings of the 1st ACM workshop on Large-Scale system and application performance ACM New York NY USA (2009) pp. 27-34,2009,Boxun Zhang Alexandru Iosup Pawel Garbacki Johan Pouwelse,@inproceedings{36232 title = {A unified format for traces of peer-to-peer systems} author = {Boxun Zhang and Alexandru Iosup and Pawel Garbacki and Johan Pouwelse} year = 2009 URL = {http://doi.acm.org/10.1145/1552272.1552279} booktitle = {LSAP '09: Proceedings of the 1st ACM workshop on Large-Scale system and application performance} pages = {27--34} address = {New York NY USA} },Peer-to-Peer (P2P) systems have recently emerged as a scalable platform for which costs are shared between the system users. Today P2P technology is serving millions of users world-wide with applications such as file sharing video streaming grid computing and massively multiplayer online games. Such diversity and scale pose important research and technical problems which in turn require a much better understanding of the usage patterns and of the performance bottlenecks. However the large amounts of P2P monitoring and measurement data that already exist have not been made public for fear of lack of anonymity and in lack of a standard format. To address this problem in this work we propose a unified format for workloads of P2P systems. Our format stores information coming from many types of P2P applications at several levels of detail has a structure that balances generic and application-specific data and protects the anonymity of the peers whose personal information was captured in monitoring and measurement data. Using two large traces taken from real P2P systems we show evidence of the usefulness of the proposed format and substantiate the hope that our unified format has the potential to become a standard for sharing P2P traces.,http://doi.acm.org/10.1145/1552272.1552279,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+unified+format+for+traces+of+peer-to-peer+systems+Zhang+Iosup+Garbacki+Pouwelse,http://research.google.com/pubs/pub36232.html
The Intervalgram: An Audio Feature for Large-Scale Cover-Song Recognition,From Sounds to Music and Emotions: 9th International Symposium CMMR 2012 London UK June 19-22 2012 Revised Selected Papers Springer Berlin Heidelberg (2013) pp. 197-213,2013,Thomas C. Walters David A. Ross Richard F. Lyon,@incollection{41764 title = {The Intervalgram: An Audio Feature for Large-Scale Cover-Song Recognition} author = {Thomas C. Walters and David A. Ross and Richard F. Lyon} year = 2013 URL = {http://link.springer.com/chapter/10.1007/978-3-642-41248-6_11} booktitle = {From Sounds to Music and Emotions: 9th International Symposium CMMR 2012 London UK June 19-22 2012 Revised Selected Papers} pages = {197-213} },We present a system for representing the musical content of short pieces of audio using a novel chroma-based representation known as the ‘intervalgram’ which is a summary of the local pattern of musical intervals in a segment of music. The intervalgram is based on a chroma representation derived from the temporal profile of the stabilized auditory image [10] and is made locally pitch invariant by means of a ‘soft’ pitch transposition to a local reference. Intervalgrams are generated for a piece of music using multiple overlapping windows. These sets of intervalgrams are used as the basis of a system for detection of identical melodic and harmonic progressions in a database of music. Using a dynamic-programming approach for comparisons between a reference and the song database performance is evaluated on the ‘covers80’ dataset [4]. A first test of an intervalgram-based system on this dataset yields a precision at top-1 of 53.8% with an ROC curve that shows very high precision up to moderate recall suggesting that the intervalgram is adept at identifying the easier-to-match cover songs in the dataset with high robustness. The intervalgram is designed to support locality-sensitive hashing such that an index lookup from each single intervalgram feature has a moderate probability of retrieving a match with few false matches. With this indexing approach a large reference database can be quickly pruned before more detailed matching as in previous content-identification systems.,http://research.google.com/pubs/archive/41764.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Intervalgram:+An+Audio+Feature+for+Large-Scale+Cover-Song+Recognition+Walters+Ross+Lyon,http://research.google.com/pubs/pub41764.html
Dynamic Covering for Recommendation Systems,CIKM (2012),2012,Ioannis Antonellis Anish Das Sarma Shaddin Dughmi,@inproceedings{40414 title = {Dynamic Covering for Recommendation Systems} author = {Ioannis Antonellis and Anish Das Sarma and Shaddin Dughmi} year = 2012 URL = {http://arxiv.org/abs/0912.2404} booktitle = {CIKM} },In this paper we identify a fundamental algorithmic problem that we term succinct dynamic covering (SDC) arising in many modern-day web applications including ad-serving and online recommendation systems in eBay and Netflix. Roughly speaking SDC applies two restrictions to the well-studied Max-Coverage problem: Given an integer k X={12...n} and I={S_1 ... S_m} S_i a subset of X find a subset J of I such that |J| <= k and the union of S in J is as large as possible. The two restrictions applied by SDC are: (1) Dynamic: At query-time we are given a query Q a subset of X and our goal is to find J such that the intersection of Q with the union of S in J is as large as possible; (2) Space-constrained: We don't have enough space to store (and process) the entire input; specifically we have o(mn) and maybe as little as O((m+n)polylog(mn)) space. The goal of SDC is to maintain a small data structure so as to answer most dynamic queries with high accuracy. We call such a scheme a Coverage Oracle. We present algorithms and complexity results for coverage oracles. We present deterministic and probabilistic near-tight upper and lower bounds on the approximation ratio of SDC as a function of the amount of space available to the oracle. Our lower bound results show that to obtain constant-factor approximations we need Omega(mn) space. Fortunately our upper bounds present an explicit tradeoff between space and approximation ratio allowing us to determine the amount of space needed to guarantee certain accuracy.,http://arxiv.org/abs/0912.2404,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+Covering+for+Recommendation+Systems+Antonellis+Das+Sarma+Dughmi,http://research.google.com/pubs/pub40414.html
Video Object Discovery and Co-segmentation with Extremely Weak Supervision,Proceedings of European Conference on Computer Vision (2014),2014,Le Wang Gang Hua Rahul Sukthankar Jianru Xue Nanning Zheng,@inproceedings{42961 title = {Video Object Discovery and Co-segmentation with Extremely Weak Supervision} author = {Le Wang and Gang Hua and Rahul Sukthankar and Jianru Xue and Nanning Zheng} year = 2014 booktitle = {Proceedings of European Conference on Computer Vision} },Video object co-segmentation refers to the problem of simultaneously segmenting a common category of objects from multiple videos. Most existing video co-segmentation methods assume that all frames from all videos contain the target objects. Unfortunately this assumption is rarely true in practice particularly for large video sets and existing methods perform poorly when the assumption is violated. Hence any practical video object co-segmentation algorithm needs to identify the relevant frames containing the target object from all videos and then co-segment the object only from these relevant frames. We present a spatiotemporal energy minimization formulation for simultaneous video object discovery and co-segmentation across multiple videos. Our formulation incorporates a spatiotemporal auto-context model which is combined with appearance modeling for superpixel labeling. The superpixel-level labels are propagated to the frame level through a multiple instance boosting algorithm with spatial reasoning (Spatial-MILBoosting) based on which frames containing the video object are identified. Our method only needs to be bootstrapped with the frame-level labels for a few video frames (e.g. usually 1 to 3) to indicate if they contain the target objects or not. Experiments on three datasets validate the efficacy of our proposed method which compares favorably with the state-of-the-art.,http://research.google.com/pubs/archive/42961.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Video+Object+Discovery+and+Co-segmentation+with+Extremely+Weak+Supervision+Wang+Hua+Sukthankar+Xue+Zheng,http://research.google.com/pubs/pub42961.html
A classifier for the latency-CPU behaviors of serving jobs in distributed environments,SoCC 15 (2015) (to appear),2015,Christophe Restif Natalia Ponomareva Krzysztof Ostrowski,@inproceedings{43472 title = {A classifier for the latency-CPU behaviors of serving jobs in distributed environments} author = {Christophe Restif and Natalia Ponomareva and Krzysztof Ostrowski} year = 2015 booktitle = {SoCC 15} },End-to-end latency of serving jobs in distributed and shared environments such as a Cloud is an important metric for jobs' owners and infrastructure providers. Yet it is notoriously challenging to model precisely since it is affected by a large collection of unrelated moving pieces from the software design to the job schedulers strategies. In this work we present a novel approach to modeling latency by tracking how it varies with CPU usage. We train a classifier to automatically assign the latency behavior of methods in three classes: constant latency regardless of CPU uncorrelated latency and CPU and predictable latency as a function of CPU. We use our model on a random sample of serving jobs running on the Google infrastructure. We illustrate unexpected and insightful patterns of latency variations with CPU. The visualization of latency-CPU variations and the corresponding class may be used by both jobs' owners and infrastructure providers for a variety of applications such as smarter latency alerting latency-aware configuration of jobs and automated detection of changes in behavior either over time during pre-release testing or across data centers.,http://research.google.com/pubs/archive/43472.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+classifier+for+the+latency-CPU+behaviors+of+serving+jobs+in+distributed+environments+Restif+Ponomareva+Ostrowski,http://research.google.com/pubs/pub43472.html
Optical Interconnects for Scale-Out Data Centers,Optical Interconnects for Future Data Center Networks Springer Avenel NJ (2013) pp. 17-31,2013,Hong Liu Ryohei Urata Amin Vahdat,@inbook{41087 title = {Optical Interconnects for Scale-Out Data Centers} author = {Hong Liu and Ryohei Urata and Amin Vahdat} year = 2013 URL = {http://books.google.com/books?id=rsWoO4oY3CkC&pg=PA17&lpg=PA17&dq=springer+optics+for+scale+out&source=bl&ots=lBnrVNlQZP&sig=hpLGJlEQV2tXYD-cypsO5fbBezk&hl=en&sa=X&ei=jsBPUebQF-iaiQL99oHoAQ&ved=0CEkQ6AEwAw#v=onepage&q=springer%20optics%20for%20scale%20out&f=false} booktitle = {Optical Interconnects for Future Data Center Networks} pages = {17-31} address = {Avenel NJ} },We review the architecture of modern datacenter networks as well as their scaling challenges; we then present opportunities and needs for emerging optical technologies to support datacenter scaling.,http://books.google.com/books?id=rsWoO4oY3CkC&pg=PA17&lpg=PA17&dq=springer+optics+for+scale+out&source=bl&ots=lBnrVNlQZP&sig=hpLGJlEQV2tXYD-cypsO5fbBezk&hl=en&sa=X&ei=jsBPUebQF-iaiQL99oHoAQ&ved=0CEkQ6AEwAw#v=onepage&q=springer%20optics%20for%20scale%20out&f=false,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optical+Interconnects+for+Scale-Out+Data+Centers+Liu+Urata+Vahdat,http://research.google.com/pubs/pub41087.html
Physically-based Grasp Quality Evaluation under Pose Uncertainty,IEEE Transactions on Robotics (2013),2013,Junggon Kim Kunihiro Iwamoto James J.Kuffner Yasuhiro Ota Nancy S. Pollard,@article{41587 title = {Physically-based Grasp Quality Evaluation under Pose Uncertainty} author = {Junggon Kim and Kunihiro Iwamoto and James J.Kuffner and Yasuhiro Ota and Nancy S. Pollard} year = 2013 journal = {IEEE Transactions on Robotics} },Although there has been great progress in robot grasp planning automatically generated grasp sets using a quality metric are not as robust as human generated grasp sets when applied to real problems. Most previous research on grasp quality metrics has focused on measuring the quality of established grasp contacts after grasping but it is difﬁcult to reproduce the same planned ﬁnal grasp conﬁguration with a real robot hand which makes the quality evaluation less useful in practice. In this study we focus more on the grasping process which usually involves changes in contact and object location and explore the efﬁcacy of using dynamic simulation in estimating the likely success or failure of a grasp in the real environment. Among many factors that can possibly affect the result of grasping we particularly investigated the effect of considering object dynamics and pose uncertainty on the performance in estimating the actual grasp success rates measured from experiments. We observed that considering both dynamics and uncertainty improved the performance signiﬁcantly and when applied to automatic grasp set generation this method generated more stable and natural grasp sets compared to a commonly used method based on kinematic simulation and force-closure analysis.,http://research.google.com/pubs/archive/41587.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Physically-based+Grasp+Quality+Evaluation+under+Pose+Uncertainty+Kim+Iwamoto+Kuffner+Ota+Pollard,http://research.google.com/pubs/pub41587.html
Topology Discovery of Sparse Random Graphs With Few Participants,ACM International Conference on Measurement and Modeling of Computer Systems SIGMETRICS (2011) Best Paper Award,2011,Animashree Anandkumar Avinatan Hassidim Jonathan Kelner,@inproceedings{37121 title = {Topology Discovery of Sparse Random Graphs With Few Participants} author = {Animashree Anandkumar and Avinatan Hassidim and Jonathan Kelner} year = 2011 URL = {http://www2.lns.mit.edu/~avinatan/research/AnandkumarHassidimKelner_Discovery} booktitle = {ACM International Conference on Measurement and Modeling of Computer Systems SIGMETRICS} pages = {Best Paper Award} },We consider the task of topology discovery of sparse random graphs using end-to-end random measurements (e.g. delay) between a subset of nodes referred to as the participants. The rest of the nodes are hidden and do not provide any information for topology discovery. We consider topology discovery under two routing models: (a) the participants exchange messages along the shortest paths and obtain end-to-end measurements and (b) additionally the participants exchange messages along the second shortest path. For scenario (a) our proposed algorithm results in a sub-linear edit-distance guarantee using a sub-linear number of uniformly selected participants. For scenario (b) we obtain a much stronger result and show that we can achieve consistent reconstruction when a sub-linear number of uniformly selected nodes participate. This implies that accurate discovery of sparse random graphs is tractable using an extremely small number of participants. Our algorithms are simple to implement computationally efficient and exploit the locally tree-like property of sparse random graphs. We finally obtain a lower bound on the number of participants required by any algorithm to reconstruct the original random graph up to a given edit distance. We also demonstrate that while consistent discovery is tractable for sparse random graphs using a small number of participants in general there are graphs which cannot be discovered by any algorithm even with a significant number of participants and with the availability of end-to-end information along all the paths between the participants.,http://research.google.com/pubs/archive/37121.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Topology+Discovery+of+Sparse+Random+Graphs+With+Few+Participants+Anandkumar+Hassidim+Kelner,http://research.google.com/pubs/pub37121.html
Modelling Events through Memory-based Open-IE Patterns for Abstractive Summarization,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL'14) (2014) pp. 892-901,2014,Daniele Pighin Marco Cornolti Enrique Alfonseca Katja Filippova,@inproceedings{42470 title = {Modelling Events through Memory-based Open-IE Patterns for Abstractive Summarization} author = {Daniele Pighin and Marco Cornolti and Enrique Alfonseca and Katja Filippova} year = 2014 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL'14)} pages = {892--901} },Abstractive text summarization of news requires a way of representing events such as a collection of pattern clusters in which every cluster represents an event (e.g. marriage) and every pattern in the cluster is a way of expressing the event (e.g. X married Y X and Y tied the knot). We compare three ways of extracting event patterns: heuristics-based compression-based and memory-based. While the former has been used previously in multi-document abstraction the latter two have never been used for this task. Compared with the first two techniques the memory-based method allows for generating significantly more grammatical and informative sentences at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances. To this end we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances.,http://research.google.com/pubs/archive/42470.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Modelling+Events+through+Memory-based+Open-IE+Patterns+for+Abstractive+Summarization+Pighin+Cornolti+Alfonseca+Filippova,http://research.google.com/pubs/pub42470.html
Schematic Surface Reconstruction,IEEE Conference on Computer Vision and Pattern Recognition IEEE (2012),2012,Changchang Wu Sameer Agarwal Brian Curless Steven M. Seitz,@inproceedings{40600 title = {Schematic Surface Reconstruction} author = {Changchang Wu and Sameer Agarwal and Brian Curless and Steven M. Seitz} year = 2012 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition} },This paper introduces a schematic representation for architectural scenes together with robust algorithms for reconstruction from sparse 3D point cloud data. The schematic models architecture as a network of transport curves approximating a ﬂoorplan with associated proﬁle curves together comprising an interconnected set of swept surfaces. The representation is extremely concise composed of a handful of planar curves and easily interpretable by humans. The approach also provides a principled mechanism for interpolating a dense surface and enables ﬁlling in holes in the data by means of a pipeline that employs a global optimization over all parameters. By incorporating a displacement map on top of the schematic surface it is possible to recover ﬁne details. Experiments show the ability to reconstruct extremely clean and simple models from sparse structure-from-motion point clouds of complex architectural scenes.,http://research.google.com/pubs/archive/40600.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Schematic+Surface+Reconstruction+Wu+Agarwal+Curless+Seitz,http://research.google.com/pubs/pub40600.html
How Many People Visit YouTube? Imputing Missing Events in Panels With Excess Zeros,; SAGE Publications - edited by Herwig Friedl and Helga Wagner Linz Austria (2015) pp. 1-6,2015,Georg M. Goerg Yuxue Jin Nicolas Remy Jim Koehler,@techreport{43286 title = {How Many People Visit YouTube? Imputing Missing Events in Panels With Excess Zeros} author = {Georg M. Goerg and Yuxue Jin and Nicolas Remy and Jim Koehler} year = 2015 },Media-metering panels track TV and online usage of people to analyze viewing behavior. However panel data is often incomplete due to non-registered devices non-compliant panelists or work usage. We thus propose a probabilistic model to impute missing events in data with excess zeros using a negative-binomial hurdle model for the unobserved events and beta-binomial sub-sampling to account for missingness. We then use the presented models to estimate the number of people in Germany who visit YouTube.,http://research.google.com/pubs/archive/43286.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Many+People+Visit+YouTube%3F+Imputing+Missing+Events+in+Panels+With+Excess+Zeros+Goerg+Jin+Remy+Koehler,http://research.google.com/pubs/pub43286.html
Optimal Hashing Schemes for Entity Matching,22nd International World Wide Web Conference WWW '13 ACM Rio de Janeiro Brazil (2013) pp. 295-306,2013,Nilesh Dalvi Vibhor Rastogi Anirban Dasgupta Anish Das Sarma Tamas Sarlos,@inproceedings{41465 title = {Optimal Hashing Schemes for Entity Matching} author = {Nilesh Dalvi and Vibhor Rastogi and Anirban Dasgupta and Anish Das Sarma and Tamas Sarlos} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2488415} booktitle = {22nd International World Wide Web Conference WWW '13} pages = {295-306} address = {Rio de Janeiro Brazil} },In this paper we consider the problem of devising blocking schemes for entity matching. There is a lot of work on blocking techniques for supporting various kinds of predicates e.g. exact matches fuzzy string-similarity matches and spatial matches. However given a complex entity matching function in the form of a Boolean expression over several such predicates we show that it is an important and non-trivial problem to combine the individual blocking techniques into an efficient blocking scheme for the entity matching function a problem that has not been studied previously. In this paper we make fundamental contributions to this problem. We consider an abstraction for modeling complex entity matching functions as well as blocking schemes. We present several results of theoretical and practical interest for the problem. We show that in general the problem of computing the optimal blocking strategy is NP-hard in the size of the DNF formula describing the matching function. We also present several algorithms for computing the exact optimal strategies (with exponential complexity but often feasible in practice) as well as fast approximation algorithms. We experimentally demonstrate over commercially used rule-based matching systems over real datasets at Yahoo! as well as synthetic datasets that our blocking strategies can be an order of magnitude faster than the baseline methods and our algorithms can efficiently find good blocking strategies.,http://dl.acm.org/citation.cfm?id=2488415,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimal+Hashing+Schemes+for+Entity+Matching+Dalvi+Rastogi+Dasgupta+Das+Sarma+Sarlos,http://research.google.com/pubs/pub41465.html
CPU bandwidth control for CFS,Proceedings of the Linux Symposium Linux Symposium (2010) pp. 245-254,2010,Paul Turner Bharata B Rao Nikhil Rao,@inproceedings{36669 title = {CPU bandwidth control for CFS} author = {Paul Turner and Bharata B Rao and Nikhil Rao} year = 2010 URL = {http://www.linuxsymposium.org/LS_2010_Proceedings_Draft.pdf} booktitle = {Proceedings of the Linux Symposium} pages = {245-254} },Over the past few years there has been an increasing focus on the development of features which deliver resource management within the Linux kernel. The addition of the fair group scheduler has enabled the provisioning of proportional CPU time through the specification of group weights. As the scheduler is inherently work-conserving in nature a task or a group may consume excess CPU share in an otherwise idle system. There are many scenarios where this unbounded CPU share may lead to unacceptable utilization or latency variation. CPU bandwidth control approaches this problem by allowing an explicit upper bound for allowable CPU bandwidth to be defined in addition to the lower bound already provided by shares. There are many enterprise scenarios where this functionality is useful. In particular are the cases of pay-per-use environments and user facing services where provisioning is latency bounded. In this paper we detail the motivations behind this feature the challenges involved in incorporating into CFS (Completely Fair Scheduler) and the future development road map.,http://research.google.com/pubs/archive/36669.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=CPU+bandwidth+control+for+CFS+Turner+Rao+Rao,http://research.google.com/pubs/pub36669.html
Sparse Non-negative Matrix Language Modeling for Geo-annotated Query Session Data,Automatic Speech Recognition and Understanding Workshop (ASRU 2015) Proceedings IEEE to appear (to appear),2015,Ciprian Chelba Noam M. Shazeer,@inproceedings{43964 title = {Sparse Non-negative Matrix Language Modeling for Geo-annotated Query Session Data} author = {Ciprian Chelba and Noam M. Shazeer} year = 2015 booktitle = {Automatic Speech Recognition and Understanding Workshop (ASRU 2015) Proceedings} pages = {to appear} },The paper investigates the impact on query language modeling when using skip-grams within query as well as across queries in a given search session in conjunction with the geo-annotation available for the query stream data. As modeling tool we use the recently proposed sparse non-negative matrix estimation technique since it offers the same expressive power as the well-established maximum entropy approach in combining arbitrary context features. Experiments on the google.com query stream show that using session-level and geo-location context we can expect reductions in perplexity of 34% relative over the Kneser Ney N-gram baseline; when evaluating on the `''local'' subset of the query stream the relative reduction in PPL is 51%---more than a bit. Both sources of context information (geo-location and previous queries in session) are about equally valuable in building a language model for the query stream.,http://research.google.com/pubs/archive/43964.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sparse+Non-negative+Matrix+Language+Modeling+for+Geo-annotated+Query+Session+Data+Chelba+Shazeer,http://research.google.com/pubs/pub43964.html
Cardinal Contests,Proceedings of the 24th International Conference on the World Wide Web (WWW) (2015) pp. 377-387,2015,Arpita Ghosh Patrick Hummel,@inproceedings{43787 title = {Cardinal Contests} author = {Arpita Ghosh and Patrick Hummel} year = 2015 URL = {http://www.www2015.it/documents/proceedings/proceedings/p377.pdf} booktitle = {Proceedings of the 24th International Conference on the World Wide Web (WWW)} pages = {377-387} },Contests are widely used as a means for effort elicitation in settings ranging from government R&D contests to online crowdsourcing contests on platforms such as Kaggle Innocentive or TopCoder. Such rank-order mechanisms—— where agents' rewards depend only on the relative ranking of their submissions' qualities——are natural mechanisms for incentivizing effort when it is easier to obtain ordinal rather than cardinal information about agents' outputs or where absolute measures of quality are unverifiable. An increasing number of online contests however rank entries according to some numerical evaluation of their absolute quality——for instance the performance of an algorithm on a test dataset or the performance of an intervention in a randomized trial. Can the contest designer incentivize higher effort by making the rewards in an ordinal rank-order mechanism contingent on such cardinal information? We model and analyze cardinal contests where a principal running a rank-order tournament has access to an absolute measure of the qualities of agents' submissions in addition to their relative rankings and ask how modifying the rank-order tournament to incorporate cardinal information can improve incentives for effort. Our main result is that a simple threshold mechanism——a mechanism that awards the prize for a rank if and only if the absolute quality of the agent at that rank exceeds a certain threshold——is optimal amongst all mixed cardinal-ordinal mechanisms where the fraction of the j-th prize awarded to the j-th-ranked agent is any arbitrary non-decreasing function of her submission's quality. Further the optimal threshold mechanism uses exactly the same threshold for each rank. We study what contest parameters determine the extent of the benefit from incorporating such cardinal information into an ordinal rank-order contest and investigate the extent of improvement in equilibrium effort via numerical simulations.,http://www.www2015.it/documents/proceedings/proceedings/p377.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cardinal+Contests+Ghosh+Hummel,http://research.google.com/pubs/pub43787.html
Span-program-based quantum algorithm for evaluating formulas,Theory of Computing vol. 8(13) (2012) pp. 291-319,2012,Ben Reichardt Robert Spalek,@article{33344 title = {Span-program-based quantum algorithm for evaluating formulas} author = {Ben Reichardt and Robert Spalek} year = 2012 URL = {http://theoryofcomputing.org/articles/v008a013/} note = {Earlier version in STOC'08} journal = {Theory of Computing} pages = {291-319} volume = {8(13)} },We give a quantum algorithm for evaluating formulas over an extended gate set including all two- and three-bit binary gates (e. g. NAND 3-majority). The algorithm is optimal on read-once formulas for which each gate’s inputs are balanced in a certain sense. The main new tool is a correspondence between a classical linear-algebraic model of computation “span programs” and weighted bipartite graphs. A span program’s evaluation corresponds to an eigenvalue-zero eigenvector of the associated graph. A quantum computer can therefore evaluate the span program by applying spectral estimation to the graph. For example the classical complexity of evaluating the balanced ternary majority formula is unknown and the natural generalization of randomized alpha-beta pruning is known to be suboptimal. In contrast our algorithm generalizes the optimal quantum AND-OR formula evaluation algorithm and is optimal for evaluating the balanced ternary majority formula.,http://research.google.com/pubs/archive/33344.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Span-program-based+quantum+algorithm+for+evaluating+formulas+Reichardt+Spalek,http://research.google.com/pubs/pub33344.html
A New Approach to Optimal Code Formatting,Google Inc. (2016),2016,Phillip Yelland,@misc{44667 title = {A New Approach to Optimal Code Formatting} author = {Phillip Yelland} year = 2016 note = {Technical note for open source project rfmt; https://github.com/google/rfmt} },The rfmt code formatter incorporates a new algorithm that optimizes code layout with respect to an intuitive notion of layout cost. This note describes the foundations of the algorithm and the programming abstractions used to facilitate its use with a variety of languages and code layout policies.,http://research.google.com/pubs/archive/44667.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+New+Approach+to+Optimal+Code+Formatting+Yelland,http://research.google.com/pubs/pub44667.html
Efficient Spatial Sampling of Large Geographical Tables,SIGMOD (2012),2012,Anish Das Sarma Hongrae Lee Hector Gonzalez Jayant Madhavan Alon Y. Halevy,@inproceedings{38123 title = {Efficient Spatial Sampling of Large Geographical Tables} author = {Anish Das Sarma and Hongrae Lee and Hector Gonzalez and Jayant Madhavan and Alon Y. Halevy} year = 2012 URL = {http://i.stanford.edu/~anishds/publications/sigmod12/modf289-dassarma.pdf} booktitle = {SIGMOD} },Large-scale map visualization systems play an increasingly important role in presenting geographic datasets to end users. Since these datasets can be extremely large a map rendering system often needs to select a small fraction of the data to visualize them in a limited space. This paper addresses the fundamental challenge of {\em thinning}: determining appropriate samples of data to be shown on specific geographical regions and zoom levels. Other than the sheer scale of the data the thinning problem is challenging because of a number of other reasons: (1) data can consist of complex geographical shapes (2) rendering of data needs to satisfy certain constraints such as data being preserved across zoom levels and adjacent regions and (3) after satisfying the constraints an {\em optimal} solution needs to be chosen based on {\em objectives} such as {\em maximality} {\em fairness} and {\em importance} of data. This paper formally defines and presents a complete solution to the thinning problem. First we express the problem as an integer programming formulation that efficiently solves thinning for desired objectives. Second we present more efficient solutions for maximality based on DFS traversal of a spatial tree. Third we consider the common special case of point datasets and present an even more efficient randomized algorithm. Finally we have implemented all techniques from this paper in Google Maps visualizations of Fusion Tables and we describe a set of experiments that demonstrate the tradeoffs among the algorithms.,http://research.google.com/pubs/archive/38123.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Spatial+Sampling+of+Large+Geographical+Tables+Das+Sarma+Lee+Gonzalez+Madhavan+Halevy,http://research.google.com/pubs/pub38123.html
Query Language Modeling for Voice Search,Proceedings of the 2010 IEEE Workshop on Spoken Language Technology IEEE pp. 127-132,2010,Ciprian Chelba Johan Schalkwyk Thorsten Brants Vida Ha Boulos Harb Will Neveitt Carolina Parada Peng Xu,@inproceedings{36732 title = {Query Language Modeling for Voice Search} author = {Ciprian Chelba and Johan Schalkwyk and Thorsten Brants and Vida Ha and Boulos Harb and Will Neveitt and Carolina Parada and Peng Xu} year = 2010 booktitle = {Proceedings of the 2010 IEEE Workshop on Spoken Language Technology} pages = {127-132} },The paper presents an empirical exploration of google.com query stream language modeling. We describe the normalization of the typed query stream resulting in out-of-vocabulary (OoV) rates below 1% for a one million word vocabulary. We present a comprehensive set of experiments that guided the design decisions for a voice search service. In the process we re-discovered a less known interaction between Kneser-Ney smoothing and entropy pruning and found empirical evidence that hints at non-stationarity of the query stream as well as strong dependence on various English locales---USA Britain and Australia.,http://research.google.com/pubs/archive/36732.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Query+Language+Modeling+for+Voice+Search+Chelba+Schalkwyk+Brants+Ha+Harb+Neveitt+Parada+Xu,http://research.google.com/pubs/pub36732.html
Efficient Learning of Sparse Ranking Functions,Empirical Inference Springer (2013),2013,Mark Stevens Samy Bengio Yoram Singer,@inbook{41474 title = {Efficient Learning of Sparse Ranking Functions} author = {Mark Stevens and Samy Bengio and Yoram Singer} year = 2013 booktitle = {Empirical Inference} },Algorithms for learning to rank can be inefficient when they employ risk functions that use structural information. We describe and analyze a learning algorithm that efficiently learns a ranking function using a domination loss. This loss is designed for problems in which we need to rank a small number of positive examples over a vast number of negative examples. In that context we propose an efficient coordinate descent approach that scales linearly with the number of examples. We then present an extension that incorporates regularization thus extending Vapnik¿s notion of regularized empirical risk minimization to ranking learning. We also discuss an extension to the case of multi-values feedback. Experiments performed on several benchmark datasets and large scale Google internal dataset demonstrate the effectiveness of learning algorithm in constructing compact models while retaining the empirical performance accuracy.,http://research.google.com/pubs/archive/41474.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Learning+of+Sparse+Ranking+Functions+Stevens+Bengio+Singer,http://research.google.com/pubs/pub41474.html
Dividing secrets to secure data outsourcing,Information Sciences vol. 263 (2014) pp. 198-210,2014,Fatih Emekci Ahmed Methwally Divyakant Agrawal Amr El Abbadi,@article{42507 title = {Dividing secrets to secure data outsourcing} author = {Fatih Emekci and Ahmed Methwally and Divyakant Agrawal and Amr El Abbadi} year = 2014 URL = {http://www.sciencedirect.com/science/article/pii/S0020025513007214} journal = {Information Sciences} pages = {198--210} volume = {263} },Data outsourcing or database as a service is a new paradigm for data management. The third party service provider hosts databases as a service. These parties provide efficient and cheap data management by obviating the need to purchase expensive hardware and software deal with software upgrades and hire professionals for administrative and maintenance tasks. However due to recent governmental legislations competition among companies and database thefts companies cannot use database service providers directly. They need secure and privacy preserving data management techniques to be able to use them in practice. Since data is remotely stored in a privacy preserving manner there are efficiency related problems such as poor query response time. We propose a new framework that provides efficient and scalable query response times by reducing the computation and communication costs. Furthermore the proposed technique uses several service providers to guarantee the availability of the services while detecting the dishonest or faulty service providers without introducing additional overhead on the query response time. The evaluations demonstrate that our data outsourcing framework is scalable and practical.,http://www.sciencedirect.com/science/article/pii/S0020025513007214,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dividing+secrets+to+secure+data+outsourcing+Emekci+Metwally+Agrawal+Abbadi,http://research.google.com/pubs/pub42507.html
Calibration-Free Rolling Shutter Removal,International Conference on Computational Photography [Best Paper] IEEE (2012),2012,Matthias Grundmann Vivek Kwatra Daniel Castro Irfan Essa,@inproceedings{37744 title = {Calibration-Free Rolling Shutter Removal} author = {Matthias Grundmann and Vivek Kwatra and Daniel Castro and Irfan Essa} year = 2012 booktitle = {International Conference on Computational Photography [Best Paper]} },We present a novel algorithm for efficient removal of rolling shutter distortions in uncalibrated streaming videos. Our proposed method is calibration free as it does not need any knowledge of the camera used nor does it require calibration using specially recorded calibration sequences. Our algorithm can perform rolling shutter removal under varying focal lengths as in videos from CMOS cameras equipped with an optical zoom. We evaluate our approach across a broad range of cameras and video sequences demonstrating robustness scaleability and repeatability. We also conducted a user study which demonstrates preference for the output of our algorithm over other state-of-the art methods. Our algorithm is computationally efficient easy to parallelize and robust to challenging artifacts introduced by various cameras with differing technologies.,http://research.google.com/pubs/archive/37744.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Calibration-Free+Rolling+Shutter+Removal+Grundmann+Kwatra+Castro+Essa,http://research.google.com/pubs/pub37744.html
The Performance Cost of Shadow Stacks and Stack Canaries,Proceedings of the 10th ACM Symposium on Information Computer and Communications Security (ASIACCS) ACM (2015) pp. 555-566,2015,Thurston H.Y. Dang Petros Maniatis David Wagner,@inproceedings{43809 title = {The Performance Cost of Shadow Stacks and Stack Canaries} author = {Thurston H.Y. Dang and Petros Maniatis and David Wagner} year = 2015 booktitle = {Proceedings of the 10th ACM Symposium on Information Computer and Communications Security (ASIACCS)} pages = {555--566} },Control flow defenses against ROP either use strict expensive but strong protection against redirected RET instructions with shadow stacks or much faster but weaker protections without. In this work we study the inherent overheads of shadow stack schemes. We find that the overhead is roughly 10% for a traditional shadow stack. We then design a new scheme the parallel shadow stack and show that its performance cost is significantly less: 3.5%. Our measurements suggest it will not be easy to improve performance on current x86 processors further due to inherent costs associated with RET and memory load/store instructions. We conclude with a discussion of the design decisions in our shadow stack instrumentation and possible lighter-weight alternatives.,http://research.google.com/pubs/archive/43809.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Performance+Cost+of+Shadow+Stacks+and+Stack+Canaries+Dang+Maniatis+Wagner,http://research.google.com/pubs/pub43809.html
Keep on Lockin' in the Free World: A Multi-National Comparison of Smartphone Locking,Proceedings of the 34th Annual ACM Conference on Human Factors in Computing Systems (CHI'16) ACM New York NY USA (2016) (to appear),2016,Marian Harbach Alexander De Luca Nathan Malkin Serge Egelman,@inproceedings{44676 title = {Keep on Lockin' in the Free World: A Multi-National Comparison of Smartphone Locking} author = {Marian Harbach and Alexander De Luca and Nathan Malkin and Serge Egelman} year = 2016 booktitle = {Proceedings of the 34th Annual ACM Conference on Human Factors in Computing Systems (CHI'16)} address = {New York NY USA} },We present the results of an online survey of smartphone unlocking (N=8286) that we conducted in eight different countries. The goal was to investigate differences in attitudes towards smartphone unlocking between different national cultures. Our results show that there are indeed significant differences across a range of categories. For instance participants in Japan considered the data on their smartphones to be much more sensitive than those in other countries and respondents in Germany were 4.5 times more likely than others to say that protecting data on their smartphones was important. The results of this study shed light on how motivations to use various security mechanisms are likely to differ from country to country.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Keep+on+Lockin'+in+the+Free+World:+A+Multi-National+Comparison+of+Smartphone+Locking+Harbach+De+Luca+Malkin+Egelman,http://research.google.com/pubs/pub44676.html
RSSAC002 - RSSAC Advisory on Measurements of the Root Server System,ICANN Root Server System Advisory Committee ( RSSAC ) Reports and Advisories Internet Corporation for Assigned Names and Numbers (ICANN) (2015) pp. 15,2015,Warren Kumari,@incollection{43973 title = {RSSAC002 - RSSAC Advisory on Measurements of the Root Server System} author = {Warren Kumari} year = 2015 URL = {https://www.icann.org/en/system/files/files/rssac-002-measurements-root-20nov14-en.pdf} booktitle = {ICANN Root Server System Advisory Committee ( RSSAC ) Reports and Advisories} pages = {15} },RSSAC has begun work to determine a list of parameters that define the desired service trends for the root zone system. These parameters include the measured latency in the distribution of the root zone the frequency of the updates and their size. With knowledge of these parameters in hand RSSAC can then seek to produce estimates of acceptable root zone size dynamics to ensure the overall system works within a set of parameters. The future work to define these parameters will involve RSSAC working closely with the root server operators to gather best practice estimates for the size and update frequency of the root zone. It must be well understood that the measurements described in this document are a response to the current awareness experience and understanding of the Root Zone System. As time progresses more less or entirely different metrics may be required to investigate new concerns or defined problem statements.,http://research.google.com/pubs/archive/43973.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RSSAC002+-+RSSAC+Advisory+on+Measurements+of+the+Root+Server+System+Kumari,http://research.google.com/pubs/pub43973.html
Performance bounds and design criteria for estimating finite rate of innovation signals,IEEE Transactions on Information Theory vol. 58 (2012) pp. 4993-5015,2012,Zvika Ben-Haim Tomer Michaeli Yonina C. Eldar,@article{40405 title = {Performance bounds and design criteria for estimating finite rate of innovation signals} author = {Zvika Ben-Haim and Tomer Michaeli and Yonina C. Eldar} year = 2012 URL = {http://ieeexplore.ieee.org/application/enterprise/entconfirmation.jsp?arnumber=6200857&icp=false} journal = {IEEE Transactions on Information Theory} pages = {4993-5015} volume = {58} },In this paper we consider the problem of estimating ﬁnite rate of innovation (FRI) signals from noisy measurements and speciﬁcally analyze the interaction between FRI techniques and the underlying sampling methods. We ﬁrst obtain a fundamental limit on the estimation accuracy attainable regardless of the sampling method. Next we provide a bound on the performance achievable using any speciﬁc sampling approach. Essential differences between the noisy and noise-free cases arise from this analysis. In particular we identify settings in which noise-free recovery techniques deteriorate substantially under slight noise levels thus quantifying the numerical instability inherent in such methods. This instability which is only present in some families of FRI signals is shown to be related to a speciﬁc type of structure which can be characterized by viewing the signal model as a union of subspaces. Finally we develop a methodology for choosing the optimal sampling kernels for linear reconstruction based on a generalization of the Karhunen–Loeve transform. The results are illustrated for several types of time-delay estimation problems.,http://ieeexplore.ieee.org/application/enterprise/entconfirmation.jsp?arnumber=6200857&icp=false,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Performance+bounds+and+design+criteria+for+estimating+finite+rate+of+innovation+signals+Ben-Haim+Michaeli+Eldar,http://research.google.com/pubs/pub40405.html
Efficient Hierarchical Graph-Based Video Segmentation,Computer Vision and Pattern Recognition (CVPR 2010),2010,Matthias Grundmann Vivek Kwatra Mei Han Irfan Essa,@inproceedings{36247 title = {Efficient Hierarchical Graph-Based Video Segmentation} author = {Matthias Grundmann and Vivek Kwatra and Mei Han and Irfan Essa} year = 2010 URL = {http://cpl.cc.gatech.edu/projects/videosegmentation/} booktitle = {Computer Vision and Pattern Recognition (CVPR 2010)} },"We present an efficient and scalable technique for spatio-temporal segmentation of long video sequences using a hierarchical graph-based algorithm. We begin by over-segmenting a volumetric video graph into space-time regions grouped by appearance. We then construct a ``region graph"" over the obtained segmentation and iteratively repeat this process over multiple levels to create a tree of spatio-temporal segmentations. This hierarchical approach generates high quality segmentations which are temporally coherent with stable region boundaries. Additionally the resulting segmentation hierarchy allows subsequent applications to choose from varying levels of granularity. We further improve segmentation quality by using dense optical flow when constructing the initial graph. We also propose two novel approaches to improve the scalability of our technique: (a) a parallel out-of-core algorithm that can process volumes much larger than an in-core algorithm and (b) a clip-based processing algorithm that divides the video into overlapping clips in time and segments them successively while enforcing consistency. We can segment video shots as long as 40 seconds without compromising quality and even support a streaming mode for arbitrarily long videos albeit without the ability to process them hierarchically.",http://research.google.com/pubs/archive/36247.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Hierarchical+Graph-Based+Video+Segmentation+Grundmann+Kwatra+Han+Essa,http://research.google.com/pubs/pub36247.html
AGILE: elastic distributed resource scaling for Infrastructure-as-a-Service,10th International Conference on Autonomic Computing (ICAC) USENIX San Jose CA USA (2013) pp. 69-82,2013,Hiep Nguyen Zhiming Shen Xiaohui Gu Sethuraman Subbiah John Wilkes,@inproceedings{41685 title = {AGILE: elastic distributed resource scaling for Infrastructure-as-a-Service} author = {Hiep Nguyen and Zhiming Shen and Xiaohui Gu and Sethuraman Subbiah and John Wilkes} year = 2013 URL = {https://www.usenix.org/conference/icac13/agile-elastic-distributed-resource-scaling-infrastructure-service} booktitle = {10th International Conference on Autonomic Computing (ICAC)} pages = {69--82} address = {San Jose CA USA} },Dynamically adjusting the number of virtual machines (VMs) assigned to a cloud application to keep up with load changes and interference from other uses typically requires detailed application knowledge and an ability to know the future neither of which are readily available to infrastructure service providers or application owners. The result is that systems need to be over-provisioned (costly) or risk missing their performance Service Level Objectives (SLOs) and have to pay penalties (also costly). AGILE deals with both issues: it uses wavelets to provide a medium-term resource demand prediction with enough lead time to start up new application server instances before performance falls short and it uses dynamic VM cloning to reduce application startup times. Tests using RUBiS and Google cluster traces show that AGILE can predict varying resource demands over the medium-term with up to 3.42_ better true positive rate and 0.34_ the false positive rate than existing schemes. Given a target SLO violation rate AGILE can efﬁciently handle dynamic application workloads reducing both penalties and user dissatisfaction.,http://research.google.com/pubs/archive/41685.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=AGILE:+elastic+distributed+resource+scaling+for+Infrastructure-as-a-Service+Nguyen+Shen+Gu+Subbiah+Wilkes,http://research.google.com/pubs/pub41685.html
Online Vertex-Weighted Bipartite Matching and Single-bid Budgeted Allocations,Proceedings of ACM-SIAM Symposium on Discrete Algorithms (2011),2011,Gagan Aggarwal Gagan Goel Chinmay Karande Aranyak Mehta,@inproceedings{36742 title = {Online Vertex-Weighted Bipartite Matching and Single-bid Budgeted Allocations} author = {Gagan Aggarwal and Gagan Goel and Chinmay Karande and Aranyak Mehta} year = 2011 URL = {http://arxiv.org/abs/1007.1271} booktitle = {Proceedings of ACM-SIAM Symposium on Discrete Algorithms} },We study the following vertex-weighted online bipartite matching problem: G(U V E) is a bipartite graph. The vertices in U have weights and are known ahead of time while the vertices in V arrive online in an arbitrary order and have to be matched upon arrival. The goal is to maximize the sum of weights of the matched vertices in U. When all the weights are equal this reduces to the classic online bipartite matching problem for which Karp Vazirani and Vazirani gave an optimal (1 _ 1/e)-competitive algorithm in their seminal work [KVV90]. Our main result is an optimal (1 _ 1/e)-competitive randomized algorithm for general vertex weights. We use random perturbations of weights by appropriately chosen multiplicative factors. Our solution constitutes the ﬁrst known generalization of the algorithm in [KVV90] in this model and provides new insights into the role of randomization in online allocation problems. It also effectively solves the problem of online budgeted allocations [MSVV05] in the case when an agent makes the same bid for any desired item even if the bid is comparable to his budget - complementing the results of [MSVV05 BJN07] which apply when the bids are much smaller than the budgets.,http://research.google.com/pubs/archive/36742.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Vertex-Weighted+Bipartite+Matching+and+Single-bid+Budgeted+Allocations+Aggarwal+Goel+Karande+Mehta,http://research.google.com/pubs/pub36742.html
Cutting the Cord: a Robust Wireless Facilities Network for Data Centers,Proceedings of the ACM Conference on Mobile Computing and Networking (Mobicom) (2014),2014,Yibo Zhu Xia Zhou Zengbin Zhang Lin Zhou Amin Vahdat Ben Y. Zhao Haitao Zheng,@inproceedings{43860 title = {Cutting the Cord: a Robust Wireless Facilities Network for Data Centers} author = {Yibo Zhu and Xia Zhou and Zengbin Zhang and Lin Zhou and Amin Vahdat and Ben Y. Zhao and Haitao Zheng} year = 2014 booktitle = {Proceedings of the ACM Conference on Mobile Computing and Networking (Mobicom)} },Today’s network control and management traffic are limited by their reliance on existing data networks. Fate sharing in this context is highly undesirable since control traffic has very different availability and traffic delivery requirements. In this paper we explore the feasibility of building a dedicated wireless facilities network for data centers. We propose Angora a low-latency facilities network using low-cost 60GHz beamforming radios that provides robust paths decoupled from the wired network and flexibility to adapt to workloads and network dynamics. We describe our solutions to address challenges in link coordination link interference and network failures. Our testbed measurements and simulation results show that Angora enables large number of low-latency control paths to run concurrently while providing low latency end-to-end message delivery with high tolerance for radio and rack failures.,http://research.google.com/pubs/archive/43860.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cutting+the+Cord:+a+Robust+Wireless+Facilities+Network+for+Data+Centers+Zhu+Zhou+Zhang+Zhou+Vahdat+Zhao+Zheng,http://research.google.com/pubs/pub43860.html
Efficient Traffic Splitting on Commodity Switches,Proceedings of the 11th ACM International on Conference on emerging Networking Experiments and Technologies (CoNEXT) ACM (2015),2015,Nanxi Kang Monia Ghobadi John Reumann Alexander Shraer Jennifer Rexford,@inproceedings{44273 title = {Efficient Traffic Splitting on Commodity Switches} author = {Nanxi Kang and Monia Ghobadi and John Reumann and Alexander Shraer and Jennifer Rexford} year = 2015 booktitle = {Proceedings of the 11th ACM International on Conference on emerging Networking Experiments and Technologies (CoNEXT)} },Traffic often needs to be split over multiple equivalent backend servers links paths or middleboxes. For example in a load-balancing system switches distribute requests of online services to backend servers. Hash-based approaches like Equal-Cost Multi-Path (ECMP) have low accuracy due to hash collision and incur significant churn during update. In a Software-Defined Network (SDN) the accuracy of traffic splits can be improved by crafting a set of wildcard rules for switches that better match the actual traffic distribution. The drawback of existing SDN-based traffic-splitting solutions is poor scalability as they generate too many rules for small rule-tables on switches. In this paper we propose Niagara an SDN-based traffic-splitting scheme that achieves accurate traffic splits while being extremely efficient in the use of rule-table space available on commodity switches. Niagara uses an incremental update strategy to minimize the traffic churn given an update. Experiments demonstrate that Niagara (1) achieves nearly optimal accuracy using only 1.2%_37% of the rule space of the current state-of-art (2) scales to tens of thousands of services with the constrained rule-table capacity and (3) offers nearly minimum churn.,http://research.google.com/pubs/archive/44273.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Traffic+Splitting+on+Commodity+Switches+Kang+Ghobadi+Reumann+Shraer+Rexford,http://research.google.com/pubs/pub44273.html
Trust transparency & control in inferred user interest models,CHI '14 Extended Abstracts on Human Factors in Computing Systems ACM New York NY USA (2014) pp. 2449-2454,2014,Sebastian Schnorf Martin Ortlieb Nikhil Sharma,@inproceedings{42486 title = {Trust transparency & control in inferred user interest models} author = {Sebastian Schnorf and Martin Ortlieb and Nikhil Sharma} year = 2014 URL = {http://dl.acm.org/ft_gateway.cfm?id=2581141&ftid=1452129&dwn=1&CFID=335406935&CFTOKEN=53022170} booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems} pages = {2449-2454} address = {New York NY USA} },This paper explores the importance of transparency and control to users in the context of inferred user interests. More specifically we illustrate the association between various levels of control the users have on their inferred interests and users' trust in organizations that provide corresponding content. Our results indicate that users value transparency and control very differently. We segment users in two groups one who states to not care about their personal interest model and another group that desires some level of control. We found substantial differences in trust impact between segments depending on actual control option provided.,http://research.google.com/pubs/archive/42486.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trust+transparency+%26+control+in+inferred+user+interest+models+Schnorf+Ortlieb+Sharma,http://research.google.com/pubs/pub42486.html
Statistical verification of probabilistic properties with unbounded until,Proceedings of the 13th Brazilian Symposium on Formal Methods Springer Berlin / Heidelberg (2010) pp. 144-160,2010,Håkan L. S. Younes Edmund M. Clarke Paolo Zuliani,@inproceedings{36556 title = {Statistical verification of probabilistic properties with unbounded until} author = {Håkan L. S. Younes and Edmund M. Clarke and Paolo Zuliani} year = 2010 URL = {http://dx.doi.org/10.1007/978-3-642-19829-8_10} booktitle = {Proceedings of the 13th Brazilian Symposium on Formal Methods} pages = {144--160} address = {Berlin / Heidelberg} },We consider statistical (sampling-based) solution methods for verifying probabilistic properties with unbounded until. Statistical solution methods for probabilistic verification use sample execution trajectories for a system to verify properties with some level of confidence. The main challenge with properties that are expressed using unbounded until is to ensure termination in the face of potentially infinite sample execution trajectories. We describe two alternative solution methods each one with its own merits. The first method relies on reachability analysis and is suitable primarily for large Markov chains where reachability analysis can be performed efficiently using symbolic data structures but for which numerical probability computations are expensive. The second method employs a termination probability and weighted sampling. This method does not rely on any specific structure of the model but error control is more challenging. We show how the choice of termination probability---when applied to Markov chains---is tied to the subdominant eigenvalue of the transition probability matrix which relates it to iterative numerical solution techniques for the same problem.,http://dx.doi.org/10.1007/978-3-642-19829-8_10,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+verification+of+probabilistic+properties+with+unbounded+until+Younes+Clarke+Zuliani,http://research.google.com/pubs/pub36556.html
Traffic Light Mapping and Detection,Proceedings of ICRA 2011,2011,Nathaniel Fairfield Chris Urmson,@inproceedings{37259 title = {Traffic Light Mapping and Detection} author = {Nathaniel Fairfield and Chris Urmson} year = 2011 booktitle = {Proceedings of ICRA 2011} },The outdoor perception problem is a major challenge for driver-assistance and autonomous vehicle systems. While these systems can often employ active sensors such as sonar radar and lidar to perceive their surroundings the state of standard traffic lights can only be perceived visually. By using a prior map a perception system can anticipate and predict the locations of traffic lights and improve detection of the light state. The prior map also encodes the control semantics of the individual lights. This paper presents methods for automatically mapping the three dimensional positions of traffic lights and robustly detecting traffic light state onboard cars with cameras. We have used these methods to map more than four thousand traffic lights and to perform onboard traffic light detection for thousands of drives through intersections.,http://research.google.com/pubs/archive/37259.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Traffic+Light+Mapping+and+Detection+Fairfield+Urmson,http://research.google.com/pubs/pub37259.html
Restricted Transfer learning for Text Categorization,NIPS Workshop (2013) (to appear),2013,Rajhans Samdani Gideon Mann,@inproceedings{41871 title = {Restricted Transfer learning for Text Categorization} author = {Rajhans Samdani and Gideon Mann} year = 2013 URL = {https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxnaWRlb25tYW5ufGd4OjY4MTlhY2Y1YjgxZDk1ZjA} booktitle = {NIPS Workshop} },In practice machine learning systems deal with multiple datasets over time. When the feature spaces between these datasets overlap it is possible to transfer information from one task to another. Typically in transfer learning all labeled data from a source task is saved to be applied to a new target task thereby raising concerns of privacy memory and scaling. To ameliorate such concerns we present a semi-supervised algorithm for text categorization that transfers information across tasks without storing the data of the source task. In particular our technique learns a sparse low-dimensional projection from unlabeled and the source task data. In particular our technique learns low-dimensional sparse word clusters-based features from the source task data and a massive amount of additional unlabeled data. Our algorithm is efﬁcient highly parallelizable and outperforms competitive baselines by up to 9% on several difﬁcult benchmark text categorization tasks.,http://research.google.com/pubs/archive/41871.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Restricted+Transfer+learning+for+Text+Categorization+Samdani+Mann,http://research.google.com/pubs/pub41871.html
MemorySanitizer: fast detector of uninitialized memory use in C++,Proceedings of the 2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO) CGO 2015 San Francisco CA USA pp. 46-55,2015,Evgeniy Stepanov Konstantin Serebryany,@inproceedings{43308 title = {MemorySanitizer: fast detector of uninitialized memory use in C++} author = {Evgeniy Stepanov and Konstantin Serebryany} year = 2015 booktitle = {Proceedings of the 2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)} pages = {46--55} address = {San Francisco CA USA} },This paper presents MemorySanitizer a dynamic tool that detects uses of uninitialized memory in C and C++. The tool is based on compile time instrumentation and relies on bit-precise shadow memory at run-time. Shadow propagation technique is used to avoid false positive reports on copying of uninitialized memory. MemorySanitizer finds bugs at a modest cost of 2.5x in execution time and 2x in memory usage; the tool has an optional origin tracking mode that provides better reports with moderate extra overhead. The reports with origins are more detailed compared to reports from other similar tools; such reports contain names of local variables and the entire history of the uninitialized memory including intermediate stores. In this paper we share our experience in deploying the tool at a large scale and demonstrate the benefits of compile time instrumentation over dynamic binary instrumentation.,http://research.google.com/pubs/archive/43308.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MemorySanitizer:+fast+detector+of+uninitialized+memory+use+in+C%2B%2B+Stepanov+Serebryany,http://research.google.com/pubs/pub43308.html
Easy Does It: More Usable CAPTCHAs,CHI '14 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems ACM 1600 Amphitheatre Pkwy (2014) pp. 2637-2646,2014,Elie Bursztein Angelika Moscicki Celine Fabry Steven Bethard John C. Mitchell Dan Jurafasky,@inproceedings{43463 title = {Easy Does It: More Usable CAPTCHAs} author = {Elie Bursztein and Angelika Moscicki and Celine Fabry and Steven Bethard and John C. Mitchell and Dan Jurafasky} year = 2014 URL = {https://www.elie.net/publication/easy-does-it-more-usable-captchas} booktitle = {CHI '14 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems} pages = {2637--2646} address = {1600 Amphitheatre Pkwy} },Websites present users with puzzles called CAPTCHAs to curb abuse caused by computer algorithms masquerading as people. While CAPTCHAs are generally effective at stopping abuse they might impair website usability if they are not properly designed. In this paper we describe how we designed two new CAPTCHA schemes for Google that focus on maximizing usability. We began by running an evaluation on Amazon Mechanical Turk with over 27000 respondents to test the us- ability of different feature combinations. Then we studied user preferences using Google’s consumer survey infrastructure. Finally drawing on the insights gleaned during those studies we tested our new captcha schemes first on Mechanical Turk and then on a fraction of production traffic. The resulting scheme is now an integral part of our production system and is served to millions of users. Our scheme achieved a 95.3% human accuracy a 6.7% improvement.,http://research.google.com/pubs/archive/43463.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Easy+Does+It:+More+Usable+CAPTCHAs+Bursztein+Moscicki+Fabry+Bethard+Mitchell+Jurafasky,http://research.google.com/pubs/pub43463.html
Capacity of Steganographic Channels,IEEE Transactions on Information Theory vol. 55 (2009) pp. 1775-1792,2009,Jeremiah Harmsen William Pearlman,@article{35245 title = {Capacity of Steganographic Channels} author = {Jeremiah Harmsen and William Pearlman} year = 2009 journal = {IEEE Transactions on Information Theory} pages = {1775-1792} volume = {55} },This work investigates a central problem in steganography that is: How much data can safely be hidden without being detected? To answer this question a formal definition of steganographic capacity is presented. Once this has been defined a general formula for the capacity is developed. The formula is applicable to a very broad spectrum of channels due to the use of an information-spectrum approach. This approach allows for the analysis of arbitrary steganalyzers as well as nonstationary nonergodic encoder and attack channels. After the general formula is presented various simplifications are applied to gain insight into example hiding and detection methodologies. Finally the context and applications of the work are summarized in a general discussion.,http://dx.doi.org/10.1109/TIT.2009.2012991,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Capacity+of+Steganographic+Channels+Harmsen+Pearlman,http://research.google.com/pubs/pub35245.html
Practical Large-Scale Latency Estimation,Computer Networks vol. 52 (2008) pp. 1343-1364,2008,Michal Szymaniak David L. Presotto Guillaume Pierre Maarten van Steen,@article{36959 title = {Practical Large-Scale Latency Estimation} author = {Michal Szymaniak and David L. Presotto and Guillaume Pierre and Maarten van Steen} year = 2008 URL = {http://dx.doi.org/10.1016/j.comnet.2007.11.022} journal = {Computer Networks} pages = {1343-1364} volume = {52} },We present the implementation of a large-scale latency estimation system based on GNP and incorporated into the Google content delivery network. Our implementation does not rely on active participation of Web clients and carefully controls the overhead incurred by latency measurements using a scalable centralized scheduler. It also requires only a small number of CDN modifications which makes it attractive for any CDN interested in large-scale latency estimation. We investigate the issue of coordinate stability over time and show that coordinates drift away from their initial values with time so that 25% of node coordinates become inaccurate by more than 33 milliseconds after one week. However daily recomputations make 75% of the coordinates stay within 6 milliseconds of their initial values. Furthermore we demonstrate that using coordinates to decide on client-to-replica redirection leads to selecting replicas closest in term of measured latency in 86% of all cases. In another 10% of all cases clients are redirected to replicas offering latencies that are at most two times longer than optimal. Finally collecting a huge volume of latency data and using clustering techniques enable us to estimate latencies between globally distributed Internet hosts that have not participated in our measurements at all. The results are sufficiently promising that Google may offer a public interface to the latency estimates in the future.,http://research.google.com/pubs/archive/36959.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Practical+Large-Scale+Latency+Estimation+Szymaniak+Presotto+Pierre+Steen,http://research.google.com/pubs/pub36959.html
Investigating prefix propagation through active BGP probing,Microprocessors and Microsystems vol. 31 no. 7 (2007) pp. 460-474,2007,Lorenzo Colitti Giuseppe Di Battista Maurizio Patrignani Maurizio Pizzonia Massimo Rimondini,@article{32974 title = {Investigating prefix propagation through active BGP probing} author = {Lorenzo Colitti and Giuseppe Di Battista and Maurizio Patrignani and Maurizio Pizzonia and Massimo Rimondini} year = 2007 journal = {Microprocessors and Microsystems} pages = {460--474} volume = {31 no. 7} },To devise effective network engineering strategies and to assess the quality of upstream providers network operators would greatly benefit from the knowledge of which Internet paths might be traversed by the traffic flows entering their networks in the case of network faults or when traffic engineering measures are used. However current methodologies do not provide this information. This paper presents methodologies to discover alternate paths that might be selected in the presence of network faults or different routing policies and to deduce the routing policies of other operators. The techniques are validated through extensive experimentation on the Internet.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Investigating+prefix+propagation+through+active+BGP+probing+Colitti+Battista+Patrignani+Pizzonia+Rimondini,http://research.google.com/pubs/pub32974.html
Imagers as sensors: Correlating plant CO2 uptake with digital visible-light imagery,Data Management for Sensor Networks (2007),2007,Josh Hyman Eric Graham Mark Hansen Deborah Estrin,@inproceedings{33255 title = {Imagers as sensors: Correlating plant CO2 uptake with digital visible-light imagery} author = {Josh Hyman and Eric Graham and Mark Hansen and Deborah Estrin} year = 2007 URL = {http://www.joshhyman.com/papers/hyman07mosscam.pdf} booktitle = {Data Management for Sensor Networks} },There exist many natural phenomena where direct measurement is either impossible or extremely invasive. To obtain approximate measurements of these phenomena we can build prediction models based on other sensing modalities such as features extracted from data collected by an imager. These models are derived from controlled experiments performed under laboratory conditions and can then be applied to the associated event in nature. In this paper we explore various different methods for generating such models and discuss their accuracy robustness and computational complexity. Given sufficiently computationally simple models we can eventually push their computation down towards the sensor nodes themselves to reduce the amount of data required to both flow through the network and be stored in a database. The addition of these models turn in-situ imagers into powerful biological sensors and image databases into useful records of biological activity.,http://www.joshhyman.com/papers/hyman07mosscam.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Imagers+as+sensors:+Correlating+plant+CO2+uptake+with+digital+visible-light+imagery+Hyman+Graham+Hansen+Estrin,http://research.google.com/pubs/pub33255.html
Nowcasting with Google Trends,String Processing and Information Retrieval Springer (2013) pp. 4,2013,Yossi Matias,@inproceedings{41692 title = {Nowcasting with Google Trends} author = {Yossi Matias} year = 2013 booktitle = {String Processing and Information Retrieval} pages = {4} },Since launching Google Trends we have seen extensive interest in what can be learned from search trends. A plethora of studies have shown how to use search trends data for effective nowcasting in diverse areas such as health finance economics politics and more. We give an overview of Google Trends and Nowcasting highlighting some exciting Big Data challenges including large scale engineering effective data analysis and domain specific considerations.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Nowcasting+with+Google+Trends+Matias,http://research.google.com/pubs/pub41692.html
Hearing the Shape of the Ising Model with a Programmable Superconducting-Flux Annealer,Scientific Reports vol. 4 (2014),2014,Walter Vinci Klas Markström Sergio Boixo Aidan Roy Federico M. Spedalieri Paul A. Warburton Simone Severini,@article{42872 title = {Hearing the Shape of the Ising Model with a Programmable Superconducting-Flux Annealer} author = {Walter Vinci and Klas Markström and Sergio Boixo and Aidan Roy and Federico M. Spedalieri and Paul A. Warburton and Simone Severini} year = 2014 URL = {http://www.nature.com/srep/2014/140716/srep05703/full/srep05703.html} journal = {Scientific Reports} volume = {4} },Two objects can be distinguished if they have different measurable properties. Thus distinguishability depends on the Physics of the objects. In considering graphs we revisit the Ising model as a framework to define physically meaningful spectral invariants. In this context we introduce a family of refinements of the classical spectrum and consider the quantum partition function. We demonstrate that the energy spectrum of the quantum Ising Hamiltonian is a stronger invariant than the classical one without refinements. For the purpose of implementing the related physical systems we perform experiments on a programmable annealer with superconducting flux technology. Departing from the paradigm of adiabatic computation we take advantage of a noisy evolution of the device to generate statistics of low energy states. The graphs considered in the experiments have the same classical partition functions but different quantum spectra. The data obtained from the annealer distinguish non-isomorphic graphs via information contained in the classical refinements of the functions but not via the differences in the quantum spectra.,http://www.nature.com/srep/2014/140716/srep05703/full/srep05703.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hearing+the+Shape+of+the+Ising+Model+with+a+Programmable+Superconducting-Flux+Annealer+Vinci+Markstr%C3%B6m+Boixo+Roy+Spedalieri+Warburton+Severini,http://research.google.com/pubs/pub42872.html
traits.js: Robust Object Composition and High-integrity Objects for ECMAScript 5,Plastic 2011: International Workshop on Programming Language And Systems Technologies for Internet Clients ACM,2011,Tom Van Cutsem Mark S. Miller,@inproceedings{37485 title = {traits.js: Robust Object Composition and High-integrity Objects for ECMAScript 5} author = {Tom Van Cutsem and Mark S. Miller} year = 2011 booktitle = {Plastic 2011: International Workshop on Programming Language And Systems Technologies for Internet Clients} },This paper introduces traits.js a small portable trait composition library for Javascript. Traits are a more robust alternative to multiple inheritance and enable object composition and reuse. traits.js is motivated by two goals: ﬁrst it is an experiment in using and extending Javascript’s recently added meta-level object description format. By reusing this standard description format traits.js can be made more interoperable with similar libraries and even with built-in primitives. Second traits.js makes it convenient to create “high-integrity” objects whose integrity cannot be violated by clients an important property in the context of interaction between mutually suspicious scripts,http://research.google.com/pubs/archive/37485.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=traits.js:+Robust+Object+Composition+and+High-integrity+Objects+for+ECMAScript+5+Cutsem+Miller,http://research.google.com/pubs/pub37485.html
Ad Click Prediction: a View from the Trenches,Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) (2013),2013,H. Brendan McMahan Gary Holt D. Sculley Michael Young Dietmar Ebner Julian Grady Lan Nie Todd Phillips Eugene Davydov Daniel Golovin Sharat Chikkerur Dan Liu Martin Wattenberg Arnar Mar Hrafnkelsson Tom Boulos Jeremy Kubica,@inproceedings{41159 title = {Ad Click Prediction: a View from the Trenches} author = {H. Brendan McMahan and Gary Holt and D. Sculley and Michael Young and Dietmar Ebner and Julian Grady and Lan Nie and Todd Phillips and Eugene Davydov and Daniel Golovin and Sharat Chikkerur and Dan Liu and Martin Wattenberg and Arnar Mar Hrafnkelsson and Tom Boulos and Jeremy Kubica} year = 2013 booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)} },Predicting ad click--through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings methods for assessing and visualizing performance practical methods for providing confidence estimates for predicted probabilities calibration methods and methods for automated management of features. Finally we also detail several directions that did not turn out to be beneficial for us despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.,http://research.google.com/pubs/archive/41159.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Ad+Click+Prediction:+a+View+from+the+Trenches+McMahan+Holt+Sculley+Young+Ebner+Grady+Nie+Phillips+Davydov+Golovin+Chikkerur+Liu+Wattenberg+Hrafnkelsson+Boulos+Kubica,http://research.google.com/pubs/pub41159.html
Norovirus Disease Surveillance Using Google Internet Query Share Data,Clinical Infectious Diseases (2012),2012,Rishi Desai Aron J. Hall Benjamin A. Lopman Yair Shimshoni Marcus Rennick Niv Efron Yossi Matias Manish M. Patel Umesh D. Parashar,@article{38318 title = {Norovirus Disease Surveillance Using Google Internet Query Share Data} author = {Rishi Desai and Aron J. Hall and Benjamin A. Lopman and Yair Shimshoni and Marcus Rennick and Niv Efron and Yossi Matias and Manish M. Patel and Umesh D. Parashar} year = 2012 URL = {http://cid.oxfordjournals.org/content/early/2012/08/07/cid.cis579} journal = {Clinical Infectious Diseases} },Google Internet query share (IQS) data for gastroenteritis-related search terms correlated strongly with contemporaneous national (R2 = 0.70) and regional (R2 = 0.74) norovirus surveillance data in the United States. IQS data may facilitate rapid identification of norovirus season onset elevated peak activity and potential emergence of novel strains.,http://cid.oxfordjournals.org/content/early/2012/08/07/cid.cis579,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Norovirus+Disease+Surveillance+Using+Google+Internet+Query+Share+Data+Desai+Hall+Lopman+Shimshoni+Rennick+Efron+Matias+Patel+Parashar,http://research.google.com/pubs/pub38318.html
Sparse Spatiotemporal Coding for Activity Recognition,Brown University (2010),2010,Thomas Dean Greg Corrado Rich Washington,@techreport{36274 title = {Sparse Spatiotemporal Coding for Activity Recognition} author = {Thomas Dean and Greg Corrado and Rich Washington} year = 2010 URL = {http://www.cs.brown.edu/research/pubs/techreports/reports/CS-10-02.html} institution = {Brown University} },We present a new approach to learning sparse spatiotemporal features and demonstrate the utility of the approach by applying the resulting sparse codes to the problem of activity recognition. Learning features that discriminate among human activities in video is difficult in part because the stable space-time events that reliably characterize the relevant motions are rare. To overcome this problem we adopt a multi-stage approach to activity recognition. In the initial preprocessing stage we first whiten and apply local contrast normalization to each frame of the video. We then apply an additional set of filters to identify and extract salient space-time volumes that exhibit smooth periodic motion. We collect a large corpus of these space-time volumes as training data for the unsupervised learning of a sparse over-complete basis using a variant of the two-phase analysis-synthesis algorithm of Olshausen and Field [1997]. We treat the synthesis phase which consists of reconstructing the input as sparse a mostly coefficient zero and most importantly the time required for reconstruction in subsequent use production we adapted existing algorithms to exploit potential parallelism through the use of readily-available SIMD hardware. To obtain better codes we developed a new approach to learning sparse spatiotemporal codes in which the number of basis vectors their orientations velocities and the size of their receptive fields change over the duration of unsupervised training. The algorithm starts with a relatively small initial basis with minimal temporal extent. This initial basis is obtained through conventional sparse coding techniques and is expanded over time by recursively constructing a new basis consisting of basis vectors with larger temporal extent that proportionally conserve regions of previously trained weights. These proportionally conserved weights are combined with the result of adjusting newly added weights to represent a greater range of primitive motion features. The size of the current basis is determined probabilistically by sampling from existing basis vectors according to their activation on the training set. The resulting algorithm produces bases consisting of filters that are bandpass spatially oriented and temporally diverse in terms of their transformations and velocities. We demonstrate the utility of our approach by using it to recognize human activity in video.,http://research.google.com/pubs/archive/36274.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sparse+Spatiotemporal+Coding+for+Activity+Recognition+Dean+Corrado+Washington,http://research.google.com/pubs/pub36274.html
Applications and Extensions of Alloy: Past Present and Future,Mathematical Structures in Computer Science vol. 23 (2013) pp. 915-933,2013,Emina Torlak Mana Taghdiri Greg Dennis Joseph Near,@article{37040 title = {Applications and Extensions of Alloy: Past Present and Future} author = {Emina Torlak and Mana Taghdiri and Greg Dennis and Joseph Near} year = 2013 URL = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=8951488} journal = {Mathematical Structures in Computer Science} pages = {915-933} volume = {23} },Alloy is a declarative language for lightweight modelling and analysis of software. The core of the language is based on first-order relational logic which offers an attractive balance between analysability and expressiveness. The logic is expressive enough to capture the intricacies of real systems but is also simple enough to support fully automated analysis with the Alloy Analyzer. The Analyzer is built on a SAT-based constraint solver and provides automated simulation checking and debugging of Alloy specifications. Because of its automated analysis and expressive logic Alloy has been applied in a wide variety of domains. These applications have motivated a number of extensions both to the Alloy language and to its SAT-based analysis. This paper provides an overview of Alloy in the context of its three largest application domains lightweight modelling bounded code verification and test-case generation and three recent application-driven extensions an imperative extension to the language a compiler to executable code and a proof-capable analyser based on SMT.,http://research.google.com/pubs/archive/37040.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Applications+and+Extensions+of+Alloy:+Past+Present+and+Future+Torlak+Taghdiri+Dennis+Near,http://research.google.com/pubs/pub37040.html
Google Fusion Tables: Web-Centered Data Management and Collaboration,Proceedings of the ACM SIGMOD conference ACM (2010),2010,Hector Gonzalez Alon Halevy Christian Jensen Anno Langen Jayant Madhavan Rebecca Shapley Warren Shen Jonathan Goldberg-Kidon,@inproceedings{36257 title = {Google Fusion Tables: Web-Centered Data Management and Collaboration} author = {Hector Gonzalez and Alon Halevy and Christian Jensen and Anno Langen and Jayant Madhavan and Rebecca Shapley and Warren Shen and Jonathan Goldberg-Kidon} year = 2010 URL = {http://www.cs.washington.edu/homes/alon/files/sigmod10.pdf} booktitle = {Proceedings of the ACM SIGMOD conference} },It has long been observed that database management systems focus on traditional business applications and that few people use a database management system outside their workplace. Many have wondered what it will take to enable the use of data management technology by a broader class of users and for a much wider range of applications. Google Fusion Tables represents an initial answer to the question of how data management functionality that focused on enabling new users and applications would look in today's computing environment. This paper characterizes such users and applications and highlights the resulting principles such as seamless Web integration emphasis on ease of use and incentives for data sharing that underlie the design of Fusion Tables. We describe key novel features such as the support for data acquisition collaboration visualization and web-publishing.,http://www.cs.washington.edu/homes/alon/files/sigmod10.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+Fusion+Tables:+Web-Centered+Data+Management+and+Collaboration+Gonzalez+Halevy+Jensen+Langen+Madhavan+Shapley+Shen+Goldberg-Kidon,http://research.google.com/pubs/pub36257.html
Sentence Compression by Deletion with LSTMs,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP'15),2015,Katja Filippova Enrique Alfonseca Carlos Colmenares Lukasz Kaiser Oriol Vinyals,@inproceedings{43852 title = {Sentence Compression by Deletion with LSTMs} author = {Katja Filippova and Enrique Alfonseca and Carlos Colmenares and Lukasz Kaiser and Oriol Vinyals} year = 2015 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP'15)} },We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones corresponding to token deletion decisions. We demonstrate that even the most basic version of the system which is given no syntactic information (no PoS or NE tags or dependencies) or desired compression length performs surprisingly well: around 30% of the compressions from a large test set could be regenerated. We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTM-based model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.,http://research.google.com/pubs/archive/43852.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sentence+Compression+by+Deletion+with+LSTMs+Filippova+Alfonseca+Colmenares+Kaiser+Vinyals,http://research.google.com/pubs/pub43852.html
Datacenter-scale Computing,IEEE Micro vol. 30 (2010) pp. 6-7,2010,Luiz André Barroso Parthasarathy Ranganathan,@article{36626 title = {Datacenter-scale Computing} author = {Luiz André Barroso and Parthasarathy Ranganathan} year = 2010 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/MM.2010.63} note = {Special issue of the IEEE Micro Magazine} journal = {IEEE Micro} pages = {6-7} volume = {30} },Although the field of datacenter computing is arguably still in its relative infancy a sizable body of work from both academia and industry is already available and some consistent technological trends have begun to emerge. This special issue presents a small sample of the work underway by researchers and professionals in this new field. The selection of articles presented reflects the key role that hardware-software codesign plays in the development of effective datacenter-scale computer systems.,http://www.computer.org/portal/web/csdl/doi/10.1109/MM.2010.63,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Datacenter-scale+Computing+Barroso+Ranganathan,http://research.google.com/pubs/pub36626.html
Entanglement in a Quantum Annealing Processor,Physical Review X vol. 4 (2014) pp. 021041,2014,T. Lanting A. J. Przybysz A. Yu. Smirnov F._M. Spedalieri M. H. Amin A. J. Berkley R. Harris F. Altomare S. Boixo P. Bunyk N. Dickson C. Enderud J. P. Hilton E. Hoskinson M. W. Johnson E. Ladizinsky N. Ladizinsky R. Neufeld T. Oh I. Perminov C. Rich M. C. Thom E. Tolkacheva S. Uchaikin A. B. Wilson G. Rose,@article{42556 title = {Entanglement in a Quantum Annealing Processor} author = {T. Lanting and A. J. Przybysz and A. Yu. Smirnov and F._M. Spedalieri and M. H. Amin and A. J. Berkley and R. Harris and F. Altomare and S. Boixo and P. Bunyk and N. Dickson and C. Enderud and J. P. Hilton and E. Hoskinson and M. W. Johnson and E. Ladizinsky and N. Ladizinsky and R. Neufeld and T. Oh and I. Perminov and C. Rich and M. C. Thom and E. Tolkacheva and S. Uchaikin and A. B. Wilson and G. Rose} year = 2014 URL = {http://dx.doi.org/10.1103/PhysRevX.4.021041} journal = {Physical Review X} pages = {021041} volume = {4} },Entanglement lies at the core of quantum algorithms designed to solve problems that are intractable by classical approaches. One such algorithm quantum annealing (QA) provides a promising path to a practical quantum processor. We have built a series of architecturally scalable QA processors consisting of networks of manufactured interacting spins (qubits). Here we use qubit tunneling spectroscopy to measure the energy eigenspectrum of two- and eight-qubit systems within one such processor demonstrating quantum coherence in these systems. We present experimental evidence that during a critical portion of QA the qubits become entangled and entanglement persists even as these systems reach equilibrium with a thermal environment. Our results provide an encouraging sign that QA is a viable technology for large-scale quantum computing.,http://research.google.com/pubs/archive/42556.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Entanglement+in+a+Quantum+Annealing+Processor+Lanting+Przybysz+Smirnov+Spedalieri+Amin+Berkley+Harris+Altomare+Boixo+Bunyk+Dickson+Enderud+Hilton+Hoskinson+Johnson+Ladizinsky+Ladizinsky+Neufeld+Oh+Perminov+Rich+Thom+Tolkacheva+Uchaikin+Wilson+Rose,http://research.google.com/pubs/pub42556.html
Hybrid Page Layout Analysis via Tab-Stop Detection,Proceedings of the 10th international conference on document analysis and recognition IEEE (2009),2009,Ray Smith,@inproceedings{35094 title = {Hybrid Page Layout Analysis via Tab-Stop Detection} author = {Ray Smith} year = 2009 URL = {http://www.cvc.uab.es/icdar2009/papers/3725a241.pdf} booktitle = {Proceedings of the 10th international conference on document analysis and recognition} },A new hybrid page layout analysis algorithm is proposed which uses bottom-up methods to form an initial data-type hypothesis and locate the tab-stops that were used when the page was formatted. The detected tab-stops are used to deduce the column layout of the page. The column layout is then applied in a top-down manner to impose structure and reading-order on the detected regions. The complete C++ source code implementation is available as part of the Tesseract open source OCR engine at http://code.google.com/p/tesseract-ocr.,http://research.google.com/pubs/archive/35094.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hybrid+Page+Layout+Analysis+via+Tab-Stop+Detection+Smith,http://research.google.com/pubs/pub35094.html
Accelerator Compiler for the VENICE Vector Processor,FPGA ACM (2012),2012,Zhiduo Liu Aaron Severance Guy G.F. Lemieux Satnam Singh,@inproceedings{37671 title = {Accelerator Compiler for the VENICE Vector Processor} author = {Zhiduo Liu and Aaron Severance and Guy G.F. Lemieux and Satnam Singh} year = 2012 booktitle = {FPGA} },This paper describes the compiler design for VENICE a new soft vector processor (SVP). The compiler is a new back-end target for Microsoft Accelerator a high-level data parallel library for C++ and C#. This allows us to automatically compile high-level programs into VENICE assembly code thus avoiding the process of writing assembly code used by previous SVPs. Experimental results show the compiler can generate scalable parallel code with execution times that are comparable to hand-written VENICE assembly code. On data-parallel applications VENICE at 100MHz on an Altera DE3 platform runs at speeds comparable to one core of a 3.5GHz Intel Xeon W3690 processor beating it in performance on four of six benchmarks by up to 3.2%.,http://research.google.com/pubs/archive/37671.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Accelerator+Compiler+for+the+VENICE+Vector+Processor+Liu+Severance+Lemieux+Singh,http://research.google.com/pubs/pub37671.html
Learning Query-Specific Distance Functions for Large-Scale Web Image Search,IEEE Transactions on Multimedia vol. 15 (2013) pp. 2022-2034,2013,Yushi Jing Michele Covell David Tsai James M. Rehg,@article{41900 title = {Learning Query-Specific Distance Functions for Large-Scale Web Image Search} author = {Yushi Jing and Michele Covell and David Tsai and James M. Rehg} year = 2013 URL = {https://www.google.com/search?q=jing+covell+tsai+rehg+learning+query-specific+distance+functions+for+large-scale+web+image+search} journal = {IEEE Transactions on Multimedia} pages = {2022--2034} volume = {15} },"Current Google image search adopts a hybrid search approach in which a text-based query (e.g. ""Paris landmarks"") is used to retrieve a set of relevant images which are then refined by the user (e.g. by re-ranking the retrieved images based on similarity to a selected example). We conjecture that given such hybrid image search engines learning per-query distance functions over image features can improve the estimation of image similarity. We proposed scalable solutions to learning query-specific distance functions by 1) adopting a simple large-margin learning framework 2) using the query-logs of a text-based image search engine to train distance functions used in content-based systems. We evaluate the feasibility and efficacy of our proposed system through comprehensive human evaluation and compare the results with the state-of-the-art image distance function used by Google image search.",https://www.google.com/search?q=jing+covell+tsai+rehg+learning+query-specific+distance+functions+for+large-scale+web+image+search,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Query-Specific+Distance+Functions+for+Large-Scale+Web+Image+Search+Jing+Covell+Cai+Rehg,http://research.google.com/pubs/pub41900.html
The Dangers of Composing Anonymous Channels,Information Hiding - 14th International Conference IH 2012 Revised Selected Papers Springer Lecture notes in Computer Science (2013) pp. 191-206,2013,Emilia Kasper George Danezis,@inproceedings{39961 title = {The Dangers of Composing Anonymous Channels} author = {Emilia Kasper and George Danezis} year = 2013 booktitle = {Information Hiding - 14th International Conference IH 2012 Revised Selected Papers} pages = {191-206} },We present traffic analyses of two anonymous communications schemes that build on the classic Crowds/Hordes protocols. The AJSS10 [1] scheme combines multiple Crowds-like forward channels with a Hordes reply channel in an attempt to offer robustness in a mobile environment. We show that the resulting scheme fails to guarantee the claimed k-anonymity and is in fact more vulnerable to malicious peers than Hordes while suffering from higher latency. Similarly the RWS11 [15] scheme invokes multiple instances of Crowds to provide receiver anonymity. We demonstrate that the sender anonymity of the scheme is susceptible to a variant of the predecessor attack [21] while receiver anonymity is fully compromised with an active attack. We conclude that the heuristic security claims of AJSS10 and RWS11 do not hold and argue that composition of multiple anonymity channels can in fact weaken overall security. In contrast we provide a rigorous security analysis of Hordes under the same threat model and reflect on design principles for future anonymous channels to make them amenable to such security analysis.,http://research.google.com/pubs/archive/39961.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Dangers+of+Composing+Anonymous+Channels+Kasper+Danezis,http://research.google.com/pubs/pub39961.html
Smoothed marginal distribution constraints for language modeling,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL) (2013) pp. 43-52,2013,Brian Roark Cyril Allauzen Michael Riley,@inproceedings{41345 title = {Smoothed marginal distribution constraints for language modeling} author = {Brian Roark and Cyril Allauzen and Michael Riley} year = 2013 URL = {http://www.aclweb.org/anthology-new/P/P13/P13-1005.pdf} booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)} pages = {43--52} },We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions along the lines of well-known Kneser-Ney smoothing. Unlike Kneser-Ney our approach is designed to be applied to any given smoothed backoff model including models that have already been heavily pruned. As a result the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al. 2007; Chelba et al. 2010) while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff n-gram models and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.,http://research.google.com/pubs/archive/41345.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Smoothed+marginal+distribution+constraints+for+language+modeling+Roark+Allauzen+Riley,http://research.google.com/pubs/pub41345.html
Balancing Usability and Security in a Video CAPTCHA,Proceedings of the 5th Symposium on Usable Privacy and Security (SOUPS '09) ACM Press (2009),2009,Kurt Alfred Kluever Richard Zanibbi,@inproceedings{35116 title = {Balancing Usability and Security in a Video CAPTCHA} author = {Kurt Alfred Kluever and Richard Zanibbi} year = 2009 URL = {http://www.kloover.com/2009/07/13/balancing-usability-and-security-in-a-video-captcha/} booktitle = {Proceedings of the 5th Symposium on Usable Privacy and Security (SOUPS '09)} },We present a technique for using a content-based video labeling task as a CAPTCHA. Our video CAPTCHAs are generated from YouTube videos which contain labels (tags) supplied by the person that uploaded the video. They are graded using a video's tags as well as tags from related videos. In a user study involving 184 participants we were able to increase the average human success rate on our video CAPTCHA from roughly 70% to 90% while keeping the average success rate of a tag frequency-based attack fixed at around 13%. Through a different parameterization of the challenge generation and grading algorithms we were able to reduce the success rate of the same attack to 2% while still increasing the human success rate from 70% to 75%. The usability and security of our video CAPTCHA appears to be comparable to existing CAPTCHAs and a majority of participants (60%) indicated that they found the video CAPTCHAs more enjoyable than traditional CAPTCHAs in which distorted text must be transcribed.,http://research.google.com/pubs/archive/35116.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Balancing+Usability+and+Security+in+a+Video+CAPTCHA+Kluever+Zanibbi,http://research.google.com/pubs/pub35116.html
ReFr: An Open-Source Reranker Framework,Interspeech 2013 pp. 756-758,2013,Daniel M. Bikel Keith B. Hall,@inproceedings{41531 title = {ReFr: An Open-Source Reranker Framework} author = {Daniel M. Bikel and Keith B. Hall} year = 2013 booktitle = {Interspeech 2013} pages = {756--758} },ReFr (http://refr.googlecode.com) is a software architecture for specifying training and using reranking models which take the n-best output of some existing system and produce new scores for each of the n hypotheses that potentially induce a different ranking ideally yielding better results than the original system. The Reranker Framework has some special support for building discriminative language models but can be applied to any reranking problem. The framework is designed with parallelism and scalability in mind being able to run on any Hadoop cluster out of the box. While extremely efﬁcient ReFr is also quite ﬂexible allowing researchers to explore a wide variety of features and learning methods. ReFr has been used for building state-of-the-art discriminative LM’s for both speech recognition and machine translation systems.,http://research.google.com/pubs/archive/41531.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ReFr:+An+Open-Source+Reranker+Framework+Bikel+Hall,http://research.google.com/pubs/pub41531.html
Concurrency-aware compiler optimizations for hardware description languages,ACM Transactions on Design Automation of Electronic Systems (TODAES) vol. Volume 18 Issue 1 (2013) 10:1-10:16,2013,Harikumar Somakumar,@article{41415 title = {Concurrency-aware compiler optimizations for hardware description languages} author = {Harikumar Somakumar} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2390201} journal = {ACM Transactions on Design Automation of Electronic Systems (TODAES)} pages = {10:1-10:16} volume = {Volume 18 Issue 1} },In this article we discuss the application of compiler technology for eliminating redundant computation in hardware simulation. We discuss how concurrency in hardware description languages (HDLs) presents opportunities for expression reuse across different threads. While accounting for discrete event simulation semantics we extend the data flow analysis framework to concurrent threads. In this process we introduce a rewriting scheme named ∂VF and a graph representation to model sensitivity relationships among threads. An algorithm for identifying common sub-expressions as applied to HDLs is presented. Related issues such as scheduling correctness are also considered.,http://research.google.com/pubs/archive/41415.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Concurrency-aware+compiler+optimizations+for+hardware+description+languages+Somakumar,http://research.google.com/pubs/pub41415.html
Using Actors to Implement Sequential Simulations,University of Saskatchewan Saskatchewan Canada (2015),2015,Ryan Harrison,@misc{43437 title = {Using Actors to Implement Sequential Simulations} author = {Ryan Harrison} year = 2015 URL = {http://etd.usask.ca/bitstream/id/8404/HarrisonThesis-final.pdf} note = {Master's Thesis} },This thesis investigates using an approach based on the Actors paradigm for implementing a discrete event simulation system and comparing the results with more traditional approaches. The goal of this work is to determine if using Actors for sequential programming is viable. If Actors are viable for this type of programming then it follows that they would be usable for general programming. One potential advantage of using Actors instead of traditional paradigms for general programming would be the elimination of a distinction between designing for a sequential environment and a concurrent/distributed one. Using Actors for general programming may also allow for a single implementation that can be deployed on both single core and multiple core systems. Most of the existing discussions about the Actors model focus on its strengths in distributed environments and its ability to scale with the amount of available computing resources. The chosen system for implementation is intentionally sequential to allow for examination of the behaviour of existing Actors implementations where managing concurrency complexity is not the primary task. Multiple implementations of the simulation system were built using different languages (C++ Erlang and Java) and different paradigms including traditional ones and Actors. These different implementations were compared quantitatively based on their execution time memory usage and code complexity. The analysis of these comparisons indicates that for certain existing development environments Erlang/OTP following the Actors paradigm produces a comparable or better implementation than traditional paradigms. Further research is suggested to solidify the validity of the results presented in this research and to extend their applicability.,http://research.google.com/pubs/archive/43437.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Actors+to+Implement+Sequential+Simulations+Harrison,http://research.google.com/pubs/pub43437.html
Timing properties of gene expression responses to environmental changes,J. Computational Biology vol. 9 (2009),2009,Gal Chechik Daphne Koller,@article{34936 title = {Timing properties of gene expression responses to environmental changes} author = {Gal Chechik and Daphne Koller} year = 2009 URL = {http://ai.stanford.edu/~gal/Papers/Chechik_JCB2009_long.pdf} note = {http://ai.stanford.edu/~gal/pubs.html} journal = {J. Computational Biology} volume = {9} },http://www.liebertonline.com/doi/pdfplus/10.1089/cmb.2008.13TT,http://research.google.com/pubs/archive/34936.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Timing+properties+of+gene+expression+responses+to+environmental+changes+Chechik+Koller,http://research.google.com/pubs/pub34936.html
Learning to Extract Local Events from the Web,SIGIR 2015,2015,John Foley Michael Bendersky Vanja Josifovski,@inproceedings{43796 title = {Learning to Extract Local Events from the Web} author = {John Foley and Michael Bendersky and Vanja Josifovski} year = 2015 booktitle = {SIGIR 2015} },The goal of this work is extraction and retrieval of local events from web pages. Examples of local events include small venue concerts theater performances garage sales movie screenings etc. We collect these events in the form of retrievable calendar entries that include structured information about event name date time and location. Between existing information extraction techniques and the availability of information on social media and semantic web technologies there are numerous ways to collect commercial high-profile events. However most extraction techniques require domain-level supervision which is not attainable at web scale. Similarly while the adoption of the semantic web has grown there will always be organizations without the resources or the expertise to add machine-readable annotations to their pages. Therefore our approach bootstraps these explicit annotations to massively scale up local event extraction. We propose a novel event extraction model that uses distant supervision to assign scores to individual event fields (event name date time and location) and a structural algorithm to optimally group these fields into event records. Our model integrates information from both the entire source document and its relevant sub-regions and is highly scalable. We evaluate our extraction model on all 700 million documents in a large publicly available web corpus ClueWeb12. Using the 217000 unique explicitly annotated events as distant supervision we are able to double recall with 85% precision and quadruple it with 65% precision with no additional human supervision. We also show that our model can be bootstrapped for a fully supervised approach which can further improve the precision by 30%. In addition we evaluate the geographic coverage of the extracted events. We find that there is a significant increase in the geo-diversity of extracted events compared to existing explicit annotations while maintaining high precision levels,http://research.google.com/pubs/archive/43796.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Extract+Local+Events+from+the+Web+Foley+Bendersky+Josifovski,http://research.google.com/pubs/pub43796.html
Gesture On: Always-On Touch Gestures for Fast Mobile Access from Device Standby Mode,CHI 2015: ACM Conference on Human Factors in Computing Systems ACM pp. 3355-3364,2015,Hao Lu Yang Li,@inproceedings{44263 title = {Gesture On: Always-On Touch Gestures for Fast Mobile Access from Device Standby Mode} author = {Hao Lu and Yang Li} year = 2015 URL = {http://dl.acm.org/citation.cfm?id=2702610} booktitle = {CHI 2015: ACM Conference on Human Factors in Computing Systems} pages = {3355-3364} },Contributes a system that overrides the mobile platform kernel behavior to enable touchscreen gesture shortcuts in standby mode. A user can issue a gesture on the touchscreen before the screen is even turned on.,http://dl.acm.org/citation.cfm?id=2702610,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Gesture+On:+Always-On+Touch+Gestures+for+Fast+Mobile+Access+from+Device+Standby+Mode+Lu+Li,http://research.google.com/pubs/pub44263.html
Pedestrian Detection with a Large-Field-Of-View Deep Network,Proceedings of ICRA 2015,2015,Anelia Angelova Alex Krizhevsky Vincent Vanhoucke,@inproceedings{43849 title = {Pedestrian Detection with a Large-Field-Of-View Deep Network} author = {Anelia Angelova and Alex Krizhevsky and Vincent Vanhoucke} year = 2015 booktitle = {Proceedings of ICRA 2015} },Pedestrian detection is of crucial importance to autonomous driving applications. Methods based on deep learning have shown significant improvements in accuracy which makes them particularly suitable for applications such as pedestrian detection where reducing miss rate is very important. Although they are accurate their runtime has been at best in seconds per image which makes them not practical for onboard applications. We present here a Large-Field-Of-View (LFOV) deep network for pedestrian detection that can achieve high accuracy and is designed to make deep networks work faster for detection problems. The idea of the proposed Large-Field-of-View deep network is to learn to make classification decisions simultaneously and accurately at multiple locations. The LFOV network processes larger image areas at much faster speeds than typical deep networks have been able to do and can intrinsically reuse computations. Our pedestrian detection solution which is a combination of a LFOV network and a standard deep network works at 280 ms per image on GPU and achieves 35.85 average miss rate on the Caltech Pedestrian Detection Benchmark.,http://research.google.com/pubs/archive/43849.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Pedestrian+Detection+with+a+Large-Field-Of-View+Deep+Network+Angelova+Krizhevsky+Vanhoucke,http://research.google.com/pubs/pub43849.html
Scalable Inference in Hierarchical Generative Models,Proceedings of the Ninth International Symposium on Artificial Intelligence and Mathematics (2006),2006,Thomas Dean,@inproceedings{34770 title = {Scalable Inference in Hierarchical Generative Models} author = {Thomas Dean} year = 2006 booktitle = {Proceedings of the Ninth International Symposium on Artificial Intelligence and Mathematics} },We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive ?eld corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable persistent representations as we ascend the hierarchy. The receptive ?elds of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.,http://research.google.com/pubs/archive/34770.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Inference+in+Hierarchical+Generative+Models+Dean,http://research.google.com/pubs/pub34770.html
Image Reconstruction in the Gigavision Camera,ICCV workshop OMNIVIS 2009,2009,Feng Yang Luciano Sbaiz Edoardo Charbon Sabine Susstrunk Martin Vetterli,@inproceedings{35578 title = {Image Reconstruction in the Gigavision Camera} author = {Feng Yang and Luciano Sbaiz and Edoardo Charbon and Sabine Susstrunk and Martin Vetterli} year = 2009 booktitle = {ICCV workshop OMNIVIS 2009} },the gigavision camera. The main feature of this camera is that the pixels have a binary response. The response function of a gigavision sensor is non-linear and similar to a logarithmic function which makes the camera suitable for high dynamic range imaging. Since the sensor can detect a single photon the camera is very sensitive and can be used for night vision and astronomical imaging. One important aspect of the gigavision camera is how to estimate the light intensity through binary observations. We model the light intensity field as 2D piecewise constant and use Maximum Penalized Likelihood Estimation (MPLE) to recover it. Dynamic programming is used to solve the optimization problem. Due to the complex computation of dynamic programming greedy algorithm and pruning quadtrees are proposed. They show acceptable reconstruction performance with low computational complexity. Experimental results with synthesized images and real images taken by a single-photon avalanche diode (SPAD) camera are given.,http://research.google.com/pubs/archive/35578.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Image+Reconstruction+in+the+Gigavision+Camera+Yang+Sbaiz+Charbon+Susstrunk+Vetterli,http://research.google.com/pubs/pub35578.html
The atoms of neural computation,Science vol. 346 (2014) pp. 551-552,2014,Gary Marcus Adam Marblestone Tom Dean,@article{43141 title = {The atoms of neural computation} author = {Gary Marcus and Adam Marblestone and Tom Dean} year = 2014 note = {Computational Neuroscience} journal = {Science} pages = {551-552} volume = {346} },The human cerebral cortex is central to a wide array of cognitive functions from vision to language reasoning decision-making and motor control. Yet nearly a century after the neuroanatomical organization of the cortex was first defined its basic logic remains unknown. One hypothesis is that cortical neurons form a single massively repeated “canonical” circuit characterized as a kind of a “nonlinear spatiotemporal filter with adaptive properties” (1). In this classic view it was “assumed that these…properties are identical for all neocortical areas.” Nearly four decades later there is still no consensus about whether such a canonical circuit exists either in terms of its anatomical basis or its function. Likewise there is little evidence that such uniform architectures can capture the diversity of cortical function in simple mammals let alone characteristically human processes such as language and abstract thinking (2). Analogous software implementations in artificial intelligence (e.g. deep learning networks) have proven effective in certain pattern classification tasks such as speech and image recognition but likewise have made little inroads in areas such as reasoning and natural language understanding. Is the search for a single canonical cortical circuit misguided?,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+atoms+of+neural+computation+Marcus+Marblestone+Dean,http://research.google.com/pubs/pub43141.html
Overcoming the Lack of Parallel Data in Sentence Compression,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP '13) pp. 1481-1491,2013,Katja Filippova Yasemin Altun,@inproceedings{41393 title = {Overcoming the Lack of Parallel Data in Sentence Compression} author = {Katja Filippova and Yasemin Altun} year = 2013 note = {(Click on the Abstract link to get access to the described dataset)} booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP '13)} pages = {1481--1491} },A subset of the described data (10000 sentence & extracted headlines pairs with source URL and annotations) is available for download.,http://research.google.com/pubs/archive/41393.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Overcoming+the+Lack+of+Parallel+Data+in+Sentence+Compression+Filippova+Altun,http://research.google.com/pubs/pub41393.html
Nearest Neighbor Search in Google Correlate,Google (2013),2013,Dan Vanderkam Rob Schonberger Henry Rowley Sanjiv Kumar,@techreport{41694 title = {Nearest Neighbor Search in Google Correlate} author = {Dan Vanderkam and Rob Schonberger and Henry Rowley and Sanjiv Kumar} year = 2013 URL = {http://www.google.com/trends/correlate/nnsearch.pdf} institution = {Google} },"This paper presents the algorithms which power Google Correlate a tool which finds web search terms whose popularity over time best matches a user-provided time series. Correlate was developed to generalize the query-based modeling techniques pioneered by Google Flu Trends and make them available to end users. Correlate searches across millions of candidate query time series to find the best matches returning results in less than 200 milliseconds. Its feature set and requirements present unique challenges for Approximate Nearest Neighbor (ANN) search techniques. In this paper we present Asymmetric Hashing (AH) the technique used by Correlate and show how it can be adapted to the specific needs of the product. We then develop experiments to test the throughput and recall of Asymmetric Hashing as compared to a brute-force search. For ""full"" search vectors we achieve a 10x speedup over brute force search while maintaining 97% recall. For search vectors which contain holdout periods we achieve a 4x speedup over brute force search also with 97% recall.",http://research.google.com/pubs/archive/41694.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Nearest+Neighbor+Search+in+Google+Correlate+Vanderkam+Schonberger+Rowley+Kumar,http://research.google.com/pubs/pub41694.html
Im2Calories: towards an automated mobile vision food diary,ICCV (2015),2015,Austin Myers Nick Johnston Vivek Rathod Anoop Korattikara Alex Gorban Nathan Silberman Sergio Guadarrama George Papandreou Jonathan Huang Kevin Murphy,@inproceedings{44321 title = {Im2Calories: towards an automated mobile vision food diary} author = {Austin Myers and Nick Johnston and Vivek Rathod and Anoop Korattikara and Alex Gorban and Nathan Silberman and Sergio Guadarrama and George Papandreou and Jonathan Huang and Kevin Murphy} year = 2015 URL = {http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Meyers_Im2Calories_Towards_an_ICCV_2015_paper.pdf} booktitle = {ICCV} },We present a system which can recognize the contents of your meal from a single image and then predict its nutritional contents such as calories. The simplest version assumes that the user is eating at a restaurant for which we know the menu. In this case we can collect images offline to train a multi-label classifier. At run time we apply the classifier (running on your phone) to predict which foods are present in your meal and we lookup the corresponding nutritional facts. We apply this method to a new dataset of images from 23 different restaurants using a CNN-based classifier significantly outperforming previous work. The more challenging setting works outside of restaurants. In this case we need to estimate the size of the foods as well as their labels. This requires solving segmentation and depth / volume estimation from a single image. We present CNN-based approaches to these problems with promising preliminary results.,http://research.google.com/pubs/archive/44321.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Im2Calories:+towards+an+automated+mobile+vision+food+diary+Myers+Johnston+Rathod+Korattikara+Gorban+Silberman+Guadarrama+Papandreou+Huang+Murphy,http://research.google.com/pubs/pub44321.html
Bayesian Language Model Interpolation for Mobile Speech Input,Interspeech 2011 pp. 1429-1432,2011,Cyril Allauzen Michael Riley,@inproceedings{37567 title = {Bayesian Language Model Interpolation for Mobile Speech Input} author = {Cyril Allauzen and Michael Riley} year = 2011 booktitle = {Interspeech 2011} pages = {1429-1432} },This paper explores various static interpolation methods for approximating a single dynamically-interpolated language model used for a variety of recognition tasks on the Google Android platform. The goal is to ﬁnd the statically-interpolated ﬁrstpass LM that best reduces search errors in a two-pass system or that even allows eliminating the more complex dynamic second pass entirely. Static interpolation weights that are uniform prior-weighted and the maximum likelihood maximum a posteriori and Bayesian solutions are considered. Analysis argues and recognition experiments on Android test data show that a Bayesian interpolation approach performs best.,http://research.google.com/pubs/archive/37567.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bayesian+Language+Model+Interpolation+for+Mobile+Speech+Input+Allauzen+Riley,http://research.google.com/pubs/pub37567.html
MEASURING NOISE CORRELATION FOR IMPROVED VIDEO DENOISING,IEEE International Conference on Image Processing IEEE 1600 Amphitheatre Parkway (2012),2012,Anil Kokaram Damien Kelly Hugh Denman Andrew Crawford,@inproceedings{40567 title = {MEASURING NOISE CORRELATION FOR IMPROVED VIDEO DENOISING} author = {Anil Kokaram and Damien Kelly and Hugh Denman and Andrew Crawford} year = 2012 booktitle = {IEEE International Conference on Image Processing} address = {1600 Amphitheatre Parkway} },The vast majority of previous work in noise reduction for visual media has assumed uncorrelated white noise sources. In practice this is almost always violated by real media. Film grain noise is never white and this paper highlights that the same applies to almost all consumer video content. We therefore present an algorithm for measuring the spatial and temporal spectral density of noise in archived video content be it consumer digital camera or film orginated. As an example of how this information can be used for video denoising the spectral density is then used for spatio-temporal noise reduction in the Fourier frequency domain. Results show improved performance for noise reduction in an easily pipelined system.,http://research.google.com/pubs/archive/40567.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MEASURING+NOISE+CORRELATION+FOR+IMPROVED+VIDEO+DENOISING+Kokaram+Kelly+Denman+Crawford,http://research.google.com/pubs/pub40567.html
Mixin' up the ML module system,Transactions on Programming Languages and Systems vol. 35 (1) (2013),2013,Andreas Rossberg Derek Dreyer,@article{43982 title = {Mixin' up the ML module system} author = {Andreas Rossberg and Derek Dreyer} year = 2013 URL = {https://www.mpi-sws.org/~rossberg/mixml/} journal = {Transactions on Programming Languages and Systems} volume = {35 (1)} },ML modules provide hierarchical namespace management as well as fine-grained control over the propagation of type information but they do not allow modules to be broken up into mutually recursive separately compilable components. Mixin modules facilitate recursive linking of separately compiled components but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition translucent ML-style data abstraction and mixin-style recursive linking. Moreover the design of MixML is clean and minimalist; it emphasizes how all the salient semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs with mixin composition playing a central role. We provide a declarative type system for MixML including two important extensions: higher-order modules and modules as first-class values. We also present a sound and complete three-pass type checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG – namely a polymorphic lambda calculus with single-assignment references and recursive type generativity – which employs a linear type and kind system to track definedness of term and type imports.,http://research.google.com/pubs/archive/43982.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mixin'+up+the+ML+module+system+Rossberg+Dreyer,http://research.google.com/pubs/pub43982.html
Corporate learning at scale: Lessons from a large online course at Google,Learning at Scale (2014),2014,Arthur Asuncion Jac de Haan Mehryar Mohri Kayur Patel Afshin Rostamizadeh Umar Syed Lauren Wong,@inproceedings{42855 title = {Corporate learning at scale: Lessons from a large online course at Google} author = {Arthur Asuncion and Jac de Haan and Mehryar Mohri and Kayur Patel and Afshin Rostamizadeh and Umar Syed and Lauren Wong} year = 2014 booktitle = {Learning at Scale} },Google Research recently tested a massive online class model for an internal engineering education program with machine learning as the topic that blended theoretical concepts and Google-specific software tool tutorials. The goal of this training was to foster engineering capacity to leverage machine learning tools in future products. The course was delivered both synchronously and asynchronously and students had the choice between studying independently or participating with a group. Since all students are company employees unlike most publicly offered MOOCs we can continue to measure the students’ behavioral change long after the course is complete. This paper describes the course outlines the available data set and presents directions for analysis.,http://research.google.com/pubs/archive/42855.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Corporate+learning+at+scale:+Lessons+from+a+large+online+course+at+Google+Asuncion+Haan+Mohri+Patel+Rostamizadeh+Syed+Wong,http://research.google.com/pubs/pub42855.html
Learning Binary Codes for High Dimensional Data Using Bilinear Projections,IEEE Computer Vision and Pattern Recognition (2013),2013,Yunchao Gong Sanjiv Kumar Henry Rowley Svetlana Lazebnik,@inproceedings{40753 title = {Learning Binary Codes for High Dimensional Data Using Bilinear Projections} author = {Yunchao Gong and Sanjiv Kumar and Henry Rowley and Svetlana Lazebnik} year = 2013 booktitle = {IEEE Computer Vision and Pattern Recognition} },Recent advances in visual recognition indicate that to achieve good retrieval and classiﬁcation accuracy on large scale datasets like ImageNet extremely high-dimensional visual descriptors e.g. Fisher Vectors are needed. We present a novel method for converting such descriptors to compact similarity-preserving binary codes that exploits their natural matrix structure to reduce their dimensionality using compact bilinear projections instead of a single large projection matrix. This method achieves comparable retrieval and classiﬁcation accuracy to the original descriptors and to the state-of-the-art Product Quantization approach while having orders of magnitude faster code generation time and smaller memory footprint.,http://research.google.com/pubs/archive/40753.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Binary+Codes+for+High+Dimensional+Data+Using+Bilinear+Projections+Gong+Kumar+Rowley+Lazebnik,http://research.google.com/pubs/pub40753.html
Swapsies on the Internet: First Steps towards Reasoning about Risk and Trust in an Open World,Tenth Workshop on Programming Languages and Analysis for Security (PLAS 2015) ACM,2015,Sophia Drossopoulou James Noble Mark S. Miller,@inproceedings{43808 title = {Swapsies on the Internet: First Steps towards Reasoning about Risk and Trust in an Open World} author = {Sophia Drossopoulou and James Noble and Mark S. Miller} year = 2015 booktitle = {Tenth Workshop on Programming Languages and Analysis for Security (PLAS 2015)} },Contemporary open systems use components developed by many different parties linked together dynamically in unforeseen constellations. Code needs to live up to strict security specifications: it has to ensure the correct functioning of its objects when they collaborate with external objects which may be malicious. In this paper we propose specifications that model risk and trust in such open systems. We specify Miller Van Cutsem and Tulloh’s escrow exchange example and discuss the meaning of such a specification. We argue informally that the code satisfies its specification.,http://research.google.com/pubs/archive/43808.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Swapsies+on+the+Internet:+First+Steps+towards+Reasoning+about+Risk+and+Trust+in+an+Open+World+Drossopoulou+Noble+Miller,http://research.google.com/pubs/pub43808.html
On the Difficulty of Nearest Neighbor Search,International Conference on Machine Learning (ICML) (2012),2012,Junfeng He Sanjiv Kumar Shih-Fu Chang,@inproceedings{38140 title = {On the Difficulty of Nearest Neighbor Search} author = {Junfeng He and Sanjiv Kumar and Shih-Fu Chang} year = 2012 URL = {http://www.sanjivk.com/RelativeContrast_ICML12.pdf} booktitle = {International Conference on Machine Learning (ICML)} },Fast approximate nearest neighbor (NN) search in large databases is becoming popular and several powerful learning-based formulations have been proposed recently. However not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality sparsity and database size simultaneously in arbitrary normed metric spaces. Moreover we present a theoretical analysis to show how relative contrast affects the complexity of Local Sensitive Hashing a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally we show that most of the previous works measuring meaningfulness or difficulty of NN search can be derived as special asymptotic cases for dense vectors of the proposed measure.,http://research.google.com/pubs/archive/38140.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+Difficulty+of+Nearest+Neighbor+Search+He+Kumar+Chang,http://research.google.com/pubs/pub38140.html
Being an On-Call Engineer: A Google SRE Perspective,;login: vol. 40 (2015) pp. 43-47,2015,Andrea Spadaccini Kavita Guliani,@article{44813 title = {Being an On-Call Engineer: A Google SRE Perspective} author = {Andrea Spadaccini and Kavita Guliani} year = 2015 URL = {https://www.usenix.org/publications/login/oct15/spadaccini} journal = {;login:} pages = {43--47} volume = {40} },Being on-call is a critical duty that many operations and engineering teams must undertake in order to keep their services reliable and available. However there are several pitfalls in the organization of on-call rotations and responsibilities that can lead to serious consequences for the services and for the teams if not avoided. We provide the primary tenets of the approach to on-call that Google’s Site Reliability Engineers have developed over years and explain how that approach has led to reliable services and sustainable workload over time.,http://research.google.com/pubs/archive/44813.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Being+an+On-Call+Engineer:+A+Google+SRE+Perspective+Spadaccini+Guliani,http://research.google.com/pubs/pub44813.html
Scalable Community Discovery from Multi-Faceted Graphs,2015 IEEE International Conference on Big Data IEEE 445 Hoes Lane Piscataway NJ 08854-4141 USA (to appear),2015,Ahmed Metwally Jia-Yu Pan Minh Doan Christos Faloutsos,@inproceedings{43972 title = {Scalable Community Discovery from Multi-Faceted Graphs} author = {Ahmed Metwally and Jia-Yu Pan and Minh Doan and Christos Faloutsos} year = 2015 booktitle = {2015 IEEE International Conference on Big Data} address = {445 Hoes Lane Piscataway NJ 08854-4141 USA} },A multi-faceted graph defines several facets on a set of nodes. Each facet is a set of edges that represent the relationships between the nodes in a specific context. Mining multi-faceted graphs have several applications including finding fraudster rings that launch advertising traffic fraud attacks tracking IP addresses of botnets over time analyzing interactions on social networks and co-authorship of scientific papers. We propose NeSim a distributed efficient clustering algorithm that does soft clustering on individual facets. We also propose optimizations to further improve the scalability the efficiency and the clusters quality. We employ general purpose graph-clustering algorithms in a novel way to discover communities across facets. Due to the qualities of NeSim we employ it as a backbone in the distributed MuFace algorithm which discovers multi-faceted communities. We evaluate the proposed algorithms on several real and synthetic datasets where NeSim is shown to be superior to MCL JP and AP the well-established clustering algorithms. We also report the success stories of MuFace in finding advertisement click rings.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Community+Discovery+from+Multi-Faceted+Graphs+Metwally+Pan+Doan+Faloutsos,http://research.google.com/pubs/pub43972.html
The Anatomy of Smartphone Unlocking: A Field Study of Android Lock Screens,Proceedings of the 34th Annual ACM Conference on Human Factors in Computing Systems (CHI'16) ACM New York NY USA (2016) (to appear),2016,Marian Harbach Alexander De Luca Serge Egelman,@inproceedings{44675 title = {The Anatomy of Smartphone Unlocking: A Field Study of Android Lock Screens} author = {Marian Harbach and Alexander De Luca and Serge Egelman} year = 2016 booktitle = {Proceedings of the 34th Annual ACM Conference on Human Factors in Computing Systems (CHI'16)} address = {New York NY USA} },"To prevent unauthorized parties from accessing data stored on their smartphones users have the option of enabling a ""lock screen"" that requires a secret code (e.g. PIN drawing a pattern or biometric) to gain access to their devices. We present a detailed analysis of the smartphone locking mechanisms currently available to billions of smartphone users worldwide. Through a month-long field study we logged events from a panel of users with instrumented smartphones (N=134). We are able to show how existing lock screen mechanisms provide users with distinct tradeoffs between usability (unlocking speed vs. unlocking frequency) and security. We find that PIN users take longer to enter their codes but commit fewer errors than pattern users who unlock more frequently and are very prone to errors. Overall PIN and pattern users spent the same amount of time unlocking their devices on average. Additionally unlock performance seemed unaffected for users enabling the stealth mode for patterns. Based on our results we identify areas where device locking mechanisms can be improved to result in fewer human errors - increasing usability - while also maintaining security.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Anatomy+of+Smartphone+Unlocking:+A+Field+Study+of+Android+Lock+Screens+Harbach+De+Luca+Egelman,http://research.google.com/pubs/pub44675.html
A Scalable Gibbs Sampler for Probabilistic Entity Linking,Advances in Information Retrieval (ECIR 2014) Springer International Publishing pp. 335-346,2014,Neil Houlsby Massimiliano Ciaramita,@inproceedings{42453 title = {A Scalable Gibbs Sampler for Probabilistic Entity Linking} author = {Neil Houlsby and Massimiliano Ciaramita} year = 2014 booktitle = {Advances in Information Retrieval (ECIR 2014)} pages = {335-346} },Entity linking involves labeling phrases in text with their referent entities such as Wikipedia or Freebase entries. This task is challenging due to the large number of possible entities in the millions and heavy-tailed mention ambiguity. We formulate the problem in terms of probabilistic inference within a topic model where each topic is associated with a Wikipedia article. To deal with the large number of topics we propose a novel efficient Gibbs sampling scheme which can also incorporate side information such as the Wikipedia graph. This conceptually simple probabilistic approach achieves state-of-the-art performance in entity-linking on the Aida-CoNLL dataset.,http://research.google.com/pubs/archive/42453.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Scalable+Gibbs+Sampler+for+Probabilistic+Entity+Linking+Houlsby+Ciaramita,http://research.google.com/pubs/pub42453.html
Triple Wollaston-prism complete-Stokes imaging polarimeter,Optics Letters vol. 38 (2013) pp. 3874-3877,2013,John D. Perreault,@article{41642 title = {Triple Wollaston-prism complete-Stokes imaging polarimeter} author = {John D. Perreault} year = 2013 URL = {http://www.opticsinfobase.org/ol/abstract.cfm?URI=ol-38-19-3874&origin=search} journal = {Optics Letters} pages = {3874-3877} volume = {38} },Imaging polarimetry is emerging as a powerful tool for remote sensing in space science Earth science biology defense national security and industry. Polarimetry provides complementary information about a scene in the visible and infrared wavelengths. For example surface texture material composition and molecular structure will affect the polarization state of reflected scattered or emitted light. We demonstrate an imaging polarimeter design that uses three Wollaston prisms addressing several technical challenges associated with moving remote-sensing platforms. This compact design has no moving polarization elements and separates the polarization components in the pupil (or Fourier) plane analogous to the way a grating spectrometer works. In addition this concept enables simultaneous characterization of unpolarized linear and circular components of optical polarization. The results from a visible-wavelength prototype of this imaging polarimeter are presented demonstrating remote sensitivity to material properties. This work enables new remote sensing capabilities and provides a viable design concept for extensions into infrared wavelengths.,http://research.google.com/pubs/archive/41642.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Triple+Wollaston-prism+complete-Stokes+imaging+polarimeter+Perreault,http://research.google.com/pubs/pub41642.html
Approximation Schemes for Capacitated Geometric Network Design,ICALP 2011 Rynek G_ówny 12 (to appear),2011,Anna Adamaszek Artur Czumaj Andrzej Lingas Jakub Onufry Wojtaszczyk,@inproceedings{37374 title = {Approximation Schemes for Capacitated Geometric Network Design} author = {Anna Adamaszek and Artur Czumaj and Andrzej Lingas and Jakub Onufry Wojtaszczyk} year = 2011 booktitle = {ICALP 2011} address = {Rynek G_ówny 12} },We study a capacitated network design problem in geometric setting. We assume that the input consists of an integral link capacity k and two sets of points on a plane sources and sinks each source/sink having an associated integral demand (amount of ﬂow to be shipped from/to). The capacitated geometric network design problem is to construct a minimum-length network N that allows to route the requested ﬂow from sources to sinks such that each link in N has capacity k; the ﬂow is splittable and parallel links are allowed in N. The capacitated geometric network design problem generalizes among others the geometric Steiner tree problem and as such it is NP-hard. We show that if the demands are polynomially bounded and the link capacity k is not too large the single-sink capacitated geometric network design problem admits a polynomial-time approximation scheme. If the capacity is arbitrarily large then we design a quasi-polynomial time approximation scheme for the capacitated geometric network design problem allowing for arbitrary number of sinks. Our results rely on a derivation of an upper bound on the number of vertices different from sources and sinks (the so called Steiner vertices) in an optimal network. The bound is polynomial in the total demand of the sources.,http://research.google.com/pubs/archive/37374.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Approximation+Schemes+for+Capacitated+Geometric+Network+Design+Adamaszek+Czumaj+Lingas+Wojtaszczyk,http://research.google.com/pubs/pub37374.html
Handling Packet Loss in WebRTC,International Conference on Image Processing (ICIP 2013) IEEE pp. 1860-1864,2013,Stefan Holmer Mikhal Shemer Marco Paniconi,@inproceedings{41611 title = {Handling Packet Loss in WebRTC} author = {Stefan Holmer and Mikhal Shemer and Marco Paniconi} year = 2013 booktitle = {International Conference on Image Processing (ICIP 2013)} pages = {1860-1864} },WebRTC is an open-source real-time interactive audio and video communication framework. This paper discusses some of the mechanisms utilized in WebRTC to handle packet losses in the video communication path. Various system details are discussed and an adaptive hybrid NACK/FEC method with temporal layers is presented. Results are shown to quantify how the method controls the quality trade-offs for real-time video communication.,http://research.google.com/pubs/archive/41611.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Handling+Packet+Loss+in+WebRTC+Holmer+Shemer+Paniconi,http://research.google.com/pubs/pub41611.html
Variable Rate Image Compression with Recurrent Neural Networks,International Conference on Learning Representations (2016),2016,George Toderici Sean M. O'Malley Sung Jin Hwang Damien Vincent David Minnen Shumeet Baluja Michele Covell Rahul Sukthankar,@inproceedings{44844 title = {Variable Rate Image Compression with Recurrent Neural Networks} author = {George Toderici and Sean M. O'Malley and Sung Jin Hwang and Damien Vincent and David Minnen and Shumeet Baluja and Michele Covell and Rahul Sukthankar} year = 2016 URL = {http://arxiv.org/abs/1511.06085} booktitle = {International Conference on Learning Representations} },A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors it has become the norm for modern graphics-heavy websites to transmit low-resolution low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus as any byte savings will significantly enhance the experience of mobile device users. Toward this end we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image) regardless of input image dimensions and the desired compression rate; (2) our networks are progressive meaning that the more bits are sent the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32_32 thumbnails our LSTM-based approaches provide better visual quality than (headerless) JPEG JPEG2000 and WebP with a storage size that is reduced by 10% or more.,http://arxiv.org/abs/1511.06085,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Variable+Rate+Image+Compression+with+Recurrent+Neural+Networks+Toderici+O'Malley+Hwang+Vincent+Minnen+Baluja+Covell+Sukthankar,http://research.google.com/pubs/pub44844.html
Efficient Runtime Service Discovery and Consumption with Hyperlinked RESTdesc,The 7th International Conference on Next Generation Web Services Practices (NWeSP 2011) Salamanca Spain,2011,Ruben Verborgh Thomas Steiner Davy Van Deursen Rik Van de Walle Joaquim Gabarro,@inproceedings{37427 title = {Efficient Runtime Service Discovery and Consumption with Hyperlinked RESTdesc} author = {Ruben Verborgh and Thomas Steiner and Davy Van Deursen and Rik Van de Walle and Joaquim Gabarro} year = 2011 URL = {http://www.lsi.upc.edu/~tsteiner/papers/2011/efficient-runtime-service-discovery-nwesp2011.pdf} booktitle = {The 7th International Conference on Next Generation Web Services Practices (NWeSP 2011)} address = {Salamanca Spain} },Hyperlinks and forms let humans navigate with ease through websites they have never seen before. In contrast automated agents can only perform preprogrammed actions on Web services reducing their generality and restricting their usefulness to a specialized domain. Many of the employed services call themselves RESTful although they neglect the hypermedia constraint as defined by Roy T. Fielding stating that the application state should be driven by hypertext. This lack of link usage on the Web of services severely limits agents in what they can do while connectedness forms a primary feature of the human Web. An urgent need for more intelligent agents becomes apparent and in this paper we demonstrate how the conjunction of functional service descriptions and hypermedia links leads to advanced interactive agent behavior. We propose a new mode for our previously introduced semantic service description format RESTdesc providing the mechanisms for agents to consume Web services based on links similar to human browsing strategies. We illustrate the potential of these descriptions by a use case that shows the enhanced capabilities they offer to automated agents and explain how this is vital for the future Web.,http://research.google.com/pubs/archive/37427.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Runtime+Service+Discovery+and+Consumption+with+Hyperlinked+RESTdesc+Verborgh+Steiner+Van+Deursen+Van+de+Walle+Gabarro,http://research.google.com/pubs/pub37427.html
LSH Banding for Large-Scale Retrieval with Memory and Recall Constraints,International Conference on Acoustics Speech and Signal Processing IEEE (2009),2009,Michele Covell Shumeet Baluja,@inproceedings{35244 title = {LSH Banding for Large-Scale Retrieval with Memory and Recall Constraints} author = {Michele Covell and Shumeet Baluja} year = 2009 booktitle = {International Conference on Acoustics Speech and Signal Processing} },Locality Sensitive Hashing (LSH) is widely used for efficient retrieval of candidate matches in very large audio video and image systems. However extremely large reference databases necessitate a guaranteed limit on the memory used by the table lookup itself no matter how the entries crowd different parts of the signature space a guarantee that LSH does not give. In this paper we provide such guaranteed limits primarily through the design of the LSH bands. When combined with data-adaptive bin splitting (needed on only 0.04% of the occupied bins) this approach provides the required guarantee on memory usage. At the same time it avoids the reduced recall that more extensive use of bin splitting would give.,http://research.google.com/pubs/archive/35244.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=LSH+Banding+for+Large-Scale+Retrieval+with+Memory+and+Recall+Constraints+Covell+Baluja,http://research.google.com/pubs/pub35244.html
Visual Vibrometry: Estimating Material Properties from Small Motion in Video,IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2015),2015,Abe Davis Katherine L. Bouman Justin G. Chen Michael Rubinstein Fredo Durand William T. Freeman,@inproceedings{43842 title = {Visual Vibrometry: Estimating Material Properties from Small Motion in Video} author = {Abe Davis and Katherine L. Bouman and Justin G. Chen and Michael Rubinstein and Fredo Durand and William T. Freeman} year = 2015 URL = {http://www.visualvibrometry.com/} booktitle = {IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)} },The estimation of material properties is important for scene understanding with many applications in vision robotics and structural engineering. This paper connects fundamentals of vibration mechanics with computer vision techniques in order to infer material properties from small often imperceptible motion in video. Objects tend to vibrate in a set of preferred modes. The shapes and frequencies of these modes depend on the structure and material properties of an object. Focusing on the case where geometry is known or fixed we show how information about an object’s modes of vibration can be extracted from video and used to make inferences about that object’s material properties. We demonstrate our approach by estimating material properties for a variety of rods and fabrics by passively observing their motion in high-speed and regular frame-rate video.,http://research.google.com/pubs/archive/43842.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Visual+Vibrometry:+Estimating+Material+Properties+from+Small+Motion+in+Video+Davis+Bouman+Chen+Rubinstein+Durand+Freeman,http://research.google.com/pubs/pub43842.html
Web Survey Methodology,Sage London (2015) pp. 344,2015,Mario Callegaro Katja Lozar Manfreda Vasja Vehovar,@book{43811 title = {Web Survey Methodology} author = {Mario Callegaro and Katja Lozar Manfreda and Vasja Vehovar} year = 2015 URL = {http://www.sagepub.com/books/Book235868} pages = {344} address = {London} },Web Survey Methodology guides the reader through the past fifteen years of research in web survey methodology. It both provides practical guidance on the latest techniques for collecting valid and reliable data and offers a comprehensive overview of research issues. Core topics from preparation to questionnaire design recruitment testing to analysis and survey software are all covered in a systematic and insightful way. The reader will be exposed to key concepts and key findings in the literature covering measurement non-response adjustments paradata and cost issues. The book also discusses the hottest research topics in survey research today such as internet panels virtual interviewing mobile surveys and the integration with passive measurements e-social sciences mixed modes and business intelligence. The book is intended for students practitioners and researchers in fields such as survey and market research psychological research official statistics and customer satisfaction research. REVIEWS Comprehensive and thoughtful! Those two words beautifully describe this terrific book. Internet surveys will be at the centre of survey research for many decades to come and this book is a must-read handbook for anyone serious about doing online surveys well or using data from such surveys. No stone is left unturned - the authors address every essential topic and do so with a remarkable command of the big picture and the subtleties involved. Readers will walk away with a clear understanding of the many challenges inherent in conducting online studies and with an appropriate sense of optimism about the promise of the methodology and how best to implement it. Jon Krosnick Frederic O. Glover Professor in Humanities and Social Sciences Stanford University This is an excellent academic standard book that every serious market researcher should own and consult. The authors have compiled an immense amount of useful and well-referenced information about every aspect of web surveys creating an invaluable resource. Ray Poynter Managing Director The Future Place,http://www.sagepub.com/books/Book235868,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Survey+Methodology+Callegaro+Manfreda+Vehovar,http://research.google.com/pubs/pub43811.html
What you should know about R,Marketing Insights (2015) pp. 12-13,2015,Chris Chapman,@article{44183 title = {What you should know about R} author = {Chris Chapman} year = 2015 journal = {Marketing Insights} pages = {12-13} },A primer on the industry’s open-source statistical analysis language.,http://research.google.com/pubs/archive/44183.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=What+you+should+know+about+R+Chapman,http://research.google.com/pubs/pub44183.html
Symmetric Splitting in the General Theory of Stable Models,In proc. Twenty-first International Joint Conference on Artificial Intelligence (IJCAI '09) (2009) pp. 797-803,2009,Paolo Ferraris Joohyung Lee Vladimir Lifschitz Ravi Palla,@inproceedings{35521 title = {Symmetric Splitting in the General Theory of Stable Models} author = {Paolo Ferraris and Joohyung Lee and Vladimir Lifschitz and Ravi Palla} year = 2009 URL = {http://ijcai.org/papers09/Papers/IJCAI09-137.pdf} booktitle = {In proc. Twenty-first International Joint Conference on Artificial Intelligence (IJCAI '09)} pages = {797-803} },Splitting a logic program allows us to reduce the task of computing its stable models to similar tasks for smaller programs. This idea is extended here to the general theory of stable models that replaces traditional logic programs by arbitrary first-order sentences and distinguishes between intensional and extensional predicates. We discuss two kinds of splitting: a set of intensional predicates can be split into subsets and a formula can be split into its conjunctive terms.,http://research.google.com/pubs/archive/35521.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Symmetric+Splitting+in+the+General+Theory+of+Stable+Models+Ferraris+Lee+Lifschitz+Palla,http://research.google.com/pubs/pub35521.html
Vanity or Privacy? Social Media as a Facilitator of Privacy and Trust,CSCW Workshop: Reconciling Privacy with Social Media (2012),2012,Jessica Staddon,@inproceedings{37673 title = {Vanity or Privacy? Social Media as a Facilitator of Privacy and Trust} author = {Jessica Staddon} year = 2012 booktitle = {CSCW Workshop: Reconciling Privacy with Social Media} },In this position paper we argue that social media provides valuable support for the perception of one’s self and others and in doing so supports privacy. In addition we suggest that engagement which reflects a certain degree of trust can be facilitated by social information. We support our arguments with results from a recent privacy survey and a study of social annotations in search.,http://research.google.com/pubs/archive/37673.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Vanity+or+Privacy%3F+Social+Media+as+a+Facilitator+of+Privacy+and+Trust+Staddon,http://research.google.com/pubs/pub37673.html
Shadow Removal for Aerial Imagery by Information Theoretic Intrinsic Image Analysis,International Conference on Computational Photography IEEE (2012),2012,Vivek Kwatra Mei Han Shengyang Dai,@inproceedings{37743 title = {Shadow Removal for Aerial Imagery by Information Theoretic Intrinsic Image Analysis} author = {Vivek Kwatra and Mei Han and Shengyang Dai} year = 2012 booktitle = {International Conference on Computational Photography} },We present a novel technique for shadow removal based on an information theoretic approach to intrinsic image analysis. Our key observation is that any illumination change in the scene tends to increase the entropy of observed texture intensities. Similarly the presence of texture in the scene increases the entropy of the illumination function. Consequently we formulate the separation of an image into texture and illumination components as minimization of entropies of each component. We employ a non-parametric kernel-based quadratic entropy formulation and present an efficient multi-scale iterative optimization algorithm for minimization of the resulting energy functional. Our technique may be employed either fully automatically using a proposed learning based method for automatic initialization or alternatively with small amount of user interaction. As we demonstrate our method is particularly suitable for aerial images which consist of either distinctive texture patterns e.g. building facades or soft shadows with large diffuse regions e.g. cloud shadows.,http://research.google.com/pubs/archive/37743.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Shadow+Removal+for+Aerial+Imagery+by+Information+Theoretic+Intrinsic+Image+Analysis+Kwatra+Han+Dai,http://research.google.com/pubs/pub37743.html
Estimating reach curves from one data point,Google Inc. (2014) pp. 1-7,2014,Georg M. Goerg,@techreport{43218 title = {Estimating reach curves from one data point} author = {Georg M. Goerg} year = 2014 institution = {Google Inc.} },Reach curves arise in advertising and media analysis as they relate the number of content impressions to the number of people who have seen it. This is especially important for measuring the effectiveness of an ad on TV or websites. For a mathematical and data-driven analysis it would be very useful to know the entire reach curve; advertisers however often only know its last data point i.e. the total number of impressions and the total reach. In this work I present a new method to estimate the entire curve using only this last data point. Furthermore analytic derivations reveal a surprisingly simple yet insightful relationship between marginal cost per reach average cost per impression and frequency. Thus advertisers can estimate the cost of an additional reach point by just knowing their total number of impressions reach and cost. A comparison of the proposed one-data point method to two competing regression models on TV reach curve data shows that the proposed methodology performs only slightly poorer than regression fits to a collection of several points along the curve.,http://research.google.com/pubs/archive/43218.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Estimating+reach+curves+from+one+data+point+Goerg,http://research.google.com/pubs/pub43218.html
What It Would Really Take to Reverse Climate Change,IEEE Spectrum December 2014 IEEE 3 Park Ave 17th floor New York NY 10016-5997 pp. 30-35,2014,Ross Koningstein David K Fork,@incollection{43326 title = {What It Would Really Take to Reverse Climate Change} author = {Ross Koningstein and David K Fork} year = 2014 URL = {http://spectrum.ieee.org/energy/renewables/what-it-would-really-take-to-reverse-climate-change} note = {Many thanks to our editor Eliza Strickland} booktitle = {IEEE Spectrum December 2014} pages = {30--35} address = {3 Park Ave 17th floor New York NY 10016-5997} },What two Googlers learned from a failed attempt to find the renewable energy source of tomorrow.,http://research.google.com/pubs/archive/43326.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=What+It+Would+Really+Take+to+Reverse+Climate+Change+Koningstein+Fork,http://research.google.com/pubs/pub43326.html
Postmarket Drug Surveillance Without Trial Costs: Discovery of Adverse Drug Reactions Through Large-Scale Analysis of Web Search Queries,Journal of Medical Internet Research vol. 15 (2013),2013,Elad Yom-Tov Evgeniy Gabrilovich,@article{42031 title = {Postmarket Drug Surveillance Without Trial Costs: Discovery of Adverse Drug Reactions Through Large-Scale Analysis of Web Search Queries} author = {Elad Yom-Tov and Evgeniy Gabrilovich} year = 2013 journal = {Journal of Medical Internet Research} volume = {15} },Background: Postmarket drug safety surveillance largely depends on spontaneous reports by patients and healthcare providers hence less common adverse drug reactions—especially those caused by long-term exposure multidrug treatments or specific to special populations—often elude discovery. Objective: Here we propose an ultra-low-cost fully automated method for continuous monitoring of adverse drug reactions in single drugs and in combinations thereof and demonstrate the discovery of heretofore unknown ones. Materials and Methods: We use aggregated search data of large populations of Internet users to extract information related to drugs and adverse reactions to them and correlate these data over time. We further extend our method to identify adverse reactions to combinations of drugs. Results: We validate our method by showing high correlation of our findings with known adverse drug reactions (ADRs). However while acute early-onset drug reactions are more likely to be reported to regulatory agencies we show that less acute later-onset ones are better captured in Web search queries. Conclusions: Our method is advantageous in identifying previously unknown adverse drug reactions. These ADRs should be considered as candidates for further scrutiny by medical regulatory authorities e.g. through Phase IV trials.,http://research.google.com/pubs/archive/42031.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Postmarket+Drug+Surveillance+Without+Trial+Costs:+Discovery+of+Adverse+Drug+Reactions+Through+Large-Scale+Analysis+of+Web+Search+Queries+Yom-Tov+Gabrilovich,http://research.google.com/pubs/pub42031.html
The Theory of Variational Hybrid Quantum-Classical Algorithms,New Journal of Physics vol. 18 (2016) pp. 023023,2016,Jarrod McClean Jonathan Romero Ryan Babbush Alán Aspuru-Guzik,@article{43965 title = {The Theory of Variational Hybrid Quantum-Classical Algorithms} author = {Jarrod McClean and Jonathan Romero and Ryan Babbush and Alán Aspuru-Guzik} year = 2016 URL = {http://dx.doi.org/10.1088/1367-2630/18/2/023023} journal = {New Journal of Physics} pages = {023023} volume = {18} },"Many quantum algorithms have daunting resource requirements when compared to what is available today. To address this discrepancy a quantum-classical hybrid optimization scheme known as ""the quantum variational eigensolver"" was developed with the philosophy that even minimal quantum resources could be made useful when used in conjunction with classical routines. In this work we extend the general theory of this algorithm and suggest algorithmic improvements for practical implementations. Specifically we develop a variational adiabatic ansatz and explore unitary coupled cluster where we establish a connection from second order unitary coupled cluster to universal gate sets through relaxation of exponential splitting. We introduce the concept of quantum variational error suppression that allows some errors to be suppressed naturally in this algorithm on a pre-threshold quantum device. Additionally we analyze truncation and correlated sampling in Hamiltonian averaging as ways to reduce the cost of this procedure. Finally we show how the use of modern derivative free optimization techniques can offer dramatic computational savings of up to three orders of magnitude over previously used optimization techniques.",http://research.google.com/pubs/archive/43965.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Theory+of+Variational+Hybrid+Quantum-Classical+Algorithms+McClean+Romero+Babbush+Aspuru-Guzik,http://research.google.com/pubs/pub43965.html
RFC7710 - Captive-Portal Identification Using DHCP or Router Advertisements (RAs),IETF RFC Internet Engineering Task Force (2015) pp. 8,2015,Warren Kumari Olafur Gudmundsson,@incollection{44318 title = {RFC7710 - Captive-Portal Identification Using DHCP or Router Advertisements (RAs)} author = {Warren Kumari and Olafur Gudmundsson} year = 2015 booktitle = {IETF RFC} pages = {8} },In many environments offering short-term or temporary Internet access (such as coffee shops) it is common to start new connections in a captive-portal mode. This highly restricts what the customer can do until the customer has authenticated. This document describes a DHCP option (and a Router Advertisement (RA) extension) to inform clients that they are behind some sort of captive-portal device and that they will need to authenticate to get Internet access. It is not a full solution to address all of the issues that clients may have with captive portals; it is designed to be used in larger solutions. The method of authenticating to and interacting with the captive portal is out of scope for this document.,http://research.google.com/pubs/archive/44318.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7710+-+Captive-Portal+Identification+Using+DHCP+or+Router+Advertisements+(RAs)+Kumari+Gudmundsson,http://research.google.com/pubs/pub44318.html
Search Worms,WORM '06: Proceedings of the 4th ACM workshop on Recurring malcode ACM Press Alexandria Virginia USA (2006) pp. 1-8,2006,Niels Provos Joe McClain Ke Wang,@inproceedings{32617 title = {Search Worms} author = {Niels Provos and Joe McClain and Ke Wang} year = 2006 URL = {http://doi.acm.org/10.1145/1179542.1179544} booktitle = {WORM '06: Proceedings of the 4th ACM workshop on Recurring malcode} pages = {1--8} address = {Alexandria Virginia USA} },Worms are becoming more virulent at the same time as operating system improvements try to contain them.Recent research demonstrates several effective methods to detect and prevent randomly scanning worms from spreading [2 13]. As a result worm authors are looking for new ways to acquire vulnerable targets without relying on randomly scanning for them. It is often possible to find vulnerable web servers by sending carefully crafted queries to search engines. Search worms1 automate this approach and spread by using popular search engines to find new attack vectors. These worms not only put significant load on search engines they also evade detection mechanisms that assume random scanning. From the point of view of a search engine signatures against search queries are only a temporary measure as many different search queries lead to the same results. In this paper we present our experience with search worms and a framework that allows search engines to quickly detect new worms and take automatic countermeasures. We argue that signature-based filtering of search queries is ill-suited for protecting against search worms and show how we prevent worm propagation without relying on query signatures. We illustrate our approach with measurements and numeric simulations.,http://doi.acm.org/10.1145/1179542.1179544,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Search+Worms+Provos+McClain+Wang,http://research.google.com/pubs/pub32617.html
Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-By-Example Applications,12th International Society for Music Information Retrieval Conference (ISMIR) (2011),2011,Vijay Chandrasekhar Matt Sharifi David Ross,@inproceedings{37261 title = {Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-By-Example Applications} author = {Vijay Chandrasekhar and Matt Sharifi and David Ross} year = 2011 booktitle = {12th International Society for Music Information Retrieval Conference (ISMIR)} },We survey and evaluate popular audio ﬁngerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance size of ﬁngerprints generated compared to size of audio probe and transmission delay if the ﬁngerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications,http://research.google.com/pubs/archive/37261.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Survey+and+Evaluation+of+Audio+Fingerprinting+Schemes+for+Mobile+Query-By-Example+Applications+Chandrasekhar+Sharifi+Ross,http://research.google.com/pubs/pub37261.html
Large Scale Online Learning of Image Similarity Through Ranking,Journal of Machine Learning Research JMLR (2010) pp. 1109-1135,2010,Gal Chechik Varun Sharma Uri Shalit Samy Bengio,@article{35114 title = {Large Scale Online Learning of Image Similarity Through Ranking} author = {Gal Chechik and Varun Sharma and Uri Shalit and Samy Bengio} year = 2010 URL = {http://jmlr.csail.mit.edu/papers/v11/chechik10a.html} journal = {Journal of Machine Learning Research JMLR} pages = {1109--1135} },Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately the approaches that exist today for learning such semantic similarity do not scale to large datasets. This is both because typically their CPU and storage requirements grow quadratically with the sample size and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a dataset with thousands of images it achieves better results than existing state-of-the-art methods while being an order of magnitude faster. For large web scale datasets OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale dataset human evaluations showed that 35% of the ten nearest neighbors of a given test image as found by OASIS were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale datasets that could not be handled before.,http://research.google.com/pubs/archive/35114.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Online+Learning+of+Image+Similarity+Through+Ranking+Chechik+Sharma+Shalit+Bengio,http://research.google.com/pubs/pub35114.html
Indirect Content Privacy Surveys: Measuring Privacy Without Asking About It,Symposium on Usable Privacy and Security (SOUPS) ACM SIGCHI (2011),2011,Alex Braunstein Laura Granka Jessica Staddon,@inproceedings{37128 title = {Indirect Content Privacy Surveys: Measuring Privacy Without Asking About It} author = {Alex Braunstein and Laura Granka and Jessica Staddon} year = 2011 booktitle = {Symposium on Usable Privacy and Security (SOUPS)} },The strong emotional reaction elicited by privacy issues is well documented (e.g. [12 8]). The emotional aspect of privacy makes it difficult to evaluate privacy concern and directly asking about a privacy issue may result in an emo- tional reaction and a biased response. This effect may be partly responsible for the dramatic privacy concern ratings coming from recent surveys ratings that often seem to be at odds with user behavior. In this paper we propose indirect techniques for measuring content privacy concerns through surveys thus hopefully diminishing any emotional response. We present a design for indirect surveys and test the design’s use as (1) a means to measure relative privacy concerns across content types (2) a tool for predicting unwillingness to share content (a possible indicator of privacy concern) and (3) a gauge for two underlying dimensions of privacy – content importance and the willingness to share content. Our evaluation consists of 3 surveys taken by 200 users each in which privacy is never asked about directly but privacy warnings are issued with increasing escalation in the instruc- tions and individual question-wording. We demonstrate that this escalation results in statistically and practically signif- icant differences in responses to individual questions. In addition we compare results against a direct privacy survey and show that rankings of privacy concerns are increasingly preserved as privacy language increases in the indirect sur- veys thus indicating our mapping of the indirect questions to privacy ratings is accurately reflecting privacy concerns.,http://research.google.com/pubs/archive/37128.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Indirect+Content+Privacy+Surveys:+Measuring+Privacy+Without+Asking+About+It+Braunstein+Granka+Staddon,http://research.google.com/pubs/pub37128.html
Classifying with Confidence From Incomplete Test Data,Journal Machine Learning Research (JMLR) vol. 14 (2013),2013,Nathan Parris Hyrum S. Anderson Maya R. Gupta Dun Yu Hsaio,@article{41699 title = {Classifying with Confidence From Incomplete Test Data} author = {Nathan Parris and Hyrum S. Anderson and Maya R. Gupta and Dun Yu Hsaio} year = 2013 journal = {Journal Machine Learning Research (JMLR)} volume = {14} },We consider the classification problem given incomplete information about a test sample. This problem arises naturally when data about the test sample is collected over time or when costs must be incurred to collect the data. For example in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time and either additional time power bandwidth or some other cost must be incurred to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability --- the probability that a label assigned to the incomplete data matches the label that would be assigned to the complete data and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classification decision rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time-series datasets where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met.,http://research.google.com/pubs/archive/41699.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Classifying+with+Confidence+From+Incomplete+Test+Data+Parris+Anderson+Gupta+Hsaio,http://research.google.com/pubs/pub41699.html
PRIME: Probabilistic Initial 3D Model Generation for Single-Particle Cryo-Electron Microscopy,Structure vol. 21 (2013) pp. 1299-1306,2013,Hans Elmlund Dominika Elmlund Samy Bengio,@article{41343 title = {PRIME}: Probabilistic Initial 3D Model Generation for Single-Particle Cryo-Electron Microscopy} author = {Hans Elmlund and Dominika Elmlund and Samy Bengio} year = 2013 journal = {Structure} pages = {1299--1306} volume = {21} },Low-dose electron microscopy of cryo-preserved individual biomolecules (single-particle cryo-EM) is a powerful tool for obtaining information about the structure and dynamics of large macromolecular assemblies. Acquiring images with low dose reduces radiation damage preserves atomic structural details but results in low signal-to-noise ratio of the individual images. The projection directions of the two-dimensional images are random and unknown. The grand challenge is to achieve the precise three-dimensional (3D) alignment of many (tens of thousands to millions) noisy projection images which may then be combined to obtain a faithful 3D map. An accurate initial 3D model is critical for obtaining the precise 3D alignment required for high-resolution (<10 Å) map reconstruction. We report a method (PRIME) that in a single step and without prior structural knowledge can generate an accurate initial 3D map directly from the noisy images.,http://research.google.com/pubs/archive/41343.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=PRIME%7D:+Probabilistic+Initial+3D+Model+Generation+for+Single-Particle+Cryo-Electron+Microscopy+Elmlund+Elmlund+Bengio,http://research.google.com/pubs/pub41343.html
Machine Learning: The High Interest Credit Card of Technical Debt,SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop),2014,D. Sculley Gary Holt Daniel Golovin Eugene Davydov Todd Phillips Dietmar Ebner Vinay Chaudhary Michael Young,@inproceedings{43146 title = {Machine Learning: The High Interest Credit Card of Technical Debt} author = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young} year = 2014 booktitle = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)} },Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion entanglement hidden feedback loops undeclared consumers data dependencies changes in the external world and a variety of system-level anti-patterns.,http://research.google.com/pubs/archive/43146.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+Learning:+The+High+Interest+Credit+Card+of+Technical+Debt+Sculley+Holt+Golovin+Davydov+Phillips+Ebner+Chaudhary+Young,http://research.google.com/pubs/pub43146.html
A Survey of Algorithms and Analysis for Adaptive Online Learning,Preprint (2014),2014,H. Brendan McMahan,@techreport{43105 title = {A Survey of Algorithms and Analysis for Adaptive Online Learning} author = {H. Brendan McMahan} year = 2014 URL = {http://arxiv.org/abs/1403.3465} institution = {Preprint} },We present tools for the analysis of Follow-The-Regularized-Leader (FTRL) Dual Averaging and Mirror Descent algorithms when the regularizer (equivalently prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g. Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further we prove a general and exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a correspond- ing FTRL update which allows us to analyze any Mirror Descent algorithm in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form holding for arbitrary norms and non-smooth regularizers with time-varying weight.,http://arxiv.org/abs/1403.3465,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Survey+of+Algorithms+and+Analysis+for+Adaptive+Online+Learning+McMahan,http://research.google.com/pubs/pub43105.html
Structured Data Meets the Web: A Few Observations,Data Engineering Bulletin (2006),2006,Jayant Madhavan Alon Halevy Shirley Cohen Xin (Luna) Dong Shawn R. Jeffery David Ko Cong Yu,@article{32593 title = {Structured Data Meets the Web: A Few Observations} author = {Jayant Madhavan and Alon Halevy and Shirley Cohen and Xin (Luna) Dong and Shawn R. Jeffery and David Ko and Cong Yu} year = 2006 URL = {http://research.microsoft.com/research/db/debull/} note = {Invited Contribution} journal = {Data Engineering Bulletin} },The World Wide Web is witnessing an increase in the amount of structured content -- vast heterogeneous collections of structured data are on the rise due to the Deep Web annotation schemes like Flickr and sites like Google Base. While this phenomenon is creating an opportunity for structured data management dealing with heterogeneity on the web-scale presents many new challenges. In this paper we articulate challenges based on our experience with addressing them at Google and offer some principles for addressing them in a general fashion.,http://research.google.com/pubs/archive/32593.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Structured+Data+Meets+the+Web:+A+Few+Observations+Madhavan+Halevy+Cohen+Dong+Jeffery+Ko+Yu,http://research.google.com/pubs/pub32593.html
History and Future of Auditory Filter Models,Proc. ISCAS IEEE (2010) pp. 3809-3812,2010,Richard F. Lyon Andreas G. Katsiamis Emmanuel M. Drakakis,@inproceedings{36895 title = {History and Future of Auditory Filter Models} author = {Richard F. Lyon and Andreas G. Katsiamis and Emmanuel M. Drakakis} year = 2010 URL = {http://dicklyon.com/tech/Hearing/AuditoryFilters_ISCAS2010.pdf} booktitle = {Proc. ISCAS} pages = {3809--3812} },Auditory filter models have a history of over a hundred years with explicit bio-mimetic inspiration at many stages along the way. From passive analogue electric delay line models through digital filter models active analogue VLSI models and abstract filter shape models these filters have both represented and driven the state of progress in auditory research. Today we are able to represent a wide range of linear and nonlinear aspects of the psychophysics and physiology of hearing with a rather simple and elegant set of circuits or computations that have a clear connection to underlying hydrodynamics and with parameters calibrated to human performance data. A key part of the progress in getting to this stage has been the experimental clarification of the nature of cochlear nonlinearities and the modelling work to map these experimental results into the domain of circuits and systems. No matter how these models are built into machine-hearing systems their bio-mimetic roots will remain key to their performance. In this paper we review some of these models explain their advantages and disadvantages and present possible ways of implementing them. As an example a continuous-time analogue CMOS implementation of the One Zero Gammatone Filter (OZGF) is presented together with its automatic gain control that models its level-dependent nonlinear behaviour.,http://research.google.com/pubs/archive/36895.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=History+and+Future+of+Auditory+Filter+Models+Lyon+Katsiamis+Drakakis,http://research.google.com/pubs/pub36895.html
Large-scale sequence-discriminative joint adaptive training for masking-based robust ASR,INTERSPEECH-2015 ISCA pp. 3571-3575,2015,Arun Narayanan Ananya Misra Kean Chin,@inproceedings{44019 title = {Large-scale sequence-discriminative joint adaptive training for masking-based robust ASR} author = {Arun Narayanan and Ananya Misra and Kean Chin} year = 2015 URL = {http://www.isca-speech.org/archive/interspeech_2015/i15_3571.html} booktitle = {INTERSPEECH-2015} pages = {3571--3575} },Recently it was shown that the performance of supervised time-frequency masking based robust automatic speech recognition techniques can be improved by training them jointly with the acoustic model [1]. The system in [1] termed deep neural network based joint adaptive training used fully-connected feed-forward deep neural networks for estimating time-frequency masks and for acoustic modeling; stacked log mel spectra was used as features and training minimized cross entropy loss. In this work we extend such jointly trained systems in several ways. First we use recurrent neural networks based on long short-term memory (LSTM) units — this allows the use of unstacked features simplifying joint optimization. Next we use a sequence discriminative training criterion for optimizing parameters. Finally we conduct experiments on large scale data and show that joint adaptive training can provide gains over a strong baseline. Systematic evaluations on noisy voice-search data show relative improvements ranging from 2% at 15 dB to 5.4% at -5 dB over a sequence discriminative multi-condition trained LSTM acoustic model.,http://research.google.com/pubs/archive/44019.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+sequence-discriminative+joint+adaptive+training+for+masking-based+robust+ASR+Narayanan+Misra+Chin,http://research.google.com/pubs/pub44019.html
MillWheel: Fault-Tolerant Stream Processing at Internet Scale,Very Large Data Bases (2013) pp. 734-746,2013,Tyler Akidau Alex Balikov Kaya Bekiroglu Slava Chernyak Josh Haberman Reuven Lax Sam McVeety Daniel Mills Paul Nordstrom Sam Whittle,@inproceedings{41378 title = {MillWheel: Fault-Tolerant Stream Processing at Internet Scale} author = {Tyler Akidau and Alex Balikov and Kaya Bekiroglu and Slava Chernyak and Josh Haberman and Reuven Lax and Sam McVeety and Daniel Mills and Paul Nordstrom and Sam Whittle} year = 2013 booktitle = {Very Large Data Bases} pages = {734--746} },MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes and the system manages persistent state and the continuous flow of records all within the envelope of the framework's fault-tolerance guarantees. This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice we find that MillWheel's unique combination of scalability fault tolerance and a versatile programming model lends itself to a wide variety of problems at Google.,http://research.google.com/pubs/archive/41378.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MillWheel:+Fault-Tolerant+Stream+Processing+at+Internet+Scale+Akidau+Balikov+Bekiroglu+Chernyak+Haberman+Lax+McVeety+Mills+Nordstrom+Whittle,http://research.google.com/pubs/pub41378.html
Pseudo-likelihood methods for community detection in large sparse networks,Annals of Statistics (2013) pp. 1-27,2013,Arash A Amini Aiyou Chen Peter Bickel Liza Levina,@article{40697 title = {Pseudo-likelihood methods for community detection in large sparse networks} author = {Arash A Amini and Aiyou Chen and Peter Bickel and Liza Levina} year = 2013 URL = {http://arxiv.org/pdf/1207.2340v1.pdf} journal = {Annals of Statistics} pages = {1-27} },Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks and often fail on sparse networks. Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees. We show that the algorithms perform well under a range of settings including on very sparse networks and illustrate on the example of a network of political blogs. We also propose spectral clustering with perturbations a method of independent interest which works well on sparse networks where regular spectral clustering fails and use it to provide an initial value for pseudo-likelihood.,http://research.google.com/pubs/archive/40697.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Pseudo-likelihood+methods+for+community+detection+in+large+sparse+networks+Amini+Chen+Bickel+Levina,http://research.google.com/pubs/pub40697.html
Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models,Association for Computational Linguistics (ACL) (2011),2011,Sameer Singh Amarnag Subramanya Fernando Pereira Andrew McCallum,@inproceedings{37560 title = {Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models} author = {Sameer Singh and Amarnag Subramanya and Fernando Pereira and Andrew McCallum} year = 2011 booktitle = {Association for Computational Linguistics (ACL)} },Cross-document coreference the task of grouping all the mentions of each entity in a document collection arises in information extraction and automated knowledge base construction. For large collections it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas we constructed a labeled corpus of 1:5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset demonstrating the scalability of our approach.,http://research.google.com/pubs/archive/37560.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Cross-Document+Coreference+Using+Distributed+Inference+and+Hierarchical+Models+Singh+Subramanya+Pereira+McCallum,http://research.google.com/pubs/pub37560.html
Clinching Auctions with Online Supply,SODA (2013) pp. 605-619,2013,Gagan Goel Vahab Mirrokni Renato Paes Leme,@inproceedings{40464 title = {Clinching Auctions with Online Supply} author = {Gagan Goel and Vahab Mirrokni and Renato Paes Leme} year = 2013 URL = {http://arxiv.org/abs/1210.1456} booktitle = {SODA} pages = {605-619} },Auctions for perishable goods such as internet ad inventory need to make real-time allocation and pricing decisions as the supply of the good arrives in an online manner without knowing the entire supply in advance. These allocation and pricing decisions get complicated when buyers have some global constraints. In this work we consider a multi-unit model where buyers have global {\em budget} constraints and the supply arrives in an online manner. Our main contribution is to show that for this setting there is an individually-rational incentive-compatible and Pareto-optimal auction that allocates these units and calculates prices on the fly without knowledge of the total supply. We do so by showing that the Adaptive Clinching Auction satisfies a {\em supply-monotonicity} property. We also analyze and discuss using examples how the insights gained by the allocation and payment rule can be applied to design better ad allocation heuristics in practice. Finally while our main technical result concerns multi-unit supply we propose a formal model of online supply that captures scenarios beyond multi-unit supply and has applications to sponsored search. We conjecture that our results for multi-unit auctions can be extended to these more general models.,http://arxiv.org/abs/1210.1456,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Clinching+Auctions+with+Online+Supply+Goel+Mirrokni+Paes+Leme,http://research.google.com/pubs/pub40464.html
Real-Time Human Pose Tracking from Range Data,Proceedings of the European Conference on Computer Vision (ECCV) (2012),2012,Varun Ganapathi Christian Plagemann Daphne Koller Sebastian Thrun,@inproceedings{39958 title = {Real-Time Human Pose Tracking from Range Data} author = {Varun Ganapathi and Christian Plagemann and Daphne Koller and Sebastian Thrun} year = 2012 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)} },Tracking human pose in real-time is a difficult problem with many interesting applications. Existing solutions suffer from a variety of problems especially when confronted with unusual human poses. In this paper we derive an algorithm for tracking human pose in real-time from depth sequences based on MAP inference in a probabilistic temporal model. The key idea is to extend the iterative closest points (ICP) objective by modeling the constraint that the observed subject cannot enter free space the area of space in front of the true range measurements. Our primary contribution is an extension to the articulated ICP algorithm that can efficiently enforce this constraint. Our experiments show that including this term improves tracking accuracy significantly. The resulting filter runs at 125 frames per second using a single desktop CPU core. We provide extensive experimental results on challenging real-world data which show that the algorithm outperforms the previous state-of the-art trackers both in computational efficiency and accuracy.,http://research.google.com/pubs/archive/39958.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Real-Time+Human+Pose+Tracking+from+Range+Data+Ganapathi+Plagemann+Koller+Thrun,http://research.google.com/pubs/pub39958.html
Practical Gammatone-Like Filters for Auditory Modeling,EURASIP Journal on Audio Speech and Music Processing vol. 2007 (2007) pp. 12,2007,Andreas G. Katsiamis Emmanuel M. Drakakis Richard F. Lyon,@article{33012 title = {Practical Gammatone-Like Filters for Auditory Modeling} author = {Andreas G. Katsiamis and Emmanuel M. Drakakis and Richard F. Lyon} year = 2007 journal = {EURASIP Journal on Audio Speech and Music Processing} pages = {12} volume = {2007} },This paper deals with continuous-time filter transfer functions that resemble tuning curves at particular set of places on the basilar membrane of the biological cochlea and that are suitable for practical VLSI implementations. The resulting filters can be used in a filterbank architecture to realize cochlea implants or auditory processors of increased biorealism. To put the reader into context the paper starts with a short review on the gammatone filter and then exposes two of its variants namely the differentiated all-pole gammatone filter (DAPGF) and one-zero gammatone filter (OZGF) filter responses that provide a robust foundation for modeling cochlea transfer functions. The DAPGF and OZGF responses are attractive because they exhibit certain characteristics suitable for modeling a variety of auditory data: level-dependent gain linear tail for frequencies well below the center frequency asymmetry and so forth. In addition their form suggests their implementation by means of cascades of N identical two-pole systems which render them as excellent candidates for efficient analog or digital VLSI realizations. We provide results that shed light on their char- acteristics and attributes and which can also serve as “design curves” for fitting these responses to frequency-domain physiological data. The DAPGF and OZGF responses are essentially a “missing link” between physiological electrical and mechanical models for auditory filtering.,http://research.google.com/pubs/archive/33012.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Practical+Gammatone-Like+Filters+for+Auditory+Modeling+Katsiamis+Drakakis+Lyon,http://research.google.com/pubs/pub33012.html
Large Scale Graph Transduction,NIPS 2009 Workshop on Large-Scale Machine Learning: Parallelism and Massive Datasets NIPS,2009,Amarnag Subramanya Jeff Bilmes,@inproceedings{35656 title = {Large Scale Graph Transduction} author = {Amarnag Subramanya and Jeff Bilmes} year = 2009 booktitle = {NIPS 2009 Workshop on Large-Scale Machine Learning: Parallelism and Massive Datasets} },We consider the issue of scalability of graph-based semi-supervised learning (SSL) algorithms. In this context we propose a fast graph node ordering algorithm that improves parallel spatial locality by being cache cognizant. This approach allows for a linear speedup on a shared-memory parallel machine to be achievable and thus means that graph-based SSL can scale to very large data sets. We use the above algorithm an a multi-threaded implementation to solve a SSL problem on a 120 million node graph in a reasonable amount of time.,http://research.google.com/pubs/archive/35656.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Graph+Transduction+Subramanya+Bilmes,http://research.google.com/pubs/pub35656.html
On-Demand Language Model Interpolation for Mobile Speech Input,Interspeech (2010) pp. 1812-1815,2010,Brandon Ballinger Cyril Allauzen Alexander Gruenstein Johan Schalkwyk,@inproceedings{36756 title = {On-Demand Language Model Interpolation for Mobile Speech Input} author = {Brandon Ballinger and Cyril Allauzen and Alexander Gruenstein and Johan Schalkwyk} year = 2010 booktitle = {Interspeech} pages = {1812-1815} },Google offers several speech features on the Android mobile operating system: search by voice voice input to any text field and an API for application developers. As a result our speech recognition service must support a wide range of usage scenarios and speaking styles: relatively short search queries addresses business names dictated SMS and e-mail messages and a long tail of spoken input to any of the applications users may install. We present a method of on-demand language model interpolation in which contextual information about each utterance determines interpolation weights among a number of n-gram language models. On-demand interpolation results in an 11.2% relative reduction in WER compared to using a single language model to handle all traffic.,http://research.google.com/pubs/archive/36756.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On-Demand+Language+Model+Interpolation+for+Mobile+Speech+Input+Ballinger+Allauzen+Gruenstein+Schalkwyk,http://research.google.com/pubs/pub36756.html
Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice,OGI/OHSU Seminar Series Portland Oregon USA (2011),2011,Ciprian Chelba Johan Schalkwyk Boulos Harb Carolina Parada Cyril Allauzen Michael Riley Peng Xu Thorsten Brants Vida Ha Will Neveitt,@misc{37075 title = {Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice} author = {Ciprian Chelba and Johan Schalkwyk and Boulos Harb and Carolina Parada and Cyril Allauzen and Michael Riley and Peng Xu and Thorsten Brants and Vida Ha and Will Neveitt} year = 2011 URL = {http://seminar.csee.ogi.edu/seminar.html} note = {http://seminar.csee.ogi.edu/seminar.html} },The talk presents key aspects faced when building language models (LM) for the google.com query stream and their use for automatic speech recognition (ASR). Distributed LM tools enable us to handle a huge amount of data and experiment with LMs that are two orders of magnitude larger than usual. An empirical exploration of the problem led us to re-discovering a less known interaction between Kneser-Ney smoothing and entropy pruning possible non-stationarity of the query stream as well as strong dependence on various English locales---USA Britain and Australia. LM compression techniques allowed us to use one billion n-gram LMs in the first pass of an ASR system built on FST technology and evaluate empirically whether a two-pass system architecture has any losses over one pass.,http://research.google.com/pubs/archive/37075.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Language+Modeling+for+Automatic+Speech+Recognition+Meets+the+Web:+Google+Search+by+Voice+Chelba+Schalkwyk+Harb+Parada+Allauzen+Riley+Xu+Brants+Ha+Neveitt,http://research.google.com/pubs/pub37075.html
Data-driven network connectivity,Proceedings of the 10th ACM Workshop on Hot Topics in Networks (2011),2011,Junda Liu,@inproceedings{39984 title = {Data-driven network connectivity} author = {Junda Liu} year = 2011 booktitle = {Proceedings of the 10th ACM Workshop on Hot Topics in Networks} },Routing on the Internet combines data plane mechanisms for forwarding traffic with control plane protocols for guaranteeing connectivity and optimizing routes (e.g. shortest-paths and load distribution). We propose data-driven connectivity (DDC) a new routing approach that achieves the fundamental connectivity guarantees in the data plane rather than the control plane while keeping the more complex requirements of route optimization in the control plane. DDC enables faster recovery from failures and easier implementation of control plane optimization.,http://research.google.com/pubs/archive/39984.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Data-driven+network+connectivity+Liu,http://research.google.com/pubs/pub39984.html
MapReduce: The programming model and practice,SIGMETRICS (2009),2009,Jerry Zhao Jelena Pjesivac-Grbovic,@misc{36249 title = {MapReduce: The programming model and practice} author = {Jerry Zhao and Jelena Pjesivac-Grbovic} year = 2009 URL = {http://research.google.com/archive/papers/mapreduce-sigmetrics09-tutorial.pdf} note = {Tutorial} },Inspired by similar concepts in functional languages dated as early as 60's Google first introduced MapReduce in 2004. Now MapReduce has become the most popular framework for large-scale data processing at Google and it is becoming the framework of choice on many off-the-shelf clusters. In this tutorial we first introduce the MapReduce programming model illustrating its power by couple of examples. We discuss the MapReduce and its relationship to MPI and DBMS. Performance is a key feature of the Google MapReduce implementation and we will discus a few techniques used to achieve this goal. Google MapReduce exploits data locality to reduce network overhead. We utilize different scheduling techniques to ensure a job is progressing in the presence of variable system load. Finally since failures are common in our data centers we provide a number of failure avoidance and recovery features to ensure the job completion in such environment.,http://research.google.com/pubs/archive/36249.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MapReduce:+The+programming+model+and+practice+Zhao+Pje%C5%A1ivac%E2%80%93Grbovi%C4%87,http://research.google.com/pubs/pub36249.html
VIP: Finding Important People in Images,Computer Vision and Pattern Recognition Computer Vision and Pattern Recognition Computer Vision and Pattern Recognition (2015) pp. 4858-4966,2015,Clint Solomon Mathialagan Andrew C. Gallagher Dhruv Batra,@inproceedings{43844 title = {VIP: Finding Important People in Images} author = {Clint Solomon Mathialagan and Andrew C. Gallagher and Dhruv Batra} year = 2015 URL = {http://arxiv.org/pdf/1502.05678v2.pdf} booktitle = {Computer Vision and Pattern Recognition} pages = {4858-4966} address = {Computer Vision and Pattern Recognition} },People preserve memories of events such as birthdays weddings or vacations by capturing photos often depicting groups of people. Invariably some individuals in the image are more important than others given the context of the event. This paper analyzes the concept of the importance of individuals in group photographs. We address two specific questions – Given an image who are the most important individuals in it? Given multiple images of a person which image depicts the person in the most important role? We introduce a measure of importance of people in images and investigate the correlation between importance and visual saliency. We find that not only can we automatically predict the importance of people from purely visual cues incorporating this predicted importance results in signifi- cant improvement in applications such as im2text (generating sentences that describe images of groups of people).,http://research.google.com/pubs/archive/43844.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=VIP:+Finding+Important+People+in+Images+Mathialagan+Gallagher+Batra,http://research.google.com/pubs/pub43844.html
Web Application Obfuscation,Syngress (2010) pp. 282,2010,Eduardo Alberto Vela Nava,@book{38288 title = {Web Application Obfuscation} author = {Eduardo Alberto Vela Nava} year = 2010 pages = {282} },Web Application Obfuscation aims to instruct developers and security professionals about the different peculiarities in browsers that can be used to attack and hide an attack in web applications as well as bypass web application firewalls and intrusion detection systems.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Application+Obfuscation+Vela+Nava,http://research.google.com/pubs/pub38288.html
Studies in Lower Bounding Probability of Evidence using the Markov Inequality,UAI Morgan Kaufmann (2007),2007,Vibhav Gogate Bozhena Bidyuk Rina Dechter,@inproceedings{32818 title = {Studies in Lower Bounding Probability of Evidence using the Markov Inequality} author = {Vibhav Gogate and Bozhena Bidyuk and Rina Dechter} year = 2007 booktitle = {UAI} },Computing the probability of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation that provides high confidence lower bounds on probability of evidence. Our proposed approximation is a randomized importance sampling based scheme that uses the Markov inequality. However a straight-forward application of the Markov inequality may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with state-of-the-art lower bounding schemes reveals the promise of our approach.,http://research.google.com/pubs/archive/32818.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Studies+in+Lower+Bounding+Probability+of+Evidence+using+the+Markov+Inequality+Gogate+Bidyuk+Dechter,http://research.google.com/pubs/pub32818.html
Inducing Sentence Structure from Parallel Corpora for Reordering,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP) Association for Computational Linguistics,2011,John DeNero Jakob Uszkoreit,@inproceedings{37163 title = {Inducing Sentence Structure from Parallel Corpora for Reordering} author = {John DeNero and Jakob Uszkoreit} year = 2011 booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)} },When translating among languages that differ substantially in word order machine translation (MT) systems beneﬁt from syntactic preordering—an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text thus establishing a new application for unsupervised grammar induction.,http://research.google.com/pubs/archive/37163.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Inducing+Sentence+Structure+from+Parallel+Corpora+for+Reordering+DeNero+Uszkoreit,http://research.google.com/pubs/pub37163.html
Allocation Folding Based on Dominance,Proceedings of the 2014 International Symposium on Memory Management ACM New York NY USA,2014,Daniel Clifford Hannes Payer Michael Starzinger Ben L. Titzer,@inproceedings{42478 title = {Allocation Folding Based on Dominance} author = {Daniel Clifford and Hannes Payer and Michael Starzinger and Ben L. Titzer} year = 2014 booktitle = {Proceedings of the 2014 International Symposium on Memory Management} address = {New York NY USA} },Memory management system performance is of increasing importance in today's managed languages. Two lingering sources of overhead are the direct costs of memory allocations and write barriers. This paper introduces allocation folding an optimization technique where the virtual machine automatically folds multiple memory allocation operations in optimized code together into a single larger allocation group. An allocation group comprises multiple objects and requires just a single bounds check in a bump-pointer style allocation rather than a check for each individual object. More importantly all objects allocated in a single allocation group are guaranteed to be contiguous after allocation and thus exist in the same generation which makes it possible to statically remove write barriers for reference stores involving objects in the same allocation group. Unlike object inlining object fusing and object colocation allocation folding requires no special connectivity or ownership relation between the objects in an allocation group. We present our analysis algorithm to determine when it is safe to fold allocations together and discuss our implementation in V8 an open-source production JavaScript virtual machine. We present performance results for the Octane and Kraken benchmark suites and show that allocation folding is a strong performance improvement even in the presence of some heap fragmentation. Additionally we use four hand-selected benchmarks JPEGEncoder NBody Soft3D and Textwriter where allocation folding has a large impact.,http://research.google.com/pubs/archive/42478.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Allocation+Folding+Based+on+Dominance+Clifford+Payer+Starzinger+Titzer,http://research.google.com/pubs/pub42478.html
Applications of Maximum Entropy Rankers to Problems in Spoken Language Processing,Interspeech 2014 International Speech Communications Association,2014,Richard Sproat Keith Hall,@inproceedings{42533 title = {Applications of Maximum Entropy Rankers to Problems in Spoken Language Processing} author = {Richard Sproat and Keith Hall} year = 2014 booktitle = {Interspeech 2014} },We report on two applications of Maximum Entropy-based ranking models to problems of relevance to automatic speech recognition and text-to-speech synthesis. The first is stress prediction in Russian a language with notoriously complex morphology and stress rules. The second is the classification of alphabetic non-standard words which may be read as words (NATO) as letter sequences (USA) or as a mixed (mymsn). For this second task we report results on English and five other European languages.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Applications+of+Maximum+Entropy+Rankers+to+Problems+in+Spoken+Language+Processing+Sproat+Hall,http://research.google.com/pubs/pub42533.html
Typicality Effects and the Logic of Reciprocity,Cornell University Ithaca NY (2009) pp. 257-274,2009,Nir Kerem Naama Friedmann Yoad Winter,@proceedings{36734 title = {Typicality Effects and the Logic of Reciprocity} editor = {Nir Kerem and Naama Friedmann and Yoad Winter} year = 2009 URL = {http://elanguage.net/journals/index.php/salt/article/viewFile/19.15/1874} pages = {257--274} address = {Ithaca NY} },The variability in the interpretation of reciprocal expressions has been extensively addressed in the literature and received detailed semantic accounts. After pointing out a central empirical limitation of previous logical accounts of reciprocity we argue that these approaches suffer from inadequacies due to ignoring typicality preferences with binary predicate concepts. We claim that typicality preferences are crucial for interpreting reciprocals and introduce a new principle the Maximal Typicality Hypothesis (MTH) which analyzes reciprocals using an extension of the Strongest Meaning Hypothesis (SMH) proposed in Dalrymple et al. (1998) Unlike the SMH which is a principle that implicitly presupposes a classical two-valued (“definitional”) treatment of predicate concepts the MTH respects the fuzziness of such concepts as manifested by their typicality preferences and expects strong correlations between these preferences and the range of logical interpretations available for reciprocal expressions. The expected correlations are supported by new empirical results elicited in a series of experiments with speakers of Hebrew.,http://research.google.com/pubs/archive/36734.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Typicality+Effects+and+the+Logic+of+Reciprocity+Kerem+Friedmann+Winter,http://research.google.com/pubs/pub36734.html
Fulfilling the Hypermedia Constraint Via HTTP OPTIONS the HTTP Vocabulary In RDF and Link Headers,Proceedings of the Second International Workshop on RESTful Design ACM New York NY USA (2011) pp. 11-14,2011,Thomas Steiner Jan Algermissen,@inproceedings{37429 title = {Fulfilling the Hypermedia Constraint Via HTTP OPTIONS the HTTP Vocabulary In RDF and Link Headers} author = {Thomas Steiner and Jan Algermissen} year = 2011 URL = {http://doi.acm.org/10.1145/1967428.1967433} booktitle = {Proceedings of the Second International Workshop on RESTful Design} pages = {11--14} address = {New York NY USA} },One of the main REST design principles is the focus on media types as the core of contracts on the Web. However not always is the service designer free to select the most appropriate media type for a task sometimes a generic media type like application/rdf+xml (or in the worst case a binary format like image/png) with no defined or possible hypermedia controls at all has to be chosen. With this position paper we present a way how the hypermedia constraint of REST can still be fulfilled using a combination of Link headers the OPTIONS method and the HTTP Vocabulary in RDF.,http://research.google.com/pubs/archive/37429.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fulfilling+the+Hypermedia+Constraint+Via+HTTP+OPTIONS+the+HTTP+Vocabulary+In+RDF+and+Link+Headers+Steiner+Algermissen,http://research.google.com/pubs/pub37429.html
Semantics-driven sensor configuration for energy reduction in medical sensor networks,Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics and design ACM pp. 303-308,2012,James Wendt Saro Meguerdichian Hyduke Noshadi Miodrag Potkonjak,@inproceedings{41353 title = {Semantics-driven sensor configuration for energy reduction in medical sensor networks} author = {James Wendt and Saro Meguerdichian and Hyduke Noshadi and Miodrag Potkonjak} year = 2012 URL = {http://dl.acm.org/citation.cfm?id=2333728} booktitle = {Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics and design} pages = {303-308} },Traditional optimization methods for large multisensory networks often use sensor array reduction and sampling techniques that attempt to reduce energy while retaining full predictability of the raw sensed data. For systems such as medical sensor networks raw data prediction is unnecessary rather only relevant semantics derived from the raw data are essential. We present a new method for sensor fusion array reduction and subsampling that reduces both energy and cost through semantics-driven system configuration. Using our method we reduce the energy requirements of a medical shoe by a factor of 17.9 over the original system configuration while maintaining semantic relevance.,http://dl.acm.org/citation.cfm?id=2333728,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantics-driven+sensor+configuration+for+energy+reduction+in+medical+sensor+networks+Wendt+Meguerdichian+Noshadi+Potkonjak,http://research.google.com/pubs/pub41353.html
Markovian Mixture Face Recognition with discriminative face alignment,automatic face and gesture recognition ieee (2008),2008,Ming Zhao,@inproceedings{34391 title = {Markovian Mixture Face Recognition with discriminative face alignment} author = {Ming Zhao} year = 2008 booktitle = {automatic face and gesture recognition} },A typical automatic face recognition system is composed of three parts: face detection face alignment and face recognition. Conventionally these three parts are processed in a bottom-up manner: face detection is performed first then the results are passed to face alignment and finally to face recognition. The bottom-up approach is one extreme of vision approaches. The other extreme approach is top-down. In this paper we proposed a stochastic mixture approach for combining bottom-up and top-down face recognition: face recognition is performed from the results of face alignment in a bottom-up way and face alignment is performed based on the results of face recognition in a top-down way. By modeling the mixture face recognition as a stochastic process the recognized person is decided probabilistically according to the probability distribution coming from the stochastic face recognition and the recognition problem becomes that “who the most probable person is when the stochastic process of face recognition goes on for a long time or ideally for an infinite duration”. This problem is solved with the theory of Markov chains by modeling the stochastic process of face recognition as a Markov chain. As conventional face alignment is not suitable for this mixture approach discriminative face alignment is proposed. And we also prove that the stochastic mixture face recognition results only depend on discriminative face alignment not on conventional face alignment. The effectiveness of our approach is shown by extensive experiments.,http://research.google.com/pubs/archive/34391.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Markovian+Mixture+Face+Recognition+with+discriminative+face+alignment+Zhao,http://research.google.com/pubs/pub34391.html
Diagnosing Latency in Multi-Tier Black-Box Services,5th Workshop on Large Scale Distributed Systems and Middleware (LADIS 2011) (to appear),2011,Krzysztof Ostrowski Gideon Mann Mark Sandler,@inproceedings{37477 title = {Diagnosing Latency in Multi-Tier Black-Box Services} author = {Krzysztof Ostrowski and Gideon Mann and Mark Sandler} year = 2011 booktitle = {5th Workshop on Large Scale Distributed Systems and Middleware (LADIS 2011)} },As multi-tier cloud applications become pervasive we need better tools for understanding their performance. This paper presents a system that analyzes observed or desired changes to end-to-end latency prole in a large distributed application and identifies their underlying causes. It recognizes changes to system conguration workload or performance of individual services that lead to the observed or desired outcome. Experiments on an industrial datacenter demonstrate the utility of the system.,http://research.google.com/pubs/archive/37477.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Diagnosing+Latency+in+Multi-Tier+Black-Box+Services+Ostrowski+Mann+Sandler,http://research.google.com/pubs/pub37477.html
Typed Normal Form Bisimulation for Parametric Polymorphism,Proceedings of the 23rd Annual IEEE Symposium on Logic in Computer Science (LICS' 08) IEEE (2008) pp. 341-352,2008,Soren B. Lassen Paul Blain Levy,@inproceedings{35310 title = {Typed Normal Form Bisimulation for Parametric Polymorphism} author = {Soren B. Lassen and Paul Blain Levy} year = 2008 URL = {http://doi.ieeecomputersociety.org/10.1109/LICS.2008.26} booktitle = {Proceedings of the 23rd Annual IEEE Symposium on Logic in Computer Science (LICS' 08)} pages = {341-352} },"This paper presents a new bisimulation theory for parametric polymorphism which enables straightforward co-inductive proofs of program equivalences involving existential types. The theory is an instance of typed normal form bisimulation and demonstrates the power of this recent framework for modeling typed lambda calculi as labelled transition systems. We develop our theory for a continuation-passing style calculus Jump-With-Argument where normal form bisimulation takes a simple form. We equip the calculus with both existential and recursive types. An ""ultimate pattern matching theorem"" enables us to define bisimilarity and we show it to be a congruence. We apply our theory to proving program equivalences type isomorphisms and genericity.",http://doi.ieeecomputersociety.org/10.1109/LICS.2008.26,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Typed+Normal+Form+Bisimulation+for+Parametric+Polymorphism+Lassen+Levy,http://research.google.com/pubs/pub35310.html
Semantic Role Labeling with Neural Network Factors,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15) Association for Computational Linguistics,2015,Nicholas FitzGerald Oscar Täckström Kuzman Ganchev Dipanjan Das,@inproceedings{43892 title = {Semantic Role Labeling with Neural Network Factors} author = {Nicholas FitzGerald and Oscar Täckström and Kuzman Ganchev and Dipanjan Das} year = 2015 URL = {http://www.aclweb.org/anthology/D15-1112} booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15)} },We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset.,http://research.google.com/pubs/archive/43892.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Role+Labeling+with+Neural+Network+Factors+FitzGerald+T%C3%A4ckstr%C3%B6m+Ganchev+Das,http://research.google.com/pubs/pub43892.html
Residential Power Load Forecasting,Conference on Systems Engineering Research (CSER 2014) Elisevier B.V. Elsevier Inc/1600 John F Kennedy Boulevard Suite 1800 Philadelphia PA 19103-2879 USA (to appear),2014,Jeffrey Stevenson Patrick Day George Ruwisch Michael Fabian Ryan Spencer Rajeshbabu Thoppay Donald Noble,@inproceedings{42179 title = {Residential Power Load Forecasting} author = {Jeffrey Stevenson and Patrick Day and George Ruwisch and Michael Fabian and Ryan Spencer and Rajeshbabu Thoppay and Donald Noble} year = 2014 booktitle = {Conference on Systems Engineering Research (CSER 2014)} address = {Elsevier Inc/1600 John F Kennedy Boulevard Suite 1800 Philadelphia PA 19103-2879 USA} },Abstract The prepaid electric power metering market is being driven in large part by advancements in and the adoption of Smart Grid technology. Advanced smart meters facilitate the deployment of prepaid systems with smart prepaid meters. A successful program hinges on the ability to accurately predict the amount of energy consumed on a daily basis for each end user. This method of forecasting is called Residential Power Load Forecasting (RPLF). This paper describes the systems engineering (SE) processes and tools that were used to develop a recommended load prediction model for the project sponsor SmartGridCIS. The basic concept is that power is treated similar to a prepaid telephone in a “pay as you go” fashion. Modeling techniques explored in the analysis of alternatives (AoA) include Fuzzy Logic Time Series Moving Average and Artificial Neural Networks (ANN). SE tools such as prioritization and Pugh matrices were used to choose the best-fit model which ended up being the ANN. Cognitive systems engineering was used in conjunction with the task analysis. Requirements were developed using the commercial tool IBM Rational DOORS®.,http://research.google.com/pubs/archive/42179.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Residential+Power+Load+Forecasting+Stevenson+Day+Ruwisch+Fabian+Spencer+Thoppay+Noble,http://research.google.com/pubs/pub42179.html
Native Client: A Sandbox for Portable Untrusted x86 Native Code,IEEE Symposium on Security and Privacy (Oakland'09) IEEE IEEE 3 Park Avenue 17th Floor New York NY 10016 (2009),2009,Bennet Yee David Sehr Greg Dardyk Brad Chen Robert Muth Tavis Ormandy Shiki Okasaka Neha Narula Nicholas Fullagar,@inproceedings{34913 title = {Native Client: A Sandbox for Portable Untrusted x86 Native Code} author = {Bennet Yee and David Sehr and Greg Dardyk and Brad Chen and Robert Muth and Tavis Ormandy and Shiki Okasaka and Neha Narula and Nicholas Fullagar} year = 2009 URL = {http://nativeclient.googlecode.com/svn/data/docs_tarball/nacl/googleclient/native_client/documentation/nacl_paper.pdf} booktitle = {IEEE Symposium on Security and Privacy (Oakland'09)} address = {IEEE 3 Park Avenue 17th Floor New York NY 10016} },Native Client is an open-source research technology for running x86 native code in web applications with the goal of maintaining the browser neutrality OS portability and safety that people expect from web apps. We released this project in December 2008 to get feedback from the security and broader open-source communities. We believe that Native Client technology will someday help web developers to create richer and more dynamic browser-based applications.,http://research.google.com/pubs/archive/34913.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Native+Client:+A+Sandbox+for+Portable+Untrusted+x86+Native+Code+Yee+Sehr+Dardyk+Chen+Muth+Ormandy+Okasaka+Narula+Fullagar,http://research.google.com/pubs/pub34913.html
Online Learning with Global Cost Functions,22nd Annual Conference on Learning Theory COLT Omnipress (2009),2009,Eyal Even-Dar Robert Kleinberg Shie Mannor Yishay Mansour,@inproceedings{35385 title = {Online Learning with Global Cost Functions} author = {Eyal Even-Dar and Robert Kleinberg and Shie Mannor and Yishay Mansour} year = 2009 URL = {http://www.cs.mcgill.ca/~colt2009/papers/005.pdf#page=1} booktitle = {22nd Annual Conference on Learning Theory COLT} },We consider an online learning setting where at each time step the decision maker has to choose how to distribute the future loss between k alternatives and then observes the loss of each alternative. Motivated by load balancing and job scheduling we consider a global cost function (over the losses incurred by each alternative) rather than a summation of the instantaneous losses as done traditionally in online learning. Such global cost functions include the makespan (the maximum over the alternatives) and the Ld norm (over the alternatives). Based on approachability theory we design an algorithm that guarantees vanishing regret for this setting where the regret is measured with respect to the best static decision that selects the same distribution over alternatives at every time step. For the special case of makespan cost we devise a simple and efficient algorithm. In contrast we show that for concave global cost functions such as Ld norms for d<1 the worst-case average regret does not vanish.,http://www.cs.mcgill.ca/~colt2009/papers/005.pdf#page=1,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Learning+with+Global+Cost+Functions+Even-Dar+Kleinberg+Mannor+Mansour,http://research.google.com/pubs/pub35385.html
Characterizing Task Usage Shapes in Google Compute Clusters,Proceedings of the 5th International Workshop on Large Scale Distributed Systems and Middleware (2011),2011,Qi Zhang Joseph Hellerstein Raouf Boutaba,@inproceedings{37201 title = {Characterizing Task Usage Shapes in Google Compute Clusters} author = {Qi Zhang and Joseph Hellerstein and Raouf Boutaba} year = 2011 booktitle = {Proceedings of the 5th International Workshop on Large Scale Distributed Systems and Middleware} },The increase in scale and complexity of large compute clus- ters motivates a need for representative workload bench- marks to evaluate the performance impact of system changes so as to assist in designing better scheduling algorithms and in carrying out management activities. To achieve this goal it is necessary to construct workload characterizations from which realistic performance benchmarks can be created. In this paper we focus on characterizing run-time task resource usage for CPU memory and disk. The goal is to find an accurate characterization that can faithfully reproduce the performance of historical workload traces in terms of key performance metrics such as task wait time and machine resource utilization. Through experiments using workload traces from Google production clusters we find that simply using the mean of task usage can generate synthetic work- load traces that accurately reproduce resource utilizations and task waiting time. This seemingly surprising result can be justified by the fact that resource usage for CPU mem- ory and disk are relatively stable over time for the majority of the tasks. Our work not only presents a simple tech- nique for constructing realistic workload benchmarks but also provides insights into understanding workload perfor- mance in production compute clusters.,http://research.google.com/pubs/archive/37201.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Characterizing+Task+Usage+Shapes+in+Google+Compute+Clusters+Zhang+Hellerstein+Boutaba,http://research.google.com/pubs/pub37201.html
Mantis: Efficient Predictions of Execution Time Energy Usage Memory Usage and Network Usage on Smart Mobile Devices,IEEE Transactions on Mobile Computing vol. 14 (2015) pp. 2059-2072,2015,Yongin Kwon Sangmin Lee Hayoon Yi Donghyun Kwon Seungjun Yang Byung-Gon Chun Ling Huang Petros Maniatis Mayur Naik Yunheung Paek,@article{44311 title = {Mantis: Efficient Predictions of Execution Time Energy Usage Memory Usage and Network Usage on Smart Mobile Devices} author = {Yongin Kwon and Sangmin Lee and Hayoon Yi and Donghyun Kwon and Seungjun Yang and Byung-Gon Chun and Ling Huang and Petros Maniatis and Mayur Naik and Yunheung Paek} year = 2015 URL = {http://doi.ieeecomputersociety.org/10.1109/TMC.2014.2374153} journal = {IEEE Transactions on Mobile Computing} pages = {2059--2072} volume = {14} },We present Mantis a framework for predicting the computational resource consumption (CRC) of Android applications on given inputs accurately and efficiently. A key insight underlying Mantis is that program codes often contain features that correlate with performance and these features can be automatically computed efficiently. Mantis synergistically combines techniques from program analysis and machine learning. It constructs concise CRC models by choosing from many program execution features only a handful that are most correlated with the program’s CRC metric yet can be evaluated efficiently from the program’s input. We apply program slicing to reduce evaluation time of a feature and automatically generate executable code snippets for efficiently evaluating features. Our evaluation shows that Mantis predicts four CRC metrics of seven Android apps with estimation error in the range of 0-11.1 percent by executing predictor code spending at most 1.3 percent of their execution time on Galaxy Nexus.,http://doi.ieeecomputersociety.org/10.1109/TMC.2014.2374153,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mantis:+Efficient+Predictions+of+Execution+Time+Energy+Usage+Memory+Usage+and+Network+Usage+on+Smart+Mobile+Devices+Kwon+Lee+Yi+Kwon+Yang+Chun+Huang+Maniatis+Naik+Paek,http://research.google.com/pubs/pub44311.html
Geometric Overpass Extraction from Vector Road Data and DSMs,Proceedings of the 19th ACM SIGSPATIAL international Conference on Advances in Geographic information Systems 2011 (to appear),2011,Joshua Schpok,@inproceedings{37364 title = {Geometric Overpass Extraction from Vector Road Data and DSMs} author = {Joshua Schpok} year = 2011 booktitle = {Proceedings of the 19th ACM SIGSPATIAL international Conference on Advances in Geographic information Systems 2011} },We present a method to extract elevated road structures typically overpassing other roads transit lines and watercourses. The technique uses a digital surface model (DSM) and roughly aligned vector road data and outputs geometry approximating the shape and elevation of the elevated road deck. Our method is robust against noise in DSM elevations and can recover elevated roads partially obscured by trees and other overpasses. We demonstrate our method parallelized over city-wide DSMs and formulate a confidence metric ranking the fidelity of the reconstruction.,http://research.google.com/pubs/archive/37364.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Geometric+Overpass+Extraction+from+Vector+Road+Data+and+DSMs+Schpok,http://research.google.com/pubs/pub37364.html
Evolving ASDF: More Cooperation Less Coordination,Proceedings of the International Lisp Conference 2010,2010,François-René Rideau Robert Goldman,@inproceedings{40391 title = {Evolving ASDF: More Cooperation Less Coordination} author = {François-René Rideau and Robert Goldman} year = 2010 URL = {http://common-lisp.net/project/asdf/ilc2010draft.pdf} booktitle = {Proceedings of the International Lisp Conference 2010} },We present ASDF 2 the current state of the art in CL build systems. From a technical standpoint ASDF 2 improves upon ASDF by integrating previous common extensions making conﬁguration easy and ﬁxing bugs. However the overriding concern driving these changes was social rather than technical: ASDF plays a central role in the CL community and we wanted to reduce the coordination costs that it imposed upon CL programmers. We outline ASDF’s history and architecture explain the link between the social issues we faced and the software features we added and explore the technical challenges involved and lessons learned notably involving inplace code upgrade of ASDF itself backward compatibility portability testing and other coding best practices.,http://common-lisp.net/project/asdf/ilc2010draft.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evolving+ASDF:+More+Cooperation+Less+Coordination+Rideau+Goldman,http://research.google.com/pubs/pub40391.html
F1 - The Fault-Tolerant Distributed RDBMS Supporting Google's Ad Business,SIGMOD (2012),2012,Jeff Shute Mircea Oancea Stephan Ellner Ben Handy Eric Rollins Bart Samwel Radek Vingralek Chad Whipkey Xin Chen Beat Jegerlehner Kyle Littleﬁeld Phoenix Tong,@inproceedings{38125 title = {F1 - The Fault-Tolerant Distributed RDBMS Supporting Google's Ad Business} author = {Jeff Shute and Mircea Oancea and Stephan Ellner and Ben Handy and Eric Rollins and Bart Samwel and Radek Vingralek and Chad Whipkey and Xin Chen and Beat Jegerlehner and Kyle Littleﬁeld and Phoenix Tong} year = 2012 note = {Talk given at SIGMOD 2012} booktitle = {SIGMOD} },Many of the services that are critical to Google’s ad business have historically been backed by MySQL. We have recently migrated several of these services to F1 a new RDBMS developed at Google. F1 implements rich relational database features including a strictly enforced schema a powerful parallel SQL query engine general transactions change tracking and notiﬁcation and indexing and is built on top of a highly distributed storage system that scales on standard hardware in Google data centers. The store is dynamically sharded supports transactionally-consistent replication across data centers and is able to handle data center outages without data loss. The strong consistency properties of F1 and its storage system come at the cost of higher write latencies compared to MySQL. Having successfully migrated a rich customerfacing application suite at the heart of Google’s ad business to F1 with no downtime we will describe how we restructured schema and applications to largely hide this increased latency from external users. The distributed nature of F1 also allows it to scale easily and to support signiﬁcantly higher throughput for batch workloads than a traditional RDBMS. With F1 we have built a novel hybrid system that combines the scalability fault tolerance transparent sharding and cost beneﬁts so far available only in “NoSQL” systems with the usability familiarity and transactional guarantees expected from an RDBMS.,http://research.google.com/pubs/archive/38125.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=F1+-+The+Fault-Tolerant+Distributed+RDBMS+Supporting+Google's+Ad++Business+Shute+Oancea+Ellner+Handy+Rollins+Samwel+Vingralek+Whipkey+Chen+Jegerlehner+Littlefield+Tong,http://research.google.com/pubs/pub38125.html
Proper Name Transcription/Transliteration with ICU Transforms,34th Internationalization & Unicode Conference (2010),2010,Sascha Brawer Martin Jansche Hiroshi Takenaka Yui Terashima,@inproceedings{36450 title = {Proper Name Transcription/Transliteration with ICU Transforms} author = {Sascha Brawer and Martin Jansche and Hiroshi Takenaka and Yui Terashima} year = 2010 URL = {http://unicodeconference.org/program-d.htm#S4-T3} booktitle = {34th Internationalization & Unicode Conference} },We describe our experience with a deep localization of Google Maps™ where millions of geographic names from diverse origins had to be represented in several target languages including Russian Mandarin and Japanese. For example a map of Western Europe on maps.google.co.jp shows Japanese labels for almost all labeled features. We tackle the problem of transliterating from several source languages into several target languages by pivoting through an explicit intermediate phonetic representation. Each transliteration scheme is implemented as a sequence of ICU transforms reusing a few existing transforms from ICU and CLDR but consisting mostly of transforms that we wrote specifically for this problem. Dividing the problem this way results in many reusable components that make it simple to transliterate between multiple languages. We discuss the steps that go into building transliteration rules describe existing official and de facto standards and guidelines and give suggestions for what to do when no consistent guidelines are available. We provide general recommendations for developing and testing custom ICU transforms. The presentation is available here.,http://unicodeconference.org/program-d.htm#S4-T3,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Proper+Name+Transcription/Transliteration+with+ICU+Transforms+Brawer+Jansche+Takenaka+Terashima,http://research.google.com/pubs/pub36450.html
Incremental Clicks Impact Of Search Advertising,Google Inc. (2011),2011,David Chan Yuan Yuan Jim Koehler Deepak Kumar,@techreport{37161 title = {Incremental Clicks Impact Of Search Advertising} author = {David Chan and Yuan Yuan and Jim Koehler and Deepak Kumar} year = 2011 institution = {Google Inc.} },Advertisers often wonder whether search ads cannibalize their organic traffic. In other words if search ads were paused would clicks on organic results increase and make up for the loss in paid traffic? Google statisticians recently ran over 400 studies on paused accounts to answer this question. In what we call “Search Ads Pause Studies” our group of researchers observed organic click volume in the absence of search ads. Then they built a statistical model to predict the click volume for given levels of ad spend using spend and organic impression volume as predictors. These models generated estimates for the incremental clicks attributable to search ads (IAC) or in other words the percentage of paid clicks that are not made up for by organic clicks when search ads are paused. The results were surprising. On average the incremental ad clicks percentage across verticals is 89%. This means that a full 89% of the traffic generated by search ads is not replaced by organic clicks when ads are paused. This number was consistently high across verticals.,http://research.google.com/pubs/archive/37161.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Incremental+Clicks+Impact+Of+Search+Advertising+Chan+Yuan+Koehler+Kumar,http://research.google.com/pubs/pub37161.html
Profiling a warehouse-scale computer,ISCA '15 Proceedings of the 42nd Annual International Symposium on Computer Architecture ACM (2014) pp. 158-169,2014,Svilen Kanev Juan Darago Kim Hazelwood Parthasarathy Ranganathan Tipp Moseley Gu-Yeon Wei David Brooks,@inproceedings{44271 title = {Profiling a warehouse-scale computer} author = {Svilen Kanev and Juan Darago and Kim Hazelwood and Parthasarathy Ranganathan and Tipp Moseley and Gu-Yeon Wei and David Brooks} year = 2014 booktitle = {ISCA '15 Proceedings of the 42nd Annual International Symposium on Computer Architecture} pages = {158--169} },"With the increasing prevalence of warehouse-scale (WSC) and cloud computing understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding this paper presents a detailed microarchitectural analysis of live datacenter jobs measured on more than 20000 Google machines over a three year period and comprising thousands of different applications. We first find that WSC workloads are extremely diverse breeding the need for architectures that can tolerate application variability without performance loss. However some patterns emerge offering opportunities for co-optimization of hardware and software. For example we identify common building blocks in the lower levels of the software stack. This ""datacenter tax"" can comprise nearly 30% of cycles across jobs running in the fleet which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors especially in the cache hierarchy. Typical workloads place significant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often but compute heavily in bursts. These observations motivate several interesting directions for future warehouse-scale computers.",http://research.google.com/pubs/archive/44271.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Profiling+a+warehouse-scale+computer+Kanev+Darago+Hazelwood+Ranganathan+Moseley+Wei+Brooks,http://research.google.com/pubs/pub44271.html
Web-Scale Job Scheduling,Lecture Notes in Computer Science vol. 7698 (2013),2013,Walfredo Cirne Eitan Frachtenberg,@article{42551 title = {Web-Scale Job Scheduling} author = {Walfredo Cirne and Eitan Frachtenberg} year = 2013 journal = {Lecture Notes in Computer Science} volume = {7698} },Web datacenters and clusters can be larger than the world’s largest supercomputers and run workloads that are at least as heteroge- neous and complex as their high-performance computing counterparts. And yet little is known about the unique job scheduling challenges of these environments. This article aims to ameliorate this situation. It dis- cusses the challenges of running web infrastructure and describes several techniques to address them. It also presents some of the problems that remain open in the field.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web-Scale+Job+Scheduling+Cirne+Frachtenberg,http://research.google.com/pubs/pub42551.html
Online panel research: History concepts applications and a look at the future,Online Panel Research: A Data Quality Perspective Wiley (2014) pp. 1-22,2014,Mario Callegaro Reg Baker Jelke Bethlehem Anja S. Goritz Jon A. Krosnick Paul J. Lavrakas,@inbook{42493 title = {Online panel research: History concepts applications and a look at the future} author = {Mario Callegaro and Reg Baker and Jelke Bethlehem and Anja S. Goritz and Jon A. Krosnick and Paul J. Lavrakas} year = 2014 URL = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html} booktitle = {Online Panel Research: A Data Quality Perspective} pages = {1-22} },In this introductory chapter written by the six editors of this volume we introduce and attempt to systematize the key concepts used when discussing online panels. The connection between Internet penetration and the evolution of panels is discussed as are the different types of online panels their composition and how they are built. Most online panels do not use probability-based methods but some do and the differences are discussed. The chapter also describes in some detail the process of joining a panel answering initial profiling questions and becoming an active panel member. We discuss the most common sampling techniques highlighting their strengths and limitations and touch on techniques to increase representativeness when using a non-probability panel. The variety of incentive methods in current use also is described. Panel maintenance is another key issue since attrition often is substantial and a panel must be constantly refreshed. Online panels can be used to support a wide range of study designs some cross-sectional or and others longitudinal where the same sample members are surveyed multiple times on the same topic. We also discuss industry standards and professional association guidelines for conducting research using online panels. The chapter concludes with a look to the future of online panels and more generally online sampling via means other than classic panels.,http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+panel+research:+History+concepts+applications+and+a+look+at+the+future+Callegaro+Baker+Bethlehem+Goritz+Krosnick+Lavrakas,http://research.google.com/pubs/pub42493.html
Collaborative Filtering for Orkut Communities: Discovery of User Latent Behavior,18th International Conference on World Wide Web (WWW) ACM (2009) pp. 681-690,2009,Wen-Yen Chen Jon Chu Junyi Luan Hongjie Bai Edward Chang,@inproceedings{35270 title = {Collaborative Filtering for Orkut Communities: Discovery of User Latent Behavior} author = {Wen-Yen Chen and Jon Chu and Junyi Luan and Hongjie Bai and Edward Chang} year = 2009 booktitle = {18th International Conference on World Wide Web (WWW)} pages = {681--690} },Users of social networking services can connect with each other by forming communities for online interaction. Yet as the number of communities hosted by such websites grows over time users have even greater need for effective community recommendations in order to meet more users. In this paper we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation. First is association rule mining (ARM) which discovers associations between sets of communities that are shared across many users. Second is latent Dirichlet allocation (LDA) which models user-community co-occurrences using latent aspects. In comparing LDA with ARM we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data. We experiment on an Orkut data set consisting of 492104 users and 118002 communities. We show that LDA consistently performs better than ARM using the top-k recommendations ranking metric and we analyze examples of the latent information learned by LDA to explain this finding. To efficiently handle the large-scale data set we parallelize LDA on distributed computers and demonstrate our parallel implementation's scalability with varying numbers of machines.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Collaborative+Filtering+for+Orkut+Communities:+Discovery+of+User+Latent+Behavior+Chen+Chu+Luan+Bai+Chang,http://research.google.com/pubs/pub35270.html
Probabilistic Distance-based Arbitration: Providing Equality of Service for Many-core CMPs,MICRO43: Proceedings of the 43rd Annual International Symposium on Microarchitecture IEEE/ACM (2010),2010,Michael M. Lee John Kim Dennis Abts Michael Marty Jae W. Lee,@inproceedings{36897 title = {Probabilistic Distance-based Arbitration: Providing Equality of Service for Many-core CMPs} author = {Michael M. Lee and John Kim and Dennis Abts and Michael Marty and Jae W. Lee} year = 2010 booktitle = {MICRO43: Proceedings of the 43rd Annual International Symposium on Microarchitecture} },Emerging many-core chip multiprocessors will integrate dozens of small processing cores with an on-chip interconnect consisting of point-to-point links. The interconnect enables the processing cores to not onl communicate but to share common resources such as main memory resources and I/O controllers. In this work we propose an arbitration scheme to enable equality of service (EoS) in access to a chip's shared resources. That is we seek to remove any bias in a core's access to a shared resource based on its location within the CMP. We propose using probabilistic arbitration combined with distance-based weights to achieve EoSand overcome the limitation of conventional round-robin arbiter. We describe how nonlinear weights need to be used with probabilistic arbiters and propose three different arbitration weight metrics -- fixed weight constantly increasing weight and variably increasing weight. By only modifying the arbitration of an on-chip router we do not require any additional buffers or virtual channels and create a simple low-cost mechanism for achieving EoS. We evaluate our arbitration scheme across a wide range of traffic patterns. In addition to providing EoS the proposed arbitration has additional benefits which include providing quality-of-service features (such as differentiated service) and providing fairness in terms of both throughput and latency that approaches the global fairness achieved with age-base arbitration -- thus providing a more stable network by achieving high sustained throughput beyond saturation.,http://research.google.com/pubs/archive/36897.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Probabilistic+Distance-based+Arbitration:+Providing+Equality+of+Service+for+Many-core+CMPs+Lee+Kim+Abts+Marty+Lee,http://research.google.com/pubs/pub36897.html
A Complete Co-Inductive Syntactic Theory of Sequential Control and State,Semantics and Algebraic Specification: Essays Dedicated to Peter D. Mosses on the Occasion of His 60th Birthday Springer (2009) pp. 329-375,2009,Kristian Støvring Soren B. Lassen,@incollection{36581 title = {A Complete Co-Inductive Syntactic Theory of Sequential Control and State} author = {Kristian Støvring and Soren B. Lassen} year = 2009 URL = {http://dx.doi.org/10.1007/978-3-642-04164-8_17} booktitle = {Semantics and Algebraic Specification: Essays Dedicated to Peter D. Mosses on the Occasion of His 60th Birthday} pages = {329-375} },We present a co-inductive syntactic theory eager normal form bisimilarity for the untyped call-by-value lambda calculus extended with continuations and mutable references. We demonstrate that the associated bisimulation proof principle is easy to use and that it is a powerful tool for proving equivalences between recursive imperative higher-order programs. The theory is modular in the sense that eager normal form bisimilarity for each of the calculi extended with continuations and/or mutable references is a fully abstract extension of eager normal form bisimilarity for its sub-calculi. For each calculus we prove that eager normal form bisimilarity is a congruence and is sound with respect to contextual equivalence. Furthermore for the calculus with both continuations and mutable references we show that eager normal form bisimilarity is complete: it coincides with contextual equivalence. Eager normal form bisimilarity is inspired by Böhm-tree equivalence in the pure lambda calculus. We clarify the associated proof principle by reviewing properties of Böhm trees and surveying previous work on normal form bisimulation. Extended version of an earlier conference paper incorporating parts of Chapter 2 of the first author’s PhD dissertation.,http://dx.doi.org/10.1007/978-3-642-04164-8_17,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Complete+Co-Inductive+Syntactic+Theory+of+Sequential+Control+and+State+St%C3%B8vring+Lassen,http://research.google.com/pubs/pub36581.html
Perfect Reconstructability of Control Flow from Demand Dependence Graphs,Transactions on Architecture and Code Optimization (2014) (to appear),2014,Helge Bahmann Nico Reissmann Magnus Jahre Jan Christian Meyer,@article{43246 title = {Perfect Reconstructability of Control Flow from Demand Dependence Graphs} author = {Helge Bahmann and Nico Reissmann and Magnus Jahre and Jan Christian Meyer} year = 2014 note = {attached pdf not yet copy-proofed by editor} journal = {Transactions on Architecture and Code Optimization} },Functional demand-based dependence graphs such as the Regionalized Value State Dependence Graph are intermediate representations that only model the flow of data and state with implicit and severely restricted control flow. While suitable for formulation of program transformations they require algorithms for conversion from and to representations with explicit control flow such as CFG. Existing solutions exhibit structural constraints limiting quality of generated control flow but we show that this is not intrinsic to RVSDGs. We provide algorithms capable of perfect round-trip conversions prove their correctness and empirically evaluate their run-time performance and representation overhead.,http://research.google.com/pubs/archive/43246.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Perfect+Reconstructability+of+Control+Flow+from+Demand+Dependence+Graphs+Bahmann+Reissmann+Jahre+Meyer,http://research.google.com/pubs/pub43246.html
Internal Access Controls,Communications of the ACM vol. 58 (2015) pp. 62-65,2015,Geetanjali Sampemane,@article{43294 title = {Internal Access Controls} author = {Geetanjali Sampemane} year = 2015 journal = {Communications of the ACM} pages = {62--65} volume = {58} },Trust but verify.,http://research.google.com/pubs/archive/43294.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Internal+Access+Controls+Sampemane,http://research.google.com/pubs/pub43294.html
AdAlyze Redux: Post-Click and Post-Conversion Text Feature Attribution for Sponsored Search Ads,WWW '15 Companion Proceedings of the 24th International Conference on World Wide Web ACM (2015),2015,Thomas Steiner,@inproceedings{44012 title = {AdAlyze Redux: Post-Click and Post-Conversion Text Feature Attribution for Sponsored Search Ads} author = {Thomas Steiner} year = 2015 URL = {http://dl.acm.org/citation.cfm?id=2740908.2741996} booktitle = {WWW '15 Companion Proceedings of the 24th International Conference on World Wide Web} },In this paper we present our ongoing research on an ads quality testing tool that we call AdAlyze Redux. This tool allows advertisers to get individual best practice recommendations based on an expandable set of textual ads features tailored to exactly the ads in an advertiser's set of accounts. This lets them optimize their ad copies against the common online advertising key performance indicators clickthrough rate and if available conversion rate. We choose the Web as the tool's platform and automatically generate the analyses as platform-independent HTML5 slides and full reports.,http://research.google.com/pubs/archive/44012.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=AdAlyze+Redux:+Post-Click+and+Post-Conversion+Text+Feature+Attribution+for+Sponsored+Search+Ads+Steiner,http://research.google.com/pubs/pub44012.html
Revisiting Stein's Paradox: Multi-Task Averaging,Journal Machine Learning Research vol. 15 (2014),2014,Sergey Feldman Maya R. Gupta Bela A. Frigyik,@article{42935 title = {Revisiting Stein's Paradox: Multi-Task Averaging} author = {Sergey Feldman and Maya R. Gupta and Bela A. Frigyik} year = 2014 journal = {Journal Machine Learning Research} volume = {15} },We present a multi-task learning approach to jointly estimate the means of multiple independent distributions from samples. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the individual task's sample averages We derive the optimal amount of regularization for the two task case for the minimum risk estimator and a minimax estimator and show that the optimal amount of regularization can be practically estimated without cross-validation. We extend the practical estimators to an arbitrary number of tasks. Simulations and real data experiments demonstrate the advantage of the proposed MTA estimators over standard averaging and James-Stein estimation.,http://research.google.com/pubs/archive/42935.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Revisiting+Stein's+Paradox:+Multi-Task+Averaging+Feldman+Gupta+Frigyik,http://research.google.com/pubs/pub42935.html
Juggler: Virtual Networks for Fun and Profit,IEEE Transactions on Mobile Computing vol. 9 (2010) pp. 31-43,2010,Anthony J. Nicholson Scott Wolchok Brian D. Noble,@article{36598 title = {Juggler: Virtual Networks for Fun and Profit} author = {Anthony J. Nicholson and Scott Wolchok and Brian D. Noble} year = 2010 URL = {http://www.computer.org/portal/web/csdl/doi?doc=doi/10.1109/TMC.2009.97} journal = {IEEE Transactions on Mobile Computing} pages = {31--43} volume = {9} },There are many situations in which an additional network interface—or two—can provide benefits to a mobile user. Additional interfaces can support parallelism in network flows improve handoff times and provide sideband communication with nearby peers. Unfortunately such benefits are outweighed by the added costs of an additional physical interface. Instead virtual interfaces have been proposed as the solution multiplexing a single physical interface across more than one communication endpoint. However the switching time of existing implementations is too high for some potential applications and the benefits of this approach to real applications are not yet clear. This paper directly addresses these two shortcomings. It describes a link-layer implementation of a virtual 802.11 networking layer called Juggler that achieves switching times of approximately 3 ms and less than 400 \mu{\rm s} in certain conditions. We demonstrate the performance of this implementation on three application scenarios. By devoting 10 percent of the duty cycle to background tasks Juggler can provide nearly instantaneous handoff between base stations or support a modest sideband channel with peer nodes without adversely affecting foreground throughput. Furthermore when the client issues concurrent network flows Juggler is able to assign these flows across more than one AP providing significant speedup when wired-side bandwidth from the AP constrains end-to-end performance.,http://research.google.com/pubs/archive/36598.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Juggler:+Virtual+Networks+for+Fun+and+Profit+Nicholson+Wolchok+Noble,http://research.google.com/pubs/pub36598.html
Lattice-based Minimum Error Rate Training for Statistical Machine Translation,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP) Association for Computational Linguistics pp. 725-734,2008,Wolfgang Macherey Franz Och Ignacio Thayer Jakob Uszkoreit,@inproceedings{34629 title = {Lattice-based Minimum Error Rate Training for Statistical Machine Translation} author = {Wolfgang Macherey and Franz Och and Ignacio Thayer and Jakob Uszkoreit} year = 2008 URL = {http://aclweb.org/anthology-new/D/D08/D08-1076.pdf} booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP)} pages = {725--734} },Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N-best MERT the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.,http://research.google.com/pubs/archive/34629.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Lattice-based+Minimum+Error+Rate+Training+for+Statistical+Machine+Translation+Macherey+Och+Thayer+Uszkoreit,http://research.google.com/pubs/pub34629.html
Adaptive Bound Optimization for Online Convex Optimization,Proceedings of the 23rd Annual Conference on Learning Theory (COLT) (2010),2010,H. Brendan McMahan Matthew Streeter,@inproceedings{36483 title = {Adaptive Bound Optimization for Online Convex Optimization} author = {H. Brendan McMahan and Matthew Streeter} year = 2010 booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)} },We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared and modify it only via a single time-dependent parameter. Our algorithm's regret bounds are worst-case optimal and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent which means they can exploit the structure of the actual problem instance. Critically however our algorithm does not need to know this structure in advance. Rather we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.,http://research.google.com/pubs/archive/36483.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adaptive+Bound+Optimization+for+Online+Convex+Optimization+McMahan+Streeter,http://research.google.com/pubs/pub36483.html
Linear classifiers are nearly optimal when hidden variables have diverse effects,Machine Learning vol. 86 (2012) pp. 209-231,2012,Nader H. Bshouty Philip M. Long,@article{37178 title = {Linear classifiers are nearly optimal when hidden variables have diverse effects} author = {Nader H. Bshouty and Philip M. Long} year = 2012 URL = {http://www.phillong.info/publications/BL11_hidden.pdf} journal = {Machine Learning} pages = {209-231} volume = {86} },We analyze classification problems in which data is generated by a two-tiered random process. The class is generated first then a layer of conditionally independent hidden variables and finally the observed variables. For sources like this the Bayes-optimal rule for predicting the class given the values of the observed variables is a two-layer neural network. We show that if the hidden variables have non-negligible effects on many observed variables a linear classifier approximates the error rate of the Bayes optimal classifier up to lower order terms. We also show that the hinge loss of a linear classifier is not much more than the Bayes error rate which implies that an accurate linear classifier can be found efficiently.,http://www.phillong.info/publications/BL11_hidden.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Linear+classifiers+are+nearly+optimal+when+hidden+variables+have+diverse+effects+Bshouty+Long,http://research.google.com/pubs/pub37178.html
Would a Privacy Fundamentalist Sell Their DNA for $1000...If Nothing Bad Happened as a Result? The Westin Categories Behavioral Intentions and Consequences,Proceedings of the Symposium On Usable Privacy and Security: SOUPS '14 USENIX (2014),2014,Allison Woodruff Vasyl Pihur Sunny Consolvo Lauren Schmidt Laura Brandimarte Alessandro Acquisti,@inproceedings{42847 title = {Would a Privacy Fundamentalist Sell Their DNA for $1000...If Nothing Bad Happened as a Result? The Westin Categories Behavioral Intentions and Consequences} author = {Allison Woodruff and Vasyl Pihur and Sunny Consolvo and Lauren Schmidt and Laura Brandimarte and Alessandro Acquisti} year = 2014 booktitle = {Proceedings of the Symposium On Usable Privacy and Security: SOUPS '14} },Westin's Privacy Segmentation Index has been widely used to measure privacy attitudes and categorize individuals into three privacy groups: fundamentalists pragmatists and unconcerned. Previous research has failed to establish a robust correlation between the Westin categories and actual or intended behaviors. Unexplored however is the connection between the Westin categories and individuals' responses to the consequences of privacy behaviors. We use a survey of 884 Amazon Mechanical Turk participants to investigate the relationship between the Westin Privacy Segmentation Index and attitudes and behavioral intentions for both privacy-sensitive scenarios and privacy-sensitive consequences. Our results indicate a lack of correlation between the Westin categories and consequences. We discuss potential implications of this attitude-consequence gap.,http://research.google.com/pubs/archive/42847.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Would+a+Privacy+Fundamentalist+Sell+Their+DNA+for+$1000...If+Nothing+Bad+Happened+as+a+Result%3F+The+Westin+Categories+Behavioral+Intentions+and+Consequences+Woodruff+Pihur+Consolvo+Schmidt+Brandimarte+Acquisti,http://research.google.com/pubs/pub42847.html
Evolving QWOP gaits,GECCO '14 Proceedings of the 2014 conference on Genetic and evolutionary computation ACM Vancouver pp. 823-830,2014,Steven Ray Vahl Scott Gordon Laurent Vaucher,@inproceedings{42902 title = {Evolving QWOP gaits} author = {Steven Ray and Vahl Scott Gordon and Laurent Vaucher} year = 2014 URL = {http://dl.acm.org/citation.cfm?id=2598248} booktitle = {GECCO '14 Proceedings of the 2014 conference on Genetic and evolutionary computation} pages = {823-830} address = {Vancouver} },QWOP is a popular Flash game in which a human player controls a sprinter in a simulated 100-meter dash. The game is notoriously difficult owing to its ragdoll physics engine and the simultaneous movements that must be carefully coordinated to achieve forward progress. While previous researchers have evolved gaits using simulations similar to QWOP we describe a software interface that connects directly to QWOP itself incorporating a genetic algorithm to evolve actual QWOP gaits. Since QWOP has no API ours detects graphical screen elements and uses them to build a fitness function. Two variable-length encoding schemes that codify sequences of QWOP control commands that loop to form gaits are tested. We then compare the performance of SGA Genitor and a Cellular Genetic Algorithm on this task. Using only the end score as the basis for fitness the cellular algorithm is consistently able to evolve a successful scooting strategy similar to one most humans employ. The results confirm that steady-state GAs are preferred when the task is sensitive to small input variations. Although the limited feedback does not yet produce performance competitive with QWOP champions it is the first autonomous software evolution of successful QWOP gaits.,http://research.google.com/pubs/archive/42902.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evolving+QWOP+gaits+Ray+Gordon+Vaucher,http://research.google.com/pubs/pub42902.html
ViSQOL: an objective speech quality model,EURASIP Journal on Audio Speech and Music Processing vol. 2015 (13) (2015) pp. 1-18,2015,Andrew Hines Jan Skoglund Anil Kokaram Naomi Harte,@article{43990 title = {ViSQOL: an objective speech quality model} author = {Andrew Hines and Jan Skoglund and Anil Kokaram and Naomi Harte} year = 2015 journal = {EURASIP Journal on Audio Speech and Music Processing} pages = {1-18} volume = {2015 (13)} },This paper presents an objective speech quality model ViSQOL the Virtual Speech Quality Objective Listener. It is a signal-based full-reference intrusive metric that models human speech quality perception using a spectro-temporal measure of similarity between a reference and a test speech signal. The metric has been particularly designed to be robust for quality issues associated with Voice over IP (VoIP) transmission. This paper describes the algorithm and compares the quality predictions with the ITU-T standard metrics PESQ and POLQA for common problems in VoIP: clock drift associated time warping and playout delays. The results indicate that ViSQOL and POLQA significantly outperform PESQ with ViSQOL competing well with POLQA. An extensive benchmarking against PESQ POLQA and simpler distance metrics using three speech corpora (NOIZEUS and E4 and the ITU-T P.Sup. 23 database) is also presented. These experiments benchmark the performance for a wide range of quality impairments including VoIP degradations a variety of background noise types speech enhancement methods and SNR levels. The results and subsequent analysis show that both ViSQOL and POLQA have some performance weaknesses and under-predict perceived quality in certain VoIP conditions. Both have a wider application and robustness to conditions than PESQ or more trivial distance metrics. ViSQOL is shown to offer a useful alternative to POLQA in predicting speech quality in VoIP scenarios.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ViSQOL:+an+objective+speech+quality+model+Hines+Skoglund+Kokaram+Harte,http://research.google.com/pubs/pub43990.html
A Class-Based Agreement Model For Generating Accurately Inflected Translations,50th Annual Meeting of the Association for Computational Linguistics (ACL 2012),2012,Spence Green John DeNero,@inproceedings{38107 title = {A Class-Based Agreement Model For Generating Accurately Inflected Translations} author = {Spence Green and John DeNero} year = 2012 booktitle = {50th Annual Meeting of the Association for Computational Linguistics (ACL 2012)} },When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number the output commonly contains morpho-syntactic agreement errors. To address this issue we present a target-side class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders.,http://research.google.com/pubs/archive/38107.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Class-Based+Agreement+Model+For+Generating+Accurately+Inflected+Translations+Green+DeNero,http://research.google.com/pubs/pub38107.html
Comparing In-Browser Methods of Measuring Resource Load Times,W3C Workshop on Web Performance 8 W3C W3C/MIT 32 Vassar Street Room 32-G515 Cambridge MA 02139 USA (2012),2012,Eric Gavaletz Dominic Hamon Jasleen Kaur,@inproceedings{40592 title = {Comparing In-Browser Methods of Measuring Resource Load Times} author = {Eric Gavaletz and Dominic Hamon and Jasleen Kaur} year = 2012 URL = {http://www.w3.org/2012/10/unc.edu.perf.paper.pdf} booktitle = {W3C Workshop on Web Performance 8} address = {W3C/MIT 32 Vassar Street Room 32-G515 Cambridge MA 02139 USA} },When looking for an excellent platform for conducting end_-to-_end network performance measurement that is large_-scale and representative researchers should look no further than the browser___ -- after all browsers are installed everywhere and are used multiple times per day by most Internet users. In this work we investigate the use of the DOM XHR and Navigation Timing API for measuring HTTP response times within browsers with the goal of estimating path latency and throughput. The response times are measured using a set of popular browsers in a controlled environment___this helps us isolate the differences between the browsers as well as study how closely the measurements match the ground truth. We show that in general the XHR method yields the most consistent measurements across browsers but that the new Navigation Timing and the proposed Resource Timing APIs could change that. We also use the measurements from our controlled environment to study the impact of each of our investigated measurement methods on a hypothetical measurement study.,http://research.google.com/pubs/archive/40592.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Comparing+In-Browser+Methods+of+Measuring+Resource+Load+Times+Gavaletz+Hamon+Kaur,http://research.google.com/pubs/pub40592.html
Question Identification on Twitter Accepted by CIKM 2011,Proceedings of the 20th ACM international conference on Information and knowledge management ACM New York NY USA (2011),2011,Baichuan Li Xiance Si Michael R. Lyu Irwin King Edward Y. Chang,@inproceedings{37565 title = {Question Identification on Twitter Accepted by CIKM 2011} author = {Baichuan Li and Xiance Si and Michael R. Lyu and Irwin King and Edward Y. Chang} year = 2011 URL = {http://dl.acm.org/citation.cfm?id=2063996} booktitle = {Proceedings of the 20th ACM international conference on Information and knowledge management} address = {New York NY USA} },In this paper we investigate the novel problem of auto- matic question identification in the microblog environment. It contains two steps: detecting tweets that contain ques- tions (we call them “interrogative tweets”) and extracting the tweets which really seek information or ask for help (so called “qweets”) from interrogative tweets. To detect inter- rogative tweets both traditional rule-based approach and state-of-the-art learning-based method are employed. To extract qweets context features like short urls and Tweet- specific features like Retweets are elaborately selected for classification. We conduct an empirical study with sampled one hour’s English tweets and report our experimental re- sults for question identification on Twitter.,http://research.google.com/pubs/archive/37565.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Question+Identification+on+Twitter+Accepted+by+CIKM+2011+Li+Si+Lyu+King+Chang,http://research.google.com/pubs/pub37565.html
1ML - core and modules united (F-ing first-class modules),International Conference on Functional Programming ACM-SIGPLAN Vancouver Canada (2015),2015,Andreas Rossberg,@inproceedings{43980 title = {1ML - core and modules united (F-ing first-class modules)} author = {Andreas Rossberg} year = 2015 URL = {https://www.mpi-sws.org/~rossberg/1ml/} booktitle = {International Conference on Functional Programming} address = {Vancouver Canada} },"ML is two languages in one: there is the core with types and expressions and there are modules with signatures structures and functors. Modules form a separate higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet it creates substantial duplication in syntax and semantics and it reduces expressiveness. For example selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However they remedy expressiveness only to some extent are syntactically cumbersome and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values and core and module layer are unified into one language. In this ""1ML"" functions functors and even type constructors are one and the same construct; likewise no distinction is made between structures records or tuples. Or viewed the other way round everything is just (""a mode of use of"") modules. Yet 1ML does not require dependent types and its type structure is expressible in terms of plain System F_ in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete but we argue not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System F_ that allows combining term and type abstraction in a more compositional manner than the bare calculus.",http://research.google.com/pubs/archive/43980.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=1ML+-+core+and+modules+united+(F-ing+first-class+modules)+Rossberg,http://research.google.com/pubs/pub43980.html
Computers and iPhones and Mobile Phones oh my! A logs-based comparison of search users on different devices,WWW 2009 MADRID pp. 801-810,2009,Maryam Kamvar Melanie Kellar Rajan Patel Ya Xu,@inproceedings{35252 title = {Computers and iPhones and Mobile Phones oh my! A logs-based comparison of search users on different devices} author = {Maryam Kamvar and Melanie Kellar and Rajan Patel and Ya Xu} year = 2009 booktitle = {WWW 2009 MADRID} pages = {801-810} },"We present a logs-based comparison of search patterns across three platforms: computers iPhones and conventional mobile phones. Our goal is to understand how mobile search users differ from computer-based search users and we focus heavily on the distribution and variability of tasks that users perform from each platform. The results suggest that search usage is much more focused for the average mobile user than for the average computer-based user. However search behavior on high-end phones resembles computer-based search behavior more so than mobile search behavior. A wide variety of implications follow from these findings. First there is no single search interface which is suitable for all mobile phones. We suggest that for the higher-end phones a close integration with the standard computer-based interface (in terms of personalization and available feature set) would be beneficial for the user since these phones seem to be treated as an extension of the users' computer. For all other phones there is a huge opportunity for personalizing the search experience for the user's ""mobile needs"" as these users are likely to repeatedly search for a single type of information need on their phone.",http://research.google.com/pubs/archive/35252.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Computers+and+iPhones+and+Mobile+Phones+oh+my!+A+logs-based+comparison+of+search+users+on+different+devices+Kamvar+Kellar+Patel+Xu,http://research.google.com/pubs/pub35252.html
Unary Data Structures for Language Models,Interspeech 2011 International Speech Communication Association pp. 1425-1428,2011,Jeffrey Sorensen Cyril Allauzen,@inproceedings{37218 title = {Unary Data Structures for Language Models} author = {Jeffrey Sorensen and Cyril Allauzen} year = 2011 booktitle = {Interspeech 2011} pages = {1425-1428} },Language models are important components of speech recognition and machine translation systems. Trained on billions of words and consisting of billions of parameters language models often are the single largest components of these systems. There have been many proposed techniques to reduce the storage requirements for language models. A technique based upon pointer-free compact storage of ordinal trees shows compression competitive with the best proposed systems while retaining the full finite state structure and without using computationally expensive block compression schemes or lossy quantization techniques.,http://research.google.com/pubs/archive/37218.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unary+Data+Structures+for+Language+Models+Sorensen+Allauzen,http://research.google.com/pubs/pub37218.html
Planet-Planet Scattering Leads to Tightly Packed Planetary Systems.,The Astrophysical Journal Letters vol. 696 (2011) pp. 98-101,2011,Sean N. Raymond Rory Barnes Dimitri Veras Philip J. Armitage Noel Gorelick Richard Greenberg,@article{42927 title = {Planet-Planet Scattering Leads to Tightly Packed Planetary Systems.} author = {Sean N. Raymond and Rory Barnes and Dimitri Veras and Philip J. Armitage and Noel Gorelick and Richard Greenberg} year = 2011 journal = {The Astrophysical Journal Letters} pages = {98-101} volume = {696} },"The known extrasolar multiple-planet systems share a surprising dynamical attribute: they cluster just beyond the Hill stability boundary. Here we show that the planet-planet scattering model which naturally explains the observed exoplanet eccentricity distribution can reproduce the observed distribution of dynamical configurations. We calculated how each of our scattered systems would appear over an appropriate range of viewing geometries; as Hill stability is weakly dependent on the masses the mass-inclination degeneracy does not significantly affect our results. We consider a wide range of initial planetary mass distributions and find that some are poor fits to the observed systems. In fact many of our scattering experiments overproduce systems very close to the stability boundary. The distribution of dynamical configurations of two-planet systems actually may provide better discrimination between scattering models than the distribution of eccentricity. Our results imply that at least in their inner regions which are weakly affected by gas or planetesimal disks planetary systems should be ""packed"" with no large gaps between planets.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Planet-Planet+Scattering+Leads+to+Tightly+Packed+Planetary+Systems.+Raymond+Barnes+Veras+Armitage+Gorelick+Greenberg,http://research.google.com/pubs/pub42927.html
Real World Functional Programming,Manning 20 Baldwin Road PO Box 261 Shelter Island NY 11964 (2010) pp. 500,2010,Tomas Petricek Jon Skeet,@book{37266 title = {Real World Functional Programming} author = {Tomas Petricek and Jon Skeet} year = 2010 booktitle = {Real World Functional Programming} pages = {500} address = {20 Baldwin Road PO Box 261 Shelter Island NY 11964} },Functional programming languages like F# Erlang and Scala are attracting attention as an efficient way to handle the new requirements for programming multi-processor and high-availability applications. Microsoft's new F# is a true functional language and C# uses functional language features for LINQ and other recent advances. Real World Functional Programming is a unique tutorial that explores the functional programming model through the F# and C# languages. The clearly presented ideas and examples teach readers how functional programming differs from other approaches. It explains how ideas look in F#-a functional language-as well as how they can be successfully used to solve programming problems in C#. Readers build on what they know about .NET and learn where a functional approach makes the most sense and how to apply it effectively in those cases.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Real+World+Functional+Programming+Petricek+Skeet,http://research.google.com/pubs/pub37266.html
Ontological Supervision for Fine Grained Classification of Street View Storefronts,CVPR15 (2015),2015,Yair Movshovitz-Attias Qian Yu Martin C. Stumpe Vinay Shet Sacha Arnoud Liron Yatziv,@inproceedings{43794 title = {Ontological Supervision for Fine Grained Classification of Street View Storefronts} author = {Yair Movshovitz-Attias and Qian Yu and Martin C. Stumpe and Vinay Shet and Sacha Arnoud and Liron Yatziv} year = 2015 URL = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Movshovitz-Attias_Ontological_Supervision_for_2015_CVPR_paper.pdf} booktitle = {CVPR15} },Modern search engines receive large numbers of business related local aware queries. Such queries are best answered using accurate up-to-date business listings that contain representations of business categories. Creating such listings is a challenging task as businesses often change hands or close down. For businesses with street side locations one can leverage the abundance of street level imagery such as Google Street View to automate the process. However while data is abundant labeled data is not; the limiting factor is creation of large scale labeled training data. In this work we utilize an ontology of geographical concepts to automatically propagate business category information and create a large multi label training dataset for fine grained storefront classification. Our learner which is based on the GoogLeNet/Inception Deep Convolutional Network architecture and classifies 208 categories achieves human level accuracy.,http://research.google.com/pubs/archive/43794.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Ontological+Supervision+for+Fine+Grained+Classification+of+Street+View+Storefronts+Movshovitz-Attias+Yu+Stumpe+Shet+Arnoud+Yatziv,http://research.google.com/pubs/pub43794.html
Distributed Acoustic Modeling with Back-off N-grams,Proceedings of ICASSP 2012 IEEE pp. 4129-4132,2012,Ciprian Chelba Peng Xu Fernando Pereira Thomas Richardson,@inproceedings{37681 title = {Distributed Acoustic Modeling with Back-off N-grams} author = {Ciprian Chelba and Peng Xu and Fernando Pereira and Thomas Richardson} year = 2012 booktitle = {Proceedings of ICASSP 2012} pages = {4129-4132} },The paper proposes an approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data and model size (as measured by the number of parameters in the model) to approximately 100 times larger than current sizes used in ASR. Dealing with unseen phonetic contexts is accomplished using the familiar back-off technique used in language modeling due to implementation simplicity. The new acoustic model is estimated and stored using the MapReduce distributed computing infrastructure. Speech recognition experiments are carried out in an Nbest rescoring framework for Google Voice Search. 87000 hours of training data is obtained in an unsupervised fashion by ﬁltering utterances in Voice Search logs on ASR conﬁdence. The resulting models are trained using maximum likelihood and contain 20-40 million Gaussians. They achieve relative reductions in WER of 11% and 6% over first-pass models trained using maximum likelihood and boosted MMI respectively.,http://research.google.com/pubs/archive/37681.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Acoustic+Modeling+with+Back-off+N-grams+Chelba+Xu+Pereira+Richardson,http://research.google.com/pubs/pub37681.html
OpenFst: An Open-Source Weighted Finite-State Transducer Library and its Applications to Speech and Language,Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT) 2009 conference Tutorials,2009,Michael Riley Cyril Allauzen Martin Jansche,@inproceedings{35189 title = {OpenFst: An Open-Source Weighted Finite-State Transducer Library and its Applications to Speech and Language} author = {Michael Riley and Cyril Allauzen and Martin Jansche} year = 2009 URL = {http://aclweb.org/anthology/N09-4005} booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics -- Human Language Technologies (NAACL HLT) 2009 conference Tutorials} },Finite-state methods are well established in language and speech processing. OpenFst (available from www.openfst.org) is a free and open-source software library for building and using ﬁnite automata in particular weighted ﬁnite-state transducers (FSTs). This tutorial is an introduction to weighted ﬁnitestate transducers and their uses in speech and language processing. While there are other weighted ﬁnite-state transducer libraries OpenFst (a) offers we believe the most comprehensive general and efﬁcient set of operations; (b) makes available full source code; (c) exposes high- and low-level C++ APIs that make it easy to embed and extend; and (d) is a platform for active research and use among many colleagues.,http://research.google.com/pubs/archive/35189.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=OpenFst:+An+Open-Source+Weighted+Finite-State+Transducer+Library+and+its+Applications+to+Speech+and+Language+Riley+Allauzen+Jansche,http://research.google.com/pubs/pub35189.html
Theoretical Advantages of Lenient Learners: An Evolutionary Game Theoretic Perspective,Journal of Machine Learning Research (2008),2008,Liviu Panait Karl Tuyls Sean Luke,@article{33014 title = {Theoretical Advantages of Lenient Learners: An Evolutionary Game Theoretic Perspective} author = {Liviu Panait and Karl Tuyls and Sean Luke} year = 2008 journal = {Journal of Machine Learning Research} },This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergenceguarantees for these algorithms and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. We demonstrate that lenience provides learners with more accurate information about the benefits of performing their actions resulting in higher likelihood of convergence to the globally optimal solution. In addition our analysis indicates that the choice of learning algorithm has an insignificant impact on the overall performance of multiagent learning algorithms; rather the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally our research supports the strength and generality of evolutionary game theory as a backbone for multiagent learning.,http://research.google.com/pubs/archive/33014.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Theoretical+Advantages+of+Lenient+Learners:+An+Evolutionary+Game+Theoretic+Perspective+Panait+Tuyls+Luke,http://research.google.com/pubs/pub33014.html
Efficient Estimation of Quantiles in Missing Data Models,Google Inc. 111 8th Avenue (2015),2015,Ivan Diaz,@techreport{44645 title = {Efficient Estimation of Quantiles in Missing Data Models} author = {Ivan Diaz} year = 2015 URL = {http://arxiv.org/abs/1512.08110} institution = {Google Inc.} },We present a method to estimate the quantile of a variable subject to missingness under the missing at random assumption. Our proposed estimator is locally efficient root-n-consistent asymptotically normal and doubly robust under regularity conditions. We use Monte Carlo simulation to compare our proposal to the one-step and inverse-probability weighted estimators. Our estimator is superior to both competitors with a mean squared error up to 8 times smaller than the one-step estimator and up to 2.5 times smaller than an inverse probability weighted estimator. We develop extensions for estimating the causal effect of treatment on a population quantile among the treated. Our methods are motivated by an application with a heavy tailed continuous outcome. In this situation the efficiency bound for estimating the effect on the mean is often large or infinite ruling out root-n-consistent inference and reducing the power for testing hypothesis of no treatment effect. Using quantiles (e.g. the median) may yield more accurate measures of the treatment effect along with more powerful hypothesis tests. In our application the proposed estimator of the effect on the median yields hypothesis tests of no treatment effect up to two times more powerful and its variance is up to four times smaller than the variance of its mean counterpart.,http://research.google.com/pubs/archive/44645.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Estimation+of+Quantiles+in+Missing+Data+Models+Diaz,http://research.google.com/pubs/pub44645.html
Learning Fine-grained Image Similarity with Deep Ranking,CVPR'2014 IEEE,2014,Jiang Wang Yang Song Thomas Leung Chuck Rosenberg Jingbin Wang James Philbin Bo Chen Ying Wu,@inproceedings{42945 title = {Learning Fine-grained Image Similarity with Deep Ranking} author = {Jiang Wang and Yang Song and Thomas Leung and Chuck Rosenberg and Jingbin Wang and James Philbin and Bo Chen and Ying Wu} year = 2014 booktitle = {CVPR'2014} },Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images. It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.,http://research.google.com/pubs/archive/42945.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Fine-grained+Image+Similarity+with+Deep+Ranking+Wang+Song+Leung+Rosenberg+Wang+Philbin+Chen+Wu,http://research.google.com/pubs/pub42945.html
Trust but Verify: Predicting Contribution Quality for Knowledge Base Construction and Curation,WSDM (2014) (to appear),2014,Chun How Tan Eugene Agichtein Panos Ipeirotis Evgeniy Gabrilovich,@inproceedings{42023 title = {Trust but Verify: Predicting Contribution Quality for Knowledge Base Construction and Curation} author = {Chun How Tan and Eugene Agichtein and Panos Ipeirotis and Evgeniy Gabrilovich} year = 2014 booktitle = {WSDM} },The largest publicly available knowledge repositories such as Wikipedia and Freebase owe their existence and growth to volunteer contributors around the globe. While the majority of contributions are correct errors can still creep in due to editors' carelessness misunderstanding of the schema malice or even lack of accepted ground truth. If left undetected inaccuracies often degrade the experience of users and the performance of applications that rely on these knowledge repositories. We present a new method CQUAL for automatically predicting the quality of contributions submitted to a knowledge base. Significantly expanding upon previous work our method holistically exploits a variety of signals including the user's domains of expertise as reflected in her prior contribution history and the historical accuracy rates of different types of facts. In a large-scale human evaluation our method exhibits precision of 91% at 80% recall. Our model verifies whether a contribution is correct immediately after it is submitted significantly alleviating the need for post-submission human reviewing.,http://research.google.com/pubs/archive/42023.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trust+but+Verify:+Predicting+Contribution+Quality+for+Knowledge+Base+Construction+and+Curation+Tan+Agichtein+Ipeirotis+Gabrilovich,http://research.google.com/pubs/pub42023.html
Kernelized Structural SVM Learning for Supervised Object Segmentation,Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 2011,2011,Luca Bertelli Tianli Yu Diem Vu Burak Gokturk,@inproceedings{36985 title = {Kernelized Structural SVM Learning for Supervised Object Segmentation} author = {Luca Bertelli and Tianli Yu and Diem Vu and Burak Gokturk} year = 2011 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 2011} },Object segmentation needs to be driven by top-down knowledge to produce semantically meaningful results. In this paper we propose a supervised segmentation approach that tightly integrates object-level top down information with low-level image cues. The information from the two levels is fused under a kernelized structural SVM learning framework. We defined a novel nonlinear kernel for comparing two image-segmentation masks. This kernel combines four different kernels: the object similarity kernel the object shape kernel the per-image color distribution kernel and the global color distribution kernel. Our experiments show that the structured SVM algorithm finds bad segmentations of the training examples given the current scoring function and punishes these bad segmentations to lower scores than the example (good) segmentations. The result is a segmentation algorithm that not only knows what good segmentations are but also learns potential segmentation mistakes and tries to avoid them. Our proposed approach can obtain comparable performance to other state-of-the-art top-down driven segmentation approaches yet is flexible enough to be applied to widely different domains.,http://research.google.com/pubs/archive/36985.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Kernelized+Structural+SVM+Learning+for+Supervised+Object+Segmentation+Bertelli+Yu+Vu+Gokturk,http://research.google.com/pubs/pub36985.html
Probabilistic Analysis of Localized DNA Hybridization Circuits,ACS Synthetic Biology (2015),2015,Neil Dalchau Harish Chandran Nikhil Gopalkrishnan Andrew Phillips John Reif,@article{43834 title = {Probabilistic Analysis of Localized DNA Hybridization Circuits} author = {Neil Dalchau and Harish Chandran and Nikhil Gopalkrishnan and Andrew Phillips and John Reif} year = 2015 URL = {http://pubs.acs.org/doi/abs/10.1021/acssynbio.5b00044} journal = {ACS Synthetic Biology} },Molecular devices made of nucleic acids can perform complex information processing tasks at the nanoscale with potential applications in biofabrication and smart therapeutics. However limitations in the speed and scalability of such devices in a well-mixed setting can significantly affect their performance. In this paper we propose designs for localized circuits involving DNA molecules that are arranged on addressable substrates and interact via hybridization reactions. We propose designs for localized elementary logic circuits which we compose to produce more complex devices including a circuit for computing the square root of a four bit number. We develop an efficient method for probabilistic model-checking of localized circuits which we implement within the Visual DSD design tool. We use this method to prove the correctness of our circuits with respect to their functional specifications and to analyze their performance over a broad range of local rate parameters. Specifically we analyze the extent to which our localized designs can overcome the limitations of well-mixed circuits with respect to speed and scalability. To provide an estimate of local rate parameters we propose a biophysical model of localized hybridization. Finally we use our analysis to identify constraints in the rate parameters that enable localized circuits to retain their advantages in the presence of unintended interferences between strands.,http://pubs.acs.org/doi/abs/10.1021/acssynbio.5b00044,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Probabilistic+Analysis+of+Localized+DNA+Hybridization+Circuits+Dalchau+Chandran+Gopalkrishnan+Phillips+Reif,http://research.google.com/pubs/pub43834.html
Web Coverage in the UK and its Potential Impact on General Population Web Surveys,Web surveys for the general population: How why and when? 25-26 February 2013. Institute of Education London (2013),2013,Mario Callegaro,@inproceedings{41149 title = {Web Coverage in the UK and its Potential Impact on General Population Web Surveys} author = {Mario Callegaro} year = 2013 URL = {http://www.natcenweb.co.uk/genpopweb/documents/Mario-Callegaro.pdf} booktitle = {Web surveys for the general population: How why and when?} },Mario Callegaro (Google UK) provided some data on internet access in the UK and the digital divide. He concluded that the UK internet access is steadily increasing and is likely to soon reach a level of almost universal coverage. But high coverage does not imply that everyone with access would be capable or willing to take part in web surveys. Furthermore internet access is becoming mobile (e.g. Smartphone) and respondents are using a wide variety of devices to answer web surveys. Making web surveys,http://www.natcenweb.co.uk/genpopweb/documents/Mario-Callegaro.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Coverage+in+the+UK+and+its+Potential+Impact+on+General+Population+Web+Surveys+Callegaro,http://research.google.com/pubs/pub41149.html
Towards Energy Proportionality for Large-Scale Latency-Critical Workloads,Proceedings of the 41th Annual International Symposium on Computer Architecture ACM (2014),2014,David Lo Liqun Cheng Rama Govindaraju Luiz André Barroso Christos Kozyrakis,@inproceedings{42523 title = {Towards Energy Proportionality for Large-Scale Latency-Critical Workloads} author = {David Lo and Liqun Cheng and Rama Govindaraju and Luiz André Barroso and Christos Kozyrakis} year = 2014 booktitle = {Proceedings of the 41th Annual International Symposium on Computer Architecture} },Reducing the energy footprint of warehouse-scale computer (WSC) systems is key to their affordability yet difficult to achieve in practice. The lack of energy proportionality of typical WSC hardware and the fact that important workloads (such as search) require all servers to remain up regardless of traffic intensity renders existing power management techniques ineffective at reducing WSC energy use. We present PEGASUS a feedback-based controller that significantly improves the energy proportionality of WSC systems as demonstrated by a real implementation in a Google search cluster. PEGASUS uses request latency statistics to dynamically adjust server power management limits in a fine-grain manner running each server just fast enough to meet global service-level latency objectives. In large cluster experiments PEGASUS reduces power consumption by up to 20%. We also estimate that a distributed version of PEGASUS can nearly double these savings.,http://research.google.com/pubs/archive/42523.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Towards+Energy+Proportionality+for+Large-Scale+Latency-Critical+Workloads+Lo+Cheng+Govindaraju+Barroso+Kozyrakis,http://research.google.com/pubs/pub42523.html
Learning Multiple Non-Linear Sub-Spaces using K-RBMs,Computer Vision and Pattern Recognition (2013),2013,Siddhartha Chandra Shailesh Kumar C. V. Jawahar,@inproceedings{41320 title = {Learning Multiple Non-Linear Sub-Spaces using K-RBMs} author = {Siddhartha Chandra and Shailesh Kumar and C. V. Jawahar} year = 2013 URL = {http://researchweb.iiit.ac.in/~siddhartha.chandra/CVPR_CRDraft.pdf} booktitle = {Computer Vision and Pattern Recognition} },Understanding the nature of data is the key to building good representations. In domains such as natural images the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution they lie in multiple nonlinear subspaces. We propose a framework that uses K-Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features which we use to build image representations. Our algorithm solves the coupled problem of ﬁnding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classiﬁcation datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.,http://research.google.com/pubs/archive/41320.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Multiple+Non-Linear+Sub-Spaces+using+K-RBMs+Chandra+Kumar+Jawahar,http://research.google.com/pubs/pub41320.html
Temporospatial SDN for Aerospace Communications,AIAA SPACE 2015 Conference and Exposition American Institute of Aeronautics and Astronautics (2016) (to appear),2016,Brian Barritt Wesley Eddy,@inproceedings{43929 title = {Temporospatial SDN for Aerospace Communications} author = {Brian Barritt and Wesley Eddy} year = 2016 URL = {http://dx.doi.org/10.2514/6.2015-4656} booktitle = {AIAA SPACE 2015 Conference and Exposition} },This paper describes the development of new methods and software leveraging Software Defined Networking (SDN) technology that has become common in terrestrial networking. We are using SDN to improve the state-of-the-art in design and operation of aerospace communication networks. SDN enables the implementation of services and applications that control monitor and reconfigure the network layer and switching functionality. SDN provides a software abstraction layer that yields a logically centralized view of the network for control plane services and applications. Recently new requirements have led to proposals to extend this concept for Software-Defined Wireless Networks (SDWN) which decouple radio control functions such as spectrum management mobility management and interference management from the radio data-plane. By combining these concepts with high-fidelity modeling of predicted mobility patterns and wireless communications models we can enable SDN applications that optimally and autonomously handle aerospace network operations including steerable beam control RF interference mitigation and network routing updates. This approach is specifically applicable to new constellation designs for LEO relay networks that include hundreds or thousands of spacecraft serving millions of users and exceed the ability of legacy network management tools.,http://research.google.com/pubs/archive/43929.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Temporospatial+SDN+for+Aerospace+Communications+Barritt+Eddy,http://research.google.com/pubs/pub43929.html
The Go Frontend for GCC,GCC Summit 2010 (to appear),2010,Ian Lance Taylor,@inproceedings{36902 title = {The Go Frontend for GCC} author = {Ian Lance Taylor} year = 2010 booktitle = {GCC Summit 2010} },A description of the Go language frontend for gcc. This is a new frontend which is a complete implementation of the new Go programming language. The frontend is currently some 50000 lines of C++ code and uses its own IR which is then converted to GENERIC. I describe the structure of the frontend and the IR issues that arise when compiling the Go language and issues with hooking up any frontend to the gcc middle-end.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Go+Frontend+for+GCC+Taylor,http://research.google.com/pubs/pub36902.html
Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning,Advances in Neural Information Processing Systems (NIPS) (2014),2014,H. Brendan McMahan Matthew Streeter,@article{43138 title = {Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning} author = {H. Brendan McMahan and Matthew Streeter} year = 2014 journal = {Advances in Neural Information Processing Systems (NIPS)} },We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods we develop algorithms that adapt not only to the sequence of gradients but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more) our new algorithms perform significantly better than standard adaptive gradient methods.,http://research.google.com/pubs/archive/43138.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Delay-Tolerant+Algorithms+for+Asynchronous+Distributed+Online+Learning+McMahan+Streeter,http://research.google.com/pubs/pub43138.html
Computational complexity of time-dependent density functional theory,New Journal of Physics vol. 16 (2014) pp. 083035,2014,J D Whitfield M-H Yung D G Templ S Boixo A Aspuru-Guzik,@article{43349 title = {Computational complexity of time-dependent density functional theory} author = {J D Whitfield and M-H Yung and D G Templ and S Boixo and A Aspuru-Guzik} year = 2014 URL = {http://dx.doi.org/10.1088/1367-2630/16/8/083035} journal = {New Journal of Physics} pages = {083035} volume = {16} },Time-dependent density functional theory (TDDFT) is rapidly emerging as a premier method for solving dynamical many-body problems in physics and chemistry. The mathematical foundations of TDDFT are established through the formal existence of a fictitious non-interacting system (known as the Kohn–Sham system) which can reproduce the one-electron reduced probability density of the actual system. We build upon these works and show that on the interior of the domain of existence the Kohn–Sham system can be efficiently obtained given the time-dependent density. We introduce a V-representability parameter which diverges at the boundary of the existence domain and serves to quantify the numerical difficulty of constructing the Kohn–Sham potential. For bounded values of V-representability we present a polynomial time quantum algorithm to generate the time-dependent Kohn–Sham potential with controllable error bounds.,http://research.google.com/pubs/archive/43349.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Computational+complexity+of+time-dependent+density+functional+theory+Whitfield+Yung+Temple+Boixo+Aspuru-Guzik,http://research.google.com/pubs/pub43349.html
The Need for Open Source Software in Machine Learning,Journal of Machine Learning Research vol. 8 (2007) pp. 2443-2466,2007,Soren Sonnenburg Mikio L. Braun Cheng Soon Ong Samy Bengio Leon Bottou Geoff Holmes Yann LeCun Klaus-Robert Mueller Fernando Pereira Carl-Edward Rasmussen Gunnar Raetsch Bernhard Schoelkopf Alexander Smola Pascal Vincent Jason Weston Robert C. Williamson,@article{33029 title = {The Need for Open Source Software in Machine Learning} author = {Soren Sonnenburg and Mikio L. Braun and Cheng Soon Ong and Samy Bengio and Leon Bottou and Geoff Holmes and Yann LeCun and Klaus-Robert Mueller and Fernando Pereira and Carl-Edward Rasmussen and Gunnar Raetsch and Bernhard Schoelkopf and Alexander Smola and Pascal Vincent and Jason Weston and Robert C. Williamson} year = 2007 URL = {http://www.jmlr.org/papers/volume8/sonnenburg07a/sonnenburg07a.pdf} journal = {Journal of Machine Learning Research} pages = {2443--2466} volume = {8} },Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However the true potential of these methods is not utilized since existing implementations are not openly shared resulting in software with low usability and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.,http://research.google.com/pubs/archive/33029.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Need+for+Open+Source+Software+in+Machine+Learning+Sonnenburg+Braun+Ong+Bengio+Bottou+Holmes+LeCun+Mueller+Pereira+Rasmussen+Raetsch+Schoelkopf+Smola+Vincent+Weston+Williamson,http://research.google.com/pubs/pub33029.html
Low-Overhead Network-on-Chip Support for Location-Oblivious Task Placement,IEEE Transactions on Computers vol. Volume 63 Issue 6 (2014) pp. 1487 - 1500,2014,Gwangsun Kim Lee M.M.-J. John Kim Dennis Abts Michael R. Marty,@article{42857 title = {Low-Overhead Network-on-Chip Support for Location-Oblivious Task Placement} author = {Gwangsun Kim and Lee M.M.-J. and John Kim and Dennis Abts and Michael R. Marty} year = 2014 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6319294&queryText%3DLow-overhead+Network-on-chip+Support+for+Location} journal = {IEEE Transactions on Computers} pages = {1487 - 1500} volume = {Volume 63 Issue 6} },Many-core processors will have many processing cores with a network-on-chip (NoC) that provides access to shared resources such as main memory and on-chip caches. However locally-fair arbitration in multi-stage NoC can lead to globally unfair access to shared resources and impact system-level performance depending on where each task is physically placed. In this work we propose an arbitration to provide equality-of-service (EoS) in the network and provide support for location-oblivious task placement. We propose using probabilistic arbitration combined with distance-based weights to achieve EoS and overcome the limitation of round-robin arbiter. However the complexity of probabilistic arbitration results in high area and long latency which negatively impacts performance. In order to reduce the hardware complexity we propose an hybrid arbiter that switches between a simple arbiter at low load and a complex arbiter at high load. The hybrid arbiter is enabled by the observation that arbitration only impacts the overall performance and global fairness at a high load. We evaluate our arbitration scheme with synthetic traffic patterns and GPGPU benchmarks. Our results shows that hybrid arbiter that combines round-robin arbiter with probabilistic distance-based arbitration reduces performance variation as task placement is varied and also improves average IPC.,http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6319294&queryText%3DLow-overhead+Network-on-chip+Support+for+Location,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Low-Overhead+Network-on-Chip+Support+for+Location-Oblivious+Task+Placement+Kim+M.M.-J.+Kim+Abts+Marty,http://research.google.com/pubs/pub42857.html
Automated Decomposition of Build Targets,Proceedings of the 37th International Conference on Software Engineering IEEE Computer Society (2015) pp. 123-133,2015,Mohsen Vakilian Raluca Sauciuc J. David Morgenthaler Vahab Mirrokni,@inproceedings{42249 title = {Automated Decomposition of Build Targets} author = {Mohsen Vakilian and Raluca Sauciuc and J. David Morgenthaler and Vahab Mirrokni} year = 2015 URL = {http://conferences.computer.org/icse/2015/content/papers/1934a123.pdf} booktitle = {Proceedings of the 37th International Conference on Software Engineering} pages = {123--133} },A (build) target specifies the information that is needed to automatically build a software artifact. This paper focuses on underutilized targets—an important dependency problem that we identified at Google. An underutilized target is one with files not needed by some of its dependents. Underutilized targets result in less modular code overly large artifacts slow builds and unnecessary build and test triggers. To mitigate these problems programmers decompose underutilized targets into smaller targets. However manually decomposing a target is tedious and error-prone. Although we prove that finding the best target decomposition is NP-hard we introduce a greedy algorithm that proposes a decomposition through iterative unification of the strongly connected components of the target. Our tool found that 19994 of 40000 Java library targets at Google can be decomposed to at least two targets. The results show that our tool is (1) efficient because it analyzes a target in two minutes on average and (2) effective because for each of 1010 targets it would save at least 50% of the total execution time of the tests triggered by the target.,http://research.google.com/pubs/archive/42249.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automated+Decomposition+of+Build+Targets+Vakilian+Sauciuc+Morgenthaler+Mirrokni,http://research.google.com/pubs/pub42249.html
Drivers and applications of optical technologies for Internet Data Center networks,Optical Fiber Communication Conference and Exposition (OFC/NFOEC) 2011 and the National Fiber Optic Engineers Conference pp. 1-3,2011,Cedric F. Lam Paul Schultz Bikash Koley,@inproceedings{37208 title = {Drivers and applications of optical technologies for Internet Data Center networks} author = {Cedric F. Lam and Paul Schultz and Bikash Koley} year = 2011 booktitle = {Optical Fiber Communication Conference and Exposition (OFC/NFOEC) 2011 and the National Fiber Optic Engineers Conference} pages = {1-3} },The rise of large-scale Data Centers to power the Internet infrastructure is driving new architectural directions for optical networking. This paper highlights these architectural options and discusses technology building blocks for scaling inter-Datacenter connectivity.,http://research.google.com/pubs/archive/37208.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Drivers+and+applications+of+optical+technologies+for+Internet+Data+Center+networks+Lam+Schultz+Koley,http://research.google.com/pubs/pub37208.html
Latent Collaborative Retrieval,International Conference on Machine Learning (2012),2012,Jason Weston Chong Wang Ron Weiss Adam Berenzweig,@inproceedings{40569 title = {Latent Collaborative Retrieval} author = {Jason Weston and Chong Wang and Ron Weiss and Adam Berenzweig} year = 2012 booktitle = {International Conference on Machine Learning} },Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks on the other hand learn models comparing users with items. In this paper we study the joint problem of recommending items to a user with respect to a given query which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query _ user _ item tensor for training instead of the more traditional user _ item matrix. Compared to document retrieval we do have a query but we may or may not have content features (we will consider both cases) and we can also take account of the user’s profile. We introduce a factorized model for this new task that optimizes the top ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.,http://research.google.com/pubs/archive/40569.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Latent+Collaborative+Retrieval+Weston+Wang+Weiss+Berenzweig,http://research.google.com/pubs/pub40569.html
Cloud-based simulations on Google Exacycle reveal ligand modulation of GPCR activation pathways,Nature Chemistry vol. 6 (2014) 15–21,2014,Kai Kohlhoff Diwakar Shukla Morgan Lawrenz Gregory Bowman David Konerding Dan Belov Russ Altman Vijay Pande,@article{41893 title = {Cloud-based simulations on Google Exacycle reveal ligand modulation of GPCR activation pathways} author = {Kai Kohlhoff and Diwakar Shukla and Morgan Lawrenz and Gregory Bowman and David Konerding and Dan Belov and Russ Altman and Vijay Pande} year = 2014 URL = {http://dx.doi.org/10.1038/nchem.1821} journal = {Nature Chemistry} pages = {15–21} volume = {6} },Simulations can provide tremendous insight into the atomistic details of biological mechanisms but micro- to millisecond timescales are historically only accessible on dedicated supercomputers. We demonstrate that cloud computing is a viable alternative that brings long-timescale processes within reach of a broader community. We used Google's Exacycle cloud-computing platform to simulate two milliseconds of dynamics of a major drug target the G-protein-coupled receptor _2AR. Markov state models aggregate independent simulations into a single statistical model that is validated by previous computational and experimental results. Moreover our models provide an atomistic description of the activation of a G-protein-coupled receptor and reveal multiple activation pathways. Agonists and inverse agonists interact differentially with these pathways with profound implications for drug design.,http://research.google.com/pubs/archive/41893.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cloud-based+simulations+on+Google+Exacycle+reveal+ligand+modulation+of+GPCR+activation+pathways+Kohlhoff+Shukla+Lawrenz+Bowman+Konerding+Belov+Altman+Pande,http://research.google.com/pubs/pub41893.html
Verified Boot on Chrome OS and How to do it yourself,Embedded Linux Conference Europe Linux Foundation 660 York Street Suite 102 San Francisco CA 94110 USA (2013),2013,Simon Glass,@inproceedings{42038 title = {Verified Boot on Chrome OS and How to do it yourself} author = {Simon Glass} year = 2013 URL = {http://embeddedlinuxconferenceeu2013.sched.org/event/57c19e0b6f7b3510b04a18d35d01bd23#.UwD83DRdVQs} booktitle = {Embedded Linux Conference Europe} address = {660 York Street Suite 102 San Francisco CA 94110 USA} },Chrome OS uses a first stage read-only firmware and second-stage updatable firmware. The updatable firmware is signed and contains kernel keys and a dm-verify hash so that the firmware Linux kernel and root filesystem are all protected against corruption and attack. This system is described and discussed. As part of Google's upstream efforts in U-Boot a generalized secure boot system has been developed and released with U-Boot 2013.07. This implementation uses the FIT format which collects together images such as kernels device tree RAM disks. Support is provided for TPMs (Trust Platform Module) RSA-based signing and verificaiton and hashing with hardware acceleration. This system is also described and discussed along with the specific steps needed to implement it in your designs.,http://research.google.com/pubs/archive/42038.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Verified+Boot+on+Chrome+OS+and+How+to+do+it+yourself+Glass,http://research.google.com/pubs/pub42038.html
The Optimal Mix of TV and Online Ads to Maximize Reach,research.google.com 76 Ninth Avenue (2013) pp. 1-16,2013,Yuxue Jin Jim Koehler Georg M. Goerg Nicolas Remy,@techreport{41669 title = {The Optimal Mix of TV and Online Ads to Maximize Reach} author = {Yuxue Jin and Jim Koehler and Georg M. Goerg and Nicolas Remy} year = 2013 institution = {research.google.com} },Brand marketers often wonder how they should allocate budget between TV and online ads in order to maximize reach or maintain the same reach at a lower cost. We use probability models based on historical cross media panel data to suggest the optimal budget allocation between TV and online ads to maximize reach to the target demographics. We take a historical TV campaign and estimate the reach and GRPs of a hypothetical cross-media campaign if some budget was shifted from TV to online. The models are validated against simulations and historical cross-media campaigns. They are illustrated on one case study to show how an optimized cross-media campaign can obtain a higher reach at the same cost or maintain the same reach at a lower cost than the TV-only campaign.,http://research.google.com/pubs/archive/41669.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Optimal+Mix+of+TV+and+Online+Ads+to+Maximize+Reach+Jin+Koehler+Goerg+Remy,http://research.google.com/pubs/pub41669.html
Unsupervised Learning for Graph Matching,International Journal of Computer Vision vol. 96 (2012) pp. 28-45,2012,Marius Leordeanu Rahul Sukthankar Martial Hebert,@article{40358 title = {Unsupervised Learning for Graph Matching} author = {Marius Leordeanu and Rahul Sukthankar and Martial Hebert} year = 2012 journal = {International Journal of Computer Vision} pages = {28--45} volume = {96} },Graph matching is an essential problem in computer vision that has been successfully applied to 2D and 3D feature matching and object recognition. Despite its importance little has been published on learning the parameters that control graph matching even though learning has been shown to be vital for improving the matching rate. In this paper we show how to perform parameter learning in an unsupervised fashion that is when no correct correspondences between graphs are given during training. Our experiments reveal that unsupervised learning compares favorably to the supervised case both in terms of efficiency and quality while avoiding the tedious manual labeling of ground truth correspondences. We verify experimentally that our learning method can improve the performance of several state-of-the-art matching algorithms. We also show that a similar method can be successfully applied to parameter learning for graphical models and demonstrate its effectiveness empirically.,http://research.google.com/pubs/archive/40358.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Learning+for+Graph+Matching+Leordeanu+Sukthankar+Hebert,http://research.google.com/pubs/pub40358.html
Product Echo State Networks: Time-Series Computation with Multiplicative Neurons,The 2015 International Joint Conference on Neural Networks (IJCNN) (to appear),2015,Alireza Goudarzi Alireza Shabani Darko Stefanovic,@inproceedings{43932 title = {Product Echo State Networks: Time-Series Computation with Multiplicative Neurons} author = {Alireza Goudarzi and Alireza Shabani and Darko Stefanovic} year = 2015 URL = {http://arxiv.org/abs/1502.00718} booktitle = {The 2015 International Joint Conference on Neural Networks (IJCNN)} },Echo state networks (ESN) a type of reservoir computing (RC) architecture are efficient and accurate artificial neural systems for time series processing and learning. An ESN consists of a core of recurrent neural networks called a reservoir with a small number of tunable parameters to generate a high-dimensional representation of an input and a readout layer which is easily trained using regression to produce a desired output from the reservoir states. Certain computational tasks involve real-time calculation of high-order time correlations which requires nonlinear transformation either in the reservoir or the readout layer. Traditional ESN employs a reservoir with sigmoid or tanh function neurons. In contrast some types of biological neurons obey response curves that can be described as a product unit rather than a sum and threshold. Inspired by this class of neurons we introduce a RC architecture with a reservoir of product nodes for time series computation. We find that the product RC shows many properties of standard ESN such as short-term memory and nonlinear capacity. On standard benchmarks for chaotic prediction tasks the product RC maintains the performance of a standard nonlinear ESN while being more amenable to mathematical analysis. Our study provides evidence that such networks are powerful in highly nonlinear tasks owing to high-order statistics generated by the recurrent product node reservoir,http://arxiv.org/abs/1502.00718,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Product+Echo+State+Networks:+Time-Series+Computation+with+Multiplicative+Neurons+Goudarzi+Shabani+Stefanovic,http://research.google.com/pubs/pub43932.html
Fiber Optic Communication Technologies: What’s Needed for Datacenter Network Operations,IEEE Communications Magazine vol. Vol.48 No.7 (2010),2010,Cedric F. Lam Hong Liu Bikash Koley Xiaoxue Zhao Valey Kamalov Vijay Gill,@article{36603 title = {Fiber Optic Communication Technologies: What’s Needed for Datacenter Network Operations} author = {Cedric F. Lam and Hong Liu and Bikash Koley and Xiaoxue Zhao and Valey Kamalov and Vijay Gill} year = 2010 journal = {IEEE Communications Magazine} volume = {Vol.48 No.7} },The authors review the growing trend of warehouse-scale mega-datacenter computing the Internet transformation driven by mega-datacenter applications and the opportunities and challenges for fiber optic communication technologies to support the growth of mega-datacenter computing in the next three to four years.,http://research.google.com/pubs/archive/36603.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fiber+Optic+Communication+Technologies:+What%E2%80%99s+Needed+for+Datacenter+Network+Operations+Lam+Liu+Koley+Zhao+Kamalov+Gill,http://research.google.com/pubs/pub36603.html
Auditory Sparse Coding,Music Data Mining CRC Press/Chapman Hall (2011),2011,Steven R. Ness Thomas Walters Richard F. Lyon,@inbook{36950 title = {Auditory Sparse Coding} author = {Steven R. Ness and Thomas Walters and Richard F. Lyon} year = 2011 booktitle = {Music Data Mining} },The concept of sparsity has attracted considerable interest in the field of machine learning in the past few years. Sparse feature vectors contain mostly values of zero and one or a few non-zero values. Although these feature vectors can be classified by traditional machine learning algorithms such as SVM there are various recently-developed algorithms that explicitly take advantage of the sparse nature of the data leading to massive speedups in time as well as improved performance. Some fields that have benefited from the use of sparse algorithms are finance bioinformatics text mining and image classification. Because of their speed these algorithms perform well on very large collections of data; large collections are becoming increasingly relevant given the huge amounts of data collected and warehoused by Internet businesses. We discuss the application of sparse feature vectors in the field of audio analysis and specifically their use in conjunction with preprocessing systems that model the human auditory system. We present results that demonstrate the applicability of the combination of auditory-based processing and sparse coding to content-based audio analysis tasks: a search task in which ranked lists of sound effects are retrieved from text queries and a music information retrieval (MIR) task dealing with the classification of music into genres.,http://research.google.com/pubs/archive/36950.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Auditory+Sparse+Coding+Ness+Walters+Lyon,http://research.google.com/pubs/pub36950.html
Corrupted DNS Resolution Paths: The Rise of a Malicious Resolution Authority,Proc. 15th Network and Distributed System Security Symposium (NDSS) Internet Society San Diego CA (2008),2008,David Dagon Chris Lee Wenke Lee Niels Provos,@inproceedings{33426 title = {Corrupted DNS Resolution Paths: The Rise of a Malicious Resolution Authority} author = {David Dagon and Chris Lee and Wenke Lee and Niels Provos} year = 2008 URL = {http://www.citi.umich.edu/u/provos/papers/ndss08_dns.pdf} booktitle = {Proc. 15th Network and Distributed System Security Symposium (NDSS)} address = {San Diego CA} },"We study and document an important development in how attackers are using Internet resources: the creation of malicious DNS resolution paths. In this growing form of attack victims are forced to use rogue DNS servers for all resolution. To document the rise of this ""second secret authority"" on the Internet we studied instances of aberrant DNS resolution on a university campus. We found dozens of viruses that corrupt resolution paths and noted that hundreds of URLs discovered per week performed drive-by alterations of host DNS settings.",http://research.google.com/pubs/archive/33426.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Corrupted+DNS+Resolution+Paths:+The+Rise+of+a+Malicious+Resolution+Authority+Dagon+Lee+Lee+Provos,http://research.google.com/pubs/pub33426.html
An Overview of Practical Exchange Design,Current Science vol. 103 no.9 (2012) pp. 1056-1063,2012,R. Preston McAfee Sergei Vassilvitskii,@article{40755 title = {An Overview of Practical Exchange Design} author = {R. Preston McAfee and Sergei Vassilvitskii} year = 2012 journal = {Current Science} pages = {1056-1063} volume = {103 no.9} },We consider the problem of designing an online exchange. We identify the goals of exchange design and present key techniques for accomplishing these goals along with the tradeoffs inherent in the choices.,http://research.google.com/pubs/archive/40755.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Overview+of+Practical+Exchange+Design+McAfee+Vassilvitskii,http://research.google.com/pubs/pub40755.html
Flywheel: Google's Data Compression Proxy for the Mobile Web,Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2015),2015,Victor Agababov Michael Buettner Victor Chudnovsky Mark Cogan Ben Greenstein Shane McDaniel Michael Piatek Colin Scott Matt Welsh Bolian Yin,@inproceedings{43447 title = {Flywheel: Google's Data Compression Proxy for the Mobile Web} author = {Victor Agababov and Michael Buettner and Victor Chudnovsky and Mark Cogan and Ben Greenstein and Shane McDaniel and Michael Piatek and Colin Scott and Matt Welsh and Bolian Yin} year = 2015 booktitle = {Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2015)} },Mobile devices are increasingly the dominant Internet access technology. Nevertheless high costs data caps and throttling are a source of widespread frustration and a significant barrier to adoption in emerging markets. This paper presents Flywheel an HTTP proxy service that extends the life of mobile data plans by compressing responses in-flight between origin servers and client browsers. Flywheel is integrated with the Chrome web browser and reduces the size of proxied web pages by 50% for a median user. We report measurement results from millions of users as well as experience gained during three years of operating and evolving the production service at Google.,http://research.google.com/pubs/archive/43447.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Flywheel:+Google's+Data+Compression+Proxy+for+the+Mobile+Web+Agababov+Buettner+Chudnovsky+Cogan+Greenstein+McDaniel+Piatek+Scott+Welsh+Yin,http://research.google.com/pubs/pub43447.html
Yield Optimization of Display Advertising with Ad Exchange,ACM Conference on Electronic Commerce (2011),2011,Santiago Balseiro Jon Feldman Vahab Mirrokni S. Muthukrishnan,@inproceedings{36975 title = {Yield Optimization of Display Advertising with Ad Exchange} author = {Santiago Balseiro and Jon Feldman and Vahab Mirrokni and S. Muthukrishnan} year = 2011 URL = {http://arxiv.org/abs/1102.2551} booktitle = {ACM Conference on Electronic Commerce} },In light of the growing market of Ad Exchanges for the real-time sale of advertising slots publishers face new challenges in choosing between the allocation of contract-based reservation ads and spot market ads. In this setting the publisher should take into account the tradeoff between short-term revenue from an Ad Exchange and quality of allocating reservation ads. In this paper we formalize this combined optimization problem as a stochastic control problem and derive an efficient policy for online ad allocation in settings with general joint distribution over placement quality and exchange bids. We prove asymptotic optimality of this policy in terms of any trade-off between quality of delivered reservation ads and revenue from the exchange and provide a rigorous bound for its convergence rate to the optimal policy. We also give experimental results on data derived from real publisher inventory showing that our policy can achieve any pareto-optimal point on the quality vs. revenue curve. Finally we study a parametric training-based algorithm in which instead of learning the dual variables from a sample data (as is done in non-parametric training-based algorithms) we learn the parameters of the distribution and construct those dual variables from the learned parameter values. We compare parametric and non-parametric ways to estimate from data both analytically and experimentally in the special case without the ad exchange and show that though both methods converge to the optimal policy as the sample size grows our parametric method converges faster and thus performs better on smaller samples.,http://arxiv.org/abs/1102.2551,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Yield+Optimization+of+Display+Advertising+with+Ad+Exchange+Balseiro+Feldman+Mirrokni+Muthukrishnan,http://research.google.com/pubs/pub36975.html
Deadline-Aware Datacenter TCP (D 2 TCP),Proceedings of the ACM SIGCOMM (2012),2012,Balajee Vamanan Jahangir Hasan T. N. Vijaykumar,@inproceedings{37678 title = {Deadline-Aware Datacenter TCP (D2TCP)} author = {Balajee Vamanan and Jahangir Hasan and T. N. Vijaykumar} year = 2012 booktitle = {Proceedings of the ACM SIGCOMM} },An important class of datacenter applications called Online Data-Intensive (OLDI) applications includes Web search online retail and advertisement. To achieve good user experience OLDI applications operate under soft-real-time constraints (e.g. 300 ms latency) which imply deadlines for network communication within the applications. Further OLDI applications typically employ tree-based algorithms which in the common case result in bursts of children-to-parent traffic with tight deadlines. Recent work on datacenter network protocols is either deadline-agnostic (DCTCP) or is deadline-aware (D3) but suffers under bursts due to race conditions. Further D3 has the practical drawbacks of requiring changes to the switch hardware and not being able to coexist with legacy TCP. We propose Deadline-Aware Datacenter TCP (D2TCP) a novel transport protocol which handles bursts is deadline-aware and is readily deployable. In designing D2TCP we make two contributions: (1) D2TCP uses a distributed and reactive approach for bandwidth allocation which fundamentally enables D2TCP’s properties. (2) D2TCP employs a novel congestion avoidance algorithm which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function. Using a small-scale implementation and at-scale simulations we show that D2TCP reduces the fraction of missed deadlines compared to DCTCP and D3 by 75% and 50% respectively.,http://research.google.com/pubs/archive/37678.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deadline-Aware+Datacenter+TCP+(D%3Csup%3E2%3C/sup%3ETCP)+Vamanan+Hasan+Vijaykumar,http://research.google.com/pubs/pub37678.html
Protecting Browsers from Extension Vulnerabilities,Network and Distributed System Security Symposium (2010),2010,Adam Barth Adrienne Porter Felt Prateek Saxena Aaron Boodman,@inproceedings{38394 title = {Protecting Browsers from Extension Vulnerabilities} author = {Adam Barth and Adrienne Porter Felt and Prateek Saxena and Aaron Boodman} year = 2010 booktitle = {Network and Distributed System Security Symposium} },Browser extensions are remarkably popular with one in three Firefox users running at least one extension. Although well-intentioned extension developers are often not security experts and write buggy code that can be exploited by malicious web site operators. In the Firefox extension system these exploits are dangerous because extensions run with the user's full privileges and can read and write arbitrary files and launch new processes. In this paper we analyze 25 popular Firefox extensions and find that 88% of these extensions need less than the full set of available privileges. Additionally we find that 76% of these extensions use unnecessarily powerful APIs making it difficult to reduce their privileges. We propose a new browser extension system that improves security by using least privilege privilege separation and strong isolation. Our system limits the misdeeds an attacker can perform through an extension vulnerability. Our design has been adopted as the Google Chrome extension system.,http://research.google.com/pubs/archive/38394.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Protecting+Browsers+from+Extension+Vulnerabilities+Barth+Felt+Saxena+Boodman,http://research.google.com/pubs/pub38394.html
TCP Fast Open,Proceedings of the 7th International Conference on emerging Networking EXperiments and Technologies (CoNEXT) ACM (2011),2011,Sivasankar Radhakrishnan Yuchung Cheng Jerry Chu Arvind Jain Barath Raghavan,@inproceedings{37517 title = {TCP Fast Open} author = {Sivasankar Radhakrishnan and Yuchung Cheng and Jerry Chu and Arvind Jain and Barath Raghavan} year = 2011 booktitle = {Proceedings of the 7th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)} },Today’s web services are dominated by TCP flows so short that they terminate a few round trips after handshaking; this handshake is a significant source of latency for such flows. In this paper we describe the design implementation and deployment of the TCP Fast Open protocol a new mechanism that enables data exchange during TCP’s initial handshake. In doing so TCP Fast Open decreases application network latency by one full round-trip time decreasing the delay experienced by such short TCP transfers. We address the security issues inherent in allowing data exchange during the three-way handshake which we mitigate using a security token that verifies IP address ownership. We detail other fall-back defense mechanisms and address issues we faced with middleboxes backwards compatibility for existing network stacks and incremental deployment. Based on traffic analysis and network emulation we show that TCP Fast Open would decrease HTTP transaction network latency by 15%and whole-page load time over 10% on average and in some cases up to 40%,http://research.google.com/pubs/archive/37517.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=TCP+Fast+Open+Radhakrishnan+Cheng+Chu+Jain+Raghavan,http://research.google.com/pubs/pub37517.html
Bayesian Touch - A Statistic Criterion of Target Selection with Finger Touch,Proceedings of UIST 2013 – The ACM Symposium on User Interface Software and Technology ACM New York NY USA pp. 51-60,2013,Xiaojun Bi Shumin Zhai,@inproceedings{41644 title = {Bayesian Touch - A Statistic Criterion of Target Selection with Finger Touch} author = {Xiaojun Bi and Shumin Zhai} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2502058} booktitle = {Proceedings of UIST 2013 – The ACM Symposium on User Interface Software and Technology} pages = {51--60} address = {New York NY USA} },To improve the accuracy of target selection for finger touch we conceptualize finger touch input as an uncertain process and derive a statistical target selection riterion Bayesian Touch Criterion from combining the basic Bayes’ rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. Bayesian Touch Criterion states that the selected target is the candidate with the shortest Bayesian Touch Distance to the touch point which is computed from the touch point to target center distance and the size of the target. We give the derivation of the Bayesian touch criterion and its empirical evaluation with two experiments. The results show for 2D circular target selection Bayesian Touch Criterion is significantly more accurate than the commonly used Visual Boundary Criterion (i.e. a target is selected if and only if the touch point falls within its boundary) and its two variants.,http://research.google.com/pubs/archive/41644.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bayesian+Touch+-+A+Statistic+Criterion+of+Target+Selection+with+Finger+Touch+Bi+Zhai,http://research.google.com/pubs/pub41644.html
Selective Disclosure,Ben Laurie (2007),2007,Ben Laurie,@techreport{32874 title = {Selective Disclosure} author = {Ben Laurie} year = 2007 URL = {http://www.links.org/files/selective-disclosure.pdf} institution = {Ben Laurie} },Selective disclosure for the non-cryptographer.,http://research.google.com/pubs/archive/32874.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Selective+Disclosure+Laurie,http://research.google.com/pubs/pub32874.html
The Prospect of Inter-Data-Center Optical Networks,IEEE Communication Magazine vol. 51 (2013) pp. 32-38,2013,Xiaoxue Zhao Vijay Vusirikala Bikash Koley Valey Kamalov Tad Hofmeister,@article{41414 title = {The Prospect of Inter-Data-Center Optical Networks} author = {Xiaoxue Zhao and Vijay Vusirikala and Bikash Koley and Valey Kamalov and Tad Hofmeister} year = 2013 URL = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588647} journal = {IEEE Communication Magazine} pages = {32-38} volume = {51} },Mega data centers and their interconnection networks have drawn great attention in recent years because of the rapid public adoption of cloud-based services. The unprecedented amount of data that needs to be communicated between data centers imposes new requirements and challenges to inter-data-center optical networks. In this article we discuss the traffic growth trends and capacity demands of Google’s inter-data-center network and how they drive the network architectures and technologies to scale capacities and operational ease on existing fiber plants. We extensively review recent research findings and emerging technologies such as digital coherent detection and the flexgrid dense wavelength-division multiplexed channel plan and propose practical implementations such as C+L-band transmission packet and optical layer integration and a software-defined networking enabled network architecture for both capacity and operational scaling. In addition we point out a few critical areas that require more attention and research to improve efficiency and flexibility of an inter-data-center optical network: optical regeneration data rate mismatch between Ethernet and optical transport and real-time optical performance monitoring.,http://research.google.com/pubs/archive/41414.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Prospect+of+Inter-Data-Center+Optical+Networks+Zhao+Vusirikala+Koley+Kamalov+Hofmeister,http://research.google.com/pubs/pub41414.html
Statistical Language Modeling,The Handbook of Computational Linguistics and Natural Language Processing Wiley-Blackwell John Wiley & Sons Ltd The Atrium Southern Gate Chichester West Sussex PO19 8SQ United Kingdom (2010) pp. 74-104,2010,Ciprian Chelba,@inbook{36557 title = {Statistical Language Modeling} author = {Ciprian Chelba} year = 2010 URL = {http://onlinelibrary.wiley.com/doi/10.1002/9781444324044.ch3/summary} note = {Parts of this chapter appeared in Computer Speech & Language vol. 14 no. 4 pages 283-332 October 2000 and are used with the permission of Elsevier Limited.} booktitle = {The Handbook of Computational Linguistics and Natural Language Processing} pages = {74--104} address = {John Wiley & Sons Ltd The Atrium Southern Gate Chichester West Sussex PO19 8SQ United Kingdom} },Many practical applications such as automatic speech recognition statistical machine translation spelling correction resort to variants of the well established source-channel model for producing the correct string of words W given an input speech signal sentence in foreign language or typed text with possible mistakes respectively. A basic component of such systems is a statistical language model which estimates the prior probability values for strings of words W.,http://onlinelibrary.wiley.com/doi/10.1002/9781444324044.ch3/summary,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+Language+Modeling+Chelba,http://research.google.com/pubs/pub36557.html
Large-scale Incremental Processing Using Distributed Transactions and Notifications,Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation USENIX (2010),2010,Daniel Peng Frank Dabek,@inproceedings{36726 title = {Large-scale Incremental Processing Using Distributed Transactions and Notifications} author = {Daniel Peng and Frank Dabek} year = 2010 booktitle = {Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation} },Updating an index of the web as documents are crawled requires continuously transforming a large repository of existing documents as new documents arrive. This task is one example of a class of data processing tasks that transform a large repository of data via small independent mutations. These tasks lie in a gap between the capabilities of existing infrastructure. Databases do not meet the storage or throughput requirements of these tasks: Google's indexing system stores tens of petabytes of data and processes billions of updates per day on thousands of machines. MapReduce and other batch-processing systems cannot process small updates individually as they rely on creating large batches for efficiency. We have built Percolator a system for incrementally processing updates to a large data set and deployed it to create the Google web search index. By replacing a batch-based indexing system with an indexing system based on incremental processing using Percolator we process the same number of documents per day while reducing the average age of documents in Google search results by 50%.,http://research.google.com/pubs/archive/36726.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+Incremental+Processing+Using+Distributed+Transactions+and+Notifications+Peng+Dabek,http://research.google.com/pubs/pub36726.html
Strategies for testing client-server interactions in mobile applications,MobileDeLi '13 Proceedings of the 2013 ACM workshop on Mobile development lifecycle ACM Indianapolis Indiana pp. 19-20,2013,Niranjan Tulpule,@inproceedings{42109 title = {Strategies for testing client-server interactions in mobile applications} author = {Niranjan Tulpule} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2542134} booktitle = {MobileDeLi '13 Proceedings of the 2013 ACM workshop on Mobile development lifecycle} pages = {19-20} address = {Indianapolis Indiana} },Modern smartphone ecosystems have their unique set of constraints which makes testing the contract between client and servers hard. In this paper we will describe the Google+ team's approaches to solving this problem. We will describe our testing philosophy followed by a couple of frameworks and test design patterns that we have found to be really useful in a client-server testing context.,http://research.google.com/pubs/archive/42109.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Strategies+for+testing+client-server+interactions+in+mobile+applications+Tulpule,http://research.google.com/pubs/pub42109.html
Safe ICF: Pointer Safe and Unwinding Aware Identical Code Folding in Gold,GCC Developers Summit (2010),2010,Sriraman Tallam Cary Coutant Ian Lance Taylor Xinliang David Li Chris Demetriou,@inproceedings{36912 title = {Safe ICF: Pointer Safe and Unwinding Aware Identical Code Folding in Gold} author = {Sriraman Tallam and Cary Coutant and Ian Lance Taylor and Xinliang David Li and Chris Demetriou} year = 2010 URL = {http://gcc.gnu.org/wiki/summit2010?action=AttachFile&do=view&target=tallam.pdf} booktitle = {GCC Developers Summit} },We have found that large C++ applications and shared libraries tend to have many functions whose code is identical with another function. As much as 10% of the code could theoretically be eliminated by merging such identical functions into a single copy. This optimization Identical Code Folding (ICF) has been implemented in the gold linker. At link time ICF detects functions with identical object code and merges them into a single copy. ICF can be unsafe however as it can change the run-time behaviour of code that relies on each function having a unique address. To address this ICF can be used in a safe mode where it identifies and folds functions whose addresses are guaranteed not to have been used in comparison operations. Further profiling and debugging binaries with merged functions can be confusing as the PC values of merged functions cannot be always disambiguated to point to the correct function. To address this we propose a new call table format for the DWARF debugging information to allow tools like the debugger and profiler to disambiguate PC values of merged functions correctly by examining the call chain. Detailed experiments on the x86 platform show that ICF can reduce the text size of a selection of Google binaries whose average text size is 64 MB by about 6%. Also the code size savings of ICF with the safe option is almost as good as the code savings obtained without the safe option. Further experiments also show that the run-time performance of the optimized binaries on the x86 platform does not change.,http://research.google.com/pubs/archive/36912.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Safe+ICF:+Pointer+Safe+and+Unwinding+Aware+Identical+Code+Folding+in+Gold+Tallam+Coutant+Taylor+Li+Demetriou,http://research.google.com/pubs/pub36912.html
Proxies: Design Principles for Robust Object-oriented Intercession APIs,Dynamic Languages Symposium ACM (2010),2010,Tom Van Cutsem Mark S. Miller,@inproceedings{36574 title = {Proxies: Design Principles for Robust Object-oriented Intercession APIs} author = {Tom Van Cutsem and Mark S. Miller} year = 2010 booktitle = {Dynamic Languages Symposium} },Proxies are a powerful approach to implement meta-objects in object-oriented languages without having to resort to metacircular interpretation. We introduce such a meta-level API based on proxies for Javascript. We simultaneously introduce a set of design principles that characterize such APIs in general and compare similar APIs of other languages in terms of these principles. We highlight how principled proxy-based APIs improve code robustness by avoiding interference between base and meta-level code that occur in more common reﬂective intercession mechanisms.,http://research.google.com/pubs/archive/36574.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Proxies:+Design+Principles+for+Robust+Object-oriented+Intercession+APIs+Cutsem+Miller,http://research.google.com/pubs/pub36574.html
Enforcing Forward-Edge Control-Flow Integrity in GCC & LLVM,Proceedings of the 23rd Usenix Security Symposium USENIX San Diego CA (2014),2014,Caroline Tice Tom Roeder Peter Collingbourne Stephen Checkoway Úlfar Erlingsson Luis Lozano Geoff Pike,@inproceedings{42808 title = {Enforcing Forward-Edge Control-Flow Integrity in GCC & LLVM} author = {Caroline Tice and Tom Roeder and Peter Collingbourne and Stephen Checkoway and Úlfar Erlingsson and Luis Lozano and Geoff Pike} year = 2014 booktitle = {Proceedings of the 23rd Usenix Security Symposium} address = {San Diego CA} },Constraining dynamic control transfers is a common technique for mitigating software vulnerabilities. This defense has been widely and successfully used to protect return addresses and stack data; hence current attacks instead typically corrupt vtable and function pointers to subvert a forward edge (an indirect jump or call) in the control-flow graph. Forward edges can be protected using Control-Flow Integrity (CFI) but to date CFI implementations have been research prototypes based on impractical assumptions or ad hoc heuristic techniques. To be widely adoptable CFI mechanisms must be integrated into production compilers and be compatible with software-engineering aspects such as incremental compilation and dynamic libraries. This paper presents implementations of fine-grained forward-edge CFI enforcement and analysis for GCC and LLVM that meet the above requirements. An analysis and evaluation of the security performance and resource consumption of these mechanisms applied to the SPEC CPU2006 benchmarks and common benchmarks for the Chromium web browser show the practicality of our approach: these fine-grained CFI mechanisms have significantly lower overhead than recent academic CFI prototypes. Implementing CFI in industrial compiler frameworks has also led to insights into design tradeoffs and practical challenges such as dynamic loading.,http://research.google.com/pubs/archive/42808.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Enforcing+Forward-Edge+Control-Flow+Integrity+in+GCC+%26+LLVM+Tice+Roeder+Collingbourne+Checkoway+Erlingsson+Lozano+Pike,http://research.google.com/pubs/pub42808.html
Universal Gigabit Optical Access,OFC/NFOEC 2011 Technical Digest Optical Society of America 2010 Massachusetts Ave NW Washington DC 20036 USA,2011,James F.Kelly,@inproceedings{37010 title = {Universal Gigabit Optical Access} author = {James F.Kelly} year = 2011 booktitle = {OFC/NFOEC 2011 Technical Digest} address = {2010 Massachusetts Ave NW Washington DC 20036 USA} },We review the imperatives on the optical communication technology industry to realize universal ultra high speed access to the world’s information.,http://research.google.com/pubs/archive/37010.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Universal+Gigabit+Optical+Access+Kelly,http://research.google.com/pubs/pub37010.html
Bridging communications and the physical world,IEEE Internet Computing vol. 16 (2012) pp. 35-43,2012,Omer Boyaci Victoria Beltran Martinez Henning Schulzrinne,@article{39962 title = {Bridging communications and the physical world} author = {Omer Boyaci and Victoria Beltran Martinez and Henning Schulzrinne} year = 2012 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/MIC.2012.18} journal = {IEEE Internet Computing} pages = {35-43} volume = {16} },Sense Everything Control Everything (SECE) is an event-driven system that lets nontechnical users create services that combine communication location social networks presence calendaring and physical devices such as sensors and actuators. SECE combines information from multiple sources to personalize services and adapt them to changes in the user's context and preferences. Events trigger associated actions which can control email delivery change how phone calls are handled update the user's social network status and set the state of actuators such as lights thermostats and electrical appliances.,http://www.computer.org/portal/web/csdl/doi/10.1109/MIC.2012.18,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bridging+communications+and+the+physical+world+Boyaci+Martinez+Schulzrinne,http://research.google.com/pubs/pub39962.html
Minimax Optimal Algorithms for Unconstrained Linear Optimization,Advances in Neural Information Processing Systems (NIPS) (2013),2013,H. Brendan McMahan Jacob Abernethy,@inproceedings{41859 title = {Minimax Optimal Algorithms for Unconstrained Linear Optimization} author = {H. Brendan McMahan and Jacob Abernethy} year = 2013 booktitle = {Advances in Neural Information Processing Systems (NIPS)} },We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained. The player strives to minimize regret the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game and we give a thorough analysis of the minimax behavior of the game providing characterizations for the value of the game as well as both the player's and the adversary's optimal strategy. We show how these objects can be computed efficiently under certain circumstances and by selecting an appropriate benchmark we construct a novel hedging strategy for an unconstrained betting game.,http://research.google.com/pubs/archive/41859.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Minimax+Optimal+Algorithms+for+Unconstrained+Linear+Optimization+McMahan+Abernethy,http://research.google.com/pubs/pub41859.html
Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,Google (2014),2014,Noam M. Shazeer Joris Pelemans Ciprian Chelba,@techreport{43222 title = {Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation} author = {Noam M. Shazeer and Joris Pelemans and Ciprian Chelba} year = 2014 URL = {http://arxiv.org/abs/1412.1454} institution = {Google} },We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.,http://research.google.com/pubs/archive/43222.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Skip-gram+Language+Modeling+Using+Sparse+Non-negative+Matrix+Probability+Estimation+Shazeer+Pelemans+Chelba,http://research.google.com/pubs/pub43222.html
GraphSC: Parallel Secure Computation Made Easy,IEEE Symposium on Security and Privacy IEEE (2015),2015,Kartik Nayak Xiao S. Wang Stratis Ioannidis Udi Weinsberg Nina Taft Elaine Shi,@inproceedings{43470 title = {GraphSC: Parallel Secure Computation Made Easy} author = {Kartik Nayak and Xiao S. Wang and Stratis Ioannidis and Udi Weinsberg and Nina Taft and Elaine Shi} year = 2015 URL = {http://www.ieee-security.org/TC/SP2015/program.html} booktitle = {IEEE Symposium on Security and Privacy} },We propose introducing modern parallel programming paradigms to secure computation enabling their secure execution on large datasets. To address this challenge we present GraphSC a framework that (i) provides a programming paradigm that allows non-cryptography experts to write secure code; (ii) brings parallelism to such secure implementations; and (iii) meets the needs for obliviousness thereby not leaking any private information. Using GraphSC developers can efficiently implement an oblivious version of graph-based algorithms (including sophisticated data mining and machine learning algorithms) that execute in parallel with minimal communication overhead. Importantly our secure version of graph-based algorithms incurs a small logarithmic overhead in comparison with the non-secure parallel version. We build GraphSC and demonstrate using several algorithms as examples that secure computation can be brought into the realm of practicality for big data analysis. Our secure matrix factorization implementation can process 1 million ratings in 13 hours which is a multiple order-of-magnitude improvement over the only other existing attempt which requires 3 hours to process 16K ratings.,http://research.google.com/pubs/archive/43470.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=GraphSC:+Parallel+Secure+Computation+Made+Easy+Nayak+Wang+Ioannidis+Weinsberg+Taft+Shi,http://research.google.com/pubs/pub43470.html
Physics Topology Logic and Computation: A Rosetta Stone,Lecture Notes in Physics vol. 813 (2011) pp. 95-172,2011,John Baez Michael Stay,@article{34405 title = {Physics Topology Logic and Computation: A Rosetta Stone} author = {John Baez and Michael Stay} year = 2011 URL = {http://arxiv.org/abs/0903.0340} journal = {Lecture Notes in Physics} pages = {95--172} volume = {813} },"In physics Feynman diagrams are used to reason about quantum processes. In the 1980s it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology: namely a linear operator behaves very much like a ""cobordism"". Similar diagrams can be used to reason about logic where they represent proofs and computation where they represent programs. With the rise of interest in quantum cryptography and quantum computation it became clear that there is extensive network of analogies between physics topology logic and computation. In this expository paper we make some of these analogies precise using the concept of ""closed symmetric monoidal category"". We assume no prior knowledge of category theory proof theory or computer science.",http://research.google.com/pubs/archive/34405.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Physics+Topology+Logic+and+Computation:+A+Rosetta+Stone+Baez+Stay,http://research.google.com/pubs/pub34405.html
Cooperative Coevolution and Univariate Estimation of Distribution Algorithms,Foundations of Genetic Algorithms (2009),2009,Christopher Vo Liviu Panait Sean Luke,@inproceedings{34619 title = {Cooperative Coevolution and Univariate Estimation of Distribution Algorithms} author = {Christopher Vo and Liviu Panait and Sean Luke} year = 2009 booktitle = {Foundations of Genetic Algorithms} },In this paper we discuss a curious relationship between Cooperative Coevolutionary Algorithms (CCEAs) and Univariate EDAs. Inspired by the theory of CCEAs we also present a new EDA with theoretical convergence guarantees and some preliminary experimental results in comparison with existing Univariate EDAs.,http://research.google.com/pubs/archive/34619.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cooperative+Coevolution+and+Univariate+Estimation+of+Distribution+Algorithms+Vo+Panait+Luke,http://research.google.com/pubs/pub34619.html
Experimenting At Scale With Google Chrome's SSL Warning,ACM CHI Conference on Human Factors in Computing Systems (2014),2014,Adrienne Porter Felt Robert W. Reeder Hazim Almuhimedi Sunny Consolvo,@inproceedings{41927 title = {Experimenting At Scale With Google Chrome's SSL Warning} author = {Adrienne Porter Felt and Robert W. Reeder and Hazim Almuhimedi and Sunny Consolvo} year = 2014 URL = {Honorable mention} booktitle = {ACM CHI Conference on Human Factors in Computing Systems} },Web browsers shown HTTPS authentication warnings (i.e. SSL warnings) when the integrity and confidentiality of users' interactions with websites are at risk. Our goal in this work is to decrease the number of users who click through the Google Chrome SSL warning. Prior research showed that the Mozilla Firefox SSL warning has a much lower click-through rate (CTR) than Chrome. We investigate several factors that could be responsible: the use of imagery extra steps before the user can proceed and style choices. To test these factors we ran six experimental SSL warnings in Google Chrome 29 and measured 130754 impressions.,http://research.google.com/pubs/archive/41927.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Experimenting+At+Scale+With+Google+Chrome's+SSL+Warning+Felt+Reeder+Almuhimedi+Consolvo,http://research.google.com/pubs/pub41927.html
Behavior-Oriented Data Resource Management in Medical Sensing Systems,ACM Transactions on Sensor Networks (TOSN) vol. 9 (2013) 12:1-12:26,2013,Hyduke Noshadi Foad Dabiri Saro Meguerdichian Miodrag Potkonjak Majid Sarrafzadeh,@article{41346 title = {Behavior-Oriented Data Resource Management in Medical Sensing Systems} author = {Hyduke Noshadi and Foad Dabiri and Saro Meguerdichian and Miodrag Potkonjak and Majid Sarrafzadeh} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2422969} journal = {ACM Transactions on Sensor Networks (TOSN)} pages = {12:1--12:26} volume = {9} },Wearable sensing systems have recently enabled a variety of medical monitoring and diagnostic applications in wireless health. The need for multiple sensors and constant monitoring leads these systems to be power hungry and expensive with short operating lifetimes. We introduce a novel methodology that takes advantage of contextual and semantic properties in human behavior to enable efficient design and optimization of such systems from the data and information point of view. This in turn directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system contextual behavior. Our goal is to select a small subset of sensors that accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling partitioning and behavioral optimization which consists of signal characterization segmentation and time shifting mutual signal prediction and a simultaneous minimization composed of subset sensor selection and opportunistic sampling. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe which cover the bottom of the entire foot resulting in energy reduction of 72% to 97% for error rates of 5% to 17.5%.,http://dl.acm.org/citation.cfm?id=2422969,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Behavior-Oriented+Data+Resource+Management+in+Medical+Sensing+Systems+Noshadi+Dabiri+Meguerdichian+Potkonjak+Sarrafzadeh,http://research.google.com/pubs/pub41346.html
Fast Data Processing with Spark,Packt (2013),2013,Holden Karau,@book{41431 title = {Fast Data Processing with Spark} author = {Holden Karau} year = 2013 URL = {http://www.packtpub.com/fastdata-processing-with-spark/book} },Spark is a framework for writing fast distributed programs. Spark solves similar problems as Hadoop MapReduce does but with a fast in-memory approach and a clean functional style API. With its ability to integrate with Hadoop and inbuilt tools for interactive query analysis (Shark) large-scale graph processing and analysis (Bagel) and real-time analysis (Spark Streaming) it can be interactively used to quickly process and query big data sets. Fast Data Processing with Spark covers how to write distributed map reduce style programs with Spark. The book will guide you through every step required to write effective distributed programs from setting up your cluster and interactively exploring the API to deploying your job to the cluster and tuning it for your purposes. Fast Data Processing with Spark covers everything from setting up your Spark cluster in a variety of situations (stand-alone EC2 and so on) to how to use the interactive shell to write distributed code interactively. From there we move on to cover how to write and deploy distributed jobs in Java Scala and Python. We then examine how to use the interactive shell to quickly prototype distributed programs and explore the Spark API. We also look at how to use Hive with Spark to use a SQL-like query syntax with Shark as well as manipulating resilient distributed datasets (RDDs).,http://www.packtpub.com/fastdata-processing-with-spark/book,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Data+Processing+with+Spark+Karau,http://research.google.com/pubs/pub41431.html
Processing a Trillion Cells per Mouse Click,PVLDB vol. 5 (2012) pp. 1436-1446,2012,Alex Hall Olaf Bachmann Robert Buessow Silviu-Ionut Ganceanu Marc Nunkesser,@article{40465 title = {Processing a Trillion Cells per Mouse Click} author = {Alex Hall and Olaf Bachmann and Robert Buessow and Silviu-Ionut Ganceanu and Marc Nunkesser} year = 2012 URL = {http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf} journal = {PVLDB} pages = {1436--1446} volume = {5} },Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset.,http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Processing+a+Trillion+Cells+per+Mouse+Click+Hall+Bachmann+Buessow+Ganceanu+Nunkesser,http://research.google.com/pubs/pub40465.html
Scalable Example-Based Refactorings with Refaster,Workshop on Refactoring Tools (2013),2013,Louis Wasserman,@inproceedings{41876 title = {Scalable Example-Based Refactorings with Refaster} author = {Louis Wasserman} year = 2013 URL = {http://dx.doi.org/10.1145/2541348.2541355} booktitle = {Workshop on Refactoring Tools} },We discuss Refaster a tool that uses normal compilable before-and-after examples of Java code to specify a Java refactoring. Refaster has been used successfully by the Java Core Libraries Team at Google to perform a wide variety of refactorings across Google's massive Java codebase. Our main contribution is that a large class of useful refactorings can be expressed in pure Java without a specialized DSL while keeping the tool easily accessible to average Java developers.,http://research.google.com/pubs/archive/41876.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Example-Based+Refactorings+with+Refaster+Wasserman,http://research.google.com/pubs/pub41876.html
Egocentric Field-of-View Localization Using First-Person Point-of-View Devices,Proceedings of Winter Conference on Applications of Computer Vision (WACV) IEEE (2015),2015,Vinay Bettadapura Irfan Essa Caroline Pantofaru,@inproceedings{43137 title = {Egocentric Field-of-View Localization Using First-Person Point-of-View Devices} author = {Vinay Bettadapura and Irfan Essa and Caroline Pantofaru} year = 2015 booktitle = {Proceedings of Winter Conference on Applications of Computer Vision (WACV)} },We present a technique that uses images videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person’s field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person’s head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality event understanding and studying social interactions.,http://research.google.com/pubs/archive/43137.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Egocentric+Field-of-View+Localization+Using+First-Person+Point-of-View+Devices+Bettadapura+Essa+Pantofaru,http://research.google.com/pubs/pub43137.html
Security and Testing,Introduction to Hardware Security and Trust Springer (2012) (to appear),2012,Kurt Rosenfeld,@inbook{37399 title = {Security and Testing} author = {Kurt Rosenfeld} year = 2012 booktitle = {Introduction to Hardware Security and Trust} },Test interfaces are present in nearly all digital hardware. In many cases the security of the system depends on the security of the test interfaces. Systems have been hacked in the field using test interfaces as an avenue for attack. Researchers in industry and academia have developed defenses over the past 20 years. A diligent designer can significantly reduce the chance of system exploitation by understanding known threats and applying known defenses.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Security+and+Testing+Rosenfeld,http://research.google.com/pubs/pub37399.html
Secular Behavior of Exoplanetary Systems: Self-Consistency and Comparisons With The Planet-Planet Scattering Hypothesis,The Astrophysics Journal vol. 146 (2011) pp. 53,2011,Miles Timpe Rory Barnes Ravikumar Kopparapu Sean N. Raymond Richard Greenberg Noel Gorelick,@article{42926 title = {Secular Behavior of Exoplanetary Systems: Self-Consistency and Comparisons With The Planet-Planet Scattering Hypothesis} author = {Miles Timpe and Rory Barnes and Ravikumar Kopparapu and Sean N. Raymond and Richard Greenberg and Noel Gorelick} year = 2011 journal = {The Astrophysics Journal} pages = {53} volume = {146} },Planet-planet scattering has been suggested as a mechanism to explain the disproportionate number of planet-planet pairs found to lie on or near an apsidal separatrix in which one planet's eccentricity periodically drops to near-zero. We present the results of numerical simulations of 2-planet systems having arisen from dynamically unstable 3-planet systems. We show that the distribution of near-separatrix systems arising after an instability is consistent with the observed systems further strengthening the planet-planet scattering hypothesis. We also note that many observed systems have been found near their extreme eccentricity values. Such a pattern may suggest a bias in exoplanet observations as planets should have an equal probability of being discovered at any point in their secular cycle. We test this possibility by numerically integrating known multiplanet systems and determining the relative time each planet spends in a given eccentricity range and then comparing this distribution of eccentricity values to the observational uncertainty. We find that planets tend to spend more time near their minimum and maximum values as they represent turning points in the oscillations. Moreover the uncertainties for many eccentricities are so large that we cannot make strong statements regarding the possibility that planets are being discovered at their extreme eccentricities too often. However as uncertainties become smaller and more multiplanet systems are discovered this potential bias should be revisited.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Secular+Behavior+of+Exoplanetary+Systems:+Self-Consistency+and+Comparisons+With+The+Planet-Planet+Scattering+Hypothesis+Timpe+Barnes+Kopparapu+Raymond+Greenberg+Gorelick,http://research.google.com/pubs/pub42926.html
Advanced DSP for 400Gb/s and beyond Optical Networks,JOURNAL OF LIGHTWAVE TECHNOLOGY vol. Vol. 32 (2014) pp. 2716-2725,2014,Xiang Zhou Lynn Nelson,@article{44204 title = {Advanced DSP for 400Gb/s and beyond Optical Networks} author = {Xiang Zhou and Lynn Nelson} year = 2014 URL = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6810180} journal = {JOURNAL OF LIGHTWAVE TECHNOLOGY} pages = {2716-2725} volume = {Vol. 32} },This paper presents a systematic review of several digital signal processing (DSP)-enabled technologies recently proposed and demonstrated for high spectral efficiency (SE) 400Gb/s – class and beyond optical networks. These include 1) a newly proposed SE-adaptable optical modulation technology _ time-domain hybrid quadrature amplitude modulation (QAM) 2) two advanced transmitter side digital spectral shaping technologies _ Nyquist signaling (for spectrally-efficient multiplexing) and digital pre-equalization (for improving tolerance toward channel narrowing effects) and 3) a newly proposed training-assisted two-stage carrier phase recovery algorithm that is designed to address the detrimental cyclic phase slipping problem with minimal training overhead. Additionally this paper presents a novel DSP-based method for mitigation of equalizer-enhanced phase noise impairments. It is shown that performance degradation caused by the interaction between the long-memory chromatic dispersion (CD) compensating filter/equalizer and local oscillator (LO) laser phase noise can be effectively mitigated by replacing the commonly used fast single-tap phase-rotation-based equalizer (for typical carrier phase recovery) with a fast multi-tap linear equalizer. Finally brief reviews of two high-SE 400Gb/s-class WDM transmission experiments employing these advanced DSP algorithms are presented.,http://research.google.com/pubs/archive/44204.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Advanced+DSP+for+400Gb/s+and+beyond+Optical+Networks+Zhou+Nelson,http://research.google.com/pubs/pub44204.html
Word Embeddings for Speech Recognition,Proceedings of the 15th Conference of the International Speech Communication Association Interspeech (2014),2014,Samy Bengio Georg Heigold,@inproceedings{42543 title = {Word Embeddings for Speech Recognition} author = {Samy Bengio and Georg Heigold} year = 2014 booktitle = {Proceedings of the 15th Conference of the International Speech Communication Association {Interspeech}} },Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models it is interesting to consider approaches that relax the assumption that words are made of states. We present here an alternative construction where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary. Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate.,http://research.google.com/pubs/archive/42543.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Word+Embeddings+for+Speech+Recognition+Bengio+Heigold,http://research.google.com/pubs/pub42543.html
Software development and crunch time; and more,Communications of the ACM vol. 53 No. 7 (2010) pp. 10-11,2010,Ruben Ortega,@article{36675 title = {Software development and crunch time; and more} author = {Ruben Ortega} year = 2010 journal = {Communications of the ACM} pages = {10-11} volume = {53 No. 7} },Software Developers and Crunch Time,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Software+development+and+crunch+time;+and+more+Ortega,http://research.google.com/pubs/pub36675.html
Sticking Together: Handcrafting Personalized Communication Interfaces.,ACM International Conference on Interaction Design and Children (IDC) 2011 ACM,2011,Natalie Freed Jie Qi Adam Stephenson Hayes Raffle Leah Buechley Cynthia Breazeal,@inproceedings{37236 title = {Sticking Together: Handcrafting Personalized Communication Interfaces.} author = {Natalie Freed and Jie Qi and Adam Stephenson and Hayes Raffle and Leah Buechley and Cynthia Breazeal} year = 2011 URL = {http://www.hayesraffle.com/wp-content/uploads/stickingtogether-idc2011.pdf} booktitle = {ACM International Conference on Interaction Design and Children (IDC) 2011} },We present I/O Stickers adhesive sensors and actuators thatchildren can use to create personalized remote communicationinterfaces. By attaching I/O Stickers to special greeting cardschildren can invent ways to communicate with long-distanceloved ones with personalized connected messages. Childrendecorate these cards with their choice of craft materials creativelyexpressing themselves while making a functioning interface. Thelow-bandwidth connections leave room for children to design notonly the look and function but also the signification of theconnections. We describe the design of the I/O Stickers a varietyof artifacts children have created and future directions for thetoolkit. Preliminary results indicate that I/O Stickers are beginningto make a space for creative learning about communication and tomake keeping in touch playful and meaningful.,http://www.hayesraffle.com/wp-content/uploads/stickingtogether-idc2011.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sticking+Together:+Handcrafting+Personalized+Communication+Interfaces.+Freed+Qi+Stephenson+Raffle+Buechley+Breazeal,http://research.google.com/pubs/pub37236.html
Machine Learning for Dialog State Tracking: A Review,Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing (2015),2015,Matthew Henderson,@inproceedings{44018 title = {Machine Learning for Dialog State Tracking: A Review} author = {Matthew Henderson} year = 2015 booktitle = {Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing} },Spoken dialog systems help users achieve a task using natural language. Noisy speech recognition and ambiguity in natural language motivate statistical approaches that exploit distributions over the user's goal at every step in the dialog. The task of tracking these distributions termed Dialog State Tracking is therefore an essential component of any Spoken dialog system. In recent years the Dialog State Tracking Challenges have provided a common test-bed and evaluation framework for this task as well as labeled dialog data. As a result a variety of machine-learned methods have been successfully applied to Dialog State Tracking. This paper reviews the machine-learning techniques that have been adapted to Dialog State Tracking and gives an overview of published evaluations. Discriminative machine-learned methods outperform generative and rule-based methods the previous state-of-the-art.,http://research.google.com/pubs/archive/44018.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+Learning+for+Dialog+State+Tracking:+A+Review+Henderson,http://research.google.com/pubs/pub44018.html
Automatic Language Identification Using Deep Neural Networks,Proc. ICASSP IEEE (2014),2014,Ignacio Lopez-Moreno Javier Gonzalez-Dominguez Oldrich Plchot,@inproceedings{42538 title = {Automatic Language Identification Using Deep Neural Networks} author = {Ignacio Lopez-Moreno and Javier Gonzalez-Dominguez and Oldrich Plchot} year = 2014 booktitle = {Proc. ICASSP} },This work studies the use of deep neural networks (DNNs) to address automatic language identification (LID). Motivated by their recent success in acoustic modelling we adapt DNNs to the problem of identifying the language of a given spoken utterance from short-term acoustic features. The proposed approach is compared to state-of-the-art i-vector based acoustic systems on two different datasets: Google 5M LID corpus and NIST LRE 2009. Results show how LID can largely benefit from using DNNs especially when a large amount of training data is available. We found relative improvements up to 70% in Cavg over the baseline system.,http://research.google.com/pubs/archive/42538.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatic+Language+Identification+Using+Deep+Neural+Networks+Lopez+Moreno+Gonzalez-Dominguez+Plchot,http://research.google.com/pubs/pub42538.html
RFC7344 - Automating DNSSEC Delegation Trust Maintenance,IETF RFCs Internet Engineering Task Force (2014),2014,Warren Kumari,@incollection{42943 title = {RFC7344 - Automating DNSSEC Delegation Trust Maintenance} author = {Warren Kumari} year = 2014 URL = {http://www.rfc-editor.org/rfc/rfc7344.txt} booktitle = {IETF RFCs} },This document describes a method to allow DNS Operators to more easily update DNSSEC Key Signing Keys using the DNS as a communication channel. The technique described is aimed at delegations in which it is currently hard to move information from the Child to Parent.,http://www.rfc-editor.org/rfc/rfc7344.txt,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7344+-+Automating+DNSSEC+Delegation+Trust+Maintenance+Teitelman,http://research.google.com/pubs/pub42943.html
The Power of Comparative Reasoning,International Conference on Computer Vision IEEE (2011),2011,Jay Yagnik Dennis Strelow David Ross Ruei-Sung Lin,@inproceedings{37298 title = {The Power of Comparative Reasoning} author = {Jay Yagnik and Dennis Strelow and David Ross and Ruei-Sung Lin} year = 2011 booktitle = {International Conference on Computer Vision} },Rank correlation measures are known for their resilience to perturbations in numeric values and are widely used in many evaluation metrics. Such ordinal measures have rarely been applied in treatment of numeric features as a representational transformation. We emphasize the beneﬁts of ordinal representations of input features both theoretically and empirically. We present a family of algorithms for computing ordinal embeddings based on partial order statistics. Apart from having the stability beneﬁts of ordinal measures these embeddings are highly nonlinear giving rise to sparse feature spaces highly favored by several machine learning methods. These embeddings are deterministic data independent and by virtue of being based on partial order statistics add another degree of resilience to noise. These machine-learning-free methods when applied to the task of fast similarity search outperform state-of-theart machine learning methods with complex optimization setups. For solving classiﬁcation problems the embeddings provide a nonlinear transformation resulting in sparse binary codes that are well-suited for a large class of machine learning algorithms. These methods show signiﬁcant improvement on VOC 2010 using simple linear classiﬁers which can be trained quickly. Our method can be extended to the case of polynomial kernels while permitting very efﬁcient computation. Further since the popular MinHash algorithm is a special case of our method we demonstrate an efﬁcient scheme for computing MinHash on conjunctions of binary features. The actual method can be implemented in about 10 lines of code in most languages (2 lines in MATLAB) and does not require any data-driven optimization.,http://research.google.com/pubs/archive/37298.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Power+of+Comparative+Reasoning+Yagnik+Strelow+Ross+Lin,http://research.google.com/pubs/pub37298.html
From Freebase to Wikidata: The Great Migration,World Wide Web Conference ACM (2016),2016,Thomas Pellissier Tanon Denny Vrande_i_ Sebastian Schaffert Thomas Steiner Lydia Pintscher,@inproceedings{44818 title = {From Freebase to Wikidata: The Great Migration} author = {Thomas Pellissier Tanon and Denny Vrande_i_ and Sebastian Schaffert and Thomas Steiner and Lydia Pintscher} year = 2016 booktitle = {World Wide Web Conference} },Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia’s Wikidata and Google’s Freebase. Due to the success of Wikidata Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper we report on the ongoing transfer efforts and data mapping challenges and provide an analysis of the effort so far. We describe the Primary Sources Tool which aims to facilitate this and future data migrations. Throughout the migration we have gained deep insights into both Wikidata and Freebase and share and discuss detailed statistics on both knowledge bases.,http://research.google.com/pubs/archive/44818.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=From+Freebase+to+Wikidata:+The+Great+Migration+Tanon+Vrande%C4%8Di%C4%87+Schaffert+Steiner+Pintscher,http://research.google.com/pubs/pub44818.html
Taxonomy Discovery for Personalized Recommendation,ACM International Conference on Web Search And Data Mining (WSDM) (2014),2014,Yuchen Zhang Amr Ahmed Vanja Josifovski Alexander J Smola,@inproceedings{42499 title = {Taxonomy Discovery for Personalized Recommendation} author = {Yuchen Zhang and Amr Ahmed and Vanja Josifovski and Alexander J Smola} year = 2014 booktitle = {ACM International Conference on Web Search And Data Mining (WSDM)} },Personalized recommender systems based on latent factor models are widely used to increase sales in e-commerce. Such systems use the past behavior of users to recommend new items that are likely to be of interest to them. However latent factor model suffer from sparse user-item interaction in online shopping data: for a large portion of items that do not have sufficient purchase records their latent factors cannot be estimated accurately. In this paper we propose a novel approach that automatically discovers the taxonomies from online shopping data and jointly learns a taxonomy-based recommendation system. Out model is non-parametric and can learn the taxonomy structure automatically from the data. Since the taxonomy allows purchase data to be shared between item- s it effectively improves the accuracy of recommending tail items by sharing strength with the more frequent items. Ex- periments on a large-scale online shopping dataset confirm that our proposed model improves significantly over state-of- the-art latent factor models. Moreover our model generates high-quality and human readable taxonomies. Finally us- ing the algorithm-generated taxonomy our model even out- performs latent factor models based on the human-induced taxonomy thus alleviating the need for costly manual taxonomy generation.,http://research.google.com/pubs/archive/42499.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Taxonomy+Discovery+for+Personalized+Recommendation+Zhang+Ahmed+Josifovski+Smola,http://research.google.com/pubs/pub42499.html
Discovering Groups of People in Images,European Conference on Computer Vision (ECCV) (2014),2014,Wongun Choi Yu-Wei Chao Caroline Pantofaru Silvio Savarese,@inproceedings{43104 title = {Discovering Groups of People in Images} author = {Wongun Choi and Yu-Wei Chao and Caroline Pantofaru and Silvio Savarese} year = 2014 booktitle = {European Conference on Computer Vision (ECCV)} },Understanding group activities from images is an important yet challenging task. This is because there is an exponentially large number of semantic and geometrical relationships among individuals that one must model in order to effectively recognize and localize the group activities. Rather than focusing on directly recognizing group activities as most of the previous works do we advocate the importance of introducing an intermediate representation for modeling groups of humans which we call structure groups. Such groups define the way people spatially interact with each other. People might be facing each other to talk while others sit on a bench side by side and some might stand alone. In this paper we contribute a method for identifying and localizing these structured groups in a single image despite their varying viewpoints number of participants and occlusions. We propose to learn an ensemble of discriminative interaction patterns to encode the relationships between people in 3D and introduce a novel efficient iterative augmentation algorithm for solving this complex inference problem. A nice byproduct of the inference scheme is an approximate 3D layout estimate of the structured groups in the scene. Finally we contribute an extremely challenging new dataset that contains images each showing multiple people performing multiple activities. Extensive evaluation confirms our theoretical findings.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discovering+Groups+of+People+in+Images+Choi+Chao+Pantofaru+Savarese,http://research.google.com/pubs/pub43104.html
RealPigment: Paint Compositing by Example,Proceedings of the Workshop on Non-Photorealistic Animation and Rendering NPAR ACM New York NY USA (2014) pp. 21-30,2014,Jingwan Lu Stephen DiVerdi Willa Chen Connelly Barnes Adam Finkelstein,@inproceedings{43465 title = {RealPigment: Paint Compositing by Example} author = {Jingwan Lu and Stephen DiVerdi and Willa Chen and Connelly Barnes and Adam Finkelstein} year = 2014 URL = {http://gfx.cs.princeton.edu/pubs/Lu_2014_RPC/} booktitle = {Proceedings of the Workshop on Non-Photorealistic Animation and Rendering NPAR} pages = {21-30} address = {New York NY USA} },The color of composited pigments in digital painting is generally computed one of two ways: either alpha blending in RGB or the Kubelka-Munk equation (KM). The former fails to reproduce paint like appearances while the latter is difficult to use. We present a data-driven pigment model that reproduces arbitrary compositing behavior by interpolating sparse samples in a high dimensional space. The input is an of a color chart which provides the composition samples. We propose two different prediction algorithms one doing simple interpolation using radial basis functions (RBF) and another that trains a parametric model based on the KM equation to compute novel values. We show that RBF is able to reproduce arbitrary compositing behaviors even non-paint-like such as additive blending while KM compositing is more robust to acquisition noise and can generalize results over a broader range of values.,http://research.google.com/pubs/archive/43465.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RealPigment:+Paint+Compositing+by+Example+Lu+DiVerdi+Chen+Barnes+Finkelstein,http://research.google.com/pubs/pub43465.html
On Estimating the Average Degree,23rd International World Wide Web Conference WWW '14 ACM (2014) (to appear),2014,Anirban Dasgupta Ravi Kumar Tamas Sarlos,@inproceedings{42030 title = {On Estimating the Average Degree} author = {Anirban Dasgupta and Ravi Kumar and Tamas Sarlos} year = 2014 booktitle = {23rd International World Wide Web Conference WWW '14} },Networks are characterized by nodes and edges. While there has been a spate of recent work on estimating the number of nodes in a network the edge-estimation question appears to be largely unaddressed. In this work we consider the problem of estimating the average degree of a large network using efficient random sampling where the number of nodes is not known to the algorithm. We propose a new estimator for this problem that relies on access to edge samples under a prescribed distribution. Next we show how to efficiently realize this ideal estimator in a random walk setting. Our estimator has a natural and simple implementation using random walks; we bound its performance in terms of the mixing time of the underlying graph. We then show that our estimators are both provably and practically better than many natural estimators for the problem. Our work contrasts with existing theoretical work on estimating average degree which assume a uniform random sample of nodes is available and the number of nodes is known.,http://research.google.com/pubs/archive/42030.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+Estimating+the+Average+Degree+Dasgupta+Kumar+Sarlos,http://research.google.com/pubs/pub42030.html
Optimal trajectory control for parallel single phase H-bridge inverters,Decision and Control (CDC) 2015 IEEE 54th Annual Conference on IEEE pp. 1983 - 1990,2015,David K. Fork Seungil You Ross Koningstein,@inproceedings{44831 title = {Optimal trajectory control for parallel single phase H-bridge inverters} author = {David K. Fork and Seungil You and Ross Koningstein} year = 2015 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7402498} booktitle = {Decision and Control (CDC) 2015 IEEE 54th Annual Conference on} pages = {1983 -- 1990} },We describe a novel inverter control method that solves an optimization problem during each switching interval to closely follow a virtual impedance control law. We report droop behavior over a wide range of applied loads and power sharing among multiple inverters.,http://research.google.com/pubs/archive/44831.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimal+trajectory+control+for+parallel+single+phase+H-bridge+inverters+Fork+You+Koningstein,http://research.google.com/pubs/pub44831.html
Student-t based Robust Spatio-Temporal Prediction,IEEE 12th International Conference on Data Mining IEEE Brussels Belgium (2012) pp. 151-160,2012,Yang Chen Feng Chen Jing Dai T. Charles Clancy Yao-Jan Wu,@inproceedings{41177 title = {Student-t based Robust Spatio-Temporal Prediction} author = {Yang Chen and Feng Chen and Jing Dai and T. Charles Clancy and Yao-Jan Wu} year = 2012 booktitle = {IEEE 12th International Conference on Data Mining} pages = {151-160} address = {Brussels Belgium} },This paper describes an efficient and effective design of Robust Spatio-Temporal Prediction based on Student’s t distribution namely St-RSTP to provide estimations based on observations over spatio-temporal neighbors. The proposed St-RSTP is more resilient to outliers or other small departures from model assumptions than its ancestor the Spatio-Temporal Random Effects (STRE) model. STRE is a state-of-the-art statistical model with linear order complexity for large scale processing. However it assumes Gaussian observations which has the well-known limitation of non-robustness. In our St-RSTP design the measurement error follows Student’s t distribution instead of a traditional Gaussian distribution. This design reduces the influence of outliers improves prediction quality and keeps the problem analytically intractable. We propose a novel approximate inference approach which approximates the model into the form that separates the high dimensional latent variables into groups and then estimates the posterior distributions of different groups of variables separately in the framework of Expectation Propagation. As a good property our approximate approach degeneralizes to the standard STRE based prediction when the degree of freedom of the Student’s t distribution is set to infinite. Extensive experimental evaluations based on both simulation and real-life data sets demonstrated the robustness and the efficiency of our Student-t prediction model. The proposed approach provides critical functionality for stochastic processes on spatio-temporal data.,http://research.google.com/pubs/archive/41177.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Student-t+based+Robust+Spatio-Temporal+Prediction+Chen+Chen+Dai+Clancy+Wu,http://research.google.com/pubs/pub41177.html
"It's Time To Retire the ""n >= 30"" rule.",Proceedings of the Joint Statistical Meetings American Statistical Association Alexandria VA (2008),2008,Tim Hesterberg,"@inproceedings{34906 title = {It's Time To Retire the ""n >= 30"" rule.} author = {Tim Hesterberg} year = 2008 URL = {http://home.comcast.net/~timhesterberg/articles/JSM08-n30.pdf} booktitle = {Proceedings of the Joint Statistical Meetings} address = {Alexandria VA} }",The old rule of using z or t tests or confidence intervals if n >= 30 is a relic of the pre-computer era and should be discarded in favor of bootstrap-based diagnostics. The diagnostics will surprise many statisticians who don't realize how lousy the classical inferences are. For example 95% confidence intervals should miss 2.5% on each side and we might expect the actual non-coverage to be within 10% of that. Using a t interval this requires n > 5000 for a moderately-skewed (exponential) population. There are better confidence intervals and tests bootstrap and others. The bootstrap also offers pedagogical benefits in teaching sampling distributions and other statistical concepts offering actual distributions that can be viewed using histograms and other familiar techniques.,http://research.google.com/pubs/archive/34906.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=It's+Time+To+Retire+the+%22n+%3E%3D+30%22+rule.+Hesterberg,http://research.google.com/pubs/pub34906.html
Automata Evaluation and Text Search Protocols with Simulation Based Security,Google Inc. (2010),2010,Carmit Hazay Rosario Gennaro Jeffrey Sorensen,@techreport{36642 title = {Automata Evaluation and Text Search Protocols with Simulation Based Security} author = {Carmit Hazay and Rosario Gennaro and Jeffrey Sorensen} year = 2010 URL = {http://eprint.iacr.org/2010/484.pdf} institution = {Google Inc.} },This paper presents an efficient protocol for securely computing the fundamental problem of pattern matching. This problem is defined in the two-party setting where party P1 holds a pattern and party P2 holds a text. The goal of P1 is to learn where the pattern appears in the text without revealing it to P2 or learning anything else about P2's text. Our protocol is the first to address this problem with full security in the face of malicious adversaries. The construction is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem party P1 holds an automaton and party P2 holds an input string and they need to decide if the automaton accepts the input without learning anything else.,http://research.google.com/pubs/archive/36642.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automata+Evaluation+and+Text+Search+Protocols+with+Simulation+Based+Security+Hazay+Gennaro+Sorensen,http://research.google.com/pubs/pub36642.html
Confucius and Its Intelligent Disciples: Integrating Social with Search,Proceedings of VLDB 2010 36th International Conference on Very Large Data Bases VLDB Endowment pp. 1505-1516,2010,Xiance Si Edward Y. Chang Zoltan Gyongyi Maosong Sun,@inproceedings{36900 title = {Confucius and Its Intelligent Disciples: Integrating Social with Search} author = {Xiance Si and Edward Y. Chang and Zoltan Gyongyi and Maosong Sun} year = 2010 URL = {http://www.vldb2010.org/proceedings/files/papers/I12.pdf} booktitle = {Proceedings of VLDB 2010 36th International Conference on Very Large Data Bases} pages = {1505-1516} },Q&A sites continue to flourish as a large number of users rely on them as useful substitutes for incomplete or missing search results. In this paper we present our experience with developing Confucius a Google Q&A service launched in 21 countries and four languages by the end of 2009. Confucius employs six data mining subroutines to harness synergy between web search and social networks. We present these subroutines’ design goals algorithms and their effects on service quality. We also describe techniques for and experience with scaling the subroutines to mine massive data sets.,http://research.google.com/pubs/archive/36900.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Confucius+and+Its+Intelligent+Disciples:+Integrating+Social+with+Search+Si+Chang+Gyongyi+Sun,http://research.google.com/pubs/pub36900.html
Overlapping Experiment Infrastructure: More Better Faster Experimentation,Proceedings 16th Conference on Knowledge Discovery and Data Mining ACM Washington DC (2010) pp. 17-26,2010,Diane Tang Ashish Agarwal Deirdre O'Brien Mike Meyer,@inproceedings{36500 title = {Overlapping Experiment Infrastructure: More Better Faster Experimentation} author = {Diane Tang and Ashish Agarwal and Deirdre O'Brien and Mike Meyer} year = 2010 booktitle = {Proceedings 16th Conference on Knowledge Discovery and Data Mining} pages = {17--26} address = {Washington DC} },At Google experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modifications to a user interface but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments how to run experiments that produce better decisions and how to run them faster. In this paper we describe Google’s overlapping experiment infrastructure that is a key component to solving these problems. In addition because an experiment infrastructure alone is insufficient we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper specifically describes the experiment system and experimental processes we have in place at Google we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications. The presentation is available online.,http://research.google.com/pubs/archive/36500.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Overlapping+Experiment+Infrastructure:+More+Better+Faster+Experimentation+Tang+Agarwal+O'Brien+Meyer,http://research.google.com/pubs/pub36500.html
Efficient Inference and Structured Learning for Semantic Role Labeling,Transactions of the Association for Computational Linguistics vol. 3 (2015) pp. 29-41,2015,Oscar Täckström Kuzman Ganchev Dipanjan Das,@article{43251 title = {Efficient Inference and Structured Learning for Semantic Role Labeling} author = {Oscar Täckström and Kuzman Ganchev and Dipanjan Das} year = 2015 journal = {Transactions of the Association for Computational Linguistics} pages = {29--41} volume = {3} },We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling. The algorithm tractably captures a majority of the structural constraints examined by prior work in this area which has resorted to either approximate methods or off-the-shelf integer linear programming solvers. In addition it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver while reaching the same solution. Furthermore we show that our structured model results in significant improvements over its local counterpart achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora.,http://research.google.com/pubs/archive/43251.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Inference+and+Structured+Learning+for+Semantic+Role+Labeling+T%C3%A4ckstr%C3%B6m+Ganchev+Das,http://research.google.com/pubs/pub43251.html
Adding Third-Party Authentication to Open edX: A Case Study,Proceedings of the Second (2015) ACM Conference on Learning @ Scale ACM New York NY USA pp. 277-280,2015,John Cox Pavel Simakov,@inproceedings{43424 title = {Adding Third-Party Authentication to Open edX: A Case Study} author = {John Cox and Pavel Simakov} year = 2015 URL = {http://doi.acm.org/10.1145/2724660.2728675} booktitle = {Proceedings of the Second (2015) ACM Conference on Learning @ Scale} pages = {277-280} address = {New York NY USA} },In this document we describe the third-party authentication system we added to Open edX. With this system Open edX administrators can allow their users to sign in with a large array of external authentication providers. We outline the features and advantages of the system describe how it can be extended and customized and highlight reusable design principles that can be applied to other authentication implementations in online education.,http://research.google.com/pubs/archive/43424.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adding+Third-Party+Authentication+to+Open+edX:+A+Case+Study+Cox+Simakov,http://research.google.com/pubs/pub43424.html
Improved Time Series Prediction and Symbolic Regression with Affine Arithmetic,Genetic Programming Theory and Practice IX Springer 233 Spring Street New York NY 10013 (2011) pp. 97-112,2011,Cassio Pennachin Moshe Looks J. A. de Vasconcelos,@inbook{37641 title = {Improved Time Series Prediction and Symbolic Regression with Affine Arithmetic} author = {Cassio Pennachin and Moshe Looks and J. A. de Vasconcelos} year = 2011 booktitle = {Genetic Programming Theory and Practice IX} pages = {97-112} address = {233 Spring Street New York NY 10013} },We show how affine arithmetic can be used to improve both the performance and the robustness of genetic programming for problems such as symbolic regression and time series prediction. Affine arithmetic is used to estimate conservative bounds on the output range of expressions during evolution which allows us to discard trees with potentially infinite bounds as well as those whose output range lies outside the desired range implied by the training dataset. Benchmark experiments are performed on 15 symbolic regression problems as well as 2 well-known time series problems. Comparison with a baseline genetic programming system shows a reduced number of ﬁtness evaluations during t raining and improved generalization on test data completely eliminating extreme errors. We also apply this technique to the problem of forecasting wind speed on a real world dataset and the use of affine arithmetic compares favorably with baseline genetic programming feedforward neural networks and support vector machines.,http://research.google.com/pubs/archive/37641.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improved+Time+Series+Prediction+and+Symbolic+Regression+with+Affine+Arithmetic+Pennachin+Looks+Vasconcelos,http://research.google.com/pubs/pub37641.html
Trends and Lessons from Three Years Fighting Malicious Extensions,USENIX Security Symposium (2015),2015,Nav Jagpal Eric Dingle Jean-Philippe Gravel Panayiotis Mavrommatis Niels Provos Moheeb Abu Rajab Kurt Thomas,@inproceedings{43824 title = {Trends and Lessons from Three Years Fighting Malicious Extensions} author = {Nav Jagpal and Eric Dingle and Jean-Philippe Gravel and Panayiotis Mavrommatis and Niels Provos and Moheeb Abu Rajab and Kurt Thomas} year = 2015 booktitle = {USENIX Security Symposium} },In this work we expose wide-spread efforts by criminals to abuse the Chrome Web Store as a platform for distributing malicious extensions. A central component of our study is the design and implementation of WebEval the first system that broadly identifies malicious extensions with a concrete measurable detection rate of 96.5%. Over the last three years we detected 9523 malicious extensions: nearly 10% of every extension submitted to the store. Despite a short window of operation---we removed 50% of malware within 25 minutes of creation---a handful of under 100 extensions escaped immediate detection and infected over 50 million Chrome users. Our results highlight that the extension abuse ecosystem is drastically different from malicious binaries: miscreants profit from web traffic and user tracking rather than email spam or banking theft.,http://research.google.com/pubs/archive/43824.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trends+and+Lessons+from+Three+Years+Fighting+Malicious+Extensions+Jagpal+Dingle+Gravel+Mavrommatis+Provos+Abu+Rajab+Thomas,http://research.google.com/pubs/pub43824.html
A Web-Based Tool for Developing Multilingual Pronunciation Lexicons,12th Annual Conference of the International Speech Communication Association (Interspeech 2011) pp. 3331-3332,2011,Samantha Ainsley Linne Ha Martin Jansche Ara Kim Masayuki Nanzawa,@inproceedings{38078 title = {A Web-Based Tool for Developing Multilingual Pronunciation Lexicons} author = {Samantha Ainsley and Linne Ha and Martin Jansche and Ara Kim and Masayuki Nanzawa} year = 2011 URL = {http://www.isca-speech.org/archive/interspeech_2011/i11_3331.html} booktitle = {12th Annual Conference of the International Speech Communication Association (Interspeech 2011)} pages = {3331--3332} },We present a web-based tool for generating and editing pronunciation lexicons in multiple languages. The tool is implemented as a web application on Google App Engine and can be accessed remotely from a web browser. The client application displays to users a textual prompt and interface that reconfigures based on language and task. It lets users generate pronunciations via constrained phoneme selection which allows users with no special training to provide phonemic transcriptions efficiently and accurately.,http://research.google.com/pubs/archive/38078.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Web-Based+Tool+for+Developing+Multilingual+Pronunciation+Lexicons+Ainsley+Ha+Jansche+Kim+Nanzawa,http://research.google.com/pubs/pub38078.html
Characterization of Impact of Transient Faults and Detection of Data Corruption Errors in Large-Scale N-Body Programs Using Graphics Processing Units,IEEE International Parallel and Distributed Processing Symposium (IPDPS) IEEE International Parallel and Distributed Processing Symposium (IPDPS) (2014) pp. 458-467,2014,Keun Soo Yim,@inproceedings{42510 title = {Characterization of Impact of Transient Faults and Detection of Data Corruption Errors in Large-Scale N-Body Programs Using Graphics Processing Units} author = {Keun Soo Yim} year = 2014 booktitle = {IEEE International Parallel and Distributed Processing Symposium (IPDPS)} pages = {458-467} },In N-body programs trajectories of simulated particles have chaotic patterns if errors are in the initial conditions or occur during some computation steps. It was believed that the global properties (e.g. total energy) of simulated particles are unlikely to be affected by a small number of such errors. In this paper we present a quantitative analysis of the impact of transient faults in GPU devices on a global property of simulated particles. We experimentally show that a single-bit error in non-control data can change the final total energy of a large- scale N-body program with ~2.1% probability. We also find that the corrupted total energy values have certain biases (e.g. the values are not a normal distribution) which can be used to reduce the expected number of re-executions. In this paper we also present a data error detection technique for N-body pro- grams by utilizing two types of properties that hold in simulated physical models. The presented technique and an existing redundancy-based technique together cover many data errors (e.g. >97.5%) with a small performance overhead (e.g. 2.3%).,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Characterization+of+Impact+of+Transient+Faults+and+Detection+of+Data+Corruption+Errors+in+Large-Scale+N-Body+Programs+Using+Graphics+Processing+Units+Yim,http://research.google.com/pubs/pub42510.html
Learning with Global Cost in Stochastic Environments,Proceedings of the 23rd Annual Conference on Learning Theory (COLT) (2010),2010,Eyal Even-Dar Shie Mannor Yishay Mansour,@inproceedings{36485 title = {Learning with Global Cost in Stochastic Environments} author = {Eyal Even-Dar and Shie Mannor and Yishay Mansour} year = 2010 booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)} },We consider an online learning setting where at each time step the decision maker has to choose how to distribute the future loss between k alternatives and then observes the loss of each alternative where the losses are assumed to come from a joint distribution. Motivated by load balancing and job scheduling we consider a global cost function (over the losses incurred by each alternative) rather than a summation of the instantaneous losses as done traditionally in online learning. Specifically we consider the global cost functions: (1) the makespan (the maximum over the alternatives) and (2) the L_d norm (over the alternatives) for d > 1. We design algorithms that guarantee logarithmic regret for this setting where the regret is measured with respect to the best static decision (one selects the same distribution over alternatives at every time step). We also show that the least loaded machine a natural algorithm for minimizing the makespan has a regret of the order of \sqrt{T} . We complement our theoretical findings with supporting experimental results.,http://research.google.com/pubs/archive/36485.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+with+Global+Cost+in+Stochastic+Environments+Even-Dar+Mannor+Mansour,http://research.google.com/pubs/pub36485.html
Minimum Description Length (MDL) Regularization for Online Learning,JMLR: Workshop and Conference Proceedings JMLR (2015) pp. 260-276,2015,Gil I Shamir,@inproceedings{44322 title = {Minimum Description Length (MDL) Regularization for Online Learning} author = {Gil I Shamir} year = 2015 URL = {http://jmlr.org/proceedings/papers/v44/shamir15.pdf} booktitle = {JMLR: Workshop and Conference Proceedings} pages = {260--276} },An approach inspired by the Minimum Description Length (MDL) principle is proposed for adaptively selecting features during online learning based on their usefulness in improving the objective. The approach eliminates noisy or useless features from the optimization process leading to improved loss. Several algorithmic variations on the approach are presented. They are based on using a Bayesian mixture in each of the dimensions of the feature space. By utilizing the MDL principle the mixture reduces the dimensionality of the feature space to its subspace with the lowest loss. Bounds on the loss derived show that the loss for that subspace is essentially achieved. The approach can be tuned for trading off between model size and the loss incurred. Empirical results on large scale real-world systems demonstrate how it improves such tradeoffs. Huge model size reductions can be achieved with no loss in performance relative to standard techniques while moderate loss improvements (translating to large regret improvements) are achieved with moderate size reductions. The results also demonstrate that overfitting is eliminated by this approach.,http://jmlr.org/proceedings/papers/v44/shamir15.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Minimum+Description+Length+(MDL)+Regularization+for+Online+Learning+Shamir,http://research.google.com/pubs/pub44322.html
Graph cube: on warehousing and OLAP multidimensional networks,SIGMOD - Proceedings of the 2011 International Conference on Management of Data ACM New York NY,2011,Peixiang Zhao Xialolei Li Dong Xin Jiawei Han,@inproceedings{37657 title = {Graph cube: on warehousing and OLAP multidimensional networks} author = {Peixiang Zhao and Xialolei Li and Dong Xin and Jiawei Han} year = 2011 booktitle = {SIGMOD - Proceedings of the 2011 International Conference on Management of Data} address = {New York NY} },We consider extending decision support facilities toward large sophisticated networks upon which multidimensional attributes are associated with network entities thereby forming the so-called multidimensional networks. Data warehouses and OLAP (Online Analytical Processing) technology have proven to be effective tools for decision support on relational data. However they are not well equipped to handle the new yet important multidimensional networks. In this paper we introduce Graph Cube a new data warehousing model that supports OLAP queries effectively on large multidimensional networks. By taking account of both attribute aggregation and structure summarization of the networks Graph Cube goes beyond the traditional data cube model involved solely with numeric value based group-by’s thus resulting in a more insightful and structure-enriched aggregate network within every possible multidimensional space. Besides traditional cuboid queries a new class of OLAP queries crossboid is introduced that is uniquely useful in multidimensional networks and has not been studied before. We implement Graph Cube by combining special characteristics of multidimensional networks with the existing well-studied data cube techniques. We perform extensive experimental studies on a series of real world data sets and Graph Cube is shown to be a powerful and efficient tool for decision support on large multidimensional networks.,http://research.google.com/pubs/archive/37657.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Graph+cube:+on+warehousing+and+OLAP+multidimensional+networks+Zhao+Li+Xin+Han,http://research.google.com/pubs/pub37657.html
Discontinuous Seam-Carving for Video Retargeting,Computer Vision and Pattern Recognition (CVPR 2010),2010,Matthias Grundmann Vivek Kwatra Mei Han Irfan Essa,@inproceedings{36246 title = {Discontinuous Seam-Carving for Video Retargeting} author = {Matthias Grundmann and Vivek Kwatra and Mei Han and Irfan Essa} year = 2010 URL = {http://cpl.cc.gatech.edu/projects/videoretargeting/} booktitle = {Computer Vision and Pattern Recognition (CVPR 2010)} },We introduce a new algorithm for video retargeting that uses discontinuous seam-carving in both space and time for resizing videos. We propose a novel appearance-based temporal coherence formulation that allows for frame-by-frame processing and results in temporally discontinuous seams as opposed to geometrically smooth and continuous seams. This formulation optimizes the difference in appearance of the resultant retargeted frame to the optimal temporally coherent one and allows for carving around fast moving salient regions. Additionally we generalize the idea of appearance-based coherence to the spatial domain by introducing piece-wise spatial seams. Our spatial coherence measure minimizes the change in gradients during retargeting which preserves spatial detail better than minimization of color difference alone. We also show that retargeting based on per-frame saliency (gradient-based or feature-based) does not always lead to desirable results and propose a novel automatically computed measure of spatio-temporal saliency. As needed the user can also augment the saliency by interactive region-brushing. Our retargeting algorithm processes the video sequentially which allows us to deal with streaming videos. We demonstrate results over a wide range of video examples and evaluate the effectiveness of each component of our algorithm.,http://research.google.com/pubs/archive/36246.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discontinuous+Seam-Carving+for+Video+Retargeting+Grundmann+Kwatra+Han+Essa,http://research.google.com/pubs/pub36246.html
Learning to hash: forgiving hash functions and applications Learning to hash: forgiving hash functions and applications,Data Mining and Knowledge Discovery (2008),2008,Shumeet Baluja Michele Covell,@article{34632 title = {Learning to hash: forgiving hash functions and applications Learning to hash: forgiving hash functions and applications} author = {Shumeet Baluja and Michele Covell} year = 2008 URL = {http://www.springerlink.com/content/j4864686064m507q/?p=867df9a14e524c7e91e457147765c039&pi=5} journal = {Data Mining and Knowledge Discovery} },The problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world tasks such as music image and video retrieval. Beyond the scaling difficulties that arise with lookups in large data sets the complexity in these domains is exacerbated by an imprecise definition of similarity. In this paper we describe a method to learn a similarity function from only weakly labeled positive examples. Once learned this similarity function is used as the basis of a hash function to severely constrain the number of points considered for each lookup. Tested on a large real-world audio dataset only a tiny fraction of the points (~0.27%) are ever considered for each lookup. To increase efficiency no comparisons in the original high-dimensional space of points are required. The performance far surpasses in terms of both efficiency and accuracy a state-of-the-art Locality-Sensitive-Hashing-based (LSH) technique for the same problem and data set.,http://www.springerlink.com/content/j4864686064m507q/?p=867df9a14e524c7e91e457147765c039&pi=5,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+hash:+forgiving+hash+functions+and+applications%09%0D%0ALearning+to+hash:+forgiving+hash+functions+and+applications+Baluja+Covell,http://research.google.com/pubs/pub34632.html
Training Data Selection Based On Context-Dependent State Matching,Proceedings of ICASSP 2014,2014,Olivier Siohan,@inproceedings{42037 title = {Training Data Selection Based On Context-Dependent State Matching} author = {Olivier Siohan} year = 2014 booktitle = {Proceedings of ICASSP 2014} },In this paper we construct a data set for semi-supervised acoustic model training by selecting spoken utterances from a massive collection of anonymized Google Voice Search utterances. Semi-supervised training usually retains high-confidence utterances which are presumed to have an accurate hypothesized transcript a necessary condition for successful training. Selecting high confidence utterances can however restrict the diversity of the resulting data set. We propose to introduce a constraint enforcing that the distribution of the context-dependent state symbols obtained by running forced alignment of the hypothesized transcript matches a reference distribution estimated from a curated development set. The quality of the obtained training set is illustrated on large scale Voice Search recognition experiments and outperforms random selection of high-confidence utterances.,http://research.google.com/pubs/archive/42037.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Training+Data+Selection+Based+On+Context-Dependent+State+Matching+Siohan,http://research.google.com/pubs/pub42037.html
Training and Testing Low-degree Polynomial Data Mappings via Linear SVM,Journal of Machine Learning Research vol. 11(Apr) (2010) 1471_1490,2010,Yin-Wen Chang Cho-Jui Hsieh Kai-Wei Chang Michael Ringgaard Chih-Jen Lin,@article{36629 title = {Training and Testing Low-degree Polynomial Data Mappings via Linear SVM} author = {Yin-Wen Chang and Cho-Jui Hsieh and Kai-Wei Chang and Michael Ringgaard and Chih-Jen Lin} year = 2010 URL = {http://jmlr.csail.mit.edu/papers/v11/chang10a.html} journal = {Journal of Machine Learning Research} pages = {1471_1490} volume = {11(Apr)} },Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space but training and testing large data sets is often time consuming. In contrast we can efficiently train and test much larger data sets using linear SVM without kernels. In this work we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements.,http://jmlr.csail.mit.edu/papers/v11/chang10a.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Training+and+Testing+Low-degree+Polynomial+Data+Mappings+via+Linear+SVM+Chang+Hsieh+Chang+Ringgaard+Lin,http://research.google.com/pubs/pub36629.html
An optimal online algorithm for retrieving heavily perturbed statistical databases in the low-dimensional querying model,Proceedings of the Twenty-Fourth ACM International Conference on Information and Knowledge Management (CIKM 2015),2015,Krzysztof Choromanski Afshin Rostamizadeh Umar Syed,@inproceedings{43978 title = {An optimal online algorithm for retrieving heavily perturbed statistical databases in the low-dimensional querying model} author = {Krzysztof Choromanski and Afshin Rostamizadeh and Umar Syed} year = 2015 booktitle = {Proceedings of the Twenty-Fourth ACM International Conference on Information and Knowledge Management (CIKM 2015)} },We give the first O(1/sqrt{T})-error online algorithm for reconstructing noisy statistical databases where T is the number of (online) sample queries received. The algorithm is optimal up to the poly(log(T)) factor in terms of the error and requires only O(log T) memory. It aims to learn a hidden database-vector w* in R^d in order to accurately answer a stream of queries regarding the hidden database which arrive in an online fashion from some unknown distribution D. We assume the distribution D is defined on the neighborhood of a low-dimensional manifold. The presented algorithm runs in O(dD)-time per query where d is the dimensionality of the query-space. Contrary to the classical setting there is no separate training set that is used by the algorithm to learn the database —- the stream on which the algorithm will be evaluated must also be used to learn the database-vector. The algorithm only has access to a binary oracle O that answers whether a particular linear function of the database-vector plus random noise is larger than a threshold which is specified by the algorithm. We note that we allow for a significant O(D) amount of noise to be added while other works focused on the low noise o(sqrt{D}) setting. For a stream of T queries our algorithm achieves an average error O(1/sqrt{T}) by filtering out random noise adapting threshold values given to the oracle based on its previous answers and as a consequence recovering with high precision a projection of a database-vector w* onto the manifold defining the query-space. Our algorithm may be also applied in the adversarial machine learning context to compromise machine learning engines by heavily exploiting the vulnerabilities of the systems that output only binary signal and in the presence of significant noise.,http://research.google.com/pubs/archive/43978.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+optimal+online+algorithm+for+retrieving+heavily+perturbed+statistical+databases+in+the+low-dimensional+querying+model+Choromanski+Rostamizadeh+Syed,http://research.google.com/pubs/pub43978.html
Using Hoarding to Increase Availability in Shared File Systems,Computer and Information Science 2009. ICIS 2009. Eighth IEEE/ACIS International Conference on IEEE pp. 422 - 429,2009,Jochen Hollmann Per Stenström,@inproceedings{37674 title = {Using Hoarding to Increase Availability in Shared File Systems} author = {Jochen Hollmann and Per Stenström} year = 2009 URL = {http://dx.doi.org/10.1109/ICIS.2009.164} booktitle = {Computer and Information Science 2009. ICIS 2009. Eighth IEEE/ACIS International Conference on} pages = {422 - 429} },Many mobile devices have reached the point where the users' (active) working set is smaller than the amount of storage available and that trend is likely to continue. Currently these resources are made available for recording new data but we think that we could make better use of this capacity. Hoarding previously not accessed data could give better data coverage in cases of disconnected operation when wireless networks are not available or access to them is expensive. We gathered a trace from a university file system used by more than 5000 people over a period of 16 months. This trace is used to drive a simulation model of distributed file systems. This paper studies a novel hoarding scheme that uses the access profile of other users to predict what files a user would need in the future. This hoarding scheme is shown to avoid between 30% and 75% of remote file accesses to files that are accessed for the first time. Furthermore hoarded but not used data can be expired because we note experimentally that the population shifts focus each month.,http://dx.doi.org/10.1109/ICIS.2009.164,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Hoarding+to+Increase+Availability+in+Shared+File+Systems+Hollmann+Stenstr%C3%B6m,http://research.google.com/pubs/pub37674.html
Accuracy at the Top,NIPS: Neural Information Processing Systems Foundation (2012),2012,Stephen Boyd Corinna Cortes Mehryar Mohri Ana Radovanovic,@inproceedings{40498 title = {Accuracy at the Top} author = {Stephen Boyd and Corinna Cortes and Mehryar Mohri and Ana Radovanovic} year = 2012 booktitle = {NIPS: Neural Information Processing Systems Foundation} },We introduce a new notion of classiﬁcation accuracy based on the top _ -quantile values of a scoring function a relevant criterion in a number of problems arising for search engines. We deﬁne an algorithm optimizing a convex surrogate of the corresponding loss and show how its solution can be obtained by solving a set of convex optimization problems. We also present margin-based guarantees for this algorithm based on the top _ -quantile of the scores of the functions in the hypothesis set. Finally we report the results of several experiments in the bipartite setting evaluating the performance of our algorithm and comparing the results to several other algorithms seeking high precision at the top. In most examples our algorithm achieves a better performance in precision at the top.,http://research.google.com/pubs/archive/40498.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Accuracy+at+the+Top+Boyd+Cortes+Mohri+Radovanovic,http://research.google.com/pubs/pub40498.html
Training Highly Multi-class Linear Classifiers,Journal Machine Learning Research (JMLR) (2014) 1461-_1492,2014,Maya R. Gupta Samy Bengio Jason Weston,@article{41872 title = {Training Highly Multi-class Linear Classifiers} author = {Maya R. Gupta and Samy Bengio and Jason Weston} year = 2014 URL = {http://jmlr.org/papers/volume15/gupta14a/gupta14a.pdf} journal = {Journal Machine Learning Research (JMLR)} pages = {1461-_1492} },Classification problems with thousands or more classes often have a large variance in the confusability between classes and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method recently proposed for automatically decreasing step sizes for convex stochastic gradient descent optimization can also be profitably applied to the nonconvex optimization stochastic gradient descent training of a joint supervised dimensionality reduction and linear classifier. Experiments on ImageNet benchmark datasets and proprietary image recognition problems with 15000 to 97000 classes show substantial gains in classification accuracy compared to one-vs-all linear SVMs and Wsabie.,http://research.google.com/pubs/archive/41872.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Training+Highly+Multi-class+Linear+Classifiers+Gupta+Bengio+Weston,http://research.google.com/pubs/pub41872.html
Fuzzy Computing Applications for Anti-Money Laundering and Distributed Storage System Load Monitoring,World conference on soft computing (2011) (2011),2011,Yu-To Chen Johan Mathe,@article{37118 title = {Fuzzy Computing Applications for Anti-Money Laundering and Distributed Storage System Load Monitoring} author = {Yu-To Chen and Johan Mathe} year = 2011 journal = {World conference on soft computing (2011)} },Fuzzy computing (FC) has made a great impact in capturing human domain knowledge and modeling non-linear mapping of input-output space. In this paper we describe the design and implementation of FC systems for detection of money laundering behaviors in financial transactions and monitoring of distributed storage system load. Our objective is to demonstrate the power of FC for real-world applications which are char- acterized by imprecise uncertain data and incomplete domain knowledge. For both applications we designed fuzzy rules based on experts’ domain knowledge depending on money laundering scenarios in transactions or the “health” of a distributed storage system. In addition we developped a generic fuzzy inference engine and contributed to the open source community.,http://research.google.com/pubs/archive/37118.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fuzzy+Computing+Applications+for+Anti-Money+Laundering+and+Distributed+Storage+System+Load+Monitoring+Chen+Mathe,http://research.google.com/pubs/pub37118.html
The Power of Smartphones,IEEE Pervasive Computing vol. 13-03 (2014) pp. 76-79,2014,Roy Want,@article{42922 title = {The Power of Smartphones} author = {Roy Want} year = 2014 journal = {IEEE Pervasive Computing} pages = {76-79} volume = {13-03} },If you’re new to power monitoring in the mobile design process either when building mobile hardware or writing software-based applications this article will point you in the right direction helping you identify what characteristics to consider and what test equipment to use.,http://research.google.com/pubs/archive/42922.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Power+of+Smartphones+Want,http://research.google.com/pubs/pub42922.html
Dynamic iSCSI at Scale: Remote Paging at Google,Linux Plumbers Conference 2015,2015,Nick Black,@inproceedings{43886 title = {Dynamic iSCSI at Scale: Remote Paging at Google} author = {Nick Black} year = 2015 URL = {https://linuxplumbersconf.org/2015/ocw//system/presentations/3069/original/Public%20LPC2015%20-%20Dynamic%20iSCSI%20at%20Scale-%20Remote%20paging%20at%20Google.pdf} booktitle = {Linux Plumbers Conference 2015} },Google is experimenting with remotely mounting jobs’ binaries and data packages. Each package is served as an ext4 filesystem atop an iSCSI-mounted block device fed through dm-multipath and dm-verity. A typical job might run on O(1K) machines employing a N-to-1 multipath configuration yielding O(10K) short-lived iSCSI sessions. Google thus sees O(100K) iSCSI sessions set up and torn down every minute a rather atypical iSCSI deployment. We will explore the challenges presented by this deployment at scale.,http://research.google.com/pubs/archive/43886.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+iSCSI+at+Scale:+Remote+Paging+at+Google+Black,http://research.google.com/pubs/pub43886.html
SAC062 - ICANN SSAC Advisory Concerning the Mitigation of Name Collision Risk,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2013),2013,Warren Kumari,@incollection{42959 title = {SAC062 - ICANN SSAC Advisory Concerning the Mitigation of Name Collision Risk} author = {Warren Kumari} year = 2013 URL = {https://www.icann.org/en/system/files/files/sac-062-en.pdf} booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} },The term “name collision” refers to the situation in which a name that is properly defined in one operational domain or naming scope may appear in another domain (in which it is also syntactically valid) where users software or other functions in that domain may misinterpret it as if it correctly belonged there. The circumstances that may cause this can be accidental or malicious. In the context of Top Level Domains (TLDs) the conflicting namespaces are the DNS namespace defined in the root zone as published by the root management partners (ICANN U.S. Dept. of Commerce National Telecommunications Information Administration (NTIA) and VeriSign) and any privately defined namespace whether that namespace is defined only for the Domain Name System (DNS) or is also intended to “work” for other namespaces such as Active Directory,http://research.google.com/pubs/archive/42959.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC062+-+ICANN+SSAC+Advisory+Concerning+the+Mitigation+of+Name++Collision+Risk+Kumari,http://research.google.com/pubs/pub42959.html
Insulin Resistance: Regression and Clustering,PLoS ONE vol. 9(6) (2014),2014,Sangho Yoon,@article{42531 title = {Insulin Resistance: Regression and Clustering} author = {Sangho Yoon} year = 2014 URL = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0094129} journal = {PLoS ONE} volume = {9(6)} },In this paper we try to define insulin resistance (IR) precisely for a group of Chinese women. Our definition deliberately does not depend upon body mass index (BMI) or age although in other studies with particular random effects models quite different from models used here BMI accounts for a large part of the variability in IR. We accomplish our goal through application of Gauss mixture vector quantization (GMVQ) a technique for clustering that was developed for application to lossy data compression. Defining data come from measurements that play major roles in medical practice. A precise statement of what the data are is in Section 1. Their family structures are described in detail. They concern levels of lipids and the results of an oral glucose tolerance test (OGTT). We apply GMVQ to residuals obtained from regressions of outcomes of an OGTT and lipids on functions of age and BMI that are inferred from the data. A bootstrap procedure developed for our family data supplemented by insights from other approaches leads us to believe that two clusters are appropriate for defining IR precisely. One cluster consists of women who are IR and the other of women who seem not to be. Genes and other features are used to predict cluster membership. We argue that prediction with ‘‘main effects’’ is not satisfactory but prediction that includes interactions may be.,http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0094129,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Insulin+Resistance:+Regression+and+Clustering+Yoon,http://research.google.com/pubs/pub42531.html
General and Nested Wiberg Minimization,Computer Vision and Pattern Recognition IEEE (2012),2012,Dennis Strelow,@inproceedings{37749 title = {General and Nested Wiberg Minimization} author = {Dennis Strelow} year = 2012 URL = {http://www.dennis-strelow.com/papers/documents/strelow_cvpr12.pdf} booktitle = {Computer Vision and Pattern Recognition} },Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U linearizing V(U) about U and iteratively minimizing ||Y - UV(U)||_2 with respect to U only. This approach factors the matrix while effectively removing V from the minimization. Recently Eriksson and van den Hengel extended this approach to L1 minimizing ||Y - UV(U)||_1. We generalize their approach beyond factorization to minimize an arbitrary function that is nonlinear in each of two sets of variables. We demonstrate the idea with a practical Wiberg algorithm for L1 bundle adjustment. We also show that one Wiberg minimization can be nested inside another effectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L1 projective bundle adjustment solving for camera matrices points and projective depths. We also revisit L1 factorization giving a greatly simplified presentation of Wiberg L1 factorization and presenting a successive linear programming factorization algorithm. Successive linear programming outperforms L1 Wiberg for most large inputs establishing a new state-of-the-art for for those cases.,http://research.google.com/pubs/archive/37749.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=General+and+Nested+Wiberg+Minimization+Strelow,http://research.google.com/pubs/pub37749.html
Security Vulnerability in Processor-Interconnect Router Design,Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security ACM New York NY pp. 358-368,2014,WonJun Song John Kim Jae W. Lee Dennis Abts,@inproceedings{43448 title = {Security Vulnerability in Processor-Interconnect Router Design} author = {WonJun Song and John Kim and Jae W. Lee and Dennis Abts} year = 2014 URL = {http://dl.acm.org/citation.cfm?id=2660267.2660290&coll=DL&dl=ACM&CFID=505496270&CFTOKEN=98216298} booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security} pages = {358--368} address = {New York NY} },Servers that consist of multiple nodes and sockets are interconnected together with a high-bandwidth low latency processor interconnect network such as Intel QPI or AMD Hypertransport technologies. The different nodes exchange packets through routers which communicate with other routers. A key component of a router is the routing table which determines which output port an arriving packet should be forwarded through. However because of the flexibility (or programmability) of the routing tables we show that it can result in security vulnerability. We describe the procedures for how the routing tables in a processor-interconnect router can be modified. Based on these modifications we propose new system attacks in a server which include both performance attacks by degrading the latency and/or the bandwidth of the processor interconnect as well as a livelock attack that hangs the system. We implement these system on an 8-node AMD server and show how performance can be significantly degraded. Based on this vulnerability we propose alternative solutions that provide various trade-off in terms of flexibility and cost while minimizing the routing table security vulnerability.,http://research.google.com/pubs/archive/43448.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Security+Vulnerability+in+Processor-Interconnect+Router+Design+Song+Kim+Lee+Abts,http://research.google.com/pubs/pub43448.html
Improving User Topic Interest Profiles by Behavior Factorization,Proceedings of the 24th International Conference on World Wide Web International World Wide Web Conferences Steering Committee Republic and Canton of Geneva Switzerland (2015) pp. 1406-1416,2015,Zhe Zhao Zhiyuan Cheng Lichan Hong Ed H. Chi,@inproceedings{43807 title = {Improving User Topic Interest Profiles by Behavior Factorization} author = {Zhe Zhao and Zhiyuan Cheng and Lichan Hong and Ed H. Chi} year = 2015 booktitle = {Proceedings of the 24th International Conference on World Wide Web} pages = {1406-1416} address = {Republic and Canton of Geneva Switzerland} },"Many recommenders aim to provide relevant recommendations to users by building personal topic interest profiles and then using these profiles to find interesting contents for the user. In social media recommender systems build user profiles by directly combining users' topic interest signals from a wide variety of consumption and publishing behaviors such as social media posts they authored commented on +1'd or liked. Here we propose to separately model users' topical interests that come from these various behavioral signals in order to construct better user profiles. Intuitively since publishing a post requires more effort the topic interests coming from publishing signals should be more accurate of a user's central interest than say a simple gesture such as a +1. By separating a single user's interest profile into several behavioral profiles we obtain better and cleaner topic interest signals as well as enabling topic prediction for different types of behavior such as topics that the user might +1 or comment on but might never write a post on that topic. To do this at large scales in Google+ we employed matrix factorization techniques to model each user's behaviors as a separate example entry in the input user-by-topic matrix. Using this technique which we call ""behavioral factorization"" we implemented and built a topic recommender predicting user's topical interests using their actions within Google+. We experimentally showed that we obtained better and cleaner signals than baseline methods and are able to more accurately predict topic interests as well as achieve better coverage.",http://research.google.com/pubs/archive/43807.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+User+Topic+Interest+Profiles+by+Behavior+Factorization+Zhao+Cheng+Hong+Chi,http://research.google.com/pubs/pub43807.html
A Butterfly Structured Design of The Hybrid Transform Coding Scheme,Picture Coding Symposium IEEE (2013) pp. 1-4,2013,Jingning Han Yaowu Xu Debargha Mukherjee,@inproceedings{41418 title = {A Butterfly Structured Design of The Hybrid Transform Coding Scheme} author = {Jingning Han and Yaowu Xu and Debargha Mukherjee} year = 2013 booktitle = {Picture Coding Symposium} pages = {1-4} },The hybrid transform coding scheme that alternates amongst the asymmetric discrete sine transform (ADST) and the discrete cosine transform (DCT) depending on the boundary prediction conditions is an efficient tool for video and image compression. It optimally exploits the statistical characteristics of prediction residual thereby achieving significant coding performance gains over the conventional DCT-based approach. A practical concern lies in the intrinsic conflict between transform kernels of ADST and DCT which prevents a butterfly structured implementation for parallel computing. Hence the hybrid transform coding scheme has to rely on matrix multiplication which presents a speed-up barrier due to under-utilization of the hardware especially for larger block sizes. In this work we devise a novel ADST-like transform whose kernel is consistent with that of DCT thereby enabling butterfly structured computation flow while largely retaining the performance advantages of hybrid transform coding scheme in terms of compression efficiency. A prototype implementation of the proposed butterfly structured hybrid transform coding scheme is available in the VP9 codec repository.,http://research.google.com/pubs/archive/41418.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Butterfly+Structured+Design+of+The+Hybrid+Transform+Coding+Scheme+Han+Xu+Mukherjee,http://research.google.com/pubs/pub41418.html
Effects of Language Modeling and its Personalization on Touchscreen Typing Performance,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015) ACM New York NY USA pp. 649-658,2015,Andrew Fowler Kurt Partridge Ciprian Chelba Xiaojun Bi Tom Ouyang Shumin Zhai,@inproceedings{43272 title = {Effects of Language Modeling and its Personalization on Touchscreen Typing Performance} author = {Andrew Fowler and Kurt Partridge and Ciprian Chelba and Xiaojun Bi and Tom Ouyang and Shumin Zhai} year = 2015 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)} pages = {649--658} address = {New York NY USA} },Modern smartphones correct typing errors and learn userspecific words (such as proper names). Both techniques are useful yet little has been published about their technical specifics and concrete benefits. One reason is that typing accuracy is difficult to measure empirically on a large scale. We describe a closed-loop smart touch keyboard (STK) evaluation system that we have implemented to solve this problem. It includes a principled typing simulator for generating human-like noisy touch input a simple-yet-effective decoder for reconstructing typed words from such spatial data a large web-scale background language model (LM) and a method for incorporating LM personalization. Using the Enron email corpus as a personalization test set we show for the first time at this scale that a combined spatial/language model reduces word error rate from a pre-model baseline of 38.4% down to 5.7% and that LM personalization can improve this further to 4.6%.,http://research.google.com/pubs/archive/43272.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Effects+of+Language+Modeling+and+its+Personalization+on+Touchscreen+Typing+Performance+Fowler+Partridge+Chelba+Bi+Ouyang+Zhai,http://research.google.com/pubs/pub43272.html
Indexing the World Wide Web: The Journey So Far,Next Generation Search Engines: Advanced Models for Information Retrieval IGI-Global (2012) pp. 1-28,2012,Abhishek Das Ankit Jain,@inbook{37043 title = {Indexing the World Wide Web: The Journey So Far} author = {Abhishek Das and Ankit Jain} year = 2012 URL = {http://www.igi-global.com/chapter/indexing-world-wide-web/64418} booktitle = {Next Generation Search Engines: Advanced Models for Information Retrieval} pages = {1-28} },In this chapter we describe the key indexing components of today’s web search engines. As the World Wide Web has grown the systems and methods for indexing have changed significantly. We present the data structures used the features extracted the infrastructure needed and the options available for designing a brand new search engine. We highlight techniques that improve relevance of results discuss trade-offs to best utilize machine resources and cover distributed processing concepts in this context. In particular we delve into the topics of indexing phrases instead of terms storage in memory vs. on disk and data partitioning. We will finish with some thoughts on information organization for the newly emerging data-forms.,http://www.igi-global.com/chapter/indexing-world-wide-web/64418,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Indexing+the+World+Wide+Web:+The+Journey+So+Far+Das+Jain,http://research.google.com/pubs/pub37043.html
Micro-Auction-Based Traffic-Light Control: Responsive Local Decision Making,International Conference on Intelligent Transportation Systems (2015),2015,Michele Covell Shumeet Baluja Rahul Sukthankar,@inproceedings{43987 title = {Micro-Auction-Based Traffic-Light Control: Responsive Local Decision Making} author = {Michele Covell and Shumeet Baluja and Rahul Sukthankar} year = 2015 URL = {http://www.esprockets.com/papers/itsc-control.pdf} booktitle = {International Conference on Intelligent Transportation Systems} },Real-time responsive optimization of traffic flow serves to address important practical problems: reducing drivers’ wasted time and improving city-wide efficiency as well as reducing gas emissions and improving air quality. Much of the current research in traffic-light optimization relies on extending the capabilities of basic traffic lights to either communicate with each other or communicate with vehicles. However before such capabilities become ubiquitous opportunities exist to improve traffic lights by being more responsive to current traffic situations within the existing deployed infrastructure. In this paper we use micro-auctions as the organizing principle with which to incorporate local induction loop information; no other outside sources of information are assumed. At every time step in which a phase change is permitted each light conducts a decentralized weighted micro-auction to determine which phase to instantiate next. We test the lights on real-world data collected over a period of several weeks around the Mountain View California area. In our simulations the auction mechanisms based only on local sensor data surpass longer-term planning approaches that rely on widely placed sensors and communications.,http://www.esprockets.com/papers/itsc-control.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Micro-Auction-Based+Traffic-Light+Control:+Responsive+Local+Decision+Making+Covell+Baluja+Sukthankar,http://research.google.com/pubs/pub43987.html
Mastering the game of Go with deep neural networks and tree search,Nature vol. 529 (2016) pp. 484-503,2016,David Silver Aja Huang Christopher J. Maddison Arthur Guez Laurent Sifre George van den Driessche Julian Schrittwieser Ioannis Antonoglou Veda Panneershelvam Marc Lanctot Sander Dieleman Dominik Grewe John Nham Nal Kalchbrenner Ilya Sutskever Timothy Lillicrap Madeleine Leach Koray Kavukcuoglu Thore Graepel Demis Hassabis,@article{44806 title = {Mastering the game of Go with deep neural networks and tree search} author = {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis} year = 2016 URL = {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html} journal = {Nature} pages = {484--503} volume = {529} },The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games and reinforcement learning from games of self-play. Without any lookahead search the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm our program AlphaGo achieved a 99.8% winning rate against other Go programs and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated full-sized game of Go a feat previously thought to be at least a decade away.,http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mastering+the+game+of+Go+with+deep+neural+networks+and+tree+search+Silver+Huang+Maddison+Guez+Sifre+van+den+Driessche+Schrittwieser+Antonoglou+Panneershelvam+Lanctot+Dieleman+Grewe+Nham+Kalchbrenner+Sutskever+Lillicrap+Leach+Kavukcuoglu+Graepel+Hassabis,http://research.google.com/pubs/pub44806.html
Building Rome in a day,Communications of the ACM vol. 54 (2011) pp. 105-112,2011,Sameer Agarwal Yasutaka Furukawa Noah Snavely Ian Simon Brian Curless Steven M. Seitz Rick Szeliski,@article{40599 title = {Building Rome in a day} author = {Sameer Agarwal and Yasutaka Furukawa and Noah Snavely and Ian Simon and Brian Curless and Steven M. Seitz and Rick Szeliski} year = 2011 journal = {Communications of the ACM} pages = {105--112} volume = {54} },We present a system that can reconstruct 3D geometry from large unorganized collections of photographs such as those found by searching for a given city (e.g. Rome) on Internet photo-sharing sites. Our system is built on a set of new distributed computer vision algorithms for image matching and 3D reconstruction designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Building+Rome+in+a+day+Agarwal+Furukawa+Snavely+Simon+Curless+Seitz+Szeliski,http://research.google.com/pubs/pub40599.html
Large Scale Deep Learning,Tsinghua University (2014),2014,Jeffrey Dean,@misc{43150 title = {Large Scale Deep Learning} author = {Jeffrey Dean} year = 2014 URL = {http://research.google.com/people/jeff/CIKM-keynote-Nov2014.pdf} },Keynote at CIKM 2014 conference Shanghai China November 2014. Talk also given at Tsinghua University.,http://research.google.com/people/jeff/CIKM-keynote-Nov2014.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Deep+Learning+Dean,http://research.google.com/pubs/pub43150.html
Digital Forensics with Open Source Tools,Syngress (2011),2011,Cory Altheide Harlan Carvey,@book{41604 title = {Digital Forensics with Open Source Tools} author = {Cory Altheide and Harlan Carvey} year = 2011 URL = {http://store.elsevier.com/Digital-Forensics-with-Open-Source-Tools/Cory-Altheide/isbn-9781597495868/} },Digital Forensics with Open Source Tools is the definitive book on investigating and analyzing computer systems and media using open source tools. The book is a technical procedural guide and explains the use of these tools on Linux and Windows systems as a platform for performing computer forensics. Both well known and novel forensic methods are demonstrated using command-line and graphical open source computer forensic tools for examining a wide range of target systems and artifacts.,http://store.elsevier.com/Digital-Forensics-with-Open-Source-Tools/Cory-Altheide/isbn-9781597495868/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Digital+Forensics+with+Open+Source+Tools+Altheide+Carvey,http://research.google.com/pubs/pub41604.html
Directed Enzymatic Activation of 1-D DNA Tiles,ACS Nano (2015),2015,Sudhanshu Garg Harish Chandran Nikhil Gopalkrishnan Thomas H. LaBean John Reif,@article{43288 title = {Directed Enzymatic Activation of 1-D DNA Tiles} author = {Sudhanshu Garg and Harish Chandran and Nikhil Gopalkrishnan and Thomas H. LaBean and John Reif} year = 2015 journal = {ACS Nano} },The tile assembly model is a Turing universal model of self-assembly where a set of square shaped tiles with programmable sticky sides undergo coordinated self-assembly to form arbitrary shapes thereby computing arbitrary functions. Activatable tiles are a theoretical extension to the Tile assembly model that enhances its robustness by protecting the sticky sides of tiles until a tile is partially incorporated into a growing assembly. In this article we experimentally demonstrate a simplified version of the Activatable tile assembly model. In particular we demonstrate the simultaneous assembly of protected DNA tiles where a set of inert tiles are activated via a DNA polymerase to undergo linear assembly. We then demonstrate stepwise activated assembly where a set of inert tiles are activated sequentially one after another as a result of attachment to a growing 1-D assembly. We hope that these results will pave the way for more sophisticated demonstrations of activated assemblies.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Directed+Enzymatic+Activation+of+1-D+DNA+Tiles+Garg+Chandran+Gopalkrishnan+LaBean+Reif,http://research.google.com/pubs/pub43288.html
Diagnosing performance changes by comparing request ﬂows,8th USENIX Symposium on Networked Systems Design and Implementation (NSDI) (2011),2011,Raja R. Sambasivan Alice X. Zheng Michael De Rosa Elie Krevat Spencer Whitman Michael Stroucken William Wang Lianghong Xu Gregory R. Ganger,@inproceedings{36986 title = {Diagnosing performance changes by comparing request ﬂows} author = {Raja R. Sambasivan and Alice X. Zheng and Michael De Rosa and Elie Krevat and Spencer Whitman and Michael Stroucken and William Wang and Lianghong Xu and Gregory R. Ganger} year = 2011 booktitle = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI)} },The causes of performance changes in a distributed system often elude even its developers. This paper develops a new technique for gaining insight into such changes: comparing system behaviours from two executions (e.g. of two system versions or time periods). Building on end-to-end request flow tracing within and across components algorithms are described for identifying and ranking changes in the flow and/or timing of request processing. The implementation of these algorithms in a tool called Spectroscope is described and evaluated. Six case studies are presented of using Spectroscope to diagnose performance changes in a distributed storage system caused by code changes configuration modifications and component degradations demonstrating the value and efficacy of comparing request flows. Preliminary experiences of using Spectroscope to diagnose performance changes within Google are also presented.,http://research.google.com/pubs/archive/36986.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Diagnosing+performance+changes+by+comparing+request+%EF%AC%82ows+Sambasivan+Zheng+De+Rosa+Krevat+Whitman+Stroucken+Wang+Xu+Ganger,http://research.google.com/pubs/pub36986.html
Sharing-aware algorithms for virtual machine colocation,Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures ACM New York NY USA (2011) pp. 367-378,2011,Michael Sindelar Ramesh Sitaraman Prashant Shenoy,@inproceedings{37147 title = {Sharing-aware algorithms for virtual machine colocation} author = {Michael Sindelar and Ramesh Sitaraman and Prashant Shenoy} year = 2011 booktitle = {Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures} pages = {367--378} address = {New York NY USA} },Virtualization technology enables multiple virtual machines (VMs) to run on a single physical server. VMs that run on the same physical server can share memory pages that have identical content thereby reducing the overall memory requirements on the server. We develop sharing-aware algorithms that can colocate VMs with similar page content on the same physical server to optimize the benefits of inter-VM sharing. We show that inter-VM sharing occurs in a largely hierarchical fashion where the sharing can be attributed to VM's running the same OS platform OS version software libraries or applications. We propose two hierarchical sharing models: a tree model and a more general cluster-tree model. Using a set of VM traces we show that up to 67% percent of the inter-VM sharing is captured by the tree model and up to 82% is captured by the cluster-tree model. Next we study two problem variants of critical interest to a virtualization service provider: the VM Maximization problem that determines the most profitable subset of the VMs that can be packed into the given set of servers and the VM packing problem that determines the smallest set of servers that can accommodate a set of VMs. While both variants are NP-hard we show that both admit provably good approximation schemes in the hierarchical sharing models. We show that VM maximization for the tree and cluster-tree models can be approximated in polytime to within a (1 - 1/e) factor of optimal. Further we show that VM packing can be approximated in polytime to within a factor of O(log n) of optimal for cluster-trees and to within a factor of 3 of optimal for trees where n is the number of VMs. Finally we evaluate our VM packing algorithm for the tree sharing model on real-world VM traces and show that our algorithm can exploit most of the available inter-VM sharing to achieve a 32% to 50% reduction in servers and a 25% to 57% reduction in memory footprint compared to sharing-oblivious algorithms.,http://research.google.com/pubs/archive/37147.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sharing-aware+algorithms+for+virtual+machine+colocation+Sindelar+Sitaraman+Shenoy,http://research.google.com/pubs/pub37147.html
Gender Differences in Factors Influencing Pursuit of Computer Science and Related Fields,Proceedings of the 20th ACM Conference on Innovation and Technology in Computer Science Education ACM (2015) (to appear),2015,Jennifer Wang Hai Hong Jason Ravitz Marielena Ivory,@inproceedings{43820 title = {Gender Differences in Factors Influencing Pursuit of Computer Science and Related Fields} author = {Jennifer Wang and Hai Hong and Jason Ravitz and Marielena Ivory} year = 2015 booktitle = {Proceedings of the 20th ACM Conference on Innovation and Technology in Computer Science Education} },Increasing women’s participation in computer science is a critical workforce and equity concern. The technology industry has committed to reversing negative trends for women in computer science as well as engineering and information technology “computing” fields. Building on previously published research this paper identifies factors that influence young women’s decisions to pursue computer science-related degrees and the ways in which these factors differ for young men. It is based on a survey of 1739 high school students and recent college graduates. Results identified encouragement and exposure as the leading factors influencing this critical choice for women while the influence of these factors is different for men. In particular the influence of family is found to play a critical role in encouragement and exposure and outreach efforts should focus on ways to engage parents.,http://research.google.com/pubs/archive/43820.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Gender+Differences+in+Factors+Influencing+Pursuit+of+Computer+Science+and+Related+Fields+Wang+Hong+Ravitz+Ivory,http://research.google.com/pubs/pub43820.html
Video CAPTCHAs: Usability vs. Security,Proceedings of the IEEE Western New York Image Processing Workshop (WNYIP '08) IEEE Press (2008),2008,Kurt Alfred Kluever Richard Zanibbi,@inproceedings{35117 title = {Video CAPTCHAs: Usability vs. Security} author = {Kurt Alfred Kluever and Richard Zanibbi} year = 2008 URL = {http://www.kloover.com/2008/09/13/video-captchas-usability-vs-security/} booktitle = {Proceedings of the IEEE Western New York Image Processing Workshop (WNYIP '08)} },A CAPTCHA is a variation of the Turing test in which a challenge is used to distinguish humans from computers (”bots”) on the internet. They are commonly used to prevent the abuse of online services. CAPTCHAs discriminate using hard artificial intelligence problems: the most common type requires a user to transcribe distorted characters displayed within a noisy image. Unfortunately many users find them frustrating and break rates as high as 60% have been reported (for Microsoft’s Hotmail). We present a new CAPTCHA in which users provide three words (”tags”) that describe a video. A challenge is passed if a user’s tag belongs to a set of automatically generated ground-truth tags. In an experiment we were able to increase human pass rates for our video CAPTCHAs from 69.7% to 90.2% (184 participants over 20 videos). Under the same conditions the pass rate for an attack submitting the three most frequent tags (estimated over 86368 videos) remained nearly constant (5% over the 20 videos roughly 12.9% over a separate sample of 5146 videos). Challenge videos were taken from YouTube.com. For each video 90 tags were added from related videos to the ground-truth set; security was maintained by pruning all tags with a frequency ≥ 0.6%. Tag stemming and approximate matching were also used to increase human pass rates. Only 20.1% of participants preferred text-based CAPTCHAs while 58.2% preferred our video-based alternative. Finally we demonstrate how our technique for extending the ground truth tags allows for different usability/security trade-offs and discuss how it can be applied to other types of CAPTCHAs.,http://research.google.com/pubs/archive/35117.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Video+CAPTCHAs:+Usability+vs.+Security+Kluever+Zanibbi,http://research.google.com/pubs/pub35117.html
Reading Digits in Natural Images with Unsupervised Feature Learning,NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011,2011,Yuval Netzer Tao Wang Adam Coates Alessandro Bissacco Bo Wu Andrew Y. Ng,@inproceedings{37648 title = {Reading Digits in Natural Images with Unsupervised Feature Learning} author = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng} year = 2011 URL = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf} booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011} },Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs however is far more difﬁcult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end we introduce a new benchmark dataset for research use containing over 600000 labeled digits cropped from Street View images. We then demonstrate the difﬁculty of recognizing these digits when the problem is approached with hand-designed features. Finally we employ variants of two recently proposed unsupervised feature learning methods and ﬁnd that they are convincingly superior on our benchmarks.,http://research.google.com/pubs/archive/37648.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reading+Digits+in+Natural+Images+with+Unsupervised+Feature+Learning+Netzer+Wang+Coates+Bissacco+Wu+Ng,http://research.google.com/pubs/pub37648.html
Rendering Fur in Life of Pi,ACM New York NY USA,2013,Ivan Neulander Toshi Kato Kevin Beason,@techreport{41375 title = {Rendering Fur in Life of Pi} author = {Ivan Neulander and Toshi Kato and Kevin Beason} year = 2013 URL = {http://neulander.org/work/sketch2013slides.pdf} note = {ACM SIGGRAPH 2013 Talks (this was a peer-reviewed technical talk not a paper)} },"We discuss the innovative fur rendering technology that Rhythm & Hues deployed in the photorealistic depiction of the tiger Richard Parker for Ang Lee's Academy-Award-winning feature film ""Life of Pi"".",http://research.google.com/pubs/archive/41375.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Rendering+Fur+in+Life+of+Pi+Neulander+Kato+Beason,http://research.google.com/pubs/pub41375.html
Why Don't Software Developers Use Static Analysis Tools to Find Bugs?,International Conference on Software Engineering (2013) pp. 672-681,2013,Brittany Johnson Yoonki Song Emerson Murphy-Hill Robert Bowdidge,@inproceedings{43477 title = {Why Don't Software Developers Use Static Analysis Tools to Find Bugs?} author = {Brittany Johnson and Yoonki Song and Emerson Murphy-Hill and Robert Bowdidge} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2486877} booktitle = {International Conference on Software Engineering} pages = {672-681} },Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs or software defects faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs research suggests that these tools are underused. In this paper we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial false positives and the way in which the warnings are presented among other things are barriers to use. We discuss several implications of these results such as the need for an interactive mechanism to help developers fix defects.,http://dl.acm.org/citation.cfm?id=2486877,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+Don't+Software+Developers+Use+Static+Analysis+Tools+to+Find+Bugs%3F+Johnson+Song+Murphy-Hill+Bowdidge,http://research.google.com/pubs/pub43477.html
Making programs forget: Enforcing Lifetime for Sensitive Data,Proceedings of the 13th USENIX conference on Hot topics in operating systems USENIX Association Berkeley CA USA (2013),2013,Jayanthkumar Kannan Gautam Altekar Petros Maniatis Byung-Gon Chun,@inproceedings{41460 title = {Making programs forget: Enforcing Lifetime for Sensitive Data} author = {Jayanthkumar Kannan and Gautam Altekar and Petros Maniatis and Byung-Gon Chun} year = 2013 booktitle = {Proceedings of the 13th USENIX conference on Hot topics in operating systems} address = {Berkeley CA USA} },"This paper introduces guaranteed data lifetime a novel system property ensuring that sensitive data cannot be retrieved from a system beyond a specified time. The trivial way to achieve this is to ""reboot""; however this is disruptive from the user's perspective and may not even eliminate disk copies. We discuss an alternate approach based on state re-incarnation where data expiry is completely transparent to the user and can be used even if the system is not designed a priori to provide the property.",http://research.google.com/pubs/archive/41460.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Making+programs+forget:+Enforcing+Lifetime+for+Sensitive+Data+Kannan+Altekar+Maniatis+Chun,http://research.google.com/pubs/pub41460.html
TRAM: Optimizing Fine-grained Communication with Topological Routing and Aggregation of Messages,International Conference on Parallel Processing (2014),2014,Lukasz Wesolowski Ramprasad Venkataraman A Gupta Jae-Seung Yeom Keith Bisset Yanhua Sun Pritish Jetley Thomas Quinn Laxmikant Kale,@inproceedings{43121 title = {TRAM: Optimizing Fine-grained Communication with Topological Routing and Aggregation of Messages} author = {Lukasz Wesolowski and Ramprasad Venkataraman and A Gupta and Jae-Seung Yeom and Keith Bisset and Yanhua Sun and Pritish Jetley and Thomas Quinn and Laxmikant Kale} year = 2014 URL = {http://ppl.cs.illinois.edu/papers/14-18} booktitle = {International Conference on Parallel Processing} },Fine-grained communication in supercomputing applications often limits performance through high communication overhead and poor utilization of network bandwidth. This paper presents Topological Routing and Aggregation Module (TRAM) a library that optimizes fine-grained communication performance by routing and dynamically combining short messages. TRAM collects units of fine-grained communication from the application and combines them into aggregated messages with a common intermediate destination. It routes these messages along a virtual mesh topology mapped onto the physical topology of the network. TRAM improves network bandwidth utilization and reduces communication overhead. It is particularly effective in optimizing patterns with global communication and large message counts such as all to-all and many-to-many as well as sparse irregular dynamic or data dependent patterns. We demonstrate how TRAM improves performance through theoretical analysis and experimental verification using benchmarks and scientific applications. We present speedups on petascale systems of 6x for communication benchmarks and up to 4x for applications.,http://ppl.cs.illinois.edu/papers/14-18,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=TRAM:+Optimizing+Fine-grained+Communication+with+Topological+Routing+and+Aggregation+of+Messages+Wesolowski+Venkataraman+Gupta+Yeom+Bisset+Sun+Jetley+Quinn+Kale,http://research.google.com/pubs/pub43121.html
Instant Google Drive Starter,Packt Publishing Packt Publishing Limited 2nd Floor Livery Place 35 Livery Street Birmingham B3 2PB (2013),2013,Michael J. Procopio,@book{42181 title = {Instant Google Drive Starter} author = {Michael J. Procopio} year = 2013 URL = {http://www.packtpub.com/google-drive-starter/book} booktitle = {Instant Google Drive Starter} address = {Packt Publishing Limited 2nd Floor Livery Place 35 Livery Street Birmingham B3 2PB} },This book is a Starter which teaches you how to use Google Drive practically. This book is perfect for people of all skill levels who want to enjoy the benefits of using Google Drive to safely store their files online and in the cloud. It's also great for anyone looking to learn more about cloud computing in general. Readers are expected to have an Internet connection and basic knowledge of using the internet.,http://www.packtpub.com/google-drive-starter/book,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Instant+Google+Drive+Starter+Procopio,http://research.google.com/pubs/pub42181.html
Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks,Interspeech (2014),2014,Hasim Sak Oriol Vinyals Georg Heigold Andrew Senior Erik McDermott Rajat Monga Mark Mao,@inproceedings{42547 title = {Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks} author = {Hasim Sak and Oriol Vinyals and Georg Heigold and Andrew Senior and Erik McDermott and Rajat Monga and Mark Mao} year = 2014 booktitle = {Interspeech} },We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic modeling where the models were trained with the cross-entropy (CE) criterion. It has also been shown that sequence discriminative training of DNNs initially trained with the CE criterion gives significant improvements. In this paper we investigate sequence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent optimization technique. We compare two sequence discriminative criteria -- maximum mutual information and state-level minimum Bayes risk and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model and the objective function. We obtain significant gains over the CE trained LSTM RNN model using sequence discriminative training techniques.,http://research.google.com/pubs/archive/42547.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sequence+Discriminative+Distributed+Training+of+Long+Short-Term+Memory+Recurrent+Neural+Networks+Sak+Vinyals+Heigold+Senior+McDermott+Monga+Mao,http://research.google.com/pubs/pub42547.html
Paxos Made Live - An Engineering Perspective (2006 Invited Talk),Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing ACM press (2007),2007,Tushar Deepak Chandra Robert Griesemer Joshua Redstone,@inproceedings{33002 title = {Paxos Made Live - An Engineering Perspective (2006 Invited Talk)} author = {Tushar Deepak Chandra and Robert Griesemer and Joshua Redstone} year = 2007 URL = {http://dx.doi.org/10.1145/1281100.1281103} booktitle = {Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing} },We describe our experience in building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered and the solutions we found for them. Our measurements indicate that we have built a competitive system.,http://dx.doi.org/10.1145/1281100.1281103,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Paxos+Made+Live+-+An+Engineering+Perspective+(2006+Invited+Talk)+Chandra+Griesemer+Redstone,http://research.google.com/pubs/pub33002.html
Internet and mobile ratings panels,Online Panel Research: A Data Quality Perspective Wiley (2014) pp. 387-407,2014,Philip M. Napoli Paul J. Lavrakas Mario Callegaro,@inbook{42495 title = {Internet and mobile ratings panels} author = {Philip M. Napoli and Paul J. Lavrakas and Mario Callegaro} year = 2014 URL = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html} booktitle = {Online Panel Research: A Data Quality Perspective} pages = {387-407} },This chapter examines how Internet (PC and mobile) ratings panels are constructed managed and utilized. We provide an overview of the history and evolution of Internet/mobile ratings panels and examines the methodological challenges associated with creating and maintaining accurate and reliable Internet/mobile ratings panels. The research that has assessed the accuracy and validity of online panel data is critically discussed; as well as research that illustrates the type of scholarly and applied research questions that can be investigated using online ratings panel data. The chapter concludes with a discussion of the future of online ratings panels within the rapidly evolving field of Internet audience measurement.,http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Internet+and+mobile+ratings+panels+Napoli+Lavrakas+Callegaro,http://research.google.com/pubs/pub42495.html
High-Availability at Massive Scale: Building Google’s Data Infrastructure for Ads,Workshop on Business Intelligence for the Real Time Enterprise (BIRTE) Springer (2015) (to appear),2015,Ashish Gupta Jeff Shute,@inproceedings{44686 title = {High-Availability at Massive Scale: Building Google’s Data Infrastructure for Ads} author = {Ashish Gupta and Jeff Shute} year = 2015 booktitle = {Workshop on Business Intelligence for the Real Time Enterprise (BIRTE)} },Google’s Ads Data Infrastructure systems run the multi- billion dollar ads business at Google. High availability and strong consistency are critical for these systems. While most distributed systems handle machine-level failures well handling datacenter-level failures is less common. In our experience handling datacenter-level failures is critical for running true high availability systems. Most of our systems (e.g. Photon F1 Mesa) now support multi-homing as a fundamental design property. Multi-homed systems run live in multiple datacenters all the time adaptively moving load between datacenters with the ability to handle outages of any scale completely transparently. This paper focuses primarily on stream processing systems and describes our general approaches for building high availability multi-homed systems discusses common challenges and solutions and shares what we have learned in building and running these large-scale systems for over ten years.,http://research.google.com/pubs/archive/44686.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=High-Availability+at+Massive+Scale:+Building+Google%E2%80%99s+Data+Infrastructure+for+Ads+Gupta+Shute,http://research.google.com/pubs/pub44686.html
Kernel Based Text-Independnent Speaker Verification,Automatic Speech and Speaker Recognition: Large Margin and Kernel Methods Wiley (2009),2009,Johnny Mariethoz Yves Grandvalet Samy Bengio,@inbook{34560 title = {Kernel Based Text-Independnent Speaker Verification} author = {Johnny Mariethoz and Yves Grandvalet and Samy Bengio} year = 2009 URL = {http://www.wiley.com/remtitle.cgi?isbn=9780470696835} booktitle = {Automatic Speech and Speaker Recognition: Large Margin and Kernel Methods} },The goal of a person authentication system is to authenticate the claimed identity of a user. When this authentication is based on the voice of the user without respect of what the user exactly said the system is called a text-independent speaker verification system. Speaker verification systems are increasingly often used to secure personal information particularly for mobile phone based applications. Furthermore text-independent versions of speaker verification systems are the most used for their simplicity as they do not require complex speech recognition modules. The most common approach to this task is based on Gaussian Mixture Models (GMMs) which do not take into account any temporal information. GMMs have been intensively used thanks to their good performance especially with the use of the Maximum A Posteriori (MAP) adaptation algorithm. This approach is based on the density estimation of an impostor data distribution followed by its adaptation to a specific client data set. Note that the estimation of these densities is not the final goal of speaker verification systems which is rather to discriminate the client and impostor classes; hence discriminative approaches might appear good candidates for this task as well. As a matter of fact Support Vector Machine (SVM) based systems have been the subject of several recent publications in the speaker verification community in which they obtain similar to or even better performance than GMMs on several text-independent speaker verification tasks. In order to use SVMs or any other discriminant approaches for speaker verification several modifications from the classical techniques need to be performed. The purpose of this chapter is to present an overview of discriminant approaches that have been used successfully for the task of text-independent speaker verification to analyze their difference and their similarities with each other and with classical generative approaches based on GMMs. An open-source version of the C++ source code used to performed all experiments described in this chapter can be found at http://speaker.abracadoudou.com.,http://research.google.com/pubs/archive/34560.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Kernel+Based+Text-Independnent+Speaker+Verification+Mariethoz+Grandvalet+Bengio,http://research.google.com/pubs/pub34560.html
Personalized News Recommendation Based on Click Behavior,2010 International Conference on Intelligent User Interfaces,2010,Jiahui Liu Elin Pedersen Peter Dolan,@inproceedings{35599 title = {Personalized News Recommendation Based on Click Behavior} author = {Jiahui Liu and Elin Pedersen and Peter Dolan} year = 2010 booktitle = {2010 International Conference on Intelligent User Interfaces} },Online news reading has become very popular as the web provides access to news articles from millions of sources around the world. A key challenge of news service website is help users to find news articles that are interesting to read. In this paper we present our research on developing personalized news recommendation system in Google News. The recommendation system builds profiles of user’s news interests based on user’s click behavior on the website. To understand the news interest change over time we first conducted a large-scale log analysis of the click behavior of Google News users. Based on the log analysis we developed a Bayesian framework for predict user’s current news interests which considers both the activities of that particular user and the news trend demonstrated in activities of a group of users. We combine the information filtering mechanism using learned user profile with an existing collaborative filtering mechanism to generate personalized news recommendation. The combined method was deployed in Google News. Experiments on the live traffic of Google News website demonstrated that the combined method improves the quality of news recommendation and attracts more frequent visit to the website.,http://research.google.com/pubs/archive/35599.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Personalized+News+Recommendation+Based+on+Click+Behavior+Liu+Pedersen+Dolan,http://research.google.com/pubs/pub35599.html
Using a Cascade of Asymmetric Resonators with Fast-Acting Compression as a Cochlear Model for Machine-Hearing Applications,Autumn Meeting of the Acoustical Society of Japan (2011) pp. 509-512,2011,Richard F. Lyon,@inproceedings{37215 title = {Using a Cascade of Asymmetric Resonators with Fast-Acting Compression as a Cochlear Model for Machine-Hearing Applications} author = {Richard F. Lyon} year = 2011 booktitle = {Autumn Meeting of the Acoustical Society of Japan} pages = {509--512} },"Every day machines process many thousands of hours of audio signals through a realistic cochlear model. They extract features inform classifiers and recommenders and identify copyrighted material. The machine-hearing approach to such tasks has taken root in recent years because hearing-based approaches perform better than we can do with more conventional sound-analysis approaches. We use a bio-mimetic ""cascade of asymmetric resonators with fast-acting compression"" (CAR-FAC)—an efficient sound analyzer that incorporates the hearing research community's findings on nonlinear auditory filter models and cochlear wave mechanics. The CAR-FAC is based on a pole–zero filter cascade (PZFC) model of auditory filtering in combination with a multi-time-scale coupled automatic-gain-control (AGC) network. It uses simple nonlinear extensions of conventional digital filter stages and runs fast due to its low complexity. The PZFC plus AGC network the CAR-FAC mimics features of auditory physiology such as masking compressive traveling-wave response and the stability of zero-crossing times with signal level. Its output ""neural activity pattern"" is converted to a ""stabilized auditory image"" to capture pitch melody and other temporal and spectral features of the sound.",http://research.google.com/pubs/archive/37215.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+a+Cascade+of+Asymmetric+Resonators+with+Fast-Acting+Compression+as+a+Cochlear+Model+for+Machine-Hearing+Applications+Lyon,http://research.google.com/pubs/pub37215.html
On Rectified Linear Units For Speech Processing,38th International Conference on Acoustics Speech and Signal Processing (ICASSP) Vancouver (2013),2013,M.D. Zeiler M. Ranzato R. Monga M. Mao K. Yang Q.V. Le P. Nguyen A. Senior V. Vanhoucke J. Dean G.E. Hinton,@inproceedings{40811 title = {On Rectified Linear Units For Speech Processing} author = {M.D. Zeiler and M. Ranzato and R. Monga and M. Mao and K. Yang and Q.V. Le and P. Nguyen and A. Senior and V. Vanhoucke and J. Dean and G.E. Hinton} year = 2013 booktitle = {38th International Conference on Acoustics Speech and Signal Processing (ICASSP)} address = {Vancouver} },Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity which is typically a logistic function. In this work we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.,http://research.google.com/pubs/archive/40811.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+Rectified+Linear+Units+For+Speech+Processing+Zeiler+Ranzato+Monga+Mao+Yang+Le+Nguyen+Senior+Vanhoucke+Dean+Hinton,http://research.google.com/pubs/pub40811.html
Generating Precise Dependencies for Large Software,Proceedings of the Forth International Workshop on Managing Technical Debt IEEE (2013) pp. 47-50,2013,Pei Wang Jinqiu Yang Lin Tan Robert Kroeger J. David Morgenthaler,@inproceedings{40739 title = {Generating Precise Dependencies for Large Software} author = {Pei Wang and Jinqiu Yang and Lin Tan and Robert Kroeger and J. David Morgenthaler} year = 2013 URL = {https://ece.uwaterloo.ca/~lintan/publications/dep-mtd13-preprint.pdf} booktitle = {Proceedings of the Forth International Workshop on Managing Technical Debt} pages = {47--50} },Intra- and inter-module dependencies can be a significant source of technical debt in the long-term software development especially for large software with millions of lines of code. This paper designs and implements a precise and scalable tool that extracts code dependencies and their utilization for large C/C++ software projects. The tool extracts both symbol-level and module-level dependencies of a software system and identifies potential underutilized and inconsistent dependencies. Such information points to potential refactoring opportunities and help developers perform large-scale refactoring tasks.,http://research.google.com/pubs/archive/40739.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Generating+Precise+Dependencies+for+Large+Software+Wang+Yang+Tan+Kroeger+Morgenthaler,http://research.google.com/pubs/pub40739.html
Topical clustering of search results,Proceedings of the fifth ACM international conference on Web search and data mining ACM New York NY USA (2012) pp. 223-232,2012,Ugo Scaiella Paolo Ferragina Andrea Marino Massimiliano Ciaramita,@inproceedings{37745 title = {Topical clustering of search results} author = {Ugo Scaiella and Paolo Ferragina and Andrea Marino and Massimiliano Ciaramita} year = 2012 booktitle = {Proceedings of the fifth ACM international conference on Web search and data mining} pages = {223--232} address = {New York NY USA} },Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9 11 16 20] applied to the search results and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%.,http://research.google.com/pubs/archive/37745.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Topical+clustering+of+search+results+Scaiella+Ferragina+Marino+Ciaramita,http://research.google.com/pubs/pub37745.html
ViSQOLAudio: An objective audio quality metric for low bitrate codecs,The Journal of the Acoustical Society of America vol. 137 (6) (2015) EL449-EL455,2015,Andrew Hines Eoin Gillen Damien Kelly Jan Skoglund Anil Kokaram Naomi Harte,@article{43991 title = {ViSQOLAudio: An objective audio quality metric for low bitrate codecs} author = {Andrew Hines and Eoin Gillen and Damien Kelly and Jan Skoglund and Anil Kokaram and Naomi Harte} year = 2015 journal = {The Journal of the Acoustical Society of America} pages = {EL449-EL455} volume = {137 (6)} },Streaming services seek to optimise their use of bandwidth across audio and visual channels to maximise the quality of experience for users. This letter evaluates whether objective quality metrics can predict the audio quality for music encoded at low bitrates by comparing objective predictions with results from listener tests. Three objective metrics were benchmarked: PEAQ POLQA and VISQOLAudio. The results demonstrate objective metrics designed for speech quality assessment have a strong potential for quality assessment of low bitrate audio codecs.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ViSQOLAudio:+An+objective+audio+quality+metric+for+low+bitrate+codecs+Hines+Gillen+Kelly+Skoglund+Kokaram+Harte,http://research.google.com/pubs/pub43991.html
(Smart) watch your taps: side-channel keystroke inference attacks using smartwatches,ACM International Symposium on Wearable Computers (2015) pp. 27-30,2015,Anindya Maiti Murtuza Jadliwala Jibo He Igor Bilogrevic,@inproceedings{44642 title = {(Smart) watch your taps: side-channel keystroke inference attacks using smartwatches} author = {Anindya Maiti and Murtuza Jadliwala and Jibo He and Igor Bilogrevic} year = 2015 URL = {http://dl.acm.org/citation.cfm?id=2808397} booktitle = {ACM International Symposium on Wearable Computers} pages = {27--30} },In this paper we investigate the feasibility of keystroke inference attacks on handheld numeric touchpads by using smartwatch motion sensors as a side-channel. The proposed attack approach employs supervised learning techniques to accurately map the uniqueness in the captured wrist movements to each individual keystroke. Experimental evaluation shows that keystroke inference using smartwatch motion sensors is not only fairly accurate but also better than similar attacks previously demonstrated using smartphone motion sensors.,http://dl.acm.org/citation.cfm?id=2808397,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=(Smart)+watch+your+taps:+side-channel+keystroke+inference+attacks+using+smartwatches+Maiti+Jadliwala+He+Bilogrevic,http://research.google.com/pubs/pub44642.html
Affiliation Networks,Proceedings of the 41st Annual ACM Symposium on Theory of Computing ACM (2009) pp. 427-434,2009,Silvio Lattanzi D. Sivakumar,@inproceedings{35369 title = {Affiliation Networks} author = {Silvio Lattanzi and D. Sivakumar} year = 2009 URL = {http://portal.acm.org/citation.cfm?id=1536414.1536474} booktitle = {Proceedings of the 41st Annual ACM Symposium on Theory of Computing} pages = {427--434} },In the last decade structural properties of several naturally arising networks (the Internet social networks the web graph etc.) have been studied intensively with a view to understanding their evolution. In recent empirical work Leskovec Kleinberg and Faloutsos identify two new and surprising properties of the evolution of many real-world networks: densification (the ratio of edges to vertices grows over time) and shrinking diameter (the diameter reduces over time to a constant). These properties run counter to conventional wisdom and are certainly inconsistent with graph models prior to their work. In this paper we present the first model that provides a simple realistic and mathematically tractable generative model that intrinsically explains all the well-known properties of the social networks as well as densification and shrinking diameter. Our model is based on ideas studied empirically in the social sciences primarily on the groundbreaking work of Breiger (1973) on bipartite models of social networks that capture the affiliation of agents to societies. We also present algorithms that harness the structural consequences of our model. Specifically we show how to overcome the bottleneck of densification in computing shortest paths between vertices by producing sparse subgraphs that preserve or approximate shortest distances to all or a distinguished subset of vertices. This is a rare example of an algorithmic benefit derived from a realistic graph model. Finally our work also presents a modular approach to connecting random graph paradigms (preferential attachment edge-copying etc.) to structural consequences (heavy-tailed degree distributions shrinking diameter etc.),http://portal.acm.org/citation.cfm?id=1536414.1536474,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Affiliation+Networks+Lattanzi+Sivakumar,http://research.google.com/pubs/pub35369.html
Engineering Reliability into Web Sites: Google SRE,Proceedings of LinuxWorld (2007),2007,Alexander R. Perry,@inproceedings{32583 title = {Engineering Reliability into Web Sites: Google SRE} author = {Alexander R. Perry} year = 2007 URL = {http://research.google.com/archive/LinuxWorld-07-describeSRE.pdf} booktitle = {Proceedings of LinuxWorld} },This talk introduces Site Reliability Engineering (SRE) at Google explaining its purpose and describing the challenges it addresses. SRE teams in Mountain View Zürich New York Santa Monica Dublin and Kirkland manage Google's many services and websites. They draw upon the Linux based computing resources that are distributed in data centers around the world.,http://research.google.com/pubs/archive/32583.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Engineering+Reliability+into+Web+Sites:+Google+SRE+Perry,http://research.google.com/pubs/pub32583.html
Semantic Vector Products: Some Initial Investigations,Proceedings of the Second AAAI Symposium on Quantum Interaction AAAI (2008),2008,Dominic Widdows,@inproceedings{33477 title = {Semantic Vector Products: Some Initial Investigations} author = {Dominic Widdows} year = 2008 booktitle = {Proceedings of the Second AAAI Symposium on Quantum Interaction} },Semantic vector models have proven their worth in a number of natural language applications whose goals can be accomplished by modelling individual semantic concepts and measuring similarities between them. By comparison the area of semantic compositionality in these models has so far remained underdeveloped. This will be a crucial hurdle for semantic vector models: in order to play a fuller part in the modelling of human language these models will need some way of modelling the way in which single concepts are put together to form more complex conceptual structures. This paper explores some of the opportunities for using vector product operations to model compositional phenomena in natural language. These vector operations are all well-known and used in mathematics and physics particularly in quantum mechanics. Instead of designing new vector composition operators this paper gathers a list of existing operators and a list of typical composition operations in natural language and describes two small experiments that begin to investigate the use of certain vector operators to model certain language phenomena. Though preliminary our results are encouraging. It is our hope that these results and the gathering of other untested semantic and vector compositional challenges into a single paper will stimulate further research in this area.,http://research.google.com/pubs/archive/33477.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Vector+Products:+Some+Initial+Investigations+Widdows,http://research.google.com/pubs/pub33477.html
PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce,Proceedings of the 35th International Conference on Very Large Data Bases (VLDB-2009),2009,Biswanath Panda Joshua S. Herbach Sugato Basu Roberto J. Bayardo,@inproceedings{36296 title = {PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce} author = {Biswanath Panda and Joshua S. Herbach and Sugato Basu and Roberto J. Bayardo} year = 2009 URL = {http://www.bayardo.org/ps/vldb2009.pdf} booktitle = {Proceedings of the 35th International Conference on Very Large Data Bases (VLDB-2009)} },Classification and regression tree learning on massive datasets is a common data mining task at Google yet many state of the art tree learning algorithms require training data to reside in memory on a single machine. While more scalable implementations of tree learning have been proposed they typically require specialized parallel computing architectures. In contrast the majority of Google’s computing infrastructure is based on commodity hardware. In this paper we describe PLANET: a scalable distributed framework for learning tree models over large datasets. PLANET defines tree learning as a series of distributed computations and implements each one using the MapReduce model of distributed computation. We show how this framework supports scalable construction of classification and regression trees as well as ensembles of such models. We discuss the benefits and challenges of using a MapReduce compute cluster for tree learning and demonstrate the scalability of this approach by applying it to a real world learning task from the domain of computational advertising.,http://research.google.com/pubs/archive/36296.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=PLANET:+Massively+Parallel+Learning+of+Tree+Ensembles+with+MapReduce+Panda+Herbach+Basu+Bayardo,http://research.google.com/pubs/pub36296.html
Repeated Contextual Auctions with Strategic Buyers,Advances in Neural Information Processing Systems (2014),2014,Kareem Amin Afshin Rostamizadeh Umar Syed,@inproceedings{43229 title = {Repeated Contextual Auctions with Strategic Buyers} author = {Kareem Amin and Afshin Rostamizadeh and Umar Syed} year = 2014 booktitle = {Advances in Neural Information Processing Systems} },Motivated by real-time advertising exchanges we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer where the former makes decisions myopically on every round and the latter may strategically react to our algorithm forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer’s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{ 2/3})) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.,http://research.google.com/pubs/archive/43229.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Repeated+Contextual+Auctions+with+Strategic+Buyers+Amin+Rostamizadeh+Syed,http://research.google.com/pubs/pub43229.html
A Feature-Rich Constituent Context Model for Grammar Induction,Proceedings of the Association for Computational Linguistics (2012),2012,Dave Golland John DeNero Jakob Uszkoreit,@inproceedings{38273 title = {A Feature-Rich Constituent Context Model for Grammar Induction} author = {Dave Golland and John DeNero and Jakob Uszkoreit} year = 2012 booktitle = {Proceedings of the Association for Computational Linguistics} },We present LLCCM a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40 LLCCM outperforms CCM by 13.9% brack- eting F1 and outperforms a right-branching baseline in regimes where CCM does not.,http://research.google.com/pubs/archive/38273.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Feature-Rich+Constituent+Context+Model+for+Grammar+Induction+Golland+DeNero+Uszkoreit,http://research.google.com/pubs/pub38273.html
Efficient Estimation of Word Representations in Vector Space,International Conference on Learning Representations (2013),2013,Tomas Mikolov Kai Chen Greg S. Corrado Jeffrey Dean,@misc{41224 title = {Efficient Estimation of Word Representations in Vector Space} author = {Tomas Mikolov and Kai Chen and Greg S. Corrado and Jeffrey Dean} year = 2013 URL = {http://arxiv.org/abs/1301.3781} },We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.,http://arxiv.org/abs/1301.3781,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Estimation+of+Word+Representations+in+Vector+Space+Mikolov+Chen+Corrado+Dean,http://research.google.com/pubs/pub41224.html
Frame by Frame Language Identification in Short Utterances using Deep Neural Networks,Neural Networks Special Issue: Neural Network Learning in Big Data (2014),2014,Javier Gonzalez-Dominguez Ignacio Lopez-Moreno Pedro J. Moreno Joaquin Gonzalez-Rodriguez,@article{42929 title = {Frame by Frame Language Identification in Short Utterances using Deep Neural Networks} author = {Javier Gonzalez-Dominguez and Ignacio Lopez-Moreno and Pedro J. Moreno and Joaquin Gonzalez-Rodriguez} year = 2014 journal = {Neural Networks Special Issue: Neural Network Learning in Big Data} },This work addresses the use of deep neural networks (DNNs) in automatic language identification (LID) focused on short test utterances. Motivated by their recent success in acoustic modelling for speech recognition we adapt DNNs to the problem of identifying the language in a given utterance from the short-term acoustic features. We show how DNNs are particularly suitable to perform LID in real-time applications due to their capacity to emit a language identification posterior at each new frame of the test utterance. We then analyse different aspects of the system such as the amount of required training data the number of hidden layers the relevance of contextual information and the effect of the test utterance duration. Finally we propose several methods to combine frame-by-frame posteriors. Experiments are conducted on two different datasets: the public NIST Language Recognition Evaluation 2009 (3 seconds task) and a much larger corpus (of 5 million utterances) known as Google 5M LID obtained from different Google Services. Reported results show relative improvements of DNNs versus the i-vector system of 40% in LRE09 3 second task and 76% in Google 5M LID.,http://research.google.com/pubs/archive/42929.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Frame+by+Frame+Language+Identification+in+Short+Utterances+using+Deep+Neural+Networks+Gonzalez-Dominguez+Lopez+Moreno+Moreno+Gonzalez-Rodriguez,http://research.google.com/pubs/pub42929.html
Scala In Depth,Manning Publications Co. Sound View Ct. #3B Greenwich CT 06830 (2011) pp. 250,2011,Josh Suereth,"@book{36605 title = {Scala In Depth} author = {Josh Suereth} year = 2011 URL = {http://www.manning.com/suereth/} note = {This is not complete only available in ""Early Access"" edition} booktitle = {Scala In Depth} pages = {250} address = {Sound View Ct. #3B Greenwich CT 06830} }","Scala is a unique and powerful new programming language for the JVM. Blending the strengths of the Functional and Imperative programming models Scala is a great tool for building highly concurrent applications without sacrificing the benefits of an OO approach. While information about the Scala language is abundant skilled practitioners great examples and insight into the best practices of the community are harder to find. Scala in Depth bridges that gap preparing you to adopt Scala successfully for real world projects. Scala in Depth is a unique new book designed to help you integrate Scala effectively into your development process. By presenting the emerging best practices and designs from the Scala community it guides you though dozens of powerful techniques example by example. There's no heavy-handed theory here-just lots of crisp practical guides for coding in Scala. For example: * Discover the ""sweet spots"" where object-oriented and functional programming intersect. * Master advanced OO features of Scala including type member inheritance multiple inheritance and composition. * Employ functional programming concepts like tail recursion immutability and monadic operations. * Learn good Scala style to keep your code concise expressive and readable. As you dig into the book you'll start to appreciate what makes Scala really shine. For instance the Scala type system is very very powerful; this book provides use case approaches to manipulating the type system and covers how to use type constraints to enforce design constraints. Java developers love Scala's deep integration with Java and the JVM Ecosystem and this book shows you how to leverage it effectively and work around the rough spots.",http://research.google.com/pubs/archive/36605.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scala+In+Depth+Suereth,http://research.google.com/pubs/pub36605.html
Large Scale Visual Semantic Extraction,Frontiers of Engineering - Reports on Leading-Edge Engineering from the 2011 Symposium The National Academies Press Washington D.C. (2012) pp. 61-68,2012,Samy Bengio,@inproceedings{37716 title = {Large Scale Visual Semantic Extraction} author = {Samy Bengio} year = 2012 booktitle = {Frontiers of Engineering - Reports on Leading-Edge Engineering from the 2011 Symposium} pages = {61--68} address = {Washington D.C.} },"Image annotation is the task of providing textual semantic to new images by ranking a large set of possible annotations according to how they correspond to a given image. In the large scale setting there could be millions of images to process and hundreds of thousands of potential distinct annotations. In order to achieve such a task we propose to build a so-called ""embedding space"" into which both images and annotations can be automatically projected. In such a space one can then find the nearest annotations to a given image or annotations similar to a given annotation. One can even build a visio-semantic tree from these annotations that corresponds to how concepts (annotations) are similar to each other with respect to their visual characteristics. Such a tree will be different from semantic-only trees such as WordNet which do not take into account the visual appearance of concepts.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Visual+Semantic+Extraction+Bengio,http://research.google.com/pubs/pub37716.html
Energy Optimization in Wireless Medical Systems Using Physiological Behavior,In Proceedings of the ACM BMES conference of Wireless Health ACM (2010) pp. 128-136,2010,Hyduke Noshadi Foad Dabiri Saro Meguerdichian Miodrag Potkonjak Majid Sarrafzadeh,@inproceedings{41366 title = {Energy Optimization in Wireless Medical Systems Using Physiological Behavior} author = {Hyduke Noshadi and Foad Dabiri and Saro Meguerdichian and Miodrag Potkonjak and Majid Sarrafzadeh} year = 2010 URL = {http://dl.acm.org/citation.cfm?id=1921097} booktitle = {In Proceedings of the ACM BMES conference of Wireless Health} pages = {128-136} },Wearable sensing systems are becoming widely used for a variety of applications including sports entertainment and military. These systems have recently enabled a variety of medical monitoring and diagnostic applications in Wireless Health. The need for multiple sensors and constant monitoring lead these systems to be power hungry and expensive with short operating lifetimes. In this paper we introduce a novel methodology that takes advantage of the influence of human behavior on signal properties and reduces those three metrics from the data size point of view. This in turn directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system behavior. Our goal is to select a small subset of sensors to accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling partitioning and behavioral optimization which consists of signal characterization segmentation and time shifting mutual signal prediction and subset sensor selection. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe which cover the bottom of the entire foot resulting in energy reduction of 56% to 96% for error rates of 5% to 17.5%.,http://dl.acm.org/citation.cfm?id=1921097,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Energy+Optimization+in+Wireless+Medical+Systems+Using+Physiological+Behavior+Noshadi+Dabiri+Meguerdichian+Potkonjak+Sarrafzadeh,http://research.google.com/pubs/pub41366.html
Prediction of Advertiser Churn for Google AdWords,JSM Proceedings American Statistical Association (2010) (to appear),2010,Sangho Yoon Jim Koehler Adam Ghobarah,@inproceedings{36678 title = {Prediction of Advertiser Churn for Google AdWords} author = {Sangho Yoon and Jim Koehler and Adam Ghobarah} year = 2010 booktitle = {JSM Proceedings} },Google AdWords has thousands of advertisers participating in auctions to show their advertisements. Google's business model has two goals: firrst provide relevant information to users and second provide advertising opportunities to advertisers to achieve their business needs. To better serve these two parties it is important to find relevant information for users and at the same time assist advertisers in advertising more efficiently and effectively. In this paper we try to tackle this problem of better connecting users and advertisers from a customer relationship management point of view. More specifically we try to retain more advertisers in AdWords by identifying and helping advertisers that are not successful in using Google AdWords. In this work we first propose a new definition of advertiser churn for AdWords advertisers; second we present a method to carefully select a homogeneous group of advertisers to use in understanding and predicting advertiser churn; and third we build a model to predict advertiser churn using machine learning algorithms.,http://research.google.com/pubs/archive/36678.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Prediction+of+Advertiser+Churn+for+Google+AdWords+Yoon+Koehler+Ghobarah,http://research.google.com/pubs/pub36678.html
When Does Improved Targeting Increase Revenue?,Proceedings of the 24th International Conference on the World Wide Web (WWW) (2015) pp. 462-472,2015,Patrick Hummel Preston McAfee,@inproceedings{43788 title = {When Does Improved Targeting Increase Revenue?} author = {Patrick Hummel and Preston McAfee} year = 2015 URL = {http://www.www2015.it/documents/proceedings/proceedings/p462.pdf} booktitle = {Proceedings of the 24th International Conference on the World Wide Web (WWW)} pages = {462-472} },In second-price auctions with symmetric bidders we find that improved targeting via enhanced information disclosure decreases revenue when there are two bidders and increases revenue if there are at least four bidders. With asymmetries improved targeting increases revenue if the most frequent winner wins less than 30.4% of the time but can decrease revenue otherwise. We derive analogous results for position auctions. Finally we show that revenue can vary non-monotonically with the number of bidders who are able to take advantage of improved targeting.,http://www.www2015.it/documents/proceedings/proceedings/p462.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=When+Does+Improved+Targeting+Increase+Revenue%3F+Hummel+McAfee,http://research.google.com/pubs/pub43788.html
Theoretical Advantages of Lenient Learners in Multiagent Systems,Proceedings of the Sixth International Conference on Autonomous Agents and Multi-agent Systems (AAMAS-07) ACM (2007),2007,Liviu Panait Karl Tuyls,@inproceedings{32579 title = {Theoretical Advantages of Lenient Learners in Multiagent Systems} author = {Liviu Panait and Karl Tuyls} year = 2007 booktitle = {Proceedings of the Sixth International Conference on Autonomous Agents and Multi-agent Systems (AAMAS-07)} },This paper presents the dynamics of multiple reinforcement learning agents from an Evolutionary Game Theoretic perspective. We provide a Replicator Dynamics model for traditional multiagent Q-learning and we then extend these differential equations to account for lenient learners: agents that forgive possible mistakes of their teammates that resulted in lower rewards. We use this extended formal model to visualize the basins of attraction of both traditional and lenient multiagent Q-learners in two benchmark coordination problems. The results indicate that lenience provides learners with more accurate estimates for the utility of their actions resulting in higher likelihood of convergence to the globally optimal solution. In addition our research supports the strength of EGT as a backbone for multiagent reinforcement learning.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Theoretical+Advantages+of+Lenient+Learners+in+Multiagent+Systems+Panait+Tuyls,http://research.google.com/pubs/pub32579.html
Gestalt: Fast Uniﬁed Fault Localization for Networked Systems,Proceedings of the USENIX Annual Technical Conference (2014),2014,Radhika Niranjan Mysore Amin Vahdat Ratul Mahajan George Varghese,@inproceedings{43857 title = {Gestalt: Fast Uniﬁed Fault Localization for Networked Systems} author = {Radhika Niranjan Mysore and Amin Vahdat and Ratul Mahajan and George Varghese} year = 2014 booktitle = {Proceedings of the USENIX Annual Technical Conference} },We show that the performance of existing fault localization algorithms differs markedly for different networks; and no algorithm simultaneously provides high localization accuracy and low computational overhead. We develop a framework to explain these behaviors by anatomizing the algorithms with respect to six important characteristics of real networks such as uncertain dependencies noise and covering relationships. We use this analysis to develop Gestalt a new algorithm that combines the best elements of existing ones and includes a new technique to explore the space of fault hypotheses. We run experiments on three real diverse networks. For each Gestalt has either significantly higher localization accuracy or an order of magnitude lower running time. For example when applied to the Lync messaging system that is used widely within corporations Gestalt localizes faults with the same accuracy as Sherlock while reducing fault localization time from days to 23 seconds,http://research.google.com/pubs/archive/43857.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Gestalt:+Fast+Uni%EF%AC%81ed+Fault+Localization+for+Networked+Systems+Niranjan+Mysore+Vahdat+Mahajan+Varghese,http://research.google.com/pubs/pub43857.html
Transparency and Choice: Protecting Consumer Privacy in an Online World,W3C Workshop on Web Tracking and User Privacy W3C (2011) pp. 3,2011,Alma Whitten Sean Harvey Ian Fette Betsy Masiello Jochen Eisinger Jane Horvath,@incollection{37350 title = {Transparency and Choice: Protecting Consumer Privacy in an Online World} author = {Alma Whitten and Sean Harvey and Ian Fette and Betsy Masiello and Jochen Eisinger and Jane Horvath} year = 2011 URL = {http://www.w3.org/2011/track-privacy/papers/Google.pdf} booktitle = {W3C Workshop on Web Tracking and User Privacy} pages = {3} },There have been concerns raised recently about online tracking. There are a variety of mechanisms by which data is collected online and for which it is used and it is unclear which of these are intended to be addressed by “Do Not Track” mechanisms. Tracking is often data collection that helps ensure the security and integrity of data determines relevancy of served content and also helps create innovation opportunities. This value ought to be central in any “Do Not Track” discussions.,http://research.google.com/pubs/archive/37350.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Transparency+and+Choice:+Protecting+Consumer+Privacy+in+an+Online+World+Whitten+Harvey+Fette+Masiello+Eisinger+Horvath,http://research.google.com/pubs/pub37350.html
Combining compile-time and run-time instrumentation for testing tools,Programmnye produkty i sistemy vol. 3 (2013) pp. 224-231,2013,Timur Iskhodzhanov Reid Kleckner Evgeniy Stepanov,@article{41440 title = {Combining compile-time and run-time instrumentation for testing tools} author = {Timur Iskhodzhanov and Reid Kleckner and Evgeniy Stepanov} year = 2013 URL = {http://swsys.ru/index.php?page=article&id=3593&lang=en} journal = {Programmnye produkty i sistemy} pages = {224-231} volume = {3} },Dynamic program analysis and testing tools typically require inserting extra instrumentation code into the program to test. The inserted instrumentation then gathers data about the program execution and hands it off to the analysis algorithm. Various analysis algorithms can be used to perform CPU profiling processor cache simulation memory error detection data race detection etc. Usually the instrumentation is done either at run time or atcompile time – called dynamic instrumentation and compiler instrumentation respectively. However each of these methods has to make a compromise between performance and versatil-ity when used in industry software development. This paper presents a combined approach to instrumentationwhich takes the best of the two worlds – the low run-time overhead and unique features of compile-time instrumentation and the flexibility of dynamic instrumentation. Wepresent modifications of two testing tools that benefit from thisapproach: AddressSanitizer and MemorySanitizer. We propose benchmarks to compare different instrumentation frameworks in conditions specific to hybrid instrumenta-tion. We discuss the changes we made to one of the state-of-the-art instrumentation frameworks to significantly improve the performance of hybrid tools.,http://swsys.ru/index.php?page=article&id=3593&lang=en,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Combining+compile-time+and+run-time+instrumentation+for+testing+tools+Iskhodzhanov+Kleckner+Stepanov,http://research.google.com/pubs/pub41440.html
RSSAC003 - RSSAC Report on Root Zone TTLs,ICANN Root Server System Advisory Committee ( RSSAC ) Reports and Advisories Internet Corporation for Assigned Names and Numbers (ICANN) (2015) pp. 35,2015,Warren Kumari,@incollection{43974 title = {RSSAC003 - RSSAC Report on Root Zone TTLs} author = {Warren Kumari} year = 2015 URL = {https://www.icann.org/en/system/files/files/rssac-003-root-zone-ttls-21aug15-en.pdf} booktitle = {ICANN Root Server System Advisory Committee ( RSSAC ) Reports and Advisories} pages = {35} },Root zone TTLs have not changed since 1999. In this report the RSSAC Caucus studies the extent to which the current root zone TTLs are still appropriate for today’s Internet environment. Selecting a TTL for a given resource record involves finding the right balance between a few tradeoffs. Intuitively shorter TTLs are beneficial for data that changes frequently whereas longer TTLs are beneficial for data that is relatively stable. Related to this longer TTLs provide robustness in the event of operational failures. All other things being equal and assuming software involved in queries and responses follow the DNS protocol standards shorter TTLs generally result in higher query rates and longer TTLs result in lower query rates.,http://research.google.com/pubs/archive/43974.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RSSAC003+-+RSSAC+Report+on+Root+Zone+TTLs+Kumari,http://research.google.com/pubs/pub43974.html
Exploiting Service Usage Information for Optimizing Server Resource Management,ACM Transactions on Internet Technology (TOIT) vol. 11 (2011) pp. 1-26,2011,Alexander Totok Vijay Karamcheti,@article{37194 title = {Exploiting Service Usage Information for Optimizing Server Resource Management} author = {Alexander Totok and Vijay Karamcheti} year = 2011 URL = {http://dx.doi.org/10.1145/1993083.1993084} journal = {ACM Transactions on Internet Technology (TOIT)} pages = {1-26} volume = {11} },It is often difficult to tune the performance of modern component-based Internet services because: (1) component middleware are complex software systems that expose several independently tuned server resource management mechanisms; (2) session-oriented client behavior with complex data access patterns makes it hard to predict what impact tuning these mechanisms has on application behavior; and (3) component-based Internet services themselves exhibit complex structural organization with requests of different types having widely ranging execution complexity. In this article we show that exposing and using detailed information about how clients use Internet services enables mechanisms that achieve two interconnected goals: (1) providing improved QoS to the service clients and (2) optimizing server resource utilization. To differentiate among levels of service usage (service access) information we introduce the notion of the service access attribute and identify four related groups of service access attributes encompassing different aspects of service usage information ranging from the high-level structure of client web sessions to low-level fine-grained information about utilization of server resources by different requests. To show how the identified service usage information can be collected we implement a request profiling infrastructure in the JBoss Java application server. In the context of four representative service management problems we show how collected service usage information is used to improve service performance optimize server resource utilization or to achieve other problem-specific service management goals.,http://research.google.com/pubs/archive/37194.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Exploiting+Service+Usage+Information+for+Optimizing+Server+Resource+Management+Totok+Karamcheti,http://research.google.com/pubs/pub37194.html
Obfuscatory obscanturism: making workload traces of commercially-sensitive systems safe to release,CloudMAN IEEE Maui HI USA (2012),2012,Charles Reiss John Wilkes Joseph L. Hellerstein,@inproceedings{41686 title = {Obfuscatory obscanturism: making workload traces of commercially-sensitive systems safe to release} author = {Charles Reiss and John Wilkes and Joseph L. Hellerstein} year = 2012 URL = {http://www.e-wilkes.com/john/papers/2012.04-obfuscation-paper.pdf} booktitle = {CloudMAN} address = {Maui HI USA} },Cloud providers such as Google are interested in fostering research on the daunting technical challenges they face in supporting planetary-scale distributed systems but no academic organizations have similar scale systems on which to experiment. Fortunately good research can still be done using traces of real-life production workloads but there are risks in releasing such data including inadvertently disclosing conﬁdential or proprietary information as happened with the Netﬂix Prize data. This paper discusses these risks and our approach to them which we call {\em systematic obfuscation}. It protects proprietary and personal data while leaving it possible to answer some interesting research questions. We explain and motivate some of the risks and concerns and propose how they can best be mitigated using as an example our recent publication of a month-long trace of a production system workload on a 11k-machine cluster.,http://research.google.com/pubs/archive/41686.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Obfuscatory+obscanturism:+making+workload+traces+of+commercially-sensitive+systems+safe+to+release+Reiss+Wilkes+Hellerstein,http://research.google.com/pubs/pub41686.html
Italy,Telephone surveys in Europe: Research and practice Springer Berlin (2012) pp. 59-72,2012,Teresio Poggio Mario Callegaro,@inbook{40655 title = {Italy} author = {Teresio Poggio and Mario Callegaro} year = 2012 URL = {http://www.springer.com/business+%26+management/marketing/book/978-3-642-25410-9} booktitle = {Telephone surveys in Europe: Research and practice} pages = {59--72} address = {Berlin} },This chapter highlights the current Italian situation about telephone surveys. Table of contents: Introduction The reality of phone surveys in Italy Main recent changes in the technological and social context Coverage error as the big issue in phone surveys Conclusions: no way to skip the low cost-low quality vicious cycle?,http://www.springer.com/business+%26+management/marketing/book/978-3-642-25410-9,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Italy+Poggio+Callegaro,http://research.google.com/pubs/pub40655.html
Logical Itemset Mining,IEEE International Conference on Data Mining (Workshop) (2012) pp. 603-610,2012,Shailesh Kumar Chandrashekhar V. C. V. Jawahar,@inproceedings{41322 title = {Logical Itemset Mining} author = {Shailesh Kumar and Chandrashekhar V. and C. V. Jawahar} year = 2012 URL = {http://cvit.iiit.ac.in/papers/Chandrashekar2012Logical.pdf} booktitle = {IEEE International Conference on Data Mining (Workshop)} pages = {603-610} },Frequent Itemset Mining (FISM) attempts to ﬁnd large and frequent itemsets in bag-of-items data such as retail market baskets. Such data has two properties that are not naturally addressed by FISM: (i) a market basket might contain items from more than one customer intent (mixture property) and (ii) only a subset of items related to a customer intent are present in most market baskets (projection property). We propose a simple and robust framework called LOGICAL ITEMSET MINING (LISM) that treats each market basket as a mixture-of projections-of latent customer intents. LISM attempts to discover logical itemsets from such bagof-items data. Each logical itemset can be interpreted as a latent customer intent in retail or semantic concept in text tagsets. While the mixture and projection properties are easy to appreciate in retail domain they are present in almost all types of bag-of-items data. Through experiments on two large datasets we demonstrate the quality novelty and actionability of logical itemsets discovered by the simple scalable and aggressively noise-robust LISM framework. We conclude that while FISM discovers a large number of noisy observed and frequent itemsets LISM discovers a small number of high quality latent logical itemsets.,http://research.google.com/pubs/archive/41322.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Logical+Itemset+Mining+Kumar+V.+Jawahar,http://research.google.com/pubs/pub41322.html
“My religious aunt asked why I was trying to sell her viagra”: Experiences with account hijacking,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: CHI '14 ACM New York NY USA (2014) pp. 2657-2666,2014,Richard Shay Iulia Ion Robert W. Reeder Sunny Consolvo,@inproceedings{41941 title = {“My religious aunt asked why I was trying to sell her viagra”: Experiences with account hijacking} author = {Richard Shay and Iulia Ion and Robert W. Reeder and Sunny Consolvo} year = 2014 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: CHI '14} pages = {2657--2666} address = {New York NY USA} },With so much of our lives digital online and not entirely under our control we risk losing access to our communications reputation and data. Recent years have brought a rash of high-profile account compromises but account hijacking is not limited to high-profile accounts. In this paper we report results of a survey about people’s experiences with and attitudes toward account hijacking. The problem is widespread; 30% of our 294 participants had an email or social networking account accessed by an unauthorized party. Five themes emerged from our results: (1) compromised accounts are often valuable to victims (2) attackers are mostly unknown but sometimes known to victims (3) users acknowledge some responsibility for keeping their accounts secure (4) users’ understanding of important security measures is incomplete and (5) harm from account hijacking is concrete and emotional. We discuss implications for designing security mechanisms to improve chances for user adoption.,http://research.google.com/pubs/archive/41941.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%E2%80%9CMy+religious+aunt+asked+why+I+was+trying+to+sell+her+viagra%E2%80%9D:+Experiences+with+account+hijacking+Shay+Ion+Reeder+Consolvo,http://research.google.com/pubs/pub41941.html
Sparse coding of auditory features for machine hearing in interference,Proc. ICASSP IEEE (2011),2011,Richard F. Lyon Gal Chechik Jay Ponte,@inproceedings{36864 title = {Sparse coding of auditory features for machine hearing in interference} author = {Richard F. Lyon and Gal Chechik and Jay Ponte} year = 2011 booktitle = {Proc. ICASSP} },A key problem in using the output of an auditory model as the input to a machine-learning system in a machine-hearing application is to find a good feature-extraction layer. For systems such as PAMIR (passive-aggressive model for image retrieval) that work well with a large sparse feature vector a conversion from auditory images to sparse features is needed. For audio-file ranking and retrieval from text queries based on stabilized auditory images we took a multi-scale approach using vector quantization to choose one sparse feature in each of many overlapping regions of different scales with the hope that in some regions the features for a sound would be stable even when other interfering sounds were present and affecting other regions. We recently extended our testing of this approach using sound mixtures and found that the sparse-coded auditory-image features degrade less in interference than vector-quantized MFCC sparse features do. This initial success suggests that our hope of robustness in interference may indeed be realizable via the general idea of sparse features that are localized in a domain where signal components tend to be localized or stable.,http://research.google.com/pubs/archive/36864.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sparse+coding+of+auditory+features+for+machine+hearing+in+interference+Lyon+Chechik+Ponte,http://research.google.com/pubs/pub36864.html
Deep Learning in Speech Synthesis,8th ISCA Speech Synthesis Workshop Barcelona Spain (2013),2013,Heiga Zen,@misc{41539 title = {Deep Learning in Speech Synthesis} author = {Heiga Zen} year = 2013 note = {Keynote speech} },Deep learning has been a hot research topic in various machine learning related areas including general object recognition and automatic speech recognition. This talk will present recent applications of deep learning to statistical parametric speech synthesis and contrast the deep learning-based approaches to the existing hidden Markov model-based one.,http://research.google.com/pubs/archive/41539.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deep+Learning+in+Speech+Synthesis+Zen,http://research.google.com/pubs/pub41539.html
Educating Learning Technology Designers: Guiding and Inspiring Future Creators of Innovative Educational Tools,Routledge Taylor & Francis Group New York New York (2008) pp. 300,2008,Chris DiGiano Shelley Goldman Mike Chorost,@book{33476 title = {Educating Learning Technology Designers: Guiding and Inspiring Future Creators of Innovative Educational Tools} author = {Chris DiGiano and Shelley Goldman and Mike Chorost} year = 2008 note = {The book was primarily written while Chris DiGiano was a learning technologist at SRI International.} pages = {300} address = {New York New York} },This is an edited book about preparing the next generation of technology designers and developers to create compelling and effective learning technologies. It is a compilation of lessons learned from college courses across the U.S. and instructional resources such as tips and tricks and sample student projects.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Educating+Learning+Technology+Designers:+Guiding+and+Inspiring+Future+Creators+of+Innovative+Educational+Tools+DiGiano+Goldman+Chorost,http://research.google.com/pubs/pub33476.html
Large-Scale Automated Refactoring Using ClangMR,Proceedings of the 29th International Conference on Software Maintenance (2013),2013,Hyrum Wright Daniel Jasper Manuel Klimek Chandler Carruth Zhanyong Wan,@inproceedings{41342 title = {Large-Scale Automated Refactoring Using ClangMR} author = {Hyrum Wright and Daniel Jasper and Manuel Klimek and Chandler Carruth and Zhanyong Wan} year = 2013 booktitle = {Proceedings of the 29th International Conference on Software Maintenance} },Maintaining large codebases can be a challenging endeavour. As new libraries APIs and standards are introduced old code is migrated to use them. To provide as clean and succinct an interface as possible for developers old APIs are ideally removed as new ones are introduced. In practice this becomes difficult as automatically finding and transforming code in a semantically correct way can be challenging particularly as the size of a codebase increases. In this paper we present a real-world implementation of a system to refactor large C++ codebases efficiently. A combination of the Clang compiler framework and the MapReduce parallel processor ClangMR enables code maintainers to easily and correctly transform large collections of code. We describe the motivation behind such a tool its implementation and then present our experiences using it in a recent API update with Google’s C++ codebase.,http://research.google.com/pubs/archive/41342.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Automated+Refactoring+Using+ClangMR+Wright+Jasper+Klimek+Carruth+Wan,http://research.google.com/pubs/pub41342.html
Summary of Opus listening test results,IETF IETF (2011),2011,Christian Hoene Jean-Marc Valin Koen Vos Jan Skoglund,@techreport{41650 title = {Summary of Opus listening test results} author = {Christian Hoene and Jean-Marc Valin and Koen Vos and Jan Skoglund} year = 2011 URL = {http://tools.ietf.org/html/ietf-codec-results-03} institution = {IETF} },This document describes and examines listening test results obtained for the Opus codec and how they relate to the requirements.,http://tools.ietf.org/html/ietf-codec-results-03,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Summary+of+Opus+listening+test+results+Hoene+Valin+Vos+Skoglund,http://research.google.com/pubs/pub41650.html
Using transparent WDM metro rings to provide an out-of-band control network for OpenFlow in MAN,ICTON 2013 15th International Conference on Tranparent Optical Networks (ICTON'13),2013,Rafael Sanchez Jose Alberto Hernandez David Larrabeiti,@inproceedings{42530 title = {Using transparent WDM metro rings to provide an out-of-band control network for OpenFlow in MAN} author = {Rafael Sanchez and Jose Alberto Hernandez and David Larrabeiti} year = 2013 URL = {http://adscom.it.uc3m.es/files/Papers_Adscom/Jose_Alberto/2013_icton_a.pdf} booktitle = {ICTON 2013} },OpenFlow is a protocol that enables networks to evolve and change flexibly by giving a remote controller the capability of modifying the behavior of network devices. In an OpenFlow network each device needs to maintain a dedicated and separated connection with a remote controller. All these connections can be described as the OpenFlow control network that is the data network which transports control plane information and can be deployed together with the data infrastructure plane (in-band) or separated (out-of-band) with advantages and disadvantages in both cases. The control network is a critical subsystem since the communication with the controller must be reliable and ideally should be protected against failures. This paper proposes a novel ring architecture to efficiently transport both the data plane and an out-of-band control network.,http://research.google.com/pubs/archive/42530.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+transparent+WDM+metro+rings+to+provide+an+out-of-band+control+network+for+OpenFlow+in+MAN+Sanchez+Hernandez+Larrabeiti,http://research.google.com/pubs/pub42530.html
Classifying YouTube Channels: a Practical System,Proceedings of the 2nd International Workshop on Web of Linked Entities (WOLE 2013) in Proceedings of the 22nd International conference on World Wide Web companion ACM pp. 1295-1304,2013,Vincent Simonet,@inproceedings{41156 title = {Classifying YouTube Channels: a Practical System} author = {Vincent Simonet} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2488164} booktitle = {Proceedings of the 2nd International Workshop on Web of Linked Entities (WOLE 2013) in Proceedings of the 22nd International conference on World Wide Web companion} pages = {1295-1304} },This paper presents a framework for categorizing channels of videos in a thematic taxonomy with high precision and coverage. The proposed approach consists of three main steps. First videos are annotated by semantic entities describing their central topics. Second semantic entities are mapped to categories using a combination of classifiers. Last the categorization of channels is obtained by combining the results of both previous steps. This framework has been deployed on the whole corpus of YouTube in 8 languages and used to build several user facing products. Beyond the description of the framework this paper gives insight into practical aspects and experience: rationale from product requirements to the choice of the solution spam filtering human-based evaluations of the quality of the results and measured metrics on the live site.,http://research.google.com/pubs/archive/41156.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Classifying+YouTube+Channels:+a+Practical+System+Simonet,http://research.google.com/pubs/pub41156.html
N-Gram Statistical Similarities and Differences between Chinese and English,First IEEE International Conference on Semantic Computing IEEE (2007),2007,Pei Cao Stewart Yang Hongjun Zhu,@inproceedings{33035 title = {N-Gram Statistical Similarities and Differences between Chinese and English} author = {Pei Cao and Stewart Yang and Hongjun Zhu} year = 2007 URL = {http://ieeexplore.ieee.org/Xplore/defdeny.jsp?url=/iel5/4338315/4338316/04338381.pdf&code=18} booktitle = {First IEEE International Conference on Semantic Computing} },Chinese and English belong to two very different families of human languages. Yet since the underlying human concepts are universal one can expect that there are many statistics similarities between Chinese texts and English texts. In this paper we present results of analyzing quantity and frequency of N-grams in 100 million randomly-sampled English web pages and 100 million randomly-sampled Chinese web papges. We found that 1-gram and 2-gram frequency distributions are very different between Chinese and English; this is understandable since one character in Chinese does not consistitute a word in English. However we found that 3-gram and 4-grams frequency distributions are surprisingly similar between Chinese and English leading us to conjecture that in both languages frequent 3-grams and 4-grams represent a set of concepts that are similar. The distribution of unique numbers of n-grams is quite different between English and Chinese. However the distribution appears to indicate that on average 1.5 Chinese characters corresponds to 1 English word.,http://research.google.com/pubs/archive/33035.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=N-Gram+Statistical+Similarities+and+Differences+between+Chinese+and+English+cao+Yang+Zhu,http://research.google.com/pubs/pub33035.html
Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization,Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS) (2011),2011,H. Brendan McMahan,@inproceedings{37013 title = {Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization} author = {H. Brendan McMahan} year = 2011 booktitle = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)} },We prove that many mirror descent algorithms for online convex optimization (such as online gradient descent) have an equivalent interpretation as follow-the-regularized-leader (FTRL) algorithms. This observation makes the relationships between many commonly used algorithms explicit and provides theoretical insight on previous experimental observations. In particular even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly it has been observed that the FTRL-style Regularized Dual Averaging (RDA) algorithm is even more effective at producing sparsity. Our results demonstrate that the key difference between these algorithms is how they handle the cumulative L1 penalty. While FOBOS handles the $L_1$ term exactly on any given update we show that it is effectively using subgradient approximations to the L1 penalty from previous rounds leading to less sparsity than RDA which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm which we introduce can be seen as a hybrid of these two algorithms and significantly outperforms both on a large real-world dataset.,http://research.google.com/pubs/archive/37013.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Follow-the-Regularized-Leader+and+Mirror+Descent:+Equivalence+Theorems+and+L1+Regularization+McMahan,http://research.google.com/pubs/pub37013.html
Suggesting (More) Friends Using the Implicit Social Graph,International Conference on Machine Learning (ICML) (2011),2011,Maayan Roth Tzvika Barenholz Assaf Ben-David David Deutscher Guy Flysher Avinatan Hassidim Ilan Horn Ari Leichtberg Naty Leiser Yossi Matias Ron Merom,@inproceedings{37120 title = {Suggesting (More) Friends Using the Implicit Social Graph} author = {Maayan Roth and Tzvika Barenholz and Assaf Ben-David and David Deutscher and Guy Flysher and Avinatan Hassidim and Ilan Horn and Ari Leichtberg and Naty Leiser and Yossi Matias and Ron Merom} year = 2011 booktitle = {International Conference on Machine Learning (ICML)} },"Although users of online communication tools rarely categorize their contacts into groups such as ""family"" ""co-workers"" or ""jogging buddies"" they nonetheless implicitly cluster contacts by virtue of their interactions with them forming implicit groups. In this paper we describe the implicit social graph which is formed by users' interactions with contacts and groups of contacts and which is distinct from explicit social graphs in which users explicitly add other individuals as their ""friends"". We introduce an interaction-based metric for estimating a user's affinity to his contacts and groups. We then describe a novel friend suggestion algorithm that uses a user's implicit social graph to generate a friend group given a small seed set of contacts which the user has already labeled as friends. We show experimental results that demonstrate the importance of both implicit group relationships and interaction-based affinity ranking in suggesting friends. Finally we discuss two applications of the Friend Suggest algorithm that have been released as Gmail features.",http://research.google.com/pubs/archive/37120.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Suggesting+(More)+Friends+Using+the+Implicit+Social+Graph+Roth+Barenholz+Ben-David+Deutscher+Flysher+Hassidim+Horn+Leichtberg+Leiser+Matias+Merom,http://research.google.com/pubs/pub37120.html
The Correctness-Security Gap in Compiler Optimization,Security and Privacy Workshops (SPW) 2015 IEEE IEEE pp. 73-87,2015,Vijay D'Silva Mathias Payer Dawn Song,@inproceedings{43856 title = {The Correctness-Security Gap in Compiler Optimization} author = {Vijay D'Silva and Mathias Payer and Dawn Song} year = 2015 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7163211} booktitle = {Security and Privacy Workshops (SPW) 2015 IEEE} pages = {73-87} },There is a significant body of work devoted to testing verifying and certifying the correctness of optimizing compilers. The focus of such work is to determine if source code and optimized code have the same functional semantics. In this paper we introduce the correctness-security gap which arises when a compiler optimization preserves the functionality of but violates a security guarantee made by source code. We show with concrete code examples that several standard optimizations which have been formally proved correct in-habit this correctness-security gap. We analyze this gap and conclude that it arises due to techniques that model the state of the program but not the state of the underlying machine. We propose a broad research programme whose goal is to identify understand and mitigate the impact of security errors introduced by compiler optimizations. Our proposal includes research in testing program analysis theorem proving and the development of new accurate machine models for reasoning about the impact of compiler optimizations on security.,http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7163211,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Correctness-Security+Gap+in+Compiler+Optimization+D'Silva+Payer+Song,http://research.google.com/pubs/pub43856.html
Lightweight Feedback-Directed Cross-Module Optimization,Proceedings of International Symposium on Code Generation and Optimization (CGO) IEEE (2010),2010,Xinliang David Li Raksit Ashok Robert Hundt,@inproceedings{36355 title = {Lightweight Feedback-Directed Cross-Module Optimization} author = {Xinliang David Li and Raksit Ashok and Robert Hundt} year = 2010 booktitle = {Proceedings of International Symposium on Code Generation and Optimization (CGO)} },Cross-module inter-procedural compiler optimization (IPO) and Feedback-Directed Optimization (FDO) are two important compiler techniques delivering solid performance gains. The combination of IPO and FDO delivers peak performance but also multiplies both techniques' usability problems. In this paper we present LIPO a novel static IPO framework which integrates IPO and FDO. Compared to existing approaches LIPO no longer requires writing of the compiler's intermediate representation eliminates the link-time inter-procedural optimization phase entirely and minimizes code re-generation overhead thus improving scalability by an order of magnitude. Compared to an FDO baseline and without further specific tuning LIPO improves performance of SPEC2006 INT by 2.5% and of SPEC2000 INT by 4.4% with up to 23% for one benchmarks. We confirm our scalability results on a set of large industrial applications demonstrating 2.9% performance improvements on average. Compile time overhead for full builds is less than 30% incremental builds take a few seconds on average and storage requirements increase by only 24% all compared to the FDO baseline.,http://research.google.com/pubs/archive/36355.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Lightweight+Feedback-Directed+Cross-Module+Optimization+Li+Ashok+Hundt,http://research.google.com/pubs/pub36355.html
Optimistic Scheduling with Geographically Replicated Services in the Cloud Environment (COLOR),Cluster Cloud and Grid Computing (CCGrid) 2012 12th IEEE/ACM International Symposium on IEEE CONFERENCE PUBLICATIONS pp. 735-740,2012,Wenbo Zhu C. Murray Woodside,@inproceedings{40340 title = {Optimistic Scheduling with Geographically Replicated Services in the Cloud Environment (COLOR)} author = {Wenbo Zhu and C. Murray Woodside} year = 2012 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6217503&contentType=Conference+Publications} booktitle = {Cluster Cloud and Grid Computing (CCGrid) 2012 12th IEEE/ACM International Symposium on} pages = {735-740} },This paper proposes a system model that unifies different optimistic algorithms designed for deploying geographically replicated services in a cloud environment. The proposed model thereby enables a generalized solution (COLOR) by which well-specified safety and timeliness guarantees are achievable in conjunction with tunable performance requirements. The proposed solution explicitly takes advantage of the unique client-cloud interface in specifying how the level of consistency violation may be bounded for instance using probabilistic rollbacks or restarts as parameters. The solution differs from traditional Eventual Consistency models in that inconsistency is solved concurrently with online client-cloud interactions over strongly connected networks. We believe that such an approach will bring clarity to the role and limitations of the ever-popular Eventual Consistency model in cloud services.,http://research.google.com/pubs/archive/40340.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimistic+Scheduling+with+Geographically+Replicated+Services+in+the+Cloud+Environment+(COLOR)+Zhu+Woodside,http://research.google.com/pubs/pub40340.html
SAC057 - ICANN SSAC Advisory on Internal Name Certificates,ICANN SSAC Reports and Advisories ICANN (Internet Corporation for Assigned Names and Numbers) (2013),2013,Warren Kumari Steve Crocker Patrik Fältström Ondrej Filip James Galvin Danny McPherson Ram Mohan Doron Shikmoni,@incollection{41396 title = {SAC057 - ICANN SSAC Advisory on Internal Name Certificates} author = {Warren Kumari and Steve Crocker and Patrik Fältström and Ondrej Filip and James Galvin and Danny McPherson and Ram Mohan and Doron Shikmoni} year = 2013 URL = {http://www.icann.org/en/groups/ssac/documents/sac-057-en.pdf} booktitle = {ICANN SSAC Reports and Advisories} },The SSAC has identified a Certificate Authority (CA) practice that if widely exploited could pose a significant risk to the privacy and integrity of secure Internet communications. This CA practice could impact the new gTLD program. The SSAC thus advises ICANN take immediate steps to mitigate the risks.,http://research.google.com/pubs/archive/41396.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC057+-+ICANN+SSAC+Advisory+on+Internal+Name+Certificates+Kumari+Crocker+F%C3%A4ltstr%C3%B6m+Filip+Galvin+McPherson+Mohan+Shikmoni,http://research.google.com/pubs/pub41396.html
Posterior vs. Parameter Sparsity in Latent Variable Models,Advances in Neural Information Processing Systems 22 (2009) pp. 664-672,2009,Joao Graca Kuzman Ganchev Ben Taskar Fernando Pereira,@incollection{38284 title = {Posterior vs. Parameter Sparsity in Latent Variable Models} author = {Joao Graca and Kuzman Ganchev and Ben Taskar and Fernando Pereira} year = 2009 booktitle = {Advances in Neural Information Processing Systems 22} pages = {664--672} },In this paper we explore the problem of biasing unsupervised models to favor sparsity. We extend the posterior regularization framework [8] to encourage the model to achieve posterior sparsity on the unlabeled training data. We apply this new method to learn ﬁrst-order HMMs for unsupervised part-of-speech (POS) tagging and show that HMMs learned this way consistently and signiﬁcantly out-performs both EM-trained HMMs and HMMs with a sparsity-inducing Dirichlet prior trained by variational EM. We evaluate these HMMs on three languages — English Bulgarian and Portuguese — under four conditions. We ﬁnd that our method always improves performance with respect to both baselines while variational Bayes actually degrades performance in most cases. We increase accuracy with respect to EM by 2.5%-8.7% absolute and we see improvements even in a semisupervised condition where a limited dictionary is provided.,http://research.google.com/pubs/archive/38284.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Posterior+vs.+Parameter+Sparsity+in+Latent+Variable+Models+Graca+Ganchev+Taskar+Pereira,http://research.google.com/pubs/pub38284.html
An Automatic Blocking Mechanism for Large-Scale De-duplication Tasks,CIKM (2012),2012,Anish Das Sarma Ankur Jain Ashwin Machanavajjhala Philip Bohannon,@inproceedings{40415 title = {An Automatic Blocking Mechanism for Large-Scale De-duplication Tasks} author = {Anish Das Sarma and Ankur Jain and Ashwin Machanavajjhala and Philip Bohannon} year = 2012 URL = {http://arxiv.org/abs/1111.3689} booktitle = {CIKM} },De-duplication---identification of distinct records referring to the same real-world entity---is a well-known challenge in data integration. Since very large datasets prohibit the comparison of every pair of records {\em blocking} has been identified as a technique of dividing the dataset for pairwise comparisons thereby trading off {\em recall} of identified duplicates for {\em efficiency}. Traditional de-duplication tasks while challenging typically involved a fixed schema such as Census data or medical records. However with the presence of large diverse sets of structured data on the web and the need to organize it effectively on content portals de-duplication systems need to scale in a new dimension to handle a large number of schemas tasks and data sets while handling ever larger problem sizes. In addition when working in a map-reduce framework it is important that canopy formation be implemented as a {\em hash function} making the canopy design problem more challenging. We present CBLOCK a system that addresses these challenges. CBLOCK learns hash functions automatically from attribute domains and a labeled dataset consisting of duplicates. Subsequently CBLOCK expresses blocking functions using a hierarchical tree structure composed of atomic hash functions. The application may guide the automated blocking process based on architectural constraints such as by specifying a maximum size of each block (based on memory requirements) impose disjointness of blocks (in a grid environment) or specify a particular objective function trading off recall for efficiency. As a post-processing step to automatically generated blocks CBLOCK {\em rolls-up} smaller blocks to increase recall. We present experimental results on two large-scale de-duplication datasets at Yahoo!---consisting of over 140K movies and 40K restaurants respectively---and demonstrate the utility of CBLOCK.,http://arxiv.org/abs/1111.3689,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Automatic+Blocking+Mechanism+for+Large-Scale+De-duplication+Tasks+Das+Sarma+Jain+Machanavajjhala+Bohannon,http://research.google.com/pubs/pub40415.html
Refractive Height Fields from Single and Multiple Images,IEEE Conference on Computer Vision and Pattern Recognition IEEE (2012),2012,Qi Shan Sameer Agarwal Brian Curless,@inproceedings{40601 title = {Refractive Height Fields from Single and Multiple Images} author = {Qi Shan and Sameer Agarwal and Brian Curless} year = 2012 URL = {http://homes.cs.washington.edu/~sagarwal/refraction12.pdf} booktitle = {IEEE Conference on Computer Vision and Pattern Recognition} },We propose a novel framework for reconstructing homogenous transparent refractive height-ﬁelds from a single viewpoint. The height-ﬁeld is imaged against a known planar background or sequence of backgrounds. Unlike existing approaches that do a point-by-point reconstruction – which is known to have intractable ambiguities – our method estimates and optimizes for the entire height-ﬁeld at the same time. The formulation supports shape recovery from measured distortions (deﬂections) or directly from the images themselves including from a single image. We report results for a variety of refractive height-ﬁelds showing signiﬁcant improvement over prior art.,http://homes.cs.washington.edu/~sagarwal/refraction12.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Refractive+Height+Fields+from+Single+and+Multiple+Images+Shan+Agarwal+Curless,http://research.google.com/pubs/pub40601.html
Google’s Auction for Radio and TV Ads,Google Inc. (2009),2009,Noam Nisan Jason Bayer Deepak Chandra Tal Franji Robert Gardner Yossi Matias Neil Rhodes Misha Seltzer Danny Tom Hal Varian Dan Zigmond,@techreport{35113 title = {Google’s Auction for Radio and TV Ads} author = {Noam Nisan and Jason Bayer and Deepak Chandra and Tal Franji and Robert Gardner and Yossi Matias and Neil Rhodes and Misha Seltzer and Danny Tom and Hal Varian and Dan Zigmond} year = 2009 institution = {Google Inc.} },This document describes the auction system used by Google for allocation and pricing of TV ads and Radio ads. It is based on a simultaneous ascending auction and has been in use since September 2008.,http://research.google.com/pubs/archive/35113.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google%E2%80%99s+Auction+for+Radio+and+TV+Ads+Nisan+Bayer+Chandra+Franji+Gardner+Matias+Rhodes+Seltzer+Tom+Varian+Zigmond,http://research.google.com/pubs/pub35113.html
Data Fusion: Resolving Conflicts from Multiple Sources,WAIM (2013) pp. 64-76 (to appear),2013,Xin Luna Dong Laure Berti-Equille Divesh Srivastava,@inproceedings{41657 title = {Data Fusion: Resolving Conflicts from Multiple Sources} author = {Xin Luna Dong and Laure Berti-Equille and Divesh Srivastava} year = 2013 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-642-38562-9_7} booktitle = {WAIM} pages = {64-76} },Many data management applications such as setting up Web portals managing enterprise data managing community data and sharing scientific data require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users it is critical to resolve conflicts and discover values that reflect the real world; this task is called data fusion. This paper describes a novel approach that finds true values from conflicting information when there are a large number of sources among which some may copy from others. We present a case study on real-world data showing that the described algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources.,http://research.google.com/pubs/archive/41657.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Data+Fusion:+Resolving+Conflicts+from+Multiple+Sources+Dong+Berti-Equille+Srivastava,http://research.google.com/pubs/pub41657.html
The Method of Moments and Degree Distributions for Network Models,Annals of Statistics (2011),2011,Peter Bickel Aiyou Chen Liza Levina,@article{37127 title = {The Method of Moments and Degree Distributions for Network Models} author = {Peter Bickel and Aiyou Chen and Liza Levina} year = 2011 journal = {Annals of Statistics} },Probability models on graphs are becoming increasingly important in many applications but statistical tools for fitting such models are not yet well developed. Here we propose a general method of moments approach that can be used to fit a large class of probability models through empirical counts of certain patterns in a graph. We establish some general asymptotic properties of empirical graph moments and prove consistency of the estimates as the graph size grows for all ranges of the average degree including Ω(1). Additional results are obtained for the important special case of degree distributions.,http://research.google.com/pubs/archive/37127.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Method+of+Moments+and+Degree+Distributions+for+Network+Models+Bickel+Chen+Levina,http://research.google.com/pubs/pub37127.html
Real-Time Pedestrian Detection With Deep Network Cascades,Proceedings of BMVC 2015 (to appear),2015,Anelia Angelova Alex Krizhevsky Vincent Vanhoucke Abhijit Ogale Dave Ferguson,@inproceedings{43850 title = {Real-Time Pedestrian Detection With Deep Network Cascades} author = {Anelia Angelova and Alex Krizhevsky and Vincent Vanhoucke and Abhijit Ogale and Dave Ferguson} year = 2015 booktitle = {Proceedings of BMVC 2015} },We present a new real-time approach to object detection that exploits the efficiency of cascade classifiers with the accuracy of deep neural networks. Deep networks have been shown to excel at classification tasks and their ability to operate on raw pixel input without the need to design special features is very appealing. However deep nets are notoriously slow at inference time. In this paper we propose an approach that cascades deep nets and fast features that is both extremely fast and extremely accurate. We apply it to the challenging task of pedestrian detection. Our algorithm runs in real-time at 15 frames per second. The resulting approach achieves a 26.2% average miss rate on the Caltech Pedestrian detection benchmark which is competitive with the very best reported results. It is the first work we are aware of that achieves extremely high accuracy while running in real-time.,http://research.google.com/pubs/archive/43850.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Real-Time+Pedestrian+Detection+With+Deep+Network+Cascades+Angelova+Krizhevsky+Vanhoucke+Ogale+Ferguson,http://research.google.com/pubs/pub43850.html
Low Cost Correction of OCR Errors Using Learning in a Multi-Engine Environment,Proceedings of the 10th international conference on document analysis and recognition IEEE (2009),2009,Ahmad Abdulkader Matthew R. Casey,@inproceedings{35525 title = {Low Cost Correction of OCR Errors Using Learning in a Multi-Engine Environment} author = {Ahmad Abdulkader and Matthew R. Casey} year = 2009 URL = {http://www.cvc.uab.es/icdar2009/papers/3725a576.pdf} booktitle = {Proceedings of the 10th international conference on document analysis and recognition} },We propose a low cost method for the correction of the output of OCR engines through the use of human labor. The method employs an error estimator neural network that learns to assess the error probability of every word from ground-truth data. The error estimator uses features computed from the outputs of multiple OCR engines. The output probability error estimate is used to decide which words are inspected by humans. The error estimator is trained to optimize the area under the word error ROC leading to an improved efficiency of the human correction process. A significant reduction in cost is achieved by clustering similar words together during the correction process. We also show how active learning techniques are used to further improve the efficiency of the error estimator.,http://research.google.com/pubs/archive/35525.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Low+Cost+Correction+of+OCR+Errors+Using+Learning+in+a+Multi-Engine+Environment+Abdulkader+Casey,http://research.google.com/pubs/pub35525.html
Where's Waldo: Matching People in Images of Crowds,Proc. IEEE Conf. on Computer Vision and Pattern Recognition (2011) pp. 1793-1800,2011,Rahul Garg Deva Ramanan Steven M. Seitz Noah Snavely,@inproceedings{37111 title = {Where's Waldo: Matching People in Images of Crowds} author = {Rahul Garg and Deva Ramanan and Steven M. Seitz and Noah Snavely} year = 2011 URL = {http://www.cs.washington.edu/homes/rahul/data/cvpr2011.pdf} booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition} pages = {1793--1800} },Given a community-contributed set of photos of a crowded public event this paper addresses the problem of ﬁnding all images of each person in the scene. This problem is very challenging due to large changes in camera viewpoints severe occlusions low resolution and photos from tens or hundreds of different photographers. Despite these challenges the problem is made tractable by exploiting a variety of visual and contextual cues – appearance timestamps camera pose and co-occurrence of people. This paper demonstrates an approach that integrates these cues to enable high quality person matching in community photo collections downloaded from Flickr.com,http://www.cs.washington.edu/homes/rahul/data/cvpr2011.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Where's+Waldo:++Matching+People+in+Images+of+Crowds+Garg+Ramanan+Seitz+Snavely,http://research.google.com/pubs/pub37111.html
Life Estimation of Pressurized-Air Solar-Thermal Receiver Tubes,ASME Journal of Solar Energy Engineering (2012) (to appear),2012,David Fork John Fitch Shawn Ziaei Robert I. Jetter,@article{39969 title = {Life Estimation of Pressurized-Air Solar-Thermal Receiver Tubes} author = {David Fork and John Fitch and Shawn Ziaei and Robert I. Jetter} year = 2012 journal = {ASME Journal of Solar Energy Engineering} },The operational conditions of the solar thermal receiver for a Brayton-cycle engine are challenging and lack a large body of operational data unlike steam plants. We explore the receiver’s fundamental element a pressurized tube in time varying solar flux for a series of 30 year service missions based on hypothetical power plant designs. We developed and compared two estimation methods to predict the receiver tube lifetime based on available creep life and fatigue data for alloy 617. We show that the choice of inelastic strain model and the level of conservatism applied through design rules will vary the lifetime predictions by orders of magnitude. Based on current data and methods a turbine inlet temperature of 850 C is a necessary 30-year-life-design condition for our receiver. We also showed that even though the time at operating temperature is about three times longer for fossil fuel powered (steady) operation the damage is always lower than cyclic operation using solar power .,http://research.google.com/pubs/archive/39969.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Life+Estimation+of+Pressurized-Air+Solar-Thermal+Receiver+Tubes+Fork+Fitch+Ziaei+Jetter,http://research.google.com/pubs/pub39969.html
Efficient and Accurate Label Propagation on Dynamic Graphs and Label Sets,International Journal on Advances in Networks and Services vol. 6 (2013) pp. 246-259,2013,Michele Covell Shumeet Baluja,@article{41896 title = {Efficient and Accurate Label Propagation on Dynamic Graphs and Label Sets} author = {Michele Covell and Shumeet Baluja} year = 2013 journal = {International Journal on Advances in Networks and Services} pages = {246--259} volume = {6} },Many web-based application areas must infer label distributions starting from a small set of sparse noisy labels. Previous work has shown that graph-based propagation can be very effective at finding the best label distribution across nodes starting from partial information and a weighted-connection graph. In their work on video recommendations Baluja et al. showed high-quality results using Adsorption a normalized propagation process. An important step in the original formulation of Adsorption was re-normalization of the label vectors associated with each node between every propagation step. That interleaved normalization forced computation of all label distributions in synchrony in order to allow the normalization to be correctly determined. Interleaved normalization also prevented use of standard linear-algebra methods like stabilized bi-conjugate gradient descent (BiCGStab) and Gaussian elimination. We show how to replace the interleaved normalization with a single pre-normalization done once before the main propagation process starts allowing use of selective label computation (label slicing) as well as large-matrix-solution methods. As a result much larger graphs and label sets can be handled than in the original formulation and more accurate solutions can be found in fewer propagation steps. We further extend that work to handle graphs that change and expand over time. We report results from using pre-normalized Adsorption in topic labeling for web domains using label slicing and BiCGStab. We also report results from using incremental updates on changing co-author network data. Finally we discuss two options for handling mixed-sign (positive and negative) graphs and labels.,http://research.google.com/pubs/archive/41896.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+and+Accurate+Label+Propagation+on+Dynamic+Graphs+and+Label+Sets+Covell+Baluja,http://research.google.com/pubs/pub41896.html
Optimizing Touchscreen Keyboards for Gesture Typing,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015) ACM New York NY USA pp. 3365-3374,2015,Brian Smith Xiaojun Bi Shumin Zhai,@inproceedings{43271 title = {Optimizing Touchscreen Keyboards for Gesture Typing} author = {Brian Smith and Xiaojun Bi and Shumin Zhai} year = 2015 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)} pages = {3365--3374} address = {New York NY USA} },Despite its growing popularity gesture typing suffers from a major problem not present in touch typing: gesture ambiguity on the Qwerty keyboard. By applying rigorous mathematical optimization methods this paper systematically investigates the optimization space related to the accuracy speed and Qwerty similarity of a gesture typing keyboard. Our investigation shows that optimizing the layout for gesture clarity (a metric measuring how unique word gestures are on a keyboard) drastically improves the accuracy of gesture typing. Moreover if we also accommodate gesture speed or both gesture speed and Qwerty similarity we can still reduce error rates by 52% and 37% over Qwerty respectively. In addition to investigating the optimization space this work contributes a set of optimized layouts such as GK-D and GK-T that can immediately benefit mobile device users.,http://research.google.com/pubs/archive/43271.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+Touchscreen+Keyboards+for+Gesture+Typing+Smith+Bi+Zhai,http://research.google.com/pubs/pub43271.html
Measuring Advertising Quality on Television: Deriving Meaningful Metrics from Audience Retention Data,Journal of Advertising Research vol. 49 (2009) pp. 419-428,2009,Dan Zigmond Sundar Dorai-Raj Yannet Interian Igor Naverniouk,@article{35667 title = {Measuring Advertising Quality on Television: Deriving Meaningful Metrics from Audience Retention Data} author = {Dan Zigmond and Sundar Dorai-Raj and Yannet Interian and Igor Naverniouk} year = 2009 journal = {Journal of Advertising Research} pages = {419-428} volume = {49} },This paper introduces a measure of television ad quality based on audience retention using logistic regression techniques to normalize such scores against expected audience behavior. By adjusting for features such as time of day network recent user behavior and household demographics we are able to isolate ad quality from these extraneous factors. We introduce the current model used in our production system as well as two new competing models that show some improvement. We also devise metrics for calculating a model’s predictive power and variance allowing us to determine which of our models performs best. We conclude with discussions of retention score applications for advertisers to evaluate their ad strategies and potential as an aid in future ad pricing.,http://research.google.com/pubs/archive/35667.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Measuring+Advertising+Quality+on+Television:+Deriving+Meaningful+Metrics+from+Audience+Retention+Data+Zigmond+Dorai-Raj+Interian+Naverniouk,http://research.google.com/pubs/pub35667.html
Using Bezier Curve to Improve the Accuracy in Integrated Circuit Design Analysis,International Conference on Convergence and Hybrid Information Technology ACM Press (2010),2010,Eric Y. Chen May Huang,@inproceedings{36625 title = {Using Bezier Curve to Improve the Accuracy in Integrated Circuit Design Analysis} author = {Eric Y. Chen and May Huang} year = 2010 booktitle = {International Conference on Convergence and Hybrid Information Technology} },In this paper we introduce a method for the application of Bezier curve algorithms to lookup-table-based interpolations particularly for the use of timing power and noise analysis in integrated circuit design. B´zier curves can replace conventional piecewise linear functions for accuracy enhancement. Selecting control points with physical implications and forcing curves to pass through sampled points are critical for achieving the envisioned improvements. This method can be easily applied to the interpolation of splines.,http://research.google.com/pubs/archive/36625.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Bezier+Curve+to+Improve+the+Accuracy+in+Integrated+Circuit+Design+Analysis+Chen+Huang,http://research.google.com/pubs/pub36625.html
An Active Approach to Measuring Routing Dynamics Induced by Autonomous Systems,Workshop of Experimental Computer Science (ExpCS) ACM (2007),2007,Samantha Lo Rocky K. C. Chang Lorenzo Colitti,@inproceedings{32887 title = {An Active Approach to Measuring Routing Dynamics Induced by Autonomous Systems} author = {Samantha Lo and Rocky K. C. Chang and Lorenzo Colitti} year = 2007 URL = {http://www.cs.huji.ac.il/~feit/exp/papers/155.pdf} booktitle = {Workshop of Experimental Computer Science (ExpCS)} },We present an active measurement study of the routing dynamics induced by AS-path prepending a common method for controlling the inbound traffic of a multi-homed ISP. Unlike other inter-domain inbound traffic engineering methods AS-path prepending not only provides network resilience but does not increase routing table size. Unfortunately ISPs often perform prepending on a trail-and-error basis which can lead to suboptimal results and to a large amount of network churn. We study these effects by actively injecting prepended routes into the Internet routing system using the RIPE NCC RIS route collectors and observing the resulting changes from almost 200 publicly-accessible sources of BGP information. Our results show that our prepending methods are simple and effective and that a small number of ASes is often responsible for large amounts of the route changes caused by prepending. Furthermore we show that our methods are able to reveal hidden prepending policies to prepending and tie-breaking decisions made by ASes; this is useful for further predicting the behavior of prepending.,http://research.google.com/pubs/archive/32887.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Active+Approach+to+Measuring+Routing+Dynamics+Induced+by+Autonomous+Systems+Lo+Chang+Colitti,http://research.google.com/pubs/pub32887.html
Wikidata: A Free Collaborative Knowledge Base,Communications of the ACM vol. 57 (2014) pp. 78-85,2014,Denny Vrande_i_ Markus Krötzsch,@article{42240 title = {Wikidata: A Free Collaborative Knowledge Base} author = {Denny Vrande_i_ and Markus Krötzsch} year = 2014 URL = {http://cacm.acm.org/magazines/2014/10/178785-wikidata/fulltext} journal = {Communications of the ACM} pages = {78--85} volume = {57} },Unnoticed by most of its readers Wikipedia is currently undergoing dramatic changes as its sister project Wikidata introduces a new multilingual ‘Wikipedia for data’ to manage the factual information of the popular online encyclopedia. With Wikipedia’s data becoming cleaned and integrated in a single location opportunities arise for many new applications. In this article we provide an extended overview of Wikidata including its essential design choices and data model. Based on up-to-date statistics we discuss the project's development so far and outline interesting application areas for this new resource.,http://research.google.com/pubs/archive/42240.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Wikidata:+A+Free+Collaborative+Knowledge+Base+Vrande%C4%8Di%C4%87+Kr%C3%B6tzsch,http://research.google.com/pubs/pub42240.html
RFC7607 - Codification of AS 0 Processing,IETF RFCs Internet Engineering Task Force (2015) pp. 5,2015,Warren Kumari Randy Bush Heather Schiller Keyur Patel,@incollection{43954 title = {RFC7607 - Codification of AS 0 Processing} author = {Warren Kumari and Randy Bush and Heather Schiller and Keyur Patel} year = 2015 URL = {http://www.rfc-editor.org/rfc/rfc7607.txt} booktitle = {IETF RFCs} pages = {5} },This document updates RFC 4271 and proscribes the use of Autonomous System (AS) 0 in the Border Gateway Protocol (BGP) OPEN AS_PATH AS4_PATH AGGREGATOR and AS4_AGGREGATOR attributes in the BGP UPDATE message.,http://research.google.com/pubs/archive/43954.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7607+-+Codification+of+AS+0+Processing+Kumari+Bush+Schiller+Patel,http://research.google.com/pubs/pub43954.html
Showing Relevant Ads via Lipschitz Context Multi-Armed Bandits,Thirteenth International Conference on Artificial Intelligence and Statistics Journal of Machine Learning Research (2010),2010,Tyler Lu Dávid Pál Martin Pál,@inproceedings{37042 title = {Showing Relevant Ads via Lipschitz Context Multi-Armed Bandits} author = {Tyler Lu and Dávid Pál and Martin Pál} year = 2010 URL = {http://jmlr.csail.mit.edu/proceedings/papers/v9/lu10a/lu10a.pdf} booktitle = {Thirteenth International Conference on Artificial Intelligence and Statistics} },We study contextual multi-armed bandit problems where the context comes from a metric space and the payoff satisfies a Lipschitz condition with respect to the metric. Abstractly a contextual multi-armed bandit problem models a situation where in a sequence of independent trials an online algorithm chooses based on a given context (side information) an action from a set of possible actions so as to maximize the total payoff of the chosen actions. The payoff depends on both the action chosen and the context. In contrast context-free multi-armed bandit problems a focus of much previous research model situations where no side information is available and the payoff depends only on the action chosen. Our problem is motivated by sponsored web search where the task is to display ads to a user of an Internet search engine based on her search query so as to maximize the click-through rate (CTR) of the ads displayed. We cast this problem as a contextual multi-armed bandit problem where queries and ads form metric spaces and the payoff function is Lipschitz with respect to both the metrics. For any $\epsilon > 0$ we present an algorithm with regret $O(T^{\frac{a+b+1}{a+b+2} + \epsilon})$ where $ab$ are the covering dimensions of the query space and the ad space respectively. We prove a lower bound $\Omega(T^{\frac{\tilde{a}+\tilde{b}+1}{\tilde{a}+\tilde{b}+2} \epsilon})$ for the regret of any algorithm where $\tilde{a} \tilde{b}$ are packing dimensions of the query spaces and the ad space respectively. For finite spaces or convex bounded subsets of Euclidean spaces this gives an almost matching upper and lower bound.,http://research.google.com/pubs/archive/37042.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Showing+Relevant+Ads+via+Lipschitz+Context+Multi-Armed+Bandits+Lu+P%C3%A1l+P%C3%A1l,http://research.google.com/pubs/pub37042.html
Catching a viral video,Journal of Intelligent Information Systems (2011) pp. 1-19,2011,Tom Broxton Yannet Interian Jon Vaver Mirjam Wattenhofer,@article{37650 title = {Catching a viral video} author = {Tom Broxton and Yannet Interian and Jon Vaver and Mirjam Wattenhofer} year = 2011 journal = {Journal of Intelligent Information Systems} pages = {1-19} },The sharing and re-sharing of videos on social sites blogs e-mail and other means has given rise to the phenomenon of viral videos—videos that become popular through internet sharing. In this paper we seek to better understand viral videos on YouTube by analyzing sharing and its relationship to video popularity using millions of YouTube videos. The socialness of a video is quantified by classifying the referrer sources for video views as social (e.g. an emailed link Facebook referral) or non-social (e.g. a link from related videos). We find that viewership patterns of highly social videos are very different from less social videos. For example the highly social videos rise to and fall from their peak popularity more quickly than less social videos. We also find that not all highly social videos become popular and not all popular videos are highly social. By using our insights on viral videos we are able develop a method for ranking blogs and websites on their ability to spread viral videos.,http://research.google.com/pubs/archive/37650.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Catching+a+viral+video+Broxton+Interian+Vaver+Wattenhofer,http://research.google.com/pubs/pub37650.html
Modeling and Synthesizing Task Placement Constraints in Google Compute Clusters,Symposium on Cloud Computing ACM (2011),2011,Victor Chudnovsky Rasekh Rifaat Joseph Hellerstein Bikash Sharma Chita Das,@inproceedings{36953 title = {Modeling and Synthesizing Task Placement Constraints in Google Compute Clusters} author = {Victor Chudnovsky and Rasekh Rifaat and Joseph Hellerstein and Bikash Sharma and Chita Das} year = 2011 booktitle = {Symposium on Cloud Computing} },Evaluating the performance of large compute clusters requires benchmarks with representative workloads. At Google performance benchmarks are used to obtain performance metrics such as task scheduling delays and machine resource utilizations to assess changes in application codes machine conﬁgurations and scheduling algorithms. Existing approaches to workload characterization for high performance computing and grids focus on task resource requirements for CPU memory disk I/O network etc. Such resource requirements address how much resource is consumed by a task. However in addition to resource requirements Google workloads commonly include task placement constraints that determine which machine resources are consumed by tasks. Task placement constraints arise because of task dependencies such as those related to hardware architecture and kernel version. This paper develops methodologies for incorporating task placement constraints and machine properties into performance benchmarks of large compute clusters. Our studies of Google compute clusters show that constraints increase average task scheduling delays by a factor of 2 to 6 which often results in tens of minutes of additional task wait time. To understand why we extend the concept of resource utilization to include constraints by introducing a new metric the Utilization Multiplier (UM). UM is the ratio of the resource utilization seen by tasks with a constraint to the average utilization of the resource. UM provides a simple model of the performance impact of constraints in that task scheduling delays increase with UM. Last we describe how to synthesize representative task constraints and machine properties and how to incorporate this synthesis into existing performance benchmarks. Using synthetic task constraints and machine properties generated by our methodology we accurately reproduce performance metrics for benchmarks of Google compute clusters with a discrepancy of only 13% in task scheduling delay and 5% in resource utilization.,http://research.google.com/pubs/archive/36953.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Modeling+and+Synthesizing+Task+Placement+Constraints+in+Google+Compute+Clusters+Chudnovsky+Rifaat+Hellerstein+Sharma+Das,http://research.google.com/pubs/pub36953.html
Effective Java Second Edition,Addison-Wesley Boston MA (2008),2008,Joshua Bloch,@book{36452 title = {Effective Java Second Edition} author = {Joshua Bloch} year = 2008 address = {Boston MA} },Second edition of the best-selling Jolt Award winning Java best practices guide. Covers Java SE 6 including all of the language features introduced in Java 5.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Effective+Java+Second+Edition+Bloch,http://research.google.com/pubs/pub36452.html
The multi-iterative closest point tracker: An online algorithm for tracking multiple interacting targets,Journal of Field Robotics vol. 29.2 (2012) pp. 258-276,2012,Adam Feldman Maria Hybinette Tucker Balch,@article{39982 title = {The multi-iterative closest point tracker: An online algorithm for tracking multiple interacting targets} author = {Adam Feldman and Maria Hybinette and Tucker Balch} year = 2012 journal = {Journal of Field Robotics} pages = {258-276} volume = {29.2} },We describe and evaluate a greedy detection-based algorithm for tracking a variable number of dynamic targets online. The algorithm leverages the well-known iterative closest point (ICP) algorithm for aligning target models with target detections. The approach differs from trackers that seek globally optimal solutions because it treats the problem as a set of individual tracking problems. The method works for multiple targets by sequentially matching models to detections and then removing detections from further consideration once models have been matched to them. This allows targets to pass close to one another with reduced risks of tracking failure due to “hijacking'' or track merging. There has been significant previous work in this area but we believe our approach addresses a number of tracking problems simultaneously that have only been addressed separately before. The algorithm is evaluated using four to eight laser range finders in three settings: quantitatively for a basketball game with 10 people and a 25-person social behavior experiment and qualitatively for a full-scale soccer game. We also provide qualitative results using video to track ants in a captive habitat. During all the experiments agents enter and leave the scene so the number of targets to track varies with time. With eight laser range finders running the system can locate and track targets at sensor frame rate 37.5 Hz on commodity computing hardware. Our evaluation shows that the tracking system correctly detects each track over 98% of the time.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+multi-iterative+closest+point+tracker:+An+online+algorithm+for+tracking+multiple+interacting+targets+Feldman+Hybinette+Balch,http://research.google.com/pubs/pub39982.html
Spatiotemporal Deformable Part Models for Action Detection,Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013),2013,Yicong Tian Rahul Sukthankar Mubarak Shah,@inproceedings{40752 title = {Spatiotemporal Deformable Part Models for Action Detection} author = {Yicong Tian and Rahul Sukthankar and Mubarak Shah} year = 2013 booktitle = {Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013)} },Deformable part models have achieved impressive performance for object detection even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.,http://research.google.com/pubs/archive/40752.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Spatiotemporal+Deformable+Part+Models+for+Action+Detection+Tian+Sukthankar+Shah,http://research.google.com/pubs/pub40752.html
Automatic Efficient Temporally-Coherent Video Enhancement for Large Scale Applications,ACM Multimedia ACM (2009) pp. 609-612,2009,George Toderici Jay Yagnik,@inproceedings{35264 title = {Automatic Efficient Temporally-Coherent Video Enhancement for Large Scale Applications} author = {George Toderici and Jay Yagnik} year = 2009 booktitle = {ACM Multimedia} pages = {609-612} },A fast and robust method for video contrast enhancement is presented. The method uses the histogram of each frame along with upper and lower bounds computed per shot in order to enhance the current frame. This ensures that the artifacts introduced during the enhancement is reduced to a minimum. Traditional methods that do not compute per-shot estimates tend to over-enhance parts of the video such as fades and transitions. Our method does not suffer from this problem which is essential for a fully automatic algorithm. We present the parameters for our methods which yielded the best human feedback which showed that out of 208 videos 203 were enhanced while the remaining 5 were of too poor quality to be enhanced. Additionally we present a visual comparison of our work with the recently-proposed Weighted Thresholded Histogram Equalization (WTHE) algorithm.,http://research.google.com/pubs/archive/35264.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatic+Efficient+Temporally-Coherent+Video+Enhancement+for+Large+Scale+Applications+Toderici+Yagnik,http://research.google.com/pubs/pub35264.html
Dynamic Race Detection with LLVM Compiler,Google (2011),2011,Konstantin Serebryany Alexander Potapenko Timur Iskhodzhanov Dmitry Vyukov,@techreport{37278 title = {Dynamic Race Detection with LLVM Compiler} author = {Konstantin Serebryany and Alexander Potapenko and Timur Iskhodzhanov and Dmitry Vyukov} year = 2011 institution = {Google} },Data races are among the most difficult to detect and costly bugs. Race detection has been studied widely but none of the existing tools satisfies the requirements of high speed detailed reports and wide availability at the same time. We describe our attempt to create a tool that works fast has detailed and understandable reports and is available on a variety of platforms. The race detector is based on our previous work ThreadSanitizer and the instrumentation is done using the LLVM compiler. We show that applying compiler instrumentation and sampling reduces the slowdown to less than 1.5x fast enough for interactive use.,http://research.google.com/pubs/archive/37278.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+Race+Detection+with+LLVM+Compiler+Serebryany+Potapenko+Iskhodzhanov+Vyukov,http://research.google.com/pubs/pub37278.html
Large Scale Language Modeling in Automatic Speech Recognition,Google (2012),2012,Ciprian Chelba Dan Bikel Maria Shugrina Patrick Nguyen Shankar Kumar,@techreport{40491 title = {Large Scale Language Modeling in Automatic Speech Recognition} author = {Ciprian Chelba and Dan Bikel and Maria Shugrina and Patrick Nguyen and Shankar Kumar} year = 2012 institution = {Google} },Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data and the size of the language model estimated from such data. Depending on the task availability and amount of training data used language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative for systems on a wide range of operating points between 17% and 52% word error rate.,http://research.google.com/pubs/archive/40491.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Language+Modeling+in+Automatic+Speech+Recognition+Chelba+Bikel+Shugrina+Nguyen+Kumar,http://research.google.com/pubs/pub40491.html
Technology-Driven Highly-Scalable Dragonfly Topology,Proceedings of the 35th International Symposium on Computer Architecture IEEE Computer Society Washington DC USA (2008) pp. 77-88,2008,John Kim William J. Dally Steve Scott Dennis Abts,@inproceedings{34926 title = {Technology-Driven Highly-Scalable Dragonfly Topology} author = {John Kim and William J. Dally and Steve Scott and Dennis Abts} year = 2008 URL = {http://portal.acm.org/results.cfm?coll=ACM&dl=ACM&CFID=9688680&CFTOKEN=35045273} booktitle = {Proceedings of the 35th International Symposium on Computer Architecture} pages = {77-88} address = {Washington DC USA} },Evolving technology and increasing pin-bandwidth motivate the use of high-radix routers to reduce the diameter latency and cost of interconnection networks. High-radix networks however require longer cables than their low-radix counterparts. Because cables dominate network cost the number of cables and particularly the number of long global cables should be minimized to realize an efficient network. In this paper we introduce the dragonfly topology which uses a group of high-radix routers as a virtual router to increase the effective radix of the network. With this organization each minimally routed packet traverses at most one global channel. By reducing global channels a dragonfly reduces cost by 20% compared to a flattened butterfly and by 52% compared to a folded Clos network in configurations with ≥ 16K nodes.We also introduce two new variants of global adaptive routing that enable load-balanced routing in the dragonfly. Each router in a dragonfly must make an adaptive routing decision based on the state of a global channel connected to a different router. Because of the indirect nature of this routing decision conventional adaptive routing algorithms give degraded performance. We introduce the use of selective virtual-channel discrimination and the use of credit round-trip latency to both sense and signal channel congestion. The combination of these two methods gives throughput and latency that approaches that of an ideal adaptive routing algorithm.,http://research.google.com/pubs/archive/34926.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Technology-Driven+Highly-Scalable+Dragonfly+Topology+Kim+Dally+Scott+Abts,http://research.google.com/pubs/pub34926.html
A theory of learning from different domains,Machine Learning vol. 79 (2010) pp. 151-175,2010,Shai Ben-David John Blitzer Koby Crammer Alex Kulesza Fernando Pereira Jennifer Vaughan,@article{36364 title = {A theory of learning from different domains} author = {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan} year = 2010 URL = {http://www.springerlink.com/content/q6qk230685577n52/} journal = {Machine Learning} pages = {151--175} volume = {79} },Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often however we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First under what conditions can a classifier trained from source data be expected to perform well on target data? Second given a small amount of labeled target data how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time? We address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier. We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error just the target error or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence the sample sizes of both domains and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors.,http://research.google.com/pubs/archive/36364.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+theory+of+learning+from+different+domains+Ben-David+Blitzer+Crammer+Kulesza+Pereira+Vaughan,http://research.google.com/pubs/pub36364.html
Language Modeling in the Era of Abundant Data,Stanford Information Theory Forum (2015),2015,Ciprian Chelba,@misc{43258 title = {Language Modeling in the Era of Abundant Data} author = {Ciprian Chelba} year = 2015 URL = {http://web.stanford.edu/group/it-forum/colloquium.html} note = {Invited Talk} },"The talk presents an overview of statistical language modeling as applied to real-word problems: speech recognition machine translation spelling correction soft keyboards to name a few prominent ones. We summarize the most successful estimation techniques and examine how they fare for applications with abundant data e.g. voice search. We conclude by highlighting a few open problems: getting an accurate estimate for the entropy of text produced by a very specific source e.g. query stream); optimally leveraging data that is of different degrees of relevance to a given ""domain""; does a bound on the size of a ""good"" model for a given source exist?",http://research.google.com/pubs/archive/43258.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Language+Modeling+in+the+Era+of+Abundant+Data+Chelba,http://research.google.com/pubs/pub43258.html
Supporting Privacy-Conscious App Update Decisions with User Reviews,Proceedings of the 5th Annual ACM CCS Workshop on Security and Privacy in Smartphones and Mobile Devices ACM New York NY USA (2015) pp. 51-61,2015,Yuan Tian Bin Liu Weisi Dai Blase Ur Patrick Tague Lorrie Faith Cranor,@inproceedings{44195 title = {Supporting Privacy-Conscious App Update Decisions with User Reviews} author = {Yuan Tian and Bin Liu and Weisi Dai and Blase Ur and Patrick Tague and Lorrie Faith Cranor} year = 2015 URL = {https://dl.acm.org/citation.cfm?id=2808124} booktitle = {Proceedings of the 5th Annual ACM CCS Workshop on Security and Privacy in Smartphones and Mobile Devices} pages = {51-61} address = {New York NY USA} },Smartphone app updates are critical to user security and privacy. New versions may fix important security bugs which is why users should usually update their apps. However occasionally apps turn malicious or radically change features in a way users dislike. Users should not necessarily always update in those circumstances but current update processes are largely automatic. Therefore it is important to understand user behaviors around updating apps and help them to make security-conscious choices. We conducted two related studies in this area. First to understand users' current update decisions we conducted an online survey of user attitudes toward updates. Based on the survey results we then designed a notification scheme integrating user reviews which we tested in a field study. Participants installed an Android app that simulated update notifications enabling us to collect users' update decisions and reactions. We compared the effectiveness of our review-based update notifications with the permission-based notifications. Compared to notifications with permission descriptions only we found our review-based update notification was more effective at alerting users of invasive or malicious app updates especially for less trustworthy apps.,http://research.google.com/pubs/archive/44195.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Supporting+Privacy-Conscious+App+Update+Decisions+with+User+Reviews+Tian+Liu+Dai+Ur+Tague+Cranor,http://research.google.com/pubs/pub44195.html
The Impact of Memory Subsystem Resource Sharing on Datacenter Applications,ISCA ACM (2011),2011,Lingjia Tang Jason Mars Neil Vachharajani Robert Hundt Mary-Lou Soffa,@inproceedings{37124 title = {The Impact of Memory Subsystem Resource Sharing on Datacenter Applications} author = {Lingjia Tang and Jason Mars and Neil Vachharajani and Robert Hundt and Mary-Lou Soffa} year = 2011 booktitle = {ISCA} },In this paper we study the impact of sharing memory resources on ﬁve Google datacenter applications: a web search engine bigtable content analyzer image stitching and protocol bu_er. While prior work has found neither positive nor negative e_ects from cache sharing across the PARSEC benchmark suite we ﬁnd that across these datacenter applications there is both a sizable beneﬁt and a potential degradation from improperly sharing resources. In this paper we ﬁrst present a study of the importance of thread-tocore mappings for applications in the datacenter as threads can be mapped to share or to not share caches and bus bandwidth. Second we investigate the impact of co-locating threads from multiple applications with diverse memory behavior and discover that the best mapping for a given application changes depending on its co-runner. Third we investigate the application characteristics that impact performance in the various thread-to-core mapping scenarios. Finally we present both a heuristics-based and an adaptive approach to arrive at good thread-to-core decisions in the datacenter. We observe performance swings of up to 25% for web search and 40% for other key applications simply based on how application threads are mapped to cores. By employing our adaptive thread-to-core mapper the performance of the datacenter applications presented in this work improved by up to 22% over status quo thread-to-core mapping and performs within 3% of optimal.,http://research.google.com/pubs/archive/37124.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Impact+of+Memory+Subsystem+Resource+Sharing+on+Datacenter+Applications+Tang+Mars+Vachharajani+Hundt+Souffa,http://research.google.com/pubs/pub37124.html
On the structure of weakly acyclic games,Theory of Computing Systems vol. 53 (2013) pp. 107-122,2013,Alex Fabrikant Aaron D Jaggard Michael Schapira,@article{42472 title = {On the structure of weakly acyclic games} author = {Alex Fabrikant and Aaron D Jaggard and Michael Schapira} year = 2013 journal = {Theory of Computing Systems} pages = {107--122} volume = {53} },The class of weakly acyclic games which includes potential games and dominance-solvable games captures many practical application domains. In a weakly acyclic game from any starting state there is a sequence of better-response moves that leads to a pure Nash equilibrium; informally these are games in which natural distributed dynamics such as better-response dynamics cannot enter inescapable oscillations. We establish a novel link between such games and the existence of pure Nash equilibria in subgames. Specifically we show that the existence of a unique pure Nash equilibrium in every subgame implies the weak acyclicity of a game. In contrast the possible existence of multiple pure Nash equilibria in every subgame is insufficient for weak acyclicity in general; here we also systematically identify the special cases (in terms of the number of players and strategies) for which this is sufficient to guarantee weak acyclicity.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+structure+of+weakly+acyclic+games+Fabrikant+Jaggard+Schapira,http://research.google.com/pubs/pub42472.html
Three Controversial Hypotheses Concerning Computation in the Primate Cortex,Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence AAAI Press (2012),2012,Thomas Dean Greg Corrado Jonathon Shlens,@inproceedings{41100 title = {Three Controversial Hypotheses Concerning Computation in the Primate Cortex} author = {Thomas Dean and Greg Corrado and Jonathon Shlens} year = 2012 URL = {http://cs.brown.edu/research/pubs/techreports/reports/CS-12-01.html} booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence} },We consider three hypotheses concerning the primate neocortex which have influenced computational neuroscience in recent years. Is the mind modular in terms of its being profitably described as a collection of relatively independent functional units? Does the regular structure of the cortex imply a single algorithm at work operating on many different inputs in parallel? Can the cognitive differences between humans and our closest primate relatives be explained in terms of a scalable cortical architecture? We bring to bear diverse sources of evidence to argue that the answers to each of these questions - with some judicious qualifications - are in the affirmative. In particular we argue that while our higher cognitive functions may interact in a complicated fashion many of the component functions operate through well-defined interfaces and perhaps more important are built on a neural substrate that scales easily under the control of a modular genetic architecture. Processing in the primary sensory cortices seem amenable to similar algorithmic principles and even for those cases where alternative principles are at play the regular structure of cortex allows the same or greater advantages as the architecture scales. Similar genetic machinery to that used by nature to scale body plans has apparently been applied to scale cortical computations. The resulting replicated computing units can be used to build larger working memory and support deeper recursions needed to qualitatively improve our abilities to handle language abstraction and social interaction.,http://research.google.com/pubs/archive/41100.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Three+Controversial+Hypotheses+Concerning+Computation+in+the+Primate+Cortex+Dean+Corrado+Shlens,http://research.google.com/pubs/pub41100.html
Joint Noise Level Estimation from Personal Photo Collections,ICCV 2013 (to appear),2013,YiChang Shih Vivek Kwatra Troy Chinen Hui Fang Sergey Ioffe,@inproceedings{41545 title = {Joint Noise Level Estimation from Personal Photo Collections} author = {YiChang Shih and Vivek Kwatra and Troy Chinen and Hui Fang and Sergey Ioffe} year = 2013 booktitle = {ICCV 2013} },"Personal photo albums are heavily biased towards faces of people but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality others may be corrupted by noise. Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically we compare geometrically and photometrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels which allows for a pairwise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally we employ the estimated noise levels for automatic denoising using ""BM3D"" and evaluate the quality of denoising on real-world photos through a user study.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Joint+Noise+Level+Estimation+from+Personal+Photo+Collections+Shih+Kwatra+Chinen+Fang+Ioffe,http://research.google.com/pubs/pub41545.html
Taming Hardware Event Samples for FDO Compilation,Proceedings of International Symposium on Code Generation and Optimization (CGO) (2010),2010,Dehao Chen Neil Vachharajani Robert Hundt Shih-wei Liao Vinodha Ramasamy Paul Yuan Wenguang Chen Weiming Zheng,@inproceedings{36358 title = {Taming Hardware Event Samples for FDO Compilation} author = {Dehao Chen and Neil Vachharajani and Robert Hundt and Shih-wei Liao and Vinodha Ramasamy and Paul Yuan and Wenguang Chen and Weiming Zheng} year = 2010 booktitle = {Proceedings of International Symposium on Code Generation and Optimization (CGO)} },Feedback-directed optimization (FDO) is effective in improving application runtime performance but has not been widely adopted due to the tedious dual-compilation model the difficulties in generating representative training data sets and the high runtime overhead of profile collection. The use of hardware-event sampling to generate estimated edge profiles overcomes these drawbacks. Yet hardware event samples are typically not precise at the instruction or basic-block granularity. These inaccuracies lead to missed performance when compared to instrumentation-based FDO. In this paper we use multiple hardware event profiles and supervised learning techniques to generate heuristics for improved precision of basic-block-level sample profiles and to further improve the smoothing algorithms used to construct edge profiles. We demonstrate that sampling-based FDO can achieve an average of 78% of the performance gains obtained using instrumentation-based exact edge profiles for SPEC2000 benchmarks matching or beating instrumentation-based FDO in many cases. The overhead of collection is only 0.74% on average while compiler based instrumentation incurs 6.8%–53.5% overhead (and 10x overhead on an industrial web search application) and dynamic instrumentation incurs 28.6%–1639.2% overhead.,http://research.google.com/pubs/archive/36358.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Taming+Hardware+Event+Samples+for+FDO+Compilation+Chen+Vachharajani+Hundt+Liao+Ramasamy+Yuan+Chen+Zheng,http://research.google.com/pubs/pub36358.html
The Definitive Guide to ImageMagick,Apress Apress Inc. 2560 Ninth St. Ste. 219 Berkeley CA 94710 (2005) pp. 335,2005,Michael Still,@book{27809 title = {The Definitive Guide to ImageMagick} author = {Michael Still} year = 2005 URL = {http://www.imagemagickbook.com} pages = {335} address = {Apress Inc. 2560 Ninth St. Ste. 219 Berkeley CA 94710} },An open source project backed by years of continual development ImageMagick supports over 90 image formats and can perform impressive operations such as creating images from scratch; changing colors; stretching rotating and overlaying images; and overlaying text on images. Whether you use ImageMagick to manage the family photos or to embark on a job involving millions of images this book will provide you with the knowledge to manage your images with ease. The Definitive Guide to ImageMagick explains all of these capabilities and more in a practical learn-by-example fashion. You’ll get comfortable using ImageMagick for any image-processing task. Through the book’s coverage of the ImageMagick interfaces for C Perl PHP and Ruby you’ll learn how to incorporate ImageMagick features into a variety of applications.,http://www.imagemagickbook.com,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Definitive+Guide+to+ImageMagick+Still,http://research.google.com/pubs/pub27809.html
Hunting in the Enterprise: Forensic Triage and Incident Response,Digital Investigation vol. 10 (2013) pp. 89-98,2013,Andreas Moser Michael Cohen,@article{41215 title = {Hunting in the Enterprise: Forensic Triage and Incident Response} author = {Andreas Moser and Michael Cohen} year = 2013 URL = {http://www.sciencedirect.com/science/article/pii/S1742287613000285} journal = {Digital Investigation} pages = {89-98} volume = {10} },In enterprise environments digital forensic analysis generates data volumes that traditional forensic methods are no longer prepared to handle. Triaging has been proposed as a solution to systematically prioritize the acquisition and analysis of digital evidence. We explore the application of automated triaging processes in such settings where reliability and customizability are crucial for a successful deployment. We specifically examine the use of GRR Rapid Response (GRR) – an advanced open source distributed enterprise forensics system – in the triaging stage of common incident response investigations. We show how this system can be leveraged for automated prioritization of evidence across the whole enterprise fleet and describe the implementation details required to obtain sufficient robustness for large scale enterprise deployment. We analyze the performance of the system by simulating several realistic incidents and discuss some of the limitations of distributed agent based systems for enterprise triaging.,http://www.sciencedirect.com/science/article/pii/S1742287613000285,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hunting+in+the+Enterprise:+Forensic+Triage+and+Incident+Response+Moser+Cohen,http://research.google.com/pubs/pub41215.html
Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model,ArXiv Google (2015),2015,Ciprian Chelba Fernando Pereira,@techreport{44280 title = {Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model} author = {Ciprian Chelba and Fernando Pereira} year = 2015 URL = {http://arxiv.org/abs/1511.01574} institution = {Google} },We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark we are able to slightly improve on our previous results which use a different loss function and employ leave-one-out training on a subset of the main training set. Surprisingly an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources that are imbalanced in size and of different degrees of relevance to the held-out and test data taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM.,http://research.google.com/pubs/archive/44280.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multinomial+Loss+on+Held-out+Data+for+the+Sparse+Non-negative+Matrix+Language+Model+Chelba+Pereira,http://research.google.com/pubs/pub44280.html
Swipe vs. scroll: web page switching on mobile browsers,In Proc. of CHI2013 ACM pp. 2171-2174,2013,Andrew Warr Ed H. Chi,@inproceedings{41201 title = {Swipe vs. scroll: web page switching on mobile browsers} author = {Andrew Warr and Ed H. Chi} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2481298} note = {[manuscript error corrected from ACM DL version]} booktitle = {In Proc. of CHI2013} pages = {2171-2174} },Tabbed web browsing interfaces enable users to multi-task and easily switch between open web pages. However tabbed browsing is difficult for mobile web browsers due to the limited screen space and the reduced precision of touch. We present an experiment comparing Safari's pages-based switching interface using horizontal swiping gestures with the stacked cards-based switching interface using vertical scrolling gestures introduced by Chrome. The results of our experiment show that cards-based switching interface allows for faster switching and is less frustrating with no significant effect on error rates. We generalize these findings and provide design implications for mobile information spaces.,http://research.google.com/pubs/archive/41201.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Swipe+vs.+scroll:+web+page+switching+on+mobile+browsers+Warr+Chi,http://research.google.com/pubs/pub41201.html
The interacting effects of distributed work arrangements and individual dispositions on willingness to engage in sensemaking behaviors,Journal of the Association for Information Science and Technology (2015),2015,Peter Gray Brian Butler Nikhil Sharma,@article{43468 title = {The interacting effects of distributed work arrangements and individual dispositions on willingness to engage in sensemaking behaviors} author = {Peter Gray and Brian Butler and Nikhil Sharma} year = 2015 URL = {http://onlinelibrary.wiley.com/doi/10.1002/asi.23306/abstract} journal = {Journal of the Association for Information Science and Technology} },Faced with highly competitive and dynamic environments organizations are increasingly investing in technologies that provide them with new options for structuring work. At the same time firms are increasingly dependent on employees' willingness and ability to make sense of novel tasks problems and rapidly changing situations. Yet in spite of its importance the impact of technology-enabled distributed work arrangements on sensemaking behavior is largely unknown. Sensemaking remains something that is perceived by many to be an idiosyncratic behavior that is at best loosely related to sociotechnical context and culture. Drawing on previous studies of cognitive dispositions (need for cognition tendency for decisiveness intolerance for ambiguity and close-mindedness) and research on how technology-enabled distributed work arrangements affect interpersonal interaction we theorize how workgroup geographic distribution interacts with individual cognitive differences to affect employees' willingness to engage in the core sociocognitive activities of sensemaking. Our results show that the consequences of individual tendencies can vary under different work arrangements suggesting that managers seeking to facilitate sensemaking activities must make careful choices about the composition of distributed work groups as well as how collaboration technologies can be used to encourage sensemaking behaviors.,http://onlinelibrary.wiley.com/doi/10.1002/asi.23306/abstract,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+interacting+effects+of+distributed+work+arrangements+and+individual+dispositions+on+willingness+to+engage+in+sensemaking+behaviors+Gray+Butler+Sharma,http://research.google.com/pubs/pub43468.html
Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,Advances in Neural Information Processing Systems NIPS (2015),2015,Samy Bengio Oriol Vinyals Navdeep Jaitly Noam M. Shazeer,@inproceedings{43984 title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks} author = {Samy Bengio and Oriol Vinyals and Navdeep Jaitly and Noam M. Shazeer} year = 2015 URL = {http://arxiv.org/abs/1506.03099} booktitle = {Advances in Neural Information Processing Systems NIPS} },Recurrent Neural Networks can be trained to produce sequences of tokens given some input as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover it was used successfully in our winning entry to the MSCOCO image captioning challenge 2015.,http://research.google.com/pubs/archive/43984.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scheduled+Sampling+for+Sequence+Prediction+with+Recurrent+Neural+Networks+Bengio+Vinyals+Jaitly+Shazeer,http://research.google.com/pubs/pub43984.html
Online Asynchronous Schema Change in F1,VLDB (2013),2013,Ian Rae Eric Rollins Jeff Shute Sukhdeep Sodhi Radek Vingralek,@inproceedings{41376 title = {Online Asynchronous Schema Change in F1} author = {Ian Rae and Eric Rollins and Jeff Shute and Sukhdeep Sodhi and Radek Vingralek} year = 2013 booktitle = {VLDB} },We introduce a protocol for schema evolution in a globally distributed database management system with shared data stateless servers and no global membership. Our protocol is asynchronous—it allows different servers in the database system to transition to a new schema at different times—and online—all servers can access and update all data during a schema change. We provide a formal model for determining the correctness of schema changes under these conditions and we demonstrate that many common schema changes can cause anomalies and database corruption. We avoid these problems by replacing corruption-causing schema changes with a sequence of schema changes that is guaranteed to avoid corrupting the database so long as all servers are no more than one schema version behind at any time. Finally we discuss a practical implementation of our protocol in F1 the database management system that stores data for Google AdWords.,http://research.google.com/pubs/archive/41376.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Asynchronous+Schema+Change+in+F1+Rae+Rollins+Shute+Sodhi+Vingralek,http://research.google.com/pubs/pub41376.html
A Generic Technique for Synthesizing Bounded Finite-State Controllers,Proceedings of the International Conference on Automated Planning and Scaduling Association for the Advancement of Artiﬁcial Intelligence (2013) pp. 109-116,2013,Yuxiao Hu Giuseppe De Giacomo,@inproceedings{41401 title = {A Generic Technique for Synthesizing Bounded Finite-State Controllers} author = {Yuxiao Hu and Giuseppe De Giacomo} year = 2013 booktitle = {Proceedings of the International Conference on Automated Planning and Scaduling} pages = {109--116} },Finite-state controllers are a compact and effective plan representation for agent widely used in AI. In this paper we proposea generic framework and related solver for synthesizing bounded finite-state controllers and show its instantiations to three different applications including generalized planning planning programs and service composition under partial observability and controllability. We show that our generic solver is sound and complete and amenable to heuristics that take into account the structure of the specific target instantiation. Experiments show that instantiations of our solver to the problems above often outperform tailored approaches in the literature. This suggests that our proposal is a promising base point for future research on finite-state controller synthesis.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Generic+Technique+for+Synthesizing+Bounded+Finite-State+Controllers+Hu+Giacomo,http://research.google.com/pubs/pub41401.html
Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud,Network and Distributed System Security Symposium Internet Society (2014),2014,Arnar Birgisson Joe Gibbs Politz Úlfar Erlingsson Ankur Taly Michael Vrable Mark Lentczner,@inproceedings{41892 title = {Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud} author = {Arnar Birgisson and Joe Gibbs Politz and Úlfar Erlingsson and Ankur Taly and Michael Vrable and Mark Lentczner} year = 2014 booktitle = {Network and Distributed System Security Symposium} },Controlled sharing is fundamental to distributed systems; yet on the Web and in the Cloud sharing is still based on rudimentary mechanisms. More flexible decentralized cryptographic authorization credentials have not been adopted largely because their mechanisms have not been incrementally deployable simple enough or efficient enough to implement across the relevant systems and devices. This paper introduces macaroons: flexible authorization credentials for Cloud services that support decentralized delegation between principals. Macaroons are based on a construction that uses nested chained MACs (e.g. HMACs) in a manner that is highly efficient easy to deploy and widely applicable. Although macaroons are bearer credentials like Web cookies macaroons embed caveats that attenuate and contextually confine when where by who and for what purpose a target service should authorize requests. This paper describes macaroons and motivates their design compares them to other credential systems such as cookies and SPKI/SDSI evaluates and measures a prototype implementation and discusses practical security and application considerations. In particular it is considered how macaroons can enable more fine-grained authorization in the Cloud e.g. by strengthening mechanisms like OAuth2 and a formalization of macaroons is given in authorization logic.,http://research.google.com/pubs/archive/41892.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Macaroons:+Cookies+with+Contextual+Caveats+for+Decentralized+Authorization+in+the+Cloud+Birgisson+Politz+Erlingsson+Taly+Vrable+Lentczner,http://research.google.com/pubs/pub41892.html
Framing Dependencies Introduced by Underground Commoditization,Workshop on the Economics of Information Security (2015),2015,Kurt Thomas Danny Huang David Wang Elie Bursztein Chris Grier Thomas J. Holt Christopher Kruegel Damon McCoy Stefan Savage Giovanni Vigna,@inproceedings{43798 title = {Framing Dependencies Introduced by Underground Commoditization} author = {Kurt Thomas and Danny Huang and David Wang and Elie Bursztein and Chris Grier and Thomas J. Holt and Christopher Kruegel and Damon McCoy and Stefan Savage and Giovanni Vigna} year = 2015 booktitle = {Workshop on the Economics of Information Security} },Internet crime has become increasingly dependent on the underground economy: a loose federation of specialists selling capabilities services and resources explicitly tailored to the abuse ecosystem. Through these emerging markets modern criminal entrepreneurs piece together dozens of à la carte components into entirely new criminal endeavors. From an abuse fighting perspective criminal reliance on this black market introduces fragile dependencies that if disrupted undermine entire operations that as a composite appear intractable to protect against. However without a clear framework for examining the costs and infrastructure behind Internet crime it becomes impossible to evaluate the effectiveness of novel intervention strategies. In this paper we survey a wealth of existing research in order to systematize the community’s understanding of the underground economy. In the process we develop a taxonomy of profit centers and support centers for reasoning about the flow of capital (and thus dependencies) within the black market. Profit centers represent activities that transfer money from victims and institutions into the underground. These activities range from selling products to unwitting customers (in the case of spamvertised products) to outright theft from victims (in case of financial fraud). Support centers provide critical resources that other miscreants request to streamline abuse. These include exploit kits compromised credentials and even human services (e.g. manual CAPTCHA solvers) that have no credible non-criminal applications. We use this framework to contextualize the latest intervention strategies and their effectiveness. In the end we champion a drastic departure from solely focusing on protecting users and systems (tantamount to a fire fight) and argue security practitioners must also strategically disrupt frail underground relationships that underpin the entire for-profit abuse ecosystem--including actors infrastructure and access to capital.,http://research.google.com/pubs/archive/43798.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Framing+Dependencies+Introduced+by+Underground+Commoditization+Thomas+Huang+Wang+Bursztein+Grier+Holt+Kruegel+McCoy+Savage+Vigna,http://research.google.com/pubs/pub43798.html
Cython: The Best of Both Worlds,Computing in Science and Engineering vol. 13.2 (2011) pp. 31-39,2011,Stefan Behnel Robert Bradshaw Craig Citro Lisandro Dalcin Dag Sverre Seljebotn Kurt Smith,@article{36727 title = {Cython: The Best of Both Worlds} author = {Stefan Behnel and Robert Bradshaw and Craig Citro and Lisandro Dalcin and Dag Sverre Seljebotn and Kurt Smith} year = 2011 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/MCSE.2010.118} journal = {Computing in Science and Engineering} pages = {31-39} volume = {13.2} },Cython is an extension to the Python language that allows explicit type declarations and is compiled directly to C. This addresses Python's large overhead for numerical loops and the difficulty of efficiently making use of existing C and Fortran code which Cython code can interact with natively. The Cython language combines the speed of C with the power and simplicity of the Python language.,http://www.computer.org/portal/web/csdl/doi/10.1109/MCSE.2010.118,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cython:+The+Best+of+Both+Worlds+Behnel+Bradshaw+Citro+Dalcin+Seljebotn+Smith,http://research.google.com/pubs/pub36727.html
Can a 2-hour Visit to a Hi-Tech Company Increase Interest in and Change Perceptions about Computer Science?,ACM Inroads vol. 2 Issue 3 (2011),2011,Larisa Eidelman Orit Hazzan Tami Lapidot Yossi Matias Daniela Raijman Michal Segalov,@article{37654 title = {Can a 2-hour Visit to a Hi-Tech Company Increase Interest in and Change Perceptions about Computer Science?} author = {Larisa Eidelman and Orit Hazzan and Tami Lapidot and Yossi Matias and Daniela Raijman and Michal Segalov} year = 2011 journal = {ACM Inroads} volume = {2 Issue 3} },"This paper presents the ""Mind the Gap"" initiative that aims to encourage female high school pupils to study computer science (CS) in high school. This is achieved by increasing their awareness to what CS is and exposing them to the essence of a hi-tech environment and to same gender role models. The initiative was undertaken by female software engineers at Google's Israel R&D Center in collaboration with the Israeli National Center for Computer Science Teachers. We describe the initiative and its impact on the female pupils' interest in CS. One of our conclusions is that even a short visit to a hi-tech company in this case – Google has the potential to change pupils' perception of what CS is and to increase their interest in CS and their desire to study it. Our initiative can be easily adapted by other companies and can be scaled to impact a rather large population.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Can+a+2-hour+Visit+to+a+Hi-Tech+Company+Increase+Interest+in+and+Change+Perceptions+about+Computer+Science%3F+Eidelman+Hazzan+Lapidot+Matias+Raijman+Segalov,http://research.google.com/pubs/pub37654.html
Google's Hybrid Approach to Research,Communications of the ACM vol. 55 Issue 7 (2012) pp. 34-37,2012,Alfred Spector Peter Norvig Slav Petrov,@article{38149 title = {Google's Hybrid Approach to Research} author = {Alfred Spector and Peter Norvig and Slav Petrov} year = 2012 URL = {http://dx.doi.org/10.1145/2209249.2209262} journal = {Communications of the ACM} pages = {34-37} volume = {55 Issue 7} },In this viewpoint we describe how we organize computer science research at Google. We focus on how we integrate research and development and discuss the benefits and risks of our approach. A video about this paper is also available.,http://research.google.com/pubs/archive/38149.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google's+Hybrid+Approach+to+Research+Spector+Norvig+Petrov,http://research.google.com/pubs/pub38149.html
Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network,Sigcomm '15 Google Inc (2015),2015,Arjun Singh Joon Ong Amit Agarwal Glen Anderson Ashby Armistead Roy Bannon Seb Boving Gaurav Desai Bob Felderman Paulie Germano Anand Kanagala Jeff Provost Jason Simmons Eiichi Tanda Jim Wanderer Urs Hölzle Stephen Stuart Amin Vahdat,@inproceedings{43837 title = {Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network} author = {Arjun Singh and Joon Ong and Amit Agarwal and Glen Anderson and Ashby Armistead and Roy Bannon and Seb Boving and Gaurav Desai and Bob Felderman and Paulie Germano and Anand Kanagala and Jeff Provost and Jason Simmons and Eiichi Tanda and Jim Wanderer and Urs Hölzle and Stephen Stuart and Amin Vahdat} year = 2015 booktitle = {Sigcomm '15} },We present our approach for overcoming the cost operational complexity and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second much of the general but complex decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third modular hardware design coupled with simple robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth.,http://research.google.com/pubs/archive/43837.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Jupiter+Rising:+A+Decade+of+Clos+Topologies+and+Centralized+Control+in+Google%E2%80%99s+Datacenter+Network+Singh+Ong+Agarwal+Anderson+Armistead+Bannon+Boving+Desai+Felderman+Germano+Kanagala+Provost+Simmons+Tanda+Wanderer+Holzle+Stuart+Vahdat,http://research.google.com/pubs/pub43837.html
Large Scale Business Discovery from Street Level Imagery,arXiv (2015),2015,Qian Yu Christian Szegedy Martin C. Stumpe Liron Yatziv Vinay Shet Julian Ibarz Sacha Arnoud,@techreport{44629 title = {Large Scale Business Discovery from Street Level Imagery} author = {Qian Yu and Christian Szegedy and Martin C. Stumpe and Liron Yatziv and Vinay Shet and Julian Ibarz and Sacha Arnoud} year = 2015 institution = {arXiv} },Search with local intent is becoming increasingly useful due to the popularity of the mobile device. The creation and maintenance of accurate listings of local businesses worldwide is time consuming and expensive. In this paper we propose an approach to automatically discover businesses that are visible on street level imagery. Precise business store-front detection enables accurate geo-location of businesses and further provides input for business categorization listing generation etc. The large variety of business categories in different countries makes this a very challenging problem. Moreover manual annotation is prohibitive due to the scale of this problem. We propose the use of a MultiBox [4] based approach that takes input image pixels and directly outputs store front bounding boxes. This end-to-end learning approach instead preempts the need for hand modelling either the proposal generation phase or the post-processing phase leveraging large labelled training datasets. We demonstrate our approach outperforms the state of the art detection techniques with a large margin in terms of performance and run-time efficiency. In the evaluation we show this approach achieves human accuracy in the low-recall settings. We also provide an end-to-end evaluation of business discovery in the real world.,http://research.google.com/pubs/archive/44629.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Business+Discovery+from+Street+Level+Imagery+Yu+Szegedy+Stumpe+Yatziv+Shet+Ibarz+Arnoud,http://research.google.com/pubs/pub44629.html
Poster Paper: Automatic Reconfiguration of Distributed Storage,The 12th International Conference on Autonomic Computing IEEE (2015) pp. 133-134,2015,Artyom Sharov Alexander Shraer Arif Merchant Murray Stokely,@inproceedings{44001 title = {Poster Paper: Automatic Reconfiguration of Distributed Storage} author = {Artyom Sharov and Alexander Shraer and Arif Merchant and Murray Stokely} year = 2015 booktitle = {The 12th International Conference on Autonomic Computing} pages = {133--134} },The configuration of a distributed storage system with multiple data replicas typically includes the set of servers and their roles in the replication protocol. The configuration can usually be changed manually but in most cases system administrators have to determine a good configuration by trial and error. We describe a new workload-driven optimization framework that dynamically determines the optimal configuration at run time. Applying the framework to a large-scale distributed storage system used internally in Google resulted in halving the operation latency in 17% of the tested databases and reducing it by more than 90% in some cases.,http://research.google.com/pubs/archive/44001.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Poster+Paper:+Automatic+Reconfiguration+of+Distributed+Storage+Sharov+Shraer+Merchant+Stokely,http://research.google.com/pubs/pub44001.html
Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice,University of Toronto (2012),2012,Ciprian Chelba Johan Schalkwyk Boulos Harb Carolina Parada Cyril Allauzen Leif Johnson Michael Riley Peng Xu Preethi Jyothi Thorsten Brants Vida Ha Will Neveitt,@misc{40380 title = {Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice} author = {Ciprian Chelba and Johan Schalkwyk and Boulos Harb and Carolina Parada and Cyril Allauzen and Leif Johnson and Michael Riley and Peng Xu and Preethi Jyothi and Thorsten Brants and Vida Ha and Will Neveitt} year = 2012 },A critical component of a speech recognition system targeting web search is the language model. The talk presents an empirical exploration of the google.com query stream with the end goal of high quality statistical language modeling for mobile voice search. Our experiments show that after text normalization the query stream is not as ``wild'' as it seems at first sight. One can achieve out-of-vocabulary rates below 1% using a one million word vocabulary and excellent n-gram hit ratios of 77/88% even at high orders such as n=5/4 respectively. Using large scale distributed language models can improve performance significantly---up to 10\% relative reductions in word-error-rate over conventional models used in speech recognition. We also find that the query stream is non-stationary which means that adding more past training data beyond a certain point provides diminishing returns and may even degrade performance slightly. Perhaps less surprisingly we have shown that locale matters significantly for English query data across USA Great Britain and Australia. In an attempt to leverage the speech data in voice search logs we successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance.,http://research.google.com/pubs/archive/40380.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Language+Modeling+for+Automatic+Speech+Recognition+Meets+the+Web:+Google+Search+by+Voice+Chelba+Schalkwyk+Harb+Parada+Allauzen+Johnson+Riley+Xu+Jyothi+Brants+Ha+Neveitt,http://research.google.com/pubs/pub40380.html
Reducing the size of resolution proofs in linear time,International Journal on Software Tools for Technology Transfer vol. 13 (2011),2011,Omer Bar Ilan Oded Fuhrmann Ofer Strichman Ohad Shacham Shlomo Hoory,@article{37039 title = {Reducing the size of resolution proofs in linear time} author = {Omer Bar Ilan and Oded Fuhrmann and Ofer Strichman and Ohad Shacham and Shlomo Hoory} year = 2011 journal = {International Journal on Software Tools for Technology Transfer} volume = {13} },DPLL-based SAT solvers progress by implicitly applying bi- nary resolution. The resolution proofs that they generate are used afterthe SAT solver’s run has terminated for various purposes. Most notable uses in formal verification are: extracting an unsatisfiable core extracting an interpolant and detecting clauses that can be reused in an incremental satisfiability setting (the latter uses the proof only implicitly during the run of the SAT solver). Making the resolution proof smaller can benefit all of these goals: it can lead to smaller cores smaller interpolants and smaller clauses that are propagated to the next SAT instance in an incremental setting. We suggest two methods that are linear in the size of the proof for doing so. Our first technique called Recycle-Units  uses each learned constant (unit clause) (x) for simplifying resolution steps in which x was the pivot prior to when it was learned. Our second technique called Recycle-Pivots simplifies proofs in which there are several nodes in the resolution graph one of which dominates the others that correspond to the same pivot. Our experiments with industrial in- stances show that these simplifications reduce the core by ≈ 5% and the proof by ≈ 13%. It reduces the core less than competing methods such as run-till-fix but whereas our algorithms are linear in the size of the proof the latter and other competing techniques are all exponential as they are based on SAT runs. If we consider the size of the proof (the resolution graph) as being polynomial in the number of variables (it is not necessarily the case in general) this gives our method an exponen- tial time reduction comparing to existing tools for small core extraction. Our experiments show that this result is evident in practice more so for the second method: rarely it takes more than a few seconds even when competing tools time out and hence it can be used as a cheap proof post-processing procedure.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+the+size+of+resolution+proofs+in+linear+time+Ilan+Fuhrmann+Strichman+Shacham+Hoory,http://research.google.com/pubs/pub37039.html
Controlling Complexity in Part-of-Speech Induction,Journal of Artificial Intelligence Research (JAIR) vol. 41 (2011) pp. 527-551,2011,Joao Graca Kuzman Ganchev Luisa Coheur Fernando Pereira Ben Taskar,@article{38280 title = {Controlling Complexity in Part-of-Speech Induction} author = {Joao Graca and Kuzman Ganchev and Luisa Coheur and Fernando Pereira and Ben Taskar} year = 2011 URL = {http://jair.org/papers/paper3348.html} journal = {Journal of Artificial Intelligence Research (JAIR)} pages = {527--551} volume = {41} },We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity adds morphological and orthographic features and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian Danish English Portuguese Spanish) achieve significant improvements compared with previous methods for the same task.,http://research.google.com/pubs/archive/38280.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Controlling+Complexity+in+Part-of-Speech+Induction+Graca+Ganchev+Coheur+Pereira+Taskar,http://research.google.com/pubs/pub38280.html
Budget-Constrained Auctions with Heterogeneous Items,Theory of Computing vol. 8 (2012) pp. 429-460,2012,Sayan Bhattacharya Gagan Goel Sreenivas Gollapudi Kamesh Munagala,@article{38395 title = {Budget-Constrained Auctions with Heterogeneous Items} author = {Sayan Bhattacharya and Gagan Goel and Sreenivas Gollapudi and Kamesh Munagala} year = 2012 URL = {http://theoryofcomputing.org/articles/v008a020/v008a020.pdf} journal = {Theory of Computing} pages = {429-460} volume = {8} },We present the ﬁrst approximation algorithms for designing revenue-optimal incentive-compatible mechanisms in the following setting: There are multiple (heterogeneous) items and bidders have arbitrary demand and budget constraints (and additive valuations). Furthermore the type of a bidder (which speciﬁes her valuations for each item) is private knowledge and the types of different bidders are drawn from publicly known mutually independent distributions. Our mechanisms are surprisingly simple. First we assume that the type of each bidder is drawn from a discrete distribution with polynomially bounded support size. This restriction on the type-distribution however allows the random variables corresponding to a bidder’s valuations for different items to be arbitrarily correlated. In this model we describe a sequential all-pay mechanism that is truthful in expectation and Bayesian incentive compatible. The outcome of our all-pay mechanism can be computed in polynomial time and its revenue is a 4-approximation to the revenue of the optimal truthful-in-expectation Bayesian incentive-compatible mechanism. Next we assume that the valuations of each bidder for different items are drawn from mutually independent discrete distributions satisfying the monotone hazard-rate condition. In this model we present a sequential posted-price mechanism that is universally truthful and incentive compatible in dominant strategies. The outcome of the mechanism is computable in polynomial time and its revenue is a O(1)-approximation to the revenue of the optimal truthful-in-expectation Bayesian incentive-compatible mechanism. If the monotone hazard-rate condition is removed then we show a logarithmic approximation and we complete the picture by proving that no sequential posted-price scheme can achieve a sub-logarithmic approximation. Finally if the distributions are regular and if the space of mechanisms is restricted to sequential posted-price schemes then we show that there is a O(1)-approximation within this space. Our results are based on formulating novel LP relaxations for these problems and developing generic rounding schemes from ﬁrst principles.,http://research.google.com/pubs/archive/38395.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Budget-Constrained+Auctions+with+Heterogeneous+Items+Bhattacharya+Goel+Gollapudi+Munagala,http://research.google.com/pubs/pub38395.html
Perspectives on cloud computing: interviews with five leading scientists from the cloud community,Journal of Internet Services and Applications (2011),2011,Gordon Blair Fabio Kon Walfredo Cirne Dejan Milojicic Raghu Ramakrishnan Dan Reed Dilma Silva,@article{42554 title = {Perspectives on cloud computing: interviews with five leading scientists from the cloud community} author = {Gordon Blair and Fabio Kon and Walfredo Cirne and Dejan Milojicic and Raghu Ramakrishnan and Dan Reed and Dilma Silva} year = 2011 journal = {Journal of Internet Services and Applications} },Cloud computing is currently one of the major topics in dis- tributed systems with large numbers of papers being writ- ten on the topic with major players in the industry releasing a range of software platforms offering novel Internet-based services and most importantly evidence of real impact on end user communities in terms of approaches to provision- ing software services. Cloud computing though is at a for- mative stage with a lot of hype surrounding the area and this makes it difficult to see the true contribution and impact of the topic. Cloud computing is a central topic for the Journal of In- ternet Services and Applications (JISA) and indeed the most downloaded paper from the first year of JISA is concerned with the state-of-the-art and research challenges related to cloud computing [1]. The Editors-in-Chief Fabio Kon and Gordon Blair therefore felt it was timely to seek clarifica- tion on the key issues around cloud computing and hence invited five leading scientists from industrial organizations central to cloud computing to answer a series of questions on the topic. The five scientists taking part are: • Walfredo Cirne from Google’s infrastructure group in California USA • Dejan Milojicic Senior Researcher and Director of the Open Cirrus Cloud Computing testbed at HP Labs • Raghu Ramakrishnan Chief Scientist for Search and Cloud Platforms at Yahoo! • Dan Reed Microsoft’s Corporate Vice President for Tech- nology Strategy and Policy and Extreme Computing • Dilma Silva researcher at the IBM T.J. Watson Research Center in New York,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Perspectives+on+cloud+computing:+interviews+with+five+leading+scientists+from+the+cloud+community+Blair+Kon+Cirne+Milojicic+Ramakrishnan+Reed+Silva,http://research.google.com/pubs/pub42554.html
Hippocratic Abbreviation Expansion,ACL ACL (2014) (to appear),2014,Brian Roark Richard Sproat,@inproceedings{42527 title = {Hippocratic Abbreviation Expansion} author = {Brian Roark and Richard Sproat} year = 2014 booktitle = {ACL} },Incorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis (TTS) or typing auto-correction where the resulting normalization is directly presented to the user versus feeding downstream applications. In this paper we focus on abbreviation expansion for TTS which requires a ``do no harm'' high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded. In the context of a large-scale real-world TTS scenario we present methods for training classifiers to establish whether a particular expansion is apt. We achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system together with a substantial reduction in incorrect expansions.,http://research.google.com/pubs/archive/42527.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hippocratic+Abbreviation+Expansion+Roark+Sproat,http://research.google.com/pubs/pub42527.html
Improved classification through runoff elections,Proceedings of the 9th IAPR International Workshop on Document Analysis Systems ACM Boston (2010) pp. 59-64,2010,Oleg Golubitsky Stephen M. Watt,@inproceedings{36587 title = {Improved classification through runoff elections} author = {Oleg Golubitsky and Stephen M. Watt} year = 2010 URL = {http://doi.acm.org/10.1145/1815330.1815338} booktitle = {Proceedings of the 9th IAPR International Workshop on Document Analysis Systems} pages = {59-64} address = {Boston} },We consider the problem of dealing with irrelevant votes when a multi-case classifier is built from an ensemble of binary classifiers. We show how run-off elections can be used to limit the effects of irrelevant votes and the occasional errors of binary classifiers improving classification accuracy. We consider as a concrete classification problem the recognition of handwritten mathematical characters. A succinct representation of handwritten symbol curves can be obtained by computing truncated Legendre-Sobolev expansions of the coordinate functions. With this representation symbol classes are well linearly separable in low dimension which yields fast classification algorithms based on linear support vector machines. A set of 280 different symbols was considered which gave 1635 classes when different variants are labelled separately. With this number of classes however the effect of irrelevant classifiers becomes significant often causing the correct class to be ranked lower. We introduce a general technique to correct this effect by replacing the conventional majority voting scheme with a runoff election scheme. We have found that such runoff elections further cut the top-1 mis-classification rate by about half.,http://doi.acm.org/10.1145/1815330.1815338,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improved+classification+through+runoff+elections+Golubitsky+Watt,http://research.google.com/pubs/pub36587.html
Large Scale Distributed Acoustic Modeling With Back-off N-grams,ICSI Berkeley California (2013),2013,Ciprian Chelba Peng Xu Fernando Pereira Thomas Richardson,@misc{41133 title = {Large Scale Distributed Acoustic Modeling With Back-off N-grams} author = {Ciprian Chelba and Peng Xu and Fernando Pereira and Thomas Richardson} year = 2013 },Google Voice Search is an application that provides a data-rich setup for both language and acoustic modeling research. The approach we take revives an older approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data and the model size (as measured by the number of parameters in the model) to approximately 100 times larger than current sizes used in automatic speech recognition. Speech recognition experiments are carried out in an N-best list rescoring framework for Google Voice Search. We use 87000 hours of training data (speech along with transcription) obtained by filtering utterances in Voice Search logs on automatic speech recognition confidence. Models ranging in size between 20--40 million Gaussians are estimated using maximum likelihood training. They achieve relative reductions in word-error-rate of 11% and 6% when combined with first-pass models trained using maximum likelihood and boosted maximum mutual information respectively. Increasing the context size beyond five phones (quinphones) does not help.,http://research.google.com/pubs/archive/41133.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Distributed+Acoustic+Modeling+With+Back-off+N-grams+Chelba+Xu+Pereira+Richardson,http://research.google.com/pubs/pub41133.html
Parallel Spectral Clustering,European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD) Springer (2008) pp. 374-389,2008,Yangqiu Song Wen-Yen Chen Hongjie Bai Chih-Jen Lin Edward Chang,@inproceedings{34703 title = {Parallel Spectral Clustering} author = {Yangqiu Song and Wen-Yen Chen and Hongjie Bai and Chih-Jen Lin and Edward Chang} year = 2008 booktitle = {European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)} pages = {374--389} },Spectral clustering algorithm has been shown to be more eective in nding clusters than most traditional algorithms. However spectral clustering suers from a scalability problem in both memory use and computational time when a dataset size is large. To perform clustering on large datasets we propose to parallelize both memory use and computation on distributed computers. Through an empirical study on a large document dataset of 193844 data instances and a large photo dataset of 637137 we demonstrate that our parallel algorithm can effectively alleviate the scalability problem.,http://research.google.com/pubs/archive/34703.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Parallel+Spectral+Clustering+Song+Chen+Bai+Lin+Chang,http://research.google.com/pubs/pub34703.html
Regret Minimization with Concept Drift,Proceedings of the 23rd Annual Conference on Learning Theory (COLT) (2010),2010,Koby Crammer Eyal Even-Dar Yishay Mansour Jennifer Wortman Vaughan,@inproceedings{36486 title = {Regret Minimization with Concept Drift} author = {Koby Crammer and Eyal Even-Dar and Yishay Mansour and Jennifer Wortman Vaughan} year = 2010 booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT)} },In standard online learning the goal of the learner is to maintain an average loss close to the loss of the best-performing function in a fixed class. Classic results show that simple algorithms can achieve an average loss arbitrarily close to that of the best function in retrospect even when input and output pairs are chosen by an adversary. However in many real-world applications such as spam prediction and classification of news articles the best target function may be drifting over time. We introduce a novel model of concept drift in which an adversary is given control of both the distribution over input at each time step and the corresponding labels. The goal of the learner is to maintain an average loss close to the 0/1 loss of the best slowly changing sequence of functions with no more than K large shifts. We provide regret bounds for learning in this model using an (inefficient) reduction to the standard no-regret setting. We then go on to provide and analyze an efficient algorithm for learning d-dimensional hyperplanes with drift. We conclude with some simulations illustrating the circumstances under which this algorithm outperforms other commonly studied algorithms when the target hyperplane is drifting.,http://research.google.com/pubs/archive/36486.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Regret+Minimization+with+Concept+Drift+Crammer+Even-Dar+Mansour+Vaughan,http://research.google.com/pubs/pub36486.html
TIMELY: RTT-based Congestion Control for the Datacenter,Sigcomm '15 Google Inc (2015),2015,Radhika Mittal Terry Lam Nandita Dukkipati Emily Blem Hassan Wassel Monia Ghobadi Amin Vahdat Yaogong Wang David Wetherall David Zats,@inproceedings{43840 title = {TIMELY: RTT-based Congestion Control for the Datacenter} author = {Radhika Mittal and Terry Lam and Nandita Dukkipati and Emily Blem and Hassan Wassel and Monia Ghobadi and Amin Vahdat and Yaogong Wang and David Wetherall and David Zats} year = 2015 booktitle = {Sigcomm '15} },Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay measured as round-trip times at hosts is an effective congestion signal without the need for switch feedback. First we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel reducing tail latency by 13X. To the best of our knowledge TIMELY is the first delay-based congestion control protocol for use in the datacenter and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.,http://research.google.com/pubs/archive/43840.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=TIMELY:+RTT-based+Congestion+Control+for+the++Datacenter+Mittal+Lam+Dukkipati+Blem+Wassel+Ghobadi+Vahdat+Wang+Wetherall+Zats,http://research.google.com/pubs/pub43840.html
MedLDA: Maximum Margin Supervised Topic Models,Journal of Machine Learning Research (2012) (to appear),2012,Jun Zhu Amr Ahmed Eric P. Xing,@article{38352 title = {MedLDA: Maximum Margin Supervised Topic Models} author = {Jun Zhu and Amr Ahmed and Eric P. Xing} year = 2012 URL = {http://www.cs.cmu.edu/~amahmed/papers/MedLDA_jmlr.pdf} journal = {Journal of Machine Learning Research} },A supervised topic model can utilize side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model which integrates the mechanism behind the max-margin prediction models (e.g. SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g. LDA) under a uni- ﬁed constrained optimization framework and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classiﬁcation or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. E_cient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance;,http://research.google.com/pubs/archive/38352.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MedLDA:+Maximum+Margin+Supervised+Topic+Models+Zhu+Ahmed+Xing,http://research.google.com/pubs/pub38352.html
Fast Routing in Very Large Public Transportation Networks Using Transfer Patterns,Algorithms - ESA 2010 18th Annual European Symposium. Proceedings Part I Springer pp. 290-301,2010,Hannah Bast Erik Carlsson Arno Eigenwillig Robert Geisberger Chris Harrelson Veselin Raychev Fabien Viger,@inproceedings{36672 title = {Fast Routing in Very Large Public Transportation Networks Using Transfer Patterns} author = {Hannah Bast and Erik Carlsson and Arno Eigenwillig and Robert Geisberger and Chris Harrelson and Veselin Raychev and Fabien Viger} year = 2010 URL = {http://dx.doi.org/10.1007/978-3-642-15775-2_25} booktitle = {Algorithms - ESA 2010 18th Annual European Symposium. Proceedings Part I} pages = {290--301} },We show how to route on very large public transportation networks (up to half a billion arcs) with average query times of a few milliseconds. We take into account many realistic features like: traffic days walking between stations queries between geographic locations instead of a source and a target station and multi-criteria cost functions. Our algorithm is based on two key observations: (1) many shortest paths share the same transfer pattern i.e. the sequence of stations where a change of vehicle occurs; (2) direct connections without change of vehicle can be looked up quickly. We precompute the respective data; in practice this can be done in time linear in the network size at the expense of a small fraction of non-optimal results. We have accelerated public transportation routing on Google Maps with a system based on our ideas. We report experimental results for three data sets of various kinds and sizes.,http://dx.doi.org/10.1007/978-3-642-15775-2_25,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Routing+in+Very+Large+Public+Transportation+Networks+Using+Transfer+Patterns+Bast+Carlsson+Eigenwillig+Geisberger+Harrelson+Raychev+Viger,http://research.google.com/pubs/pub36672.html
Study on Interaction between Entropy Pruning and Kneser-Ney Smoothing,Proceedings of Interspeech (2010) pp. 2242-2245,2010,Ciprian Chelba Thorsten Brants Will Neveitt Peng Xu,@inproceedings{36472 title = {Study on Interaction between Entropy Pruning and Kneser-Ney Smoothing} author = {Ciprian Chelba and Thorsten Brants and Will Neveitt and Peng Xu} year = 2010 booktitle = {Proceedings of Interspeech} pages = {2242--2245} },The paper presents an in-depth analysis of a less known interaction between Kneser-Ney smoothing and entropy pruning that leads to severe degradation in language model performance under aggressive pruning regimes. Experiments in a data-rich setup such as google.com voice search show a significant impact in WER as well: pruning Kneser-Ney and Katz models to 0.1% of their original impacts speech recognition accuracy significantly approx. 10% relative. Any third party with LDC membership should be able to reproduce our experiments using the scripts available at http://code.google.com/p/kneser-ney-pruning-experiments.,http://research.google.com/pubs/archive/36472.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Study+on+Interaction+between+Entropy+Pruning+and+Kneser-Ney+Smoothing+Chelba+Brants+Neveitt+Xu,http://research.google.com/pubs/pub36472.html
The End is Nigh: Generic Solving of Text-based CAPTCHAs,WOOT'14 Proceedings of the 8th USENIX conference on Offensive Technologies Usenix (2014),2014,Elie Bursztein Jonathan Aigrain Angelika Moscicki John C. Mitchell,@inproceedings{43464 title = {The End is Nigh: Generic Solving of Text-based CAPTCHAs} author = {Elie Bursztein and Jonathan Aigrain and Angelika Moscicki and John C. Mitchell} year = 2014 URL = {https://www.elie.net/publication/the-end-is-nigh-generic-solving-of-text-based-captchas} booktitle = {WOOT'14 Proceedings of the 8th USENIX conference on Offensive Technologies} },Over the last decade it has become well-established that a captcha’s ability to withstand automated solving lies in the difficulty of segmenting the image into individual characters. The standard approach to solving captchas automatically has been a sequential process wherein a segmentation algorithm splits the image into segments that contain individual characters followed by a character recognition step that uses machine learning. While this approach has been effective against particular captcha schemes its generality is limited by the segmentation step which is hand-crafted to defeat the distortion at hand. No general algorithm is known for the character collapsing anti-segmentation technique used by most prominent real world captcha schemes. This paper introduces a novel approach to solving captchas in a single step that uses machine learning to attack the segmentation and the recognition problems simultaneously. Performing both operations jointly allows our algorithm to exploit information and context that is not available when they are done sequentially. At the same time it removes the need for any hand-crafted component making our approach generalize to new captcha schemes where the previous approach can not. We were able to solve all the real world captcha schemes we evaluated ac- curately enough to consider the scheme insecure in practice including Yahoo (5.33%) and ReCaptcha (33.34%) without any adjustments to the algorithm or its parameters. Our success against the Baidu (38.68%) and CNN (51.09%) schemes that use occluding lines as well as character collapsing leads us to believe that our approach is able to defeat occluding lines in an equally general manner. The effectiveness and universality of our results suggests that combining segmentation and recognition is the next evolution of captcha solving and that it supersedes the sequential approach used in earlier works. More generally our approach raises questions about how to develop sufficiently secure captchas in the future.,http://research.google.com/pubs/archive/43464.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+End+is+Nigh:+Generic+Solving+of+Text-based+CAPTCHAs+Bursztein+Aigrain+Moscicki+Mitchell,http://research.google.com/pubs/pub43464.html
Using the Wave Protocol to Represent Individuals’ Health Records,HealthSec '10 Position Paper (2010),2010,Shirley Gaw Umesh Shankar,@inproceedings{36501 title = {Using the Wave Protocol to Represent Individuals’ Health Records} author = {Shirley Gaw and Umesh Shankar} year = 2010 booktitle = {HealthSec '10 Position Paper} },There are several challenges in aggregating health records from multiple sources including merging data preserving proper attribution and allowing corrections. unfortunately standards for exchanging medical records data such as CCR and CCD tend to focus on representing particular clinical data as some subset of a patient’s complete record. This provides a snapshot of a patient record but there is very little to describe how a sequence of changes to the record should be interpreted as a coherent whole.there is something available that gives us the data aggregation conflict resolution and audit trail that what we want: the Wave federation protocol.,http://research.google.com/pubs/archive/36501.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+the+Wave+Protocol+to+Represent+Individuals%E2%80%99+Health+Records+Gaw+Shankar,http://research.google.com/pubs/pub36501.html
Security Challenges During VLSI Test,Proceedings of 2011 IEEE NEWCAS Conference IEEE,2011,Kurt Rosenfeld,@inproceedings{37396 title = {Security Challenges During VLSI Test} author = {Kurt Rosenfeld} year = 2011 booktitle = {Proceedings of 2011 IEEE NEWCAS Conference} },VLSI testing is a practical requirement but unless proper care is taken features that enhance testability can reduce system security. Data confidentiality and intellectual property protection can be breached through testing security breaches. In this paper we review testing security problems focusing on the scan technique. We then present some countermeasures which have recently been published and we discuss their characteristics.,http://research.google.com/pubs/archive/37396.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Security+Challenges+During+VLSI+Test+Rosenfeld,http://research.google.com/pubs/pub37396.html
Privacy protection and face recognition,Handbook of Face recognition Springer 236 Gray's Inn Road | Floor 6 London | WC1X 8HL | UK (2011) pp. 671-692,2011,Andrew Senior Sharat Pankanti,@inbook{36368 title = {Privacy protection and face recognition} author = {Andrew Senior and Sharat Pankanti} year = 2011 booktitle = {Handbook of Face recognition} pages = {671--692} address = {236 Gray's Inn Road | Floor 6 London | WC1X 8HL | UK} },Invited chapter in second edition of Handbook of Face recognition ed Stan Li & Anil K. Jain. Covers privacy protecting technologies applied to face detection and recognition.,http://research.google.com/pubs/archive/36368.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Privacy+protection+and+face+recognition+Senior+Pankanti,http://research.google.com/pubs/pub36368.html
Web-Scale Multi-Task Feature Selection for Behavioral Targeting,Proceedings of The 21st ACM International Conference on Information and Knowledge Management (CIKM) ACM (2012) (to appear),2012,Amr Ahmed Mohamed Aly Abhimanyu Das Alex Smola Tasos Anastasakos,@inproceedings{38345 title = {Web-Scale Multi-Task Feature Selection for Behavioral Targeting} author = {Amr Ahmed and Mohamed Aly and Abhimanyu Das and Alex Smola and Tasos Anastasakos} year = 2012 URL = {http://www.cs.cmu.edu/~amahmed/papers/MT_feature.pdf} booktitle = {Proceedings of The 21st ACM International Conference on Information and Knowledge Management (CIKM)} },A typical behavioral targeting system optimizing purchase activities called conversions faces two main challenges: the web-scale amounts of user histories to process on a daily basis and the relative sparsity of conversions. In this paper we try to address these challenges through feature selection. We formulate a multi-task (or group) feature-selection problem among a set of related tasks (sharing a common set of features) namely advertising campaigns. We apply a group-sparse penalty consisting of a combination of an `1 and `2 penalty and an associated fast optimization algorithm for distributed parameter estimation. Our algorithm relies on a variant of the well known Fast Iterative Thresholding Algorithm (FISTA) a closed-form solution for mixed norm programming and a distributed subgradient oracle. To eciently handle web-scale user histories we present a distributed inference algorithm for the problem that scales to billions of instances and millions of attributes. We show the superiority of our algorithm in terms of both sparsity and ROC performance over baseline feature selection methods (both single-task L1-regularization and multi-task mutual-information gain).,http://research.google.com/pubs/archive/38345.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web-Scale+Multi-Task+Feature+Selection+for+Behavioral+Targeting+Ahmed+Aly+Das+Smola+Anastasakos,http://research.google.com/pubs/pub38345.html
Fast Covariance Computation and Dimensionality Reduction for Sub-Window Features in Images,European Conference on Computer Vision (ECCV 2010),2010,Vivek Kwatra Mei Han,@inproceedings{36416 title = {Fast Covariance Computation and Dimensionality Reduction for Sub-Window Features in Images} author = {Vivek Kwatra and Mei Han} year = 2010 URL = {http://cs.unc.edu/~kwatra/publications.html#fastcovariance} booktitle = {European Conference on Computer Vision (ECCV 2010)} },This paper presents algorithms for efficiently computing the covariance matrix for features that form sub-windows in a large multi-dimensional image. For example several image processing applications e.g. texture analysis/synthesis image retrieval and compression operate upon patches within an image. These patches are usually projected onto a low-dimensional feature space using dimensionality reduction techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) which in-turn requires computation of the covariance matrix from a set of features. Covariance computation is usually the bottleneck during PCA or LDA (O(nd^2) where n is the number of pixels in the image and d is the dimensionality of the vector). Our approach reduces the complexity of covariance computation by exploiting the redundancy between feature vectors corresponding to overlapping patches. Specifically we show that the covariance between two feature components can be reduced to a function of the relative displacement between those components in patch space. One can then employ a lookup table to store covariance values by relative displacement. By operating in the frequency domain this lookup table can be computed in O(n log n) time. We allow the patches to sub-sample the image which is useful for hierarchical processing and also enables working with filtered responses over these patches such as local gist features. We also propose a method for fast projection of sub-window patches onto the low-dimensional space.,http://research.google.com/pubs/archive/36416.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Covariance+Computation+and+Dimensionality+Reduction+for+Sub-Window+Features+in+Images+Kwatra+Han,http://research.google.com/pubs/pub36416.html
On the Impact of Kernel Approximation on Learning Accuracy,Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010),2010,Corinna Cortes Mehryar Mohri Ameet Talwalkar,@inproceedings{36469 title = {On the Impact of Kernel Approximation on Learning Accuracy} author = {Corinna Cortes and Mehryar Mohri and Ameet Talwalkar} year = 2010 URL = {http://www.cs.nyu.edu/~mohri/pub/kst.pdf} booktitle = {Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)} },Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms including SVMs KRR and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However we also give a specific analysis of the Nystr¨om low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystr¨om low-rank kernel approximation when used with ridge regression.,http://research.google.com/pubs/archive/36469.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+Impact+of+Kernel+Approximation+on+Learning+Accuracy+Cortes+Mohri+Talwalkar,http://research.google.com/pubs/pub36469.html
Cicada: Predictive Guarantees for Cloud Network Bandwidth,MIT (2014) MIT-CSAIL-TR-2014-004,2014,Katrina LaCurts Jeffrey C Mogul Hari Balakrishnan Yoshio Turner,@techreport{42462 title = {Cicada: Predictive Guarantees for Cloud Network Bandwidth} author = {Katrina LaCurts and Jeffrey C Mogul and Hari Balakrishnan and Yoshio Turner} year = 2014 URL = {http://dspace.mit.edu/handle/1721.1/85975} note = {MIT-CSAIL-TR-2014-004} institution = {MIT} },"In cloud-computing systems network-bandwidth guarantees have been shown to improve predictability of application performance and cost. Most previous work on cloud-bandwidth guarantees has assumed that cloud tenants know what bandwidth guarantees they want. However application bandwidth demands can be complex and time-varying and many tenants might lack sufficient information to request a bandwidth guarantee that is well-matched to their needs. A tenant's lack of accurate knowledge about its future bandwidth demands can lead to over-provisioning (and thus reduced cost-efficiency) or under-provisioning (and thus poor user experience in latency-sensitive user-facing applications). We analyze traffic traces gathered over six months from an HP Cloud Services datacenter finding that application bandwidth consumption is both time-varying and spatially inhomogeneous. This variability makes it hard to predict requirements. To solve this problem we develop a prediction algorithm usable by a cloud provider to suggest an appropriate bandwidth guarantee to a tenant. The key idea in the prediction algorithm is to treat a set of previously observed traffic matrices as ""experts"" and learn online the best weighted linear combination of these experts to make its prediction. With tenant VM placement using these predictive guarantees we find that the inter-rack network utilization in certain datacenter topologies can be more than doubled.",http://research.google.com/pubs/archive/42462.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cicada:+Predictive+Guarantees+for+Cloud+Network+Bandwidth+LaCurts+Mogul+Balakrishnan+Turner,http://research.google.com/pubs/pub42462.html
Improving Book OCR by Adaptive Language and Image Models,Proceedings of 2012 10th IAPR International Workshop on Document Analysis Systems IEEE pp. 115-119,2012,Dar-Shyang Lee Ray Smith,@inproceedings{37481 title = {Improving Book OCR by Adaptive Language and Image Models} author = {Dar-Shyang Lee and Ray Smith} year = 2012 booktitle = {Proceedings of 2012 10th IAPR International Workshop on Document Analysis Systems} pages = {115-119} },In order to cope with the vast diversity of book content and typefaces it is important for OCR systems to leverage the strong consistency within a book but adapt to variations across books. In this work we describe a system that combines two parallel correction paths using document-specific image and language models. Each model adapts to shapes and vocabularies within a book to identify inconsistencies as correction hypotheses but relies on the other for effective cross-validation. Using the open source Tesseract engine as baseline results on a large dataset of scanned books demonstrate that word error rates can be reduced by 25% using this approach.,http://research.google.com/pubs/archive/37481.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+Book+OCR+by+Adaptive+Language+and+Image+Models+Lee+Smith,http://research.google.com/pubs/pub37481.html
Federated Learning of Deep Networks using Model Averaging,Preprint (2016),2016,H. Brendan McMahan Eider Moore Daniel Ramage Blaise Aguera y Arcas,@misc{44822 title = {Federated Learning of Deep Networks using Model Averaging} author = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Blaise Aguera y Arcas} year = 2016 URL = {http://arxiv.org/abs/1602.05629} },Modern mobile devices have access to a wealth of data suitable for learning models which in turn can greatly improve the user experience on the device. For example language models can improve speech recognition and text entry and image models can automatically select good photos. However this rich data is often privacy sensitive large in quantity or both which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize parameter averaging over updates from multiple clients produces surprisingly good results for example decreasing the communication needed to train an LSTM language model by two orders of magnitude.,http://arxiv.org/abs/1602.05629,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Federated+Learning+of+Deep+Networks+using+Model+Averaging+McMahan+Moore+Ramage+Arcas,http://research.google.com/pubs/pub44822.html
Usage Patterns in an Urban WiFi Network,IEEE/ACM Transactions on Networking vol. 18  Issue: 5 (2010) 1359 - 1372,2010,Mikhail Afanasyev Tsuwei Chen,@article{39965 title = {Usage Patterns in an Urban WiFi Network} author = {Mikhail Afanasyev and Tsuwei Chen} year = 2010 journal = {IEEE/ACM Transactions on Networking} pages = {1359 - 1372} volume = {18  Issue: 5} },While WiFi was initially designed as a local-area access network mesh networking technologies have led to increasingly expansive deployments of WiFi networks. In urban environments the WiFi mesh frequently supplements a number of existing access technologies including wired broadband networks 3G cellular and commercial WiFi hotspots. It is an open question what role citywide WiFi deployments play in the increasingly diverse access network spectrum. We study the usage of the Google WiFi network deployed in Mountain View CA and find that usage naturally falls into three classes based almost entirely on client device type which we divide into traditional laptop users fixed-location access devices and PDA-like smartphone devices. Moreover each of these classes of use has significant geographic locality following the distribution of residential commercial and transportation areas of the city. When comparing the network usage of each device class we find a diverse set of mobility patterns that map well to the archetypal use cases for traditional access technologies. To help place our results in context we also provide key performance measurements of the mesh backbone and where possible compare them to those of previously studied urban mesh networks.,http://research.google.com/pubs/archive/39965.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Usage+Patterns+in+an+Urban+WiFi+Network+Afanasyev+Chen,http://research.google.com/pubs/pub39965.html
Cloud-based robot grasping with the google object recognition engine,IEEE Int’l Conf. on Robotics and Automation (2013) pp. 8,2013,Ben Kehoe Akihiro Matsukawa Sal Candido James Kuffner Ken Goldberg,@inproceedings{41434 title = {Cloud-based robot grasping with the google object recognition engine} author = {Ben Kehoe and Akihiro Matsukawa and Sal Candido and James Kuffner and Ken Goldberg} year = 2013 booktitle = {IEEE Int’l Conf. on Robotics and Automation} pages = {8} },"Rapidly expanding internet resources and wireless networking have potential to liberate robots and automation systems from limited onboard computation memory and software. ""Cloud Robotics"" describes an approach that recognizes the wide availability of networking and incorporates opensource elements to greatly extend earlier concepts of ""Online Robots"" and ""Networked Robots"". In this paper we consider how cloud-based data and computation can facilitate 3D robot grasping. We present a system architecture implemented prototype and initial experimental data for a cloud-based robot grasping system that incorporates a Willow Garage PR2 robot with onboard color and depth cameras Google’s proprietary object recognition engine the Point Cloud Library (PCL) for pose estimation Columbia University’s GraspIt! toolkit and OpenRAVE for 3D grasping and our prior approach to sampling-based grasp analysis to address uncertainty in pose. We report data from experiments in recognition (a recall rate of 80% for the objects in our test set) pose estimation (failure rate under 14%) and grasping (failure rate under 23%) and initial results on recall and false positives in larger data sets using conﬁdence measures.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cloud-based+robot+grasping+with+the+google+object+recognition+engine+Kehoe+Matsukawa+Candido+Kuffner+Goldberg,http://research.google.com/pubs/pub41434.html
Joint consideration of energy-efficiency and coverage-preservation in microsensor networks,Wireless Communications & Mobile Computing vol. 11 (2011) pp. 707-722,2011,Navid Amini Alireza Vahdatpour Foad Dabiri Hyduke Noshadi Majid Sarrafzadeh,@article{41349 title = {Joint consideration of energy-efficiency and coverage-preservation in microsensor networks} author = {Navid Amini and Alireza Vahdatpour and Foad Dabiri and Hyduke Noshadi and Majid Sarrafzadeh} year = 2011 URL = {http://dl.acm.org/citation.cfm?id=2000685} journal = {Wireless Communications & Mobile Computing} pages = {707-722} volume = {11} },This paper presents an energy-efficient and coverage-preserving communication protocol which distributes a uniform energy load to the sensors in a wireless microsensor network. This protocol called Distance-based Segmentation (DBS) is a cluster-based protocol that divides the entire network into equal-area segments and applies different clustering policies to each segment to (1) reduce total energy dissipation and (2) balance the energy load among the sensors. Therefore it prolongs the lifetime of the network and improves the sensing coverage. Moreover the proposed routing protocol does not need any centralized support from a certain node which is at odds with aiming to establish a scalable communication protocol. Results from extensive simulations on two different network configurations show that by lowering the number of wasteful transmissions in the network the DBS can achieve as much as a 20% reduction in total dissipated energy as compared with current cluster-based protocols. In addition this protocol is able to distribute energy load more evenly among the sensors in the network. Hence it yields up to a 66% increase in the useful network lifetime. According to the simulation results the sensing coverage degradation of the DBS is considerably slower than that of the other cluster-based protocols.,http://dl.acm.org/citation.cfm?id=2000685,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Joint+consideration+of+energy-efficiency+and+coverage-preservation+in+microsensor+networks+Amini+Vahdatpour+Dabiri+Noshadi+Sarrafzadeh,http://research.google.com/pubs/pub41349.html
Dialing Back Abuse on Phone Verified Accounts,Proceedings of the 21st ACM Conference on Computer and Communications Security (2014),2014,Kurt Thomas Dmytro Iatskiv Elie Bursztein Tadek Pietraszek Chris Grier Damon McCoy,@inproceedings{43134 title = {Dialing Back Abuse on Phone Verified Accounts} author = {Kurt Thomas and Dmytro Iatskiv and Elie Bursztein and Tadek Pietraszek and Chris Grier and Damon McCoy} year = 2014 booktitle = {Proceedings of the 21st ACM Conference on Computer and Communications Security} },In the past decade the increase of for-profit cybercrime has given rise to an entire underground ecosystem supporting large-scale abuse a facet of which encompasses the bulk registration of fraudulent accounts. In this paper we present a 10 month longitudinal study of the underlying technical and financial capabilities of criminals who register phone verified accounts (PVA). To carry out our study we purchase 4695 Google PVA as well as acquire a random sample of 300000 Google PVA through a collaboration with Google. We find that miscreants rampantly abuse free VOIP services to circumvent the intended cost of acquiring phone numbers in effect undermining phone verification. Combined with short lived phone numbers from India and Indonesia that we suspect are tied to human verification farms this confluence of factors correlates with a market-wide price drop of 30--40% for Google PVA until Google penalized verifications from frequently abused carriers. We distill our findings into a set of recommendations for any services performing phone verification as well as highlight open challenges related to PVA abuse moving forward.,http://research.google.com/pubs/archive/43134.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dialing+Back+Abuse+on+Phone+Verified+Accounts+Thomas+Iatskiv+Bursztein+Pietraszek+Grier+McCoy,http://research.google.com/pubs/pub43134.html
Efficient Topologies for Large-Scale Cluster Networks,2010 Conference on OFC/NFOEC IEEE pp. 1-3,2010,John Kim William J. Dally Dennis Abts,@inproceedings{36740 title = {Efficient Topologies for Large-Scale Cluster Networks} author = {John Kim and William J. Dally and Dennis Abts} year = 2010 booktitle = {2010 Conference on OFC/NFOEC} pages = {1--3} },Increasing integrated-circuit pin bandwidth has motivated a corresponding increase in the degree or radix of interconnection networks and their routers. This paper describes the flattened butterfly a cost-efficient topology for high-radix networks. On benign (load-balanced) traffic the flattened butterfly approaches the cost/performance of a butterfly network and has roughly half the cost of a comparable performance Clos network. The advantage over the Clos is achieved by eliminating redundant hops when they are not needed for load balance. On adversarial traffic the flattened butterfly matches the cost/performance of a folded-Clos network and provides an order of magnitude better performance than a conventional butterfly. In this case global adaptive routing is used to switch the flattened butterfly from minimal to non-minimal routing — using redundant hops only when they are needed. Different routing algorithms are evaluated on the flattened butterfly and compared against alternative topologies. We also provide a detailed cost model for an interconnection network and compare the cost of the flattened butterfly to alternative topologies to show the cost advantages of the flattened butterfly.,http://research.google.com/pubs/archive/36740.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Topologies+for+Large-Scale+Cluster+Networks+Kim+Dally+Abts,http://research.google.com/pubs/pub36740.html
Milgram-routing in social networks.,Proceedings of the 20th International Conference on World Wide Web WWW 2011 pp. 725-734,2011,Silvio Lattanzi Alessandro Panconesi D. Sivakumar,@inproceedings{37235 title = {Milgram-routing in social networks.} author = {Silvio Lattanzi and Alessandro Panconesi and D. Sivakumar} year = 2011 booktitle = {Proceedings of the 20th International Conference on World Wide Web WWW 2011} pages = {725-734} },We demonstrate how a recent model of social networks (“Affiliation Networks”) offers powerful cues in local routing within social networks a theme made famous by sociologist Milgram’s “six degrees of separation” experiments. This model posits the existence of an “interest space” that underlies a social network; we prove that in networks produced by this model not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively. Specifically we show that local routing can discover paths of length O(log^2 n) to targets chosen uniformly at random and paths of length O(1) to targets chosen with probability proportional to their degrees. Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results and shed light into the power of one step of lookahead in routing algorithms for social networks.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Milgram-routing+in+social+networks.+Lattanzi+Panconesi+Sivakumar,http://research.google.com/pubs/pub37235.html
Compression Progress Pseudorandomness & Hyperbolic Discounting,The Third Conference on Artificial General Intelligence Atlantis Press http://www.atlantis-press.com (2010) pp. 186-187,2010,Moshe Looks,@inproceedings{36293 title = {Compression Progress Pseudorandomness & Hyperbolic Discounting} author = {Moshe Looks} year = 2010 URL = {http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_64.pdf} booktitle = {The Third Conference on Artificial General Intelligence} pages = {186-187} address = {http://www.atlantis-press.com} },"General intelligence requires open-ended exploratory learning. The principle of compression progress proposes that agents should derive intrinsic reward from maximizing ""interestingness"" the first derivative of compression progress over the agent's history. Schmidhuber posits that such a drive can explain ""essential aspects of ... curiosity creativity art science music [and] jokes"" implying that such phenomena might be replicated in an artificial general intelligence programmed with such a drive. I pose two caveats: 1) as pointed out by Rayhawk not everything that can be considered ""interesting"" according to this definition is interesting to humans; 2) because of (irrational) hyperbolic discounting of future rewards humans have an additional preference for rewards that are structured to prevent premature satiation often superseding intrinsic preferences for compression progress.",http://research.google.com/pubs/archive/36293.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Compression+Progress+Pseudorandomness+%26+Hyperbolic+Discounting+Looks,http://research.google.com/pubs/pub36293.html
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing Association for Computational Linguistics pp. 620-629,2008,Roy Tromble Shankar Kumar Franz Och Wolfgang Macherey,@inproceedings{34628 title = {Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation} author = {Roy Tromble and Shankar Kumar and Franz Och and Wolfgang Macherey} year = 2008 URL = {http://aclweb.org/anthology-new/D/D08/D08-1065.pdf} booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing} pages = {620--629} },We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score~\cite{papineni01} that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate consistent gains in translation performance over N-best MBR decoding on Arabic-to-English Chinese-to-English and English-to-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and also study the impact of various parameters on MBR performance.,http://research.google.com/pubs/archive/34628.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Lattice+Minimum+Bayes-Risk+Decoding+for+Statistical+Machine+Translation+Tromble+Kumar+Och+Macherey,http://research.google.com/pubs/pub34628.html
Recursion in Scalable Protocols via Distributed Data Flows,Languages for Distributed Algorithms (2012) (to appear),2012,Krzysztof Ostrowski,@inproceedings{37478 title = {Recursion in Scalable Protocols via Distributed Data Flows} author = {Krzysztof Ostrowski} year = 2012 booktitle = {Languages for Distributed Algorithms} },This paper proposes a new approach to representing scalable hierarchical distributed multi-party protocols and reasoning about their behavior. The established endpoint-to-endpoint message-passing abstraction provides little support for modeling distributed algorithms in hierarchical systems in which the hierarchy and membership dynamically evolve. This paper explains how with our new Distributed Data Flow (DDF) abstraction hierarchical architecture can be modeled via recursion in the language. This facilitates a more concise code and it enables automated generation of scalable hierarchical implementations for heterogeneous network environments.,http://research.google.com/pubs/archive/37478.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recursion+in+Scalable+Protocols+via+Distributed+Data+Flows+Ostrowski,http://research.google.com/pubs/pub37478.html
Optimizing Budget Constrained Spend in Search Advertising,Sixth ACM International Conference on Web Search and Data Mining WSDM 2013 ACM pp. 697-706,2013,Chinmay Karande Aranyak Mehta Ramakrishnan Srikant,@inproceedings{40804 title = {Optimizing Budget Constrained Spend in Search Advertising} author = {Chinmay Karande and Aranyak Mehta and Ramakrishnan Srikant} year = 2013 booktitle = {Sixth ACM International Conference on Web Search and Data Mining WSDM 2013} pages = {697--706} },Search engine ad auctions typically have a significant fraction of advertisers who are budget constrained i.e. if allowed to participate in every auction that they bid on they would spend more than their budget. This yields an important problem: selecting the ad auctions in which these advertisers participate in order to optimize different system objectives such as the return on investment for advertisers and the quality of ads shown to users. We present a system and algorithms for optimizing such budget constrained spend. The system is designed be deployed in a large search engine with hundreds of thousands of advertisers millions of searches per hour and with the query stream being only partially predictable. We have validated the system design by implementing it in the Google ads serving system and running experiments on live traffic. We have also compared our algorithm to previous work that casts this problem as a large linear programming problem limited to popular queries and show that our algorithms yield substantially better results.,http://research.google.com/pubs/archive/40804.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+Budget+Constrained+Spend+in+Search+Advertising+Karande+Mehta+Srikant,http://research.google.com/pubs/pub40804.html
Strato: A Retargetable Framework for Low-level Inlined Reference Monitors,Proceedings of the 22nd USENIX Conference on Security USENIX Association Berkeley CA USA (2013) pp. 369-382,2013,Bin Zeng Gang Tan Úlfar Erlingsson,@inproceedings{41882 title = {Strato: A Retargetable Framework for Low-level Inlined Reference Monitors} author = {Bin Zeng and Gang Tan and Úlfar Erlingsson} year = 2013 booktitle = {Proceedings of the 22nd USENIX Conference on Security} pages = {369--382} address = {Berkeley CA USA} },Low-level Inlined Reference Monitors (IRM) such as control-flow integrity and software-based fault isolation can foil numerous software attacks. Conventionally those IRMs are implemented through binary rewriting or transformation on equivalent low-level programs that are tightly coupled with a specific Instruction Set Architecture (ISA). Resulting implementations have poor retargetability to different ISAs. This paper introduces an IRM-implementation framework at a compiler intermediate-representation (IR) level. The IR-level framework enables easy retargetability to different ISAs but raises the challenge of how to preserve security at the low level as the compiler backend might invalidate the assumptions at the IR level. We propose a constraint language to encode the assumptions and check whether they still hold after the backend transformations and optimizations. Furthermore an independent verifier is implemented to validate the security of low-level code. We have implemented the framework inside LLVM to enforce the policy of control-flow integrity and data sandboxing for both reads and writes. Experimental results demonstrate that it incurs modest runtime overhead of 19.90% and 25.34% on SPECint2000 programs for _86- 32 and _86-64 respectively.,http://research.google.com/pubs/archive/41882.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Strato:+A+Retargetable+Framework+for+Low-level+Inlined+Reference+Monitors+Zeng+Tan+Erlingsson,http://research.google.com/pubs/pub41882.html
Knowledge Base Completion via Search-Based Question Answering,WWW (2014),2014,Robert West Evgeniy Gabrilovich Kevin Murphy Shaohua Sun Rahul Gupta Dekang Lin,@inproceedings{42024 title = {Knowledge Base Completion via Search-Based Question Answering} author = {Robert West and Evgeniy Gabrilovich and Kevin Murphy and Shaohua Sun and Rahul Gupta and Dekang Lin} year = 2014 URL = {http://www.cs.ubc.ca/~murphyk/Papers/www14.pdf} booktitle = {WWW} },"Over the past few years massive amounts of world knowledge have been accumulated in publicly available knowledge bases such as Freebase NELL and YAGO. Yet despite their seemingly huge size these knowledge bases are greatly incomplete. For example over 70% of people included in Freebase have no known place of birth and 99% have no known ethnicity. In this paper we propose a way to leverage existing Web-search--based question-answering technology to fill in the gaps in knowledge bases in a targeted way. In particular for each entity attribute we learn the best set of queries to ask such that the answer snippets returned by the search engine are most likely to contain the correct value for that attribute. For example if we want to find Frank Zappa's mother we could ask the query ""who is the mother of Frank Zappa"". However this is likely to return ""The Mothers of Invention"" which was the name of his band. Our system learns that it should (in this case) add disambiguating terms such as Zappa's place of birth in order to make it more likely that the search results contain snippets mentioning his mother. Our system also learns how many different queries to ask for each attribute since in some cases asking too many can hurt accuracy (by introducing false positives). We discuss how to aggregate candidate answers across multiple queries ultimately returning probabilistic predictions for possible values for each attribute. Finally we evaluate our system and show that it is able to extract a large number of facts with high confidence.",http://research.google.com/pubs/archive/42024.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Knowledge+Base+Completion+via+Search-Based+Question+Answering+West+Gabrilovich+Murphy+Sun+Gupta+Lin,http://research.google.com/pubs/pub42024.html
Please Permit Me: Stateless Delegated Authorization in Mashups,Proceedings of the Annual Computer Security Applications Conference IEEE Press Anaheim CA (2008) pp. 173-182,2008,Ragib Hasan Marianne Winslett Richard Conlan Brian Slesinsky Nandakumar Ramani,@inproceedings{35042 title = {Please Permit Me: Stateless Delegated Authorization in Mashups} author = {Ragib Hasan and Marianne Winslett and Richard Conlan and Brian Slesinsky and Nandakumar Ramani} year = 2008 URL = {http://ragibhasan.com/publications/papers/hasan-acsac2009mashup.pdf} booktitle = {Proceedings of the Annual Computer Security Applications Conference} pages = {173-182} address = {Anaheim CA} },Mashups have emerged as a Web 2.0 phenomenon connecting disjoint applications together to provide unified services. However scalable access control for mashups is difficult. To enable a mashup to gather data from legacy applications and services users must give the mashup their login names and passwords for those services. This all-or-nothing approach violates the principle of least privilege (not to mention the terms of service) and leaves users vulnerable to misuse of their credentials by malicious mashups. In this paper we introduce Permits - a stateless approach to access rights delegation in mashups - and describe our complete implementation of a permit-based authorization delegation service.,http://research.google.com/pubs/archive/35042.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Please+Permit+Me:+Stateless+Delegated+Authorization+in+Mashups+Hasan+Winslett+Conlan+Slesinsky+Ramani,http://research.google.com/pubs/pub35042.html
Nonlinear Latent Factorization by Embedding Multiple User Interests,ACM International Conference on Recommender Systems (RecSys) (2013),2013,Jason Weston Ron Weiss Hector Yee,@inproceedings{41535 title = {Nonlinear Latent Factorization by Embedding Multiple User Interests} author = {Jason Weston and Ron Weiss and Hector Yee} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2507209} booktitle = {ACM International Conference on Recommender Systems (RecSys)} },Classical matrix factorization approaches to collaborative filtering learn a latent vector for each user and each item and recommendations are scored via the similarity between two such vectors which are of the same dimension. In this work we are motivated by the intuition that a user is a much more complicated entity than any single item and cannot be well described by the same representation. Hence the variety of a user’s interests could be better captured by a more complex representation. We propose to model the user with a richer set of functions speciﬁcally via a set of latent vectors where each vector captures one of the user’s latent interests or tastes. The overall recommendation model is then nonlinear where the matching score between a user and a given item is the maximum matching score over each of the user’s latent interests with respect to the item’s latent representation. We describe a simple general and efficient algorithm for learning such a model and apply it to large scale real world datasets from YouTube and Google Music where our approach outperforms existing techniques.,http://research.google.com/pubs/archive/41535.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Nonlinear+Latent+Factorization+by+Embedding+Multiple+User+Interests+Weston+Weiss+Yee,http://research.google.com/pubs/pub41535.html
Acoustic Modeling in Statistical Parametric Speech Synthesis - From HMM to LSTM-RNN,Proc. MLSLP (2015),2015,Heiga Zen,@inproceedings{43893 title = {Acoustic Modeling in Statistical Parametric Speech Synthesis - From HMM to LSTM-RNN} author = {Heiga Zen} year = 2015 note = {Invited paper} booktitle = {Proc. MLSLP} },Statistical parametric speech synthesis (SPSS) combines an acoustic model and a vocoder to render speech given a text. Typically decision tree-clustered context-dependent hidden Markov models (HMMs) are employed as the acoustic model which represent a relationship between linguistic and acoustic features. Recently artificial neural network-based acoustic models such as deep neural networks mixture density networks and long short-term memory recurrent neural networks (LSTM-RNNs) showed significant improvements over the HMM-based approach. This paper reviews the progress of acoustic modeling in SPSS from the HMM to the LSTM-RNN.,http://research.google.com/pubs/archive/43893.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Acoustic+Modeling+in+Statistical+Parametric+Speech+Synthesis+-+From+HMM+to+LSTM-RNN+Zen,http://research.google.com/pubs/pub43893.html
Challenges in Automatic Speech Recognition,Interspeech 2010,2010,Ciprian Chelba Johan Schalkwyk Michiel Bacchiani,@inproceedings{36913 title = {Challenges in Automatic Speech Recognition} author = {Ciprian Chelba and Johan Schalkwyk and Michiel Bacchiani} year = 2010 note = {ISCA Student panel presentation slides} booktitle = {Interspeech 2010} },ISCA Student panel presentation slides,http://research.google.com/pubs/archive/36913.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Challenges+in+Automatic+Speech+Recognition+Chelba+Schalkwyk+Bacchiani,http://research.google.com/pubs/pub36913.html
Memento Mori: Dynamic Allocation-site-based Optimizations,Proceedings of the 2015 ACM SIGPLAN International Symposium on Memory Management ACM New York NY USA pp. 105-117,2015,Daniel Clifford Hannes Payer Michael Stanton Ben L. Titzer,@inproceedings{43823 title = {Memento Mori: Dynamic Allocation-site-based Optimizations} author = {Daniel Clifford and Hannes Payer and Michael Stanton and Ben L. Titzer} year = 2015 booktitle = {Proceedings of the 2015 ACM SIGPLAN International Symposium on Memory Management} pages = {105--117} address = {New York NY USA} },Languages that lack static typing are ubiquitous in the world of mobile and web applications. The rapid rise of larger applications like interactive web GUIs games and cryptography presents a new range of implementation challenges for modern virtual machines to close the performance gap between typed and untyped languages. While all languages can benefit from efficient automatic memory management languages like JavaScript present extra thrill with innocent-looking but difficult features like dynamically-sized arrays deletable properties and prototypes. Optimizing such languages requires complex dynamic techniques with more radical object layout strategies such as dynamically evolving representations for arrays. This paper presents a general approach for gathering temporal allocation site feedback that tackles both the general problem of object lifetime estimation and improves optimization of these problematic language features. We introduce a new implementation technique where allocation mementos processed by the garbage collector and runtime system efficiently tie objects back to allocation sites in the program and dynamically estimate object lifetime representation and size to inform three optimizations: pretenuring pretransitioning and presizing. Unlike previous work on pretenuring our system utilizes allocation mementos to achieve fully dynamic allocation-site-based pretenuring in a production system. We implement all of our techniques in V8 a high performance virtual machine for JavaScript and demonstrate solid performance improvements across a range of benchmarks.,http://research.google.com/pubs/archive/43823.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Memento+Mori:+Dynamic+Allocation-site-based+Optimizations+Clifford+Payer+Stanton+Titzer,http://research.google.com/pubs/pub43823.html
Scalable I/O Event Handling for GHC,Proceedings of the 2010 ACM SIGPLAN Haskell Symposium (Haskell'10) pp. 103-108,2010,Bryan O’Sullivan Johan Tibell,@inproceedings{36841 title = {Scalable I/O Event Handling for GHC} author = {Bryan O’Sullivan and Johan Tibell} year = 2010 booktitle = {Proceedings of the 2010 ACM SIGPLAN Haskell Symposium (Haskell'10)} pages = {103--108} },We have developed a new portable I/O event manager for the Glasgow Haskell Compiler (GHC) that scales to the needs of modern server applications. Our new code is transparently available to existing Haskell applications. Performance at lower concurrency levels is comparable with the existing implementation. We support millions of concurrent network connections with millions of active timeouts from a single multithreaded program levels far beyond those achievable with the current I/O manager. In addition we provide a public API to developers who need to create event-driven network applications.,http://research.google.com/pubs/archive/36841.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+I/O+Event+Handling+for+GHC+O'Sullivan+Tibell,http://research.google.com/pubs/pub36841.html
Mechanism Design for Fair Division: Allocating Divisible Items without Payments,EC 2013 ACM,2013,Richard Cole Vasilis Gkatzelis Gagan Goel,@inproceedings{42185 title = {Mechanism Design for Fair Division: Allocating Divisible Items without Payments} author = {Richard Cole and Vasilis Gkatzelis and Gagan Goel} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2482582} booktitle = {EC 2013} },We revisit the classic problem of fair division from a mechanism design perspective using Proportional Fairness as a benchmark. In particular we aim to allocate a collection of divisible items to a set of agents while incentivizing the agents to be truthful in reporting their valuations. For the very large class of homogeneous valuations we design a truthful mechanism that provides every agent with at least 0.368 fraction of her Proportionally Fair valuation. To complement this result we show that no truthful mechanism can guarantee more than a 0.5 fraction even for the restricted class of additive linear valuations. We also propose another mechanism for additive linear valuations that works really well when every item is highly demanded. To guarantee truthfulness our mechanisms discard a carefully chosen fraction of the allocated resources; we conclude by uncovering interesting connections between our mechanisms and known mechanisms that use money instead.,http://research.google.com/pubs/archive/42185.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mechanism+Design+for+Fair+Division:++Allocating+Divisible+Items+without+Payments+Cole+Gkatzelis+Goel,http://research.google.com/pubs/pub42185.html
Decision Tree State Clustering with Word and Syllable Features,Interspeech ISCA (2010) 2958 – 2961,2010,Hank Liao Chris Alberti Michiel Bacchiani Olivier Siohan,@inproceedings{36828 title = {Decision Tree State Clustering with Word and Syllable Features} author = {Hank Liao and Chris Alberti and Michiel Bacchiani and Olivier Siohan} year = 2010 booktitle = {Interspeech} pages = {2958 – 2961} },In large vocabulary continuous speech recognition decision trees are widely used to cluster triphone states. In addition to commonly used phonetically based questions others have proposed additional questions such as phone position within word or syllable. This paper examines using the word or syllable context itself as a feature in the decision tree providing an elegant way of introducing word- or syllable-specific models into the system. Positive results are reported on two state-of-the-art systems: voicemail transcription and a search by voice tasks across a variety of acoustic model and training set sizes.,http://research.google.com/pubs/archive/36828.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Decision+Tree+State+Clustering+with+Word+and+Syllable+Features+Liao+Alberti+Bacchiani+Siohan,http://research.google.com/pubs/pub36828.html
Searching for Build Debt: Experiences Managing Technical Debt at Google,Proceedings of the Third International Workshop on Managing Technical Debt IEEE (2012) pp. 1-6,2012,J. David Morgenthaler Misha Gridnev Raluca Sauciuc Sanjay Bhansali,@inproceedings{37755 title = {Searching for Build Debt: Experiences Managing Technical Debt at Google} author = {J. David Morgenthaler and Misha Gridnev and Raluca Sauciuc and Sanjay Bhansali} year = 2012 booktitle = {Proceedings of the Third International Workshop on Managing Technical Debt} pages = {1--6} },With a large and rapidly changing codebase Google software engineers are constantly paying interest on various forms of technical debt. Google engineers also make efforts to pay down that debt whether through special Fixit days or via dedicated teams variously known as janitors cultivators or demolition experts. We describe several related efforts to measure and pay down technical debt found in Google's BUILD files and associated dead code. We address debt found in dependency specifications unbuildable targets and unnecessary command line flags. These efforts often expose other forms of technical debt that must first be managed.,http://research.google.com/pubs/archive/37755.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Searching+for+Build+Debt:+Experiences+Managing+Technical+Debt+at+Google+Morgenthaler+Gridnev+Sauciuc+Bhansali,http://research.google.com/pubs/pub37755.html
Statistical parametric speech synthesis: from HMM to LSTM-RNN,RTTH Summer School on Speech Technology -- A Deep Learning Perspective Barcelona Spain (2015),2015,Heiga Zen,@misc{44312 title = {Statistical parametric speech synthesis: from HMM to LSTM-RNN} author = {Heiga Zen} year = 2015 note = {Lecture given at RTTH Summer School on Speech Technology Barcelona Spain} },This talk will present progress of acoustic modeling in statistical parametric speech synthesis from the conventional hidden Markov model HMM to the state-of-the-art long short-term memory recurrent neural network. The details of implementation and applications of statistical parametric speech synthesis are also included.,http://research.google.com/pubs/archive/44312.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+parametric+speech+synthesis:+from+HMM+to+LSTM-RNN+Zen,http://research.google.com/pubs/pub44312.html
Online Learning in the Manifold of Low-Rank Matrices,Neural Information Processing Systems (NIPS 23) Curran Associates Inc. (2011) pp. 2128-2136,2011,Gal Chechik Daphna Weinshall Uri Shalit,@inproceedings{36898 title = {Online Learning in the Manifold of Low-Rank Matrices} author = {Gal Chechik and Daphna Weinshall and Uri Shalit} year = 2011 note = {Edited by J. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R.S. Zemel and A. Culotta} booktitle = {Neural Information Processing Systems (NIPS 23)} pages = {2128--2136} },When learning models that are represented in matrix forms enforcing a low-rank constraint can dramatically improve the memory and run time complexity while providing a natural regularization of the model. However naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds and describe an iterative online learning procedure consisting of a gradient step followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute and so is the projection operator that approximates it we describe another second-order retraction that can be computed efﬁciently with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m _ n given rank-one gradients. We use this algorithm LORETA to learn a matrixform similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classiﬁcation task.,http://research.google.com/pubs/archive/36898.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Learning+in+the+Manifold+of+Low-Rank+Matrices+Chechik+Weinshall+Shalit,http://research.google.com/pubs/pub36898.html
Reusable Components of Semantic Specifications,Lecture Notes in Computer Science vol. 8989 (2015) pp. 132-179,2015,Martin Churchill Peter D. Mosses Neil Sculthorpe Paolo Torrini,@article{44832 title = {Reusable Components of Semantic Specifications} author = {Martin Churchill and Peter D. Mosses and Neil Sculthorpe and Paolo Torrini} year = 2015 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-662-46734-3_4} journal = {Lecture Notes in Computer Science} pages = {132-179} volume = {8989} },Semantic specifications of programming languages typically have poor modularity. This hinders reuse of parts of the semantics of one language when specifying a different language – even when the two languages have many constructs in common – and evolution of a language may require major reformulation of its semantics. Such drawbacks have discouraged language developers from using formal semantics to document their designs. In the PLanCompS project we have developed a component-based approach to semantics. Here we explain its modularity aspects and present an illustrative case study: a component-based semantics for Caml Light. We have tested the correctness of the semantics by running programs on an interpreter generated from the semantics comparing the output with that produced on the standard implementation of the language. Our approach provides good modularity facilitates reuse and should support co-evolution of languages and their formal semantics. It could be particularly useful in connection with domain-specific languages and language-driven software development.,http://rd.springer.com/chapter/10.1007%2F978-3-662-46734-3_4,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reusable+Components+of+Semantic+Specifications+Churchill+Mosses+Sculthorpe+Torrini,http://research.google.com/pubs/pub44832.html
Topology-Driven Trajectory Synthesis with an Example on Retinal Cell Motions,14th Workshop on Algorithms in Bioinformatics Springer Wroclaw Poland (2014) pp. 326-339,2014,Chen Gu Leonidas Guibas Michael Kerber,@inproceedings{42966 title = {Topology-Driven Trajectory Synthesis with an Example on Retinal Cell Motions} author = {Chen Gu and Leonidas Guibas and Michael Kerber} year = 2014 URL = {http://rd.springer.com/chapter/10.1007/978-3-662-44753-6_24} booktitle = {14th Workshop on Algorithms in Bioinformatics} pages = {326--339} address = {Wroclaw Poland} },We design a probabilistic trajectory synthesis algorithm for generating time-varying sequences of geometric configuration data. The algorithm takes a set of observed samples (each may come from a different trajectory) and simulates the dynamic evolution of the patterns in O(n^2 log n) time. To synthesize geometric configurations with indistinct identities we use the pair correlation function to summarize point distribution and alpha-shapes to maintain topological shape features based on a fast persistence matching approach. We apply our method to build a computational model for the geometric transformation of the cone mosaic in retinitis pigmentosa --- an inherited and currently untreatable retinal degeneration.,http://rd.springer.com/chapter/10.1007/978-3-662-44753-6_24,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Topology-Driven+Trajectory+Synthesis+with+an+Example+on+Retinal+Cell+Motions+Gu+Guibas+Kerber,http://research.google.com/pubs/pub42966.html
When the Cloud Goes Local: The Global Problem with Data Localization,Computer vol. 46 No. 12 (2013) pp. 54-59,2013,Patrick Ryan Sarah Falvey Ronak Merchant,@article{42544 title = {When the Cloud Goes Local: The Global Problem with Data Localization} author = {Patrick Ryan and Sarah Falvey and Ronak Merchant} year = 2013 URL = {http://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2370850} journal = {Computer} pages = {54-59} volume = {46 No. 12} },Ongoing efforts to legally define cloud computing and regulate separate parts of the Internet are unlikely to address underlying concerns about data security and privacy. Data localization initiatives led primarily by European countries could actually bring the cloud to the ground and make the Internet less secure.,http://research.google.com/pubs/archive/42544.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=When+the+Cloud++Goes+Local:+The++Global+Problem++with+Data++Localization+Ryan+Falvey+Merchant,http://research.google.com/pubs/pub42544.html
Availability in Globally Distributed Storage Systems,Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation USENIX (2010),2010,Daniel Ford Francois Labelle Florentina Popovici Murray Stokely Van-Anh Truong Luiz Barroso Carrie Grimes Sean Quinlan,@inproceedings{36737 title = {Availability in Globally Distributed Storage Systems} author = {Daniel Ford and Francois Labelle and Florentina Popovici and Murray Stokely and Van-Anh Truong and Luiz Barroso and Carrie Grimes and Sean Quinlan} year = 2010 booktitle = {Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation} },Highly available cloud storage is often implemented with complex multi-tiered distributed systems built on top of clusters of commodity servers and disk drives. Sophisticated management load balancing and recovery techniques are needed to achieve high performance and availability amidst an abundance of failure sources that include software hardware network connectivity and power issues. While there is a relative wealth of failure studies of individual components of storage systems such as disk drives relatively little has been reported so far on the overall availability behavior of large cloud-based storage services. We characterize the availability properties of cloud storage systems based on an extensive one year study of Google's main storage infrastructure and present statistical models that enable further insight into the impact of multiple design choices such as data placement and replication strategies. With these models we compare data availability under a variety of system parameters given the real patterns of failures observed in our fleet.,http://research.google.com/pubs/archive/36737.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Availability+in+Globally+Distributed+Storage+Systems+Ford+Labelle+Popovici+Stokely+Truong+Barroso+Grimes+Quinlan,http://research.google.com/pubs/pub36737.html
Recurrent Neural Networks for Voice Activity Detection,ICASSP IEEE (2013) pp. 7378-7382,2013,Thad Hughes Keir Mierle,@inproceedings{41186 title = {Recurrent Neural Networks for Voice Activity Detection} author = {Thad Hughes and Keir Mierle} year = 2013 booktitle = {ICASSP} pages = {7378--7382} },We present a novel recurrent neural network (RNN) model for voice activity detection. Our multi-layer RNN model in which nodes compute quadratic polynomials outperforms a much larger baseline system composed of Gaussian mixture models (GMMs) and a hand-tuned state machine (SM) for temporal smoothing. All parameters of our RNN model are optimized together so that it properly weights its preference for temporal continuity against the acoustic features in each frame. Our RNN uses one tenth the parameters and outperforms the GMM+SM baseline system by 26% reduction in false alarms reducing overall speech recognition computation time by 17% while reducing word error rate by 1% relative.,http://research.google.com/pubs/archive/41186.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recurrent+Neural+Networks+for+Voice+Activity+Detection+Hughes+Mierle,http://research.google.com/pubs/pub41186.html
Libra: Divide and Conquer to Verify Forwarding Tables in Huge Networks,11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14) USENIX Association (2014) pp. 87-99,2014,Amin Vahdat Fei Ye Shidong Zhang Junda Liu,@inproceedings{42805 title = {Libra: Divide and Conquer to Verify Forwarding Tables in Huge Networks} author = {Amin Vahdat and Fei Ye and Shidong Zhang and Junda Liu} year = 2014 booktitle = {11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)} pages = {87-99} },Data center networks often have errors in the forwarding tables causing packets to loop indefinitely fall into black-holes or simply get dropped before they reach the correct destination. Finding forwarding errors is possible using static analysis but none of the existing tools scale to a large data center network with thousands of switches and millions of forwarding entries. Worse still in a large data center network the forwarding state is constantly in flux which makes it hard to take an accurate snapshot of the state for static analysis. We solve these problems with Libra a new tool for verifying forwarding tables in very large networks. Libra runs fast because it can exploit the scaling properties of MapReduce. We show how Libra can take an accurate snapshot of the forwarding state 99.9% of the time and knows when the snapshot cannot be trusted. We show results for Libra analyzing a 10000 switch,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Libra:+Divide+and+Conquer+to+Verify+Forwarding+Tables+in+Huge+Networks+Vahdat+Ye+Zhang+Liu,http://research.google.com/pubs/pub42805.html
Zero-Shot Learning by Convex Combination of Semantic Embeddings,International Conference on Learning Representations (2014),2014,Mohammad Norouzi Tomas Mikolov Samy Bengio Yoram Singer Jonathon Shlens Andrea Frome Greg Corrado Jeffrey Dean,@inproceedings{42371 title = {Zero-Shot Learning by Convex Combination of Semantic Embeddings} author = {Mohammad Norouzi and Tomas Mikolov and Samy Bengio and Yoram Singer and Jonathon Shlens and Andrea Frome and Greg Corrado and Jeffrey Dean} year = 2014 URL = {http://arxiv.org/abs/1312.5650} booktitle = {International Conference on Learning Representations} },Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional \nway{} classification framing of image understanding particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper we propose a simple method for constructing an image embedding system from any existing \nway{} image classifier and a semantic word embedding model which contains the $\n$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors and requires no additional training. We show that this simple and direct method confers many of and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.,http://research.google.com/pubs/archive/42371.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Zero-Shot+Learning+by+Convex+Combination+of+Semantic+Embeddings+Norouzi+Mikolov+Bengio+Singer+Shlens+Frome+Corrado+Dean,http://research.google.com/pubs/pub42371.html
Flywheel: Google’s Data Compression Proxy for the Mobile Web,Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation NSDI 2015 USENIX 2560 Ninth Street Suite 215 Berkeley CA 94710 USA,2015,Victor Agababov Michael Buettner Victor Chudnovsky,@inproceedings{43458 title = {Flywheel: Google’s Data Compression Proxy for the Mobile Web} author = {Victor Agababov and Michael Buettner and Victor Chudnovsky} year = 2015 URL = {http://www.michaelpiatek.com/papers/flywheel-nsdi15.pdf} booktitle = {Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation NSDI 2015} address = {2560 Ninth Street Suite 215 Berkeley CA 94710 USA} },Mobile devices are increasingly the dominant Internet access technology. Nevertheless high costs data caps and throttling are a source of widespread frustration and a significant barrier to adoption in emerging markets. This paper presents Flywheel an HTTP proxy service that extends the life of mobile data plans by compressing responses in-flight between origin servers and client browsers. Flywheel is integrated with the Chrome web browser and reduces the size of proxied web pages by 50% for a median user. We report measurement results from millions of users as well as experience gained during three years of operating and evolving the production service at Google.,http://www.michaelpiatek.com/papers/flywheel-nsdi15.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Flywheel:+Google%E2%80%99s+Data+Compression+Proxy+for+the+Mobile+Web+Agababov+Buettner+Chudnovsky,http://research.google.com/pubs/pub43458.html
A new approach to the semantics of model diagrams,18th International Workshop on Types for Proofs and Programs (TYPES 2011) LIPICS (2013) pp. 28-40,2013,Johan G. Granstrom,@inproceedings{40678 title = {A new approach to the semantics of model diagrams} author = {Johan G. Granstrom} year = 2013 URL = {http://www.dagstuhl.de/dagpub/978-3-939897-49-1} booktitle = {18th International Workshop on Types for Proofs and Programs (TYPES 2011)} pages = {28-40} },Sometimes a diagram can say more than a thousand lines of code. But sadly most of the time software engineers give up on diagrams after the design phase and all real work is done in code. The supremacy of code over diagrams would be leveled if diagrams were code. This paper suggests that model and instance diagrams or which amounts to the same class and object diagrams become first level entities in a suitably expressive programming language viz. type theory. The proposed semantics of diagrams is compositional and self-describing i.e. reflexive or metacircular. Moreover it is well suited for metamodelling and model driven engineering as it is possible to prove model transformations correct in type theory. The encoding into type theory has the additional benefit of making diagrams immediately useful given an implementation of type theory.,http://research.google.com/pubs/archive/40678.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+new+approach+to+the+semantics+of+model+diagrams+Granstrom,http://research.google.com/pubs/pub40678.html
Software engineering for privacy in-the-large,International Conference in Software Engineering IEEE Press (2015),2015,Pauline Anthonysamy Awais Rashid,@inproceedings{44672 title = {Software engineering for privacy in-the-large} author = {Pauline Anthonysamy and Awais Rashid} year = 2015 booktitle = {International Conference in Software Engineering} },There will be an estimated 35 zettabytes (35_ 1021) of digital records worldwide by the year 2020. This effectively amounts to privacy management on an ultra-large-scale. In this briefing we discuss the privacy challenges posed by such an ultralarge-scale ecosystem-we term this “Privacy in the Large”. We will contrast existing approaches to privacy management reflect on their strengths and limitations in this regard and outline key software engineering research and practice challenges to be addressed in the future.,http://research.google.com/pubs/archive/44672.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Software+engineering+for+privacy+in-the-large+Anthonysamy+Rashid,http://research.google.com/pubs/pub44672.html
Backoff Inspired Features for Maximum Entropy Language Models,Proceedings of Interspeech ISCA (2014),2014,Fadi Biadsy Keith Hall Pedro Moreno Brian Roark,@inproceedings{43114 title = {Backoff Inspired Features for Maximum Entropy Language Models} author = {Fadi Biadsy and Keith Hall and Pedro Moreno and Brian Roark} year = 2014 booktitle = {Proceedings of Interspeech} },Maximum Entropy (MaxEnt) language models are linear models that are typically regularized via well-known L1 or L2 terms in the likelihood objective hence avoiding the need for the kinds of backoff or mixture weights used in smoothed n-gram language models using Katz backoff and similar techniques. Even though backoff cost is not required to regularize the model we investigate the use of backoff features in MaxEnt models as well as some backoff-inspired variants. These features are shown to improve model quality substantially as shown in perplexity and word-error rate reductions even in very large scale training scenarios of tens or hundreds of billions of words and hundreds of millions of features.,http://research.google.com/pubs/archive/43114.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Backoff+Inspired+Features+for+Maximum+Entropy+Language+Models+Biadsy+Hall+Moreno+Roark,http://research.google.com/pubs/pub43114.html
Forecasting Web Page Views: Methods and Observations,JMLR vol. 9(Oct) (2008) pp. 2217-2250,2008,Jia Li Andrew Moore,@article{34912 title = {Forecasting Web Page Views: Methods and Observations} author = {Jia Li and Andrew Moore} year = 2008 URL = {http://www.jmlr.org/papers/volume9/li08a/li08a.pdf} journal = {JMLR} pages = {2217--2250} volume = {9(Oct)} },Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper we focus on extracting trends and seasonal patterns from page view series two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method namely the Elastic Smooth Season Fitting (ESSF) algorithm to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization a quadratic optimization problem. It is shown that for long-term prediction ESSF improves accuracy significantly over other methods that ignore the yearly seasonality.,http://research.google.com/pubs/archive/34912.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Forecasting+Web+Page+Views:+Methods+and+Observations+Li+Moore,http://research.google.com/pubs/pub34912.html
Computing TCP's Retransmission Timer,IETF (2011),2011,Vern Paxson Mark Allman H.K. Jerry Chu Matt Sargent,@misc{37177 title = {Computing TCP's Retransmission Timer} author = {Vern Paxson and Mark Allman and H.K. Jerry Chu and Matt Sargent} year = 2011 URL = {http://www.rfc-editor.org/rfc/rfc6298.txt} },This document defines the standard algorithm that Transmission Control Protocol (TCP) senders are required to use to compute and manage their retransmission timer. It expands on the discussion in Section 4.2.3.1 of RFC1122 and upgrades the requirement of supporting the algorithm from a SHOULD to a MUST. This document obsoletes RFC 2988. This is an Internet Standards Track document.,http://www.rfc-editor.org/rfc/rfc6298.txt,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Computing+TCP's+Retransmission+Timer+Paxson+Allman+Chu+Sargent,http://research.google.com/pubs/pub37177.html
Scalable all-pairs similarity search in metric spaces,Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining ACM 2 Pennsylvania Plaza New York NY (2013) pp. 829-837,2013,Ye Wang Ahmed Metwally Srinivasan Parthasarathy,@inproceedings{41467 title = {Scalable all-pairs similarity search in metric spaces} author = {Ye Wang and Ahmed Metwally and Srinivasan Parthasarathy} year = 2013 URL = {http://dl.acm.org/citation.cfm?doid=2487575.2487625} booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining} pages = {829--837} address = {2 Pennsylvania Plaza New York NY} },Given a set of entities the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article we propose a parallel framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the input dataset with minimal redundancy to achieve good load-balance in the presence of limited computing resources; iii) an on-the- fly lossless compression strategy to reduce both the running time and the final output size. We validate the utility scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets.,http://research.google.com/pubs/archive/41467.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+all-pairs+similarity+search+in+metric+spaces+Wang+Metwally+Parthasarathy,http://research.google.com/pubs/pub41467.html
Probabilistic Models for Melodic Prediction,Artificial Intelligence Journal vol. 173 (2009) pp. 1266-1274,2009,Jean-Francois Paiement Samy Bengio Douglas Eck,@article{33332 title = {Probabilistic Models for Melodic Prediction} author = {Jean-Francois Paiement and Samy Bengio and Douglas Eck} year = 2009 journal = {Artificial Intelligence Journal} pages = {1266--1274} volume = {173} },Chord progressions are the building blocks from which tonal music is constructed. The choice of a particular representation for chords has a strong impact on statistical modeling of the dependence between chord symbols and the actual sequences of notes in polyphonic music. Melodic prediction is used in this paper as a benchmark task to evaluate the quality of four chord representations using two probabilistic model architectures derived from Input/Output Hidden Markov Models (IOHMMs). Likelihoods and conditional and unconditional prediction error rates are used as complementary measures of the quality of each of the proposed chord representations. We observe empirically that different chord representations are optimal depending on the chosen evaluation metric. Also representing chords only by their roots appears to be a good compromise in most of the reported experiments.,http://research.google.com/pubs/archive/33332.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Probabilistic+Models+for+Melodic+Prediction+Paiement+Bengio+Eck,http://research.google.com/pubs/pub33332.html
A taste of Capsicum: practical capabilities for UNIX,Communications of the ACM vol. 55(3) (2012) pp. 97-104,2012,Robert N. M. Watson Jonathan Anderson Ben Laurie Kris Kennaway,@article{41098 title = {A taste of Capsicum: practical capabilities for UNIX} author = {Robert N. M. Watson and Jonathan Anderson and Ben Laurie and Kris Kennaway} year = 2012 URL = {http://dl.acm.org/citation.cfm?id=2093572} journal = {Communications of the ACM} pages = {97-104} volume = {55(3)} },Capsicum is a lightweight operating system (OS) capability and sandbox framework planned for inclusion in FreeBSD 9. Capsicum extends rather than replaces UNIX APIs providing new kernel primitives (sandboxed capability mode and capabilities) and a userspace sandbox API. These tools support decomposition of monolithic UNIX applications into compartmentalized logical applications an increasingly common goal that is supported poorly by existing OS access control primitives. We demonstrate our approach by adapting core FreeBSD utilities and Google,http://dl.acm.org/citation.cfm?id=2093572,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+taste+of+Capsicum:+practical+capabilities+for+UNIX+Watson+Anderson+Laurie+Kennaway,http://research.google.com/pubs/pub41098.html
DDoS Protections for SMTP Servers,International Journal of Computer Science and Security (IJCSS) vol. 4 (2011) pp. 497-610,2011,Michael Still Eric McCreath,@article{37363 title = {DDoS Protections for SMTP Servers} author = {Michael Still and Eric McCreath} year = 2011 URL = {http://www.cscjournals.org/csc/manuscriptinfo.php?ManuscriptCode=68.69.62.78.78.40.46.51.47.103&JCode=IJCSS&EJCode=69.70.63.79.79.102&Volume=46.104&Issue=51.101} journal = {International Journal of Computer Science and Security (IJCSS)} pages = {497--610} volume = {4} },Many businesses rely on email of some form for their day to day operation. This is especially true for product support organizations who are largely unable to perform their role in the company if their in boxes are flooded with malicious email or if important email is delayed because of the processing of attack traffic. Simple Message Transfer Protocol (SMTP) is the Internet protocol for the transmission of these emails. Denial of Service (DoS) attacks are deliberate attempts by an attacker to disrupt the normal operation of a service with the goal of stopping legitimate requests for the service from being processed. This disruption normally takes the form of large delays in responding to requests dropped requests and other service interruptions. In this paper we explore the current state of research into Distributed Denial of Service (DDoS) attack detection protection and mitigation for SMTP servers connected to the Internet. We find that whilst there has been significant research into DDoS protection and detection generally much of it is not relevant to SMTP servers. During our survey we found only two papers directly addressing defending SMTP servers against such attacks.,http://www.cscjournals.org/csc/manuscriptinfo.php?ManuscriptCode=68.69.62.78.78.40.46.51.47.103&JCode=IJCSS&EJCode=69.70.63.79.79.102&Volume=46.104&Issue=51.101,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=DDoS+Protections+for+SMTP+Servers+Still+McCreath,http://research.google.com/pubs/pub37363.html
LIL: CLOS reaches higher-order sheds identity and has a transformative experience,Proceedings of the International Lisp Conference 2012 (to appear),2012,François-René Rideau,@inproceedings{40390 title = {LIL: CLOS reaches higher-order sheds identity and has a transformative experience} author = {François-René Rideau} year = 2012 booktitle = {Proceedings of the International Lisp Conference 2012} },LIL the Lisp Interface Library is a data structure library based on Interface-Passing Style. This programming style was designed to allow for parametric polymorphism (abstracting over types classes functions data) as well as ad-hoc polymorphism (incremental development with inheritance and mixins). It consists in isolating algorithmic information into first-class interfaces explicitly passed around as arguments dispatched upon by generic functions. As compared to traditional objects these interfaces typically lack identity and state while they manipulate data structures without intrinsic behavior. This style makes it just as easy to use pure functional persistent data structures without identity or state as to use stateful imperative ephemeral data structures. Judicious Lisp macros allow developers to avoid boilerplate and to abstract away interface objects to expose classic-looking Lisp APIs. Using on a very simple linear type system to model the side-effects of methods it is even possible to transform pure interfaces into stateful interfaces or the other way around or to transform a stateful interface into a traditional object-oriented API.,http://research.google.com/pubs/archive/40390.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=LIL:+CLOS+reaches+higher-order+sheds+identity+and+has+a+transformative+experience+Rideau,http://research.google.com/pubs/pub40390.html
CDE: A Tool For Creating Portable Experimental Software Packages,Computing in Science & Engineering vol. 14 (2012) pp. 32-35,2012,Philip Guo,@article{38290 title = {CDE: A Tool For Creating Portable Experimental Software Packages} author = {Philip Guo} year = 2012 journal = {Computing in Science & Engineering} pages = {32-35} volume = {14} },One technical barrier to reproducible computational science is that it's hard to distribute scientific code in a form that other researchers can easily execute on their own computers. To help eliminate this barrier the CDE tool packages all software dependencies required to rerun Linux-based computational experiments on other computers.,http://research.google.com/pubs/archive/38290.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=CDE:+A+Tool+For+Creating+Portable+Experimental+Software+Packages+Guo,http://research.google.com/pubs/pub38290.html
Vision Paper: Towards an Understanding of the Limits of Map-Reduce Computation,CloudFutures Workshop (2012),2012,Foto Afrati Anish Das Sarma Semih Salihoglu Jeffrey Ullman,@techreport{40412 title = {Vision Paper: Towards an Understanding of the Limits of Map-Reduce Computation} author = {Foto Afrati and Anish Das Sarma and Semih Salihoglu and Jeffrey Ullman} year = 2012 URL = {http://arxiv.org/abs/1204.1754} institution = {CloudFutures Workshop} },"A significant amount of recent research work has addressed the problem of solving various data management problems in the cloud. The major algorithmic challenges in map-reduce computations involve balancing a multitude of factors such as the number of machines available for mappers/reducers their memory requirements and communication cost (total amount of data sent from mappers to reducers). Most past work provides custom solutions to specific problems e.g. performing fuzzy joins in map-reduce clustering graph analyses and so on. While some problems are amenable to very efficient map-reduce algorithms some other problems do not lend themselves to a natural distribution and have provable lower bounds. Clearly the ease of ""map-reducability"" is closely related to whether the problem can be partitioned into independent pieces which are distributed across mappers/reducers. What makes a problem distributable? Can we characterize general properties of problems that determine how easy or hard it is to find efficient map-reduce algorithms? This is a vision paper that attempts to answer the questions described above.",http://arxiv.org/abs/1204.1754,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Vision+Paper:+Towards+an+Understanding+of+the+Limits+of+Map-Reduce+Computation+Afrati+Das+Sarma+Salihoglu+Ullman,http://research.google.com/pubs/pub40412.html
On inter-deriving small-step and big-step semantics: A case study for storeless call-by-need evaluation,Theoretical Computer Science vol. 435 (2012) pp. 21-42,2012,Olivier Danvy Kevin Millikin Johan Munk Ian Zerny,@article{40357 title = {On inter-deriving small-step and big-step semantics: A case study for storeless call-by-need evaluation} author = {Olivier Danvy and Kevin Millikin and Johan Munk and Ian Zerny} year = 2012 URL = {http://www.sciencedirect.com/science/article/pii/S0304397512001648} journal = {Theoretical Computer Science} pages = {21--42} volume = {435} },Starting from the standard call-by-need reduction for the _-calculus that is common to Ariola Felleisen Maraist Odersky and Wadler we inter-derive a series of hygienic semantic artifacts: a reduction-free storeless abstract machine a continuation-passing evaluation function and what appears to be the first heapless natural semantics for call-by-need evaluation. Furthermore we observe that the evaluation function implementing this natural semantics is in defunctionalized form. The refunctionalized counterpart of this evaluation function implements an extended direct semantics in the sense of Cartwright and Felleisen. Overall the semantic artifacts presented here are simpler than many other such artifacts that have been independently worked out and which require ingenuity skill and independent soundness proofs on a case-by-case basis. They are also simpler to inter-derive because the inter-derivational tools (e.g. refocusing and defunctionalization) already exist.,http://research.google.com/pubs/archive/40357.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+inter-deriving+small-step+and+big-step+semantics:+A+case+study+for+storeless+call-by-need+evaluation+Danvy+Millikin+Munk+Zerny,http://research.google.com/pubs/pub40357.html
SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois - Semantic Web Challenge,9th International Semantic Web Conference (ISWC 2010),2010,Thomas Steiner Michael Hausenblas,@inproceedings{37431 title = {SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois -- Semantic Web Challenge} author = {Thomas Steiner and Michael Hausenblas} year = 2010 URL = {http://www.cs.vu.nl/~pmika/swc/submissions/swc2010_submission_12.pdf} booktitle = {9th International Semantic Web Conference (ISWC 2010)} },SemWebVid is an online Ajax application that allows for the automatic generation of Resource Description Framework (RDF) video descriptions. These descriptions are based on two pillars: first on a combination of user-generated metadata such as title summary and tags; and second on closed captions which can be user-generated or be auto-generated via speech recognition. The plaintext contents of both pillars are being analyzed using multiple Natural Language Processing (NLP) Web services in parallel whose results are then merged and where possible matched back to concepts in the sense of Linking Open Data (LOD). The final result is a deep-linkable RDF description of the video and a “scroll-along” view of the video as an example of video visualization formats.,http://research.google.com/pubs/archive/37431.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SemWebVid+-+Making+Video+a+First+Class+Semantic+Web+Citizen+and+a+First+Class+Web+Bourgeois+--+Semantic+Web+Challenge+Steiner+Hausenblas,http://research.google.com/pubs/pub37431.html
Sources of Traffic Demand Variability and Use of Monte Carlo for Network Capacity Planning,Performance and Capacity 2014 by CMG Conference Performance and Capacity 2014 by CMG Conference Performance and Capacity 2014 by CMG Conference,2014,Alexander Gilgur Brian Eck,@inproceedings{42848 title = {Sources of Traffic Demand Variability and Use of Monte Carlo for Network Capacity Planning} author = {Alexander Gilgur and Brian Eck} year = 2014 booktitle = {Performance and Capacity 2014 by CMG Conference} address = {Performance and Capacity 2014 by CMG Conference} },When sizing any network capacity several factors such as Traffic Quality of Service (QoS) and Total Cost of Ownership (TCO) are usually taken into account. Generally it boils down to a joint minimization of cost and maximization of traffic subject to the constraints of protocol and QoS requirements. Stochastic nature of network traffic and link saturation queueing issues add uncertainty to the already complex optimization problem. In this paper we examine the sources of traffic demand variability and dive into Monte-Carlo methodology as an efficient way for solving these problems. Other sources of uncertainty in network capacity forecasting are briefly discussed in the Attachment.,http://research.google.com/pubs/archive/42848.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sources+of+Traffic+Demand+Variability+and+Use+of+Monte+Carlo+for+Network+Capacity+Planning+Gilgur+Eck,http://research.google.com/pubs/pub42848.html
Distributed divide-and-conquer techniques for effective DDoS attack defenses,IEEE International Conference on Distributed Computing Systems (ICDCS) (2008),2008,Muthuprasanna Muthusrinivasan Manimaran Govindarasu,@inproceedings{34344 title = {Distributed divide-and-conquer techniques for effective DDoS attack defenses} author = {Muthuprasanna Muthusrinivasan and Manimaran Govindarasu} year = 2008 URL = {http://research.google.com/archive/papers/34344.pdf} booktitle = {IEEE International Conference on Distributed Computing Systems (ICDCS)} },Distributed Denial-of-Service (DDoS) attacks have emerged as a popular means of causing mass targeted service disruptions often for extended periods of time. The relative ease and low costs of launching such attacks supplemented by the current woeful state of any viable defense mechanism have made them one of the top threats to the Internet community today. While distributed packet logging and/or packet marking have been explored in the past for DDoS attack traceback/mitigation we propose to advance the state of the art by using a novel distributed divide-and-conquer approach in designing a new data dissemination architecture that efficiently tracks attack sources. The main focus of our work is to tackle the three disjoint aspects of the problem namely attack tree construction attack path frequency detection and packet to path association independently and to use succinct recurrence relations to express their individual implementations. We also evaluate the network traffic and storage overhead induced by our proposed deployment on real-life Internet topologies supporting hundreds of victims each subject to thousands of high-bandwidth flows simultaneously and conclude that we can truly achieve single packet traceback guarantees with minimal overhead and high efficiency.,http://research.google.com/pubs/archive/34344.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+divide-and-conquer+techniques+for+effective+DDoS+attack+defenses+Muthusrinivasan+Manimaran,http://research.google.com/pubs/pub34344.html
Discriminative Keyword Spotting,Speech Communication (2009) pp. 317-329,2009,Joseph Keshet David Grangier Samy Bengio,@article{34844 title = {Discriminative Keyword Spotting} author = {Joseph Keshet and David Grangier and Samy Bengio} year = 2009 journal = {Speech Communication} pages = {317--329} },This paper proposes a new approach for keyword spotting which is based on large margin and kernel methods rather than on HMMs. Unlike previous approaches the proposed method employs a discriminative learning procedure in which the learning phase aims at achieving a high area under the ROC curve as this quantity is the most common measure to evaluate keyword spotters. The keyword spotter we devise is based on mapping the input acoustic representation of the speech utterance along with the target keyword into a vector space. Building on techniques used for large margin and kernel methods for predicting whole sequences our keyword spotter distills to a classifier in this vector-space which separates speech utterances in which the keyword is uttered from speech utterances in which the keyword is not uttered. We describe a simple iterative algorithm for training the keyword spotter and discuss its formal properties showing theoretically that it attains high area under the ROC curve. Experiments on read speech with the TIMIT corpus show that the resulted discriminative system outperforms the conventional context-independent HMM-based system. Further experiments using the TIMIT trained model but tested on both read (HTIMIT WSJ) and spontaneous speech (OGI-Stories) show that without further training or adaptation to the new corpus our discriminative system outperforms the conventional context-independent HMM-based system.,http://research.google.com/pubs/archive/34844.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discriminative+Keyword+Spotting+Keshet+Grangier+Bengio,http://research.google.com/pubs/pub34844.html
The SMAPH System for Query Entity Recognition and Disambiguation,ERD 2014: Entity Recognition and Disambiguation Challenge. SIGIR Forum. ACM,2014,Marco Cornolti Paolo Ferragina Massimiliano Ciaramita Stefan Rued Hinrich Schuetze,@inproceedings{42720 title = {The SMAPH System for Query Entity Recognition and Disambiguation} author = {Marco Cornolti and Paolo Ferragina and Massimiliano Ciaramita and Stefan Rued and Hinrich Schuetze} year = 2014 booktitle = {ERD 2014: Entity Recognition and Disambiguation Challenge. SIGIR Forum.} },The SMAPH system implements a pipeline of four main steps: (1) Fetching – it fetches the search results returned by a search engine given the query to be annotated; (2) Spotting – search result snippets are parsed to identify candidate mentions for the entities to be annotated. This is done in a novel way by detecting the keywords-in-context by looking at the bold parts of the search snippets; (3) Candidate generation – candidate entities are generated in two ways: from the Wikipedia pages occurring in the search results and from an existing annotator using the mentions identified in the spotting step as input; (4) Pruning – a binary SVM classifier is used to decide which entities to keep/discard in order to generate the final annotation set for the query. The SMAPH system ranked third on the development set and first on the final blind test of the 2014 ERD Challenge short text track.,http://research.google.com/pubs/archive/42720.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+SMAPH+System+for+Query+Entity+Recognition+and+Disambiguation+Cornolti+Ferragina+Ciaramita+Rued+Schuetze,http://research.google.com/pubs/pub42720.html
Yes–no answers versus check-all in self-administered modes. A systematic review and analyses,International Journal of Market Research vol. 57 (2015) pp. 203-223,2015,Mario Callegaro Mike Murakami Ziv Tepman Vani Henderson,@article{43431 title = {Yes–no answers versus check-all in self-administered modes. A systematic review and analyses} author = {Mario Callegaro and Mike Murakami and Ziv Tepman and Vani Henderson} year = 2015 URL = {https://www.mrs.org.uk/pdf/Callegaro.pdf} journal = {International Journal of Market Research} pages = {203-223} volume = {57} },When writing questions with dichotomous response options those administering surveys on the web or on paper can choose from a variety of formats including a check-all-that-apply or a forced-choice format (e.g. yes-no) in self-administered questionnaires. These two formats have been compared and evaluated in many experimental studies. In this paper we conduct a systematic review and a few meta-analyses of different aspects of the available research that compares these two formats. We find that endorsement levels increase by a factor of 1.42 when questions are posed in a forced-choice rather than check-all format. However when comparing across a battery of questions the rank order of endorsement rates remains the same for both formats. While most authors hypothesise that respondents endorse more alternatives presented in a forced-choice (versus check-all-that-apply) format because they process that format at a deeper cognitive level we introduce the acquiescence bias hypothesis as an alternative and complementary explanation. Further research is required to identify which format elicits answers closer to the ‘true level’ of endorsement since the few validation studies have proved inconclusive.,https://www.mrs.org.uk/pdf/Callegaro.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Yes%E2%80%93no+answers+versus+check-all+in+self-administered+modes.+A+systematic+review+and+analyses+Callegaro+Murakami+Tepman+Henderson,http://research.google.com/pubs/pub43431.html
A representation theorem for second-order functionals,Journal of Functional Programming vol. 25 (2015),2015,Mauro Jaskelioff Russell O'Connor,@article{43961 title = {A representation theorem for second-order functionals} author = {Mauro Jaskelioff and Russell O'Connor} year = 2015 URL = {http://dx.doi.org/10.1017/S0956796815000088} journal = {Journal of Functional Programming} volume = {25} },Representation theorems relate seemingly complex objects to concrete more tractable ones. In this paper we take advantage of the abstraction power of category theory and provide a datatype-generic representation theorem. More precisely we prove a representation theorem for a wide class of second-order functionals which are polymorphic over a class of functors. Types polymorphic over a class of functors are easily representable in languages such as Haskell but are difficult to analyse and reason about. The concrete representation provided by the theorem is easier to analyse but it might not be as convenient to implement. Therefore depending on the task at hand the change of representation may prove valuable in one direction or the other. We showcase the usefulness of the representation theorem with a range of examples. Concretely we show how the representation theorem can be used to prove that traversable functors are finitary containers how coalgebras of a parameterised store comonad relate to very well-behaved lenses and how algebraic effects might be implemented in a functional language.,http://research.google.com/pubs/archive/43961.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+representation+theorem+for+second-order+functionals+Jaskelioff+O'Connor,http://research.google.com/pubs/pub43961.html
Applying Control Theory in the Real World: Experience With Building a Controller for the .NET Thread Pool,Sigmetrics Performance Evaluation Review (2009) pp. 38-42,2009,Joseph L. Hellerstein Vance Morrison Eric Eilebrecht,@article{35085 title = {Applying Control Theory in the Real World: Experience With Building a Controller for the .NET Thread Pool} author = {Joseph L. Hellerstein and Vance Morrison and Eric Eilebrecht} year = 2009 journal = {Sigmetrics Performance Evaluation Review} pages = {38-42} },While much has been published about the value of using formal techniques from control engineering to build software systems little has been reported on software engineering considerations for building closed loop systems especially widely deployed resource managers. This paper discusses the design testing and tuning of a controller that optimizes concurrency levels in the .NET thread pool a feature that is present in approximately 1 billion computers that run the Windows Operating System. Some of the issues we encountered are: (a) designing an extensible controller that easily incorporates a diverse set of techniques; (b) creating a scalable test infrastructure to address running a large number of test cases; (c) providing test cases for which the optimal concurrency level is known a priori; and (d) choosing settings of tuning parameters that result in good controller performance for multiple evaluation criteria.,http://research.google.com/pubs/archive/35085.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Applying+Control+Theory+in+the+Real+World:+Experience+With+Building+a+Controller+for+the+.NET+Thread+Pool+Hellerstein+Morrison+Eilebrecht,http://research.google.com/pubs/pub35085.html
Dense Subgraph Maintenance under Streaming Edge Weight Updates for Real-time Story Identification,The VLDB Journal (2013) pp. 1-25,2013,Albert Angel Nick Koudas Nikos Sarkas Divesh Srivastava Michael Svendsen Srikanta Tirthapura,@article{41607 title = {Dense Subgraph Maintenance under Streaming Edge Weight Updates for Real-time Story Identification} author = {Albert Angel and Nick Koudas and Nikos Sarkas and Divesh Srivastava and Michael Svendsen and Srikanta Tirthapura} year = 2013 URL = {http://dx.doi.org/10.1007/s00778-013-0340-z} note = {VLDBJ Special Issue: Best of VLDB 2012} journal = {The VLDB Journal} pages = {1-25} },Recent years have witnessed an unprecedented proliferation of social media. People around the globe author every day millions of blog posts micro-blog posts social network status updates etc. This rich stream of information can be used to identify on an ongoing basis emerging stories and events that capture popular attention. Stories can be identified via groups of tightly-coupled real-world entities namely the people locations products etc. that are involved in the story. The sheer scale and rapid evolution of the data involved necessitate highly efficient techniques for identifying important stories at every point of time. The main challenge in real-time story identification is the maintenance of dense subgraphs (corresponding to groups of tightly-coupled entities) under streaming edge weight updates (resulting from a stream of user-generated content). This is the first work to study the efficient maintenance of dense subgraphs under such streaming edge weight updates. For a wide range of definitions of density we derive theoretical results regarding the magnitude of change that a single edge weight update can cause. Based on these we propose a novel algorithm DynDens which outperforms adaptations of existing techniques to this setting and yields meaningful results. Our approach is validated by a thorough experimental evaluation on large-scale real and synthetic datasets.,http://dx.doi.org/10.1007/s00778-013-0340-z,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dense+Subgraph+Maintenance+under+Streaming+Edge+Weight+Updates+for+Real-time+Story+Identification+Angel+Koudas+Sarkas+Srivastava+Svendsen+Tirthapura,http://research.google.com/pubs/pub41607.html
Program Representation for General Intelligence,The Second Conference on Artificial General Intelligence (2009),2009,Moshe Looks Ben Goertzel,@inproceedings{34939 title = {Program Representation for General Intelligence} author = {Moshe Looks and Ben Goertzel} year = 2009 URL = {http://agi-conf.org/2009/papers/paper_69.pdf} booktitle = {The Second Conference on Artificial General Intelligence} },Traditional machine learning systems work with relatively flat uniform data representations such as feature vectors time-series and context-free grammars. However reality often presents us with data which are best understood in terms of relations types hierarchies and complex functional forms. One possible representational scheme for coping with this sort of complexity is computer programs. This immediately raises the question of how programs are to be best represented. We propose an answer in the context of ongoing work towards artificial general intelligence.,http://research.google.com/pubs/archive/34939.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Program+Representation+for+General+Intelligence+Looks+Goertzel,http://research.google.com/pubs/pub34939.html
HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm,Proceedings of the EDBT 2013 Conference ACM Genoa Italy (to appear),2013,Stefan Heule Marc Nunkesser Alex Hall,@inproceedings{40671 title = {HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm} author = {Stefan Heule and Marc Nunkesser and Alex Hall} year = 2013 booktitle = {Proceedings of the EDBT 2013 Conference} address = {Genoa Italy} },Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past and the HyperLogLog algorithm is one of them. In this paper we present a series of improvements to this algorithm that reduce its memory requirements and signi?cantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically comparing it to the original HyperLogLog algorithm. Like HyperLogLog our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.,http://research.google.com/pubs/archive/40671.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HyperLogLog+in+Practice:+Algorithmic+Engineering+of+a+State+of+The+Art+Cardinality+Estimation+Algorithm+Heule+Nunkesser+Hall,http://research.google.com/pubs/pub40671.html
A Tweet Consumers' Look At Twitter Trends,Workshop Making Sense of Microposts (MSM 2011) at the Extended Semantic Web Conference (ESWC 2011) Heraklion Crete,2011,Thomas Steiner Arnaud Brousseau Raphael Troncy,@inproceedings{37428 title = {A Tweet Consumers' Look At Twitter Trends} author = {Thomas Steiner and Arnaud Brousseau and Raphael Troncy} year = 2011 URL = {http://research.hypios.com/msm2011/posters/steiner.pdf} booktitle = {Workshop Making Sense of Microposts (MSM 2011) at the Extended Semantic Web Conference (ESWC 2011)} address = {Heraklion Crete} },Twitter Trends allows for a global or local view on “what’s happening in my world right now” from a tweet producers’ point of view. In this paper we explore a way to complete Twitter Trends by having a closer look at the other side: the tweet consumers’ point of view. While Twitter Trends works by analyzing the frequency of terms and their velocity of appearance in tweets being written our approach is based on the popularity of extracted named entities in tweets being read.,http://research.google.com/pubs/archive/37428.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Tweet+Consumers'+Look+At+Twitter+Trends+Steiner+Brousseau+Troncy,http://research.google.com/pubs/pub37428.html
Tunable Performance and Consistency Tradeoffs for Geographically Replicated Cloud Services (COLOR),Cyber Security and Cloud Computing (CSCloud) 2015 IEEE 2nd International Conference on IEEE pp. 457-463,2015,Wenbo Zhu C. Murray Woodside,@inproceedings{44809 title = {Tunable Performance and Consistency Tradeoffs for Geographically Replicated Cloud Services (COLOR)} author = {Wenbo Zhu and C. Murray Woodside} year = 2015 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7371522} booktitle = {Cyber Security and Cloud Computing (CSCloud) 2015 IEEE 2nd International Conference on} pages = {457-463} },"COLOR (client-oriented layered optimistic replication) is a combination of optimistic and conservative data replication that allows cloud services to be replicated across widely distributed locations without suffering from the latency overhead of strict algorithms and with quantifiable and controllable tradeoffs between performance and consistency guarantees. The COLOR solution adopts a layered approach to enable optimistic delivery of client messages on top of any existing storage layer that manages the strict replication of the cloud service. When clients may be temporarily exposed to inconsistent states due to replication failures such inconsistency is made recoverable similar to ""optimistic concurrency control"" for clients that cache the server state. COLOR supports different numeric parameters to trade the strict consistency for better performance to possibly match Eventual Consistency while the end-to-end consistency is always guaranteed as the storage layer will never deliver any client messages generated from inconsistent states",http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7371522,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Tunable+Performance+and+Consistency+Tradeoffs+for+Geographically+Replicated+Cloud+Services+(COLOR)+Zhu+Woodside,http://research.google.com/pubs/pub44809.html
End-to-end Verification of QoS Policies,The 13th IEEE/IFIP Network Operations and Management Symposium (NOMS 2012),2012,Adel El-Atawy Taghrid Samak,@inproceedings{37516 title = {End-to-end Verification of QoS Policies} author = {Adel El-Atawy and Taghrid Samak} year = 2012 booktitle = {The 13th IEEE/IFIP Network Operations and Management Symposium (NOMS 2012)} },Conﬁguring a large number of routers and network devices to achieve quality of service (QoS) goals is a challenging task. In a differentiated services (DiffServ) environment trafﬁc ﬂows are assigned speciﬁc classes of service and service level agreements (SLA) are enforced at routers within each domain. We present a model for QoS conﬁgurations that facilitates efﬁcient property-based veriﬁcation. Network conﬁguration is given as a set of policies governing each device. The model efﬁciently checks the required properties against the current conﬁguration using computation tree logic (CTL) model checking. By symbolically modeling possible decision paths for different ﬂows from source to destination properties can be checked at each hop and assessments can be made on how closely conﬁgurations adhere to the speciﬁed agreement. The model also covers conﬁguration debugging given a speciﬁc QoS violation. Efﬁciency and scalability of the model are analyzed for policy per-hop behavior (PHB) parameters over large network conﬁgurations.,http://research.google.com/pubs/archive/37516.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=End-to-end+Verification+of+QoS+Policies+El-Atawy+Samak,http://research.google.com/pubs/pub37516.html
ParTes. Test Suite for Parsing Evaluation,Procesamiento del Lenguaje Natural vol. 53 (2014) pp. 87-94,2014,Marina Lloberes Irene Castellón Lluís Padró Edgar Gonzàlez,@article{42946 title = {ParTes. Test Suite for Parsing Evaluation} author = {Marina Lloberes and Irene Castellón and Lluís Padró and Edgar Gonzàlez} year = 2014 URL = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/5040} journal = {Procesamiento del Lenguaje Natural} pages = {87-94} volume = {53} },This paper presents ParTes the first test suite in Spanish and Catalan for parsing qualitative evaluation. This resource is a hierarchical test suite of the representative syntactic structure and argument order phenomena. ParTes proposes a simplification of the qualitative evaluation by contributing to the automatization of this task.,http://research.google.com/pubs/archive/42946.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ParTes.+Test+Suite+for+Parsing+Evaluation+Lloberes+Castell%C3%B3n+Padr%C3%B3+Gonz%C3%A0lez+Pellicer,http://research.google.com/pubs/pub42946.html
Instant Foodie: Predicting Expert Ratings From Grassroots,CIKM’13 Oct. 27–Nov. 1 2013 San Francisco CA USA ACM,2013,Chenhao Tan Ed H. Chi David Huffaker Gueorgi Kossinets Alex J. Smola,@inproceedings{41382 title = {Instant Foodie: Predicting Expert Ratings From Grassroots} author = {Chenhao Tan and Ed H. Chi and David Huffaker and Gueorgi Kossinets and Alex J. Smola} year = 2013 booktitle = {CIKM’13 Oct. 27–Nov. 1 2013 San Francisco CA USA} },Consumer review sites and recommender systems typically rely on a large volume of user-contributed ratings which makes rating acquisition an essential component in the design of such systems. User ratings are then summarized to provide an aggregate score representing a popular evaluation of an item. An inherent problem in such summarization is potential bias due to raters’ self-selection and heterogeneity in terms of experiences tastes and rating scale interpretations. There are two major approaches to collecting ratings which have different advantages and disadvantages. One is to allow a large number of volunteers to choose and rate items directly (a method employed by e.g. Yelp and Google Places). Alternatively a panel of raters may be maintained and invited to rate a predeﬁned set of items at regular intervals (such as in Zagat Survey). The latter approach arguably results in more consistent reviews and reduced selection bias however at the expense of much smaller coverage (fewer rated items). In this paper we examine the two different approaches to collecting user ratings of restaurants and explore the question of whether it is possible to reconcile them. Speciﬁcally we study the problem of inferring the more calibrated Zagat Survey ratings (which we dub “expert ratings”) from the user-contributed ratings (“grassroots”) in Google Places. To achieve this we employ latent factor models and provide a probabilistic treatment of the ordinal ratings. We can predict Zagat Survey ratings accurately from ad hoc user-generated ratings by employing joint optimization. Furthermore the resulting model show that users become more discerning as they submit more ratings. We also describe an approach towards cross-city recommendations answering questions such as “What is the equivalent of the Per Se restaurant in Chicago?”,http://research.google.com/pubs/archive/41382.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Instant+Foodie:+Predicting+Expert+Ratings+From+Grassroots+Tan+Chi+Huffaker+Kossinets+Smola,http://research.google.com/pubs/pub41382.html
FTTH look ahead — technologies & architectures,ECOC 2010 Turin Italy,2010,Cedric F. Lam,@inproceedings{36936 title = {FTTH look ahead — technologies & architectures} author = {Cedric F. Lam} year = 2010 booktitle = {ECOC 2010} address = {Turin Italy} },We review the trade-offs challenges and potentials of various FTTH architecture options.,http://research.google.com/pubs/archive/36936.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=FTTH+look+ahead+%E2%80%94+technologies+%26+architectures+Lam,http://research.google.com/pubs/pub36936.html
De Bruijn Sequences Revisited,International Journal of Foundations of Computer Science vol. 23 (2012) pp. 1307-1322,2012,Lila Kari Zhi Xu,@article{42296 title = {De Bruijn Sequences Revisited} author = {Lila Kari and Zhi Xu} year = 2012 journal = {International Journal of Foundations of Computer Science} pages = {1307--1322} volume = {23} },A (non-circular) de Bruijn sequence w of order n is a word such that every word of length n appears exactly once in w as a factor. In this paper we generalize the concept to different settings: the multi-shift de Bruijn sequence and the pseudo de Bruijn sequence. An m-shift de Bruijn sequence of order n is a word such that every word of length n appears exactly once in w as a factor that starts at a position im + 1 for some integer i ≥ 0. A pseudo de Bruijn sequence of order n with respect to an antimorphic involution _ is a word such that for every word u of length n the total number of appearances of u and _(u) as a factor is one. We show that the number of m-shift de Bruijn sequences of order n is an!a(m-n)(an-1) for 1 ≤ n ≤ m and is (am!)an-m for 1 ≤ m ≤ n where a is the size of the alphabet. We provide two algorithms for generating a multi-shift de Bruijn sequence. The multi-shift de Bruijn sequence is important for solving the Frobenius problem in a free monoid. We show that the existence of pseudo de Bruijn sequences depends on the given alphabet and antimorphic involution and obtain formulas for the number of such sequences in some particular settings.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=De+Bruijn+Sequences+Revisited+Kari+Xu,http://research.google.com/pubs/pub42296.html
Wsabie: Scaling Up To Large Vocabulary Image Annotation,Proceedings of the International Joint Conference on Artificial Intelligence IJCAI (2011),2011,Jason Weston Samy Bengio Nicolas Usunier,@inproceedings{37180 title = {Wsabie: Scaling Up To Large Vocabulary Image Annotation} author = {Jason Weston and Samy Bengio and Nicolas Usunier} year = 2011 booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence IJCAI} },Image annotation datasets are becoming larger and larger with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method called Wsabie both outperforms several baseline methods and is faster and consumes less memory.,http://research.google.com/pubs/archive/37180.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Wsabie:+Scaling+Up+To+Large+Vocabulary+Image+Annotation+Weston+Bengio+Usunier,http://research.google.com/pubs/pub37180.html
More Bang for Their Bucks: Assessing New Features for Online Advertisers,AdKDD07 (in the ACM digital library) (2007),2007,Diane Lambert Daryl Pregibon,@inproceedings{33329 title = {More Bang for Their Bucks: Assessing New Features for Online Advertisers} author = {Diane Lambert and Daryl Pregibon} year = 2007 booktitle = {AdKDD07 (in the ACM digital library)} },Online search systems that display ads continually offer new features that advertisers can use to fine-tune and enhance their ad campaigns. An important question is whether a new feature actually helps advertisers. In an ideal world for statisticians we would answer this question by running a statistically designed experiment. But that would require randomly assigning a set of advertisers to the treatment group and forcing them to use the feature which is not realistic. Accordingly in the real world new features for advertisers are seldom evaluated with a traditional experimental protocol. Instead customer service representatives (CSRs) select advertisers who are invited to be among the first to test a new feature (i.e. white-listed) and then each white-listed advertiser chooses whether or not to use the new feature. Neither the CSR nor the advertiser chooses at random. This paper addresses the problem of drawing valid inferences from white-list trials about the effects of new features on advertiser happiness. We are guided by three principles. First statistical procedures for white-list trials are likely to be applied in an automated way so they should be robust to violations of modeling assumptions. Second standard analysis tools should be preferred over custom-built ones both for clarity and for robustness. Standard tools have withstood the test of time and have been thoroughly debugged. Finally it should be easy to compute reliable confidence intervals for the estimator. We review an estimator that has all these attributes allowing us to make valid inferences about the effects of a new feature on advertiser happiness. In the example in this paper the new feature was introduced during the holiday shopping season thereby further complicating the analysis.,http://research.google.com/pubs/archive/33329.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=More+Bang+for+Their+Bucks:+Assessing+New+Features+for+Online+Advertisers+Lambert+Pregibon,http://research.google.com/pubs/pub33329.html
TSum: Fast Principled Table Summarization.,Proceedings of the Seventh International Workshop on Data Mining for Online Advertising ACM (2013),2013,Jieying Chen Jia-Yu Pan Christos Faloutsos Spiros Papadimitriou,@inproceedings{41683 title = {TSum: Fast Principled Table Summarization.} author = {Jieying Chen and Jia-Yu Pan and Christos Faloutsos and Spiros Papadimitriou} year = 2013 booktitle = {Proceedings of the Seventh International Workshop on Data Mining for Online Advertising} },"Given a table where rows correspond to records and columns correspond to attributes we want to find a small number of patterns that succinctly summarize the dataset. For example given a set of patient records with several attributes each how can we find (a) that the ""most representative"" pattern is say (male adult *) followed by (* child low-cholesterol) etc.? We propose TSum a method that provides a sequence of patterns ordered by their ""representativeness."" It can decide both which these patterns are as well as how many are necessary to properly summarize the data. Our main contribution is formulating a general framework TSum using compression principles. TSum can easily accommodate different optimization strategies for selecting and refining patterns. The discovered patterns can be used to both represent the data efficiently as well as interpret it quickly. Extensive experiments demonstrate the effectiveness and intuitiveness of our discovered patterns.",http://research.google.com/pubs/archive/41683.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=TSum:+Fast+Principled+Table+Summarization.+Chen+Pan+Faloutsos+Papadimitriou,http://research.google.com/pubs/pub41683.html
Measuring Ad Effectiveness Using Geo Experiments,Google Inc. (2011),2011,Jon Vaver Jim Koehler,@techreport{38355 title = {Measuring Ad Effectiveness Using Geo Experiments} author = {Jon Vaver and Jim Koehler} year = 2011 URL = {http://googleresearch.blogspot.com/2011/12/measuring-ad-effectiveness-using-geo.html} institution = {Google Inc.} },Advertisers have a fundamental need to quantify the effectiveness of their advertising. For search ad spend this information provides a basis for formulating strategies related to bidding budgeting and campaign design. One approach that Google has successfully employed to measure advertising effectiveness is geo experiments. In these experiments non-overlapping geographic regions are randomly assigned to a control or treatment condition and each region realizes its assigned condition through the use of geo-targeted advertising. This paper describes the application of geo experiments and demon- strates that they are conceptually simple have a systematic and effective design process and provide results that are easy to interpret.,http://research.google.com/pubs/archive/38355.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Measuring+Ad+Effectiveness+Using+Geo+Experiments+Vaver+Koehler,http://research.google.com/pubs/pub38355.html
Bridging Text and Knowledge with Frames,ACL Workshop on Frame Semantics (in honor of Charles FIllmore) (2014),2014,Srini Narayanan,@inproceedings{43016 title = {Bridging Text and Knowledge with Frames} author = {Srini Narayanan} year = 2014 URL = {http://www.icsi.berkeley.edu/~snarayan/acl2014.pdf} booktitle = {ACL Workshop on Frame Semantics (in honor of Charles FIllmore)} },FrameNet is the current best operational version of Chuck Fillmore’s Frame Semantics. As FrameNet has evolved over the years we have been building a series of increasingly ambitious prototype applications that exploit the ideas of frame semantics and FrameNet as a resource. Results from this work suggest that frames are a natural semantic representation linking issue of textual meaning and world knowledge.,http://research.google.com/pubs/archive/43016.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bridging+Text+and+Knowledge+with+Frames+Narayanan,http://research.google.com/pubs/pub43016.html
Spherical Random Features for Polynomial Kernels,Neural Information Processing Systems (NIPS) (2015),2015,Jeffrey Pennington Felix X. Yu Sanjiv Kumar,@inproceedings{44290 title = {Spherical Random Features for Polynomial Kernels} author = {Jeffrey Pennington and Felix X. Yu and Sanjiv Kumar} year = 2015 booktitle = {Neural Information Processing Systems (NIPS)} },Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels for which low approximation error has thus far necessitated explicit feature maps of large dimensionality especially for higher-order polynomials. Meanwhile because polynomial kernels are unbounded they are frequently applied to data that has been normalized to unit l2 norm. The question we address in this work is: if we know a priori that data is normalized can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting and introduce a new approximation paradigm Spherical Random Fourier (SRF) features which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work SRF features are less rank-deficient more compact and achieve better kernel approximation especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.,http://research.google.com/pubs/archive/44290.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Spherical+Random+Features+for+Polynomial+Kernels+Pennington+Yu+Kumar,http://research.google.com/pubs/pub44290.html
Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics Association for Computational Linguistics Jeju Republic of Korea (2012) pp. 28-32,2012,Joern Wuebker Hermann Ney Richard Zens,@inproceedings{40409 title = {Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation} author = {Joern Wuebker and Hermann Ney and Richard Zens} year = 2012 URL = {http://www.aclweb.org/anthology-new/P/P12/P12-2006.pdf} booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics} pages = {28--32} address = {Jeju Republic of Korea} },In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT) aiming at increased efﬁ- ciency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second Moses reaches 17.2% BLEU whereas our approach yields 20.0% with identical models.,http://research.google.com/pubs/archive/40409.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+and+Scalable+Decoding+with+Language+Model+Look-Ahead+for+Phrase-based+Statistical+Machine+Translation+Wuebker+Ney+Zens,http://research.google.com/pubs/pub40409.html
Recognition of Multilingual Speech in Mobile Applications,ICASSP (2012),2012,Hui Lin Jui-Ting Huang Francoise Beaufays Brian Strope Yun-hsuan Sung,@inproceedings{37830 title = {Recognition of Multilingual Speech in Mobile Applications} author = {Hui Lin and Jui-Ting Huang and Francoise Beaufays and Brian Strope and Yun-hsuan Sung} year = 2012 booktitle = {ICASSP} },We evaluate different architectures to recognize multilingual speech for real-time mobile applications. In particular we show that combining the results of several recognizers greatly outperforms other solutions such as training a single large multilingual system or using an explicit language identification system to select the appropriate recognizer. Experiments are conducted on a trilingual English-French-Mandarin mobile speech task. The data set includes Google searches Maps queries as well as more general inputs such as email and short message dictation. Without pre-specifying the input language the combined system achieves comparable accu- racy to that of the monolingual systems when the input language is known. The combined system is also roughly 5% absolute better than an explicit language identification approach and 10% better than a single large multilingual system.,http://research.google.com/pubs/archive/37830.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recognition+of+Multilingual+Speech+in+Mobile+Applications+Lin+Huang+Beaufays+Strope+Sung,http://research.google.com/pubs/pub37830.html
Guidelines and Registration Procedures for URI Schemes,IETF RFCs Internet Engineering Task Force (2015) pp. 19,2015,Dave Thaler Tony Hansen Ted Hardie,@incollection{44669 title = {Guidelines and Registration Procedures for URI Schemes} author = {Dave Thaler and Tony Hansen and Ted Hardie} year = 2015 URL = {https://www.rfc-editor.org/rfc/pdfrfc/rfc7595.txt.pdf} booktitle = {IETF RFCs} pages = {19} },The Uniform Resource Identifier (URI) protocol element and generic syntax is defined by [RFC3986]. Each URI begins with a scheme name as defined by Section 3.1 of RFC 3986 that refers to a specification for identifiers within that scheme. The URI syntax provides a federated and extensible naming system where each scheme’s specification can further restrict the syntax and define the semantics of identifiers using that scheme. This document provides updated guidelines for the definition of new schemes for consideration by those who are defining registering or evaluating those definitions.,http://research.google.com/pubs/archive/44669.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Guidelines+and+Registration+Procedures+for+URI+Schemes+Thaler+Hansen+Hardie,http://research.google.com/pubs/pub44669.html
Online Panel Research: A Data Quality Perspective,Wiley (2014) pp. 512,2014,Mario Callegaro Reg Baker Jelke Bethlehem Anja S. Goritz Jon A. Krosnick Paul J. Lavrakas,@book{42492 title = {Online Panel Research: A Data Quality Perspective} author = {Mario Callegaro and Reg Baker and Jelke Bethlehem and Anja S. Goritz and Jon A. Krosnick and Paul J. Lavrakas} year = 2014 URL = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html} pages = {512} },This edited volume provides new insights into the accuracy and value of online panels for completing surveys Over the last decade there has been a major global shift in survey and market research towards data collection using samples selected from online panels. Yet despite their widespread use remarkably little is known about the quality of the resulting data. This edited volume is one of the first attempts to carefully examine the quality of the survey data being generated by online samples. It describes some of the best empirically-based research on what has become a very important yet controversial method of collecting data. Online Panel Research presents 19 chapters of previously unpublished work addressing a wide range of topics including coverage bias nonresponse measurement error adjustment techniques the relationship between nonresponse and measurement error impact of smartphone adoption on data collection Internet rating panels and operational issues. The datasets used to prepare the analyses reported in the chapters are available on the accompanying website: www.wiley.com/go/online_panel,http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119941776.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Panel+Research:+A+Data+Quality+Perspective+Callegaro+Baker+Bethlehem+Goritz+Krosnick+Lavrakas,http://research.google.com/pubs/pub42492.html
Fast Accurate Detection of 100000 Object Classes on a Single Machine,Proceedings of IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society Washington DC USA (2013),2013,Thomas Dean Mark Ruzon Mark Segal Jonathon Shlens Sudheendra Vijayanarasimhan Jay Yagnik,@inproceedings{40814 title = {Fast Accurate Detection of 100000 Object Classes on a Single Machine} author = {Thomas Dean and Mark Ruzon and Mark Segal and Jonathon Shlens and Sudheendra Vijayanarasimhan and Jay Yagnik} year = 2013 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition} address = {Washington DC USA} },Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique we apply it to evaluate 100000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.,http://research.google.com/pubs/archive/40814.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Accurate+Detection+of+100000+Object+Classes+on+a+Single+Machine+Dean+Ruzon+Segal+Shlens+Vijayanarasimhan+Yagnik,http://research.google.com/pubs/pub40814.html
Competition and Fraud in Online Advertising Markets,Financial Cryptography (2008),2008,Bob Mungamuru Stephen A. Weis,@inproceedings{34324 title = {Competition and Fraud in Online Advertising Markets} author = {Bob Mungamuru and Stephen A. Weis} year = 2008 booktitle = {Financial Cryptography} },An economic model of the online advertising market is presented focusing on the effect of ad fraud. In the model the market is comprised of three classes of players: publishers advertising networks and advertisers. The central question is whether ad networks have an incentive to aggressively combat fraud. The main outcome of the model is to answer this question in the affirmative,http://research.google.com/pubs/archive/34324.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Competition+and+Fraud+in+Online+Advertising+Markets+Mungamuru+Weis,http://research.google.com/pubs/pub34324.html
Non-interactive CCA-Secure threshold cryptosystems with adaptive security: new framework and constructions,Proceedings of the 9th international conference on Theory of Cryptography Springer-Verlag Berlin Heidelberg (2012) pp. 75-93,2012,Benoit Libert Moti Yung,@inproceedings{38224 title = {Non-interactive CCA-Secure threshold cryptosystems with adaptive security: new framework and constructions} author = {Benoit Libert and Moti Yung} year = 2012 URL = {http://dx.doi.org/10.1007/978-3-642-28914-9_5} booktitle = {Proceedings of the 9th international conference on Theory of Cryptography} pages = {75--93} address = {Berlin Heidelberg} },In threshold cryptography private keys are divided into n shares each one of which is given to a different server in order to avoid single points of failure. In the case of threshold public-key encryption at least t ≤ n servers need to contribute to the decryption process. A threshold primitive is said robust if no coalition of t malicious servers can prevent remaining honest servers from successfully completing private key operations. So far most practical non-interactive threshold cryptosystems where no interactive conversation is required among decryption servers were only proved secure against static corruptions. In the adaptive corruption scenario (where the adversary can corrupt servers at any time based on its complete view) all existing robust threshold encryption schemes that also resist chosen-ciphertext attacks (CCA) till recently require interaction in the decryption phase. A specific method (in composite order groups) for getting rid of interaction was recently suggested leaving the question of more generic frameworks and constructions with better security and better exibility (i.e. compatibility with distributed key generation). This paper describes a general construction of adaptively secure robust non-interactive threshold cryptosystems with chosen-ciphertext security. We dene the notion of all-but-one perfectly sound threshold hash proof systems that can be seen as (threshold) hash proof systems with publicly verifiable and simulation-sound proofs. We show that this notion generically implies threshold cryptosystems combining the aforementioned properties. Then we provide ecient instantiations under well-studied assumptions in bilinear groups (e.g. in such groups of prime order). These instantiations have a tighter security proof and are indeed compatible with distributed key generation protocols.,http://research.google.com/pubs/archive/38224.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Non-interactive+CCA-Secure+threshold+cryptosystems+with+adaptive+security:+new+framework+and+constructions+Libert+Yung,http://research.google.com/pubs/pub38224.html
Estimating the Spectral Reflectance of Natural Imagery Using Color Image Features,Workshop on Applications Systems and Algorithms for Image Sensing (2008),2008,Josh Hyman Mark Hansen Eric Graham Deborah Estrin,@inproceedings{34854 title = {Estimating the Spectral Reflectance of Natural Imagery Using Color Image Features} author = {Josh Hyman and Mark Hansen and Eric Graham and Deborah Estrin} year = 2008 URL = {http://www.joshhyman.com/papers/hyman2008reflectance.pdf} booktitle = {Workshop on Applications Systems and Algorithms for Image Sensing} },Relative spectral reﬂectance is an illumination invariant image feature that is related to many ecological phenomena that are difﬁcult to measure such as plant CO2 uptake. We describe a procedure to estimate the relative spectral reﬂectance of known subject using color image features. Through application we show that this procedure produces accurate estimates in the presence of changing ﬁeld conditions. Using this procedure we can use imagers as sensors to measure natural phenomena that cannot be easily measured using any other available sensing modality.,http://www.joshhyman.com/papers/hyman2008reflectance.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Estimating+the+Spectral+Reflectance+of+Natural+Imagery+Using+Color+Image+Features+Hyman+Hansen+Graham+Estrin,http://research.google.com/pubs/pub34854.html
Quantum Simulation of Helium Hydride Cation in a Solid-State Spin Register,ACS Nano vol. 9 (2015) 7769–7774,2015,Ya Wang Florian Dolde Jacob Biamonte Ryan Babbush Ville Bergholm Sen Yang Ingmar Jakobi Philipp Neumann Alán Aspuru-Guzik James Whitfield Jörg Wrachtrup,@article{43941 title = {Quantum Simulation of Helium Hydride Cation in a Solid-State Spin Register} author = {Ya Wang and Florian Dolde and Jacob Biamonte and Ryan Babbush and Ville Bergholm and Sen Yang and Ingmar Jakobi and Philipp Neumann and Alán Aspuru-Guzik and James Whitfield and Jörg Wrachtrup} year = 2015 URL = {http://pubs.acs.org/doi/abs/10.1021/acsnano.5b01651} journal = {ACS Nano} pages = {7769–7774} volume = {9} },Ab initio computation of molecular properties is one of the most promising applications of quantum computing. While this problem is widely believed to be intractable for classical computers efficient quantum algorithms exist which have the potential to vastly accelerate research throughput in fields ranging from material science to drug discovery. Using a solid-state quantum register realized in a nitrogen-vacancy (NV) defect in diamond we compute the bond dissociation curve of the minimal basis helium hydride cation HeH+. Moreover we report an energy uncertainty (given our model basis) of the order of 1e–14 hartree which is 10 orders of magnitude below the desired chemical precision. As NV centers in diamond provide a robust and straightforward platform for quantum information processing our work provides an important step toward a fully scalable solid-state implementation of a quantum chemistry simulator.,http://research.google.com/pubs/archive/43941.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Quantum+Simulation+of+Helium+Hydride+Cation+in+a+Solid-State+Spin+Register+Wang+Dolde+Biamonte+Babbush+Bergholm+Yang+Jakobi+Neumann+Aspuru-Guzik+Whitfield+Wrachtrup,http://research.google.com/pubs/pub43941.html
Solving connectivity problems parameterized by treewidth in single exponential time,Foundations of Computer Science 2011 Rynek G_ówny 12 (to appear),2011,Marek Cygan Jesper Nederlof Marcin Pilipczuk Micha_ Pilipczuk Johann M. M. van Rooij Jakub Onufry Wojtaszczyk,@inproceedings{37373 title = {Solving connectivity problems parameterized by treewidth in single exponential time} author = {Marek Cygan and Jesper Nederlof and Marcin Pilipczuk and Micha_ Pilipczuk and Johann M. M. van Rooij and Jakub Onufry Wojtaszczyk} year = 2011 booktitle = {Foundations of Computer Science 2011} address = {Rynek G_ówny 12} },For the vast majority of local problems on graphs of small treewidth (where by local we mean that a solution can be verified by checking separately the neighbourhood of each vertex) standard dynamic programming techniques give c^tw |V|^O(1) time algorithms where tw is the treewidth of the input graph G = (V; E) and c is a constant. On the other hand for problems with a global requirement (usually connectivity) the best–known algorithms were naive dynamic programming schemes running in at least tw^tw time. We breach this gap by introducing a technique we named Cut&Count that allows to produce c^tw |V|^O(1) time Monte Carlo algorithms for most connectivity-type problems including HAMILTONIAN PATH STEINER TREE FEEDBACK VERTEX SET and CONNECTED DOMINATING SET. These results have numerous consequences in various ﬁelds like parameterized complexity exact and approximate algorithms on planar and H-minor-free graphs and exact algorithms on graphs of bounded degree. The constant c in our algorithms is in all cases small and in several cases we are able to show that improving those constants would cause the Strong Exponential Time Hypothesis to fail. In contrast to the problems aiming to minimize the number of connected components that we solve using Cut&Count as mentioned above we show that assuming the Exponential Time Hypothesis the aforementioned gap cannot be breached for some problems that aim to maximize the number of connected components like CYCLE PACKING.,http://research.google.com/pubs/archive/37373.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Solving+connectivity+problems+parameterized+by+treewidth+in+single+exponential+time+Cygan+Nederlof+Pilipczuk+Pilipczuk+Rooij+Wojtaszczyk,http://research.google.com/pubs/pub37373.html
Succinct approximate counting of skewed data,IJCAI-09 Proceedings (2009) pp. 1243-1248,2009,David Talbot,@inproceedings{35519 title = {Succinct approximate counting of skewed data} author = {David Talbot} year = 2009 URL = {http://ijcai.org/papers09/Papers/IJCAI09-209.pdf} booktitle = {IJCAI-09 Proceedings} pages = {1243--1248} },Practical data analysis relies on the ability to count observations of objects succinctly and efficiently. Unfortunately the space usage of an exact estimator grows with the size of the a priori set from which objects are drawn while the time required to maintain such an estimator grows with the size of the data set. We present static and on-line approximation schemes that avoid these limitations when approximate frequency estimates are acceptable. Our Log-Frequency Sketch extends the approximate counting algorithm of Morris [1978] to estimate frequencies with bounded relative error via a single pass over a data set. It uses constant space per object when the frequencies follow a power law and can be maintained in constant time per observation. We give an (__)-approximation scheme which we verify empirically on a large natural language data set where for instance 95 percent of frequencies are estimated with relative error less than 0.25 using fewer than 11 bits per object in the static case and 15 bits per object on-line.,http://research.google.com/pubs/archive/35519.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Succinct+approximate+counting+of+skewed+data+Talbot,http://research.google.com/pubs/pub35519.html
HTAF: Hybrid Testing Automation Framework to Leverage Local and Global Computing Resources,Lecture Notes in Computer Science vol. 6784 (2011) pp. 479-494,2011,Keun Soo Yim David Hreczany Ravishankar K. Iyer,@article{41143 title = {HTAF: Hybrid Testing Automation Framework to Leverage Local and Global Computing Resources} author = {Keun Soo Yim and David Hreczany and Ravishankar K. Iyer} year = 2011 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-642-21931-3_37?LI=true} journal = {Lecture Notes in Computer Science} pages = {479--494} volume = {6784} },In web application development testing forms an increasingly large portion of software engineering costs due to the growing complexity and short time-to-market of these applications. This paper presents a hybrid testing automation framework (HTAF) that can automate routine works in testing and releasing web software. Using this framework an individual software engineer can easily describe his routine software engineering tasks and schedule these described tasks by using both his local machine and global cloud computers in an efficient way. This framework is applied to commercial web software development processes. Our industry practice shows four example cases where the hybrid and decentralized architecture of HTAF is helpful at effectively managing both hardware resources and manpower required for testing and releasing web applications.,http://research.google.com/pubs/archive/41143.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HTAF:+Hybrid+Testing+Automation+Framework+to+Leverage+Local+and+Global+Computing+Resources+Yim+Hreczany+Iyer,http://research.google.com/pubs/pub41143.html
Improving SSL Warnings: Comprehension and Adherence,Proceedings of the Conference on Human Factors and Computing Systems ACM (2015),2015,Adrienne Porter Felt Alex Ainslie Robert W. Reeder Sunny Consolvo Somas Thyagaraja Alan Bettes Helen Harris Jeff Grimes,@inproceedings{43265 title = {Improving SSL Warnings: Comprehension and Adherence} author = {Adrienne Porter Felt and Alex Ainslie and Robert W. Reeder and Sunny Consolvo and Somas Thyagaraja and Alan Bettes and Helen Harris and Jeff Grimes} year = 2015 booktitle = {Proceedings of the Conference on Human Factors and Computing Systems} },Browsers warn users when the privacy of an SSL/TLS connection might be at risk. An ideal SSL warning would empower users to make informed decisions and failing that guide confused users to safety. Unfortunately users struggle to understand and often disregard real SSL warnings. We report on the task of designing a new SSL warning with the goal of improving comprehension and adherence. We designed a new SSL warning based on recommendations from warning literature and tested our proposal with microsurveys and a field experiment. We ultimately failed at our goal of a well-understood warning. However nearly 30% more total users chose to remain safe after seeing our warning. We attribute this success to opinionated design which promotes safety with visual cues. Subsequently our proposal was released as the new Google Chrome SSL warning. We raise questions about warning comprehension advice and recommend that other warning designers use opinionated design.,http://research.google.com/pubs/archive/43265.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+SSL+Warnings:+Comprehension+and+Adherence+Felt+Ainslie+Reeder+Consolvo+Thyagaraja+Bettes+Harris+Grimes,http://research.google.com/pubs/pub43265.html
Improving Word Alignment with Bridge Languages,Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning Association for Computational Linguistics 209 N. Eighth Street East Stroudsburg PA USA (2007),2007,Shankar Kumar Franz Och Wolfgang Macherey,@inproceedings{33430 title = {Improving Word Alignment with Bridge Languages} author = {Shankar Kumar and Franz Och and Wolfgang Macherey} year = 2007 URL = {http://www.aclweb.org/anthology-new/D/D07/D07-1005.pdf} booktitle = {Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning} address = {209 N. Eighth Street East Stroudsburg PA USA} },We describe an approach to improve Statistical Machine Translation (SMT) performance using multi-lingual parallel sentence-aligned corpora in several bridge languages. Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for combining word alignment systems from multiple bridge languages. The final translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual parallel text in Spanish French Russian and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task.,http://research.google.com/pubs/archive/33430.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+Word+Alignment+with+Bridge+Languages+Kumar+Och+Macherey,http://research.google.com/pubs/pub33430.html
Managing Crowdsourced Human Computation,20th International World Wide Web Conference WWW 2011,2011,Panagiotis G. Ipeirotis Praveen K. Paritosh,@misc{36946 title = {Managing Crowdsourced Human Computation} author = {Panagiotis G. Ipeirotis and Praveen K. Paritosh} year = 2011 },The tutorial covers an emerging topic of wide interest: Crowdsourcing. Specifically we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields including computer science statistics economics and psychology. Furthermore the material will include real-life examples and case studies from years of experience in running and managing crowdsourcing applications in business settings. The tutorial presenters have an extensive academic and systems building experience and will provide the audience with data sets that can be used for hands-on tasks.,http://research.google.com/pubs/archive/36946.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Managing+Crowdsourced+Human+Computation+Ipeirotis+Paritosh,http://research.google.com/pubs/pub36946.html
Web 2.0 and Performance: Using Social Media to Facilitate Learning at Google,Michael Allen's e-Learning Annual 2012 Pfeiffer 989 Market St San Francisco CA 94103 (2011) pp. 171-179,2011,Julia Bulkowski,@inbook{37212 title = {Web 2.0 and Performance: Using Social Media to Facilitate Learning at Google} author = {Julia Bulkowski} year = 2011 booktitle = {Michael Allen's e-Learning Annual 2012} pages = {171-179} address = {989 Market St San Francisco CA 94103} },Are you leveraging Web 2.0 technologies to solve performance problems? Google has tapped the power of online collaboration to solve business problems and engage learners. It is easier than you might think to leverage scalable and free technologies to address your organization's needs. In this hands-on session explore case studies of how Google is using blogs wikis shared documents RSS readers and online video sharing to transform learning and performance.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+2.0+and+Performance:+Using+Social+Media+to+Facilitate+Learning+at+Google+Wilkowski,http://research.google.com/pubs/pub37212.html
A Generative Model for Distance Patterns in Music,NIPS Workshop on Music Brain and Cognition (2007),2007,Jean-Francois Paiement Yves Grandvalet Samy Bengio Douglas Eck,@inproceedings{32978 title = {A Generative Model for Distance Patterns in Music} author = {Jean-Francois Paiement and Yves Grandvalet and Samy Bengio and Douglas Eck} year = 2007 URL = {http://bengio.abracadoudou.com/publications/pdf/paiement_2007_nips.pdf} booktitle = {NIPS Workshop on Music Brain and Cognition} },In order to cope for the difficult problem of long term dependencies in sequential data in general and in musical data in particular a generative model for distance patterns especially designed for music is introduced. A specific implementation of the model when considering Hamming distances over rhythms is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy over two different music databases.,http://bengio.abracadoudou.com/publications/pdf/paiement_2007_nips.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Generative+Model+for+Distance+Patterns+in+Music+Paiement+Grandvalet+Bengio+Eck,http://research.google.com/pubs/pub32978.html
Cross Platform Network Access Control,RVASec 2013 RVASec 2013 RIchmond VA,2013,Paul (Tony) Watson,@inproceedings{41404 title = {Cross Platform Network Access Control} author = {Paul (Tony) Watson} year = 2013 URL = {http://rvasec.com/speakers/#PaulWatson} note = {http://rvasec.com/speakers/#PaulWatson} booktitle = {RVASec 2013} address = {RIchmond VA} },Discussion of Capirca an open-sourced multi-platform Network ACL generation system. This talk will discuss the history of Capirca originating as an internal Google project through its current form and use in the open-source community. Attendees will gain an understand of how to use the system to simplify and improve the efficiency and reliability of network security management. A significant portion of time will also be dedicated to an overview of how the software and libraries work internally including how to develop new modules and contribute to the open source effort.,http://rvasec.com/speakers/#PaulWatson,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cross+Platform+Network+Access+Control+Watson,http://research.google.com/pubs/pub41404.html
Dremel: Interactive Analysis of Web-Scale Datasets,Proc. of the 36th Int'l Conf on Very Large Data Bases (2010) pp. 330-339,2010,Sergey Melnik Andrey Gubarev Jing Jing Long Geoffrey Romer Shiva Shivakumar Matt Tolton Theo Vassilakis,@inproceedings{36632 title = {Dremel: Interactive Analysis of Web-Scale Datasets} author = {Sergey Melnik and Andrey Gubarev and Jing Jing Long and Geoffrey Romer and Shiva Shivakumar and Matt Tolton and Theo Vassilakis} year = 2010 URL = {http://www.vldb2010.org/accept.htm} booktitle = {Proc. of the 36th Int'l Conf on Very Large Data Bases} pages = {330-339} },Dremel is a scalable interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data and has thousands of users at Google. In this paper we describe the architecture and implementation of Dremel and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.,http://research.google.com/pubs/archive/36632.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dremel:+Interactive+Analysis+of+Web-Scale+Datasets+Melnik+Gubarev+Long+Romer+Shivakumar+Tolton+Vassilakis,http://research.google.com/pubs/pub36632.html
Mesa: Geo-Replicated Near Real-Time Scalable Data Warehousing,VLDB (2014),2014,Ashish Gupta Fan Yang Jason Govig Adam Kirsch Kelvin Chan Kevin Lai Shuo Wu Sandeep Dhoot Abhilash Kumar Ankur Agiwal Sanjay Bhansali Mingsheng Hong Jamie Cameron Masood Siddiqi David Jones Jeff Shute Andrey Gubarev Shivakumar Venkataraman Divyakant Agrawal,@inproceedings{42851 title = {Mesa: Geo-Replicated Near Real-Time Scalable Data Warehousing} author = {Ashish Gupta and Fan Yang and Jason Govig and Adam Kirsch and Kelvin Chan and Kevin Lai and Shuo Wu and Sandeep Dhoot and Abhilash Kumar and Ankur Agiwal and Sanjay Bhansali and Mingsheng Hong and Jamie Cameron and Masood Siddiqi and David Jones and Jeff Shute and Andrey Gubarev and Shivakumar Venkataraman and Divyakant Agrawal} year = 2014 booktitle = {VLDB} },Mesa is a highly scalable analytic data warehousing system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy a complex and challenging set of user and systems requirements including near real-time data ingestion and queryability as well as high availability reliability fault tolerance and scalability for large data and query volumes. Specifically Mesa handles petabytes of data processes millions of row updates per second and serves billions of queries that fetch trillions of rows per day. Mesa is geo-replicated across multiple datacenters and provides consistent and repeatable query answers at low latency even when an entire datacenter fails. This paper presents the Mesa system and reports the performance and scale that it achieves.,http://research.google.com/pubs/archive/42851.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mesa:+Geo-Replicated+Near+Real-Time+Scalable+Data+Warehousing+Gupta+Yang+Govig+Kirsch+Chan+Lai+Wu+Dhoot+Rajesh+Kumar+Agiwal+Bhansali+Hong+Cameron+Siddiqi+Jones+Shute+Gubarev+Venkataraman+Agrawal,http://research.google.com/pubs/pub42851.html
The landscape of digital media research: big data big research right impact,Digital Media Education Foundation Conference Las Vegas NV (2012),2012,Chris Chapman,@inproceedings{44182 title = {The landscape of digital media research: big data big research right impact} author = {Chris Chapman} year = 2012 booktitle = {Digital Media Education Foundation Conference} address = {Las Vegas NV} },Invited keynote,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+landscape+of+digital+media+research:+big+data+big+research+right+impact+Chapman,http://research.google.com/pubs/pub44182.html
Finding Images and Line Drawings in Document-Scanning Systems,Proc. International Conference on Document Analysis and Retrieval IAPR (2009),2009,Shumeet Baluja Michele Covell,@inproceedings{35522 title = {Finding Images and Line Drawings in Document-Scanning Systems} author = {Shumeet Baluja and Michele Covell} year = 2009 URL = {http://www.cvc.uab.es/icdar2009/papers/3725b096.pdf} booktitle = {Proc. International Conference on Document Analysis and Retrieval} },This work addresses the problem of finding images and line-drawings in scanned pages. It is a crucial processing step in the creation of a large-scale system to detect and index images found in books and historic documents. Within the scanned pages that contain both text and images the images are found through the use of local-feature extraction applied across the full scanned page. This is followed by a novel learning system to categorize the local features into either text or image. The discrimination is based on using multiple classifiers trained via stochastic sampling of weak classifiers for each AdaBoost stage. The approach taken in sampling includes stochastic hill climbing across weak detectors allowing us to reduce our classification error by as much as 25% relative to more naive stochastic sampling. Stochastic hill climbing in the weak classifier space is possible due to the manner in which we parameterize the weak classifier space. Through the use of this system we improve image detection by finding more line-drawings graphics and photographs as well as reducing the number of spurious detections due to misclassified text discoloration and scanning artifacts.,http://research.google.com/pubs/archive/35522.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Finding+Images+and+Line+Drawings+in+Document-Scanning+Systems+Baluja+Covell,http://research.google.com/pubs/pub35522.html
RLint: Reformatting R Code to Follow the Google Style Guide,R User Conference (2014),2014,Alex Blocker Andy Chen Andy Chu Tim Hesterberg Jeffrey D. Oldham Caitlin Sadowski Tom Zhang,@inproceedings{42577 title = {RLint: Reformatting R Code to Follow the Google Style Guide} author = {Alex Blocker and Andy Chen and Andy Chu and Tim Hesterberg and Jeffrey D. Oldham and Caitlin Sadowski and Tom Zhang} year = 2014 booktitle = {R User Conference} },RLint (https://code.google.com/p/google-rlint/) both checks and reformats R code to the Google R Style Guide. It warns of violations and optionally produces compliant code. It considers proper spacing line alignment inside brackets and other style violations but like all lint programs does not try to handle all syntax issues. Code that follows a uniform style eases maintenance modification and ensuring correctness especially when multiple programmers are involved. Thus RLint is automatically used within Google as part of the peer review process for R code. We encourage CRAN package authors and other R programmers to use this tool. A user can run the open-source Python-based program in a Linux Unix Mac or Windows machine via a command line.,http://research.google.com/pubs/archive/42577.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RLint:+Reformatting+R+Code+to+Follow+the+Google+Style+Guide+Blocker+Chen+Chu+Hesterberg+Oldham+Sadowski+Zhang,http://research.google.com/pubs/pub42577.html
Communicating Semantics: Reference by Description,ArXiv (2015),2015,Ramanathan V. Guha Vineet Gupta,@article{44679 title = {Communicating Semantics: Reference by Description} author = {Ramanathan V. Guha and Vineet Gupta} year = 2015 URL = {http://arxiv.org/abs/1511.06341} journal = {ArXiv} },Messages often refer to entities such as people places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for `Reference by Description' derive results on the conditions under which with high probability programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results.,http://research.google.com/pubs/archive/44679.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Communicating+Semantics:+Reference+by+Description+Guha+Gupta,http://research.google.com/pubs/pub44679.html
Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations,Proceedings of the 27th Annual Conference on Learning Theory (COLT) (2014),2014,H. Brendan McMahan Francesco Orabona,@inproceedings{42504 title = {Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations} author = {H. Brendan McMahan and Francesco Orabona} year = 2014 booktitle = {Proceedings of the 27th Annual Conference on Learning Theory (COLT)} },We study algorithms for online linear optimization in Hilbert spaces focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms recovering and even improving several previous results as immediate corollaries. Moreover using our tools we develop an algorithm that provides a regret bound of $O(U \sqrt{T \log( U \sqrt{T} \log^2 T +1)})$ where $U$ is the $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to the player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ is known we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown $T$ case a Normal approximation to the conditional value of the game proves to be the key analysis tool.,http://research.google.com/pubs/archive/42504.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unconstrained+Online+Linear+Learning+in+Hilbert+Spaces:+Minimax+Algorithms+and+Normal+Approximations+McMahan+Orabona,http://research.google.com/pubs/pub42504.html
Why Locally-Fair Maximal Flows in Client-Server Networks Perform Well,Computing and Combinatorics Springer Berlin Heidelberg 12715 NE 81st PL (2009) pp. 368-377,2009,Chad Yoshikawa Ken Berman,@inproceedings{42243 title = {Why Locally-Fair Maximal Flows in Client-Server Networks Perform Well} author = {Chad Yoshikawa and Ken Berman} year = 2009 booktitle = {Computing and Combinatorics} pages = {368-377} address = {12715 NE 81st PL} },Maximal flows reach at least a 1/2 approximation of the maximum flow in client-server networks. By adding only 1 additional time round to any distributed maximal flow algorithm we show how this 1/2-approximation can be improved on bounded-degree networks. We call these modified maximal flows ‘locally fair’ since there is a measure of fairness prescribed to each client and server in the network. Let N_=_(UVEb) represent a client-server network with clients U servers V network links E and node capacities b where we assume that each capacity is at least one unit. Let d(u) denote the b-weighted degree of any node u___U___V __=_ max {d(u) | u___U } and __=_ min { d(v) | v___V }. We show that a locally-fair maximal flow f achieves an approximation to the maximum flow of min{1_2__2_2_____ } and this result is sharp for any given integers _ and _. This results are of practical importance since local-fairness loosely models the steady-state behavior of TCP/IP and these types of degree-bounds often occur naturally (or are easy to enforce) in real client-server systems.,http://research.google.com/pubs/archive/42243.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+Locally-Fair+Maximal+Flows+in+Client-Server+Networks+Perform+Well+Yoshikawa+Berman,http://research.google.com/pubs/pub42243.html
packetdrill: Scriptable Network Stack Testing from Sockets to Packets,Proceedings of the USENIX Annual Technical Conference (USENIX ATC 2013) USENIX 2560 Ninth Street Suite 215 Berkeley CA 94710 USA pp. 213-218,2013,Neal Cardwell Yuchung Cheng Lawrence Brakmo Matt Mathis Barath Raghavan Nandita Dukkipati Hsiao-keng Jerry Chu Andreas Terzis Tom Herbert,@inproceedings{41316 title = {packetdrill: Scriptable Network Stack Testing from Sockets to Packets} author = {Neal Cardwell and Yuchung Cheng and Lawrence Brakmo and Matt Mathis and Barath Raghavan and Nandita Dukkipati and Hsiao-keng Jerry Chu and Andreas Terzis and Tom Herbert} year = 2013 URL = {https://www.usenix.org/conference/atc13/packetdrill-scriptable-network-stack-testing-sockets-packets} booktitle = {Proceedings of the USENIX Annual Technical Conference (USENIX ATC 2013)} pages = {213--218} address = {2560 Ninth Street Suite 215 Berkeley CA 94710 USA} },Testing today’s increasingly complex network protocol implementations can be a painstaking process. To help meet this challenge we developed packetdrill a portable open-source scripting tool that enables testing the correctness and performance of entire TCP/UDP/IP network stack implementations from the system call layer to the hardware network interface for both IPv4 and IPv6. We describe the design and implementation of the tool and our experiences using it to execute 657 test cases. The tool was instrumental in our development of three new features for Linux TCP—Early Retransmit Fast Open and Loss Probes—and allowed us to find and fix 10 bugs in Linux. Our team uses packetdrill in all phases of the development process for the kernel used in one of the world’s largest Linux installations.,http://research.google.com/pubs/archive/41316.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=packetdrill:+Scriptable+Network+Stack+Testing+from+Sockets+to+Packets+Cardwell+Cheng+Brakmo+Mathis+Raghavan+Dukkipati+Chu+Terzis+Herbert,http://research.google.com/pubs/pub41316.html
Estimating Uncertainty for Massive Data Streams,Google (2012),2012,Nicholas Chamandy Omkar Muralidharan Amir Najmi Siddartha Naidu,@techreport{43157 title = {Estimating Uncertainty for Massive Data Streams} author = {Nicholas Chamandy and Omkar Muralidharan and Amir Najmi and Siddartha Naidu} year = 2012 institution = {Google} },We address the problem of estimating the variability of an estimator computed from a massive data stream. While nearly-linear statistics can be computed exactly or approximately from “Google- scale” data second-order analysis is a challenge. Unfortunately massive sample sizes do not obviate the need for uncertainty calculations: modern data often have heavy tails large coefficients of variation tiny effect sizes and generally exhibit bad behaviour. We describe in detail this New Frontier in statistics outline the computing infrastructure required and motivate the need for modification of existing methods. We introduce two procedures for basic uncertainty estimation one derived from the bootstrap and the other from a form of subsampling. Their costs and theoretical properties are briefly discussed and their use is demonstrated using Google data.,http://research.google.com/pubs/archive/43157.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Estimating+Uncertainty+for+Massive+Data+Streams+chamandy+Muralidharan+Najmi+Naidu,http://research.google.com/pubs/pub43157.html
Object views: Fine-grained sharing in browsers,Proceedings of the International Conference on World Wide Web World Wide Web Consortium (2010),2010,Leo Meyerovich Adrienne Felt Mark S. Miller,@inproceedings{36273 title = {Object views: Fine-grained sharing in browsers} author = {Leo Meyerovich and Adrienne Felt and Mark S. Miller} year = 2010 URL = {http://www.cs.berkeley.edu/~afelt/views-www-2010.pdf} booktitle = {Proceedings of the International Conference on World Wide Web} },Browsers do not currently support the secure sharing of JavaScript objects between principals. We present this problem as the need for object views which are consistent and controllable versions of objects. Multiple views can be made for the same object and customized for the recipients. We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts. Developers can control the fine-grained behavior of objects with an aspect system that accepts programmatic policies. The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation. To facilitate simple document sharing we build a policy system for declaratively defining policies for document object views. Notably our document policy system makes it possible to hide elements without breaking document structure invariants. We discuss how object views can be deployed in two settings: same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja and inter-origin sharing between browser frames over a message-passing channel.,http://research.google.com/pubs/archive/36273.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Object+views:+Fine-grained+sharing+in+browsers+Meyerovich+Felt+Miller,http://research.google.com/pubs/pub36273.html
JustSpeak: Enabling Universal Voice Control on Android,W4A 2014,2014,Yu Zhong T. V. Raman Casey Burkhardt Fadi Biadsy Jeffrey P. Bigham,@inproceedings{41924 title = {JustSpeak: Enabling Universal Voice Control on Android} author = {Yu Zhong and T. V. Raman and Casey Burkhardt and Fadi Biadsy and Jeffrey P. Bigham} year = 2014 URL = {http://dl.acm.org/citation.cfm?id=2596720} booktitle = {W4A 2014} },In this paper we introduce JustSpeak a universal voice control solution for non-visual access to the Android operating system. JustSpeak offers two contributions as compared to existing systems. First it enables system wide voice control on Android that can accommodate any application. JustSpeak constructs the set of available voice commands based on application context; these commands are directly synthesized from on-screen labels and accessibility metadata and require no further intervention from the application developer. Second it provides more efficient and natural interaction with support of multiple voice commands in the same utterance. We present the system design of JustSpeak and describe its utility in various use cases. We then discuss the system level supports required by a service like JustSpeak on other platforms. By eliminating the target locating and pointing tasks JustSpeak can significantly improve experience of graphic interface interaction for blind and motion-impaired users.,http://research.google.com/pubs/archive/41924.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=JustSpeak:+Enabling+Universal+Voice+Control+on+Android+Zhong+Raman+Burkhardt+Biadsy+Bigham,http://research.google.com/pubs/pub41924.html
Trustworthy Proxies: Virtualizing Objects with Invariants,ECOOP 2013,2013,Tom Van Cutsem Mark S. Miller,@inproceedings{40736 title = {Trustworthy Proxies: Virtualizing Objects with Invariants} author = {Tom Van Cutsem and Mark S. Miller} year = 2013 URL = {http://soft.vub.ac.be/Publications/2013/vub-soft-tr-13-03.pdf} booktitle = {ECOOP 2013} },Proxies are a common technique to virtualize objects in object-oriented languages. A proxy is a placeholder object that emulates or wraps another target object. Both the proxy's representation and behavior may differ substantially from that of its target object. In many object-oriented languages objects may have language-enforced invariants associated with them. For instance an object may declare immutable fields which are guaranteed to point to the same value throughout the execution of the program. Clients of an object can blindly rely on these invariants as they are enforced by the language. In a language with both proxies and objects with invariants these features interact. Can a proxy emulate or replace a target object purporting to uphold such invariants? If yes does the client of the proxy need to trust the proxy to uphold these invariants or are they still enforced by the language? This paper sheds light on these questions in the context of a Javascript-like language and describes the design of a Proxy API that allows proxies to emulate objects with invariants yet have these invariants continue to be language-enforced. This design forms the basis of proxies in ECMAScript 6.,http://research.google.com/pubs/archive/40736.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trustworthy+Proxies:+Virtualizing+Objects+with+Invariants+Cutsem+Miller,http://research.google.com/pubs/pub40736.html
Adapting the Tesseract Open Source OCR Engine for Multilingual OCR,MOCR '09: Proceedings of the International Workshop on Multilingual OCR (2009),2009,Ray Smith Daria Antonova Dar-Shyang Lee,@inproceedings{35248 title = {Adapting the Tesseract Open Source OCR Engine for Multilingual OCR} author = {Ray Smith and Daria Antonova and Dar-Shyang Lee} year = 2009 URL = {http://doi.acm.org/10/1145/1577802.1577804} booktitle = {MOCR '09: Proceedings of the International Workshop on Multilingual OCR} },We describe efforts to adapt the Tesseract open source OCR engine for multiple scripts and languages. Effort has been concentrated on enabling generic multi-lingual operation such that negligible customization is required for a new language beyond providing a corpus of text. Although change was required to various modules including physical layout analysis and linguistic post-processing no change was required to the character classifier beyond changing a few limits. The Tesseract classifier has adapted easily to Simplified Chinese. Test results on English a mixture of European languages and Russian taken from a random sample of books show a reasonably consistent word error rate between 3.72% and 5.78% and Simplified Chinese has a character error rate of only 3.77%. ©ACM 2009. This is the authors’ version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in Proceedings of the International Workshop on Multilingual OCR 2009 Barcelona Spain July 25 2009.,http://research.google.com/pubs/archive/35248.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adapting+the+Tesseract+Open+Source+OCR+Engine+for+Multilingual+OCR+Smith+Antonova+Lee,http://research.google.com/pubs/pub35248.html
Capirca,Blackhat USA (2011) (to appear),2011,Paul (Tony) Watson,@misc{41836 title = {Capirca} author = {Paul (Tony) Watson} year = 2011 URL = {http://www.blackhat.com/html/bh-us-11/bh-us-11-arsenal.html#Watson} },Capirca is an open-sourced cross-platform network security policy compiler developed at Google. It allows the creation and deployment of ACL filters across multiple target platforms based on a single security policy and shared network and service definitions. The software is ideal for both small and large organizations to eliminate common errors while greatly simplifying security policy maintenance.,http://www.blackhat.com/html/bh-us-11/bh-us-11-arsenal.html#Watson,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Capirca+Watson,http://research.google.com/pubs/pub41836.html
Enlisting the Ghost: Modeling Empty Categories for Machine Translation,Proceedings of ACL ACL (2013) pp. 822-831,2013,Bing Xiang Xiaoqiang Luo Bowen Zhou,@inproceedings{41389 title = {Enlisting the Ghost: Modeling Empty Categories for Machine Translation} author = {Bing Xiang and Xiaoqiang Luo and Bowen Zhou} year = 2013 URL = {http://www.aclweb.org/anthology/P/P13/P13-1081.pdf} booktitle = {Proceedings of ACL} pages = {822-831} },Empty categories (EC) are artiﬁcial elements in Penn Treebanks motivated by the government-binding (GB) theory to explain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treatment of ECs by ﬁrst recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches including the extraction of EC-speciﬁc sparse features. We show that the recovered empty categories not only improve the word alignment quality but also lead to signiﬁcant improvements in a large-scale state-of-the-art syntactic MT system.,http://research.google.com/pubs/archive/41389.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Enlisting+the+Ghost:+Modeling+Empty+Categories+for+Machine+Translation+Xiang+Luo+Zhou,http://research.google.com/pubs/pub41389.html
Multi-Armed Recommendation Bandits for Selecting State Machine Policies for Robotic Systems,Proceedings of International Conference on Robotics and Automation (ICRA 2013),2013,Pyry Matikainen P. Michael Furlong Rahul Sukthankar Martial Hebert,@inproceedings{40750 title = {Multi-Armed Recommendation Bandits for Selecting State Machine Policies for Robotic Systems} author = {Pyry Matikainen and P. Michael Furlong and Rahul Sukthankar and Martial Hebert} year = 2013 booktitle = {Proceedings of International Conference on Robotics and Automation (ICRA 2013)} },We investigate the problem of selecting a state-machine from a library to control a robot. We are particularly interested in this problem when evaluating such state machines on a particular robotics task is expensive. As a motivating example we consider a problem where a simulated vacuuming robot must select a driving state machine well-suited for a particular (unknown) room layout. By borrowing concepts from collaborative filtering (recommender systems such as Netflix and Amazon.com) we present a multi-armed bandit formulation that incorporates recommendation techniques to efficiently select state machines for individual room layouts. We show that this formulation outperforms the individual approaches (recommendation multi-armed bandits) as well as the baseline of selecting the `average best' state machine across all rooms.,http://research.google.com/pubs/archive/40750.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multi-Armed+Recommendation+Bandits+for+Selecting+State+Machine+Policies+for+Robotic+Systems+Matikainen+Furlong+Sukthankar+Hebert,http://research.google.com/pubs/pub40750.html
IsoMatch: Creating Informative Grid Layouts,Computer Graphics Forum (Proceedings of Eurographics) vol. 34(2) (2015) (to appear),2015,Ohad Fried Stephen DiVerdi Maciej Halber Elena Sizikova Adam Finkelstein,@article{43467 title = {IsoMatch: Creating Informative Grid Layouts} author = {Ohad Fried and Stephen DiVerdi and Maciej Halber and Elena Sizikova and Adam Finkelstein} year = 2015 URL = {http://gfx.cs.princeton.edu/pubs/Fried_2015_ICI/index.php} journal = {Computer Graphics Forum (Proceedings of Eurographics)} volume = {34(2)} },Collections of objects such as images are often presented visually in a grid because it is a compact representation that lends itself well for search and exploration. Most grid layouts are sorted using very basic criteria such as date or filename. In this work we present a method to arrange collections of objects respecting an arbitrary distance measure. Pairwise distances are preserved as much as possible while still producing the specific target arrangement which may be a 2D grid the surface of a sphere a hierarchy or any other shape. We show that our method can be used for infographics collection exploration summarization data visualization and even for solving problems such as where to seat family members at a wedding. We present a fast algorithm that can work on large collections and quantitatively evaluate how well distances are preserved.,http://research.google.com/pubs/archive/43467.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=IsoMatch:+Creating+Informative+Grid+Layouts+Fried+DiVerdi+Halber+Sizikova+Finkelstein,http://research.google.com/pubs/pub43467.html
Silicon Photonics Technologies: Gaps Analysis for Datacenter Interconnects,Silicon Photonics III Springer (2016),2016,Ryohei Urata Hong Liu Lieven Verslegers Chris Johnson,@inbook{44816 title = {Silicon Photonics Technologies: Gaps Analysis for Datacenter Interconnects} author = {Ryohei Urata and Hong Liu and Lieven Verslegers and Chris Johnson} year = 2016 URL = {http://www.springer.com/us/book/9783642105029#aboutBook} booktitle = {Silicon Photonics III} },We give an overview of optical interconnect requirements for large scale datacenters. We then make a comparison between silicon photonics technologies and more traditional options in meeting these requirements.,http://www.springer.com/us/book/9783642105029#aboutBook,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Silicon+Photonics+Technologies:+Gaps+Analysis+for+Datacenter+Interconnects+Urata+Liu+Verslegers+Johnson,http://research.google.com/pubs/pub44816.html
Recognition of Complex Events: Exploiting Temporal Dynamics between Underlying Concepts,Proceedings of International Computer Vision and Pattern Recognition (CVPR 2014) IEEE,2014,Subhabrata Bhattacharya Mahdi M. Kalayeh Rahul Sukthankar Mubarak Shah,@inproceedings{42456 title = {Recognition of Complex Events: Exploiting Temporal Dynamics between Underlying Concepts} author = {Subhabrata Bhattacharya and Mahdi M. Kalayeh and Rahul Sukthankar and Mubarak Shah} year = 2014 booktitle = {Proceedings of International Computer Vision and Pattern Recognition (CVPR 2014)} },While approaches based on bags of features excel at low-level action classification they are ill-suited for recognizing complex events in video where concept-based temporal representations currently dominate. This paper proposes a novel representation that captures the temporal dynamics of windowed mid-level concept detectors in order to improve complex event recognition. We first express each video as an ordered vector time series where each time step consists of the vector formed from the concatenated confidences of the pre-trained concept detectors. We hypothesize that the dynamics of time series for different instances from the same event class as captured by simple linear dynamical system (LDS) models are likely to be similar even if the instances differ in terms of low-level visual features. We propose a two-part representation composed of fusing: (1) a singular value decomposition of block Hankel matrices (SSID-S) and (2) a harmonic signature (H-S) computed from the corresponding eigen-dynamics matrix. The proposed method offers several benefits over alternate approaches: our approach is straightforward to implement directly employs existing concept detectors and can be plugged into linear classification frameworks. Results on standard datasets such as NIST's TRECVID Multimedia Event Detection task demonstrate the improved accuracy of the proposed method.,http://research.google.com/pubs/archive/42456.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recognition+of+Complex+Events:+Exploiting+Temporal+Dynamics+between+Underlying+Concepts+Bhattacharya+Kalayeh+Sukthankar+Shah,http://research.google.com/pubs/pub42456.html
ESOMAR/GRBN Online research guideline,Esomar Esomar (2015),2015,Reg Baker Peter Milla Mario Callegaro Melanie Courtright Brian Fine Philippe Guilbert Debrah Harding Kathy Joe Jackie Lorch Bruno Paro Efrain Ribeiro Alina Serbanica,@techreport{44647 title = {ESOMAR/GRBN Online research guideline} author = {Reg Baker and Peter Milla and Mario Callegaro and Melanie Courtright and Brian Fine and Philippe Guilbert and Debrah Harding and Kathy Joe and Jackie Lorch and Bruno Paro and Efrain Ribeiro and Alina Serbanica} year = 2015 URL = {https://www.esomar.org/knowledge-and-standards/codes-and-guidelines/online-research-guideline.php} institution = {Esomar} },This ESOMAR/GRBN Online Research Guideline is designed to help researchers address legal ethical and practical considerations in using new technologies when conducting research online and is an update of guidance issued in 2011. To ensure that it is in line with most recent practice in addition to other updated sections this new draft Guideline also contains: New guidance on passive data collection requirements A new section on incentives sweepstakes and free prize draws A new section on sample source and management An updated section on specific online technologies such as tracking cloud storage and static and dynamic IDs,https://www.esomar.org/knowledge-and-standards/codes-and-guidelines/online-research-guideline.php,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ESOMAR/GRBN+Online+research+guideline+Baker+Milla+Callegaro+Courtright+Fine+Guilbert+Harding+Joe+Lorch+Paro+Ribeiro+Serbanica,http://research.google.com/pubs/pub44647.html
A QCQP Approach to Triangulation,European Conference on Computer Vision Springer Verlag (2012),2012,Chris Aholt Rekha Thomas Sameer Agarwal,@inproceedings{40603 title = {A QCQP Approach to Triangulation} author = {Chris Aholt and Rekha Thomas and Sameer Agarwal} year = 2012 URL = {http://homes.cs.washington.edu/~sagarwal/aat.pdf} booktitle = {European Conference on Computer Vision} },Triangulation of a three-dimensional point from n >=2 two-dimensional images can be formulated as a quadratically constrained quadratic program. We propose an algorithm to extract candidate solutions to this problem from its semidefinite programming relaxations. We then describe a sucient condition and a polynomial time test for certifying when such a solution is optimal. This test has no false positives. Experiments indicate that false negatives are rare and the algorithm has excellent performance in practice. We explain this phenomenon in terms of the geometry of the triangulation problem.,http://homes.cs.washington.edu/~sagarwal/aat.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+QCQP+Approach+to+Triangulation+Aholt+Thomas+Agarwal,http://research.google.com/pubs/pub40603.html
High Performance Datacenter Networks: Architectures Algorithms and Opportunities,Morgan & Claypool San Rafael California (2011),2011,Dennis Abts John Kim,@book{37069 title = {High Performance Datacenter Networks: Architectures Algorithms and Opportunities} author = {Dennis Abts and John Kim} year = 2011 URL = {http://dx.doi.org/10.2200/S00341ED1V01Y201103CAC014} booktitle = {High Performance Datacenter Networks: Architectures Algorithms and Opportunities} address = {San Rafael California} },"Datacenter networks provide the communication substrate for large parallel computer systems that form the ecosystem for high performance computing (HPC) systems and modern Internet applications. The design of new datacenter networks is motivated by an array of applications ranging from communication intensive climatology complex material simulations and molecular dynamics to such Internet applications as Web search language translation collaborative Internet applications streaming video and voice-over-IP. For both Supercomputing and Cloud Computing the network enables distributed applications to communicate and interoperate in an orchestrated and efficient way. This book describes the design and engineering tradeoffs of datacenter networks. It describes interconnection networks from topology and network architecture to routing algorithms and presents opportunities for taking advantage of the emerging technology trends that are influencing router microarchitecture. With the emergence of ""many-core"" processor chips it is evident that we will also need ""many-port"" routing chips to provide a bandwidth-rich network to avoid the performance limiting effects of Amdahl's Law. We provide an overview of conventional topologies and their routing algorithms and show how technology signaling rates and cost-effective optics are motivating new network topologies that scale up to millions of hosts. The book also provides detailed case studies of two high performance parallel computer systems and their networks.",http://research.google.com/pubs/archive/37069.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=High+Performance+Datacenter+Networks:+Architectures+Algorithms+and+Opportunities+Abts+Kim,http://research.google.com/pubs/pub37069.html
V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity Joins of Multisets and Vectors,PVLDB Proceedings of the VLDB Endowment vol. 5 (2012) pp. 704-715,2012,Ahmed Metwally Christos Faloutsos,@article{37740 title = {V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity Joins of Multisets and Vectors} author = {Ahmed Metwally and Christos Faloutsos} year = 2012 journal = {PVLDB Proceedings of the VLDB Endowment} pages = {704--715} volume = {5} },This work proposes V-SMART-Join a scalable MapReduce-based framework for discovering all pairs of similar entities. The V-SMART-Join framework is applicable to sets multisets and vectors. V-SMART-Join is motivated by the observed skew in the underlying distributions of Internet traffic and is a family of 2-stage algorithms where the first stage computes and joins the partial results and the second stage computes the similarity exactly for all candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in the number of entities as well as their cardinalities. They were up to 30 times faster than the state of the art algorithm VCL when compared on a real dataset of a small size. We also established the scalability of the proposed algorithms by running them on a dataset of a realistic size on which VCL never succeeded to finish. Experiments were run using real datasets of IPs and cookies where each IP is represented as a multiset of cookies and the goal is to discover similar IPs to identify Internet proxies.,http://research.google.com/pubs/archive/37740.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=V-SMART-Join:+A+Scalable+MapReduce+Framework+for+All-Pair+Similarity+Joins+of+Multisets+and+Vectors+Metwally+Faloutsos,http://research.google.com/pubs/pub37740.html
No-Regret Algorithms for Unconstrained Online Convex Optimization,Advances in Neural Information Processing Systems (NIPS) (2012),2012,Matthew Streeter H. Brendan McMahan,@inproceedings{40564 title = {No-Regret Algorithms for Unconstrained Online Convex Optimization} author = {Matthew Streeter and H. Brendan McMahan} year = 2012 URL = {http://arxiv.org/abs/1211.2260} booktitle = {Advances in Neural Information Processing Systems (NIPS)} },Some of the most compelling applications of online convex optimization including online prediction and classification are unconstrained: the natural feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance. We present algorithms that without such prior knowledge offer near-optimal regret bounds with respect to any choice of x*. In particular regret with respect to x* = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.,http://research.google.com/pubs/archive/40564.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=No-Regret+Algorithms+for+Unconstrained+Online+Convex+Optimization+Streeter+McMahan,http://research.google.com/pubs/pub40564.html
RFC7535 - AS112 Redirection Using DNAME,IETF RFCs Internet Engineering Task Force (2015) pp. 16,2015,Warren Kumari Joe Abley Brian Dickson George Michaelson,@incollection{43822 title = {RFC7535 - AS112 Redirection Using DNAME} author = {Warren Kumari and Joe Abley and Brian Dickson and George Michaelson} year = 2015 URL = {http://www.rfc-editor.org/rfc/rfc7535.txt} booktitle = {IETF RFCs} pages = {16} },AS112 provides a mechanism for handling reverse lookups on IP addresses that are not unique (e.g. RFC 1918 addresses). This document describes modifications to the deployment and use of AS112 infrastructure that will allow zones to be added and dropped much more easily using DNAME resource records. This approach makes it possible for any DNS zone administrator to sink traffic relating to parts of the global DNS namespace under their control to the AS112 infrastructure without coordination with the operators of AS112 infrastructure.,http://research.google.com/pubs/archive/43822.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7535+-+AS112+Redirection+Using+DNAME+Kumari+Abley+Dickson+Michaelson,http://research.google.com/pubs/pub43822.html
AngularJS,O'Reilly (2013) pp. 196,2013,Brad Green Shyam Seshadri,@book{41445 title = {AngularJS} author = {Brad Green and Shyam Seshadri} year = 2013 URL = {http://www.amazon.com/AngularJS-Brad-Green/dp/1449344852} pages = {196} },Develop smaller lighter web apps that are simple to create and easy to test extend and maintain as they grow. This hands-on guide introduces you to AngularJS the open source JavaScript framework that uses Model–view–controller (MVC) architecture data binding client-side templates and dependency injection to create a much-needed structure for building web apps. Guided by two engineers who worked on AngularJS at Google you’ll walk through the framework’s key features and then build a working AngularJS app—from layout to testing compiling and debugging. You’ll learn how AngularJS helps reduce the complexity of your web app. Dive deep into Angular’s building blocks and learn how they work together Gain maximum flexibility by separating logic data and presentation responsibilities with MVC Assemble your full app in the browser using client-side templates Use AngularJS directives to extend HTML with declarative syntax Communicate with the server and implement simple caching with the $http service Use dependency injection to improve refactoring testability and multiple environment design Get code samples for common problems you face in most web apps,http://www.amazon.com/AngularJS-Brad-Green/dp/1449344852,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=AngularJS+Green+Seshadri,http://research.google.com/pubs/pub41445.html
Going Deeper with Convolutions,CVPR 2015,2015,Christian Szegedy Wei Liu Yangqing Jia Pierre Sermanet Scott Reed Dragomir Anguelov Dumitru Erhan Vincent Vanhoucke Andrew Rabinovich,@inproceedings{43022 title = {Going Deeper with Convolutions} author = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich} year = 2015 URL = {http://arxiv.org/abs/1409.4842} booktitle = {CVPR 2015} },We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design we increased the depth and width of the network while keeping the computational budget constant. To optimize quality the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture GoogLeNet a 22 layers deep network was used to assess its quality in the context of object detection and classification.,http://research.google.com/pubs/archive/43022.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Going+Deeper+with+Convolutions+Szegedy+Liu+Jia+Sermanet+Reed+Anguelov+Erhan+Vanhoucke+Rabinovich,http://research.google.com/pubs/pub43022.html
Secrets Lies and Account Recovery: Lessons from the Use of Personal Knowledge Questions at Google,WWW'15 - Proceedings of the 22nd international conference on World Wide Web ACM (2015),2015,Joseph Bonneau Elie Bursztein Ilan Caron Rob Jackson Mike Williamson,@inproceedings{43783 title = {Secrets Lies and Account Recovery: Lessons from the Use of Personal Knowledge Questions at Google} author = {Joseph Bonneau and Elie Bursztein and Ilan Caron and Rob Jackson and Mike Williamson} year = 2015 booktitle = {WWW'15 - Proceedings of the 22nd international conference on World Wide Web} },"We examine the first large real-world data set on personal knowledge question's security and memorability from their deployment at Google. Our analysis confirms that secret questions generally offer a security level that is far lower than user-chosen passwords. It turns out to be even lower than proxies such as the real distribution of surnames in the population would indicate. Surprisingly we found that a significant cause of this insecurity is that users often don't answer truthfully. A user survey we conducted revealed that a significant fraction of users (37%) who admitted to providing fake answers did so in an attempt to make them ""harder to guess"" although on aggregate this behavior had the opposite effect as people ""harden"" their answers in a predictable way. On the usability side we show that secret answers have surprisingly poor memorability despite the assumption that reliability motivates their continued deployment. From millions of account recovery attempts we observed a significant fraction of users (e.g 40\% of our English-speaking US users) were unable to recall their answers when needed. This is lower than the success rate of alternative recovery mechanisms such as SMS reset codes (over 80%). Comparing question strength and memorability reveals that the questions that are potentially the most secure (e.g what is your first phone number) are also the ones with the worst memorability. We conclude that it appears next to impossible to find secret questions that are both secure and memorable. Secret questions continue have some use when combined with other signals but they should not be used alone and best practice should favor more reliable alternatives.",http://research.google.com/pubs/archive/43783.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Secrets+Lies+and+Account+Recovery:+Lessons+from+the+Use+of+Personal+Knowledge+Questions+at+Google+Bonneau+Bursztein+Caron+Jackson+Williamson,http://research.google.com/pubs/pub43783.html
Portable and Performant Userspace SCTP Stack,Computer Communications and Networks (ICCCN) 2012 21st International Conference on IEEE,2012,Brad Penoff Alan Wagner Michael Tuexen Irene Ruengeler,@inproceedings{40282 title = {Portable and Performant Userspace SCTP Stack} author = {Brad Penoff and Alan Wagner and Michael Tuexen and Irene Ruengeler} year = 2012 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6289222} booktitle = {Computer Communications and Networks (ICCCN) 2012 21st International Conference on} },One of only two new transport protocols introduced in the last 30 years is the Stream Control Transmission Protocol (SCTP). SCTP enables capabilities like additional throughput and fault tolerance for multihomed hosts. An SCTP implementation is included with the Linux kernel and another implementation called sctplib functions successfully in userspace on several platforms but unfortunately neither of these implementations have all of the latest features nor do they perform as well as the FreeBSD kernel implementation of SCTP. We were motivated to produce a portable implementation of the FreeBSD kernel SCTP stack that operates in userspace of any system because of both our desires to obtain a higher performance SCTP stack for Linux as well as to exploit recent developments in hardware virtualization and transport protocol onloading. Unlike any other userspace transport implementation for TCP or SCTP our userspace SCTP stack simultaneously achieves similar throughput and latency as the Linux kernel TCP stack without compromising on any of the transport's features as well as maintaining true portability across multiple operating systems and devices. We create a callback API and implement a threshold to control its usage; our userspace SCTP stack with these optimizations obtains higher throughput than the Linux kernel implementation of SCTP. We describe our userspace SCTP stack's design and demonstrate how it gives similar throughput and latency on Linux as the kernel TCP implementation with the benefits of the new features of SCTP.,http://research.google.com/pubs/archive/40282.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Portable+and+Performant+Userspace+SCTP+Stack+Penoff+Wagner+Tuexen+Ruengeler,http://research.google.com/pubs/pub40282.html
Science in the Cloud,IEEE Internet Computing (2012),2012,Joseph L Hellerstein Kai Kohlhoff David E. Konerding,@article{37684 title = {Science in the Cloud} author = {Joseph L Hellerstein and Kai Kohlhoff and David E. Konerding} year = 2012 journal = {IEEE Internet Computing} },Scientific discovery is in transition from a focus on data collection to an emphasis on analysis and prediction using large scale computation. These computations can be done with unused cycles in commercial Clouds if there is appropriate software support. Moving science into the Cloud will promote data sharing and collaborations that will accelerate scientific discovery in the 21st century.,http://research.google.com/pubs/archive/37684.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Science+in+the+Cloud+Hellerstein+Kohlhoff+Konerding,http://research.google.com/pubs/pub37684.html
Best-response dynamics out of sync: complexity and characterization,EC ACM (2013) pp. 379-396,2013,Roee Engelberg Alex Fabrikant Michael Schapira David Wajc,@inproceedings{42473 title = {Best-response dynamics out of sync: complexity and characterization} author = {Roee Engelberg and Alex Fabrikant and Michael Schapira and David Wajc} year = 2013 booktitle = {EC} pages = {379--396} },"In many computational and economic models of multi-agent interaction each participant repeatedly ""best-responds"" to the others' actions. Game theory research on the prominent ""best-response dynamics"" model typically relies on the premise that the interaction between agents is somehow synchronized. However in many real-life settings e.g. internet protocols and large-scale markets the interaction between participants is asynchronous. We tackle the following important questions: (1) When are best-response dynamics guaranteed to converge to an equilibrium even under asynchrony? (2) What is the (computational and communication) complexity of verifying guaranteed convergence? We show that in general verifying guaranteed convergence is intractable. In fact our main negative result establishes that this task is undecidable. We exhibit in contrast positive results for several environments of interest including complete computationally-tractable characterizations of convergent systems. We discuss the algorithmic implications of our results which extend beyond best-response dynamics to applications such as asynchronous Boolean circuits.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Best-response+dynamics+out+of+sync:+complexity+and+characterization+Engelberg+Fabrikant+Schapira+Wajc,http://research.google.com/pubs/pub42473.html
Crowdsourcing Event Detection in YouTube Videos,Detection Representation and Exploitation of Events in the Semantic Web (DeRiVE 2011) Bonn Germany,2011,Thomas Steiner Ruben Verborgh Rik Van de Walle Michael Hausenblas Joaquim Gabarro,@inproceedings{37424 title = {Crowdsourcing Event Detection in YouTube Videos} author = {Thomas Steiner and Ruben Verborgh and Rik Van de Walle and Michael Hausenblas and Joaquim Gabarro} year = 2011 URL = {http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-779/derive2011_submission_8.pdf} booktitle = {Detection Representation and Exploitation of Events in the Semantic Web (DeRiVE 2011)} address = {Bonn Germany} },Considerable efforts have been put into making video content on the Web more accessible searchable and navigable by research on both textual and visual analysis of the actual video content and the accompanying metadata. Nevertheless most of the time videos are opaque objects in websites. With Web browsers gaining more support for the HTML5 element videos are becoming first class citizens on the Web. In this paper we show how events can be detected on-the-fly through crowdsourcing (i) textual (ii) visual and (iii) behavioral analysis in YouTube videos at scale. The main contribution of this paper is a generic crowdsourcing framework for automatic and scalable semantic annotations of HTML5 videos. Eventually we discuss our preliminary results using traditional server-based approaches to video event detection as a baseline.,http://research.google.com/pubs/archive/37424.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Crowdsourcing+Event+Detection+in+YouTube+Videos+Steiner+Verborgh+Van+de+Walle+Hausenblas+Gabarro,http://research.google.com/pubs/pub37424.html
RealBrush: Painting with Examples of Physical Media,ACM Transactions on Graphics (TOG) - SIGGRAPH 2013 Conference Proceedings vol. 32 no. 4 (2013) 117:1-117:12,2013,Jingwan Lu Connelly Barnes Stephen DiVerdi Adam Finkelstein,@article{41462 title = {RealBrush: Painting with Examples of Physical Media} author = {Jingwan Lu and Connelly Barnes and Stephen DiVerdi and Adam Finkelstein} year = 2013 journal = {ACM Transactions on Graphics (TOG) -- SIGGRAPH 2013 Conference Proceedings} pages = {117:1--117:12} volume = {32 no. 4} },Conventional digital painting systems rely on procedural rules and physical simulation to render paint strokes. We present an interactive data-driven painting system that uses scanned images of real natural media to synthesize both new strokes and complex stroke interactions obviating the need for physical simulation. First users capture images of real media including examples of isolated strokes pairs of overlapping strokes and smudged strokes. Online the user inputs a new stroke path and our system synthesizes its 2D texture appearance with optional smearing or smudging when strokes overlap. We demonstrate high-fidelity paintings that closely resemble the captured media style and also quantitatively evaluate our synthesis quality via user studies.,http://research.google.com/pubs/archive/41462.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RealBrush:+Painting+with+Examples+of+Physical+Media+Lu+Barnes+DiVerdi+Finkelstein,http://research.google.com/pubs/pub41462.html
Network Utilization: The Flow View,IEEE INFOCOM 2013 IEEE Turin Italy,2013,Avinatan Hassidim Danny Raz Michal Segalov Ariel Shaqed (Scolnicov),@inproceedings{41315 title = {Network Utilization: The Flow View} author = {Avinatan Hassidim and Danny Raz and Michal Segalov and Ariel Shaqed (Scolnicov)} year = 2013 booktitle = {IEEE INFOCOM 2013} address = {Turin Italy} },Building and operating a large backbone network can take months or even years and it requires a substantial investment. Therefore there is an economical drive to increase the utilization of network resources (links switches etc.) in order to improve the cost efﬁciency of the network. At the same time the utilization of network components has a direct impact on the performance of the network and its resilience to failure and thus operational considerations are a critical aspect of the decision regarding the desired network load and utilization. However the actual utilization of the network resources is not easy to predict or control. It depends on many parameters like the trafﬁc demand and the routing scheme (or Trafﬁc Engineering if deployed) and it varies over time and space. As a result it is very difﬁcult to actually deﬁne real network utilization and to understand the reasons for this utilization. In this paper we introduce a novel way to look at the network utilization. Unlike traditional approaches that consider the average link utilization we take the ﬂow perspective and consider the network utilization in terms of the growth potential of the ﬂows in the network. After deﬁning this new Flow Utilization and discussing how it differs from common deﬁnitions of network utilization we study ways to efﬁciently compute it over large networks. We then show using real backbone data that Flow Utilization is very useful in identifying network state and evaluating performance of TE algorithms.,http://research.google.com/pubs/archive/41315.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Network+Utilization:+The+Flow+View+Hassidim+Raz+Segalov+Shaqed+(Scolnicov),http://research.google.com/pubs/pub41315.html
RFC5635 - Remote Triggered Black Hole filtering with uRPF,IETF IETF (2009),2009,Warren Kumari,@incollection{35216 title = {RFC5635 - Remote Triggered Black Hole filtering with uRPF} author = {Warren Kumari} year = 2009 URL = {http://www.ietf.org/internet-drafts/draft-ietf-opsec-blackhole-urpf-03.txt} booktitle = {IETF} },Remote Triggered Black Hole (RTBH) filtering is a popular and effective technique for the mitigation of denial-of-service attacks. This document expands upon destination-based RTBH filtering by outlining a method to enable filtering by source address as well.,http://research.google.com/pubs/archive/35216.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC5635+-+Remote+Triggered+Black+Hole+filtering+with+uRPF+Kumari,http://research.google.com/pubs/pub35216.html
Automated Analysis of Security-Critical JavaScript APIs,IEEE Symposium on Security & Privacy (SP) IEEE (2011),2011,Ankur Taly Úlfar Erlingsson John C. Mitchell Mark S. Miller Jasvir Nagra,@inproceedings{37199 title = {Automated Analysis of Security-Critical JavaScript APIs} author = {Ankur Taly and Úlfar Erlingsson and John C. Mitchell and Mark S. Miller and Jasvir Nagra} year = 2011 URL = {http://www-cs-students.stanford.edu/~ataly/Papers/sp11.pdf} booktitle = {IEEE Symposium on Security & Privacy (SP)} },JavaScript is widely used to provide client-side functionality in Web applications. To provide services ranging from maps to advertisements Web applications may incorporate untrusted JavaScript code from third parties. The trusted portion of each application may then expose an API to untrusted code interposing a reference monitor that mediates access to security-critical resources. However a JavaScript reference monitor can only be effective if it cannot be circum- vented through programming tricks or programming language idiosyncracies. In order to verify complete mediation of critical resources for applications of interest we deﬁne the semantics of a restricted version of JavaScript devised by the ECMA Standards committee for isolation purposes and develop and test an automated tool that can soundly establish that a given API cannot be circumvented or subverted. Our tool reveals a previously-undiscovered vulnerability in the widely-examined Yahoo! ADsafe ﬁlter and veriﬁes conﬁnement of the repaired ﬁlter and other examples from the Object-Capability literature.,http://research.google.com/pubs/archive/37199.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automated+Analysis+of+Security-Critical+JavaScript+APIs+Taly+Erlingsson+Mitchell+Miller+Nagra,http://research.google.com/pubs/pub37199.html
Representative Skylines using Threshold-based Preference Distributions,International Conference on Data Engineering (ICDE) (2011),2011,Atish Das Sarma Ashwin Lall Danupon Nanongkai Richard J. Lipton Jim Xu,@inproceedings{36988 title = {Representative Skylines using Threshold-based Preference Distributions} author = {Atish Das Sarma and Ashwin Lall and Danupon Nanongkai and Richard J. Lipton and Jim Xu} year = 2011 booktitle = {International Conference on Data Engineering (ICDE)} },The study of skylines and their variants has received considerable attention in recent years. Skylines are essentially sets of most interesting (undominated) tuples in a database. However since the skyline is often very large much research effort has been devoted to identifying a smaller subset of (say k) “representative skyline” points. Several different definitions of representative skylines have been considered. Most of these formulations are intuitive in that they try to achieve some kind of clustering “spread” over the entire skyline with k points. In this work we take a more principled approach in defining the representative skyline objective. One of our main contributions is to formulate the problem of displaying k representative skyline points such that the probability that a random user would click on one of them is maximized. Two major research questions arise naturally from this formulation. First how does one mathematically model the likelihood with which a user is interested in and will “click” on a certain tuple? Second how does one negotiate the absence of the knowledge of an explicit set of target users; in particular what do we mean by “a random user”? To answer the first question we model users based on a novel formulation of threshold preferences which we will motivate further in the paper. To answer the second question we assume a probability distribution of users instead of a fixed set of users. While this makes the problem harder it lends more mathematical structures that can be exploited as well as one can now work with probabilities of thresholds and handle cumulative density functions. On the theoretical front our objective is NP-hard. For the case of a finite set of users with known thresholds we present a simple greedy algorithm that attains an approximation ratio of (1 _ 1/e) of the optimal. For the case of user distributions we show that a careful yet similar greedy algorithm achieves the same approximation ratio. Unfortunately it turns out that this algorithm is rather involved and computationally expensive. So we present a threshold sampling based algorithm that is more computationally affordable and for any fixed epsilon > 0 has an approximation ratio of (1 _ 1/e _ epsilon). We perform experiments on both real and synthetic data to show that our algorithm significantly outperforms previously proposed approaches.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Representative+Skylines+using+Threshold-based+Preference+Distributions+Das+Sarma+Lall+Nanongkai+Lipton+Xu,http://research.google.com/pubs/pub36988.html
Confidence-Weighted Linear Classification,International Conference on Machine Learning (ICML) (2008),2008,Mark Dredze Koby Crammer Fernando Pereira,@inproceedings{34667 title = {Confidence-Weighted Linear Classification} author = {Mark Dredze and Koby Crammer and Fernando Pereira} year = 2008 booktitle = {International Conference on Machine Learning (ICML)} },We introduce confidence-weighted linear classifiers which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods learns faster in the online setting and lends itself to better classifier combination after parallel training.,http://research.google.com/pubs/archive/34667.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Confidence-Weighted+Linear+Classification+Dredze+Crammer+Pereira,http://research.google.com/pubs/pub34667.html
Sistema GaitGrabber na captação de dados cinemáticos durante a marcha,Motriz: Revista de Educação Física vol. 18 (2012) pp. 505-514,2012,Scott Alexander Kirkwood,@article{42957 title = {Sistema GaitGrabber na captação de dados cinemáticos durante a marcha} author = {Scott Alexander Kirkwood} year = 2012 URL = {http://www.scielo.br/pdf/motriz/v18n3/a11v18n3.pdf} journal = {Motriz: Revista de Educação Física} pages = {505-514} volume = {18} },The purpose of this study was to develop and test the validity and reliability of the GaitGrabber System in measuring kinematic variables in the sagittal plane during gait. Eighteen individuals participated in the reliability study and 28 in the concurrent validity study. The Qualisys Pro-Reflex System used as a gold standard reference system. The GaitGrabber calculates the relative angles at the hip knee and ankle in the sagittal plane. The Intraclass Correlation Coefficient (ICC) was used to compare the average of the angular peaks between visits. The principal component analysis was used to test the validity of the system. The ICC ranged from moderate to excellent and the validity of the system was proved for the ankle. There were significant differences in the range of motion for the hip and knee joints which were attributed to different instrumental characteristics. The GaitGrabber system is valid and reliable and can be clinically used to analyze kinematics during gait in the sagittal plane.,http://www.scielo.br/pdf/motriz/v18n3/a11v18n3.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sistema+GaitGrabber+na+capta%C3%A7%C3%A3o+de+dados+cinem%C3%A1ticos+durante+a+marcha+Kirkwood,http://research.google.com/pubs/pub42957.html
T(ether): Spatially-Aware Handhelds Gestures and Proprioception for Multi-User 3D Modeling and Animation,ACM Symposium on Spatial User Interaction ACM (2014) pp. 90-93,2014,Dávid Lakatos Matthew Blackshaw Alex Olwal Zachary Barryte Ken Perlin Hiroshi Ishii,@inproceedings{43152 title = {T(ether): Spatially-Aware Handhelds Gestures and Proprioception for Multi-User 3D Modeling and Animation} author = {Dávid Lakatos and Matthew Blackshaw and Alex Olwal and Zachary Barryte and Ken Perlin and Hiroshi Ishii} year = 2014 URL = {http://olwal.com/#proprioception_and_gestures_in_3d_space} booktitle = {ACM Symposium on Spatial User Interaction} pages = {90-93} },T(ether) is a spatially-aware display system for multi-user collaborative manipulation and animation of virtual 3D objects. The handheld display acts as a window into virtual reality providing users with a perspective view of 3D data. T(ether) tracks users' heads hands fingers and pinching in addition to a handheld touch screen to enable rich interaction with the virtual scene. We introduce gestural interaction techniques that exploit proprioception to adapt the UI based on the hand's position above behind or on the surface of the display. These spatial interactions use a tangible frame of reference to help users manipulate and animate the model in addition to controlling environment properties. We report on initial user observations from an experiment for 3D modeling which indicate T(ether)'s potential for embodied viewport control and 3D modeling interactions.,http://research.google.com/pubs/archive/43152.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=T(ether):+Spatially-Aware+Handhelds+Gestures+and+Proprioception+for+Multi-User+3D+Modeling+and+Animation+Lakatos+Blackshaw+Olwal+Barryte+Perlin+Ishii,http://research.google.com/pubs/pub43152.html
Personalized Speech Recognition On Mobile Devices,Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2016) (to appear),2016,Ian McGraw Rohit Prabhavalkar Raziel Alvarez Montse Gonzalez Arenas Kanishka Rao David Rybach Ouais Alsharif Hasim Sak Alexander Gruenstein Françoise Beaufays Carolina Parada,@inproceedings{44631 title = {Personalized Speech Recognition On Mobile Devices} author = {Ian McGraw and Rohit Prabhavalkar and Raziel Alvarez and Montse Gonzalez Arenas and Kanishka Rao and David Rybach and Ouais Alsharif and Hasim Sak and Alexander Gruenstein and Françoise Beaufays and Carolina Parada} year = 2016 booktitle = {Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)} },We describe a large vocabulary speech recognition system that is accurate has low latency and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets and further reduce its memory footprint using an SVD-based compression scheme. Additionally we minimize our memory footprint by using a single language model for both dictation and voice command domains constructed using Bayesian interpolation. Finally in order to properly handle device-specific information such as proper names and other context-dependent information we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5% word error rate on an open-ended dictation task running with a median speed that is seven times faster than real-time.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Personalized+Speech+Recognition+On+Mobile+Devices+McGraw+Prabhavalkar+Alvarez+Arenas+Rao+Rybach+Alsharif+Sak+Gruenstein+Beaufays+Parada,http://research.google.com/pubs/pub44631.html
How Many Millennials Visit YouTube? Estimating Unobserved Events From Incomplete Panel Data Conditioned on Demographic Covariates,TBD Google Inc. (2015) pp. 1-27 (to appear),2015,Georg M. Goerg Yuxue Jin Nicolas Remy Jim Koehler,@techreport{43451 title = {How Many Millennials Visit YouTube? Estimating Unobserved Events From Incomplete Panel Data Conditioned on Demographic Covariates} author = {Georg M. Goerg and Yuxue Jin and Nicolas Remy and Jim Koehler} year = 2015 institution = {Google Inc.} },Many socio-economic studies rely on panel data as they also provide detailed demographic information about consumers. For example advertisers use TV and web metering panels to estimate ads effectiveness in selected target demographics. However panels often record only a fraction of all events due to non-registered devices technical problems or work usage. Goerg et al. (2015) present a beta-binomial negative-binomial hurdle (BBNBH) model to impute missing events in count data with excess zeros. In this work we study empirical properties of the MLE for the BBNBH model extend it to categorical covariates introduce a penalized maximum likelihood estimator (MLE) to get accurate estimates by demographic group and apply the methodology to a German media panel to learn about demographic patterns in the YouTube viewership.,http://research.google.com/pubs/archive/43451.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Many+Millennials+Visit+YouTube%3F+Estimating+Unobserved+Events+From+Incomplete+Panel+Data+Conditioned+on+Demographic+Covariates+Goerg+Jin+Remy+Koehler,http://research.google.com/pubs/pub43451.html
Modeling similarity in the age of data,MAA (2009),2009,Kevin S. McCurley,@misc{36728 title = {Modeling similarity in the age of data} author = {Kevin S. McCurley} year = 2009 URL = {http://maa.org/news/120309mccurley.html} },The process of applying mathematics to the real world is undergoing a radical change through our ability to gather data at a massive scale. This is particularly true at Google where we routinely process petabytes of human language and interact with many millions of users. In this talk I describe some surprising realizations that arose from this data while trying to improve part of our search quality. It turns out that everything I thought I knew about similarity was wrong and I should have been talking to psychologists.,http://maa.org/news/120309mccurley.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Modeling+similarity+in+the+age+of+data+McCurley,http://research.google.com/pubs/pub36728.html
Trickle: Rate Limiting YouTube Video Streaming,Proceedings of the USENIX Annual Technical Conference (2012) pp. 6,2012,Monia Ghobadi Yuchung Cheng Ankur Jain Matt Mathis,@inproceedings{38103 title = {Trickle: Rate Limiting YouTube Video Streaming} author = {Monia Ghobadi and Yuchung Cheng and Ankur Jain and Matt Mathis} year = 2012 booktitle = {Proceedings of the USENIX Annual Technical Conference} pages = {6} },YouTube traffic is bursty. These bursts trigger packet losses and stress router queues causing TCP’s congestion-control algorithm to kick in. In this paper we introduce Trickle a server-side mechanism that uses TCP to rate limit YouTube video streaming. Trickle paces the video stream by placing an upper bound on TCP’s congestion window as a function of the streaming rate and the round-trip time. We evaluated Trickle on YouTube production data centers in Europe and India and analyzed its impact on losses bandwidth RTT and video buffer under-run events. The results show that Trickle reduces the average TCP loss rate by up to 43% and the average RTT by up to 28% while maintaining the streaming rate requested by the application.,http://research.google.com/pubs/archive/38103.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trickle:+Rate+Limiting+YouTube+Video+Streaming+Ghobadi+Cheng+Jain+Mathis,http://research.google.com/pubs/pub38103.html
From Research to Practice: Experiences Engineering a Production Metadata Database for a Scale Out File System,Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 2014) USENIX,2014,Charles Johnson Kimberly Keeton Charles B. Morrey III Craig A. N. Soules Alistair Veitch Stephen Bacon Oskar Batuner Marcelo Condotta Hamilton Coutinho Patrick J. Doyle Rafael Eichelberger Hugo Kiehl Guilherme Magalhaes James McEvoy Padmanabhan Nagarajan Patrick Osborne Joaquim Souza Andy Sparkes Mike Spitzer Sebastien Tandel Lincoln Thomas Sebastian Zangaro,@inproceedings{42191 title = {From Research to Practice: Experiences Engineering a Production Metadata Database for a Scale Out File System} author = {Charles Johnson and Kimberly Keeton and Charles B. Morrey III and Craig A. N. Soules and Alistair Veitch and Stephen Bacon and Oskar Batuner and Marcelo Condotta and Hamilton Coutinho and Patrick J. Doyle and Rafael Eichelberger and Hugo Kiehl and Guilherme Magalhaes and James McEvoy and Padmanabhan Nagarajan and Patrick Osborne and Joaquim Souza and Andy Sparkes and Mike Spitzer and Sebastien Tandel and Lincoln Thomas and Sebastian Zangaro} year = 2014 URL = {https://www.usenix.org/system/files/conference/fast14/fast14-paper_johnson.pdf} booktitle = {Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 2014)} },HP’s StoreAll with Express Query is a scalable commercial file archiving product that offers sophisticated file metadata management and search capabilities. A new REST API enables fast efficient searching to find all files that meet a given set of metadata criteria and the ability to tag files with custom metadata fields. The product brings together two significant systems: a scale out file system and a metadata database based on LazyBase. In designing and building the combined product we identified several real-world issues in using a pipelined database system in a distributed environment and overcame several interesting design challenges that were not contemplated by the original research prototype. This paper highlights our experiences.,https://www.usenix.org/system/files/conference/fast14/fast14-paper_johnson.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=From+Research+to+Practice:+Experiences+Engineering+a+Production+Metadata+Database+for+a+Scale+Out+File+System+Johnson+Keeton+III+Soules+Veitch+Bacon+Batuner+Condotta+Coutinho+Doyle+Eichelberger+Kiehl+Magalhaes+McEvoy+Nagarajan+Osborne+Souza+Sparkes+Spitzer+Tandel+Thomas+Zangaro,http://research.google.com/pubs/pub42191.html
Uncertainty in Aggregate Estimates from Sampled Distributed Traces,2012 Workshop on Managing Systems Automatically and Dynamically USENIX,2012,Nate Coehlo Arif Merchant Murray Stokely,@inproceedings{40378 title = {Uncertainty in Aggregate Estimates from Sampled Distributed Traces} author = {Nate Coehlo and Arif Merchant and Murray Stokely} year = 2012 booktitle = {2012 Workshop on Managing Systems Automatically and Dynamically} },Tracing mechanisms in distributed systems give important insight into system properties and are usually sampled to control overhead. At Google Dapper [8] is the always-on system for distributed tracing and performance analysis and it samples fractions of all RPC trafﬁc. Due to difﬁcult implementation excessive data volume or a lack of perfect foresight there are times when system quantities of interest have not been measured directly and Dapper samples can be aggregated to estimate those quantities in the short or long term. Here we ﬁnd unbiased variance estimates of linear statistics over RPCs taking into account all layers of sampling that occur in Dapper and allowing us to quantify the sampling uncertainty in the aggregate estimates. We apply this methodology to the problem of assigning jobs and data to Google datacenters using estimates of the resulting cross-datacenter trafﬁc as an optimization criterion and also to the detection of change points in access patterns to certain data partitions.,http://research.google.com/pubs/archive/40378.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Uncertainty+in+Aggregate+Estimates+from+Sampled+Distributed+Traces+Coehlo+Merchant+Stokely,http://research.google.com/pubs/pub40378.html
BwE: Flexible Hierarchical Bandwidth Allocation for WAN Distributed Computing,Sigcomm '15 Google Inc (2015),2015,Alok Kumar Sushant Jain Uday Naik Nikhil Kasinadhuni Enrique Cauich Zermeno C. Stephen Gunn Jing Ai Björn Carlin Mihai Amarandei-Stavila Mathieu Robin Aspi Siganporia Stephen Stuart Amin Vahdat,@inproceedings{43838 title = {BwE: Flexible Hierarchical Bandwidth Allocation for WAN Distributed Computing} author = {Alok Kumar and Sushant Jain and Uday Naik and Nikhil Kasinadhuni and Enrique Cauich Zermeno and C. Stephen Gunn and Jing Ai and Björn Carlin and Mihai Amarandei-Stavila and Mathieu Robin and Aspi Siganporia and Stephen Stuart and Amin Vahdat} year = 2015 booktitle = {Sigcomm '15} },WAN bandwidth remains a constrained resource that is economically infeasible to substantially overprovision. Hence it is important to allocate capacity according to service priority and based on the incremental value of additional allocation. For example it may be the highest priority for one service to receive 10Gb/s of bandwidth but upon reaching such an allocation incremental priority may drop sharply favoring allocation to other services. Motivated by the observation that individual flows with fixed priority may not be the ideal basis for bandwidth allocation we present the design and implementation of Bandwidth Enforcer (BwE) a global hierarchical bandwidth allocation infrastructure. BwE supports: i) service-level bandwidth allocation following prioritized bandwidth functions where a service can represent an arbitrary collection of flows ii) independent allocation and delegation policies according to user-defined hierarchy all accounting for a global view of bandwidth and failure conditions iii) multi-path forwarding common in traffic-engineered networks and iv) a central administrative point to override (perhaps faulty) policy during exceptional conditions. BwE has delivered more service-efficient bandwidth utilization and simpler management in production for multiple years.,http://research.google.com/pubs/archive/43838.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=BwE:+Flexible+Hierarchical+Bandwidth+Allocation+for+WAN+Distributed+Computing+Kumar+Jain+Naik+Kasinadhuni+Cauich+Zermeno+Gunn+Ai+Carlin+Amarandei-Stavila+Robin+Siganporia+Stuart+Vahdat,http://research.google.com/pubs/pub43838.html
The Politics of Search: A Decade Retrospective.,The Information Society Journal vol. 26 (2010) pp. 364-374,2010,Laura Ann Granka,@article{36914 title = {The Politics of Search: A Decade Retrospective.} author = {Laura Ann Granka} year = 2010 URL = {http://www.informaworld.com/smpp/content~content=a927317720~db=all~jumptype=rss} journal = {The Information Society Journal} pages = {364-374} volume = {26} },In “Shaping theWeb:Why the Politics of Search Engines Matters” Introna and Nissenbaum (2000) introduced scholars to the political as well as technical issues central to the development of online search engines. Since that time scholars have critically evaluated the role that search engines play in structuring the scope of online information access for the rest of society with an emphasis on the implications for a democratic and diverseWeb. This article describes the thought behind search engine regulation online diversity and information bias and it places these issues within the context of the technical and societal changes that have occurred in the online search industry. The author assesses which of the initial concerns expressed about online search engines remain relevant today and discusses how technical changes demand a new approach to measuring online diversity and democracy. The author concludes with a proposal to direct the research and thought in online search going forward.,http://research.google.com/pubs/archive/36914.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Politics+of+Search:+A+Decade+Retrospective.+Granka,http://research.google.com/pubs/pub36914.html
What can performance counters do for memory subsystem analysis?,ACM SIGPLAN Workshop on Memory Systems Performance & Correctness (MSPC'08) ACM Seattle (2008) pp. 26-30,2008,Stephane Eranian,@inproceedings{33526 title = {What can performance counters do for memory subsystem analysis?} author = {Stephane Eranian} year = 2008 URL = {http://doi.acm.org/10.1145/1353522.1353531} booktitle = {ACM SIGPLAN Workshop on Memory Systems Performance & Correctness (MSPC'08)} pages = {26--30} address = {Seattle} },Nowadays all major processors provide a set of performance counters which capture micro-architectural level information such as the number of elapsed cycles cache misses or instructions executed. Counters can be found in processor cores processor die chipsets or in I/O cards. They can provide a wealth of information as to how the hardware is being used by software. Many processors now support events to measure precisely and with very limited overhead the traffic between a core and the memory subsystem. It is possible to compute average load latency and bus bandwidth utilization. This valuable information can be used to improve code quality and placement of threads to maximize hardware utilization.,http://doi.acm.org/10.1145/1353522.1353531,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=What+can+performance+counters+do+for+memory+subsystem+analysis%3F+Eranian,http://research.google.com/pubs/pub33526.html
Smart Pricing Grows the Pie,Google Inc (2012),2012,Guy Calvert,@misc{38097 title = {Smart Pricing Grows the Pie} author = {Guy Calvert} year = 2012 },Some publisher advertising networks provide features intended to help advertisers bid more efficiently with a single bid in many publishers’ click auctions at once – Smart Pricing on the Google Display Network is one example. Typically such features involve discounting advertiser bids or prices for clicks on publisher websites according to how click values vary across sites (for some appropriate measure of advertiser value). Contrary to concerns that such features necessarily result in reduced publisher (and network) revenue we find that in many simple cases the modified auction dynamics produce rational incentives for advertisers to bid more – and spend more – than they would without the benefit of these features. So if advertisers act in their own interest then publishers and networks stand to make more revenue as well.,http://research.google.com/pubs/archive/38097.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Smart+Pricing+Grows+the+Pie+Calvert,http://research.google.com/pubs/pub38097.html
Wireless Techniques in Optical Transport,Proceedings of the 14th Optoelectronics and Communications Conference (2009),2009,Cedric F. Lam,@inproceedings{35476 title = {Wireless Techniques in Optical Transport} author = {Cedric F. Lam} year = 2009 booktitle = {Proceedings of the 14th Optoelectronics and Communications Conference} },Abstract The field of optical communications is undergoing a transformation from analog to digital. Advanced signal processing techniques which have been widely used in wireless communications and local access loops are now being applied to long haul optical transmission networks. In this paper we discuss the implications of such transformations and postulate a new paradigm for optical transport in future high speed optical backbone networks.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Wireless+Techniques+in+Optical+Transport+Lam,http://research.google.com/pubs/pub35476.html
Children's Roles Using Keyword Search Interfaces in the Home,Proceedings of CHI 2010 ACM Press,2010,Allison Druin Elizabeth Foss Hilary Hutchinson Evan Golub Leshell Hatley,@inproceedings{36300 title = {Children's Roles Using Keyword Search Interfaces in the Home} author = {Allison Druin and Elizabeth Foss and Hilary Hutchinson and Evan Golub and Leshell Hatley} year = 2010 booktitle = {Proceedings of CHI 2010} },Children want to find information about their world but there are barriers to finding what they seek. Young people have varying abilities to formulate multi-step queries and comprehend search results. Challenges in understanding where to type confusion about what tools are available and frustration with how to parse the results page all have led to a lack of perceived search success for children 7-11 years old. In this paper we describe seven search roles children display as information seekers using Internet keyword interfaces based on a home study of 83 children ages 7 9 and 11. These roles are defined not only by the children’s search actions but also by who influences their searching their perceived success and trends in age and gender. These roles suggest a need for new interfaces that expand the notion of keywords scaffold results and develop a search culture among children.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Children's+Roles+Using+Keyword+Search+Interfaces+in+the+Home+Druin+Foss+Hutchinson+Golub+Hatley,http://research.google.com/pubs/pub36300.html
Towards better measurement of attention and satisfaction in mobile search,SIGIR '14 Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval (2014) pp. 113-122,2014,Dmitry Lagun Dale Webster Chih-Hung Hsieh Vidhya Navalpakkam,@inproceedings{43224 title = {Towards better measurement of attention and satisfaction in mobile search} author = {Dmitry Lagun and Dale Webster and Chih-Hung Hsieh and Vidhya Navalpakkam} year = 2014 booktitle = {SIGIR '14 Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval} pages = {113-122} },Web Search has seen two big changes recently: rapid growth in mobile search traffic and an increasing trend towards providing answer-like results for relatively simple information needs (e.g. [weather today]). Such results display the answer or relevant information on the search page itself without requiring a user to click. While clicks on organic search results have been used extensively to infer result relevance and search satisfaction clicks on answer-like results are often rare (or meaningless) making it challenging to evaluate answer quality. Together these call for better measurement and understanding of search satisfaction on mobile devices. In this paper we studied whether tracking the browser viewport (visible portion of a web page) on mobile phones could enable accurate measurement of user attention at scale and provide good measurement of search satisfaction in the absence of clicks. Focusing on answer-like results in web search we designed a lab study to systematically vary answer presence and relevance (to the user's information need) obtained satisfaction ratings from users and simultaneously recorded eye gaze and viewport data as users performed search tasks. Using this ground truth we identified increased scrolling past answer and increased time below answer as clear measurable signals of user dissatisfaction with answers. While the viewport may contain three to four results at any given time we found strong correlations between gaze duration and viewport duration on a per result basis and that the average user attention is focused on the top half of the phone screen suggesting that we may be able to scalably and reliably identify which specific result the user is looking at from viewport data alone.,http://research.google.com/pubs/archive/43224.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Towards+better+measurement+of+attention+and+satisfaction+in+mobile+search+Lagun+Webster+Hsieh+Navalpakkam,http://research.google.com/pubs/pub43224.html
In-Memory Performance for Big Data,Proceedings of the VLDB Endowment vol. 8 (2014) pp. 37-48,2014,Goetz Graefe Haris Volos Hideaki Kimura Harumi Kuno Joseph Tucek Mark Lillibridge Alistair Veitch,@article{43985 title = {In-Memory Performance for Big Data} author = {Goetz Graefe and Haris Volos and Hideaki Kimura and Harumi Kuno and Joseph Tucek and Mark Lillibridge and Alistair Veitch} year = 2014 URL = {http://dl.acm.org/citation.cfm?id=2735465} journal = {Proceedings of the VLDB Endowment} pages = {37--48} volume = {8} },"When a working set fits into memory the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However despite the large memory available with modern hardware data skew shifting workloads and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts we enable buffer pool designs to match in-memory performance while supporting the ""big data"" workloads that continue to require secondary storage thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects) and uses it to practically eliminate buffer pool overheads for memoryresident data. Our implementation and experimental evaluation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the buffer pool size and graceful improvement when the working set shrinks towards and below the memory and buffer pool sizes.",http://research.google.com/pubs/archive/43985.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=In-Memory+Performance+for+Big+Data+Graefe+Volos+Kimura+Kuno+Tucek+Lillibridge+Veitch,http://research.google.com/pubs/pub43985.html
An Unexceptional Implementation of First-Class Continuations,Proceedings of the 2009 International Lisp Conference Association of Lisp Users 1938 East Beech Road Sterling Virginia 20164 pp. 36-40,2009,Joseph Marshall,@inproceedings{35161 title = {An Unexceptional Implementation of First-Class Continuations} author = {Joseph Marshall} year = 2009 booktitle = {Proceedings of the 2009 International Lisp Conference} pages = {36--40} address = {1938 East Beech Road Sterling Virginia 20164} },Describes how to implement first-class continuations on a virtual machine that does not support stack inspection. Unlike previous work use of the exception handler is avoided. Measurements demonstrate acceptable performance.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Unexceptional+Implementation+of+First-Class+Continuations+Marshall,http://research.google.com/pubs/pub35161.html
Machine learning: a probabilistic perspective,MIT Press Cambridge MA (2012),2012,Kevin P Murphy,@book{38136 title = {Machine learning: a probabilistic perspective} author = {Kevin P Murphy} year = 2012 address = {Cambridge MA} },Today’s Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning using a unified probabilistic approach. The coverage combines breadth and depth offering necessary background material on such topics as probability optimization and linear algebra as well as discussion of recent developments in the field including conditional random fields L1 regularization and deep learning. The book is written in an informal accessible style complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology text processing computer vision and robotics. Rather than providing a cookbook of different heuristic methods the book stresses a principled model-based approach often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.,http://research.google.com/pubs/archive/38136.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+learning:+a+probabilistic+perspective+Murphy,http://research.google.com/pubs/pub38136.html
VisualRank: Applying PageRank to Large-Scale Image Search,IEEE Transactions on Pattern Analysis and Machine Intelligence vol. 30 (2008) pp. 1877-1890,2008,Yushi Jing Shumeet Baluja,@article{34634 title = {VisualRank: Applying PageRank to Large-Scale Image Search} author = {Yushi Jing and Shumeet Baluja} year = 2008 URL = {http://www.kevinjing.com/jing_pami.pdf} journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} pages = {1877-1890} volume = {30} },Because of the relative ease in understanding and processing text commercial image-search systems often rely on techniques that are largely indistinguishable from text search. Recently academic studies have demonstrated the effectiveness of employing image-based features to provide either alternative or additional signals to use in this process. However it remains uncertain whether such techniques will generalize to a large number of popular Web queries and whether the potential improvement to search quality warrants the additional computational cost. In this work we cast the image-ranking problem into the task of identifying “authority” nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be “authorities” are chosen as those that answer the image-queries well. To understand the performance of such an approach in a real system we conducted a series of large-scale experiments based on the task of retrieving images for 2000 of the most popular products queries. Our experimental results show significant improvement in terms of user satisfaction and relevancy in comparison to the most recent Google Image Search results. Maintaining modest computational cost is vital to ensuring that this procedure can be used in practice; we describe the techniques required to make this system practical for large-scale deployment in commercial search engines.,http://www.kevinjing.com/jing_pami.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=VisualRank:+Applying+PageRank+to+Large-Scale+Image+Search+Jing+Baluja,http://research.google.com/pubs/pub34634.html
PowerNap: An Energy Efficient MAC Layer for Random Routing in Wireless Sensor Networks,IEEE SECON 2011,2011,Onur Soysal Sami Ayyorgun Murat Demirbas,@inproceedings{37222 title = {PowerNap: An Energy Efficient MAC Layer for Random Routing in Wireless Sensor Networks} author = {Onur Soysal and Sami Ayyorgun and Murat Demirbas} year = 2011 booktitle = {IEEE SECON 2011} },Idle-listening is the biggest challenge for energyefﬁciency and longevity of multihop wireless sensor network (WSN) deployments. While existing coordinated sleep/wakeup scheduling protocols eliminate idle-listening for simple trafﬁc patterns they are unsuitable to handle the complex trafﬁc patterns of the random routing protocols. We present a novel coordinated sleep/wakeup protocol POWERNAP  which avoids the overhead of distributing complex large sleep/wakeup scheduling information to the nodes. POWERNAP piggybacks onto the relayed data packets the seed of the pseudo-random generator that encodes the scheduling information and enables any recipient/snooper to calculate its sleep/wakeup schedule from this seed. In essence POWERNAP trades off doing extra computation in order to avoid expensive control packet transmissions. We show through simulations and real implementation on TelosB motes that POWERNAP eliminates the idle-listening problem efﬁciently and achieves selfstabilizing low-latency and low-cost relaying of data packets for random routing protocols.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=PowerNap:+An+Energy+Efficient+MAC+Layer+for+Random+Routing+in+Wireless+Sensor+Networks+Soysal+Ayyorgun+Demirbas,http://research.google.com/pubs/pub37222.html
LaDeDa: Languages for Debuggable Distributed Algorithms,Lada 2012: Workshop on Languages for Distributed Algorithms (to appear),2012,Mark S. Miller Tom Van Cutsem,@incollection{37626 title = {LaDeDa: Languages for Debuggable Distributed Algorithms} author = {Mark S. Miller and Tom Van Cutsem} year = 2012 booktitle = {Lada 2012: Workshop on Languages for Distributed Algorithms} },When programming language designs are presented the examples are almost exclusively of correct programs. Most attention of programming language designers is indeed on the beauty and elegance of correct programs. For incorrect programs great design attention is paid to catching errors early---such as fancy static type systems---so that many incorrect programs are never run. Due to the success of these efforts many programs are either correct or inadmissible conserving on the need for programmer attention. As a result most of the attention working programmers spend looking at code is spent debugging incorrect running code. Often this is code written by others and only partially understood. What properties should such code have? How can programming language design encourage incorrect programs to have those properties that facilitate debugging? Distributed programs introduce additional difficult bugs of a different character. How should distributed language design facilitate the debugging of distributed programs? We explain how these considerations have affected four distributed language designs (E AmbientTalk Joe-E/Waterken Dr. SES) and one distributed debugging tool (Causeway).,http://research.google.com/pubs/archive/37626.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=LaDeDa:+Languages+for+Debuggable+Distributed+Algorithms+Miller+Cutsem,http://research.google.com/pubs/pub37626.html
Troubleshooting PON networks effectively with Carrier-grade Ethernet and WDM-PON,IEEE Communications Magazine vol. 52 (2014) S7-S13,2014,Rafael Sanchez Jose Alberto Hernandez David Larrabeiti,@article{42528 title = {Troubleshooting PON networks effectively with Carrier-grade Ethernet and WDM-PON} author = {Rafael Sanchez and Jose Alberto Hernandez and David Larrabeiti} year = 2014 URL = {http://adscom.it.uc3m.es/app/webroot/files/Papers_Adscom/David/all.pdf} journal = {IEEE Communications Magazine} pages = {S7-S13} volume = {52} },WDM-PONs have recently emerged to provide dedicated and separated point-to-point wavelengths to individual Optical Network Units (ONTs). In addition the recently standardised Ethernet OAM capabilities under the IEEE 802.1ag standard and the ITU-T Y.1731 recommendation together with state-of-the-art Optical Time-Domain Reflectometry (OTDR) provide new link-layer and physical tools for the effective troubleshooting of WDM-PONs. This article proposes an Integrated Troubleshooting Box (ITB) for the effectively combination of both physical and link-layer information into an effective and efficient set of management procedures for WDM-PONs. We show its applicability in a number of realistic troubleshooting scenarios including failure situations involving either the feeder fibre one of its branches and even Ethernet links after the ONT.,http://research.google.com/pubs/archive/42528.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Troubleshooting+PON+networks+effectively+with+Carrier-grade+Ethernet+and+WDM-PON+Sanchez+Hernandez+Larrabeiti,http://research.google.com/pubs/pub42528.html
Return of Gonzo Gizmos,Chicago Review Press 814 North Franklin Street Chicago Illinois 60610 (2006) pp. 1-147,2006,Simon Quellen Field,@book{28009 title = {Return of Gonzo Gizmos} author = {Simon Quellen Field} year = 2006 pages = {1--147} address = {814 North Franklin Street Chicago Illinois 60610} },Book Description This fresh collection of more than 20 science projects—from hydrogen fuel cells to computer-controlled radio transmitters—is perfect for the tireless tinkerer. Innovative activities include taking detailed plant cell photographs through a microscope using a disposable camera; building a rocket engine out of aluminum foil paper clips and kitchen matches; and constructing a geodesic dome out of gumdrops and barbecue skewers. Organized by scientific topic each chapter includes explanations of the physics chemistry biology or mathematics behind the projects. Most of the devices can be built using common household products or components available at hardware or electronic stores and each experiment contains illustrated step-by-step instructions with photographs and diagrams that make construction easy. No workbench warrior science teacher or grown-up geek should be without this idea-filled resource.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Return+of+Gonzo+Gizmos+Field,http://research.google.com/pubs/pub28009.html
An Argument for Increasing TCP's Initial Congestion Window,ACM SIGCOMM Computer Communications Review vol. 40 (2010) pp. 27-33,2010,Nandita Dukkipati Tiziana Refice Yuchung Cheng Jerry Chu Tom Herbert Amit Agarwal Arvind Jain Natalia Sutin,@article{36640 title = {An Argument for Increasing TCP's Initial Congestion Window} author = {Nandita Dukkipati and Tiziana Refice and Yuchung Cheng and Jerry Chu and Tom Herbert and Amit Agarwal and Arvind Jain and Natalia Sutin} year = 2010 URL = {http://ccr.sigcomm.org/drupal/?q=node/621} journal = {ACM SIGCOMM Computer Communications Review} pages = {27--33} volume = {40} },TCP flows start with an initial congestion window of at most four segments or approximately 4KB of data. Because most Web transactions are short-lived the initial congestion window is a critical TCP parameter in determining how quickly flows can finish. While the global network access speeds increased dramatically on average in the past decade the standard value of TCP’s initial congestion window has remained unchanged. In this paper we propose to increase TCP’s initial congestion window to at least ten segments (about 15KB). Through large-scale Internet experiments we quantify the latency benefits and costs of using a larger window as functions of network bandwidth round-trip time (RTT) bandwidthdelay product (BDP) and nature of applications. We show that the average latency of HTTP responses improved by approximately 10% with the largest benefits being demonstrated in high RTT and BDP networks. The latency of low bandwidth networks also improved by a significant amount in our experiments. The average retransmission rate increased by a modest 0.5% with most of the increase coming from applications that effectively circumvent TCP’s slow start algorithm by using multiple concurrent connections. Based on the results from our experiments we believe the initial congestion window should be at least ten segments and the same be investigated for standardization by the IETF.,http://research.google.com/pubs/archive/36640.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Argument+for+Increasing+TCP's+Initial+Congestion+Window+Dukkipati+Refice+Cheng+Chu+Herbert+Agarwal+Jain+Sutin,http://research.google.com/pubs/pub36640.html
Hostload prediction in a Google compute cloud with a Bayesian model,Supercomputing 2012,2012,Sheng Di Derrick Kondo Walfredo Cirne,@inproceedings{42553 title = {Hostload prediction in a Google compute cloud with a Bayesian model} author = {Sheng Di and Derrick Kondo and Walfredo Cirne} year = 2012 booktitle = {Supercomputing 2012} },Prediction of host load in Cloud systems is crit- ical for achieving service-level agreements. However accurate prediction of host load in Clouds is extremely challenging because it fluctuates drastically at small timescales. We design a prediction method based on Bayes model to predict the mean load over a long-term time interval as well as the mean load in consecutive future time intervals. We identify novel predictive features of host load that capture the expectation predictabil- ity trends and patterns of host load. We also determine the most effective combinations of these features for prediction. We evaluate our method using a detailed one-month trace of a Google data center with thousands of machines. Experiments show that the Bayes method achieves high accuracy with a mean squared error of 0.0014. Moreover the Bayes method improves the load prediction accuracy by 5.6-50% compared to other state-of-the-art methods based on moving averages auto-regression and/or noise filters.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hostload+prediction+in+a+Google+compute+cloud+with+a+Bayesian+model+Di+Kondo+Cirne,http://research.google.com/pubs/pub42553.html
Causeway: a message-oriented distributed debugger,HP Labs (2009),2009,Terry Stanley Tyler Close Mark S. Miller,@techreport{35127 title = {Causeway: a message-oriented distributed debugger} author = {Terry Stanley and Tyler Close and Mark S. Miller} year = 2009 URL = {http://www.hpl.hp.com/techreports/2009/HPL-2009-78.html} note = {HP Labs tech report HPL-2009-78} institution = {HP Labs} },An increasing number of developers face the difficult task of debugging distributed asynchronous programs. This trend has outpaced the development of adequate debugging tools and currently the best option for many is an ad hoc patchwork of sequential tools and printf debugging. This paper presents Causeway a postmortem distributed debugger that demonstrates a novel approach to understanding the behavior of a distributed program. Our message-oriented approach borrows an effective strategy from sequential debugging: To find the source of unintended side- effects start with the chain of expressed intentions. We show how Causeway's integrated views - describing both distributed and sequential computation - help users navigate causal pathways as they pursue suspicions. We highlight Causeway's innovative features which include adaptive customizable event abstraction mechanisms and graphical views that follow message flow across process and machine boundaries.,http://research.google.com/pubs/archive/35127.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Causeway:+a+message-oriented+distributed+debugger+Stanley+Close+Miller,http://research.google.com/pubs/pub35127.html
Optimal Size Freshness and Time-frame for Voice Search Vocabulary,Google (2012),2012,Maryam Kamvar Ciprian Chelba,@techreport{40492 title = {Optimal Size Freshness and Time-frame for Voice Search Vocabulary} author = {Maryam Kamvar and Ciprian Chelba} year = 2012 institution = {Google} },In this paper we investigate how to optimize the vocabulary for a voice search language model. The metric we optimize over is the out-of-vocabulary (OoV) rate since it is a strong indicator of user experience. In a departure from the usual way of measuring OoV rates web search logs allow us to compute the per-session OoV rate and thus estimate the percentage of users that experience a given OoV rate. Under very conservative text normalization we ﬁnd that a voice search vocabulary consisting of 2 to 2.5M words extracted from 1 week of search query data will result in an aggregate OoV rate of 0.01; at that size the same OoV rate will also be experienced by 90% of users. The number of words included in the vocabulary is a stable indicator of the OoV rate. Altering the freshness of the vocabulary or the duration of the time window over which the training data is gathered does not signiﬁcantly change the OoV rate. Surprisingly a signiﬁcantly larger vocabulary (approx. 10 million words) is required to guarantee OoV rates below 0.01 (1%) for 95% of the users.,http://research.google.com/pubs/archive/40492.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimal+Size+Freshness+and+Time-frame+for+Voice+Search+Vocabulary+Kamvar+Chelba,http://research.google.com/pubs/pub40492.html
Janus: Optimal Flash Provisioning for Cloud Storage Workloads,Proceedings of the USENIX Annual Technical Conference USENIX Advanced Computing System Association 2560 Ninth Street Suite 215 Berkeley CA 94710 USA (2013) pp. 91-102,2013,Christoph Albrecht Arif Merchant Murray Stokely Muhammad Waliji Francois Labelle Nathan Coehlo Xudong Shi Eric Schrock,@inproceedings{41179 title = {Janus: Optimal Flash Provisioning for Cloud Storage Workloads} author = {Christoph Albrecht and Arif Merchant and Murray Stokely and Muhammad Waliji and Francois Labelle and Nathan Coehlo and Xudong Shi and Eric Schrock} year = 2013 URL = {https://www.usenix.org/system/files/conference/atc13/atc13-albrecht.pdf} booktitle = {Proceedings of the USENIX Annual Technical Conference} pages = {91--102} address = {2560 Ninth Street Suite 215 Berkeley CA 94710 USA} },Janus is a system for partitioning the ﬂash storage tier between workloads in a cloud-scale distributed ﬁle system with two tiers ﬂash storage and disk. The ﬁle system stores newly created ﬁles in the ﬂash tier and moves them to the disk tier using either a First-In-First-Out (FIFO) policy or a Least-Recently-Used (LRU) policy subject to per-workload allocations. Janus constructs compact metrics of the cacheability of the different workloads using sampled distributed traces because of the large scale of the system. From these metrics we formulate and solve an optimization problem to determine the ﬂash allocation to workloads that maximizes the total reads sent to the ﬂash tier subject to operator-set priorities and bounds on ﬂash write rates. Using measurements from production workloads in multiple data centers using these recommendations as well as traces of other production workloads we show that the resulting allocation improves the ﬂash hit rate by 47–76% compared to a uniﬁed tier shared by all workloads. Based on these results and an analysis of several thousand production workloads we conclude that ﬂash storage is a cost-effective complement to disks in data centers.,http://research.google.com/pubs/archive/41179.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Janus:+Optimal+Flash+Provisioning+for+Cloud+Storage+Workloads+Albrecht+Merchant+Stokely+Waliji+Labelle+Coehlo+Shi+Schrock,http://research.google.com/pubs/pub41179.html
Supervised Learning of Complete Morphological Paradigms,Proceedings of the North American Chapter of the Association for Computational Linguistics (2013),2013,Greg Durrett John DeNero,@inproceedings{41850 title = {Supervised Learning of Complete Morphological Paradigms} author = {Greg Durrett and John DeNero} year = 2013 URL = {http://aclweb.org/anthology//N/N13/N13-1138.pdf} booktitle = {Proceedings of the North American Chapter of the Association for Computational Linguistics} },We describe a supervised approach to predicting the set of all inflected forms of a lexical item. Our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples and then learns the contexts in which those transformations apply using a discriminative sequence model. Because our approach is completely data-driven and the model is trained on examples extracted from Wiktionary our method can extend to new languages without change. Our end-to-end system is able to predict complete paradigms with 86.1% accuracy and individual inflected forms with 94.9% accuracy averaged across three languages and two parts of speech.,http://research.google.com/pubs/archive/41850.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Supervised+Learning+of+Complete+Morphological+Paradigms+Durrett+DeNero,http://research.google.com/pubs/pub41850.html
Entire Relaxation Path for Maximum Entropy Problems,EMNLP 2011 (to appear),2011,Moshe Dubiner Yoram Singer,@inproceedings{37166 title = {Entire Relaxation Path for Maximum Entropy Problems} author = {Moshe Dubiner and Yoram Singer} year = 2011 booktitle = {EMNLP 2011} },We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. Our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet.,http://research.google.com/pubs/archive/37166.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Entire+Relaxation+Path+for+Maximum+Entropy+Problems+Dubiner+Singer,http://research.google.com/pubs/pub37166.html
Search by Voice in Mandarin Chinese,Interspeech 2010 pp. 354-357,2010,Jiulong Shan Genqing Wu Zhihong Hu Xiliu Tang Martin Jansche Pedro J. Moreno,@inproceedings{36463 title = {Search by Voice in Mandarin Chinese} author = {Jiulong Shan and Genqing Wu and Zhihong Hu and Xiliu Tang and Martin Jansche and Pedro J. Moreno} year = 2010 booktitle = {Interspeech 2010} pages = {354--357} },In this paper we describe our efforts to build a Mandarin Chinese voice search system. We describe our strategies for data collection language lexicon and acoustic modeling as well as issues related to text normalization that are an integral part of building voice search systems. We show excellent performance on typical spoken search queries under a variety of accents and acoustic conditions. The system has been in operation since October 2009 and has received very positive user reviews.,http://research.google.com/pubs/archive/36463.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Search+by+Voice+in+Mandarin+Chinese+Shan+Wu+Hu+Tang+Jansche+Moreno,http://research.google.com/pubs/pub36463.html
Managing Distributed UPS Energy for Effective Power Capping in Data Centers,International Symposium on Computer Architecture (2012) pp. 488-499,2012,Vasileios Kontorinis Liuyi Eric Zhang Baris Aksanli Jack Sampson Houman Homayoun Eddie Pettis Dean M. Tullsen Tajana Simunic Rosing,@inproceedings{39964 title = {Managing Distributed UPS Energy for Effective Power Capping in Data Centers} author = {Vasileios Kontorinis and Liuyi Eric Zhang and Baris Aksanli and Jack Sampson and Houman Homayoun and Eddie Pettis and Dean M. Tullsen and Tajana Simunic Rosing} year = 2012 booktitle = {International Symposium on Computer Architecture} pages = {488--499} },Power over-subscription can reduce costs for modern data centers. However designing the power infrastructure for a lower operating power point than the aggregated peak power of all servers requires dynamic techniques to avoid high peak power costs and even worse tripping circuit breakers. This work presents an architecture for distributed per-server UPSs that stores energy during low activity periods and uses this energy during power spikes. This work leverages the distributed nature of the UPS batteries and develops policies that prolong the duration of their usage. The specific approach shaves 19.4% of the peak power for modern servers at no cost in performance allowing the installation of 24% more servers within the same power budget. More servers amortize infrastructure costs better and hence reduce total cost of ownership per server by 6.3%.,http://research.google.com/pubs/archive/39964.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Managing+Distributed+UPS+Energy+for+Effective+Power+Capping+in+Data+Centers+Kontorinis+Zhang+Aksanli+Sampson+Homayoun+Pettis+Tullsen+Rosing,http://research.google.com/pubs/pub39964.html
Repeatedly Appending Any Digit to Generate Composite Numbers,The American Mathematical Monthly (2012) (to appear),2012,John Grantham Witold Jarnicki Jon Rickert Stan Wagon,@article{40337 title = {Repeatedly Appending Any Digit to Generate Composite Numbers} author = {John Grantham and Witold Jarnicki and Jon Rickert and Stan Wagon} year = 2012 journal = {The American Mathematical Monthly} },We investigate the problem of nding integers k such that ap- pending any number of copies of the base-ten digit d to k yields a composite number. In particular we prove that there exist innitely many integers co- prime to all digits such that repeatedly appending any digit yields a composite number.,http://research.google.com/pubs/archive/40337.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Repeatedly+Appending+Any+Digit+to+Generate+Composite+Numbers+Grantham+Witold+Jarnicki+Rickert+Wagon,http://research.google.com/pubs/pub40337.html
Look Who I Found: Understanding the Effects of Sharing Curated Friend Groups,Proceedings of ACM Web Science 2012 ACM pp. 137-146,2012,Lujun Fang Alex Fabrikant Kristen LeFevre,@inproceedings{38169 title = {Look Who I Found: Understanding the Effects of Sharing Curated Friend Groups} author = {Lujun Fang and Alex Fabrikant and Kristen LeFevre} year = 2012 booktitle = {Proceedings of ACM Web Science 2012} pages = {137--146} },"Online social networks like Google+ Twitter and Facebook allow users to build organize and manage their social connections for the purposes of information sharing and consumption. Nonetheless most social network users still report that building and curating contact groups is a time-consuming burden. To help users overcome the burdens of contact discovery and grouping Google+ recently launched a new feature known as ""circle sharing"". The feature makes it easy for users to share the benefits of their own contact curation by sharing entire ""circles"" (contact groups) with others. Recipients of a shared circle can adopt the circle as a whole merge the circle into one of their own circles or select specific members of the circle to add. In this paper we investigate the impact that circle-sharing has had on the growth and structure of the Google+ social network. Using a cluster analysis we identify two natural categories of shared circles which represent two qualitatively different use cases: circles comprised primarily of celebrities (celebrity circles) and circles comprised of members of a community (community circles). We observe that exposure to circle-sharing accelerates the rate at which a user adds others to his or her circles. More specifically we notice that circle-sharing has accelerated the ""densification"" rate of community circles and also that it has disproportionately affected users with few connections allowing them to find new contacts at a faster rate than would be expected based on accepted models of network growth. Finally we identify features that can be used to predict which of a user’s circles (s)he is most likely to share thus demonstrating that it is feasible to suggest to a user which circles to share with friends.",http://research.google.com/pubs/archive/38169.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Look+Who+I+Found:+Understanding+the+Effects+of+Sharing+Curated+Friend+Groups+Fang+Fabrikant+LeFevre,http://research.google.com/pubs/pub38169.html
Pay by the Bit: An Information-Theoretic Metric for Collective Human Judgment,Proc CSCW ACM ACM New York NY USA (2013) pp. 623-638,2013,Tamsyn P. Waterhouse,@inproceedings{40700 title = {Pay by the Bit: An Information-Theoretic Metric for Collective Human Judgment} author = {Tamsyn P. Waterhouse} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2441846} booktitle = {Proc CSCW} pages = {623--638} address = {ACM New York NY USA} },We consider the problem of evaluating the performance of human contributors for tasks involving answering a series of questions each of which has a single correct answer. The answers may not be known a priori. We assert that the measure of a contributor's judgments is the amount by which having these judgments decreases the entropy of our discovering the answer. This quantity is the pointwise mutual information between the judgments and the answer. The expected value of this metric is the mutual information between the contributor and the answer prior which can be computed using only the prior and the conditional probabilities of the contributor's judgments given a correct answer without knowing the answers themselves. We also propose using multivariable information measures such as conditional mutual information to measure the interactions between contributors' judgments. These metrics have a variety of applications. They can be used as a basis for contributor performance evaluation and incentives. They can be used to measure the efficiency of the judgment collection process. If the collection process allows assignment of contributors to questions they can also be used to optimize this scheduling.,http://research.google.com/pubs/archive/40700.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Pay+by+the+Bit:+An+Information-Theoretic+Metric+for+Collective+Human+Judgment+Waterhouse,http://research.google.com/pubs/pub40700.html
Back-off Language Model Compression,Proceedings of Interspeech 2009 International Speech Communication Association (ISCA) pp. 325-355,2009,Boulos Harb Ciprian Chelba Jeffrey Dean Sanjay Ghemawat,@inproceedings{35612 title = {Back-off Language Model Compression} author = {Boulos Harb and Ciprian Chelba and Jeffrey Dean and Sanjay Ghemawat} year = 2009 booktitle = {Proceedings of Interspeech 2009} pages = {325--355} },With the availability of large amounts of training data relevant to speech recognition scenarios scalability becomes a very productive way to improve language model performance. We present a technique that represents a back-off n-gram language model using arrays of integer values and thus renders it amenable to effective block compression. We propose a few such compression algorithms and evaluate the resulting language model along two dimensions: memory footprint and speed reduction relative to the uncompressed one. We experimented with a model that uses a 32-bit word vocabulary (at most 4B words) and log-probabilities/back-off-weights quantized to 1 byte respectively. The best compression algorithm achieves 2.6 bytes/n-gram at ≈18X slower than uncompressed. For faster LM operation we found it feasible to represent the LM at ≈4.0 bytes/n-gram and ≈3X slower than the uncompressed LM. The memory footprint of a LM containing one billion n-grams can thus be reduced to 3–4 Gbytes without impacting its speed too much. See the presentation material from a talk about this paper.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Back-off+Language+Model+Compression+Harb+Chelba+Dean+Ghemawat,http://research.google.com/pubs/pub35612.html
Feedback-Directed Optimizations in GCC with Estimated Edge Profiles from Hardware Event Sampling,Proceedings of GCC Summit 2008 pp. 87-102,2008,Vinodha Ramasamy Paul Yuan Dehao Chen Robert Hundt,@inproceedings{36576 title = {Feedback-Directed Optimizations in GCC with Estimated Edge Profiles from Hardware Event Sampling} author = {Vinodha Ramasamy and Paul Yuan and Dehao Chen and Robert Hundt} year = 2008 URL = {http://www.capsl.udel.edu/conferences/open64/2008/Papers/113.pdf} booktitle = {Proceedings of GCC Summit 2008} pages = {87-102} },Traditional feedback-directed optimization (FDO) in GCC uses static instrumentation to collect edge and value profiles. This method has shown good application performance gains but is not commonly used in practice due to the high runtime overhead of profile collection the tedious dual-compile usage model and difficulties in generating representative training data sets. In this paper we show that edge frequency estimates can be successfully constructed with heuristics using profile data collected by sampling of hardware events incurring low runtime overhead (e.g. less then 2%) and requiring no instrumentation yet achieving competitive performance gains. We describe the motivation design and implementation of FDO using sample profiles in GCC and also present our initial experimental results with SPEC2000int C benchmarks that show approximately 70% to 90% of the performance gains obtained using traditional FDO with exact edge profiles.,http://research.google.com/pubs/archive/36576.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Feedback-Directed+Optimizations+in+GCC+with+Estimated+Edge+Profiles+from+Hardware+Event+Sampling+Ramasamy+Yuan+Chen+Hundt,http://research.google.com/pubs/pub36576.html
Discriminative pronunciation modeling for dialectal speech recognition,Proc. Interspeech (2014) (to appear),2014,Maider Lehr Kyle Gorman Izhak Shafran,@inproceedings{42900 title = {Discriminative pronunciation modeling for dialectal speech recognition} author = {Maider Lehr and Kyle Gorman and Izhak Shafran} year = 2014 booktitle = {Proc. Interspeech} },Speech recognizers are typically trained with data from a standard dialect and do not generalize to non-standard dialects. Mismatch mainly occurs in the acoustic realization of words which is represented by acoustic models and pronunciation lexicon. Standard techniques for addressing this mismatch are generative in nature and include acoustic model adaptation and expansion of lexicon with pronunciation variants both of which have limited effectiveness. We present a discriminative pronunciation model whose parameters are learned jointly with parameters from the language models. We tease apart the gains from modeling the transitions of canonical phones the transduction from surface to canonical phones and the language model. We report experiments on African American Vernacular English (AAVE) using NPR's StoryCorps corpus. Our models improve the performance over the baseline by about 2.1% on AAVE of which 0.6% can be attributed to the pronunciation model. The model learns the most relevant phonetic transformations for AAVE speech.,http://research.google.com/pubs/archive/42900.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discriminative+pronunciation+modeling+for+dialectal+speech+recognition+Lehr+Gorman+Shafran,http://research.google.com/pubs/pub42900.html
A Distance Model for Rhythms,International Conference on Machine Learning (ICML) (2008),2008,Jean-Francois Paiement Yves Grandvalet Samy Bengio Douglas Eck,@inproceedings{34393 title = {A Distance Model for Rhythms} author = {Jean-Francois Paiement and Yves Grandvalet and Samy Bengio and Douglas Eck} year = 2008 URL = {http://bengio.abracadoudou.com/publications/pdf/paiement_2008_icml.pdf} booktitle = {International Conference on Machine Learning (ICML)} },Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.,http://research.google.com/pubs/archive/34393.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Distance+Model+for+Rhythms+Paiement+Grandvalet+Bengio+Eck,http://research.google.com/pubs/pub34393.html
Moving Beyond End-to-End Path Information to Optimize CDN Performance,Internet Measurement Conference (IMC) ACM Chicago IL (2009) pp. 190-201,2009,Rupa Krishnan Harsha V. Madhyastha Sushant Jain Sridhar Srinivasan Arvind Krishnamurthy Thomas Anderson Jie Gao,@inproceedings{35590 title = {Moving Beyond End-to-End Path Information to Optimize CDN Performance} author = {Rupa Krishnan and Harsha V. Madhyastha and Sushant Jain and Sridhar Srinivasan and Arvind Krishnamurthy and Thomas Anderson and Jie Gao} year = 2009 URL = {http://research.google.com/archive/imc191/imc191.pdf} booktitle = {Internet Measurement Conference (IMC)} pages = {190--201} address = {Chicago IL} },Replicating content across a geographically distributed set of servers and redirecting clients to the closest server in terms of latency has emerged as a common paradigm for improving client performance. In this paper we analyze latencies measured from servers in Google’s content distribution network (CDN) to clients all across the Internet to study the effectiveness of latency-based server selection. Our main result is that redirecting every client to the server with least latency does not suffice to optimize client latencies. First even though most clients are served by a geographically nearby CDN node a sizeable fraction of clients experience latencies several tens of milliseconds higher than other clients in the same region. Second we find that queueing delays often override the benefits of a client interacting with a nearby server. To help the administrators of Google’s CDN cope with these problems we have built a system called WhyHigh. First WhyHigh measures client latencies across all nodes in the CDN and correlates measurements to identify the prefixes affected by inflated latencies. Second since clients in several thousand prefixes have poor latencies WhyHigh prioritizes problems based on the impact that solving them would have e.g. by identifying either an AS path common to several inflated prefixes or a CDN node where path inflation is widespread. Finally WhyHigh diagnoses the causes for inflated latencies using active measurements such as traceroutes and pings in combination with datasets such as BGP paths and flow records. Typical causes discovered include lack of peering routing misconfigurations and side-effects of traffic engineering. We have used WhyHigh to diagnose several instances of inflated latencies and our efforts over the course of a year have significantly helped improve the performance offered to clients by Google’s CDN. An anonymized data set is available for download.,http://research.google.com/pubs/archive/35590.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Moving+Beyond+End-to-End+Path+Information+to+Optimize+CDN+Performance+Krishnan+Madhyastha+Jain+Srinivasan+Krishnamurthy+Anderson+Gao,http://research.google.com/pubs/pub35590.html
Large Scale Page-Based Book Similarity Clustering,ICDAR 2011,2011,Nemanja Spasojevic Guillaume Poncin,@inproceedings{37296 title = {Large Scale Page-Based Book Similarity Clustering} author = {Nemanja Spasojevic and Guillaume Poncin} year = 2011 booktitle = {ICDAR 2011} },The Google Books corpus now counts over 15M books spanning 7 centuries and countless languages. Traditional cataloguing at that scale is imprecise and often fails to identify more complex book-to-book relationships such as ‘same text different pagination’ or ‘partial overlap’. Our contribution is a two-step technique for clustering books based on content similarity (at both book and page level) and classifying their relationships. We run this on our corpora consisting of more than 15M books (5B pages). We ﬁrst detect similar books and similar pages within matching books using hashing techniques and judicious thresholds. We then combine those features to identify the exact relationship between matching books. In this paper we describe the basic approach to making the problem tractable as well as the features and classiﬁers that we used. We enumerate a small number of relationships to qualify the link between scanned real-world books. Finally we provide precision and recall measurements of the classiﬁer.,http://research.google.com/pubs/archive/37296.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Page-Based+Book+Similarity+Clustering+Spasojevic+Poncin,http://research.google.com/pubs/pub37296.html
Understanding Sensitivity by Analyzing Anonymity,IEEE Security & Privacy vol. 13 (2015) pp. 14-21,2015,Sai Teja Peddinti Aleksandra Korolova Elie Bursztein Geetanjali Sampemane,@article{43426 title = {Understanding Sensitivity by Analyzing Anonymity} author = {Sai Teja Peddinti and Aleksandra Korolova and Elie Bursztein and Geetanjali Sampemane} year = 2015 journal = {IEEE Security & Privacy} pages = {14--21} volume = {13} },The range of topics that users of online services consider sensitive is often broader than what service providers or regulators deem sensitive. A data-driven approach can help providers improve products with features that let users exercise privacy preferences more effectively.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Understanding+Sensitivity+by+Analyzing+Anonymity+Peddinti+Korolova+Bursztein+Sampemane,http://research.google.com/pubs/pub43426.html
E Unum Pluribus - Google Network Filtering Management,LISA'09 23rd Large Installation System Administration Conference (2009),2009,Paul (Tony) Watson Peter Moody,@inproceedings{40401 title = {E Unum Pluribus - Google Network Filtering Management} author = {Paul (Tony) Watson and Peter Moody} year = 2009 URL = {http://static.usenix.org/events/lisa09/tech/} booktitle = {LISA'09 23rd Large Installation System Administration Conference} },Network filtering can be a very difficult challenge in large complex and sprawling networks. Through the use of internally developed software Google has automated and simplified many of the difficult tasks and provided the capability to easily audit and validate its filters. This talk will discuss our efforts in this area and release some of these tools to the community.,http://research.google.com/pubs/archive/40401.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=E+Unum+Pluribus+-+Google+Network+Filtering+Management+Watson+Moody,http://research.google.com/pubs/pub40401.html
Testable JavaScript,O'Reilly Media 1005 Gravenstein Highway North Sebastopol CA 95472 (2013),2013,Mark Ethan Trostler,@book{41412 title = {Testable JavaScript} author = {Mark Ethan Trostler} year = 2013 URL = {http://shop.oreilly.com/product/0636920024699.do} booktitle = {Testable JavaScript} address = {1005 Gravenstein Highway North Sebastopol CA 95472} },One skill that’s essential for any professional JavaScript developer is the ability to write testable code. This book shows you what writing and maintaining testable JavaScript for the client- or server-side actually entails whether you’re creating a new application or rewriting legacy code. From methods to reduce code complexity to unit testing code coverage debugging and automation you’ll learn a holistic approach for writing JavaScript code that you and your colleagues can easily fix and maintain going forward. Testing JavaScript code is complicated. This book helps you simply the process considerably. Get an overview of Agile test-driven development and behavior-driven development Use patterns from static languages and standards-based JavaScript to reduce code complexity Learn the advantages of event-based architectures including modularity loose coupling and reusability Explore tools for writing and running unit tests at the functional and application level Generate code coverage to measure the scope and effectiveness of your tests Conduct integration performance and load testing using Selenium or CasperJS Use tools for in-browser Node.js mobile and production debugging Understand what when and how to automate your development processes,http://shop.oreilly.com/product/0636920024699.do,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Testable+JavaScript+Trostler,http://research.google.com/pubs/pub41412.html
Sparse Non-negative Matrix Language Modeling For Skip-grams,Proceedings of Interspeech 2015 ISCA pp. 1428-1432,2015,Noam M. Shazeer Joris Pelemans Ciprian Chelba,@inproceedings{43829 title = {Sparse Non-negative Matrix Language Modeling For Skip-grams} author = {Noam M. Shazeer and Joris Pelemans and Ciprian Chelba} year = 2015 booktitle = {Proceedings of Interspeech 2015} pages = {1428--1432} },We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating these techniques on the One Billion Word Benchmark [3] shows that with skip-gram features SNMLMs are able to match the state-of-the art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNNLM estimation are probably its main strength promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.,http://research.google.com/pubs/archive/43829.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sparse+Non-negative+Matrix+Language+Modeling+For+Skip-grams+Shazeer+Pelemans+Chelba,http://research.google.com/pubs/pub43829.html
You Are What You Emote: Emotional Facial Expressions Impact Sexual Orientation Judgments,In the symposium Beyond Right/Wrong: Novel Approaches to Understanding Accuracy and Consensus in Social Perception chairs: Joshua A. Tabak and David J. Lick 25th Annual Convention of the Association for Psychological Science Washington D.C. (2013),2013,Joshua Tabak Sapna Cheryan,@inproceedings{42027 title = {You Are What You Emote: Emotional Facial Expressions Impact Sexual Orientation Judgments} author = {Joshua Tabak and Sapna Cheryan} year = 2013 URL = {http://aps.psychologicalscience.org/convention/program_2013/search/viewProgram.cfm?Abstract_ID=27876&AbType&AbAuthor=164147&Subject_ID&Day_ID=all&keyword} booktitle = {In the symposium Beyond Right/Wrong: Novel Approaches to Understanding Accuracy and Consensus in Social Perception chairs: Joshua A. Tabak and David J. Lick} address = {Washington D.C.} },Emotions convey more than sentiment. We found that gendered emotional expressions modulated sexual orientation judgments from faces consistent with stereotypes of gay individuals as gender-atypical. This is the first research on accuracy of person perceptions at the intersection of stable (sexual orientation) and fleeting (emotion) person characteristics.,http://aps.psychologicalscience.org/convention/program_2013/search/viewProgram.cfm?Abstract_ID=27876&AbType&AbAuthor=164147&Subject_ID&Day_ID=all&keyword,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=You+Are+What+You+Emote:+Emotional+Facial+Expressions+Impact+Sexual+Orientation+Judgments+Tabak+Cheryan,http://research.google.com/pubs/pub42027.html
Gender Differences in High School Students’ Decisions to Study Computer Science and Related Fields,Proceedings of the 46th ACM Technical Symposium on Computer Science Education ACM (2015),2015,Hai Hong Jennifer Wang Jason Ravitz Mo-Yun Lei Fong,@inproceedings{43401 title = {Gender Differences in High School Students’ Decisions to Study Computer Science and Related Fields} author = {Hai Hong and Jennifer Wang and Jason Ravitz and Mo-Yun Lei Fong} year = 2015 booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education} },Increasing women’s participation in Computer Science (CS) is a critical workforce and equity concern. The technology industry has committed to reversing negative trends for women in CS engineering and related fields. Building on previous research we surveyed 1739 high school students and recent college graduates to understand factors influencing decisions to pursue CS-related college degrees. Results indicate social encouragement career perception academic exposure and self perception are the leading factors for women while the influence of these factors is different for men. These factors are actionable and understanding differences in their influence on men and women will inform our approaches to achieving gender parity in tech.,http://research.google.com/pubs/archive/43401.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Gender+Differences+in+High+School+Students%E2%80%99+Decisions+to+Study+Computer+Science+and+Related+Fields+Hong+Wang+Ravitz+Fong,http://research.google.com/pubs/pub43401.html
A Hierarchical Conditional Random Field Model for Labeling and Images of Street Scenes,International Conference on Computer Vision and Pattern Recognition (2011),2011,Qixing Huang Mei Han Bo Wu Sergey Ioffe,@inproceedings{37125 title = {A Hierarchical Conditional Random Field Model for Labeling and Images of Street Scenes} author = {Qixing Huang and Mei Han and Bo Wu and Sergey Ioffe} year = 2011 booktitle = {International Conference on Computer Vision and Pattern Recognition} },Simultaneously segmenting and labeling images is a fundamental problem in Computer Vision. In this paper we introduce a hierarchical CRF model to deal with the problem of labeling images of street scenes by several distinctive object classes. In addition to learning a CRF model from all the labeled images we group images into clusters of similar images and learn a CRF model from each cluster separately. When labeling a new image we pick the closest cluster and use the associated CRF model to label this image. Experimental results show that this hierarchical image labeling method is comparable to and in many cases superior to previous methods on benchmark data sets. In addition to segmentation and labeling results we also showed how to apply the image labeling result to rerank Google similar images.,http://research.google.com/pubs/archive/37125.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Hierarchical+Conditional+Random+Field+Model+for+Labeling+and+Images+of+Street+Scenes+Huang+Han+Wu+Ioffe,http://research.google.com/pubs/pub37125.html
A Generalized Composition Algorithm for Weighted Finite-State Transducers,Interspeech 2009,2009,Cyril Allauzen Michael Riley Johan Schalkwyk,@inproceedings{35539 title = {A Generalized Composition Algorithm for Weighted Finite-State Transducers} author = {Cyril Allauzen and Michael Riley and Johan Schalkwyk} year = 2009 booktitle = {Interspeech 2009} },This paper describes a weighted finite-state transducer composition algorithm that generalizes the notion of the composition filter and present filters that remove useless epsilon paths and push forward labels and weights along epsilon paths. This filtering allows us to compose together large speech recognition context-dependent lexicons and language models much more efficiently in time and space than previously possible. We present experiments on Broadcast News and Google Search by Voice that demonstrate a 5% to 10% overhead for dynamic runtime composition compared to a static offline composition of the recognition transducer. To our knowledge this is the first such system with such small overhead.,http://research.google.com/pubs/archive/35539.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Generalized+Composition+Algorithm+for+Weighted+Finite-State+Transducers+Allauzen+Riley+Schalkwyk,http://research.google.com/pubs/pub35539.html
Student Skill and Goal Achievement in the Mapping with Google MOOC,ACM Learning at Scale (2014),2014,Julia Wilkowski Amit Deutsch Daniel M. Russell,@inproceedings{41929 title = {Student Skill and Goal Achievement in the Mapping with Google MOOC} author = {Julia Wilkowski and Amit Deutsch and Daniel M. Russell} year = 2014 booktitle = {ACM Learning at Scale} },Students who registered for the Mapping with Google massive open online course (MOOC) were asked several questions during the registration process to identify prior experience with eleven skills as well as their goals for registering for the course. Students selected goals from a list; they were periodically reminded of these goals during the course. At the end of the course we compared students’ self report of goal achievement on a post-course survey with behavioral click-stream analysis. In addition we compared whether possessing skills at the outset of the course or completing course activities had a larger effect on course completion. We discovered that prior skill had no significant predictive value on certification but students who completed course activities were more likely to earn certificates of completion than peers who did not complete activities.,http://research.google.com/pubs/archive/41929.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Student+Skill+and+Goal+Achievement+in+the+Mapping+with+Google+MOOC+Wilkowski+Deutsch+Russell,http://research.google.com/pubs/pub41929.html
The Goals and Challenges of Click Fraud Penetration Testing Systems,International Symposium on Software Reliability Engineering International Symposium on Software Reliability Engineering (2009),2009,Carmelo Kintana David Turner Jia-Yu Pan Ahmed Metwally Neil Daswani Erika Chin Andrew Bortz,@inproceedings{34836 title = {The Goals and Challenges of Click Fraud Penetration Testing Systems} author = {Carmelo Kintana and David Turner and Jia-Yu Pan and Ahmed Metwally and Neil Daswani and Erika Chin and Andrew Bortz} year = 2009 booktitle = {International Symposium on Software Reliability Engineering} },It is important for search and pay-per-click engines to penetration test their click fraud detection systems in order to find potential vulnerabilities and correct them before fraudsters can exploit them. In this paper we describe: (1) some goals and desirable qualities of a click fraud penetration testing system based on our experience and (2) our experiences with the challenges of building and using a click fraud penetration testing system called Camelot that has been in use at Google.,http://research.google.com/pubs/archive/34836.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Goals+and+Challenges+of+Click+Fraud+Penetration+Testing+Systems+Kintana+Turner+Pan+Metwally+Daswani+Chin+Bortz,http://research.google.com/pubs/pub34836.html
Theoretical Foundations for Learning Kernels in Supervised Kernel PCA,Modern Nonparametrics 3: Automating the Learning Pipeline Neural Information Processing Systems Workshop (2014),2014,Mehryar Mohri Afshin Rostamizadeh Dmitry Storcheus,@inproceedings{43968 title = {Theoretical Foundations for Learning Kernels in Supervised Kernel PCA} author = {Mehryar Mohri and Afshin Rostamizadeh and Dmitry Storcheus} year = 2014 URL = {https://sites.google.com/site/nips2014modernnonparametric/P8_full_Theoretical%20Foundations%20for%20Learning%20Kernels%20in%20Supervised%20Kernel%20PCA.pdf?attredirects=0&d=1} booktitle = {Modern Nonparametrics 3: Automating the Learning Pipeline} },This paper presents a novel learning scenario which combines dimensionality reduction supervised learning as well as kernel selection. We carefully define the hypothesis class that addresses this setting and provide an analysis of its Rademacher complexity and thereby provide generalization guarantees. The proposed algorithm uses KPCA to reduce the dimensionality of the feature space i.e. by projecting data onto top eigenvectors of covariance operator in a kernel reproducing space. Moreover it simultaneously learns a linear combination of base kernel functions which defines a reproducing space as well as the parameters of a supervised learning algorithm in order to minimize a regularized empirical loss. The bound on Rademacher complexity of our hypothesis is shown to be logarithmic in the number of base kernels which encourages practitioners to combine as many base kernels as possible.,http://research.google.com/pubs/archive/43968.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Theoretical+Foundations+for+Learning+Kernels+in+Supervised+Kernel+PCA+Mohri+Rostamizadeh+Storcheus,http://research.google.com/pubs/pub43968.html
GyroPen: Gyroscopes for Pen-input with Mobile Phones,IEEE Transactions on Human-Machine Systems vol. 45 (2015) pp. 263-271,2015,Thomas Deselaers Daniel Keysers Jan Hosang Henry Rowley,@article{43127 title = {GyroPen: Gyroscopes for Pen-input with Mobile Phones} author = {Thomas Deselaers and Daniel Keysers and Jan Hosang and Henry Rowley} year = 2015 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6975206} journal = {IEEE Transactions on Human-Machine Systems} pages = {263--271} volume = {45} },We present GyroPen a method for text entry into mobile devices using pen-like writing interaction reconstructed from standard built-in sensors. The key idea is to reconstruct a representation of the trajectory of the phone's corner that is touching a writing surface from the measurements obtained from the phone's gyroscopes and accelerometers. We propose to directly use the angular trajectory for this reconstruction which removes the necessity for accurate absolute 3D position estimation a task that can be difficult using low-cost accelerometers. Recognition is then performed using an off-the-shelf handwriting recognition system allowing easy extension to new languages and scripts. In a small user study (n=10) the average novice participant was able to write the first word only 37 seconds after the starting to use GyroPen for the first time. With some experience users were able to write at the speed of 3-4s for one English word and with a character error rate of 18%.,http://research.google.com/pubs/archive/43127.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=GyroPen:+Gyroscopes+for+Pen-input+with+Mobile+Phones+Deselaers+Keysers+Hosang+Rowley,http://research.google.com/pubs/pub43127.html
Large Vocabulary Automatic Speech Recognition for Children,Interspeech (2015),2015,Hank Liao Golan Pundak Olivier Siohan Melissa Carroll Noah Coccaro Qi-Ming Jiang Tara N. Sainath Andrew Senior Françoise Beaufays Michiel Bacchiani,@inproceedings{44268 title = {Large Vocabulary Automatic Speech Recognition for Children} author = {Hank Liao and Golan Pundak and Olivier Siohan and Melissa Carroll and Noah Coccaro and Qi-Ming Jiang and Tara N. Sainath and Andrew Senior and Françoise Beaufays and Michiel Bacchiani} year = 2015 booktitle = {Interspeech} },Recently Google launched YouTube Kids a mobile application for children that uses a speech recognizer built specifically for recognizing children’s speech. In this paper we present techniques we explored to build such a system. We describe the use of a neural network classifier to identify matched acoustic training data filtering data for language modeling to reduce the chance of producing offensive results. We also compare long short-term memory (LSTM) recurrent networks to convolutional LSTM deep neural networks (CLDNN). We found that a CLDNN acoustic model outperforms an LSTM across a variety of different conditions but does not specifically model child speech relatively better than adult. Overall these findings allow us to build a successful state-of-the-art large vocabulary speech recognizer for both children and adults.,http://research.google.com/pubs/archive/44268.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Vocabulary+Automatic+Speech+Recognition+for+Children+Liao+Pundak+Siohan+Carroll+Coccaro+Jiang+Sainath+Senior+Beaufays+Bacchiani,http://research.google.com/pubs/pub44268.html
Traffic Anomaly Detection Based on the IP Size Distribution,INFOCOM International Conference on Computer Communications Joint Conference of the IEEE Computer and Communications Societies IEEE (2012) pp. 2005-2013,2012,Fabio Soldo Ahmed Metwally,@inproceedings{37739 title = {Traffic Anomaly Detection Based on the IP Size Distribution} author = {Fabio Soldo and Ahmed Metwally} year = 2012 booktitle = {INFOCOM International Conference on Computer Communications Joint Conference of the IEEE Computer and Communications Societies} pages = {2005--2013} },In this paper we present a data-driven framework for detecting machine-generated traffic based on the IP size i.e. the number of users sharing the same source IP. Our main observation is that diverse machine-generated traffic attacks share a common characteristic: they induce an anomalous deviation from the expected IP size distribution. We develop a principled framework that automatically detects and classifies these deviations using statistical tests and ensemble learning. We evaluate our approach on a massive dataset collected at Google for 90 consecutive days. We argue that our approach combines desirable characteristics: it can accurately detect fraudulent machine-generated traffic; it is based on a fundamental characteristic of these attacks and is thus robust (e.g. to DHCP re-assignment) and hard to evade; it has low complexity and is easy to parallelize making it suitable for large-scale detection; and finally it does not entail profiling users but leverages only aggregate statistics of network traffic.,http://research.google.com/pubs/archive/37739.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Traffic+Anomaly+Detection+Based+on+the+IP+Size+Distribution+Soldo+Metwally,http://research.google.com/pubs/pub37739.html
Distributed Balanced Partitioning via Linear Embedding,WSDM 2016: Ninth ACM International Conference on Web Search and Data Mining ACM (to appear),2016,Kevin Aydin Mohammadhossein Bateni Vahab Mirrokni,@inproceedings{44315 title = {Distributed Balanced Partitioning via Linear Embedding} author = {Kevin Aydin and Mohammadhossein Bateni and Vahab Mirrokni} year = 2016 booktitle = {WSDM 2016: Ninth ACM International Conference on Web Search and Data Mining} },Balanced partitioning is often a crucial first step in solving large-scale graph optimization problems: in some cases a big graph is chopped into pieces that fit on one machine to be processed independently before stitching the results together leading to certain suboptimality from the interaction among different pieces. In other cases links between different parts may show up in the running time and/or network communications cost hence the desire to have small cut size. We study a distributed balanced partitioning problem where the goal is to partition the vertices of a given graph into k pieces minimizing the total cut size. Our algorithm is composed of a few steps that are easily implementable in distributed computation frameworks e.g. MapReduce. The algorithm first embeds nodes of the graph onto a line and then processes nodes in a distributed manner guided by the linear embedding order. We examine various ways to find the first embedding e.g. via a hierarchical clustering or Hilbert curves. Then we apply four different techniques such as local swaps minimum cuts on partition boundaries as well as contraction and dynamic programming. Our empirical study compares the above techniques with each other and to previous work in distributed algorithms e.g. a label propagation method [34] FENNEL [32] and Spinner [23]. We report our results both on a private map graph and several public social networks and show that our results beat previous distributed algorithms: we notice e.g. 15-25% reduction in cut size over [34]. We also observe that our algorithms allow for scalable distributed implementation for any number of partitions. Finally we apply our techniques for the Google Maps Driving Directions to minimize the number of multi-shard queries with the goal of saving in CPU usage. During live experiments we observe an ≈ 40% drop in the number of multi-shard queries when comparing our method with a standard geography-based method.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Balanced+Partitioning+via+Linear+Embedding+Aydin+Bateni+Mirrokni,http://research.google.com/pubs/pub44315.html
Large-Scale Learning with Less RAM via Randomization,Proceedings of the 30 International Conference on Machine Learning (ICML) (2013) pp. 10,2013,Daniel Golovin D. Sculley H. Brendan McMahan Michael Young,@inproceedings{40813 title = {Large-Scale Learning with Less RAM via Randomization} author = {Daniel Golovin and D. Sculley and H. Brendan McMahan and Michael Young} year = 2013 note = {Extended version with additional proofs.} booktitle = {Proceedings of the 30 International Conference on Machine Learning (ICML)} pages = {10} },We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance dominating standard approaches across memory versus accuracy tradeoffs.,http://research.google.com/pubs/archive/40813.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Learning+with+Less+RAM+via+Randomization+Golovin+Sculley+McMahan+Young,http://research.google.com/pubs/pub40813.html
Entrepreneurial Innovation at Google,IEEE: Computer vol. 0018-9162/11 (2011) pp. 7,2011,Patrick Copeland Alberto Savoia,@article{41469 title = {Entrepreneurial Innovation at Google} author = {Patrick Copeland and Alberto Savoia} year = 2011 journal = {IEEE: Computer} pages = {7} volume = {0018-9162/11} },Large organizations have enormous innovation potential at their disposal. However the innovation actually realized in successful products and services is usually only a small fraction of that potential. The amount and type of innovation a company achieves are directly related to the way it approaches fosters selects and funds innovation efforts. To maximize innovation and avoid the dilemmas that mature companies face Google complements the time-proven model of topdown innovation with its own brand of entrepreneurial innovation.,http://research.google.com/pubs/archive/41469.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Entrepreneurial+Innovation+at+Google+Copeland+Savoia,http://research.google.com/pubs/pub41469.html
Understanding Indoor Scenes using 3D Geometric Phrases,Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013),2013,Wongun Choi Yu-Wei Chao Caroline Pantofaru Silvio Savarese,@inproceedings{41340 title = {Understanding Indoor Scenes using 3D Geometric Phrases} author = {Wongun Choi and Yu-Wei Chao and Caroline Pantofaru and Silvio Savarese} year = 2013 booktitle = {Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013)} },Visual scene understanding is a difficult problem interleaving object detection geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable can be learned from a reasonable amount of training data and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics geometry and object groupings from a single image while also improving individual object detections.,http://research.google.com/pubs/archive/41340.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Understanding+Indoor+Scenes+using+3D+Geometric+Phrases+Choi+Chao+Pantofaru+Savarese,http://research.google.com/pubs/pub41340.html
Custom AST transformations with Project Lombok,IBM developerWorks (2011),2011,Alex Ruiz,@article{37352 title = {Custom AST transformations with Project Lombok} author = {Alex Ruiz} year = 2011 URL = {http://www.ibm.com/developerworks/java/library/j-lombok/} journal = {IBM developerWorks} },Alex Ruiz introduces Project Lombok in this article discussing some of the programming sugar that makes it unique including annotation-driven code generation and clean compact and readable code. He then draws your attention to one of the more rewarding uses of Lombok: extending it with custom AST (Abstract Syntax Tree) transformations. Extending Lombok will enable you to generate your own project- or domain-specific boilerplate code but it does require a fair amount of work. Alex concludes with his tips for easing through key stages of the process along with a freely usable custom extension for JavaBeans.,http://www.ibm.com/developerworks/java/library/j-lombok/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Custom+AST+transformations+with+Project+Lombok+Ruiz,http://research.google.com/pubs/pub37352.html
End-to-End Text-Dependent Speaker Verification,International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2016),2016,Georg Heigold Ignacio Moreno Samy Bengio Noam M. Shazeer,@inproceedings{44681 title = {End-to-End Text-Dependent Speaker Verification} author = {Georg Heigold and Ignacio Moreno and Samy Bengio and Noam M. Shazeer} year = 2016 booktitle = {International Conference on Acoustics Speech and Signal Processing (ICASSP)} },In this paper we present a data-driven integrated approach to speaker verification which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system’s components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture including the estimation of a speaker model on only a few utterances and evaluate it on our internal ”Ok Google” benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate easy-to-maintain systems with a small footprint.,http://research.google.com/pubs/archive/44681.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=End-to-End+Text-Dependent+Speaker+Verification+Heigold+Lopez+Moreno+Bengio+Shazeer,http://research.google.com/pubs/pub44681.html
Scene Aligned Pooling for Complex Video Recognition,ECCV (2012) pp. 688-701,2012,Liangliang Cao Yadong Mu Apostol Natsev Shih-Fu Chang Gang Hua John R. Smith,@inproceedings{41794 title = {Scene Aligned Pooling for Complex Video Recognition} author = {Liangliang Cao and Yadong Mu and Apostol Natsev and Shih-Fu Chang and Gang Hua and John R. Smith} year = 2012 booktitle = {ECCV} pages = {688-701} },Real-world videos often contain dynamic backgrounds and evolving people activities especially for those web videos generated by users in unconstrained scenarios. This paper proposes a new visual representation namely scene aligned pooling for the task of event recognition in complex videos. Based on the observation that a video clip is often composed with shots of different scenes the key idea of scene aligned pooling is to decompose any video features into concurrent scene components and to construct classification models adaptive to different scenes. The experiments on two large scale real-world datasets including the TRECVID Multimedia Event Detection 2011 and the Human Motion Recognition Databases (HMDB) show that our new visual representation can consistently improve various kinds of visual features such as different low-level color and texture features or middle-level histogram of local descriptors such as SIFT or space-time interest points and high level semantic model features by a significant margin. For example we improve the-state-of-the-art accuracy on HMDB dataset by 20% in terms of accuracy.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scene+Aligned+Pooling+for+Complex+Video+Recognition+Cao+Mu+Natsev+Chang+Hua+Smith,http://research.google.com/pubs/pub41794.html
System and method for determining active topics,Patent (2013),2013,Michael Jeffrey Procopio,none,A method for determining active topics may include receiving topic information for a document the information including at least one topic and a weight for each topic where the topic relates to content of the document and the weight represents how strongly the topic is associated with the document. User activity information for the document including a user activity value including at least one of a number of viewers and a number of editors of the document may be received. A topic intensity for each topic may be generated and stored by multiplying the user activity value for the document by the weight of the topic in the document. The topic intensity may be monitored over time. An alert may be generated based on the topic intensity.,http://research.google.com/pubs/archive/42182.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=System+and+method+for+determining+active+topics+Procopio,http://research.google.com/pubs/pub42182.html
SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois,International Semantic Web Conference 2010 (ISWC2010),2010,Thomas Steiner,@inproceedings{36825 title = {SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois} author = {Thomas Steiner} year = 2010 URL = {http://www.lsi.upc.edu/~tsteiner/papers/2010/semwebvid-demo-iswc2010.pdf} booktitle = {International Semantic Web Conference 2010 (ISWC2010)} },SemWebVid is an online Ajax application that allows for the automatic generation of Resource Description Framework (RDF) video descriptions. These descriptions are based on two pillars: first on a combination of user-generated metadata such as title summary and tags; and second on closed captions which can be user-generated or be auto-generated via speech recognition. The plaintext contents of both pillars are being analyzed using multiple Natural Language Processing (NLP) Web services in parallel whose results are then merged and where possible matched back to concepts in the sense of Linking Open Data (LOD). The final result is a deep-linkable RDF description of the video and a “scroll-along” view of the video as an example of video visualization formats.,http://research.google.com/pubs/archive/36825.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SemWebVid+-+Making+Video+a+First+Class+Semantic+Web+Citizen+and+a+First+Class+Web+Bourgeois+Steiner,http://research.google.com/pubs/pub36825.html
Learning to Target: What Works for Behavioral Targeting,CIKM '11 ACM Glasgow Scotland UK (2011) pp. 1805-1814,2011,Sandeep Pandey Mohamed Aly Abraham Bagherjeiran Andrew Hatch Peter Ciccolo Adwait Ratnaparkhi Martin Zinkevich,@inproceedings{37667 title = {Learning to Target: What Works for Behavioral Targeting} author = {Sandeep Pandey and Mohamed Aly and Abraham Bagherjeiran and Andrew Hatch and Peter Ciccolo and Adwait Ratnaparkhi and Martin Zinkevich} year = 2011 booktitle = {CIKM '11} pages = {1805--1814} address = {Glasgow Scotland UK} },Understanding what interests and delights users is critical to effective behavioral targeting especially in information-poor contexts. As users interact with content and advertising their passive behavior can reveal their interests towards advertising. Two issues are critical for building effective targeting methods: what metric to optimize for and how to optimize. More specifically we ﬁrst attempt to understand what the learning objective should be for behavioral targeting so as to maximize advertiser’s performance. While most popular advertising methods optimize for user clicks as we will show maximizing clicks does not necessarily imply maximizing purchase activities or transactions called conversions which directly translate to advertiser’s revenue. In this work we focus on conversions which makes a more relevant metric but also the more challenging one. Second is the issue of how to represent and combine the plethora of user activities such as search queries page views ad clicks to perform the targeting. We investigate several sources of user activities as well as methods for inferring conversion likelihood given the activities. We also explore the role played by the temporal aspect of user activities for targeting e.g. how recent activities compare to the old ones. Based on a rigorous offline empirical evaluation over 200 individual advertising campaigns we arrive at what we believe are best practices for behavioral targeting. We deploy our approach over live user traffic to demonstrate its superiority over existing state-of-the-art targeting methods.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Target:+What+Works+for+Behavioral+Targeting+Pandey+Aly+Bagherjeiran+Hatch+Ciccolo+Ratnaparkhi+Zinkevich,http://research.google.com/pubs/pub37667.html
Abstract Interpretation as Automated Deduction,Automated Deduction - CADE 25 Springer International Publishing (2015) pp. 450-464,2015,Vijay D'Silva Caterina Urban,@inproceedings{43855 title = {Abstract Interpretation as Automated Deduction} author = {Vijay D'Silva and Caterina Urban} year = 2015 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-319-21401-6_31} booktitle = {Automated Deduction - CADE 25} pages = {450--464} },Algorithmic deduction and abstract interpretation are two widely used and successful approaches to implementing program verifiers. A major impediment to combining these approaches is that their mathematical foundations and implementation approaches are fundamentally different. This paper presents a new logical perspective on abstract interpreters that perform reachability analysis using non-relational domains. We encode reachability of a location in a control-flow graph as satisfiability in a monadic second-order logic parameterized by a first-order theory. We show that three components of an abstract interpreter the lattice transformers and iteration algorithm represent a first-order substructural theory parametric deduction and abduction in that theory and second-order constraint propagation.,http://research.google.com/pubs/archive/43855.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Abstract+Interpretation+as+Automated+Deduction+D'Silva+Urban,http://research.google.com/pubs/pub43855.html
Plato: A Selective Context Model for Entity Resolution,Transactions of the Association for Computational Linguistics vol. 3 (2015) pp. 503-515,2015,Nevena Lazic Amarnag Subramanya Michael Ringgaard Fernando Pereira,@article{44016 title = {Plato: A Selective Context Model for Entity Resolution} author = {Nevena Lazic and Amarnag Subramanya and Michael Ringgaard and Fernando Pereira} year = 2015 URL = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637} journal = {Transactions of the Association for Computational Linguistics} pages = {503--515} volume = {3} },We present Plato a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative featuresand supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers allowing it to scale to over 10^7 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.,https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Plato:+A+Selective+Context+Model+for+Entity+Resolution+Lazic+Subramanya+Ringgaard+Pereira,http://research.google.com/pubs/pub44016.html
Pruning Sparse Non-negative Matrix N-gram Language Models,Proceedings of Interspeech 2015 ISCA pp. 1433-1437,2015,Joris Pelemans Noam M. Shazeer Ciprian Chelba,@inproceedings{43830 title = {Pruning Sparse Non-negative Matrix N-gram Language Models} author = {Joris Pelemans and Noam M. Shazeer and Ciprian Chelba} year = 2015 booktitle = {Proceedings of Interspeech 2015} pages = {1433--1437} },In this paper we present a pruning algorithm and experimental results for our recently proposed Sparse Non-negative Matrix (SNM) family of language models (LMs). We have uncovered a bug in the experimental setup for SNM pruning; see Errata section for correct results. We also illustrate a method for converting an SNMLM to ARPA back-off format which can be readily used in a single-pass decoder for Automatic Speech Recognition.,http://research.google.com/pubs/archive/43830.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Pruning+Sparse+Non-negative+Matrix+N-gram+Language+Models+Pelemans+Shazeer+Chelba,http://research.google.com/pubs/pub43830.html
Construction of non-convex polynomial loss functions for training a binary classifier with quantum annealing,arXiv preprint 1406.4203 (2014),2014,Ryan Babbush Vasil Denchev Nan Ding Sergei Isakov Hartmut Neven,@article{43944 title = {Construction of non-convex polynomial loss functions for training a binary classifier with quantum annealing} author = {Ryan Babbush and Vasil Denchev and Nan Ding and Sergei Isakov and Hartmut Neven} year = 2014 URL = {http://arxiv.org/abs/1406.4203} journal = {arXiv preprint 1406.4203} },Quantum annealing is a heuristic quantum algorithm which exploits quantum resources to minimize an objective function embedded as the energy levels of a programmable physical system. To take advantage of a potential quantum advantage one needs to be able to map the problem of interest to the native hardware with reasonably low overhead. Because experimental considerations constrain our objective function to take the form of a low degree PUBO (polynomial unconstrained binary optimization) we employ non-convex loss functions which are polynomial functions of the margin. We show that these loss functions are robust to label noise and provide a clear advantage over convex methods. These loss functions may also be useful for classical approaches as they compile to regularized risk expressions which can be evaluated in constant time with respect to the number of training examples.,http://research.google.com/pubs/archive/43944.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Construction+of+non-convex+polynomial+loss+functions+for+training+a+binary+classifier+with+quantum+annealing+Babbush+Denchev+Ding+Isakov+Neven,http://research.google.com/pubs/pub43944.html
A Support Vector Approach to Censored Targets,Seventh IEEE International Conference on Data Mining (ICDM) (2007) pp. 655-660,2007,Pannagadatta Shivaswamy Wei Chu Martin Jansche,@inproceedings{34327 title = {A Support Vector Approach to Censored Targets} author = {Pannagadatta Shivaswamy and Wei Chu and Martin Jansche} year = 2007 note = {doi:10.1109/ICDM.2007.93} booktitle = {Seventh IEEE International Conference on Data Mining (ICDM)} pages = {655--660} },Censored targets such as the time to events in survival analysis can generally be represented by intervals on the real line. In this paper we propose a novel support vector technique (named SVCR) for regression on censored targets. SVCR inherits the strengths of support vector methods such as a globally optimal solution by convex programming fast training speed and strong generalization capacity. In contrast to ranking approaches to survival analysis our approach is able not only to achieve superior ordering performance but also to predict the survival time very well. Experiments show a significant performance improvement when the majority of the training data is censored. Experimental results on several survival analysis datasets demonstrate that SVCR is very competitive against classical survival analysis models.,http://research.google.com/pubs/archive/34327.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Support+Vector+Approach+to+Censored+Targets+Shivaswamy+Chu+Jansche,http://research.google.com/pubs/pub34327.html
Robust Local Search for Solving RCPSP/max with Durational Uncertainty,Journal of Artificial Intelligence Research vol. 43 (2012) pp. 43-86,2012,Na Fu Hoong Chuin Lau Pradeep Varakantha Fei Xiao,@article{37715 title = {Robust Local Search for Solving RCPSP/max with Durational Uncertainty} author = {Na Fu and Hoong Chuin Lau and Pradeep Varakantha and Fei Xiao} year = 2012 journal = {Journal of Artificial Intelligence Research} pages = {43-86} volume = {43} },Scheduling problems in manufacturing logistics and project management have frequently been modeled using the framework of Resource Constrained Project Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the importance of these problems providing scalable solution schedules for RCPSP/max problems is a topic of extensive research. However all existing methods for solving RCPSP/max assume that durations of activities are known with certainty an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability weather changes etc. lead to delays or advances in completion of activities. Thus in this paper our focus is on providing a scalable method for solving RCPSP/max problems with durational uncertainty. To that end we introduce the robust local search method consisting of three key ideas: (a) Introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) Deriving the expression for robust makespan of an execution strategy based on decision rule approximations; and (c) A robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty. Furthermore we also provide enhancements to local search that exploit temporal dependencies between activities. Our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently.,http://research.google.com/pubs/archive/37715.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Robust+Local+Search+for+Solving+RCPSP/max+with+Durational+Uncertainty+Fu+Lau+Varakantha+Xiao,http://research.google.com/pubs/pub37715.html
Mahout in Action,Manning Manning Publications Co. Sound View Ct. #3B Greenwich CT 06830 (2010) pp. 350,2010,Robin Anil Sean Owen Ted Dunning Ellen Friedman,@book{36682 title = {Mahout in Action} author = {Robin Anil and Sean Owen and Ted Dunning and Ellen Friedman} year = 2010 URL = {http://manning.com/owen/} booktitle = {Mahout in Action} pages = {350} address = {Manning Publications Co. Sound View Ct. #3B Greenwich CT 06830} },"A computer system that learns and adapts as it collects data is an extraordinarily interesting and powerful concept. With new technologies to capture store and process information machine learning has moved from the academic edges of computer science to the middle of the mainstream. Mahout an open source machine learning library captures the core algorithms of recommendation systems classification and clustering in ready-to-use scalable libraries. With Mahout you can immediately apply the machine learning techniques that drive Amazon Netflix and other data-centric businesses to your own projects. Mahout in Action explores machine learning through Apache's scalable machine learning project Mahout. Following real-world examples it introduces practical use cases and then illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability and how to apply these techniques against large data sets using the Apache Hadoop framework. In this book you'll use Mahout to dive into three practical applications of machine learning: Recommendations. Using group user history and preferences you can make accurate recommendations for individual users. This is an extremely powerful principle because accurate recommendations are beneficial both to customers and vendors. Clustering. Learn to automatically discover logical groupings with groups of data or data sets such as documents or lists. This technique is especially useful to search and data mining applications. Classification. Determining on the fly whether a thing fits a category based on its attributes and previous history can help instantaneously organize unstructured groups. For instance you'll learn about filtering techniques that decide whether email messages should be considered ""spam."" Mahout in Action is written primarily for developers who need to become better practitioners of machine learning techniques. It is also appropriate for researchers who understand the techniques and want to understand how to apply them effectively at scale. It assumes familiarity with Java and some basic grounding in machine learning techniques but no previous exposure to Mahout is necessary.",http://manning.com/owen/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mahout+in+Action+Anil+Owen+Dunning+Friedman,http://research.google.com/pubs/pub36682.html
MAO - an Extensible Micro-Architectural Optimizer,Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization ACM (2011),2011,Robert Hundt Easwaran Raman Martin Thuresson Neil Vachharajani,@inproceedings{37077 title = {MAO - an Extensible Micro-Architectural Optimizer} author = {Robert Hundt and Easwaran Raman and Martin Thuresson and Neil Vachharajani} year = 2011 booktitle = {Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization} },Performance matters and so does repeatability and predictability. Today's processors' micro-architectures have become so complex as to now contain many undocumented not understood and even puzzling performance cliffs. Small changes in the instruction stream such as the insertion of a single NOP instruction can lead to significant performance deltas with the effect of exposing compiler and performance optimization efforts to perceived unwanted randomness. This paper presents MAO an extensible micro-architectural assembly to assembly optimizer which seeks to address this problem for x86/64 processors. In essence MAO is a thin wrapper around a common open source assembler infrastructure. It offers basic operations such as creation or modification of instructions simple data-flow analysis and advanced infra-structure such as loop recognition and a repeated relaxation algorithm to compute instruction addresses and lengths. This infrastructure enables a plethora of passes for pattern matching alignment specific optimizations peep-holes experiments (such as random insertion of NOPs) and fast prototyping of more sophisticated optimizations. MAO can be integrated into any compiler that emits assembly code or can be used standalone. MAO can be used to discover micro-architectural details semi-automatically. Initial performance results are encouraging.,http://research.google.com/pubs/archive/37077.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MAO+-+an+Extensible+Micro-Architectural+Optimizer+Hundt+Raman+Thuresson+Vachharajani,http://research.google.com/pubs/pub37077.html
How to Break Software,Addison-Wesley (2002),2002,James A. Whittaker,@book{37377 title = {How to Break Software} author = {James A. Whittaker} year = 2002 URL = {http://www.amazon.com/How-Break-Software-Practical-Testing/dp/0201796198/ref=sr_1_2?s=books&ie=UTF8&qid=1316462515&sr=1-2} },"How to Break Software is a departure from conventional testing in which testers prepare a written test plan and then use it as a script when testing the software. The testing techniques in this book are as flexible as conventional testing is rigid. And flexibility is needed in software projects in which requirements can change bugs can become features and schedule pressures often force plans to be reassessed. Software testing is not such an exact science that one can determine what to test in advance and then execute the plan and be done with it. Instead of a plan intelligence insight experience and a ""nose for where the bugs are hiding"" should guide testers. This book helps testers develop this insight. The techniques presented in this book not only allow testers to go off-script they encourage them to do so. Don't blindly follow a document that may be out of date and that was written before the product was even testable. Instead use your head! Open your eyes! Think a little test a little and then think a little more. This book does teach planning but in an ""on- the-fly while you are testing"" way. It also encourages automation with many repetitive and complex tasks that require good tools (one such tool is shipped with this book on the companion CD). However tools are never used as a replacement for intelligence. Testers do the thinking and use tools to collect data and help them explore applications more efficiently and effectively.",http://www.amazon.com/How-Break-Software-Practical-Testing/dp/0201796198/ref=sr_1_2?s=books&ie=UTF8&qid=1316462515&sr=1-2,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+to+Break+Software+Whittaker,http://research.google.com/pubs/pub37377.html
Heterogeneity in “Homogeneous” Warehouse-Scale Computers: A Performance Opportunity,IEEE Computer Architecture Letters (CAL) vol. Vol. 10 No. 2 (2011) pp. 29-32,2011,Jason Mars Lingjia Tang Robert Hundt,@article{37676 title = {Heterogeneity in “Homogeneous” Warehouse-Scale Computers: A Performance Opportunity} author = {Jason Mars and Lingjia Tang and Robert Hundt} year = 2011 URL = {http://www.computer.org/csdl/letters/ca/2011/02/lca2011020029-abs.html} journal = {IEEE Computer Architecture Letters (CAL)} pages = {29-32} volume = {Vol. 10 No. 2} },The class of modern datacenters recently coined as “warehouse scale computers” (WSCs) has traditionally been embraced as homogeneous computing platforms. However due to frequent machine replacements and upgrades modern WSCs are in fact composed of diverse commodity microarchitectures and machine configurations. Yet current WSCs are designed with an assumption of homogeneity leaving a potentially significant performance opportunity unexplored. In this paper we investigate the key factors impacting the available heterogeneity in modern WSCs and the benefit of exploiting this heterogeneity to maximize overall performance. We also introduce a new metric opportunity factor which can be used to quantify an application’s sensitivity to the heterogeneity in a given WSC. For applications that are sensitive to heterogeneity we observe a performance improvement of up to 70% when employing our approach. In a WSC composed of state-of-the-art machines we can improve the overall performance of the entire datacenter by 16% over the status quo.,http://www.computer.org/csdl/letters/ca/2011/02/lca2011020029-abs.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Heterogeneity+in+%E2%80%9CHomogeneous%E2%80%9D+Warehouse-Scale+Computers:+A+Performance+Opportunity+Mars+Tang+Hundt,http://research.google.com/pubs/pub37676.html
Empowering Online Advertisements by Empowering Viewers with the Right to Choose,Journal of Advertising Research vol. 52 (2012) pp. 65-71,2012,Max Pashkevich Sundar Dorai-Raj Melanie Kellar Dan Zigmond,@article{40590 title = {Empowering Online Advertisements by Empowering Viewers with the Right to Choose} author = {Max Pashkevich and Sundar Dorai-Raj and Melanie Kellar and Dan Zigmond} year = 2012 journal = {Journal of Advertising Research} pages = {65--71} volume = {52} },In 2010 YouTube introduced TrueView in-stream advertising—online video advertisements that allowed the user to skip directly to the desired video content after five seconds of viewing. Google sought to compare these “skippable” in-stream advertisements to the conventional (non-skippable) in-stream video advertising formats using a new advertising effectiveness metric based on the propensity to search for terms related to advertising content. Google’s findings indicated that skippable video advertisements may be as effective on a per-impression basis as traditional video advertisements. In addition data from randomized experiments showed a strong implied viewer preference for the skippable advertisements. Taken together these results suggest that formats like TrueView in-stream advertisements can improve the viewing experience for users without sacrificing advertising value for advertisers or content owners.,http://research.google.com/pubs/archive/40590.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Empowering+Online+Advertisements+by+Empowering+Viewers+with+the+Right+to+Choose+Pashkevich+Dorai-Raj+Kellar+Zigmond,http://research.google.com/pubs/pub40590.html
Power Management of Online Data-Intensive Services,Proceedings of the 38th ACM International Symposium on Computer Architecture (2011),2011,David Meisner Christopher M. Sadler Luiz André Barroso Wolf-Dietrich Weber Thomas F. Wenisch,@inproceedings{37062 title = {Power Management of Online Data-Intensive Services} author = {David Meisner and Christopher M. Sadler and Luiz André Barroso and Wolf-Dietrich Weber and Thomas F. Wenisch} year = 2011 booktitle = {Proceedings of the 38th ACM International Symposium on Computer Architecture} },Much of the success of the Internet services model can be attributed to the popularity of a class of workloads that we call Online Data-Intensive (OLDI) services. These workloads perform significant computing over massive data sets per user request but unlike their offline counterparts (such as MapReduce computations) they require responsiveness in the sub-second time scale at high request rates. Large search products online advertising and machine translation are examples of workloads in this class. Although the load in OLDI services can vary widely during the day their energy consumption sees little variance due to the lack of energy proportionality of the underlying machinery. The scale and latency sensitivity of OLDI workloads also make them a challenging target for power management techniques. We investigate what if anything can be done to make OLDI systems more energy-proportional. Specifically we evaluate the applicability of active and idle low-power modes to reduce the power consumed by the primary server components (processor memory and disk) while maintaining tight response time constraints particularly on 95th-percentile latency. Using Web search as a representative example of this workload class we first characterize a production Web search workload at cluster-wide scale. We provide a fine-grain characterization and expose the opportunity for power savings using low-power modes of each primary server component. Second we develop and validate a performance model to evaluate the impact of processor- and memory-based low-power modes on the search latency distribution and consider the benefit of current and foreseeable low-power modes. Our results highlight the challenges of power management for this class of workloads. In contrast to other server workloads for which idle low-power modes have shown great promise for OLDI workloads we find that energy-proportionality with acceptable query latency can only be achieved using coordinated full-system active low-power modes.,http://research.google.com/pubs/archive/37062.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Power+Management+of+Online+Data-Intensive+Services+Meisner+Sadler+Barroso+Weber+Wenisch,http://research.google.com/pubs/pub37062.html
Fast Elliptic Curve Cryptography in OpenSSL,Financial Cryptography and Data Security: FC 2011 Workshops RLCPS and WECSR Springer,2011,Emilia Kasper,@inproceedings{37376 title = {Fast Elliptic Curve Cryptography in OpenSSL} author = {Emilia Kasper} year = 2011 booktitle = {Financial Cryptography and Data Security: FC 2011 Workshops RLCPS and WECSR} },We present a 64-bit optimized implementation of the NIST and SECG-standardized elliptic curve P-224. Our implementation is fully integrated into OpenSSL 1.0.1: full TLS handshakes using a 1024-bit RSA certificate and ephemeral Elliptic Curve Diffie-Hellman key exchange over P-224 now run at twice the speed of standard OpenSSL while atomic elliptic curve operations are up to 4 times faster. In addition our implementation is immune to timing attacks - most notably we show how to do small table look-ups in a cache-timing resistant way allowing us to use precomputation. To put our results in context we also discuss the various security performance trade-offs available to TLS applications.,http://research.google.com/pubs/archive/37376.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Elliptic+Curve+Cryptography+in+OpenSSL+Kasper,http://research.google.com/pubs/pub37376.html
How difficult is it to develop a perfect spell-checker? A cross-linguistic analysis through complex network approach,Textgraphs 2 Workshop at HLT/NAACL ACL (2007) pp. 8,2007,Monojit Choudhury Markose Thomas Animesh Mukherjee Niloy Ganguly Anupam Basu,@inproceedings{32805 title = {How difficult is it to develop a perfect spell-checker? A cross-linguistic analysis through complex network approach} author = {Monojit Choudhury and Markose Thomas and Animesh Mukherjee and Niloy Ganguly and Anupam Basu} year = 2007 URL = {http://www.textgraphs.org/ws07} booktitle = {Textgraphs 2 Workshop at HLT/NAACL} pages = {8} },The difficulties involved in spelling error detection and correction in a language have been investigated in this work through the conceptualization of SpellNet - a weighted network of words where edges indicate orthographic proximity between two words. We construct SpellNets for three languages - Bengali English and Hindi. Through appropriate mathematical analysis and/or intuitive justification we interpret the different topological metrics of SpellNet from the perspective of the issues related to spell-checking. We make many interesting observations the most significant being that the probability of making a read word error in a language is proportionate to the average weighted degree of SpellNet which is found to be highest for Hindi followed by Bengali and English.,http://www.textgraphs.org/ws07,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+difficult+is+it+to+develop+a+perfect+spell-checker%3F+A+cross-linguistic+analysis+through+complex+network+approach+Choudhury+Thomas+Mukherjee+Ganguly+Basu,http://research.google.com/pubs/pub32805.html
Software Defined Networking at Scale,Light Reading (2014) pp. 22,2014,Bikash Koley,@misc{42948 title = {Software Defined Networking at Scale} author = {Bikash Koley} year = 2014 },Software Defined Networks require Software Defined Operations. Google made great progress in SDN data and control plane. This talk discusses how we are working with the industry to transform the network management plane into a software defined framework.,http://research.google.com/pubs/archive/42948.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Software+Defined+Networking+at+Scale+Koley,http://research.google.com/pubs/pub42948.html
Deploying Google Search by Voice in Cantonese,12th Annual Conference of the International Speech Communication Association (Interspeech 2011) pp. 2865-2868,2011,Yun-hsuan Sung Martin Jansche Pedro Moreno,@inproceedings{37116 title = {Deploying Google Search by Voice in Cantonese} author = {Yun-hsuan Sung and Martin Jansche and Pedro Moreno} year = 2011 URL = {http://www.isca-speech.org/archive/interspeech_2011/i11_2865.html} booktitle = {12th Annual Conference of the International Speech Communication Association (Interspeech 2011)} pages = {2865--2868} },We describe our efforts in deploying Google search by voice for Cantonese a southern Chinese dialect widely spoken in and around Hong Kong and Guangzhou. We collected audio data from local Cantonese speakers in Hong Kong and Guangzhou by using our DataHound smartphone application. This data was used to create appropriate acoustic models. Language models were trained on anonymized query logs from Google Web Search for Hong Kong. Because users in Hong Kong frequently mix English and Cantonese in their queries we designed our system from the ground up to handle both languages. We report on experiments with different techniques for mapping the phoneme inventories for both languages into a common space. Based on extensive experiments we report word error rates and web scores for both Hong Kong and Guangzhou data. Cantonese Google search by voice was launched in December 2010.,http://research.google.com/pubs/archive/37116.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deploying+Google+Search+by+Voice+in+Cantonese+Sung+Jansche+Moreno,http://research.google.com/pubs/pub37116.html
SAC079 - SSAC Advisory on the Changing Nature of IPv4 Address Semantics,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2016) pp. 17,2016,Warren Kumari Don Blumenthal Russ Mundy,@incollection{44842 title = {SAC079 - SSAC Advisory on the Changing Nature of IPv4 Address Semantics} author = {Warren Kumari and Don Blumenthal and Russ Mundy} year = 2016 booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} pages = {17} },In this advisory the SSAC considers the changing role of Internet Protocol Version 4 (IPv4) addresses caused by the increasing scarcity and subsequent exhaustion of IPv4 addresses. The exhaustion of the IPv4 address supply has been predicted since the end of the 1980s. However the large scale adoption of mobile devices and their associated IPv4 addressing needs accelerated the exhaustion timetable and placed increased pressure on network operators to conserve IPv4 addresses. This pressure has resulted in a marked increase in the use of Network Address Translation (NAT) technologies altering the attributability characteristics of IPv4 addresses and requiring changes to their interpretation by parties wishing to use them as endpoint identifiers.,http://research.google.com/pubs/archive/44842.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC079+-+SSAC+Advisory+on+the+Changing+Nature+of+IPv4+Address+Semantics+Kumari+Blumenthal+Mundy,http://research.google.com/pubs/pub44842.html
Deep Mixture Density Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2014) pp. 3872-3876,2014,Heiga Zen Andrew Senior,@inproceedings{42020 title = {Deep Mixture Density Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis} author = {Heiga Zen and Andrew Senior} year = 2014 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {3872--3876} },Statistical parametric speech synthesis (SPSS) using deep neural networks (DNNs) has shown its potential to produce naturally-sounding synthesized speech. However there are limitations in the current implementation of DNN-based acoustic modeling for speech synthesis such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.,http://research.google.com/pubs/archive/42020.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deep+Mixture+Density+Networks+for+Acoustic+Modeling+in+Statistical+Parametric+Speech+Synthesis+Zen+Senior,http://research.google.com/pubs/pub42020.html
Approximation Algorithms for the Directed k-Tour and k-Stroll Problems,Algorithmica vol. 65 (2013) pp. 545-561,2013,Mohammadhossein Bateni Julia Chuzhoy,@article{40757 title = {Approximation Algorithms for the Directed k-Tour and k-Stroll Problems} author = {Mohammadhossein Bateni and Julia Chuzhoy} year = 2013 journal = {Algorithmica} pages = {545-561} volume = {65} },We consider two natural generalizations of the Asymmetric Traveling Salesman problem: the k-Stroll and the k-Tour problems. The input to the k-Stroll problem is a directed n-vertex graph with nonnegative edge lengths an integer k as well as two special vertices s and t. The goal is to find a minimum-length s-t walk containing at least k distinct vertices (including the endpoints st). The k-Tour problem can be viewed as a special case of k-Stroll where s=t. That is the walk is required to be a tour containing some pre-specified vertex s. When k=n the k-Stroll problem becomes equivalent to Asymmetric Traveling Salesman Path and k-Tour to Asymmetric Traveling Salesman. Our main result is a polylogarithmic approximation algorithm for the k-Stroll problem. Prior to our work only bicriteria (O(log2 k)3)-approximation algorithms have been known producing walks whose length is bounded by 3OPT while the number of vertices visited is ?(k/log2 k). We also show a simple O(log2 n/loglogn)-approximation algorithm for the k-Tour problem. The best previously known approximation algorithms achieved min(O(log3 k)O(log2 n?logk/loglogn)) approximation in polynomial time and O(log2 k) approximation in quasipolynomial time.,http://research.google.com/pubs/archive/40757.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Approximation+Algorithms+for+the+Directed+k-Tour+and+k-Stroll+Problems+Bateni+Chuzhoy,http://research.google.com/pubs/pub40757.html
Bayes and Big Data: The Consensus Monte Carlo Algorithm,Bayes 250 (2013) (to appear),2013,Steven L. Scott Alexander W. Blocker Fernando V. Bonassi,@inproceedings{41849 title = {Bayes and Big Data: The Consensus Monte Carlo Algorithm} author = {Steven L. Scott and Alexander W. Blocker and Fernando V. Bonassi} year = 2013 booktitle = {Bayes 250} },A useful definition of ``big data'' is data that is too big to comfortably process on a single machine either because of processor memory or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated) so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine and then averaging individual Monte Carlo draws across machines. Depending on the model the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available for large single-layer hierarchical models and for Bayesian additive regression trees (BART).,http://research.google.com/pubs/archive/41849.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bayes+and+Big+Data:++The+Consensus+Monte+Carlo+Algorithm+Scott+Blocker+Bonassi,http://research.google.com/pubs/pub41849.html
Alice in Warningland: A Large-Scale Field Study of Browser Security Warning Effectiveness,USENIX Security Symposium USENIX (2013),2013,Devdatta Akhawe Adrienne Porter Felt,@inproceedings{41323 title = {Alice in Warningland: A Large-Scale Field Study of Browser Security Warning Effectiveness} author = {Devdatta Akhawe and Adrienne Porter Felt} year = 2013 booktitle = {USENIX Security Symposium} },We empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature. We used Mozilla Firefox and Google Chrome's in-browser telemetry to observe over 25 million warning impressions in situ. During our field study users continued through a tenth of Mozilla Firefox's malware and phishing warnings a quarter of Google Chrome's malware and phishing warnings and a third of Mozilla Firefox's SSL warnings. This demonstrates that security warnings can be effective in practice; security experts and system architects should not dismiss the goal of communicating security information to end users. We also find that user behavior varies across warnings. In contrast to the other warnings users continued through 70.2% of Google Chrome's SSL warnings. This indicates that the user experience of a warning can have a significant impact on user behavior. Based on our findings we make recommendations for warning designers and researchers.,http://research.google.com/pubs/archive/41323.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Alice+in+Warningland:+A+Large-Scale+Field+Study+of+Browser+Security+Warning+Effectiveness+Akhawe+Felt,http://research.google.com/pubs/pub41323.html
How Surfers Watch: Measuring audience response to video advertising online,Proceedings of ADKDD (2010),2010,Sundar Dorai-Raj Dan Zigmond,@inproceedings{36496 title = {How Surfers Watch: Measuring audience response to video advertising online} author = {Sundar Dorai-Raj and Dan Zigmond} year = 2010 booktitle = {Proceedings of ADKDD} },For several years Google has been analyzing television set-top box data to measure audience response to specific TV ads. This paper presents how similar techniques can be applied to online video advertising on YouTube. As more and more video programming is made available online it will become increasingly important to understand how to engage with online viewers through video advertising. Furthermore we find that viewing behavior is even more effected by specific video ad creatives online than it is on TV. This suggests that online viewing can become a valuable source data on viewer response to video ad creatives more generally.,http://research.google.com/pubs/archive/36496.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Surfers+Watch:+Measuring+audience+response+to+video+advertising+online+Dorai-Raj+Zigmond,http://research.google.com/pubs/pub36496.html
Shopping for Top Forums: Discovering Online Discussion for Product Research,KDD SOMA 2010 Workshop on Social Media Analytics,2010,Jonathan Elsas Natalie Glance,@inproceedings{36551 title = {Shopping for Top Forums: Discovering Online Discussion for Product Research} author = {Jonathan Elsas and Natalie Glance} year = 2010 URL = {http://snap.stanford.edu/soma2010/papers/soma2010_4.pdf} booktitle = {KDD SOMA 2010 Workshop on Social Media Analytics} },Community generated content or social media has become increasingly important over the past several years. Social media sites such as blogs twitter and online discussion boards have been recognized as valuable sources of market intelligence for companies wishing to keep abreast of their customers' attitudes expressed online. There has been little focus however on providing a similar service to potential customers. In this paper we present a system for aiding consumers with their product research by providing access to community generated content. We focus specifically on online forums or message boards which are particularly useful for product research. These web sites often host discussion among users with ﬁrst-hand product experiences expert users and enthusiasts. The system presented here is designed to integrate with a shopping search portal providing access to online forums that are likely to have a significant amount of discussion relating to a user's expressed interest in product brands and categories. We describe this system and present experiments showing that in the context of a shopping search engine the proposed system is preferred or equivalent to results from a web search engine 80% of the time and achieves accuracy at the top ranked result of 85%.,http://research.google.com/pubs/archive/36551.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Shopping+for+Top+Forums:+Discovering+Online+Discussion+for+Product+Research+Elsas+Glance,http://research.google.com/pubs/pub36551.html
Semi-supervised Learning of Dependency Parsers using Generalized Expectation Criteria,IJCNLP-ACL (2009),2009,Gregory Druck Gideon S. Mann Andrew McCallum,@inproceedings{35524 title = {Semi-supervised Learning of Dependency Parsers using Generalized Expectation Criteria} author = {Gregory Druck and Gideon S. Mann and Andrew McCallum} year = 2009 booktitle = {IJCNLP-ACL} },In this paper we propose a novel method for semi-supervised learning of nonprojective log-linear dependency parsers using directly expressed linguistic prior knowledge (e.g. a noun’s parent is often a verb). Model parameters are estimated using a generalized expectation (GE) objective function that penalizes the mismatch between model predictions and linguistic expectation constraints. In a comparison with two prominent “unsupervised” learning methods that require indirect biasing toward the correct syntactic structure we show that GE can attain better accuracy with as few as 20 intuitive constraints. We also present positive experimental results on longer sentences in multiple languages.,http://research.google.com/pubs/archive/35524.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semi-supervised+Learning+of+Dependency+Parsers+using+Generalized+Expectation+Criteria+Druck+Mann+McCallum,http://research.google.com/pubs/pub35524.html
Measuring the Impact of Advertising on YouTube Traffic,The Market Research Event The Market Research Event Orlando FL (2011),2011,Sundar Dorai-Raj,@inproceedings{37562 title = {Measuring the Impact of Advertising on YouTube Traffic} author = {Sundar Dorai-Raj} year = 2011 booktitle = {The Market Research Event} address = {Orlando FL} },At YouTube balancing ad load and user happiness is a major concern. One way we measure this tradeoff is through live experiments which we run on a small percentage of traffic. For example by holding back certain ad formats we can build metrics around the impact of YouTube advertising on the user experience. In this talk we will discuss the benefits and challenges of running large-scale advertising experiments.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Measuring+the+Impact+of+Advertising+on+YouTube+Traffic+Dorai-Raj,http://research.google.com/pubs/pub37562.html
Haplotype Inference Constrained by Plausible Haplotype Data,CPM '09: Proceedings of the 20th Annual Symposium on Combinatorial Pattern Matching Springer-Verlag Berlin Heidelberg (2009) pp. 339-352,2009,Michael R. Fellows Tzvika Hartman Danny Hermelin Gad M. Landau Frances Rosamond Liat Rozenberg,@inproceedings{36219 title = {Haplotype Inference Constrained by Plausible Haplotype Data} author = {Michael R. Fellows and Tzvika Hartman and Danny Hermelin and Gad M. Landau and Frances Rosamond and Liat Rozenberg} year = 2009 booktitle = {CPM '09: Proceedings of the 20th Annual Symposium on Combinatorial Pattern Matching} pages = {339--352} address = {Berlin Heidelberg} },The haplotype inference problem (HIP) asks to find a set of haplotypes which resolve a given set of genotypes. This problem is of enormous importance in many practical fields such as the investigation of diseases or other types of genetic mutations. In order to find the haplotypes that are as close as possible to the real set of haplotypes that comprise the genotypes two models have been suggested which by now have become widely accepted: The perfect phylogeny model and the pure parsimony model. All known algorithms up till now for the above problem may find haplotypes that are not necessarily plausible i.e. very rare haplotypes or haplotypes that were never observed in the population. In order to overcome this disadvantage we study in this paper for the first time a new constrained version of HIP under the above mentioned models. In this new version a pool of plausible haplotypes ~H is given together with the set of genotypes G and the goal is to find a subset $H \subseteq \widetilde{H}$ that resolves G. For the constrained perfect phylogeny haplotyping (CPPH) problem we provide initial insights and polynomial-time algorithms for some restricted cases that help understanding the complexity of that problem. We also prove that the constrained parsimony haplotyping (CPH) problem is fixed parameter tractable by providing a parameterized algorithm that applies an interesting dynamic programming technique for solving the problem.,http://dx.doi.org/10.1007/978-3-642-02441-2_30,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Haplotype+Inference+Constrained+by+Plausible+Haplotype+Data+Fellows+Hartman+Hermelin+Landau+Rosamond+Rozenberg,http://research.google.com/pubs/pub36219.html
Web Search for a Planet: The Google Cluster Architecture,IEEE Micro vol. 23 (2003) pp. 22-28,2003,Luiz Andre Barroso Jeffrey Dean Urs Hölzle,@article{49 title = {Web Search for a Planet: The Google Cluster Architecture} author = {Luiz Andre Barroso and Jeffrey Dean and Urs Hölzle} year = 2003 URL = {http://research.google.com/archive/googlecluster.html} journal = {IEEE Micro} pages = {22-28} volume = {23} },Amenable to extensive parallelization Google's Web search application lets different queries run on different processors and by partitioning the overall index also lets a single query use multiple processors. To handle this workload Google's architecture features clusters of more than 15000 commodity class PCs with fault-tolerant software. This architecture achieves superior performance at a fraction of the cost of a system built from fewer but more expensive high-end servers.,http://research.google.com/archive/googlecluster.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Search+for+a+Planet:+The+Google+Cluster+Architecture+Barroso+Dean+Holzle,http://research.google.com/pubs/pub49.html
Public vs. Publicized: Content Use Trends and Privacy Expectations,6th USENIX Workshop on Hot Topics in Security (HotSec '11) USENIX (2011),2011,Jessica Staddon Andrew Swerdlow,@inproceedings{37150 title = {Public vs. Publicized: Content Use Trends and Privacy Expectations} author = {Jessica Staddon and Andrew Swerdlow} year = 2011 booktitle = {6th USENIX Workshop on Hot Topics in Security (HotSec '11)} },From a semantic standpoint there is a clear differentia- tion between the meanings of public and publicized con- tent. The former includes any content that is accessible by anyone while the latter emphasizes visibility – publi- cized content is actively made available. As a user’s on- line experience becomes more personalized and data is increasingly pushed rather than pulled the line between public and publicized content is inevitably blurred. In this position paper we present quantitative evidence that despite this trend in some settings users do not antici- pate the use of public content beyond the narrow context in which is was disclosed; they do not anticipate that the content may be publicized. While providing a “publicized” option for data is an important counterpart to the ability to limit access to data (e.g. through access con- trol lists) such an option must be accompanied by both greater user awareness of the ramifications of such an option and by transparency into data usage.,http://research.google.com/pubs/archive/37150.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Public+vs.+Publicized:+Content+Use+Trends+and+Privacy+Expectations+Staddon+Swerdlow,http://research.google.com/pubs/pub37150.html
Distributed Gibbs sampling for latent variable models,Scaling up Machine Learning Cambridge (2012) (to appear),2012,Arthur Asuncion Padhraic Smyth Max Welling David Newman Ian Porteous Scott Triglia,@inbook{37361 title = {Distributed Gibbs sampling for latent variable models} author = {Arthur Asuncion and Padhraic Smyth and Max Welling and David Newman and Ian Porteous and Scott Triglia} year = 2012 booktitle = {Scaling up Machine Learning} },This book presents an integrated collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits trade-offs and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters concurrent programming frameworks including CUDA MPI MapReduce and DryadLINQ and learning settings (supervised unsupervised semi-supervised and online learning). Extensive coverage of parallelization of boosted trees SVMs spectral clustering belief propagation and other popular learning algorithms and deep dives into several applications make the book equally useful for researchers students and practitioners,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Gibbs+sampling+for+latent+variable+models+Asuncion+Smyth+Welling+Newman+Porteous+Triglia,http://research.google.com/pubs/pub37361.html
Learning to be a software engineer in a complex organization: A case study focusing on apprenticeship/practice based learning for getting new engineers productive in contributing to the Google codebase,Journal of Workplace Learning vol. 22 no. 3 (2010) pp. 180-194,2010,Maggie Johnson Max Senges,@article{34908 title = {Learning to be a software engineer in a complex organization: A case study focusing on apprenticeship/practice based learning for getting new engineers productive in contributing to the Google codebase} author = {Maggie Johnson and Max Senges} year = 2010 URL = {http://dx.doi.org/10.1108/13665621011028620} journal = {Journal of Workplace Learning} pages = {180-194} volume = {22 no. 3} },Purpose This paper seeks to analyse the effectiveness and impact of how Google currently trains its new software engineers (“Nooglers”) to become productive in the software engineering community. The research focuses on the institutions and support for practice-based learning and cognitive apprenticeship in the Google environment. Design/methodology/approach The study uses a series of semi-structured interviews with 24 Google stakeholders. These interviews are complemented by observations document analysis and review of existing survey and statistical data. Findings It is found that Google offers a state-of-the-art onboarding program and benchmark qualities that provide legitimate peripheral participation. The research reveals how Google empowers programmers to “feel at home” using company coding practices as well as maximizing peer-learning and collaborative practices. These practices reduce isolation enhance collegiality and increase employee morale and job satisfaction. Research limitations/implications The case study describes the practices in one company. Practical implications The research documented in the paper can be used as a benchmark for other onboarding and practice-based learning set-ups. Originality/value This is the first research that gives insights into the practice-based learning and onboarding practices at Google. The practices are assessed to be state-of-the-art and the insights therefore relevant for benchmarking exercises of other companies.,http://dx.doi.org/10.1108/13665621011028620,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+be+a+software+engineer+in+a+complex+organization:+A+case+study+focusing+on+apprenticeship/practice+based+learning+for+getting+new+engineers+productive+in+contributing+to+the+Google+codebase+Johnson+Senges,http://research.google.com/pubs/pub34908.html
Web-scale Data Integration: You can only afford to Pay As You Go,CIDR (2007),2007,Jayant Madhavan Shawn R. Jeffery Shirley Cohen Xin (Luna) Dong David Ko Cong Yu Alon Halevy,@inproceedings{32784 title = {Web-scale Data Integration: You can only afford to Pay As You Go} author = {Jayant Madhavan and Shawn R. Jeffery and Shirley Cohen and Xin (Luna) Dong and David Ko and Cong Yu and Alon Halevy} year = 2007 booktitle = {CIDR} },The World Wide Web is witnessing an increase in the amount of structured content - vast heterogeneous collections of structured data are on the rise due to the Deep Web annotation schemes like Flickr and sites like Google Base. While this phenomenon is creating an opportunity for structured data management dealing with heterogeneity on the web-scale presents many new challenges. In this paper we highlight these challenges in two scenarios - the Deep Web and Google Base. We contend that traditional data integration techniques are no longer valid in the face of such heterogeneity and scale. We propose a new data integration architecture PAYGO which is inspired by the concept of dataspaces and emphasizes pay-as-you-go data management as means for achieving web-scale data integration.,http://research.google.com/pubs/archive/32784.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web-scale+Data+Integration:+You+can+only+afford+to+Pay+As+You+Go+Madhavan+Jeffery+Cohen+Dong+Ko+Yu+Halevy,http://research.google.com/pubs/pub32784.html
Building Useful Program Analysis Tools Using an Extensible Java Compiler,International Working Conference on Source Code Analysis and Manipulation (SCAM) IEEE (2012) pp. 14-23,2012,Edward Aftandilian Raluca Sauciuc Siddharth Priya Sundaresan Krishnan,@inproceedings{38275 title = {Building Useful Program Analysis Tools Using an Extensible Java Compiler} author = {Edward Aftandilian and Raluca Sauciuc and Siddharth Priya and Sundaresan Krishnan} year = 2012 URL = {http://www.eecs.tufts.edu/~eaftan/papers/experience-scam-2012.pdf} booktitle = {International Working Conference on Source Code Analysis and Manipulation (SCAM)} pages = {14--23} },Large software companies need customized tools to manage their source code. These tools are often built in an ad-hoc fashion using brittle technologies such as regular expressions and home-grown parsers. Changes in the language cause the tools to break. More importantly these ad-hoc tools often do not support uncommon-but-valid code code patterns. We report our experiences building source-code analysis tools at Google on top of a third-party open-source extensible compiler. We describe three tools in use on our Java codebase. The first Strict Java Dependencies enforces our dependency policy in order to reduce JAR file sizes and testing load. The second error-prone adds new error checks to the compilation process and automates repair of those errors at a whole-codebase scale. The third Thindex reduces the indexing burden for a Java IDE so that it can support Google-sized projects.,http://research.google.com/pubs/archive/38275.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Building+Useful+Program+Analysis+Tools+Using+an+Extensible+Java+Compiler+Aftandilian+Sauciuc+Priya+Krishnan,http://research.google.com/pubs/pub38275.html
Catching a Viral Video,IEEE SIASP@ICDM 2010,2010,Tom Broxton Yannet Interian Jon Vaver Mirjam Wattenhofer,@inproceedings{36697 title = {Catching a Viral Video} author = {Tom Broxton and Yannet Interian and Jon Vaver and Mirjam Wattenhofer} year = 2010 booktitle = {IEEE SIASP@ICDM 2010} },The sharing and re-sharing of videos on social sites blogs e-mail and other means has given rise to the phenomenon of viral videos – videos that become popular through internet sharing. In this paper we seek to better understand viral videos on YouTube by analyzing sharing and its relationship to video popularity using 1.5 million YouTube videos. The socialness of a video is quantified by classifying the referrer sources for video views as social (e.g. an emailed link) or non-social (e.g. a link from related videos). By segmenting videos according to their fraction of social views we find that viewership patterns of highly social videos is very different than less social videos. For example the highly social videos rise to and fall from their peak popularity more quickly than less social videos. We also find that not all highly social videos become popular and not all popular videos are highly social. And despite their ability to generate large volumes of views over a short period of time only 21% of the most popular videos (in terms of 30-day views) can be classified as viral. The observations made here lay the ground work for future work related to the creation of classification and predictive models for online videos.,http://research.google.com/pubs/archive/36697.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Catching+a+Viral+Video+Broxton+Interian+Vaver+Wattenhofer,http://research.google.com/pubs/pub36697.html
Cascades of two-pole–two-zero asymmetric resonators are good models of peripheral auditory function,Journal of the Acoustical Society of America vol. 130 (2011) pp. 3893-3904,2011,Richard F. Lyon,@article{36963 title = {Cascades of two-pole–two-zero asymmetric resonators are good models of peripheral auditory function} author = {Richard F. Lyon} year = 2011 URL = {http://dicklyon.com/tech/Hearing/Lyon2011_JASMAN13063893_1.pdf} journal = {Journal of the Acoustical Society of America} pages = {3893--3904} volume = {130} },A cascade of two-pole–two-zero filter stages is a good model of the auditory periphery in two distinct ways. First in the form of the pole–zero filter cascade it acts as an auditory filter model that provides an excellent fit to data on human detection of tones in masking noise with fewer fitting parameters than previously reported filter models such as the roex and gammachirp models. Second when extended to the form of the cascade of asymmetric resonators with fast-acting compression it serves as an efficient front-end filterbank for machine-hearing applications including dynamic nonlinear effects such as fast wide-dynamic-range compression. In their underlying linear approximations these filters are described by their poles and zeros that is by rational transfer functions which makes them simple to implement in analog or digital domains. Other advantages in these models derive from the close connection of the filter-cascade architecture to wave propagation in the cochlea. These models also reflect the automatic-gain-control function of the auditory system and can maintain approximately constant impulse-response zero-crossing times as the level-dependent parameters change. Copyright (2011) Acoustical Society of America. This article may be downloaded for personal use only. Any other use requires prior permission of the author and the Acoustical Society of America. The article appeared in J. Acoust. Soc. Am. vol. 130 and may be found via http://asadl.org/jasa/resource/1/jasman/v130/i6/p3893_s1.,http://research.google.com/pubs/archive/36963.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cascades+of+two-pole%E2%80%93two-zero+asymmetric+resonators+are+good+models+of+peripheral+auditory+function+Lyon,http://research.google.com/pubs/pub36963.html
Tracking Large-Scale Video Remix in Real-World Events,IEEE Transactions on Multimedia vol. 15 no. 6 (2013) pp. 1244-1254,2013,Lexing Xie Apostol Natsev Xuming He John R. Kender Matthew L. Hill John R. Smith,@article{41769 title = {Tracking Large-Scale Video Remix in Real-World Events} author = {Lexing Xie and Apostol Natsev and Xuming He and John R. Kender and Matthew L. Hill and John R. Smith} year = 2013 journal = {IEEE Transactions on Multimedia} pages = {1244-1254} volume = {15 no. 6} },Content sharing networks such as YouTube contain traces of both explicit online interactions (such as likes comments or subscriptions) as well as latent interactions (such as quoting or remixing parts of a video). We propose visual memes or frequently re-posted short video segments for detecting and monitoring such latent video interactions at scale. Visual memes are extracted by scalable detection algorithms that we develop with high accuracy. We further augment visual memes with text via a statistical model of latent topics. We model content interactions on YouTube with visual memes deﬁning several measures of inﬂuence and building predictive models for meme popularity. Experiments are carried out with over 2 million video shots from more than 40000 videos on two prominent news events in 2009: the election in Iran and the swine ﬂu epidemic. In these two events a high percentage of videos contain remixed content and it is apparent that traditional news media and citizen journalists have different roles in disseminating remixed content. We perform two quantitative evaluations for annotating visual memes and predicting their popularity. The proposed joint statistical model of visual memes and words outperforms an alternative concurrence model with an average error of 2% for predicting meme volume and 17% for predicting meme lifespan.,http://research.google.com/pubs/archive/41769.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Tracking+Large-Scale+Video+Remix+in+Real-World+Events+Xie+Natsev+He+Kender+Hill+Smith,http://research.google.com/pubs/pub41769.html
Universally optimal privacy mechanisms for minimax agents,Proc. ACM SIGMOD ACM Indianapolis Indiana (2010) pp. 135-146,2010,Mangesh Gupte Mukund Sundararajan,@inproceedings{36584 title = {Universally optimal privacy mechanisms for minimax agents} author = {Mangesh Gupte and Mukund Sundararajan} year = 2010 URL = {http://doi.acm.org/10.1145/1807085.1807105} booktitle = {Proc. ACM SIGMOD} pages = {135-146} address = {Indianapolis Indiana} },"A scheme that publishes aggregate information about sensitive data must resolve the trade-off between utility to information consumers and privacy of the database participants. Differential privacy is a well-established definition of privacy--this is a universal guarantee against all attackers whatever their side-information or intent. Can we have a similar universal guarantee for utility? There are two standard models of utility considered in decision theory: Bayesian and minimax. Ghosh et. al. show that a certain ""geometric mechanism"" gives optimal utility to all Bayesian information consumers. In this paper we prove a similar result for minimax information consumers. Our result also works for a wider class of information consumers which includes Bayesian information consumers and subsumes the result from [8]. We model information consumers as minimax (risk-averse) agents each endowed with a loss-function which models their tolerance to inaccuracies and each possessing some side-information about the query. Further information consumers are rational in the sense that they actively combine information from the mechanism with their side-information in a way that minimizes their loss. Under this assumption of rational behavior we show that for every fixed count query the geometric mechanism is universally optimal for all minimax information consumers. Additionally our solution makes it possible to release query results when information consumers are at different levels of privacy in a collusion-resistant manner.",http://doi.acm.org/10.1145/1807085.1807105,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Universally+optimal+privacy+mechanisms+for+minimax+agents+Gupte+Sundararajan,http://research.google.com/pubs/pub36584.html
Keeping the Web in Web 2.0: An HCI Approach to Designing Web Applications,ACM CHI 2007,2007,J. Mittleman Steffen Meschkat,@inproceedings{32823 title = {Keeping the Web in Web 2.0: An HCI Approach to Designing Web Applications} author = {J. Mittleman and Steffen Meschkat} year = 2007 note = {Conference course notes} booktitle = {ACM CHI 2007} },We will discuss javascript programming and AJAX the dominant tools for developing sophisticated applications on the web. You will come away with a general understanding of the building blocks and capabilities AJAX applications; and will have a headstart on learning to apply these tools to your own projects.,http://research.google.com/pubs/archive/32823.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Keeping+the+Web+in+Web+2.0:+An+HCI+Approach+to+Designing+Web+Applications+Mittleman+Meschkat,http://research.google.com/pubs/pub32823.html
Restoring Punctuation and Capitalization in Transcribed Speech,IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) (2009) pp. 4741-4744,2009,Agustín Gravano Martin Jansche Michiel Bacchiani,@inproceedings{34562 title = {Restoring Punctuation and Capitalization in Transcribed Speech} author = {Agustín Gravano and Martin Jansche and Michiel Bacchiani} year = 2009 note = {doi:10.1109/ICASSP.2009.4960690} booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4741--4744} },Adding punctuation and capitalization greatly improves the readability of automatic speech transcripts. We discuss an approach for performing both tasks in a single pass using a purely text-based n-gram language model. We study the effect on performance of varying the n-gram order (from n = 3 to n = 6) and the amount of training data (from 58 million to 55 billion tokens). Our results show that using larger training data sets consistently improves performance while increasing the n-gram order does not help nearly as much.,http://research.google.com/pubs/archive/34562.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Restoring+Punctuation+and+Capitalization+in+Transcribed+Speech+Gravano+Jansche+Bacchiani,http://research.google.com/pubs/pub34562.html
On Using Nearly-Independent Feature Families for High Precision and Confidence,Fourth Asian Machine Learning Conference JMLR workshop and conference proceedings (2012) pp. 269-284,2012,Omid Madani Manfred Georg David Ross,@inproceedings{40644 title = {On Using Nearly-Independent Feature Families for High Precision and Confidence} author = {Omid Madani and Manfred Georg and David Ross} year = 2012 URL = {http://jmlr.csail.mit.edu/proceedings/papers/v25/} booktitle = {Fourth Asian Machine Learning Conference} pages = {269-284} },Often we require classification at a very high precision level such as 99%. We report that when very different sources of evidence such as text audio and video features are available combining the outputs of base classifiers trained on each feature type separately aka late fusion can substantially increase the recall of the combination at high precisions compared to the performance of a single classifier trained on all the feature types i.e. early fusion or compared to the individual base classifiers. We show how the probability of a joint false-positive mistake can be upper bounded by the product of individual probabilities of conditional false-positive mistakes by identifying a simple key criterion that needs to hold. This provides an explanation for the high precision phenomenon and motivates referring to such feature families as (nearly) independent. We assess the relevant factors for achieving high precision empirically and explore combination techniques informed by the analysis. We compare a number of early and late fusion methods and observe that classifier combination via late fusion can more than double the recall at high precision.,http://research.google.com/pubs/archive/40644.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+Using+Nearly-Independent+Feature+Families+for+High+Precision+and+Confidence+Madani+Georg+Ross,http://research.google.com/pubs/pub40644.html
Modelling Score Distributions Without Actual Scores,Proceedings of the 2013 Conference on the Theory of Information Retrieval ACM New York NY USA pp. 85-92,2013,Stephen Robertson Evangelos Kanoulas Emine Yilmaz,@inproceedings{42036 title = {Modelling Score Distributions Without Actual Scores} author = {Stephen Robertson and Evangelos Kanoulas and Emine Yilmaz} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2499181} booktitle = {Proceedings of the 2013 Conference on the Theory of Information Retrieval} pages = {85--92} address = {New York NY USA} },Score-distribution models are used for various practical purposes in search for example for results merging and threshold setting. In this paper the basic ideas of the score-distributional approach to viewing and analyzing the effectiveness of search systems are re-examined. All recent score-distribution modelling work depends on the availability of actual scores generated by systems and makes assumptions about these scores. Such work is therefore not applicable to systems which do not generate or reveal such scores or whose scoring/ranking approach violates the assumptions. We demonstrate that it is possible to apply at least some score-distributional ideas without access to real scores knowing only the rankings produced (together with a single effectiveness metric based on relevance judgments). This new basic insight is illustrated by means of simulation experiments on a range of TREC runs some of whose reported scores are clearly unsuitable for existing methods.,http://research.google.com/pubs/archive/42036.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Modelling+Score+Distributions+Without+Actual+Scores+Robertson+Kanoulas+Yilmaz,http://research.google.com/pubs/pub42036.html
Circulant Binary Embedding,International Conference on Machine Learning (ICML) (2014),2014,Felix X. Yu Sanjiv Kumar Yunchao Gong Shih-Fu Chang,@inproceedings{43144 title = {Circulant Binary Embedding} author = {Felix X. Yu and Sanjiv Kumar and Yunchao Gong and Shih-Fu Chang} year = 2014 URL = {http://sanjivk.com/CBE_ICML14.pdf} booktitle = {International Conference on Machine Learning (ICML)} },Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices the proposed method improves the time complexity from O(d^2) to O(dlogd) and the space complexity from O(d^2) to O(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time and provides much faster computation with no performance degradation for fixed number of bits.,http://research.google.com/pubs/archive/43144.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Circulant+Binary+Embedding+Yu+Kumar+Gong+Chang,http://research.google.com/pubs/pub43144.html
Dynamic Stylized Shading Primitives,Proc. Symposium on NonPhotorealistic Animation and Rendering (NPAR 2011) ACM,2011,David Vanderhaeghe Romain Vergne Pascal Barla William Baxter,@inproceedings{37394 title = {Dynamic Stylized Shading Primitives} author = {David Vanderhaeghe and Romain Vergne and Pascal Barla and William Baxter} year = 2011 URL = {http://www.dlyr.fr/papers/DSSP/} booktitle = {Proc. Symposium on NonPhotorealistic Animation and Rendering (NPAR 2011)} },Shading appearance in illustrations comics and graphic novels is designed to convey illumination material and surface shape characteristics at once. Moreover shading may vary depending on different configurations of surface distance lighting character expressions timing of the action to articulate storytelling or draw attention to a part of an object. In this paper we present a method that imitates such expressive stylized shading techniques in dynamic 3D scenes and which offers a simple and flexible means for artists to design and tweak the shading appearance and its dynamic behavior. The key contribution of our approach is to seamlessly vary appearance by using a combination of shading primitives that take into account lighting direction material characteristics and surface features. We demonstrate their flexibility in a number of scenarios: minimal shading comics or cartoon rendering glossy and anisotropic material effects; including a variety of dynamic variations based on orientation timing or depth. Our prototype implementation combines shading primitives with a layered approach and runs in real-time on the GPU,http://www.dlyr.fr/papers/DSSP/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+Stylized+Shading+Primitives+Vanderhaeghe+Vergne+Barla+Baxter,http://research.google.com/pubs/pub37394.html
Robust Symbolic Regression with Affine Arithmetic,Genetic and Evolutionary Computation COnference (GECCO) (2010),2010,Cassio Pennachin Moshe Looks João A. de Vasconcelos,@inproceedings{36294 title = {Robust Symbolic Regression with Affine Arithmetic} author = {Cassio Pennachin and Moshe Looks and João A. de Vasconcelos} year = 2010 URL = {http://metacog.org/papers/gecco10.pdf} booktitle = {Genetic and Evolutionary Computation COnference (GECCO)} },We use affine arithmetic to improve both the performance and the robustness of genetic programming for symbolic regression. During evolution we use affine arithmetic to analyze expressions generated by the genetic operators giving an estimate of their output range given the ranges of their inputs over the training data. These estimated output ranges allow us to discard trees that contain asymptotes as well as those whose output is too far from the desired output range determined by the training instances. We also perform linear scaling of outputs before fitness evaluation. Experiments are performed on 15 problems comparing the proposed system with a baseline genetic programming system with protected operators and with a similar system based on interval arithmetic. Results show integrating affine arithmetic with an implementation of standard genetic programming reduces the number of fitness evaluations during training and improves generalization performance minimizes overfitting and completely avoids extreme errors on unseen test data.,http://research.google.com/pubs/archive/36294.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Robust+Symbolic+Regression+with+Affine+Arithmetic+Pennachin+Looks+Vasconcelos,http://research.google.com/pubs/pub36294.html
Social media in public opinion research,AAPOR AAPOR (2014) pp. 57,2014,Michael Link Joe Muphy Michael F. Schober Trent D. Buskirk Jennifer Hunter Childs Casey Langer Tesfaye Mario Callegaro Jon Cohen Elizabeth Dean Paul Harwood Josh Pasek Michael Stern,@techreport{42521 title = {Social media in public opinion research} author = {Michael Link and Joe Muphy and Michael F. Schober and Trent D. Buskirk and Jennifer Hunter Childs and Casey Langer Tesfaye and Mario Callegaro and Jon Cohen and Elizabeth Dean and Paul Harwood and Josh Pasek and Michael Stern} year = 2014 URL = {http://www.aapor.org/Content/aapor/AdvocacyandInitiatives/Reports/SocialMediaTaskForceReport/AAPOR_Social_Media_Report_FNL.pdf} note = {authored by the Emerging Technologies Task Force} institution = {AAPOR} },AAPOR announces the release of an important report Social Media in Public Opinion Research authored by the Emerging Technologies Task Force. As social media platforms – such as Facebook Twitter and LinkedIn to name a few – expand and proliferate so does access to users’ thoughts feelings and actions expressed instantaneously organically and often publicly across these platforms. At question is how researchers and others interested in public opinion derive reliable and valid insights from the data generated by social media users. The report Social Media in Public Opinion Research highlights the use of social media as a vehicle for facilitating the survey research process (i.e. questionnaire development recruitment locating etc.) and as a way of potentially supplementing or replacing traditional survey methods (i.e. content analysis of existing data). It offers an initial set of guidelines and considerations for researchers and consumers of social media-based studies noting the opportunities and challenges in this new area.,http://www.aapor.org/Content/aapor/AdvocacyandInitiatives/Reports/SocialMediaTaskForceReport/AAPOR_Social_Media_Report_FNL.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Social+media+in+public+opinion+research+Link+Muphy+Schober+Buskirk+Childs+Tesfaye+Callegaro+Cohen+Dean+Harwood+Pasek+Stern,http://research.google.com/pubs/pub42521.html
Compressing Deep Neural Networks using a Rank-Constrained Topology,Proceedings of Annual Conference of the International Speech Communication Association (Interspeech) ISCA (2015) pp. 1473-1477,2015,Preetum Nakkiran Raziel Alvarez Rohit Prabhavalkar Carolina Parada,@inproceedings{43813 title = {Compressing Deep Neural Networks using a Rank-Constrained Topology} author = {Preetum Nakkiran and Raziel Alvarez and Rohit Prabhavalkar and Carolina Parada} year = 2015 booktitle = {Proceedings of Annual Conference of the International Speech Communication Association (Interspeech)} pages = {1473--1477} },We present a general approach to reduce the size of feed-forward deep neural networks (DNNs). We propose a rank-constrained topology which factors the weights in the input layer of the DNN in terms of a low-rank representation: unlike previous work our technique is applied at the level of the filters learned at individual hidden layer nodes and exploits the natural two-dimensional time-frequency structure in the input. These techniques are applied on a small-footprint DNN-based keyword spotting task where we find that we can reduce model size by 75% relative to the baseline without any loss in performance. Furthermore we find that the proposed approach is more effective at improving model performance compared to other popular dimensionality reduction techniques when evaluated with a comparable number of parameters.,http://research.google.com/pubs/archive/43813.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Compressing+Deep+Neural+Networks+using+a+Rank-Constrained+Topology+Nakkiran+Alvarez+Prabhavalkar+Parada,http://research.google.com/pubs/pub43813.html
Cluster forest,Computational Statistics and Data Analysis vol. 66 (2013) pp. 178-192,2013,Donghui Yan Aiyou Chen Michael I Jordan,@article{41339 title = {Cluster forest} author = {Donghui Yan and Aiyou Chen and Michael I Jordan} year = 2013 journal = {Computational Statistics and Data Analysis} pages = {178-192} volume = {66} },"With inspiration from Random Forests (RF) in the context of classification a new clustering ensemble method---Cluster Forests (CF) is proposed. Geometrically CF randomly probes a high-dimensional data cloud to obtain ""good local clusterings"" and then aggregates via spectral clustering to obtain cluster assignments for the whole dataset. The search for good local clusterings is guided by a cluster quality measure kappa. CF progressively improves each local clustering in a fashion that resembles the tree growth in RF. Empirical studies on several real-world datasets under two different performance metrics show that CF compares favorably to its competitors. Theoretical analysis reveals that the kappa measure makes it possible to grow the local clustering in a desirable way---it is ""noise-resistant"". A closed-form expression is obtained for the mis-clustering rate of spectral clustering under a perturbation model which yields new insights into some aspects of spectral clustering.",http://research.google.com/pubs/archive/41339.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cluster+forest+Yan+Chen+Jordan,http://research.google.com/pubs/pub41339.html
Parallel Boosting with Momentum,ECML PKDD 2013 Part III LNAI 8190 Springer Heidelberg pp. 17-32 (to appear),2013,Indraneel Mukherjee Kevin Canini Rafael Frongillo Yoram Singer,@inproceedings{41341 title = {Parallel Boosting with Momentum} author = {Indraneel Mukherjee and Kevin Canini and Rafael Frongillo and Yoram Singer} year = 2013 URL = {http://canini.me/research_files/ecml_boom.pdf} booktitle = {ECML PKDD 2013 Part III LNAI 8190} pages = {17-32} address = {Heidelberg} },We describe a new simpliﬁed and general analysis of a fusion of Nesterov’s accelerated gradient with parallel coordinate descent. The resulting algorithm which we call BOOM for boosting with momentum enjoys the merits of both techniques. Namely BOOM retains the momentum and convergence properties of the accelerated gradient method while taking into account the curvature of the objective function. We describe a distributed implementation of BOOM which is suitable for massive high dimensional datasets. We show experimentally that BOOM is especially e_ective in large scale learning problems with rare yet informative features.,http://research.google.com/pubs/archive/41341.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Parallel+Boosting+with+Momentum+Mukherjee+Canini+Frongillo+Singer,http://research.google.com/pubs/pub41341.html
An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections,International Conference on Computer Vision (ICCV) (2015),2015,Yu Cheng Felix X. Yu Rogerio Feris Sanjiv Kumar Shih-Fu Chang,@inproceedings{43993 title = {An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections} author = {Yu Cheng and Felix X. Yu and Rogerio Feris and Sanjiv Kumar and Shih-Fu Chang} year = 2015 booktitle = {International Conference on Computer Vision (ICCV)} },We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes and d output nodes this method improves the time complexity from O(d^2) to O(dlogd) and space complexity from O(d^2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Exploration+of+Parameter+Redundancy+in+Deep+Networks+with+Circulant+Projections+Cheng+Yu+Feris+Kumar+Chang,http://research.google.com/pubs/pub43993.html
Vote calibration in community question-answering systems,SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (2012) pp. 781-790,2012,Bee-Chung Chen Anirban Dasgupta Xuanhui Wang Jie Yang,@inproceedings{40345 title = {Vote calibration in community question-answering systems} author = {Bee-Chung Chen and Anirban Dasgupta and Xuanhui Wang and Jie Yang} year = 2012 URL = {http://dl.acm.org/citation.cfm?id=2348388} booktitle = {SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval} pages = {781-790} },User votes are important signals in community question-answering (CQA) systems. Many features of typical CQA systems e.g. the best answer to a question status of a user are dependent on ratings or votes cast by the community. In a popular CQA site Yahoo! Answers users vote for the best answers to their questions and can also thumb up or down each individual answer. Prior work has shown that these votes provide useful predictors for content quality and user expertise where each vote is usually assumed to carry the same weight as others. In this paper we analyze a set of possible factors that indicate bias in user voting behavior -- these factors encompass different gaming behavior as well as other eccentricities e.g. votes to show appreciation of answerers. These observations suggest that votes need to be calibrated before being used to identify good answers or experts. To address this problem we propose a general machine learning framework to calibrate such votes. Through extensive experiments based on an editorially judged CQA dataset we show that our supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking.,http://research.google.com/pubs/archive/40345.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Vote+calibration+in+community+question-answering+systems+Chen+Dasgupta+Wang+Yang,http://research.google.com/pubs/pub40345.html
Algorithmic Thermodynamics,Mathematical Structures in Computer Science vol. 22 (2012) pp. 771-787,2012,John Baez Michael Stay,@article{39973 title = {Algorithmic Thermodynamics} author = {John Baez and Michael Stay} year = 2012 URL = {http://arxiv.org/abs/1010.2067} journal = {Mathematical Structures in Computer Science} pages = {771--787} volume = {22} },Algorithmic entropy can be seen as a special case of entropy as studied in statistical mechanics. This viewpoint allows us to apply many techniques developed for use in thermodynamics to the subject of algorithmic information theory. In particular suppose we fix a universal prefix-free Turing machine and let X be the set of programs that halt for this machine. Then we can regard X as a set of 'microstates' and treat any function on X as an 'observable'. For any collection of observables we can study the Gibbs ensemble that maximizes entropy subject to constraints on expected values of these observables. We illustrate this by taking the log runtime length and output of a program as observables analogous to the energy E volume V and number of molecules N in a container of gas. The conjugate variables of these observables allow us to define quantities which we call the 'algorithmic temperature' T 'algorithmic pressure' P and algorithmic potential' mu since they are analogous to the temperature pressure and chemical potential. We derive an analogue of the fundamental thermodynamic relation dE = T dS - P d V + mu dN and use it to study thermodynamic cycles analogous to those for heat engines. We also investigate the values of T P and mu for which the partition function converges. At some points on the boundary of this domain of convergence the partition function becomes uncomputable. Indeed at these points the partition function itself has nontrivial algorithmic entropy.,http://research.google.com/pubs/archive/39973.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Algorithmic+Thermodynamics+Baez+Stay,http://research.google.com/pubs/pub39973.html
Improving DNN Speaker Independence with I-vector Inputs,Proc. ICASSP IEEE (2014),2014,Andrew Senior Ignacio Lopez-Moreno,@inproceedings{42536 title = {Improving DNN Speaker Independence with I-vector Inputs} author = {Andrew Senior and Ignacio Lopez-Moreno} year = 2014 booktitle = {Proc. ICASSP} },We propose providing additional utterance-level features as inputs to a deep neural network (DNN) to facilitate speaker channel and background normalization. Modifications of the basic algorithm are developed which result in significant reductions in word error rates (WERs). The algorithms are shown to combine well with speaker adaptation by backpropagation resulting in a 9\% relative WER reduction. We address implementation of the algorithm for a streaming task.,http://research.google.com/pubs/archive/42536.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improving+DNN+Speaker+Independence+with+I-vector+Inputs+Senior+Lopez+Moreno,http://research.google.com/pubs/pub42536.html
Thialfi: A Client Notification Service for Internet-Scale Applications,Proc. 23rd ACM Symposium on Operating Systems Principles (SOSP) (2011) pp. 129-142,2011,Atul Adya Gregory Cooper Daniel Myers Michael Piatek,@inproceedings{37474 title = {Thialfi: A Client Notification Service for Internet-Scale Applications} author = {Atul Adya and Gregory Cooper and Daniel Myers and Michael Piatek} year = 2011 URL = {http://sigops.org/sosp/sosp11/proceedings/2011-Cascais/printable/10-adya.pdf} booktitle = {Proc. 23rd ACM Symposium on Operating Systems Principles (SOSP)} pages = {129--142} },Ensuring the freshness of client data is a fundamental problem for applications that rely on cloud infrastructure to store data and mediate sharing. Thialfi is a notification service developed at Google to simplify this task. Thialfi supports applications written in multiple programming languages and running on multiple platforms e.g. browsers phones and desktops. Applications register their interest in a set of shared objects and receive notifications when those objects change. Thialfi servers run in multiple Google data centers for availability and replicate their state asynchronously. Thialfi's approach to recovery emphasizes simplicity: all server state is soft and clients drive recovery and assist in replication. A principal goal of our design is to provide a straightforward API and good semantics despite a variety of failures including server crashes communication failures storage unavailability and data center failures. Evaluation of live deployments confirms that Thialfi is scalable efficient and robust. In production use Thialfi has scaled to millions of users and delivers notifications with an average delay of less than one second.,http://research.google.com/pubs/archive/37474.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Thialfi:+A+Client+Notification+Service+for+Internet-Scale+Applications+Adya+Cooper+Myers+Piatek,http://research.google.com/pubs/pub37474.html
Face Tracking and Recognition with Visual Constraints in Real-World Videos,IEEE Computer Vision and Pattern Recognition (CVPR) (2008),2008,Minyoung Kim Sanjiv Kumar Vladimir Pavlovic Henry A. Rowley,@inproceedings{34394 title = {Face Tracking and Recognition with Visual Constraints in Real-World Videos} author = {Minyoung Kim and Sanjiv Kumar and Vladimir Pavlovic and Henry A. Rowley} year = 2008 URL = {http://www.sanjivk.com/faceTracking.pdf} booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR)} },We address the problem of tracking and recognizing faces in real-world noisy videos. We track faces using a tracker that adaptively builds a target model reflecting changes in appearance typical of a video setting. However adaptive appearance trackers often suffer from drift a gradual adaptation of the tracker to non-targets. To alleviate this problem our tracker introduces visual constraints using a combination of generative and discriminative models in a particle filtering framework. The generative term conforms the particles to the space of generic face poses while the discriminative one ensures rejection of poorly aligned targets. This leads to a tracker that significantly improves robustness against abrupt appearance changes and occlusions critical for the subsequent recognition phase. Identity of the tracked subject is established by fusing pose-discriminant and person-discriminant features over the duration of a video sequence. This leads to a robust video-based face recognizer with state-of-the-art recognition performance. We test the quality of tracking and face recognition on realworld noisy videos from YouTube as well as the standard Honda/UCSD database. Our approach produces successful face tracking results on over 80% of all videos without video or person-specific parameter tuning. The good tracking performance induces similarly high recognition rates: 100% on Honda/UCSD and over 70% on the YouTube set containing 35 celebrities in 1500 sequences.,http://research.google.com/pubs/archive/34394.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Face+Tracking+and+Recognition+with+Visual+Constraints+in+Real-World+Videos+Kim+Kumar+Pavlovic+Rowley,http://research.google.com/pubs/pub34394.html
Web Derived Pronunciations for Spoken Term Detection,32nd Annual International ACM SIGIR Conference (2009) pp. 83-90,2009,Do_an Can Erica Cooper Arnab Ghoshal Martin Jansche Sanjeev Khudanpur Bhuvana Ramabhadran Michael Riley Murat Saraçlar Abhinav Sethy Morgan Ulinski Christopher White,@inproceedings{35097 title = {Web Derived Pronunciations for Spoken Term Detection} author = {Do_an Can and Erica Cooper and Arnab Ghoshal and Martin Jansche and Sanjeev Khudanpur and Bhuvana Ramabhadran and Michael Riley and Murat Saraçlar and Abhinav Sethy and Morgan Ulinski and Christopher White} year = 2009 note = {doi:10.1145/1571941.1571958} booktitle = {32nd Annual International ACM SIGIR Conference} pages = {83--90} },Indexing and retrieval of speech content in various forms such as broadcast news customer care data and on-line media has gained a lot of interest for a wide range of applications from customer analytics to on-line media search. For most retrieval applications the speech content is typically first converted to a lexical or phonetic representation using automatic speech recognition (ASR). The first step in searching through indexes built on these representations is the generation of pronunciations for named entities and foreign language query terms. This paper summarizes the results of the work conducted during the 2008 JHU Summer Workshop by the Multilingual Spoken Term Detection team on mining the web for pronunciations and analyzing their impact on spoken term detection. We will first present methods to use the vast amount of pronunciation information available on the Web in the form of IPA and ad-hoc transcriptions. We describe techniques for extracting candidate pronunciations from Web pages and associating them with orthographic words filtering out poorly extracted pronunciations normalizing IPA pronunciations to better conform to a common transcription standard and generating phonemic representations from ad-hoc transcriptions. We then present an analysis of the effectiveness of using these pronunciations to represent Out-Of-Vocabulary (OOV) query terms on the performance of a spoken term detection (STD) system. We will provide comparisons of Web pronunciations against automated techniques for pronunciation generation as well as pronunciations generated by human experts. Our results cover a range of speech indexes based on lattices confusion networks and one-best transcriptions at both word and word fragments levels.,http://research.google.com/pubs/archive/35097.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Derived+Pronunciations+for+Spoken+Term+Detection+Can+Cooper+Ghoshal+Jansche+Khudanpur+Ramabhadran+Riley+Sara%C3%A7lar+Sethy+Ulinski+White,http://research.google.com/pubs/pub35097.html
Drilling Network Stacks with packetdrill,USENIX ;login: vol. 38 (2013) pp. 48-52,2013,Neal Cardwell Barath Raghavan,@article{41848 title = {Drilling Network Stacks with packetdrill} author = {Neal Cardwell and Barath Raghavan} year = 2013 journal = {USENIX ;login:} pages = {48--52} volume = {38} },Testing and troubleshooting network protocols and stacks can be painstaking. To ease this process our team built packetdrill a tool that lets you write precise scripts to test entire network stacks from the system call layer down to the NIC hardware. packetdrill scripts use a familiar syntax and run in seconds making them easy to use during development debugging and regression testing and for learning and investigation.,http://research.google.com/pubs/archive/41848.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Drilling+Network+Stacks+with+packetdrill+Cardwell+Raghavan,http://research.google.com/pubs/pub41848.html
Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends,IEEE Signal Processing Magazine vol. 32 (2015) pp. 35-52,2015,Zhen-Hua Ling Shiyin Kang Heiga Zen Andrew Senior Mike Schuster Xiao-Jun Qian Helen Meng Li Deng,@article{43434 title = {Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends} author = {Zhen-Hua Ling and Shiyin Kang and Heiga Zen and Andrew Senior and Mike Schuster and Xiao-Jun Qian and Helen Meng and Li Deng} year = 2015 URL = {http://dx.doi.org/10.1109/MSP.2014.2359987} journal = {IEEE Signal Processing Magazine} pages = {35-52} volume = {32} },Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However these models have their limitations in representing complex nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR) deep learning techniques have also been applied successfully to speech generation as reported in recent literature.,http://dx.doi.org/10.1109/MSP.2014.2359987,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deep+Learning+for+Acoustic+Modeling+in+Parametric+Speech+Generation:+A+systematic+review+of+existing+techniques+and+future+trends+Ling+Kang+Zen+Senior+Schuster+Qian+Meng+Deng,http://research.google.com/pubs/pub43434.html
An Audio Indexing System for Election Video Material,Proceedings of ICASSP (2009) pp. 4873-4876,2009,Christopher Alberti Michiel Bacchiani Ari Bezman Ciprian Chelba Anastassia Drofa Hank Liao Pedro Moreno Ted Power Arnaud Sahuguet Maria Shugrina Olivier Siohan,@inproceedings{35613 title = {An Audio Indexing System for Election Video Material} author = {Christopher Alberti and Michiel Bacchiani and Ari Bezman and Ciprian Chelba and Anastassia Drofa and Hank Liao and Pedro Moreno and Ted Power and Arnaud Sahuguet and Maria Shugrina and Olivier Siohan} year = 2009 URL = {http://doi.ieeecomputersociety.org/10.1109/ICASSP.2009.4960723} booktitle = {Proceedings of ICASSP} pages = {4873-4876} },In the 2008 presidential election race in the United States the prospective candidates made extensive use of YouTube to post video material. We developed a scalable system that transcribes this material and makes the content searchable (by indexing the meta-data and transcripts of the videos) and allows the user to navigate through the video material based on content. The system is available as an iGoogle gadget as well as a Labs product. Given the large exposure special emphasis was put on the scalability and reliability of the system. This paper describes the design and implementation of this system.,http://research.google.com/pubs/archive/35613.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Audio+Indexing+System+for+Election+Video+Material+Alberti+Bacchiani+Bezman+Chelba+Drofa+Liao+Moreno+Power+Sahuguet+Shugrina+Siohan,http://research.google.com/pubs/pub35613.html
Energy Proportional Datacenter Networks,Proceedings of the International Symposium on Computer Architecture ACM (2010) pp. 338-347,2010,Dennis Abts Mike Marty Philip Wells Peter Klausler Hong Liu,@inproceedings{36462 title = {Energy Proportional Datacenter Networks} author = {Dennis Abts and Mike Marty and Philip Wells and Peter Klausler and Hong Liu} year = 2010 note = {ISCA'10 June 19--232010 Saint-Malo France} booktitle = {Proceedings of the International Symposium on Computer Architecture} pages = {338--347} },Numerous studies have shown that datacenter computers rarely operate at full utilization leading to a number of proposals for creating servers that are energy proportional with respect to the computation that they are performing. In this paper we show that as servers themselves become more energy proportional the datacenter network can become a significant fraction (up to 50%) of cluster power. In this paper we propose several ways to design a high-performance datacenter network whose power consumption is more proportional to the amount of traffic it is moving --- that is we propose energy proportional datacenter networks. We first show that a flattened butterfly topology itself is inherently more power efficient than the other commonly proposed topology for high-performance datacenter networks. We then exploit the characteristics of modern plesiochronous links to adjust their power and performance envelopes dynamically. Using a network simulator driven by both synthetic workloads and production datacenter traces we characterize and understand design tradeoffs and demonstrate an 85% reduction in power --- which approaches the ideal energy-proportionality of the network. Our results also demonstrate two challenges for the designers of future network switches: 1) We show that there is a significant power advantage to having independent control of each unidirectional channel comprising a network link since many traffic patterns show very asymmetric use and 2) system designers should work to optimize the high-speed channel designs to be more energy efficient by choosing optimal data rate and equalization technology. Given these assumptions we demonstrate that energy proportional datacenter communication is indeed possible.,http://research.google.com/pubs/archive/36462.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Energy+Proportional+Datacenter+Networks+Abts+Marty+Wells+Klausler+Liu,http://research.google.com/pubs/pub36462.html
Learning to Rank Recommendations with the k-Order Statistic Loss,ACM International Conference on Recommender Systems (RecSys) (2013),2013,Jason Weston Hector Yee Ron Weiss,@inproceedings{41534 title = {Learning to Rank Recommendations with the k-Order Statistic Loss} author = {Jason Weston and Hector Yee and Ron Weiss} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2507210_} booktitle = {ACM International Conference on Recommender Systems (RecSys)} },Making recommendations by learning to rank is becoming an increasingly studied area. Approaches that use stochastic gradient descent scale well to large collaborative ﬁltering datasets and it has been shown how to approximately optimize the mean rank or more recently the top of the ranked list. In this work we present a family of loss functions the korder statistic loss that includes these previous approaches as special cases and also derives new ones that we show to be useful. In particular we present (i) a new variant that more accurately optimizes precision at k and (ii) a novel procedure of optimizing the mean maximum rank which we hypothesize is useful to more accurately cover all of the user’s tastes. The general approach works by sampling N positive items ordering them by the score assigned by the model and then weighting the example as a function of this ordered set. Our approach is studied in two real-world systems Google Music and YouTube video recommendations where we obtain improvements for computable metrics and in the YouTube case increased user click through and watch duration when deployed live on www.youtube.com.,http://research.google.com/pubs/archive/41534.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Rank+Recommendations+with+the+k-Order+Statistic+Loss+Weston+Yee+Weiss,http://research.google.com/pubs/pub41534.html
SAC063 - SSAC Advisory on DNSSEC Key Rollover in the Root Zone,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2013),2013,Warren Kumari Russ Mundy Matt Larson Jaap Akkerhuis,@incollection{42949 title = {SAC063 - SSAC Advisory on DNSSEC Key Rollover in the Root Zone} author = {Warren Kumari and Russ Mundy and Matt Larson and Jaap Akkerhuis} year = 2013 URL = {https://www.icann.org/en/system/files/files/sac-063-en.pdf} booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} },There is consensus in the security and domain name system (DNS) communities that the root zone DNS Security Extensions (DNSSEC) system poses unique challenges for standard DNSSEC practices. While there is agreement that an eventual root zone Key-Signing Key (KSK) rollover is inevitable regardless of whether that rollover is caused by a key compromise or other factors there is no solid consensus in the technical community regarding the frequency of routine scheduled KSK rollovers. In this Advisory the SSAC addresses the following topics: * Terminology and definitions relating to DNSSEC key rollover in the root zone; * Key management in the root zone; * Motivations for root zone KSK rollover; * Risks associated with root zone KSK rollover; * Available mechanisms for root zone KSK rollover; * DNS response size considerations; * Quantifying the risk of failed trust anchor update; and * DNS response size considerations,http://research.google.com/pubs/archive/42949.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC063+-+SSAC+Advisory+on+DNSSEC+Key+Rollover+in+the+Root+Zone+Kumari+Mundy+Larson+Akkerhuis,http://research.google.com/pubs/pub42949.html
Building Transcribed Speech Corpora Quickly and Cheaply for Many Languages,Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010) International Speech Communication Association pp. 1914-1917,2010,Thad Hughes Kaisuke Nakajima Linne Ha Atul Vasu Pedro Moreno Mike LeBeau,@inproceedings{36801 title = {Building Transcribed Speech Corpora Quickly and Cheaply for Many Languages} author = {Thad Hughes and Kaisuke Nakajima and Linne Ha and Atul Vasu and Pedro Moreno and Mike LeBeau} year = 2010 booktitle = {Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)} pages = {1914-1917} },We present a system for quickly and cheaply building transcribed speech corpora containing utterances from many speakers in a variety of acoustic conditions. The system consists of a client application running on an Android mobile device with an intermittent Internet connection to a server. The client application collects demographic information about the speaker fetches textual prompts from the server for the speaker to read records the speaker’s voice and uploads the audio and associated metadata to the server. The system has so far been used to collect over 3000 hours of transcribed audio in 17 languages around the world.,http://research.google.com/pubs/archive/36801.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Building+Transcribed+Speech+Corpora+Quickly+and+Cheaply+for+Many+Languages+Hughes+Nakajima+Ha+Vasu+Moreno+LeBeau,http://research.google.com/pubs/pub36801.html
Google Street View: Capturing the World at Street Level,Computer vol. 43 (2010),2010,Dragomir Anguelov Carole Dulong Daniel Filip Christian Frueh Stéphane Lafon Richard Lyon Abhijit Ogale Luc Vincent Josh Weaver,@article{36899 title = {Google Street View: Capturing the World at Street Level} author = {Dragomir Anguelov and Carole Dulong and Daniel Filip and Christian Frueh and Stéphane Lafon and Richard Lyon and Abhijit Ogale and Luc Vincent and Josh Weaver} year = 2010 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5481932&tag=1} journal = {Computer} volume = {43} },Street View serves millions of Google users daily with panoramic imagery captured in hundreds of cities in 20 countries across four continents. A team of Google researchers describes the technical challenges involved in capturing processing and serving street-level imagery on a global scale.,http://research.google.com/pubs/archive/36899.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+Street+View:+Capturing+the+World+at+Street+Level+Anguelov+Dulong+Filip+Frueh+Lafon+Lyon+Ogale+Vincent+Weaver,http://research.google.com/pubs/pub36899.html
Video Description Length Guided Constant Quality Video Coding with Bitrate Constraint,Multimedia and Expo Workshops (ICMEW) 2012 IEEE International Conference on IEEE 2001 L Street NW. Suite 700 Washington DC 20036-4910 USA pp. 366-371,2012,Lei Yang Debargha Mukherjee Dapeng Wu,@inproceedings{40346 title = {Video Description Length Guided Constant Quality Video Coding with Bitrate Constraint} author = {Lei Yang and Debargha Mukherjee and Dapeng Wu} year = 2012 URL = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266411} booktitle = {Multimedia and Expo Workshops (ICMEW) 2012 IEEE International Conference on} pages = {366--371} address = {2001 L Street NW. Suite 700 Washington DC 20036-4910 USA} },In this paper we propose a new video encoding strategy - Video description length guided Constant Quality video coding with Bitrate Constraint (V-CQBC) for large scale video transcoding systems of video charing websites with varying unknown video contents. It provides smooth quality and saves bitrate and computation for transcoding millions of videos in both real time and batch mode. The new encoding strategy is based on the average bitrate-quality regression model and adapt to the encoded videos. Furthermore three types of video description length (VDL) describing the video overall spatial and temporal content complexity are proposed to guide video coding. Experimental results show that the proposed coding strategy with saved computation could achieve better or similar RD performance than other coding strategies.,http://research.google.com/pubs/archive/40346.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Video+Description+Length+Guided+Constant+Quality+Video+Coding+with+Bitrate+Constraint+Yang+Mukherjee+Wu,http://research.google.com/pubs/pub40346.html
The Data on Diversity: It's not just about being fair,Communications of the ACM vol. 57 number 11 (2014) pp. 86-95,2014,Beryl Nelson,@article{43135 title = {The Data on Diversity: It's not just about being fair} author = {Beryl Nelson} year = 2014 URL = {http://cacm.acm.org/magazines/2014/11/179827-the-data-on-diversity/fulltext} journal = {Communications of the ACM} pages = {86-95} volume = {57 number 11} },Teams and organizations whose members are heterogeneous in meaningful ways have a higher potential for innovation than teams whose members are homogeneous. However making such teams effective requires addressing a number of barriers. Social science experiments using quantitative methods show bias stereotype threat and methods to combat them. The effectiveness of diverse teams depends on trusting and supportive cultures. Data publication is one of the most important tools to identify and combat identity threat and biased decision making. Despite the challenges there is hope! There are tools that have been shown to combat bias and identity threat effectively.,http://research.google.com/pubs/archive/43135.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Data+on+Diversity:+It's+not+just+about+being+fair+Nelson,http://research.google.com/pubs/pub43135.html
Reconstructing the World's Museums,European Conference on Computer Vision (2012) (to appear),2012,Jianxiong Xiao Yasutaka Furukawa,@inproceedings{39963 title = {Reconstructing the World's Museums} author = {Jianxiong Xiao and Yasutaka Furukawa} year = 2012 booktitle = {European Conference on Computer Vision} },"Photorealistic maps are a useful navigational guide for large indoor environments such as museums and businesses. However it is impossible to acquire photographs covering a large indoor environment from aerial viewpoints. This paper presents a 3D reconstruction and visualization system to automatically produce clean and well-regularized texture-mapped 3D models for large indoor scenes from ground-level photographs and 3D laser points. The key component is a new algorithm called ""Inverse CSG"" for reconstructing a scene in a Constructive Solid Geometry (CSG) representation consisting of volumetric primitives which imposes powerful regularization constraints to exploit structural regularities. We also propose several techniques to adjust the 3D model to make it suitable for rendering the 3D maps from aerial viewpoints. The visualization system enables users to easily browse a large scale indoor environment from a bird's-eye view locate specific room interiors fly into a place of interest view immersive ground-level panorama views and zoom out again all with seamless 3D transitions. We demonstrate our system on various museums including the Metropolitan Museum of Art in New York City -- one of the largest art galleries in the world.",http://research.google.com/pubs/archive/39963.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reconstructing+the+World's+Museums+Xiao+Furukawa,http://research.google.com/pubs/pub39963.html
A Framework for Benchmarking Entity-Annotation Systems,Proceedings of the International World Wide Web Conference (WWW) (Practice & Experience Track) ACM (2013),2013,Marco Cornolti Paolo Ferragina Massimiliano Ciaramita,@inproceedings{40749 title = {A Framework for Benchmarking Entity-Annotation Systems} author = {Marco Cornolti and Paolo Ferragina and Massimiliano Ciaramita} year = 2013 booktitle = {Proceedings of the International World Wide Web Conference (WWW) (Practice & Experience Track)} },In this paper we design and implement a benchmarking framework for fair and exhaustive comparison of entity-annotation systems. The framework is based upon the definition of a set of problems related to the entity-annotation task a set of measures to evaluate systems performance and a systematic comparative evaluation involving all publicly available datasets containing texts of various types such as news tweets and Web pages. Our framework is easily-extensible with novel entity annotators datasets and evaluation measures for comparing systems and it has been released to the public as open source. We use this framework to perform the first extensive comparison among all available entity annotators over all available datasets and draw many interesting conclusions upon their efficiency and effectiveness. We also draw conclusions between academic versus commercial annotators.,http://research.google.com/pubs/archive/40749.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Framework+for+Benchmarking+Entity-Annotation+Systems+Cornolti+Ferragina+Ciaramita,http://research.google.com/pubs/pub40749.html
Ad Injection at Scale: Assessing Deceptive Advertisement Modifications,Proceedings of the IEEE Symposium on Security and Privacy (2015),2015,Kurt Thomas Elie Bursztein Chris Grier Grant Ho Nav Jagpal Alexandros Kapravelos Damon McCoy Antonio Nappa Vern Paxson Paul Pearce Niels Provos Moheeb Abu Rajab,@inproceedings{43346 title = {Ad Injection at Scale: Assessing Deceptive Advertisement Modifications} author = {Kurt Thomas and Elie Bursztein and Chris Grier and Grant Ho and Nav Jagpal and Alexandros Kapravelos and Damon McCoy and Antonio Nappa and Vern Paxson and Paul Pearce and Niels Provos and Moheeb Abu Rajab} year = 2015 booktitle = {Proceedings of the IEEE Symposium on Security and Privacy} },Today web injection manifests in many forms but fundamentally occurs when malicious and unwanted actors tamper directly with browser sessions for their own profit. In this work we illuminate the scope and negative impact of one of these forms ad injection in which users have ads imposed on them in addition to or different from those that websites originally sent them. We develop a multi-staged pipeline that identifies ad injection in the wild and captures its distribution and revenue chains. We find that ad injection has entrenched itself as a cross-browser monetization platform impacting more than 5% of unique daily IP addresses accessing Google—tens of millions of users around the globe. Injected ads arrive on a client’s machine through multiple vectors: our measurements identify 50870 Chrome extensions and 34407 Windows binaries 38% and 17% of which are explicitly malicious. A small number of software developers support the vast majority of these injectors who in turn syndicate from the larger ad ecosystem. We have contacted the Chrome Web Store and the advertisers targeted by ad injectors to alert each of the deceptive practices involved.,http://research.google.com/pubs/archive/43346.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Ad+Injection+at+Scale:+Assessing+Deceptive+Advertisement+Modifications+Thomas+Bursztein+Grier+Ho+Jagpal+Kapravelos+McCoy+Nappa+Paxson+Pearce+Provos+Abu+Rajab,http://research.google.com/pubs/pub43346.html
Amdahl's Law in the Multicore Era,IEEE Computer vol. 41 (2008) pp. 33-38,2008,Mark D. Hill Michael R. Marty,@article{34400 title = {Amdahl's Law in the Multicore Era} author = {Mark D. Hill and Michael R. Marty} year = 2008 journal = {IEEE Computer} pages = {33-38} volume = {41} },Augmenting Amdahl’s Law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.,http://research.google.com/pubs/archive/34400.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Amdahl's+Law+in+the+Multicore+Era+Hill+Marty,http://research.google.com/pubs/pub34400.html
Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers,IEEE Micro (2010) pp. 65-79,2010,Gang Ren Eric Tune Tipp Moseley Yixin Shi Silvius Rus Robert Hundt,@article{36575 title = {Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers} author = {Gang Ren and Eric Tune and Tipp Moseley and Yixin Shi and Silvius Rus and Robert Hundt} year = 2010 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/MM.2010.68} journal = {IEEE Micro} pages = {65-79} },Google-Wide Profiling (GWP) a continuous profiling infrastructure for data centers provides performance insights for cloud applications. With negligible overhead GWP provides stable accurate profiles and a datacenter-scale tool for traditional performance analyses. Furthermore GWP introduces novel applications of its profiles such as application- platform affinity measurements and identification of platform-specific microarchitectural peculiarities.,http://research.google.com/pubs/archive/36575.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google-Wide+Profiling:+A+Continuous+Profiling+Infrastructure+for+Data+Centers+Ren+Tune+Moseley+Shi+Rus+Hundt,http://research.google.com/pubs/pub36575.html
Advanced DSP for 400 Gb/s and Beyond Optical Networks,J. Lightwave Technology vol. 32 (2014) pp. 2716-2725,2014,Xiang Zou Lynn Nelson,@article{43831 title = {Advanced DSP for 400 Gb/s and Beyond Optical Networks} author = {Xiang Zou and Lynn Nelson} year = 2014 journal = {J. Lightwave Technology} pages = {2716-2725} volume = {32} },This paper presents a systematic review of several digital signal processing (DSP)-enabled technologies recently proposed and demonstrated for high spectral efficiency (SE) 400 Gb/s–class and beyond optical networks. These include 1) a newly proposed SE-adaptable optical modulation technology—time-domain hybrid quadrature amplitude modulation (QAM) 2) two advanced transmitter side digital spectral shaping technologies—Nyquist signaling (for spectrally-efficient multiplexing) and digital preequalization (for improving tolerance toward channel narrowing effects) and 3) a newly proposed training-assisted two-stage carrier phase recovery algorithm that is designed to address the detrimental cyclic phase slipping problem with minimal training overhead. Additionally this paper presents a novel DSP-based method for mitigation of equalizer-enhanced phase noise impairments. It is shown that performance degradation caused by the interaction between the long-memory chromatic dispersion compensating filter/equalizer and local oscillator laser phase noise can be effectively mitigated by replacing the commonly used fast single-tap phaserotation-based equalizer (for typical carrier phase recovery) with a fast multi-tap linear equalizer. Finally brief reviews of two high-SE 400 Gb/s-class WDM transmission experiments employing these advanced DSP algorithms are presented.,http://research.google.com/pubs/archive/43831.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Advanced+DSP+for+400+Gb/s+and+Beyond+Optical+Networks+Zhou+Nelson,http://research.google.com/pubs/pub43831.html
A Computational Model of the Cerebral Cortex,Proceedings of AAAI-05 MIT Press Cambridge Massachusetts (2005) pp. 938-943,2005,Thomas Dean,@inproceedings{34772 title = {A Computational Model of the Cerebral Cortex} author = {Thomas Dean} year = 2005 booktitle = {Proceedings of AAAI-05} pages = {938-943} address = {Cambridge Massachusetts} },Our current understanding of the primate cerebral cortex (neocortex) and in particular the posterior sensory association cortex has matured to a point where it is possible to develop a family of graphical models that capture the structure scale and power of the neocortex for purposes of associative recall sequence prediction and pattern completion among other functions. Implementing such models using readily available computing clusters is now within the grasp of many labs and would provide scientists with the opportunity to experiment with both hard-wired connection schemes and structure-learning algorithms inspired by animal learning and developmental studies. While neural circuits involving structures external to the neocortex such as the thalamic nuclei are less well understood the availability of a computational model on which to test hypotheses would likely accelerate our understanding of these circuits. Furthermore the existence of an agreed-upon cortical substrate would not only facilitate our understanding of the brain but enable researchers to combine lessons learned from biology with state-of-the-art graphical-model and machine-learning techniques to design hybrid systems that combine the best of biological and traditional computing approaches.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Computational+Model+of+the+Cerebral+Cortex+Dean,http://research.google.com/pubs/pub34772.html
Technology Companies are Best Positioned to Offer Health Record Trusts,HealthSec '10 Position Paper (2010),2010,Shirley Gaw Umesh Shankar,@inproceedings{36502 title = {Technology Companies are Best Positioned to Offer Health Record Trusts} author = {Shirley Gaw and Umesh Shankar} year = 2010 booktitle = {HealthSec '10 Position Paper} },The current health system lacks assurances to patients of data retention and privacy control. We argue that this is due to discrepancies in how health data is reported and consumed and contrast this with how financial credit data is reported and consumed. To address these health system gaps in protection of medical data we would like to evangelize the implementation of health record trusts. Finally we argue that Personal Health Records (PHRs) are the closest to offering the main features of health record trusts.,http://research.google.com/pubs/archive/36502.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Technology+Companies+are+Best+Positioned+to+Offer+Health+Record+Trusts+Gaw+Shankar,http://research.google.com/pubs/pub36502.html
Experiences Scaling Use of Google's Sawzall,DIMACS Workshop on Parallelism: A 2020 Vision http://dimacs.rutgers.edu/Workshops/Parallel/ (2011),2011,Jeffrey D. Oldham,@inproceedings{37061 title = {Experiences Scaling Use of Google's Sawzall} author = {Jeffrey D. Oldham} year = 2011 URL = {http://dimacs.rutgers.edu/Workshops/Parallel/slides/oldham.pdf} booktitle = {DIMACS Workshop on Parallelism: A 2020 Vision} address = {http://dimacs.rutgers.edu/Workshops/Parallel/} },Sawzall is a procedural language developed at Google for parallel analysis of very large data sets. Given a log sharded into many separate files its companion tool named saw runs Sawzall interpreters to perform an analysis. Hundreds of Googlers have written thousands of saw+Sawzall programs which form a significant minority of Google's daily data processing. Short programs grew to become longer programs which were not easily shared nor tested. In other words scaling naively written Sawzall led to unmaintainable programs. The simple idea of writing programs functionally not iteratively yielded shareable testable programs. The functions reflect fundamental map reduction concepts: mapping reducing and iterating. Each can be easily tested. This case study demonstrates that developers of parallel processing systems should also simultaneously develop ways for users to decompose code into sharable pieces that reflect fundamental underlying concepts. As importantly they must develop ways for users to easily write tests of their code.,http://research.google.com/pubs/archive/37061.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Experiences+Scaling+Use+of+Google's+Sawzall+Oldham,http://research.google.com/pubs/pub37061.html
Data enrichment for incremental reach estimation,Google Inc. (2014) pp. 1-21 (to appear),2014,Aiyou Chen Jim Koehler Art Owen Nicolas Remy Minghui Shi,@techreport{42246 title = {Data enrichment for incremental reach estimation} author = {Aiyou Chen and Jim Koehler and Art Owen and Nicolas Remy and Minghui Shi} year = 2014 institution = {Google Inc.} },There is increasing interest in measuring the overlap and/or incremental reach of cross-media campaigns. The direct method is to use a cross-media panel but these are expensive to scale across all media. Typically the cross-media panel is too small to produce reliable estimates when the interest comes down to subsets of the population. An alternative is to combine information from a small cross-media panel with a larger cheaper but potentially biased single media panel. In this article we develop a data enrichment approach specifically for incremental reach estimation. The approach not only integrates information from both panels that takes into account potential panel bias but borrows strength from modeling conditional dependence of cross-media reaches. We demonstrate the approach with data from six campaigns for estimating YouTube video ad incremental reach over TV. In a simulation directly modeled on the actual data we find that data enrichment yields much greater accuracy than one would get by either ignoring the larger panel or by using it in a data fusion.,http://research.google.com/pubs/archive/42246.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Data+enrichment+for+incremental+reach+estimation+Chen+Koehler+Owen+Remy+Shi,http://research.google.com/pubs/pub42246.html
Point Representation for Local Optimization: Towards Multi-Dimensional Gray Codes,Proceedings IEEE Congress on Evolutionary Computation IEEE (2013),2013,Shumeet Baluja Michele Covell,@inproceedings{41332 title = {Point Representation for Local Optimization: Towards Multi-Dimensional Gray Codes} author = {Shumeet Baluja and Michele Covell} year = 2013 URL = {http://www.esprockets.com/papers/2013-cec-point-representation.pdf} booktitle = {Proceedings IEEE Congress on Evolutionary Computation} },In the context of stochastic search once regions of high performance are found having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this Gray Codes are often employed for encoding ordinal points or discretized real numbers. In this paper we present a method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be viewed as an approximate high-dimensional variant of Gray Codes. The labeling procedure is useful for any task in which the solution requires the search algorithm to select a small subset of items out of many. A large number of empirical results using these encodings with a combination of genetic algorithms and hill-climbing are presented.,http://www.esprockets.com/papers/2013-cec-point-representation.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Point+Representation+for+Local+Optimization:+Towards+Multi-Dimensional+Gray+Codes+Baluja+Covell,http://research.google.com/pubs/pub41332.html
Sequence Kernels for Predicting Protein Essentiality,Proceedings of ICML 2008,2008,Cyril Allauzen Mehryar Mohri Ameet Talwalkar,@inproceedings{34406 title = {Sequence Kernels for Predicting Protein Essentiality} author = {Cyril Allauzen and Mehryar Mohri and Ameet Talwalkar} year = 2008 booktitle = {Proceedings of ICML 2008} },The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database which includes several Pfam domains. We then present novel and general {\em domain-based} sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem -- several thousands of domains with individual domains sometimes containing thousand of sequences -- by representing and efficiently computing these kernels using automata. We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality while requiring no manual tuning.,http://research.google.com/pubs/archive/34406.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sequence+Kernels+for+Predicting+Protein+Essentiality+Allauzen+Mohri+Talwalkar,http://research.google.com/pubs/pub34406.html
Activity Motifs Reveal Principles of Timing in Transcriptional Control of the Yeast Metabolic Network,Nature Biotechnology vol. 26 (11) (2008) pp. 1251-1259,2008,Gal Chechik Eugene Oh Oliver Rando Jonathan Weissman Aviv Regev Daphne Koller,@article{34617 title = {Activity Motifs Reveal Principles of Timing in Transcriptional Control of the Yeast Metabolic Network} author = {Gal Chechik and Eugene Oh and Oliver Rando and Jonathan Weissman and Aviv Regev and Daphne Koller} year = 2008 URL = {http://ai.stanford.edu/~gal/Research/ActivityMotifs/} journal = {Nature Biotechnology} pages = {1251-1259} volume = {26 (11)} },Significant insight about biological networks arises from the study of network motifs—overly abundant network subgraphs but such wiring patterns do not specify when and how potential routes within a cellular network are used. To address this limitation we introduce activity motifs which capture patterns in the dynamic use of a network. Using this framework to analyze transcription in Saccharomyces cerevisiae metabolism we find that cells use different timing activity motifs to optimize transcription timing in response to changing conditions: forward activation to produce metabolic compounds efficiently backward shutoff to rapidly stop production of a detrimental product and synchronized activation for co-production of metabolites required for the same reaction. Measuring protein abundance over a time course reveals that mRNA timing motifs also occur at the protein level. Timing motifs significantly overlap with binding activity motifs where genes in a linear chain have ordered binding affinity to a transcription factor suggesting a mechanism for ordered transcription. Finely timed transcriptional regulation is therefore abundant in yeast metabolism optimizing the organism's adaptation to new environmental conditions.,http://ai.stanford.edu/~gal/Research/ActivityMotifs/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Activity+Motifs+Reveal+Principles+of+Timing+in+Transcriptional+Control+of+the+Yeast+Metabolic+Network+Chechik+Oh+Rando+Weissman+Regev+Koller,http://research.google.com/pubs/pub34617.html
The Network Value of Products,Journal of Marketing vol. 77(3) (2013) pp. 1-14,2013,Gal Oestreicher-Singer Barak Libai Liron Sivan Eyal Carmi Ohad Yassin,@article{42035 title = {The Network Value of Products} author = {Gal Oestreicher-Singer and Barak Libai and Liron Sivan and Eyal Carmi and Ohad Yassin} year = 2013 URL = {http://journals.ama.org/doi/abs/10.1509/jm.11.0400} journal = {Journal of Marketing} pages = {1--14} volume = {77(3)} },Traditionally the value of a product has been assessed according to the direct revenues the product creates. However products do not exist in isolation but rather influence one another’s sales. Such influence is especially evident in e-commerce environments in which products are often presented as a collection of web pages linked by recommendation hyperlinks creating a large-scale product network. The authors present a systematic approach to estimate products’ true value to a firm in such a product network. Their approach which is in the spirit of the PageRank algorithm uses available data from large-scale e-commerce sites and separates a product’s value into its own intrinsic value the value it receives from the network and the value it contributes to the network. The authors demonstrate their approach using data collected from the product network of books on Amazon.com. Specifically they show that the value of low sellers may be underestimated whereas the value of best sellers may be overestimated. The authors explore the sources of this discrepancy and discuss the implications for managing products in the growing environment of product networks.,http://journals.ama.org/doi/abs/10.1509/jm.11.0400,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Network+Value+of+Products+Oestreicher-Singer+Libai+Sivan+Carmi+Yassin,http://research.google.com/pubs/pub42035.html
Spectral Intersections for Non-Stationary Signal Separation,Proceedings of InterSpeech 2012 Portland OR,2012,Trausti Kristjansson Thad Hughes,@inproceedings{39988 title = {Spectral Intersections for Non-Stationary Signal Separation} author = {Trausti Kristjansson and Thad Hughes} year = 2012 booktitle = {Proceedings of InterSpeech 2012} address = {Portland OR} },We describe a new method for non-stationary noise suppression that is simple to implement yet has performance rivaling far more complex algorithms. Spectral Intersections is a model based MMSE signal separation method that uses a new simple approximation to the observation likelihood. Furthermore Spectral Intersections uses an efficient approximation to the expectation integral of the MMSE estimate that could be described as unscented importance sampling. We apply the new method to the task of separating speech mixed with music. We report results on the Google Voice Search task where the new method provides a 7% relative reduction in WER at 10dB SNR. Interestingly the new method provides considerably greater reduction in average WER than the MAX method and approaches the performance of the more complex Algonquin algorithm.,http://research.google.com/pubs/archive/39988.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Spectral+Intersections+for+Non-Stationary+Signal+Separation+Kristjansson+Hughes,http://research.google.com/pubs/pub39988.html
Discrete Point Based Signatures and Applications to Document Matching,ICIAP 2011,2011,Nemanja Spasojevic Guillaume Poncin Dan Bloomberg,@inproceedings{37297 title = {Discrete Point Based Signatures and Applications to Document Matching} author = {Nemanja Spasojevic and Guillaume Poncin and Dan Bloomberg} year = 2011 booktitle = {ICIAP 2011} },Document analysis often starts with robust signatures for instance for document lookup from low-quality photographs or similarity analysis between scanned books. Signatures based on OCR typically work well but require good quality OCR which is not always available and can be very costly. In this paper we describe a novel scheme for extracting discrete signatures from document images. It operates on points that describe the position of words typically the centroid. Each point is extracted using one of several techniques and assigned a signature based on its relation to the nearest neighbors. We will discuss the benefits of this approach and demonstrate its application to multiple problems including fast image similarity calculation and document lookup.,http://research.google.com/pubs/archive/37297.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discrete+Point+Based+Signatures+and+Applications+to+Document+Matching+Spasojevic+Poncin+Bloomberg,http://research.google.com/pubs/pub37297.html
Statistical Parametric Speech Synthesis Using Deep Neural Networks,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2013) pp. 7962-7966,2013,Heiga Zen Andrew Senior Mike Schuster,@inproceedings{40837 title = {Statistical Parametric Speech Synthesis Using Deep Neural Networks} author = {Heiga Zen and Andrew Senior and Mike Schuster} year = 2013 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {7962--7966} },Conventional approaches to statistical parametric speech synthesis typically use decision tree-clustered context-dependent hidden Markov models (HMMs) to represent probability densities of speech parameters given texts. Speech parameters are generated from the probability densities to maximize their output probabilities then a speech waveform is reconstructed from the generated parameters. This approach is reasonably effective but has a couple of limitations e.g. decision trees are inef?cient to model complex context dependencies. This paper examines an alternative scheme that is based on a deep neural network (DNN). The relationship between input texts and their acoustic realizations is modeled by a DNN. The use of the DNN can address some limitations of the conventional approach. Experimental results show that the DNN-based systems outperformed the HMM-based systems with similar numbers of parameters.,http://research.google.com/pubs/archive/40837.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+Parametric+Speech+Synthesis+Using+Deep+Neural+Networks+Zen+Senior+Schuster,http://research.google.com/pubs/pub40837.html
Unsupervised Discovery and Training of Maximally Dissimilar Cluster Models,Proc Interspeech (2010),2010,Francoise Beaufays Vincent Vanhoucke Brian Strope,@inproceedings{36487 title = {Unsupervised Discovery and Training of Maximally Dissimilar Cluster Models} author = {Francoise Beaufays and Vincent Vanhoucke and Brian Strope} year = 2010 booktitle = {Proc Interspeech} },One of the difficult problems of acoustic modeling for Automatic Speech Recognition (ASR) is how to adequately model the wide variety of acoustic conditions which may be present in the data. The problem is especially acute for tasks such as Google Search by Voice where the amount of speech available per transaction is small and adaptation techniques start showing their limitations. As training data from a very large user population is available however it is possible to identify and jointly model subsets of the data with similar acoustic qualities. We describe a technique which allows us to perform this modeling at scale on large amounts of data by learning a treestructured partition of the acoustic space and we demonstrate that we can significantly improve recognition accuracy in various conditions through unsupervised Maximum Mutual Information (MMI) training. Being fully unsupervised this technique scales easily to increasing numbers of conditions.,http://research.google.com/pubs/archive/36487.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Discovery+and+Training+of+Maximally+Dissimilar+Cluster+Models+Beaufays+Vanhoucke+Strope,http://research.google.com/pubs/pub36487.html
Large Scale Content-Based Audio Retrieval from Text Queries,ACM International Conference on Multimedia Information Retrieval (MIR) ACM (2008),2008,Gal Chechik Eugene Ie Martin Rehn Samy Bengio Richard F. Lyon,@inproceedings{33429 title = {Large Scale Content-Based Audio Retrieval from Text Queries} author = {Gal Chechik and Eugene Ie and Martin Rehn and Samy Bengio and Richard F. Lyon} year = 2008 booktitle = {ACM International Conference on Multimedia Information Retrieval (MIR)} },In content-based audio retrieval the goal is to find sound recordings (audio documents) based on their acoustic features. This content-based approach differs from retrieval approaches that index media files using metadata such as file names and user tags. In this paper we propose a machine learning approach for retrieving sounds that is novel in that it (1) uses free-form text queries rather sound sample based queries (2) searches by audio content rather than via textual meta data and (3) can scale to very large number of audio documents and very rich query vocabulary. We handle generic sounds including a wide variety of sound effects animal vocalizations and natural scenes. We test a scalable approach based on a passive-aggressive model for image retrieval (PAMIR) and compare it to two state-of-the-art approaches; Gaussian mixture models (GMM) and support vector machines (SVM). We test our approach on two large real-world datasets: a collection of short sound effects and a noisier and larger collection of user-contributed user-labeled recordings (25K files 2000 terms vocabulary). We find that all three methods achieved very good retrieval performance. For instance a positive document is retrieved in the first position of the ranking more than half the time and on average there are more than 4 positive documents in the first 10 retrieved for both datasets. PAMIR completed both training and retrieval of all data in less than 6 hours for both datasets on a single machine. It was one to three orders of magnitude faster than the competing approaches. This approach should therefore scale to much larger datasets in the future.,http://research.google.com/pubs/archive/33429.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Content-Based+Audio+Retrieval+from+Text+Queries+Chechik+Ie+Rehn+Bengio+Lyon,http://research.google.com/pubs/pub33429.html
Estimation Optimization and Parallelism when Data is Sparse,Advances in Neural Information Processing Systems (NIPS) (2013),2013,John C. Duchi Michael I. Jordan H. Brendan McMahan,@inproceedings{41858 title = {Estimation Optimization and Parallelism when Data is Sparse} author = {John C. Duchi and Michael I. Jordan and H. Brendan McMahan} year = 2013 booktitle = {Advances in Neural Information Processing Systems (NIPS)} },We study stochastic optimization problems when the \emph{data} is sparse which is in a sense dual to current perspectives on high-dimensional statistical learning and optimization. We highlight both the difficulties---in terms of increased sample complexity that sparse data necessitates---and the potential benefits in terms of allowing parallelism and asynchrony in the design of algorithms. Concretely we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data and we exhibit algorithms achieving these rates. We also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms providing experimental evidence complementing our theoretical results on several medium to large-scale learning tasks.,http://research.google.com/pubs/archive/41858.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Estimation+Optimization+and+Parallelism+when+Data+is+Sparse+Duchi+Jordan+McMahan,http://research.google.com/pubs/pub41858.html
SAC064 - ICANN SSAC Advisory on Search List Processing,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2014) pp. 17,2014,Warren Kumari Jaap Akkerhuis Don Blumenthal,@incollection{42190 title = {SAC064 - ICANN SSAC Advisory on Search List Processing} author = {Warren Kumari and Jaap Akkerhuis and Don Blumenthal} year = 2014 URL = {https://www.icann.org/en/groups/ssac/documents/sac-064-en.pdf} booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} pages = {17} },This advisory examines how current operating systems and applications process search lists. It outlines the issues related to the current search list behavior and proposes both a strawman to improve search list processing in the long term and mitigation options for the Internet Corporation for Assigned Names and Numbers (ICANN) and the Internet community to consider in the short term. The purpose of these proposals is to help introduce new generic Top Level Domains (gTLDs) in a secure and stable manner with minimum disruptions to currently deployed systems.,http://research.google.com/pubs/archive/42190.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC064+-+ICANN+SSAC+Advisory+on+Search+List+Processing+Kumari+Akkerhuis+Blumenthal,http://research.google.com/pubs/pub42190.html
Combined Orientation and Script Detection using the Tesseract OCR Engine,Workshop on Multilingual OCR (MOCR) Proc. 10th Intl. Conf. on Document Analysis and Recognition (ICDAR) (2009),2009,Ranjith Unnikrishnan Ray Smith,@inproceedings{35506 title = {Combined Orientation and Script Detection using the Tesseract OCR Engine} author = {Ranjith Unnikrishnan and Ray Smith} year = 2009 booktitle = {Workshop on Multilingual OCR (MOCR) Proc. 10th Intl. Conf. on Document Analysis and Recognition (ICDAR)} },This paper proposes a simple but effective algorithm to estimate the script and dominant page orientation of the text contained in an image. A candidate set of shape classes for each script is generated using synthetically rendered text and used to train a fast shape classifier. At run time the classifier is applied independently to connected components in the image for each possible orientation of the component and the accumulated confidence scores are used to determine the best estimate of page orientation and script. Results demonstrate the effectiveness of the approach on a dataset of 1846 documents containing a diverse set of images in 14 scripts and any of four possible page orientations.,http://research.google.com/pubs/archive/35506.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Combined+Orientation+and+Script+Detection+using+the+Tesseract+OCR+Engine+Unnikrishnan+Smith,http://research.google.com/pubs/pub35506.html
Large-Scale Parallel Statistical Forecasting Computations in R,JSM Proceedings Section on Physical and Engineering Sciences American Statistical Association Alexandria VA (2011),2011,Murray Stokely Farzan Rohani Eric Tassone,@inproceedings{37483 title = {Large-Scale Parallel Statistical Forecasting Computations in R} author = {Murray Stokely and Farzan Rohani and Eric Tassone} year = 2011 booktitle = {JSM Proceedings Section on Physical and Engineering Sciences} address = {Alexandria VA} },We demonstrate the utility of massively parallel computational infrastructure for statistical computing using the MapReduce paradigm for R. This framework allows users to write computations in a high-level language that are then broken up and distributed to worker tasks in Google datacenters. Results are collected in a scalable distributed data store and returned to the interactive user session. We apply our approach to a forecasting application that fits a variety of models prohibiting an analytical description of the statistical uncertainty associated with the overall forecast. To overcome this we generate simulation-based uncertainty bands which necessitates a large number of computationally intensive realizations. Our technique cut total run time by a factor of 300. Distributing the computation across many machines permits analysts to focus on statistical issues while answering questions that would be intractable without significant parallel computational infrastructure. We present real-world performance characteristics from our application to allow practitioners to better understand the nature of massively parallel statistical simulations in R.,http://research.google.com/pubs/archive/37483.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Parallel+Statistical+Forecasting+Computations+in+R+Stokely+Rohani+Tassone,http://research.google.com/pubs/pub37483.html
Moving Targets: Security and Rapid-Release in Firefox,Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security ACM New York NY pp. 1256-1266,2014,Sandy Clark Michael Collis Matt Blaze Jonathan M. Smith,@inproceedings{43239 title = {Moving Targets: Security and Rapid-Release in Firefox} author = {Sandy Clark and Michael Collis and Matt Blaze and Jonathan M. Smith} year = 2014 URL = {http://dx.doi.org/10.1145/2660267.2660320} booktitle = {Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security} pages = {1256-1266} address = {New York NY} },"Software engineering practices strongly affect the security of the code produced. The increasingly popular Rapid Release Cycle (RRC) development methodology and easy network software distribution have enabled rapid feature introduction. RRC's defining characteristic of frequent software revisions would seem to conflict with traditional software engineering wisdom regarding code maturity reliability and reuse as well as security. Our investigation of the consequences of rapid release comprises a quantitative data-driven study of the impact of rapid-release methodology on the security of the Mozilla Firefox browser. We correlate reported vulnerabilities in multiple rapid release versions of Firefox code against those in corresponding extended release versions of the same system; using a common software base with different release cycles eliminates many causes other than RRC for the observables. Surprisingly the resulting data show that Firefox RRC does not result in higher vulnerability rates and further that it is exactly the unfamiliar newly released software (the ""moving targets"") that requires time to exploit. These provocative results suggest that a rethinking of the consequences of software engineering practices for security may be warranted.",http://research.google.com/pubs/archive/43239.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Moving+Targets:+Security+and+Rapid-Release+in+Firefox+Clark+Collis+Blaze+Smith,http://research.google.com/pubs/pub43239.html
Maglev: A Fast and Reliable Software Network Load Balancer,13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16) USENIX Association Santa Clara CA (2016) (to appear),2016,Daniel E. Eisenbud Cheng Yi Carlo Contavalli Cody Smith Roman Kononov Eric Mann-Hielscher Ardas Cilingiroglu Bin Cheyney Wentao Shang Jinnah Dylan Hosein,@inproceedings{44824 title = {Maglev: A Fast and Reliable Software Network Load Balancer} author = {Daniel E. Eisenbud and Cheng Yi and Carlo Contavalli and Cody Smith and Roman Kononov and Eric Mann-Hielscher and Ardas Cilingiroglu and Bin Cheyney and Wentao Shang and Jinnah Dylan Hosein} year = 2016 URL = {https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/eisenbud} booktitle = {13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16)} address = {Santa Clara CA} },Maglev is Google’s network load balancer. It is a large distributed software system that runs on commodity Linux servers. Unlike traditional hardware network load balancers it does not require a specialized physical rack deployment and its capacity can be easily adjusted by adding or removing servers. Network routers distribute packets evenly to the Maglev machines via Equal Cost Multipath (ECMP); each Maglev machine then matches the packets to their corresponding services and spreads them evenly to the service endpoints. To accommodate high and ever-increasing traffic Maglev is specifically optimized for packet processing performance. A single Maglev machine is able to saturate a 10Gbps link with small packets. Maglev is also equipped with consistent hashing and connection tracking features to minimize the negative impact of unexpected faults and failures on connection-oriented protocols. Maglev has been serving Google's traffic since 2008. It has sustained the rapid global growth of Google services and it also provides network load balancing for Google Cloud Platform.,http://research.google.com/pubs/archive/44824.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Maglev:+A+Fast+and+Reliable+Software+Network+Load+Balancer+Eisenbud+Yi+Contavalli+Smith+Kononov+Hielscher+Cilingiroglu+Cheyney+Shang+Hosein,http://research.google.com/pubs/pub44824.html
Globally Minimal Surfaces by Continuous Maximal Flows,IEEE Trans. Pattern Anal. Mach. Intell. vol. 28 (2006) pp. 106-118,2006,Ben Appleton Hugues Talbot,@article{32799 title = {Globally Minimal Surfaces by Continuous Maximal Flows} author = {Ben Appleton and Hugues Talbot} year = 2006 URL = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2006.12} journal = {IEEE Trans. Pattern Anal. Mach. Intell.} pages = {106-118} volume = {28} },In this paper we address the computation of globally minimal curves and surfaces for image segmentation and stereo reconstruction. We present a solution simulating a continuous maximal flow by a novel system of partial differential equations. Existing methods are either grid-biased (graph-based methods) or sub-optimal (active contours and surfaces). The solution simulates the flow of an ideal fluid with isotropic velocity constraints. Velocity constraints are defined by a metric derived from image data. An auxiliary potential function is introduced to create a system of partial differential equations. It is proven that the algorithm produces a globally maximal continuous flow at convergence and that the globally minimal surface may be obtained trivially from the auxiliary potential. The bias of minimal surface methods toward small objects is also addressed. An efficient implementation is given for the flow simulation. The globally minimal surface algorithm is applied to segmentation in 2D and 3D as well as to stereo matching. Results in 2D agree with an existing minimal contour algorithm for planar images. Results in 3D segmentation and stereo matching demonstrate that the new algorithm is robust and free from grid bias.,http://research.google.com/pubs/archive/32799.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Globally+Minimal+Surfaces+by+Continuous+Maximal+Flows+Appleton+Talbot,http://research.google.com/pubs/pub32799.html
Bimanual gesture keyboard,Proceeding of UIST 2012 – The ACM Symposium on User Interface Software and Technology ACM New York NY USA pp. 137-146,2012,Xiaojun Bi Ciprian Chelba Tom Ouyang Kurt Partridge Shumin Zhai,@inproceedings{41647 title = {Bimanual gesture keyboard} author = {Xiaojun Bi and Ciprian Chelba and Tom Ouyang and Kurt Partridge and Shumin Zhai} year = 2012 URL = {http://dl.acm.org/citation.cfm?id=2380116.2380136&coll=DL&dl=ACM&CFID=244634215&CFTOKEN=16005854} booktitle = {Proceeding of UIST 2012 – The ACM Symposium on User Interface Software and Technology} pages = {137--146} address = {New York NY USA} },Gesture keyboards represent an increasingly popular way to input text on mobile devices today. However current gesture keyboards are exclusively unimanual. To take advantage of the capability of modern multi-touch screens we created a novel bimanual gesture text entry system extending the gesture keyboard paradigm from one finger to multiple fingers. To address the complexity of recognizing bimanual gesture we designed and implemented two related interaction methods finger-release and space-required both based on a new multi-stroke gesture recognition algorithm. A formal experiment showed that bimanual gesture behaviors were easy to learn. They improved comfort and reduced the physical demand relative to unimanual gestures on tablets. The results indicated that these new gesture keyboards were valuable complements to unimanual gesture and regular typing keyboards.,http://dl.acm.org/citation.cfm?id=2380116.2380136&coll=DL&dl=ACM&CFID=244634215&CFTOKEN=16005854,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bimanual+gesture+keyboard+Bi+Chelba+Ouyang+Partridge+Zhai,http://research.google.com/pubs/pub41647.html
Channeling the data deluge,Nature Methods vol. 8 (2011) pp. 463,2011,Jason Swedlow Gianluigi Zanetti Christoph Best,@article{37117 title = {Channeling the data deluge} author = {Jason Swedlow and Gianluigi Zanetti and Christoph Best} year = 2011 URL = {http://www.nature.com/nmeth/journal/v8/n6/full/nmeth.1616.html} journal = {Nature Methods} pages = {463} volume = {8} },With vast increases in biological data generation mechanisms for data storage and analysis have become limiting. A data structure semantically typed data hypercubes (SDCubes) that combines hierarchical data format version 5 (HDF5) and extensible markup language (XML) file formats now permits the flexible storage annotation and retrieval of large and heterogenous datasets.,http://www.nature.com/nmeth/journal/v8/n6/full/nmeth.1616.html,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Channeling+the+data+deluge+Swedlow+Zanetti+Best,http://research.google.com/pubs/pub37117.html
Speech Acoustic Modeling from Raw Multichannel Waveforms,International Conference on Acoustics Speech and Signal Processing IEEE (2015),2015,Yedid Hoshen Ron Weiss Kevin W Wilson,@inproceedings{43290 title = {Speech Acoustic Modeling from Raw Multichannel Waveforms} author = {Yedid Hoshen and Ron Weiss and Kevin W Wilson} year = 2015 booktitle = {International Conference on Acoustics Speech and Signal Processing} },Standard deep neural network-based acoustic models for automatic speech recognition (ASR) rely on hand-engineered input features typically log-mel filterbank magnitudes. In this paper we describe a convolutional neural network - deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input i.e. without any preceding feature extraction and learns a similar feature representation through supervised training. By operating directly in the time domain the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival i.e. a bank of bandpass beamformers with an auditory-like frequency scale. When trained on data corrupted with noise coming from different spatial locations the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions.,http://research.google.com/pubs/archive/43290.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Speech+Acoustic+Modeling+from+Raw+Multichannel+Waveforms+Hoshen+Weiss+Wilson,http://research.google.com/pubs/pub43290.html
On The Compression Of Recurrent Neural Networks With An Application To LVCSR Acoustic Modeling For Embedded Speech Recognition,Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2016) (to appear),2016,Rohit Prabhavalkar Ouais Alsharif Antoine Bruguier Ian McGraw,@inproceedings{44632 title = {On The Compression Of Recurrent Neural Networks With An Application To LVCSR Acoustic Modeling For Embedded Speech Recognition} author = {Rohit Prabhavalkar and Ouais Alsharif and Antoine Bruguier and Ian McGraw} year = 2016 booktitle = {Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)} },We study the problem of compressing recurrent neural networks (RNNs). In particular we focus on the compression of RNN acoustic models which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+The+Compression+Of+Recurrent+Neural+Networks+With+An+Application+To+LVCSR+Acoustic+Modeling+For+Embedded+Speech+Recognition+Prabhavalkar+Alsharif+Bruguier+McGraw,http://research.google.com/pubs/pub44632.html
Bubble-Up: Increasing Utilization In Modern Warehouse Scale Computers Via Sensible Co-Locations,Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture 2011 IEEE New York NY USA,2011,Jason Mars Linjia Tang Robert Hundt Kevin Skadron Mary Lou Souffa,@inproceedings{37675 title = {Bubble-Up: Increasing Utilization In Modern Warehouse Scale Computers Via Sensible Co-Locations} author = {Jason Mars and Linjia Tang and Robert Hundt and Kevin Skadron and Mary Lou Souffa} year = 2011 URL = {http://www.cs.virginia.edu/~skadron/Papers/mars_micro2011.pdf} booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture 2011} address = {New York NY USA} },As much of the world’s computing continues to move into the cloud the over-provisioning of computing resources to ensure the performance isolation of latency-sensitive tasks such as web search in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper we present Bubble-Up a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of “pressure” to the memory subsystem on processors in production datacenters our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at “sensible” co-locations in Google’s production datacenters with real-world large-scale applications we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications.,http://research.google.com/pubs/archive/37675.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bubble-Up:+Increasing+Utilization+In+Modern+Warehouse+Scale+Computers+Via+Sensible+Co-Locations+Mars+Tang+Hundt+Skadron+Souffa,http://research.google.com/pubs/pub37675.html
Compacting Large and Loose Communities,Asian Conference on Pattern Recognition (2013) (to appear),2013,Chandrashekhar V. Shailesh Kumar C. V. Jawahar,@inproceedings{41436 title = {Compacting Large and Loose Communities} author = {Chandrashekhar V. and Shailesh Kumar and C. V. Jawahar} year = 2013 booktitle = {Asian Conference on Pattern Recognition} },Detecting compact overlapping communities in large networks is an important pattern recognition problem with applications in many domains. Most community detection algorithms trade-off between community sizes their compactness and the scalability of finding communities. Clique Percolation Method (CPM) and Local Fitness Maximization (LFM) are two prominent and commonly used overlapping community detection methods that scale with large networks. However significant number of communities found by them are large noisy and loose. In this paper we propose a general algorithm that takes such large and loose communities generated by any method and refines them into compact communities in a systematic fashion. We define a new measure of community-ness based on eigenvector centrality identify loose communities using this measure and propose an algorithm for partitioning such loose communities into compact communities. We refine the communities found by CPM and LFM using our method and show their effectiveness compared to the original communities in a recommendation engine task.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Compacting+Large+and+Loose+Communities+V.+Kumar+Jawahar,http://research.google.com/pubs/pub41436.html
Physical Telepresence: Shape Capture and Display for Embodied Computer-mediated Remote Collaboration,ACM Symposium on User Interface Software and Technology ACM (2014) pp. 461-470,2014,Daniel Leithinger Sean Follmer Alex Olwal Hiroshi Ishii,@inproceedings{43153 title = {Physical Telepresence: Shape Capture and Display for Embodied Computer-mediated Remote Collaboration} author = {Daniel Leithinger and Sean Follmer and Alex Olwal and Hiroshi Ishii} year = 2014 URL = {http://olwal.com/#dynamic_physical_user_interfaces} booktitle = {ACM Symposium on User Interface Software and Technology} pages = {461-470} },We propose a new approach to Physical Telepresence based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper we describe the concept of shape transmission and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system.,http://research.google.com/pubs/archive/43153.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Physical+Telepresence:+Shape+Capture+and+Display+for+Embodied+Computer-mediated+Remote+Collaboration+Leithinger+Follmer+Olwal+Ishii,http://research.google.com/pubs/pub43153.html
Failure Trends in a Large Disk Drive Population,5th USENIX Conference on File and Storage Technologies (FAST 2007) pp. 17-29,2007,Eduardo Pinheiro Wolf-Dietrich Weber Luiz André Barroso,@inproceedings{32774 title = {Failure Trends in a Large Disk Drive Population} author = {Eduardo Pinheiro and Wolf-Dietrich Weber and Luiz André Barroso} year = 2007 URL = {http://research.google.com/archive/disk_failures.pdf} booktitle = {5th USENIX Conference on File and Storage Technologies (FAST 2007)} pages = {17-29} },It is estimated that over 90% of all new information produced in the world is being stored on magnetic media most of it in hard disk drives. Despite their importance there is relatively little published work on the failure patterns of disk drives and the key factors that affect their lifetime. Most available data are either based on extrapolation from accelerated aging experiments or from relatively modest sized field studies. Moreover larger population studies rarely have the infrastructure in place to collect health signals from components in operation which is critical information for detailed failure analysis. We present data collected from detailed observations of a large disk drive population in a production Internet services deployment. The population observed is many times larger than that of previous studies. In addition to presenting failure statistics we analyze the correlation between failures and several parameters generally believed to impact longevity. Our analysis identifies several parameters from the drive’s self monitoring facility (SMART) that correlate highly with failures. Despite this high correlation we conclude that models based on SMART parameters alone are unlikely to be useful for predicting individual drive failures. Surprisingly we found that temperature and activity levels were much less correlated with drive failures than previously reported.,http://research.google.com/archive/disk_failures.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Failure+Trends+in+a+Large+Disk+Drive+Population+Pinheiro+Weber+Barroso,http://research.google.com/pubs/pub32774.html
Tricorder: Building a Program Analysis Ecosystem,International Conference on Software Engineering (ICSE) (2015),2015,Caitlin Sadowski Jeffrey van Gogh Ciera Jaspan Emma Soederberg Collin Winter,@inproceedings{43322 title = {Tricorder: Building a Program Analysis Ecosystem} author = {Caitlin Sadowski and Jeffrey van Gogh and Ciera Jaspan and Emma Soederberg and Collin Winter} year = 2015 booktitle = {International Conference on Software Engineering (ICSE)} },Static analysis tools help developers find bugs improve code readability and ensure consistent style across a project. However these tools can be difficult to smoothly integrate with each other and into the developer workflow particularly when scaling to large codebases. We present Tricorder a program analysis platform aimed at building a data-driven ecosystem around program analysis. We present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles. We include an empirical in-situ evaluation of the tool as it is used by developers across,http://research.google.com/pubs/archive/43322.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Tricorder:+Building+a+Program+Analysis+Ecosystem+Sadowski+van+Gogh+Jaspan+Soederberg+Winter,http://research.google.com/pubs/pub43322.html
2nd international workshop on user evaluations for software engineering researchers (USER),International Conference on Software Engineering (ICSE) (2013),2013,Andrew Begel Caitlin Sadowski,@proceedings{41461 title = {2nd international workshop on user evaluations for software engineering researchers (USER)} editor = {Andrew Begel and Caitlin Sadowski} year = 2013 },We have met many software engineering researchers who would like to evaluate a tool or system they developed with real users but do not know how to begin. In this second iteration of the USER workshop attendees will collaboratively design develop and pilot plans for conducting user evaluations of their own tools and/or software engineering research projects. Attendees will gain practical experience with various user evaluation methods through scaffolded group exercises panel discussions and mentoring by a panel of user-focused software engineering researchers. Together we will establish a community of likeminded researchers and developers to help one another improve our research and practice through user evaluation.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=2nd+international+workshop+on+user+evaluations+for+software+engineering+researchers+(USER)+Begel+Sadowski,http://research.google.com/pubs/pub41461.html
APOSTLE: Longterm Transit Monitoring and Stability Analysis of XO-2b,The Astrophysical Journal vol. 770 (2013) pp. 36,2013,Praveen Kundurthy Rory Barnes Andrew C Becker Eric Agol Benjamin F Williams Noel Gorelick Amy Rose,@article{42925 title = {APOSTLE: Longterm Transit Monitoring and Stability Analysis of XO-2b} author = {Praveen Kundurthy and Rory Barnes and Andrew C Becker and Eric Agol and Benjamin F Williams and Noel Gorelick and Amy Rose} year = 2013 journal = {The Astrophysical Journal} pages = {36} volume = {770} },The Apache Point Survey of Transit Lightcurves of Exoplanets (APOSTLE) observed 10 transits of XO-2b over a period of 3 yr. We present measurements that confirm previous estimates of system parameters like the normalized semi-major axis (a/R) stellar density (_) impact parameter (b) and orbital inclination (iorb). Our errors on system parameters like a/R and _ have improved by _40% compared to previous best ground-based measurements. Our study of the transit times show no evidence for transit timing variations (TTVs) and we are able to rule out co-planar companions with masses 0.20 M_ in low order mean motion resonance with XO-2b. We also explored the stability of the XO-2 system given various orbital configurations of a hypothetical planet near the 2:1 mean motion resonance. We find that a wide range of orbits (including Earth-mass perturbers) are both dynamically stable and produce observable TTVs. We find that up to 51% of our stable simulations show TTVs that are smaller than the typical transit timing errors (_20 s) measured for XO-2b and hence remain undetectable. Key,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=APOSTLE:+Longterm+Transit+Monitoring+and+Stability+Analysis+of+XO-2b+Kundurthy+Barnes+Becker+Agol+Williams+Gorelick+Rose,http://research.google.com/pubs/pub42925.html
Indoor Scene Understanding with Geometric and Semantic Contexts,International Journal of Computer Vision (IJCV) (2014),2014,Wongun Choi Yu-Wei Chao Caroline Pantofaru Silvio Savarese,@article{43206 title = {Indoor Scene Understanding with Geometric and Semantic Contexts} author = {Wongun Choi and Yu-Wei Chao and Caroline Pantofaru and Silvio Savarese} year = 2014 URL = {http://rd.springer.com/article/10.1007/s11263-014-0779-4} journal = {International Journal of Computer Vision (IJCV)} },Truly understanding a scene involves integrating information at multiple levels as well as studying the interactions between scene elements. Individual object detectors layout estimators and scene classifiers are powerful but ultimately confounded by complicated real-world scenes with high variability different viewpoints and occlusions. We propose a method that can automatically learn the interactions among scene elements and apply them to the holistic understanding of indoor scenes from a single image. This interpretation is performed within a hierarchical interaction model which describes an image by a parse graph thereby fusing together object detection layout estimation and scene classification. At the root of the parse graph is the scene type and layout while the leaves are the individual detections of objects. In between is the core of the system our 3D Geometric Phrases (3DGP). We conduct extensive experimental evaluations on single image 3D scene understanding using both 2D and 3D metrics. The results demonstrate that our model with 3DGPs can provide robust estimation of scene type 3D space and 3D objects by leveraging the contextual relationships among the visual elements.,http://rd.springer.com/article/10.1007/s11263-014-0779-4,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Indoor+Scene+Understanding+with+Geometric+and+Semantic+Contexts+Choi+Chao+Pantofaru+Savarese,http://research.google.com/pubs/pub43206.html
Vocaine the Vocoder and Applications in Speech Synthesis,ICASSP IEEE (2015) (to appear),2015,Yannis Agiomyrgiannakis,@inproceedings{43336 title = {Vocaine the Vocoder and Applications in Speech Synthesis} author = {Yannis Agiomyrgiannakis} year = 2015 booktitle = {ICASSP} },Vocoders received renewed attention recently as basic components in speech synthesis applications such as voice transformation voice conversion and statistical parametric speech synthesis. This paper presents a new vocoder synthesizer referred to as Vocaine that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator. Extensive evaluations are made against several state-ofthe-art methods in Copy-Synthesis and Text-To-Speech synthesis experiments. Vocaine matches or outperforms STRAIGHT in CopySynthesis experiments and outperforms our baseline real-time optimized Mixed-Excitation vocoder with the same computational cost. We report that Vocaine considerably improves our statistical TTS synthesizers and that our new statistical parametric synthesizer [1] matched the quality of our mature production Unit-Selection system with uncompressed waveforms.,http://research.google.com/pubs/archive/43336.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Vocaine+the+Vocoder+and+Applications+in+Speech+Synthesis+Agiomyrgiannakis,http://research.google.com/pubs/pub43336.html
Practical MythTV: Building a PVR and Media Center PC,Apress (2007) pp. 350,2007,Michael Still Stewart Smith,@book{32813 title = {Practical MythTV: Building a PVR and Media Center PC} author = {Michael Still and Stewart Smith} year = 2007 pages = {350} },MythTV is a powerful open source personal video recorder (PVR) application that runs on Linux. Developed for several years by volunteers it offers a stable and extensible platform for automating all of the things you would expect from a PVR and much more. Practical MythTV: Open Source PVR and Media Center takes a project-based approach to implementing your own MythTV setup. You get to pick and choose the functionality you want to install for your PVR and will learn the details of everything from selecting hardware to advanced customization. You will learn how to record your favorite television shows store your DVDs for later playback create a music library out of your CD collection and even use your PVR for Voice over IP. Your PVR wouldn't be complete without a remote control or the ability to play back content to other TVs in your home. You'll learn how to do both of these things in this book. You'll even learn to how to utilize your Xbox as a remote front end to play back content. Beyond these basics you will learn advanced techniques like commercial detection and skipping auto-expiring content creating your own themes for MythTV and utilizing plug-ins to do things like display weather conditions RSS feeds and photo slide shows.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Practical+MythTV:+Building+a+PVR+and+Media+Center+PC+Still+Smith,http://research.google.com/pubs/pub32813.html
Exploring Photobios,ACM Trans. on Graphics (Proc. SIGGRAPH) vol. 30(4) (2011) (to appear),2011,Ira Kemelmacher-Shlizerman Eli Shechtman Rahul Garg Steven Seitz,@article{37110 title = {Exploring Photobios} author = {Ira Kemelmacher-Shlizerman and Eli Shechtman and Rahul Garg and Steven Seitz} year = 2011 URL = {http://grail.cs.washington.edu/photobios/paper.pdf} journal = {ACM Trans. on Graphics (Proc. SIGGRAPH)} volume = {30(4)} },We present an approach for generating face animations from large image collections of the same person. Such collections which we call photobios sample the appearance of a person over changes in pose facial expression hairstyle age and other variations. By optimizing the order in which images are displayed and crossdissolving between them we control the motion through face space and create compelling animations (e.g. render a smooth transition from frowning to smiling). Used in this context the cross dissolve produces a very strong motion effect; a key contribution of the paper is to explain this effect and analyze its operating range. The approach operates by creating a graph with faces as nodes and similarities as edges and solving for walks and shortest paths on this graph. The processing pipeline involves face detection locating ﬁducials (eyes/nose/mouth) solving for pose warping to frontal views and image comparison based on Local Binary Patterns. We demonstrate results on a variety of datasets including time-lapse photography personal photo collections and images of celebrities downloaded from the Internet. Our approach is the basis for the Face Movies feature in Google’s Picasa.,http://grail.cs.washington.edu/photobios/paper.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Exploring+Photobios+Kemelmacher-Shlizerman+Shechtman+Garg+Seitz,http://research.google.com/pubs/pub37110.html
Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL '11) (2011) Best Paper Award,2011,Dipanjan Das Slav Petrov,@inproceedings{37071 title = {Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections} author = {Dipanjan Das and Slav Petrov} year = 2011 URL = {http://petrovi.de/data/acl11.pdf} booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL '11)} pages = {Best Paper Award} },We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed) making it applicable for a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as constraints in an unsupervised model. Across six European languages our approach results in an average absolute improvement of 9.7\% over the state-of-the-art baseline and 17.0\% over vanilla hidden Markov models induced with EM.,http://research.google.com/pubs/archive/37071.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Part-of-Speech+Tagging+with+Bilingual+Graph-Based+Projections+Das+Petrov,http://research.google.com/pubs/pub37071.html
Multicut in trees viewed through the eyes of vertex cover,WADS 2011,2011,Jianer Chen Jiahao Fan Iyad A. Kanj Yang Liu Fenghui Zhang,@inproceedings{37068 title = {Multicut in trees viewed through the eyes of vertex cover} author = {Jianer Chen and Jiahao Fan and Iyad A. Kanj and Yang Liu and Fenghui Zhang} year = 2011 booktitle = {WADS 2011} },We take a new look at the multicut problem in trees through the eyes of the vertex cover problem. This connection together with other techniques that we develop allows us to signicantly improve the O(k^6) upper bound on the kernel size for multicut given by Bousquet et al. to O(k^3). We exploit this connection further to present a parameterized algorithm for multicut that runs in time O(p^k) where p = ( sqrt(5) + 1)/2 ~= 1.618. This improves the previous (time) upper bound of O(2^k) given by Guo and Niedermeier for the problem.,http://research.google.com/pubs/archive/37068.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multicut+in+trees+viewed+through+the+eyes+of+vertex+cover+Chen+Fan+Kanj+Liu+Zhang,http://research.google.com/pubs/pub37068.html
Herschel/PACS Survey of protoplanetary disks in Taurus/Auriga - Observations of [OI] and [CII] and far infrared continuum,The Astrophysical Journal vol. 776 (2013) pp. 21-45,2013,Christian Howard,@article{41762 title = {Herschel/PACS Survey of protoplanetary disks in Taurus/Auriga -- Observations of [OI] and [CII] and far infrared continuum} author = {Christian Howard} year = 2013 URL = {http://iopscience.iop.org/0004-637X/776/1/21/pdf/0004-637X_776_1_21.pdf} journal = {The Astrophysical Journal} pages = {21--45} volume = {776} },The Herschel Space Observatory was used to observe ~ 120 pre-main-sequence stars in Taurus as part of the GASPS Open Time Key project. PACS was used to measure the continuum as well as several gas tracers such as [OI] 63 _m [OI] 145 _m [CII] 158 _m OH H2O and CO. The strongest line seen is [OI] at 63 _m. We find a clear correlation between the strength of the [OI] 63 _m line and the 63 _m continuum for disk sources. In outflow sources the line emission can be up to 20 times stronger than in disk sources suggesting that the line emission is dominated by the outflow. The tight correlation seen for disk sources suggests that the emission arises from the inner disk (< 50 AU) and lower surface layers of the disk where the gas and dust are coupled. The [OI] 63 _m is fainter in transitional stars than in normal Class II disks. Simple SED models indicate that the dust responsible for the continuum emission is colder in these disks leading to weaker line emission. [CII] 158 _m emission is only detected in strong outflow sources. The observed line ratios of [OI] 63 _m to [OI] 145 _m are in the regime where we are insensitive to the gas-to-dust ratio neither can we discriminate between shock or PDR emission. We detect no Class III object in [OI] 63 _m and only three in continuum at least one of which is a candidate debris disk.,http://research.google.com/pubs/archive/41762.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Herschel/PACS+Survey+of+protoplanetary+disks+in+Taurus/Auriga+--+Observations+of+%5BOI%5D+and+%5BCII%5D+and+far+infrared+continuum+Howard,http://research.google.com/pubs/pub41762.html
Evaluating Static Analysis Defect Warnings on Production Software,Proceedings of the 7th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering ACM Press New York NY USA (2007) pp. 1-8,2007,Nathaniel Ayewah William Pugh J. David Morgenthaler John Penix YuQian Zhou,@inproceedings{32791 title = {Evaluating Static Analysis Defect Warnings on Production Software} author = {Nathaniel Ayewah and William Pugh and J. David Morgenthaler and John Penix and YuQian Zhou} year = 2007 URL = {http://doi.acm.org/10.1145/1251535.1251536} booktitle = {Proceedings of the 7th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering} pages = {1-8} address = {New York NY USA} },Classification of static analysis warnings into false positive trivial or serious bugs: Experience on Java JDK and Google codebase,http://research.google.com/pubs/archive/32791.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evaluating+Static+Analysis+Defect+Warnings+on+Production+Software+Ayewah+Pugh+Morgenthaler+Penix+Zhou,http://research.google.com/pubs/pub32791.html
The YouTube Social Network,Sixth International AAAI Conference on Weblogs and Social Media (ICWSM 2012),2012,Mirjam Wattenhofer Roger Wattenhofer Zack Zhu,@proceedings{37738 title = {The YouTube Social Network} editor = {Mirjam Wattenhofer and Roger Wattenhofer and Zack Zhu} year = 2012 booktitle = {ICWSM 2012} },Today YouTube is the largest user-driven video content provider in the world; it has become a major platform for disseminating multimedia information. A major contribution to its success comes from the user-to-user social experience that differentiates it from traditional content broadcasters. This work examines the social network aspect of YouTube by measuring the fullscale YouTube subscription graph comment graph and video content corpus. We ﬁnd YouTube to deviate signiﬁcantly from network characteristics that mark traditional online social networks such as homophily reciprocative linking and assortativity. However comparing to reported characteristics of another content-driven online social network Twitter YouTube is remarkably similar. Examining the social and content facets of user popularity we ﬁnd a stronger correlation between a user’s social popularity and his/her most popular content as opposed to typical content popularity. Finally we demonstrate an application of our measurements for classifying YouTube Partners who are selected users that share YouTube’s advertisement revenue. Results are motivating despite the highly imbalanced nature of the classiﬁcation proble,http://research.google.com/pubs/archive/37738.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+YouTube+Social+Network+Wattenhofer+Wattenhofer+Zhu,http://research.google.com/pubs/pub37738.html
Designing for Healthy Lifestyles: Design Considerations for Mobile Technologies to Encourage Consumer Health and Wellness,Foundations and Trends® in Human-Computer Interaction vol. 6 (2014) 167–315,2014,Sunny Consolvo Predrag Klasnja David W. McDonald James A. Landay,@article{42488 title = {Designing for Healthy Lifestyles: Design Considerations for Mobile Technologies to Encourage Consumer Health and Wellness} author = {Sunny Consolvo and Predrag Klasnja and David W. McDonald and James A. Landay} year = 2014 journal = {Foundations and Trends® in Human-Computer Interaction} pages = {167–315} volume = {6} },As the rates of lifestyle diseases such as obesity diabetes and heart disease continue to rise the development of e_ective tools that can help people adopt and sustain healthier habits is becoming ever more important. Mobile computing holds great promise for providing e_ective support for helping people manage their health in everyday life. Yet for this promise to be realized mobile wellness systems need to be well designed not only in terms of how they implement speciﬁc behavior-change techniques but also among other factors in terms of how much burden they put on the user how well they integrate into the user’s daily life and how they address the user’s privacy concerns. Designing for all of these constraints is di_cult and it is often not clear what tradeo_s particular design decisions have on how a wellness application is experienced and used. In this monograph we provide an account of di_erent design approaches to common features of mobile wellness applications and we discuss the tradeo_s inherent in those approaches. We also outline the key challenges that HCI researchers and designers will need to address to move the state of the art for mobile wellness technologies forward.,http://research.google.com/pubs/archive/42488.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Designing+for+Healthy+Lifestyles:+Design+Considerations+for+Mobile+Technologies+to+Encourage+Consumer+Health+and+Wellness+Consolvo+Klasnja+McDonald+Landay,http://research.google.com/pubs/pub42488.html
An optimized template matching approach to intra coding in video/image compression,IS&T/SPIE Electronic Imaging 2014 SPIE pp. 1-6,2014,Hui Su Jingning Han Yaowu Xu,@inproceedings{43249 title = {An optimized template matching approach to intra coding in video/image compression} author = {Hui Su and Jingning Han and Yaowu Xu} year = 2014 booktitle = {IS&T/SPIE Electronic Imaging 2014} pages = {1-6} },The template matching prediction is an established approach to intra-frame coding that makes use of previously coded pixels in the same frame for reference. It compares the previously reconstructed upper and left boundaries in searching from the reference area the best matched block for prediction and hence eliminates the need of sending additional information to reproduce the same prediction at decoder. In viewing the image signal as an auto-regressive model this work is premised on the fact that pixels closer to the known block boundary are better predicted than those far apart. It significantly extends the scope of the template matching approach which is typically followed by a conventional discrete cosine transform (DCT) for the prediction residuals by employing an asymmetric discrete sine transform (ADST) whose basis functions vanish at the prediction boundary and reach maximum magnitude at far end to fully exploit statistics of the residual signals. It was experimentally shown that the proposed scheme provides substantial coding performance gains on top of the conventional template matching method over the baseline.,http://research.google.com/pubs/archive/43249.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+optimized+template+matching+approach+to+intra+coding+in+video/image+compression+Su+Han+Xu,http://research.google.com/pubs/pub43249.html
Frame-Semantic Parsing,Computational Linguistics vol. 40:1 (2014) pp. 9-56,2014,Dipanjan Das Desai Chen André F. T. Martins Nathan Schneider Noah A. Smith,@article{41227 title = {Frame-Semantic Parsing} author = {Dipanjan Das and Desai Chen and André F. T. Martins and Nathan Schneider and Noah A. Smith} year = 2014 journal = {Computational Linguistics} pages = {9--56} volume = {40:1} },Frame semantics (Fillmore 1982) is a linguistic theory that has been instantiated for English in the FrameNet lexicon (Fillmore Johnson and Petruck 2003). We solve the problem of frame-semantic parsing using a two-stage statistical model that takes lexical targets (i.e. content words and phrases) in their sentential contexts and predicts frame-semantic structures. Given a target in context the first stage disambiguates it to a semantic frame. This model employs latent variables and semi-supervised learning to improve frame disambiguation for targets unseen at training time. The second stage finds the target's locally expressed semantic arguments. At inference time a fast exact dual decomposition algorithm collectively predicts all the arguments of a frame at once in order to respect declaratively stated linguistic constraints resulting in qualitatively better structures than naïve local predictors. Both components are feature-based and discriminatively trained on a small set of annotated frame-semantic parses. On the SemEval 2007 benchmark dataset the approach along with a heuristic identifier of frame-evoking targets outperforms the prior state of the art by significant margins. Additionally we present experiments on the much larger FrameNet 1.5 dataset. We have released our frame-semantic parser as open-source software.,http://research.google.com/pubs/archive/41227.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Frame-Semantic+Parsing+Das+Chen+Martins+Schneider+Smith,http://research.google.com/pubs/pub41227.html
Detecting influenza epidemics using search engine query data,Nature vol. 457 (2009) pp. 1012-1014,2009,Jeremy Ginsberg Matthew Mohebbi Rajan Patel Lynnette Brammer Mark Smolinski Larry Brilliant,@article{34503 title = {Detecting influenza epidemics using search engine query data} author = {Jeremy Ginsberg and Matthew Mohebbi and Rajan Patel and Lynnette Brammer and Mark Smolinski and Larry Brilliant} year = 2009 URL = {http://www.nature.com/nature/journal/v457/n7232/full/nature07634.html} note = {doi:10.1038/nature07634} journal = {Nature} pages = {1012-1014} volume = {457} },Seasonal influenza epidemics are a major public health concern causing tens of millions of respiratory illnesses and 250000 to 500000 deaths worldwide each year. In addition to seasonal influenza a new strain of influenza virus against which no previous immunity exists and that demonstrates human-to-human transmission could result in a pandemic with millions of fatalities. Early detection of disease activity when followed by a rapid response can reduce the impact of both seasonal and pandemic influenza. One way to improve early detection is to monitor health-seeking behaviour in the form of queries to online search engines which are submitted by millions of users around the world each day. Here we present a method of analysing large numbers of Google search queries to track influenza-like illness in a population. Because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza-like symptoms we can accurately estimate the current level of weekly influenza activity in each region of the United States with a reporting lag of about one day. This approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web search users. Self-archived manuscript (PDF),http://research.google.com/pubs/archive/34503.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Detecting+influenza+epidemics+using+search+engine+query+data+Ginsberg+Mohebbi+Patel+Brammer+Smolinski+Brilliant,http://research.google.com/pubs/pub34503.html
Optimizing the update packet stream for web applications,ICST International Conference on Broadband Communications Networks and Systems (BROADNETS) (2010),2010,Muthuprasanna Muthusrinivasan Manimaran Govindarasu,@inproceedings{35159 title = {Optimizing the update packet stream for web applications} author = {Muthuprasanna Muthusrinivasan and Manimaran Govindarasu} year = 2010 booktitle = {ICST International Conference on Broadband Communications Networks and Systems (BROADNETS)} },The Internet has evolved to an extent where users now expect any-where any-time and any-form access to their personalized data and applications of choice. However providing a coherent (seamless) user experience across multiple devices has been relatively hard to achieve. While the 'how to sync' problem has been well studied in literature the complementary 'when to sync' problem has remained relatively unexplored. While frequent updates providing higher user satisfaction/retention are naturally more desirable than sparse updates the steadily escalating resource costs are a significant bottleneck. We thus propose extensions to the traditional periodic refresh model based on an adaptive 'smart sync approach' that enables variable rate updates closely modeling expected user behavior over time. An experimental evaluation of the proposed mechanism on a sizeable subset of users of the GMAIL web interface indicates that the proposed refresh policy can achieve the best of both worlds - limited resource provisioning and minimal user-perceived delays.,http://research.google.com/pubs/archive/35159.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+the+update+packet+stream+for+web+applications+Muthusrinivasan+Manimaran,http://research.google.com/pubs/pub35159.html
Harmonizing classes functions tuples and type parameters in Virgil III,Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation ACM New York New York (2013) pp. 85-94,2013,Ben L. Titzer,@inproceedings{41446 title = {Harmonizing classes functions tuples and type parameters in Virgil III} author = {Ben L. Titzer} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2491956.2491962} booktitle = {Proceedings of the 34th ACM SIGPLAN conference on Programming language design and implementation} pages = {85-94} address = {New York New York} },Languages are becoming increasingly multi-paradigm. Subtype polymorphism in statically-typed object-oriented languages is being supplemented with parametric polymorphism in the form of generics. Features like first-class functions and lambdas are appearing everywhere. Yet existing languages like Java C# C++ D and Scala seem to accrete ever more complexity when they reach beyond their original paradigm into another; inevitably older features have some rough edges that lead to nonuniformity and pitfalls. Given a fresh start a new language designer is faced with a daunting array of potential features. Where to start? What is important to get right first and what can be added later? What features must work together and what features are orthogonal? We report on our experience with Virgil III a practical language with a careful balance of classes functions tuples and type parameters. Virgil intentionally lacks many advanced features yet we find its core feature set enables new species of design patterns that bridge multiple paradigms and emulate features not directly supported such as interfaces abstract data types ad hoc polymorphism and variant types. Surprisingly we find variance for function types and tuple types often replaces the need for other kinds of type variance when libraries are designed in a more functional style.,http://research.google.com/pubs/archive/41446.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Harmonizing+classes+functions+tuples+and+type+parameters+in+Virgil+III+Titzer,http://research.google.com/pubs/pub41446.html
Street View Motion-from-Structure-from-Motion,Proceedings of the International Conference on Computer Vision IEEE (2013),2013,Bryan Klingner David Martin James Roseborough,@inproceedings{41413 title = {Street View Motion-from-Structure-from-Motion} author = {Bryan Klingner and David Martin and James Roseborough} year = 2013 booktitle = {Proceedings of the International Conference on Computer Vision} },"We describe a structure-from-motion framework that handles ""generalized"" cameras such as moving rolling-shutter cameras and works at an unprecedented scale--billions of images covering millions of linear kilometers of roads--by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale appearance-augmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.",http://research.google.com/pubs/archive/41413.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Street+View+Motion-from-Structure-from-Motion+Klingner+Martin+Roseborough,http://research.google.com/pubs/pub41413.html
Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search,ICCV 2013,2013,Dror Aiger Efi Kokiopoulou Ehud Rivlin,@inproceedings{41388 title = {Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search} author = {Dror Aiger and Efi Kokiopoulou and Ehud Rivlin} year = 2013 booktitle = {ICCV 2013} },We propose two solutions for both nearest neigh- bors and range search problems. For the nearest neighbors problem we propose a c-approximate so- lution for the restricted version of the decision prob- lem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descrip- tors). We compare our algorithms to the best known methods for these problems i.e. LSH ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. In contrast to tree structures our algorithms are trivial to parallelize. In the experiments con- ducted running on couple of million images our algorithms show meaningful speed-ups when com- pared with the above mentioned methods.,http://research.google.com/pubs/archive/41388.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Random+Grids:+Fast+Approximate+Nearest+Neighbors+and+Range+Searching+for+Image+Search+Aiger+Kokiopoulou+Rivlin,http://research.google.com/pubs/pub41388.html
Conflict-Driven Conditional Termination,Computer Aided Verification Springer International Publishing (2015) pp. 271-286,2015,Vijay D'Silva Caterina Urban,@inproceedings{43854 title = {Conflict-Driven Conditional Termination} author = {Vijay D'Silva and Caterina Urban} year = 2015 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-319-21668-3_16} booktitle = {Computer Aided Verification} pages = {271--286} },Conflict-driven learning which is essential to the performance of sat and smt solvers consists of a procedure that searches for a model of a formula and refutation procedure for proving that no model exists. This paper shows that conflict-driven learning can improve the precision of a termination analysis based on abstract interpretation. We encode non-termination as satisfiability in a monadic second-order logic and use abstract interpreters to reason about the satisfiability of this formula. Our search procedure combines decisions with reachability analysis to find potentially non-terminating executions and our refutation procedure uses a conditional termination analysis. Our implementation extends the set of conditional termination arguments discovered by an existing termination analyzer.,http://research.google.com/pubs/archive/43854.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Conflict-Driven+Conditional+Termination+D'Silva+Urban,http://research.google.com/pubs/pub43854.html
Multilingual Open Relation Extraction Using Cross-lingual Projection,Proceedings of NAACL (2015),2015,Manaal Faruqui Shankar Kumar,@inproceedings{43449 title = {Multilingual Open Relation Extraction Using Cross-lingual Projection} author = {Manaal Faruqui and Shankar Kumar} year = 2015 booktitle = {Proceedings of NAACL} },Open domain relation extraction systems identify relation and argument phrases in a sentence without relying on any underlying schema. However current state-of-the-art relation extraction systems are available only for English because of their heavy reliance on linguistic tools such as part-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia.,http://research.google.com/pubs/archive/43449.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multilingual+Open+Relation+Extraction+Using+Cross-lingual+Projection+Faruqui+Kumar,http://research.google.com/pubs/pub43449.html
Let's Parse to Prevent Pwnage,USENIX workshop on Large-Scale Exploits and Emergent Threats USENIX (2012),2012,Mike Samuel Úlfar Erlingsson,@inproceedings{38080 title = {Let's Parse to Prevent Pwnage} author = {Mike Samuel and Úlfar Erlingsson} year = 2012 URL = {https://www.usenix.org/lets-parse-prevent-pwnage} booktitle = {USENIX workshop on Large-Scale Exploits and Emergent Threats} },Software that processes rich content suffers from endemic security vulnerabilities. Frequently these bugs are due to data confusion: discrepancies in how content data is parsed composed and otherwise processed by different applications frameworks and language runtimes. Data confusion often enables code injection attacks such as cross-site scripting or SQL injection by leading to incorrect assumptions about the encodings and checks applied to rich content of uncertain provenance. However even for well-structured value-only content data confusion can critically impact security e.g. as shown by XML signature vulnerabilities [12]. This paper advocates the position that data confusion can be effectively prevented through the use of simple mechanisms—based on parsing—that eliminate ambiguities by fully resolving content data to normalized clearly-understood forms. Using code injection on the Web as our motivation we make the case that automatic defense mechanisms should be integrated with programming languages application frameworks and runtime libraries and applied with little or no developer intervention. We outline a scalable sustainable approach for developing and maintaining those mechanisms. The resulting tools can offer comprehensive protection against data confusion even when multiple types of rich content data are processed and composed in complex ways.,http://research.google.com/pubs/archive/38080.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Let's+Parse+to+Prevent+Pwnage+Samuel+Erlingsson,http://research.google.com/pubs/pub38080.html
Neighborhood Preserving Codes for Assigning Point Labels: Applications to Stochastic Search,Procedia Computer Science: 2013 International Conference on Computational Science Elsevier pp. 956-965,2013,Shumeet Baluja Michele Covell,@inproceedings{41333 title = {Neighborhood Preserving Codes for Assigning Point Labels: Applications to Stochastic Search} author = {Shumeet Baluja and Michele Covell} year = 2013 URL = {http://www.esprockets.com/papers/2013-neighborhood-preserving-coms.pdf} booktitle = {Procedia Computer Science: 2013 International Conference on Computational Science} pages = {956--965} },Selecting a good representation of a solution-space is vital to solving any search and optimization problem. In particular once regions of high performance are found having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this it is common for stochastic search algorithms such as stochastic hillclimbing evolutionary algorithms (including genetic algorithms) and simulated annealing to employ Gray Codes for encoding ordinal points or discretized real numbers. In this paper we present a novel method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be seen as an approximate high-dimensional variant of Gray Codes with standard Gray Codes as a subset of the labels found here. The labeling procedure is applicable to any task in which the solution requires the search algorithm to select a small subset of items out of many. Such tasks include vertex selection in graphs knapsack-constrained item selection bin packing prototype selection for machine learning and numerous scheduling problems to name a few.,http://www.esprockets.com/papers/2013-neighborhood-preserving-coms.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Neighborhood+Preserving+Codes+for+Assigning+Point+Labels:+Applications+to+Stochastic+Search+Baluja+Covell,http://research.google.com/pubs/pub41333.html
HERMES: Mobile system for instability analysis and balance assessment,ACM Transactions on Embedded Computing Systems (TECS) vol. 12 (2013) 57:1-57:24,2013,Hyduke Noshadi Foad Dabiri Shaun Ahmadian Navid Amini Majid Sarrafzadeh,@article{41347 title = {HERMES: Mobile system for instability analysis and balance assessment} author = {Hyduke Noshadi and Foad Dabiri and Shaun Ahmadian and Navid Amini and Majid Sarrafzadeh} year = 2013 URL = {http://dl.acm.org/citation.cfm?id=2435253} journal = {ACM Transactions on Embedded Computing Systems (TECS)} pages = {57:1--57:24} volume = {12} },We introduce Hermes a lightweight smart shoe and its supporting infrastructure aimed at extending gait and instability analysis and human instability/balance monitoring outside of a laboratory environment. We aimed to create a scientific tool capable of high-level measures by combining embedded sensing signal processing and modeling techniques. Hermes monitors walking behavior and uses an instability assessment model to generate quantitative value with episodes of activity identified by physician researchers or investigators as important. The underlying instability assessment model incorporates variability and correlation of features extracted during ambulation that have been identified by geriatric motion study experts as precursor to instability balance abnormality and possible fall risk. Hermes provides a mobile affordable and long-term instability analysis and detection system that is customizable to individual users and is context-aware with the capability of being guided by experts. Our experiments demonstrate the feasibility of our model and the complimentary role our system can play by providing long-term monitoring of patients outside a hospital or clinical setting at a reduced cost with greater user convenience compliance and inference capabilities that meet the physician's or investigator's needs.,http://dl.acm.org/citation.cfm?id=2435253,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HERMES:+Mobile+system+for+instability+analysis+and+balance+assessment+Noshadi+Dabiri+Ahmadian+Amini+Sarrafzadeh,http://research.google.com/pubs/pub41347.html
Identifying and Exploiting Windows Kernel Race Conditions via Memory Access Patterns,Bochspwn: Exploiting Kernel Race Conditions Found via Memory Access Patterns The Symposium on Security for Asia Network 102F Pasir Panjang Road #08-02 Singapore 118530 (2013) pp. 69,2013,Mateusz Jurczyk Gynvael Coldwind,@inproceedings{42189 title = {Identifying and Exploiting Windows Kernel Race Conditions via Memory Access Patterns} author = {Mateusz Jurczyk and Gynvael Coldwind} year = 2013 booktitle = {Bochspwn: Exploiting Kernel Race Conditions Found via Memory Access Patterns} pages = {69} address = {102F Pasir Panjang Road #08-02 Singapore 118530} },The overall security posture of operating systems’ kernels – and specifically the Microsoft Windows NT kernel – against both local and remote attacks has visibly improved throughout the last decade. In our opinion this is primarily due to the increasing interest in kernel-mode vulnerabilities by both white and black-hat parties as they ultimately allow attackers to subvert the currently widespread defense-in-depth technologies implemented on operating system level such as sandboxing or other features enabling better management of privileges within the execution environment (e.g. Mandatory Integrity Control ). As a direct outcome Microsoft has invested considerable resources in both improving the development process with programs like Secure Development Lifecycle and explicitly hardening the kernel against existing attacks; the latter was particularly characteristic to Windows 8 which introduced more kernel security improvements than any NT-family system thus far[11]. In this paper we discuss the concept of employing CPU-level operating system instrumentation to identify potential instances of local race conditions in fetching user-mode input data within system call handlers and other user-facing ring-0 code and how it was successfully implemented in the Bochspwn project. Further in the document we present a number of generic techniques easing the exploitation of timing bound kernel vulnerabilities and show how these techniques can be employed in practical attacks against three exemplary vulnerabilities discovered by Bochspwn. In the last sections we conclusively provide some suggestions on related research areas that haven’t been fully explored and require further development.,http://research.google.com/pubs/archive/42189.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Identifying+and+Exploiting+Windows+Kernel+Race+Conditions+via+Memory+Access+Patterns+Jurczyk+Coldwind,http://research.google.com/pubs/pub42189.html
A Method for Measuring Online Audiences,Google Inc (2013) pp. 1-24 (to appear),2013,Jim Koehler Evgeny Skvortsov Wiesner Vos,@techreport{41089 title = {A Method for Measuring Online Audiences} author = {Jim Koehler and Evgeny Skvortsov and Wiesner Vos} year = 2013 institution = {Google Inc} },We present a method for measuring the reach and frequency of online ad campaigns by audience attributes. This method uses a combination of data sources including ad server logs publisher provided user data (PPD) census data and a representative online panel. It adjusts for known problems with cookie data and potential non-representative and inaccurate PPD. It generalizes for multiple publishers and for targeting based on the PPD. The method includes the conversion of adjusted cookie counts to unique audience counts. The benefit of our method is that we get both reduced variance from server logs and reduced bias from the panel. Simulation results and a case study are presented.,http://research.google.com/pubs/archive/41089.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Method+for+Measuring+Online+Audiences+Koehler+Skvortsov+Vos,http://research.google.com/pubs/pub41089.html
Self-evaluation in Advanced Power Searching and Mapping with Google MOOCs,ACM Learning at Scale (2014),2014,Julia Wilkowski Daniel M. Russell Amit Deutsch,@inproceedings{41928 title = {Self-evaluation in Advanced Power Searching and Mapping with Google MOOCs} author = {Julia Wilkowski and Daniel M. Russell and Amit Deutsch} year = 2014 booktitle = {ACM Learning at Scale} },While there is a large amount of work on creating autograded massive open online courses (MOOCs) some kinds of complex qualitative exam questions are still beyond the current state of the art. For MOOCs that need to deal with these kinds of questions it is not possible for a small course staff to grade students’ qualitative work. To test the efficacy of self-evaluation as a method for complex-question evaluation students in two Google MOOCs have submitted projects and evaluated their own work. For both courses teaching assistants graded a random sample of papers and compared their grades with self-evaluated student grades. We found that many of the submitted projects were of very high quality and that a large majority of self-evaluated projects were accurately evaluated scoring within just a few points of the gold standard grading.,http://research.google.com/pubs/archive/41928.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Self-evaluation+in+Advanced+Power+Searching+and+Mapping+with+Google+MOOCs+Wilkowski+Russell+Deutsch,http://research.google.com/pubs/pub41928.html
Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,2013,Kuzman Ganchev Dipanjan Das,@inproceedings{41533 title = {Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization} author = {Kuzman Ganchev and Dipanjan Das} year = 2013 booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing} },We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resource-impoverished target language that incorporates soft constraints via posterior regularization. To this end we use automatically word aligned bitext between the source and target language pair and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and named-entity segmentation.,http://research.google.com/pubs/archive/41533.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cross-Lingual+Discriminative+Learning+of+Sequence+Models+with+Posterior+Regularization+Ganchev+Das,http://research.google.com/pubs/pub41533.html
Web-derived Pronunciations,IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) (2009) pp. 4289-4292,2009,Arnab Ghoshal Martin Jansche Sanjeev Khudanpur Michael Riley Morgan Ulinski,@inproceedings{34837 title = {Web-derived Pronunciations} author = {Arnab Ghoshal and Martin Jansche and Sanjeev Khudanpur and Michael Riley and Morgan Ulinski} year = 2009 note = {doi:10.1109/ICASSP.2009.4960577} booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4289--4292} },Pronunciation information is available in large quantities on the Web in the form of IPA and ad-hoc transcriptions. We describe techniques for extracting candidate pronunciations from Web pages and associating them with orthographic words filtering out poorly extracted pronunciations normalizing IPA pronunciations to better conform to a common transcription standard and generating phonemic from ad-hoc transcriptions. We show improvements on a letter-to-phoneme task when using web-derived vs. Pronlex pronunciations.,http://research.google.com/pubs/archive/34837.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web-derived+Pronunciations+Ghoshal+Jansche+Khudanpur+Riley+Ulinski,http://research.google.com/pubs/pub34837.html
Warehouse-scale Computing: entering the teenage decade,Association for Computing Machinery (2011),2011,Luiz André Barroso,@misc{37206 title = {Warehouse-scale Computing: entering the teenage decade} author = {Luiz André Barroso} year = 2011 URL = {http://dl.acm.org/citation.cfm?id=2019527} },Video recording of a plenary talk delivered at the 2011 ACM Federated Computing Research Conference focusing on some important challenges awaiting programmers and designers of Warehouse-scale Computers as it enters its second decade. June 8 2011 San Jose CA.,http://dl.acm.org/citation.cfm?id=2019527,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Warehouse-scale+Computing:+entering+the+teenage+decade+Barroso,http://research.google.com/pubs/pub37206.html
Using Entity Information from a Knowledge Base to Improve Relation Extraction,Proceedings of the 13th annual workshop of The Australasian Language Technology Association Association for Computational Linguistics (2015),2015,Lan Du Anish Kumar M. Johnson Massimiliano Ciaramita,@inproceedings{44316 title = {Using Entity Information from a Knowledge Base to Improve Relation Extraction} author = {Lan Du and Anish Kumar and M. Johnson and Massimiliano Ciaramita} year = 2015 booktitle = {Proceedings of the 13th annual workshop of The Australasian Language Technology Association} },Relation extraction is the task of extracting predicate-argument relationships between entities from natural language text. This paper investigates whether background information about entities available in knowledge bases such as FreeBase can be used to improve the accuracy of a state-of-the-art relation extraction system. We describe a simple and effective way of incorporating FreeBase’s notable types into a state-of-the-art relation extraction system (Riedel et al. 2013). Experimental results show that our notable type-based system achieves an average 7.5% weighted MAP score improvement. To understand where the notable type information contributes the most we perform a series of ablation experiments. Results show that the notable type information improves relation extraction more than NER labels alone across a wide range of entity types and relations.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Entity+Information+from+a+Knowledge+Base+to+Improve+Relation+Extraction+Du+Kumar+Johnson+Ciaramita,http://research.google.com/pubs/pub44316.html
SAC070 - ICANN SSAC Advisory on the Use of Static TLD / Suffix Lists,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2015) pp. 32,2015,Warren Kumari Jaap Akkerhuis Patrik Fältström,@incollection{43821 title = {SAC070 - ICANN SSAC Advisory on the Use of Static TLD / Suffix Lists} author = {Warren Kumari and Jaap Akkerhuis and Patrik Fältström} year = 2015 URL = {https://www.icann.org/en/system/files/files/sac-070-en.pdf} booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} pages = {32} },"This advisory investigates the security and stability needs surrounding the growing use of public suffix lists on the Internet. For the purposes of this Advisory a public suffix is defined as “a domain under which multiple parties that are unaffiliated with the owner of the Public Suffix domain may register subdomains.” Examples of Public Suffix domains include ""org"" ""co.uk"" ""k12.wa.us"" and ""uk.com"". There is no programmatic way to determine the boundary where a Domain Name System (DNS) label changes stewardship from a public suffix yet tracking the boundary accurately is critically important for security privacy and usability issues in many modern systems and applications such as web browsers. One method of determining this boundary is by use of public suffix lists (PSLs) which are static files listing the known public suffixes.",http://research.google.com/pubs/archive/43821.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC070+-+ICANN+SSAC+Advisory+on+the+Use+of+Static+TLD+/+Suffix+Lists+Kumari+Akkerhuis+F%C3%A4ltstr%C3%B6m,http://research.google.com/pubs/pub43821.html
Large-scale Video Classiﬁcation with Convolutional Neural Networks,Proceedings of International Computer Vision and Pattern Recognition (CVPR 2014) IEEE,2014,Andrej Karpathy George Toderici Sanketh Shetty Thomas Leung Rahul Sukthankar Li Fei-Fei,@inproceedings{42455 title = {Large-scale Video Classiﬁcation with Convolutional Neural Networks} author = {Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei} year = 2014 booktitle = {Proceedings of International Computer Vision and Pattern Recognition (CVPR 2014)} },Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results we provide an extensive empirical evaluation of CNNs on large-scale video classification using a dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multi-resolution foveated architecture as a promising way of regularizing the learning problem and speeding up training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%) but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).,http://research.google.com/pubs/archive/42455.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+Video+Classi%EF%AC%81cation+with+Convolutional+Neural+Networks+Karpathy+Toderici+Shetty+Leung+Sukthankar+Fei-Fei,http://research.google.com/pubs/pub42455.html
Logical Attestation: An Authorization Architecture for Trustworthy Computing,Proceedings of the 23rd ACM Symposium on Operating System Principles ACM New York NY USA (2011),2011,Emin Gün Sirer Willem de Bruijn Patrick Reynolds Alan Shieh Kevin Walsh Dan Williams Fred B. Schneider,@inproceedings{37512 title = {Logical Attestation: An Authorization Architecture for Trustworthy Computing} author = {Emin Gün Sirer and Willem de Bruijn and Patrick Reynolds and Alan Shieh and Kevin Walsh and Dan Williams and Fred B. Schneider} year = 2011 URL = {http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/18-sirer.pdf} note = {Finished while post-doctoral researcher at Cornell University Ithaca NY USA} booktitle = {Proceedings of the 23rd ACM Symposium on Operating System Principles} address = {New York NY USA} },This paper describes the design and implementation of a new operating system authorization architecture to support trustworthy computing. Called logical attestation this architecture provides a sound framework for reasoning about run time behavior of applications. Logical attestation is based on attributable unforgeable statements about program properties expressed in a logic. These statements are suitable for mechanical processing proof construction and verification; they can serve as credentials support authorization based on expressive authorization policies and enable remote principals to trust software components without restricting the local user’s choice of binary implementations. We have implemented logical attestation in a new operating system called the Nexus. The Nexus executes natively on x86 platforms equipped with secure coprocessors. It supports both native Linux applications and uses logical attestation to support new trustworthy-computing applications. When deployed on a trustworthy cloud-computing stack logical attestation is efficient achieves high-performance and can run applications that provide qualitative guarantees not possible with existing modes of attestation.,http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/18-sirer.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Logical+Attestation:+An+Authorization+Architecture+for+Trustworthy+Computing+Sirer+de+Bruijn+Reynolds+Shieh+Walsh+Williams+Schneider,http://research.google.com/pubs/pub37512.html
Acoustic Modeling for Speech Synthesis: from HMM to RNN,IEEE ASRU Scottsdale Arizona U.S.A. (2015),2015,Heiga Zen,@misc{44630 title = {Acoustic Modeling for Speech Synthesis: from HMM to RNN} author = {Heiga Zen} year = 2015 note = {Invited talk given at ASRU} },Statistical parametric speech synthesis (SPSS) combines an acoustic model and a vocoder to render speech given a text. Typically decision tree-clustered context-dependent hidden Markov models (HMMs) are employed as the acoustic model which represent a relationship between linguistic and acoustic features. There have been attempts to replace the HMMs by alternative acoustic models which provide trajectory and context modeling. Recently artificial neural network-based acoustic models such as deep neural networks mixture density networks and recurrent neural networks (RNNs) showed significant improvements over the HMM-based one. This talk reviews the progress of acoustic modeling in SPSS from the HMM to the RNN.,http://research.google.com/pubs/archive/44630.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Acoustic+Modeling+for+Speech+Synthesis:+from+HMM+to+RNN+Zen,http://research.google.com/pubs/pub44630.html
How opinions are received by online communities: A case study on Amazon.com helpfulness votes,Proceedings of the 18th International Conference on World Wide Web WWW 2009 Madrid Spain April 20-24 2009 pp. 141-150,2009,Cristian Danescu-Niculescu-Mizil Gueorgi Kossinets Jon Kleinberg Lillian Lee,@inproceedings{35388 title = {How opinions are received by online communities: A case study on Amazon.com helpfulness votes} author = {Cristian Danescu-Niculescu-Mizil and Gueorgi Kossinets and Jon Kleinberg and Lillian Lee} year = 2009 booktitle = {Proceedings of the 18th International Conference on World Wide Web WWW 2009 Madrid Spain April 20-24 2009} pages = {141--150} },There are many on-line settings in which users publicly express opinions. A number of these offer mechanisms for other users to evaluate these opinions; a canonical example is Amazon.com where reviews come with annotations like ``26 of 32 people found the following review helpful.'' Opinion evaluation appears in many off-line settings as well including market research and political campaigns. Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself: rather than asking ``What did Y think of X?'' we are asking ``What did Z think of Y's opinion of X?'' Here we develop a framework for analyzing and modeling opinion evaluation using a large-scale collection of Amazon book reviews as a dataset. We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. As part of our approach we develop novel methods that take advantage of the phenomenon of review ``plagiarism'' to control for the effects of text in opinion evaluation and we provide a simple and natural mathematical model consistent with our findings. Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from different countries.,http://research.google.com/pubs/archive/35388.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+opinions+are+received+by+online+communities:+A+case+study+on+Amazon.com+helpfulness+votes+Danescu-Niculescu-Mizil+Kossinets+Kleinberg+Lee,http://research.google.com/pubs/pub35388.html
Reducing Web Latency: the Virtue of Gentle Aggression,Proceedings of the ACM Conference of the Special Interest Group on Data Communication (SIGCOMM '13) ACM (2013),2013,Tobias Flach Nandita Dukkipati Andreas Terzis Barath Raghavan Neal Cardwell Yuchung Cheng Ankur Jain Shuai Hao Ethan Katz-Bassett Ramesh Govindan,@inproceedings{41217 title = {Reducing Web Latency: the Virtue of Gentle Aggression} author = {Tobias Flach and Nandita Dukkipati and Andreas Terzis and Barath Raghavan and Neal Cardwell and Yuchung Cheng and Ankur Jain and Shuai Hao and Ethan Katz-Bassett and Ramesh Govindan} year = 2013 URL = {http://conferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p159.pdf} booktitle = {Proceedings of the ACM Conference of the Special Interest Group on Data Communication (SIGCOMM '13)} },To serve users quickly Web service providers build infrastructure closer to clients and use multi-stage transport connections. Although these changes reduce client-perceived round-trip times TCP's current mechanisms fundamentally limit latency improvements. We performed a measurement study of a large Web service provider and found that while connections with no loss complete close to the ideal latency of one round-trip time TCP's timeout-driven recovery causes transfers with loss to take five times longer on average. In this paper we present the design of novel loss recovery mechanisms for TCP that judiciously use redundant transmissions to minimize timeout-driven recovery. Proactive Reactive and Corrective are three qualitatively different easily-deployable mechanisms that (1) proactively recover from losses (2) recover from them as quickly as possible and (3) reconstruct packets to mask loss. Crucially the mechanisms are compatible both with middleboxes and with TCP's existing congestion control and loss recovery. Our large-scale experiments on Google's production network that serves billions of flows demonstrate a 23% decrease in the mean and 47% in 99th percentile latency over today's TCP.,http://research.google.com/pubs/archive/41217.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+Web+Latency:+the+Virtue+of+Gentle+Aggression+Flach+Dukkipati+Terzis+Raghavan+Cardwell+Cheng+Jain+Hao+Katz-Bassett+Govindan,http://research.google.com/pubs/pub41217.html
Condor: Better Topologies through Declarative Design,Sigcomm '15 Google Inc (2015),2015,Brandon Schlinker Radhika Niranjan Mysore Sean Smith Jeffrey C. Mogul Amin Vahdat Minlan Yu Ethan Katz-Bassett Michael Rubin,@inproceedings{43839 title = {Condor: Better Topologies through Declarative Design} author = {Brandon Schlinker and Radhika Niranjan Mysore and Sean Smith and Jeffrey C. Mogul and Amin Vahdat and Minlan Yu and Ethan Katz-Bassett and Michael Rubin} year = 2015 booktitle = {Sigcomm '15} },The design space for large multipath datacenter networks is large and complex and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective reliable and maintainable networks and typically cannot explore much of the design space. We present Condor our approach to enabling a rapid efficient design cycle. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL) rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies which can be analyzed against multiple criteria. We show that TDL supports concise descriptions of topologies such as fat-trees BCube and DCell; that we can generate known and novel variants of fat-trees with simple changes to a TDL file; and that we can synthesize large topologies in tens of seconds. We also show that Condor supports the daunting task of designing multi-phase network expansions that can be carried out on live networks.,http://research.google.com/pubs/archive/43839.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Condor:+Better+Topologies+through+Declarative+Design+Schlinker+Niranjan+Mysore+Smith+Mogul+Vahdat+Yu+Katz-Bassett+Rubin,http://research.google.com/pubs/pub43839.html
Comparison of Clustering Approaches for Summarizing Large Populations of Images,Proceedings ICME VCIDS IEEE Singapore (2010),2010,Yushi Jing Michele Covell Henry A. Rowley,@inproceedings{36929 title = {Comparison of Clustering Approaches for Summarizing Large Populations of Images} author = {Yushi Jing and Michele Covell and Henry A. Rowley} year = 2010 booktitle = {Proceedings ICME VCIDS} address = {Singapore} },This paper compares the efficacy and efficiency of different clustering approaches for selecting a set of exemplar images to present in the context of a semantic concept. We evaluate these approaches using 900 diverse queries each associated with 1000 web images and comparing the exemplars chosen by clustering to the top 20 images for that search term. Our results suggest that Affinity Propagation is effective in selecting exemplars that match the top search images but at high computational cost. We improve on these early results using a simple distribution-based selection ﬁlter on incomplete clustering results. This improvement allows us to use more computationally efficient approaches to clustering such as Hierarchical Agglomerative Clustering (HAC) and Partitioning Around Medoids (PAM) while still reaching the same (or better) quality of results as were given by Affinity Propagation in the original study. The computational savings is significant since these alternatives are 7-27 times faster than Affinity Propagation.,http://research.google.com/pubs/archive/36929.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Comparison+of+Clustering+Approaches+for+Summarizing+Large+Populations+of+Images+Jing+Covell+Rowley,http://research.google.com/pubs/pub36929.html
Embedded Voxel Colouring with Adaptive Threshold Selection Using Globally Minimal Surfaces,IJCV vol. 99 (2012) pp. 215-231,2012,Carlos Leung Ben Appleton Mitchell Buckley Changming Sun,@article{40353 title = {Embedded Voxel Colouring with Adaptive Threshold Selection Using Globally Minimal Surfaces} author = {Carlos Leung and Ben Appleton and Mitchell Buckley and Changming Sun} year = 2012 URL = {http://rd.springer.com/article/10.1007/s11263-012-0525-8} journal = {IJCV} pages = {215--231} volume = {99} },Image-based 3D reconstruction remains a competitive field of research as state-of-the-art algorithms continue to improve. This paper presents a voxel-based algorithm that adapts the earliest space-carving methods and utilises a minimal surface technique to obtain a cleaner result. Embedded Voxel Colouring is built in two stages: (a) progressive voxel carving is used to build a volume of embedded surfaces and (b) the volume is processed to obtain a surface that maximises photo-consistency data in the volume. This algorithm combines the strengths of classical carving techniques with those of minimal surface approaches. We require only a single pass through the voxel volume this significantly reduces computation time and is the key to the speed of our approach. We also specify three requirements for volumetric reconstruction: monotonic carving order causality of carving and water-tightness. Experimental results are presented that demonstrate the strengths of this approach.,http://rd.springer.com/article/10.1007/s11263-012-0525-8,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Embedded+Voxel+Colouring+with+Adaptive+Threshold+Selection+Using+Globally+Minimal+Surfaces+Leung+Appleton+Buckley+Sun,http://research.google.com/pubs/pub40353.html
Open Problem: Better Bounds for Online Logistic Regression,COLT/ICML Joint Open Problem Session JMLR: Workshop and Conference Proceedings (2012),2012,H. Brendan McMahan Matthew Streeter,@inproceedings{38137 title = {Open Problem: Better Bounds for Online Logistic Regression} author = {H. Brendan McMahan and Matthew Streeter} year = 2012 booktitle = {COLT/ICML Joint Open Problem Session JMLR: Workshop and Conference Proceedings} },Known algorithms applied to online logistic regression on a feasible set of L2 diameter D achieve regret bounds like O(e D log T) in one dimension but we show a bound of O(sqrt(D) + log T) is possible in a binary 1-dimensional problem. Thus we pose the following question: Is it possible to achieve a regret bound for online logistic regression that is O(poly(D)log(T))? Even if this is not possible in general it would be interesting to have a bound that reduces to our bound in the one-dimensional case.,http://research.google.com/pubs/archive/38137.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Open+Problem:+Better+Bounds+for+Online+Logistic+Regression+McMahan+Streeter,http://research.google.com/pubs/pub38137.html
Simultaneous Technology Mapping and Placement for Delay Minimization,IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS vol. 30 (2011) pp. 416-426,2011,Yifang Liu Rupesh S. Shelar Jiang Hu,@article{42859 title = {Simultaneous Technology Mapping and Placement for Delay Minimization} author = {Yifang Liu and Rupesh S. Shelar and Jiang Hu} year = 2011 URL = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715609} journal = {IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS} pages = {416--426} volume = {30} },Abstract—Technology mapping and placement have a significant impact on delays in standard cell-based very large scale integrated circuits. Traditionally these steps are applied separately to optimize the delays possibly since efficient algorithms that allow the simultaneous exploration of the mapping and placement solution spaces are unknown. In this paper we present an exact polynomial time algorithm for delay-optimal placement of a tree and extend the same to simultaneous technology mapping and placement for the optimal delay in the tree. We extend the algorithm by employing Lagrangian relaxation technique which assesses the timing criticality of paths beyond a tree to optimize the delays in directed acyclic graphs. Experimental results on benchmark circuits in a 70 nm technology show that our algorithms improve timing significantly with remarkably less runtimes compared to a competitive approach of iterative conventional timing-driven mapping and multilevel placement. Index Terms—algorithms directed acyclic graph physical synthesis placement technology mapping tree.,http://research.google.com/pubs/archive/42859.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Simultaneous+Technology+Mapping+and+Placement+for+Delay+Minimization+Liu+Shelar+Hu,http://research.google.com/pubs/pub42859.html
Half Transductive Ranking,Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010),2010,Bing Bai Jason Weston David Grangier Ronan Collobert Corinna Cortes Mehryar Mohri,@inproceedings{36470 title = {Half Transductive Ranking} author = {Bing Bai and Jason Weston and David Grangier and Ronan Collobert and Corinna Cortes and Mehryar Mohri} year = 2010 URL = {http://www.cs.nyu.edu/~mohri/pub/half.pdf} booktitle = {Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)} },We study the standard retrieval task of ranking a fixed set of items given a previously unseen query and pose it as the half transductive ranking problem. The task is transductive as the set of items is fixed. Transductive representations (where the vector representation of each example is learned) allow the generation of highly nonlinear embeddings that capture object relationships without relying on a specific choice of features and require only relatively simple optimization. Unfortunately they have no direct outof- sample extension. Inductive approaches on the other hand allow for the representation of unknown queries. We describe algorithms for this setting which have the advantages of both transductive and inductive approaches and can be applied in unsupervised (either reconstruction-based or graph-based) and supervised ranking setups. We show empirically that our methods give strong performance on all three tasks.,http://research.google.com/pubs/archive/36470.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Half+Transductive+Ranking+Bai+Weston+Grangier+Collobert+Cortes+Mohri,http://research.google.com/pubs/pub36470.html
SPI-SNOOPER: a hardware-software approach for transparent network monitoring in wireless sensor networks,Proceedings of the eighth IEEE/ACM/IFIP international conference on Hardware/software codesign and system synthesis ACM New York NY USA (2012) pp. 53-62,2012,Mohammad Sajjad Hossain Woo Suk Lee Vijay Raghunathan,@inproceedings{41526 title = {SPI-SNOOPER: a hardware-software approach for transparent network monitoring in wireless sensor networks} author = {Mohammad Sajjad Hossain and Woo Suk Lee and Vijay Raghunathan} year = 2012 URL = {http://dl.acm.org/citation.cfm?id=2380460} booktitle = {Proceedings of the eighth IEEE/ACM/IFIP international conference on Hardware/software codesign and system synthesis} pages = {53-62} address = {New York NY USA} },The lack of post-deployment visibility into system operation is one of the major challenges in ensuring reliable operation of remotely deployed embedded systems such as wireless sensor nodes. Over the years many software-based solutions (in the form of debugging tools and protocols) have been proposed for in-situ system monitoring. However all of them share the trait that the monitoring functionality is implemented as software executing on the same embedded processor that the main application executes on. This is a poor design choice from a reliability perspective. This paper makes the case for a joint hardware-software solution to this problem and advocates the use of a dedicated reliability co-processor that is tasked with monitoring the operation of the embedded system. As an embodiment of this design principle this paper presents Spi-Snooper a co-processor augmented hardware platform specifically designed for network monitoring. Spi-Snooper is completely cross-compatible with the Telos wireless sensor nodes from an operational standpoint and is based on a novel hardware architecture that enables transparent snooping of the communication bus between the main processor and the radio of the wireless embedded system. The accompanying software architecture provides a powerful tool for monitoring logging and even controlling all the communication that takes place between the main processor and the radio. We present a rigorous evaluation of our prototype and demonstrate its utility using a variety of usage scenarios.,http://research.google.com/pubs/archive/41526.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SPI-SNOOPER:+a+hardware-software+approach+for+transparent+network+monitoring+in+wireless+sensor+networks+Hossain+Lee+Raghunathan,http://research.google.com/pubs/pub41526.html
The Dataflow Model: A Practical Approach to Balancing Correctness Latency and Cost in Massive-Scale Unbounded Out-of-Order Data Processing,Proceedings of the VLDB Endowment vol. 8 (2015) pp. 1792-1803,2015,Tyler Akidau Robert Bradshaw Craig Chambers Slava Chernyak Rafael J. Fernández-Moctezuma Reuven Lax Sam McVeety Daniel Mills Frances Perry Eric Schmidt Sam Whittle,@article{43864 title = {The Dataflow Model: A Practical Approach to Balancing Correctness Latency and Cost in Massive-Scale Unbounded Out-of-Order Data Processing} author = {Tyler Akidau and Robert Bradshaw and Craig Chambers and Slava Chernyak and Rafael J. Fernández-Moctezuma and Reuven Lax and Sam McVeety and Daniel Mills and Frances Perry and Eric Schmidt and Sam Whittle} year = 2015 journal = {Proceedings of the VLDB Endowment} pages = {1792-1803} volume = {8} },Unbounded unordered global-scale datasets are increasingly common in day-to-day business (e.g. Web logs mobile usage statistics and sensor networks). At the same time consumers of these datasets have evolved sophisticated requirements such as event-time ordering and windowing by features of the data themselves in addition to an insatiable hunger for faster answers. Meanwhile practicality dictates that one can never fully optimize along all dimensions of correctness latency and cost for these types of input. As a result data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions often resulting in disparate implementations and systems. We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete and instead live and breathe under the assumption that we will never know if or when we have seen all of our data only that new data will arrive old data may be retracted and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness latency and cost. In this paper we present one such approach the Dataflow Model along with a detailed examination of the semantics it enables an overview of the core principles that guided its design and a validation of the model itself via the real-world experiences that led to its development.,http://research.google.com/pubs/archive/43864.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Dataflow+Model:+A+Practical+Approach+to+Balancing+Correctness+Latency+and+Cost+in+Massive-Scale+Unbounded+Out-of-Order+Data+Processing+Akidau+Bradshaw+Chambers+Chernyak+Fern%C3%A1ndez-Moctezuma+Lax+McVeety+Mills+Perry+Schmidt+Whittle,http://research.google.com/pubs/pub43864.html
Focus on the Long-Term: It's better for Users and Business,Proceedings 21st Conference on Knowledge Discovery and Data Mining ACM Sydney Australia (2015),2015,Henning Hohnhold Deirdre O'Brien Diane Tang,@inproceedings{43887 title = {Focus on the Long-Term: It's better for Users and Business} author = {Henning Hohnhold and Deirdre O'Brien and Diane Tang} year = 2015 URL = {http://dl.acm.org/citation.cfm?doid=2783258.2788583} booktitle = {Proceedings 21st Conference on Knowledge Discovery and Data Mining} address = {Sydney Australia} },Over the past 10+ years online companies large and small have adopted widespread A/B testing as a robust data-based method for evaluating potential product improvements. In online experimentation it is straightforward to measure the short-term effect i.e. the impact observed during the experiment. However the short-term effect is not always predictive of the long-term effect i.e. the final impact once the product has fully launched and users have changed their behavior in response. Thus the challenge is how to determine the long-term user impact while still being able to make decisions in a timely manner. We tackle that challenge in this paper by first developing experiment methodology for quantifying long-term user learning. We then apply this methodology to ads shown on Google search more specifically to determine and quantify the drivers of ads blindness and sightedness the phenomenon of users changing their inherent propensity to click on or interact with ads. We use these results to create a model that uses metrics measurable in the short-term to predict the long-term. We learn that user satisfaction is paramount: ads blindness and sightedness are driven by the quality of previously viewed or clicked ads as measured by both ad relevance and landing page quality. Focusing on user satisfaction both ensures happier users but also makes business sense as our results illustrate. We describe two major applications of our findings: a conceptual change to our search ads auction that further increased the importance of ads quality and a 50% reduction of the ad load on Google’s mobile search interface. The results presented in this paper are generalizable in two major ways. First the methodology may be used to quantify user learning effects and to evaluate online experiments in contexts other than ads. Second the ads blindness/sightedness results indicate that a focus on user satisfaction could help to reduce the ad load on the internet at large with long-term neutral or even positive business impact.,http://research.google.com/pubs/archive/43887.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Focus+on+the+Long-Term:+It's+better+for+Users+and+Business+Hohnhold+O'Brien+Tang,http://research.google.com/pubs/pub43887.html
Bayesian Sampling using Stochastic Gradient Thermostats,Advances in Neural Information Processing Systems (2014) pp. 3203-3211,2014,Nan Ding Youhan Fang Ryan Babbush Changyou Chen Robert Skeel Hartmut Neven,@inproceedings{43934 title = {Bayesian Sampling using Stochastic Gradient Thermostats} author = {Nan Ding and Youhan Fang and Ryan Babbush and Changyou Chen and Robert Skeel and Hartmut Neven} year = 2014 URL = {http://papers.nips.cc/paper/5592-bayesian-sampling-using-stochastic-gradient-thermostats} booktitle = {Advances in Neural Information Processing Systems} pages = {3203-3211} },Dynamics-based sampling methods such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD) are commonly used to sample target distributions. Recently such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.,http://research.google.com/pubs/archive/43934.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bayesian+Sampling+using+Stochastic+Gradient+Thermostats+Ding+Fang+Babbush+Chen+Skeel+Neven,http://research.google.com/pubs/pub43934.html
Fast Accurate Detection of 100000 Object Classes on a Single Machine: Technical Supplement,Proceedings of IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society Washington DC USA (2013),2013,Thomas Dean Mark Ruzon Mark Segal Jonathon Shlens Sudheendra Vijayanarasimhan Jay Yagnik,@inproceedings{41104 title = {Fast Accurate Detection of 100000 Object Classes on a Single Machine: Technical Supplement} author = {Thomas Dean and Mark Ruzon and Mark Segal and Jonathon Shlens and Sudheendra Vijayanarasimhan and Jay Yagnik} year = 2013 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition} address = {Washington DC USA} },In the companion paper published in CVPR 2013 we presented a method that can directly use deformable part models (DPMs) trained as in [Felzenszwalb et al CVPR 2008]. After training HOG based part filters are hashed and during inference counts of hashing collisions summed over all hash bands serve as a proxy for part-filter / sliding-window dot products i.e. filter responses. These counts are an approximation and so we take the original HOG-based filters for the top hash counts and calculate the exact dot products for scoring. It is possible to train DPM models not on HOG data but on a hashed WTA [Yagnik et al ICCV 2011] version of this data. The resulting part filters are sparse real-valued vectors the size of WTA vectors computed from sliding windows. Given the WTA hash of a window we exactly recover dot products of the top responses using an extension of locality-sensitive hashing. In this supplement we sketch a method for training such WTA-based models.,http://research.google.com/pubs/archive/41104.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Accurate+Detection+of+100000+Object+Classes+on+a+Single+Machine:+Technical+Supplement+Dean+Ruzon+Segal+Shlens+Vijayanarasimhan+Yagnik,http://research.google.com/pubs/pub41104.html
C# in Depth,Manning 20 Baldwin Road PO Box 261 Shelter Island NY 11964 (2010) pp. 584,2010,Jon Skeet,@book{37264 title = {C# in Depth} author = {Jon Skeet} year = 2010 booktitle = {C# in Depth} pages = {584} address = {20 Baldwin Road PO Box 261 Shelter Island NY 11964} },"C# has changed significantly since it was first introduced. With the many upgraded features C# is more expressive than ever. However an in depth understanding is required to get the most out of the language. C# in Depth Second Edition is a thoroughly revised up-to-date book that covers the new features of C# 4 as well as Code Contracts. In it you’ll see the subtleties of C# programming in action learning how to work with high-value features that you’ll be glad to have in your toolkit. The book helps readers avoid hidden pitfalls of C# programming by understanding ""behind the scenes"" issues.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=C%23+in+Depth+Skeet,http://research.google.com/pubs/pub37264.html
Online Matching with Stochastic Rewards,Symposium on Foundations of Computer Science (FOCS) IEEE (2012),2012,Aranyak Mehta Debmalya Panigrahi,@inproceedings{40363 title = {Online Matching with Stochastic Rewards} author = {Aranyak Mehta and Debmalya Panigrahi} year = 2012 booktitle = {Symposium on Foundations of Computer Science (FOCS)} },The online matching problem has received significant attention in recent years because of its connections to allocation problems in Internet advertising crowd-sourcing etc. In these real-world applications the typical goal is not to maximize the number of allocations rather it is to maximize the number of successful allocations where success of an allocation is governed by a stochastic process which follows the allocation. To address such applications we propose and study the online matching problem with stochastic rewards (called the Online Stochastic Matching problem) in this paper. Our problem also has close connections to the existing literature on stochastic packing problems in fact our work initiates the study of online stochastic packing problems. We give a deterministic algorithm for the Online Stochastic Matching problem whose competitive ratio converges to (approximately) 0.567 for uniform and vanishing probabilities. We also give a randomized algorithm which outperforms the deterministic algorithm for higher probabilities. Finally we complement our algorithms by giving an upper bound on the competitive ratio of any algorithm for this problem. This result shows that the best achievable competitive ratio for the Online Stochastic Matching problem is provably worse than that for the (non-stochastic) online matching problem.,http://research.google.com/pubs/archive/40363.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Matching+with+Stochastic+Rewards+Mehta+Panigrahi,http://research.google.com/pubs/pub40363.html
App Isolation: Get the Security of Multiple Browsers with Just One,18th ACM Conference on Computer and Communications Security ACM (2011),2011,Eric Y. Chen Jason Bau Charles Reis Adam Barth Collin Jackson,@inproceedings{37198 title = {App Isolation: Get the Security of Multiple Browsers with Just One} author = {Eric Y. Chen and Jason Bau and Charles Reis and Adam Barth and Collin Jackson} year = 2011 booktitle = {18th ACM Conference on Computer and Communications Security} },Many browser-based attacks can be prevented by using separate browsers for separate web sites. However most users access the web with only one browser. We explain the security benefits that using multiple browsers provides in terms of two concepts: entry-point restriction and state isolation. We combine these concepts into a general app isolation mechanism that can provide the same security benefits in a single browser. While not appropriate for all types of web sites many sites with high-value user data can opt in to app isolation to gain defenses against a wide variety of browser-based attacks. We implement app isolation in the Chromium browser and verify its security properties using finite-state model checking. We also measure the performance overhead of app isolation and conduct a large-scale study to evaluate its adoption complexity for various types of sites demonstrating how the app isolation mechanisms are suitable for protecting a number of high-value Web applications such as online banking.,http://research.google.com/pubs/archive/37198.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=App+Isolation:+Get+the+Security+of+Multiple+Browsers+with+Just+One+Chen+Bau+Reis+Barth+Jackson,http://research.google.com/pubs/pub37198.html
Delay Learning and Polychronization for Reservoir Computing,Neurocomputing vol. 71 (2008) pp. 1143-1158,2008,Hélène Paugam-Moisy Régis Martinez Samy Bengio,@article{33245 title = {Delay Learning and Polychronization for Reservoir Computing} author = {Hélène Paugam-Moisy and Régis Martinez and Samy Bengio} year = 2008 journal = {Neurocomputing} pages = {1143--1158} volume = {71} },We propose a multi-scale learning rule for spiking neuron networks in the vein of the recently emerging field of reservoir computing. The reservoir is a network model of spiking neurons with random topology and driven by STDP (Spike-Time-Dependent Plasticity) a temporal Hebbian unsupervised learning mode biologically observed. The model is further driven by a supervised learning algorithm based on a margin criterion that effects the synaptic delays linking the network to the readout neurons with classification as a goal task. The network processing and the resulting performance can be explained by the concept of polychronization proposed by Izhikevich (2006 Neural Computation 181) on physiological bases. The model emphasizes the computational capabilities of this concept.,http://research.google.com/pubs/archive/33245.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Delay+Learning+and+Polychronization+for+Reservoir+Computing+Paugam-Moisy+Martinez+Bengio,http://research.google.com/pubs/pub33245.html
Evaluating TV Ad Campaigns Using Set-Top Box Data,Re:Think 2010,2010,Sundar Dorai-Raj Yannet Interian Dan Zigmond,@inproceedings{36495 title = {Evaluating TV Ad Campaigns Using Set-Top Box Data} author = {Sundar Dorai-Raj and Yannet Interian and Dan Zigmond} year = 2010 booktitle = {Re:Think 2010} },Google has developed new metrics based on set-top box data for predicting the future audience retention of TV ads. This paper examines how to use these metrics to judge the effectiveness of TV ad campaigns. More specifically we analyze how these metrics can inform future campaign targeting and placement goals.,http://research.google.com/pubs/archive/36495.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evaluating+TV+Ad+Campaigns+Using+Set-Top+Box+Data+Dorai-Raj+Interian+Zigmond,http://research.google.com/pubs/pub36495.html
Learning Part-based Templates from Large Collections of 3D Shapes,ACM Transactions on Graphics (TOG) - SIGGRAPH 2013 Conference Proceedings vol. 32 no. 4 (2013) 70:1-70:12,2013,Vladimir Kim Wilmot Li Niloy Mitra Siddhartha Chaudhuri Stephen DiVerdi Thomas Funkhouser,@article{41463 title = {Learning Part-based Templates from Large Collections of 3D Shapes} author = {Vladimir Kim and Wilmot Li and Niloy Mitra and Siddhartha Chaudhuri and Stephen DiVerdi and Thomas Funkhouser} year = 2013 journal = {ACM Transactions on Graphics (TOG) - SIGGRAPH 2013 Conference Proceedings} pages = {70:1--70:12} volume = {32 no. 4} },As large repositories of 3D shape collections continue to grow understanding the data especially encoding the inter-model similarity and their variations is of central importance. For example many data-driven approaches now rely on access to semantic segmentation information accurate inter-model point-to-point correspondence and deformation models that characterize the model collections. Existing approaches however are either supervised requiring manual labeling; or employ super-linear matching algorithms and thus are unsuited for analyzing large collections spanning many thousands of models. We propose an automatic algorithm that starts with an initial template model and then jointly optimizes for part segmentation point-to-point surface correspondence and a compact deformation model to best explain the input model collection. As output the algorithm produces a set of probabilistic part-based templates that groups the original models into clusters of models capturing their styles and variations. We evaluate our algorithm on several standard datasets and demonstrate its scalability by analyzing much larger collections of up to thousands of shapes.,http://research.google.com/pubs/archive/41463.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Part-based+Templates+from+Large+Collections+of+3D+Shapes+Kim+Li+Mitra+Chaudhuri+DiVerdi+Funkhouser,http://research.google.com/pubs/pub41463.html
The Complete April Fools RFCs,Peer-to-Peer Communications LLC PO Box 6970 Charlottesville VA US 22906-6970 (2007),2007,Thomas A. Limoncelli Peter H. Salus,@book{32806 title = {The Complete April Fools RFCs} author = {Thomas A. Limoncelli and Peter H. Salus} year = 2007 URL = {http://www.rfc-humor.com/} address = {PO Box 6970 Charlottesville VA US 22906-6970} },"Collection of the ""April Fools"" RFCs published by IETF from 1969 through 2005. Commentary and forewards by various Internet innovators.",http://www.rfc-humor.com/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Complete+April+Fools+RFCs+Limoncelli+Salus,http://research.google.com/pubs/pub32806.html
Adaptive Dynamic of Realistic Small World Networks,2009 European Conference on Complex Systems (to appear),2009,Olof Mogren Oskar Sandberg Vilhelm Verendel Devdatt Dubhashi,@inproceedings{35581 title = {Adaptive Dynamic of Realistic Small World Networks} author = {Olof Mogren and Oskar Sandberg and Vilhelm Verendel and Devdatt Dubhashi} year = 2009 booktitle = {2009 European Conference on Complex Systems} },Continuing in the steps of Jon Kleinberg’s and others celebrated work on decentralized search we conduct an experimental analysis of destination sampling a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data even when vertices are unevenly and non-homogeneously distributed. We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adaptive+Dynamic+of+Realistic+Small+World+Networks+Mogren+Sandberg+Verendel+Dubhashi,http://research.google.com/pubs/pub35581.html
Metrics and Design Tool for Building and Evaluating Probability-Based Online Panels,Social Science Computer Review vol. 34 (2016) pp. 26-40,2016,Charles DiSogra Mario Callegaro,@article{43804 title = {Metrics and Design Tool for Building and Evaluating Probability-Based Online Panels} author = {Charles DiSogra and Mario Callegaro} year = 2016 URL = {http://ssc.sagepub.com/content/34/1/26.abstract} journal = {Social Science Computer Review} pages = {26-40} volume = {34} },Probability-based online panels are beginning to replace traditional survey modes for existing established surveys in Europe and the United States. In light of this current standards for panel response rate calculations are herein reviewed. To populate these panels cost-effectively more diverse recruitment methods such as mail telephone and recruitment modules added to existing surveys are being used either alone or in combinations. This results in panel member cohorts from different modes complicating panel response rate calculations. Also as a panel ages with inevitable attrition multiple cohorts result from panel refreshment and growth strategies. Formulas are presented to illustrate how to handle multiple cohorts for panel metrics. Additionally drawing on relevant metrics used for a panel response rate we further demonstrate a computational tool to assist planners in building a probability-based panel. This provides a means to estimate the recruitment effort required to build a panel of a predetermined size.,http://ssc.sagepub.com/content/34/1/26.abstract,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Metrics+and+Design+Tool+for+Building+and+Evaluating+Probability-Based+Online+Panels+DiSogra+Callegaro,http://research.google.com/pubs/pub43804.html
DRAM Errors in the Wild: A Large-Scale Field Study,SIGMETRICS (2009),2009,Bianca Schroeder Eduardo Pinheiro Wolf-Dietrich Weber,@inproceedings{35162 title = {DRAM Errors in the Wild: A Large-Scale Field Study} author = {Bianca Schroeder and Eduardo Pinheiro and Wolf-Dietrich Weber} year = 2009 booktitle = {SIGMETRICS} },Errors in dynamic random access memory (DRAM) are a common form of hardware failure in modern compute clusters. Failures are costly both in terms of hardware replacement costs and service disruption. While a large body of work exists on DRAM in laboratory conditions little has been reported on real DRAM failures in large production clusters. In this paper we analyze measurements of memory errors in a large fleet of commodity servers over a period of 2.5 years. The collected data covers multiple vendors DRAM capacities and technologies and comprises many millions of DIMM days. The goal of this paper is to answer questions such as the following: How common are memory errors in practice? What are their statistical properties? How are they affected by external factors such as temperature and utilization and by chip-specific factors such as chip density memory technology and DIMM age? We find that DRAM error behavior in the field differs in many key aspects from commonly held assumptions. For example we observe DRAM error rates that are orders of magnitude higher than previously reported with 25000 to 70000 errors per billion device hours per Mbit and more than 8\% of DIMMs affected by errors per year. We provide strong evidence that memory errors are dominated by hard errors rather than soft errors which previous work suspects to be the dominant error mode. We find that temperature known to strongly impact DIMM error rates in lab conditions has a surprisingly small effect on error behavior in the field when taking all other factors into account. Finally unlike commonly feared we don't observe any indication that newer generations of DIMMs have worse error behavior.,http://research.google.com/pubs/archive/35162.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=DRAM+Errors+in+the+Wild:+A+Large-Scale+Field+Study+Schroeder+Pinheiro+Weber,http://research.google.com/pubs/pub35162.html
Rootkits in your web application,28C3: Chaos Communications Congress Berlin Germany (2011),2011,Artur Janc,@inproceedings{37661 title = {Rootkits in your web application} author = {Artur Janc} year = 2011 booktitle = {28C3: Chaos Communications Congress} address = {Berlin Germany} },"In this work I discuss practical approaches for exploiting cross-site scripting (XSS) and other client-side script injection vulnerabilities and introduce novel techniques for maintaining and escalating access within the victim's browser. In particular I introduce the concept of resident XSS where attacker-supplied code is running in the context of an affected user's main application window and describe its consequences. I also draw analogies between such persistent Web threats and traditional rootkits including similarities in the areas of embedding malicious code maintaining access and the difficulty of detecting and removing attacker-supplied code. Details Despite a few high profile cases of XSS worms the exploitation of script injection vulnerabilities has historically been mostly limited to cookie-stealing and executing simple malicious actions in the context of the affected Web application. However as a consequence of inter-document interactions allowed by the same-origin policy and a combination of other browser mechanisms a single XSS vulnerability can often lead to a long-term compromise all of a user's interactions with an affected webapp in the same browser profile long after the original bug has been fixed. In particular an attacker can maintain access across window/browser closures survive cookie and cache deletions and compromise other user accounts accessed from the same browser. Yet more troubling is the fact that Web application authors currently have no means to detect or mitigate such threats once an attack has taken place. In the talk I provide an overview of script injection attacks against Web clients describe techniques to escalate an XSS into long-term account compromise and introduce the concept of resident XSS where attacker-supplied code is injected into the user's main application window. Additionally I explore the similarities between such Web bugs and traditional rootkits. In particular I: Provide an overview of script injection vulnerabilities and describe real-world considerations for exploiting them against modern Web applications. Introduce the concept of resident XSS where malicious JavaScript is executed in the context of the victim's main application window/tab. Contrary to the traditional methods of exploiting XSS via a hidden frame or malicious link which are opened in a separate usually short-lived window resident XSS gives an attacker full freedom to monitor and alter the user's interaction with the affected application. Describe several techniques to convert various Web bugs into a resident XSS. Such techniques include backdooring client-side persistent storage mechanisms (WebSQL localStorage Flash LSOs) opening poisoned application windows with injected malicious scripts exploiting persistent (self-)XSS and others. Discuss the consequences of resident XSS which usually allow the attacker to get permanent access to an affected user's account and/or obtain the user's application login credentials. On sensitive domains for which users have enabled access to additional browser or plugin features (geolocation camera/microphone) it can enable persistent snooping on the exploited user. In a large number of cases it can also enable full compromise of the user's machine by exploiting the application-user trust relationship (e.g. by requiring the user to install attacker-supplied plugins to use the affected webapp or by hijacking file download links within the vulnerable domain). Analyze the techniques for maintaining access to a once-compromised origin. In addition to backdooring persistent storage APIs this can be achieved by exploiting self-XSS bugs spawning same-origin pop-unders with references to the original window and hiding in frames created by advertising networks on popular websites. In most cases a combination of those techniques suffices to bypass a variety of the most common ""cleanup"" actions taken by users and allows an on-going compromise of the affected origin. Present the difficulties faced by Web application authors when trying to clean up a compromised origin. Short of wiping/re-creating a browser profile there are currently no fully reliable methods to restore a browser's state to a secure configuration once a malicious script has run in the context of an affected domain. A video of the presentation is below.",,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Rootkits+in+your+web+application+Janc,http://research.google.com/pubs/pub37661.html
Defining and detecting quantum speedup,Science vol. 345 (2014) pp. 420-424,2014,Troels F. Rønnow Zhihui Wang Joshua Job Sergio Boixo Sergei V. Isakov David Wecker John M. Martinis Daniel A. Lidar Matthias Troyer,@article{42862 title = {Defining and detecting quantum speedup} author = {Troels F. Rønnow and Zhihui Wang and Joshua Job and Sergio Boixo and Sergei V. Isakov and David Wecker and John M. Martinis and Daniel A. Lidar and Matthias Troyer} year = 2014 URL = {http://www.sciencemag.org/content/345/6195/420} journal = {Science} pages = {420--424} volume = {345} },The development of small-scale quantum devices raises the question of how to fairly assess and detect quantum speedup. Here we show how to define and measure quantum speedup and how to avoid pitfalls that might mask or fake such a speedup. We illustrate our discussion with data from tests run on a D-Wave Two device with up to 503 qubits. By using random spin glass instances as a benchmark we found no evidence of quantum speedup when the entire data set is considered and obtained inconclusive results when comparing subsets of instances on an instance-by-instance basis. Our results do not rule out the possibility of speedup for other classes of problems and illustrate the subtle nature of the quantum speedup question. How to benchmark a quantum computer: Quantum machines offer the possibility of performing certain computations much faster than their classical counterparts. However how to define and measure quantum speedup is a topic of debate. Rønnow et al. describe methods for fairly evaluating the difference in computational power between classical and quantum processors. They define various types of quantum speedup and consider quantum processors that are designed to solve a specific class of problems. Science this issue p. 420,http://www.sciencemag.org/content/345/6195/420,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Defining+and+detecting+quantum+speedup+R%C3%B8nnow+Wang+Job+Boixo+Isakov+Wecker+Martinis+Lidar+Troyer,http://research.google.com/pubs/pub42862.html
Visibility Based Preconditioning for Bundle Adjustment,IEEE Conference on Computer Vision and Pattern Recognition IEEE (2012),2012,Avanish Kushal Sameer Agarwal,@inproceedings{40602 title = {Visibility Based Preconditioning for Bundle Adjustment} author = {Avanish Kushal and Sameer Agarwal} year = 2012 URL = {http://homes.cs.washington.edu/~sagarwal/vbp.pdf} booktitle = {IEEE Conference on Computer Vision and Pattern Recognition} },We present Visibility Based Preconditioning (VBP) a new technique for efﬁciently solving the linear least squares problems that arise in bundle adjustment. Using the camera-point visibility structure of the scene we describe the construction of two preconditioners. These preconditioners when combined with an inexact step LevenbergMarquardt algorithm offer state of the art performance on the BAL data set with 3-5x reduction in execution time over currently available methods while delivering comparable or better solution quality,http://homes.cs.washington.edu/~sagarwal/vbp.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Visibility+Based+Preconditioning+for+Bundle+Adjustment+Kushal+Agarwal,http://research.google.com/pubs/pub40602.html
ShellOS: Enabling fast detection and forensic analysis of code injection attacks,USENIX Security Symposium (2011),2011,Kevin Snow Srinivas Krishnan Fabian Monrose Niels Provos,@inproceedings{38102 title = {ShellOS: Enabling fast detection and forensic analysis of code injection attacks} author = {Kevin Snow and Srinivas Krishnan and Fabian Monrose and Niels Provos} year = 2011 URL = {http://static.usenix.org/events/sec11/tech/full_papers/Snow.pdf} booktitle = {USENIX Security Symposium} },The availability of off-the-shelf exploitation toolkits for compromising hosts coupled with the rapid rate of exploit discovery and disclosure has made exploit or vulnerability-based detection far less effective than it once was. For instance the increasing use of metamorphic and polymorphic techniques to deploy code injection attacks continues to confound signature-based detection techniques. The key to detecting these attacks lies in the ability to discover the presence of the injected code (or shellcode). One promising technique for doing so is to examine data (be that from network streams or buffers of a process) and efﬁciently execute its content to ﬁnd what lurks within. Unfortunately current approaches for achieving this goal are not robust to evasion or scalable primarily because of their reliance on software-based CPU emulators. In this paper we argue that the use of software-based emulation techniques are not necessary and instead propose a new framework that leverages hardware virtualization to better enable the detection of code injection attacks. We also report on our experience using this framework to analyze a corpus of malicious Portable Document Format (PDF) ﬁles and network-based attacks.,http://static.usenix.org/events/sec11/tech/full_papers/Snow.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ShellOS:+Enabling+fast+detection+and+forensic+analysis+of+code+injection+attacks+Snow+Krishnan+Monrose+Provos,http://research.google.com/pubs/pub38102.html
DNA Nanorobotics,Nanorobotics (2013) pp. 355-382,2013,Harish Chandran Nikhil Gopalkrishnan John H. Reif,@inbook{43319 title = {DNA Nanorobotics} author = {Harish Chandran and Nikhil Gopalkrishnan and John H. Reif} year = 2013 booktitle = {Nanorobotics} pages = {355-382} },This chapter overviews the current state of the emerging discipline of DNA nanorobotics that make use of synthetic DNA to self-assemble operational molecular-scale devices. Recently there have been a series of quite astonishing experimental results—which have taken the technology from a state of intriguing possibilities into demonstrated capabilities of quickly increasing scale and complexity. We first state the challenges in molecular robotics and discuss why DNA as a nanoconstruction material is ideally suited to overcome these. We then review the design and demonstration of a wide range of molecular-scale devices; from DNA nanomachines that change conformation in response to their environment to DNA walkers that can be programmed to walk along predefined paths on nanostructures while carrying cargo or performing computations to tweezers that can repeatedly switch states. We conclude by listing major challenges in the field along with some possible future directions.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=DNA+Nanorobotics+Chandran+Gopalkrishnan+Reif,http://research.google.com/pubs/pub43319.html
Privacy Protection in Video Surveillance,Springer (2009),2009,Andrew W. Senior,@book{34729 title = {Privacy Protection in Video Surveillance} author = {Andrew W. Senior} year = 2009 booktitle = {Privacy Protection in Video Surveillance} },An edited book dealing with various aspects of privacy protection in automatic video surveillance systems. Chapters deal with redaction/obscuration cryptography detection integration with RFID performance analysis social issues and acceptance.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Privacy+Protection+in+Video+Surveillance+Senior,http://research.google.com/pubs/pub34729.html
Research trails: getting back where you left off,Proceedings of the 19th international conference on World Wide Web ACM Raleigh North Carolina (2010) pp. 1151-1152,2010,Jiahui Liu Peter Jin Hong Elin Rønby Pedersen,@inproceedings{36583 title = {Research trails: getting back where you left off} author = {Jiahui Liu and Peter Jin Hong and Elin Rønby Pedersen} year = 2010 URL = {http://doi.acm.org/10.1145/1772690.1772849} booktitle = {Proceedings of the 19th international conference on World Wide Web} pages = {1151-1152} address = {Raleigh North Carolina} },In this paper we present a prototype system that helps users in early-stage web research to create and reestablish context across fragmented work process without requiring them to explicitly collect and organize the material they visit. The system clusters a user's web history and shows it as research trails. We present two user interaction models with the research trails. The first interaction model is implemented as a standalone application which presents a hierarchical view of research trails. The second interaction model is integrated with the web browser. It shows the user's research trails as selectable and manipulable visual streams when they open a new tab. Thereby the NewTab page serves as a springboard in the browser for a user resuming an ongoing task.,http://research.google.com/pubs/archive/36583.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Research+trails:+getting+back+where+you+left+off+Liu+Hong+Pedersen,http://research.google.com/pubs/pub36583.html
Extracting Patterns from Location History,ACM SIGSPATIAL GIS 2011 ACM http://www.sigspatial.org/ pp. 397-400,2011,Andrew Kirmse Tushar Udeshi Pablo Bellver Jim Shuma,@inproceedings{37522 title = {Extracting Patterns from Location History} author = {Andrew Kirmse and Tushar Udeshi and Pablo Bellver and Jim Shuma} year = 2011 booktitle = {ACM SIGSPATIAL GIS 2011} pages = {397--400} address = {http://www.sigspatial.org/} },In this paper we describe how a user's location history (recorded by tracking the user's mobile device location with his permission) is used to extract the user's location patterns. We describe how we compute the user's commonly visited places (including home and work) and commute patterns. The analysis is displayed on the Google Latitude history dashboard [7] which is only accessible to the user.,http://research.google.com/pubs/archive/37522.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Extracting+Patterns+from+Location+History+Kirmse+Udeshi+Bellver+Shuma,http://research.google.com/pubs/pub37522.html
Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2015) pp. 4470-4474,2015,Heiga Zen Hasim Sak,@inproceedings{43266 title = {Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis} author = {Heiga Zen and Hasim Sak} year = 2015 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4470--4474} },Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the concerns for applying them to text-to-speech applications is its effect on latency. To address this concern this paper proposes a low-latency streaming speech synthesis architecture using unidirectional LSTM-RNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of output acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch processing.,http://research.google.com/pubs/archive/43266.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unidirectional+Long+Short-Term+Memory+Recurrent+Neural+Network+with+Recurrent+Output+Layer+for+Low-Latency+Speech+Synthesis+Zen+Sak,http://research.google.com/pubs/pub43266.html
Full-Chip Simulations Keys to Success,SNUG Silicon Valley 2015 Proceedings Silicon Valley,2015,Dan Steinberg,@inproceedings{43890 title = {Full-Chip Simulations Keys to Success} author = {Dan Steinberg} year = 2015 URL = {http://www.synopsys.com/news/pubs/snug/2015/silicon-valley/tb04_steinberg_paper.pdf} booktitle = {SNUG Silicon Valley 2015 Proceedings} address = {Silicon Valley} },As designs continue to grow larger and ever more complex full-chip simulations remain a critical component of design verification. These simulations pose a unique set of challenges that require different approaches than those used at the block or sub-chip level. This paper defines the key goals of full-chip simulations and outlines guiding principles to follow when developing a new environment. Special attention is paid to architecting for speed both speed of simulation as well as speed of debug. Lessons learned over the years along with specific recommendations are presented.,http://www.synopsys.com/news/pubs/snug/2015/silicon-valley/tb04_steinberg_paper.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Full-Chip+Simulations+Keys+to+Success+Steinberg,http://research.google.com/pubs/pub43890.html
Crowdsourcing and the Semantic Web: A Research Manifesto,Human Computation vol. 2 (2015),2015,Cristina Sarasua Elena Simperl Natasha Noy Abraham Bernstein Jan Marco Leimeister,@article{43889 title = {Crowdsourcing and the Semantic Web: A Research Manifesto} author = {Cristina Sarasua and Elena Simperl and Natasha Noy and Abraham Bernstein and Jan Marco Leimeister} year = 2015 URL = {http://hcjournal.org/ojs/index.php?journal=jhc&page=article&op=view&path%5B%5D=45&path%5B%5D=51} journal = {Human Computation} volume = {2} },Our goal with this research manifesto is to define a roadmap to guide the evolution of the new research field that is emerging at the intersection between crowdsourcing and the Semantic Web. We analyze the confluence of these two disciplines by exploring their relationship. First we focus on how the application of crowdsourcing techniques can enhance the machine-driven execution of Semantic Web tasks. Second we look at the ways in which machine-processable semantics can benefit the design and management of crowdsourcing projects. As a result we are able to describe a list of successful or promising scenarios for both perspectives identify scientific and technological challenges and compile a set of recommendations to realize these scenarios effectively. This research manifesto is an outcome of the Dagstuhl Seminar 14282: Crowdsourcing and the Semantic Web.,http://research.google.com/pubs/archive/43889.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Crowdsourcing+and+the+Semantic+Web:+A+Research+Manifesto+Sarasua+Simperl+Noy+Bernstein+Leimeister,http://research.google.com/pubs/pub43889.html
SAC078 - SSAC Advisory on Uses of the Shared Global Domain Name Space,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2016) pp. 6,2016,Warren Kumari Geoff Huston Lyman Chapin,@incollection{44841 title = {SAC078 - SSAC Advisory on Uses of the Shared Global Domain Name Space} author = {Warren Kumari and Geoff Huston and Lyman Chapin} year = 2016 booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} pages = {6} },It is widely known that the Domain Name System (DNS) includes both a set of rules for constructing syntactically valid domain names (the “domain name space”) and a protocol for associating domain names with data such as IP addresses (“domain name resolution”). It is less widely understood however that DNS name resolution coexists with other name resolution systems that also use domain names. In many cases these other name resolution systems deliberately use domain names rather than some other naming scheme for compatibility with the widely deployed infrastructure of the DNS. They depend on the ability of DNS name resolution protocols and interface conventions to recognize their domain names but treat them in some special way. ... The SSAC wishes to ensure that the ICANN Board and ICANN community are aware of discussions and ongoing work in multiple venues to more fully define what a namespace is and how to avoid potential side effects including name collisions across the broad set of name resolution systems and expectations for their use.,http://research.google.com/pubs/archive/44841.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC078+-+SSAC+Advisory+on+Uses+of+the+Shared+Global+Domain+Name+Space+Kumari+Huston+Chapin,http://research.google.com/pubs/pub44841.html
Systematic Analysis of Challenge-Driven Improvements in Molecular Prognostic Models for Breast Cancer,Science Translational Medicine vol. 5.181 (2013) 181re1-181re1,2013,Adam Margolin Erhan Bilal Erich Huang Ben Sauerwine Nicole Deflaux Lamia Youseff Tyler Pirtle Craig Citro Joseph L. Hellerstein,@article{41398 title = {Systematic Analysis of Challenge-Driven Improvements in Molecular Prognostic Models for Breast Cancer} author = {Adam Margolin and Erhan Bilal and Erich Huang and Ben Sauerwine and Nicole Deflaux and Lamia Youseff and Tyler Pirtle and Craig Citro and Joseph L. Hellerstein} year = 2013 URL = {http://stm.sciencemag.org/content/5/181/181re1.short} journal = {Science Translational Medicine} pages = {181re1-181re1} volume = {5.181} },Although molecular prognostics in breast cancer are among the most successful examples of translating genomic analysis to clinical applications optimal approaches to breast cancer clinical risk prediction remain controversial. The Sage Bionetworks–DREAM Breast Cancer Prognosis Challenge (BCC) is a crowdsourced research study for breast cancer prognostic modeling using genome-scale data. The BCC provided a community of data analysts with a common platform for data access and blinded evaluation of model accuracy in predicting breast cancer survival on the basis of gene expression data copy number data and clinical covariates. This approach offered the opportunity to assess whether a crowdsourced community Challenge would generate models of breast cancer prognosis commensurate with or exceeding current best-in-class approaches. The BCC comprised multiple rounds of blinded evaluations on held-out portions of data on 1981 patients resulting in more than 1400 models submitted as open source code. Participants then retrained their models on the full data set of 1981 samples and submitted up to five models for validation in a newly generated data set of 184 breast cancer patients. Analysis of the BCC results suggests that the best-performing modeling strategy outperformed previously reported methods in blinded evaluations; model performance was consistent across several independent evaluations; and aggregating community-developed models achieved performance on par with the best-performing individual models.,http://research.google.com/pubs/archive/41398.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Systematic+Analysis+of+Challenge-Driven+Improvements+in+Molecular+Prognostic+Models+for+Breast+Cancer+Margolin+Bilal+Huang+Sauerwine+Deflaux+Youseff+Pirtle+Citro+Hellerstein,http://research.google.com/pubs/pub41398.html
Sui sondaggi politici in Italia,Il Mulino vol. 5 (2014) pp. 827-838,2014,Piergiorgio Corbetta Mario Callegaro,@article{43128 title = {Sui sondaggi politici in Italia} author = {Piergiorgio Corbetta and Mario Callegaro} year = 2014 URL = {http://www.rivistailmulino.it/journal/articlefulltext/index/Article/Journal:RWARTICLE:77827} journal = {Il Mulino} pages = {827-838} volume = {5} },In this discussion piece Piergiorgio Corbetta and Mario Callegaro analyse the results of Italian pre-election polls for the European election of May 2014. The paper is in Italian language.,http://www.rivistailmulino.it/journal/articlefulltext/index/Article/Journal:RWARTICLE:77827,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sui+sondaggi+politici+in+Italia+Corbetta+Callegaro,http://research.google.com/pubs/pub43128.html
Impact Of Ranking Of Organic Search Results On The Incrementality Of Search Ads,Google Inc. (2012),2012,David Chan Deepak Kumar Sheng Ma Jim Koehler,@techreport{37731 title = {Impact Of Ranking Of Organic Search Results On The Incrementality Of Search Ads} author = {David Chan and Deepak Kumar and Sheng Ma and Jim Koehler} year = 2012 institution = {Google Inc.} },In an earlier study we reported that on average 89% of the visits to the advertiser’s site from search ad clicks were incremental. In this research we examine how the ranking of an advertiser’s organic listings on the search results page affects the incrementality of ad clicks expressed through Incremental Ad Clicks (IAC) and as estimated by Search Ads Pause models. A meta-analysis of 390 Search Ads Pause studies highlights the limited opportunity for clicks from organic search results to substitute for ad clicks when the ads are turned off. On average 81% of ad impressions and 66% of ad clicks occur in the absence of an associated organic search result. We find that having an associated organic search result in rank one does not necessarily mean a low IAC. On average 50% of the ad clicks that occur with a top rank organic result are incremental compared to 100% of the ad clicks being incremental in the absence of an associated organic result.,http://research.google.com/pubs/archive/37731.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Impact+Of+Ranking+Of+Organic+Search+Results+On+The+Incrementality+Of+Search+Ads+Chan+Kumar+Ma+Koehler,http://research.google.com/pubs/pub37731.html
The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines Second Edition,Morgan & Claypool Publishers (2013),2013,Luiz André Barroso Jimmy Clidaras Urs Hölzle,@book{41606 title = {The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines Second Edition} author = {Luiz André Barroso and Jimmy Clidaras and Urs Hölzle} year = 2013 URL = {http://dx.doi.org/10.2200/S00516ED2V01Y201306CAC024} },As computation continues to move into the cloud the computing platform of interest no longer resembles a pizza box or a refrigerator but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance something that can only be achieved by a holistic approach to their design and deployment. In other words we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs the main factors influencing their design operation and cost structure and the characteristics of their software base. We hope it will be useful to architects and programmers of today’s WSCs as well as those of future many-core platforms which may one day implement the equivalent of today’s WSCs on a single board. Notes for the Second Edition After nearly four years of substantial academic and industrial developments in warehouse-scale computing we are delighted to present our first major update to this lecture. The increased popularity of public clouds has made WSC software techniques relevant to a larger pool of programmers since our first edition. Therefore we expanded Chapter 2 to reflect our better understanding of WSC software systems and the toolbox of software techniques for WSC programming. In Chapter 3 we added to our coverage of the evolving landscape of wimpy vs. brawny server trade-offs and we now present an overview of WSC interconnects and storage systems that was promised but lacking in the original edition. Thanks largely to the help of our new co-author Google Distinguished Engineer Jimmy Clidaras the material on facility mechanical and power distribution design has been updated and greatly extended (see Chapters 4 and 5). Chapters 6 and 7 have also been revamped significantly. We hope this revised edition continues to meet the needs of educators and professionals in this area.,http://dx.doi.org/10.2200/S00516ED2V01Y201306CAC024,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Datacenter+as+a+Computer:+An+Introduction+to+the+Design+of+Warehouse-Scale+Machines+Second+Edition+Barroso+Clidaras+Holzle,http://research.google.com/pubs/pub41606.html
Intriguing properties of neural networks,International Conference on Learning Representations (2014),2014,Christian Szegedy Wojciech Zaremba Ilya Sutskever Joan Bruna Dumitru Erhan Ian Goodfellow Rob Fergus,@inproceedings{42503 title = {Intriguing properties of neural networks} author = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus} year = 2014 URL = {http://arxiv.org/abs/1312.6199} booktitle = {International Conference on Learning Representations} },Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First we find that there is no distinction between individual high level units and random linear combinations of high level units according to various methods of unit analysis. It suggests that it is the space rather than the individual units that contains of the semantic information in the high layers of neural networks. Second we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation which is found by maximizing the network's prediction error. In addition the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network that was trained on a different subset of the dataset to misclassify the same input.,http://research.google.com/pubs/archive/42503.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Intriguing+properties+of+neural+networks+Szegedy+Zaremba+Sutskever+Bruna+Erhan+Goodfellow+Fergus,http://research.google.com/pubs/pub42503.html
Attitudes Toward Vehicle-Based Sensing and Recording,Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing ACM pp. 1017-1028,2015,Manya Sleeper Sebastian Schnorf Brian Kemler Sunny Consolvo,@inproceedings{43975 title = {Attitudes Toward Vehicle-Based Sensing and Recording} author = {Manya Sleeper and Sebastian Schnorf and Brian Kemler and Sunny Consolvo} year = 2015 booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing} pages = {1017--1028} },Vehicles increasingly include features that rely on hi-tech sensors and recording; however little is known of public attitudes toward such recording. We use two studies an online survey (n=349) and an interview-based study (n=15) to examine perceptions of vehicle-based sensing and recording. We focus on: 1) how vehicle-based recording and sensing may differ from perceptions of current recording; 2) factors that impact comfort with vehicle-based recording for hypothetical drivers versus bystanders; and 3) perceptions of potential privacy-preserving techniques. We find that vehicle-based recording challenges current mental models of recording awareness. Comfort tends to depend on perceived bene- fits which can vary by stakeholder type. Perceived privacy in spaces near cars can also impact comfort and reflect mental models of private spaces as well as the range of potentially sensitive activities people perform in and near cars. Privacy-preserving techniques may increase perceived comfort but may require addressing trust and usability issues.,http://research.google.com/pubs/archive/43975.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Attitudes+Toward+Vehicle-Based+Sensing+and+Recording+Sleeper+Schnorf+Kemler+Consolvo,http://research.google.com/pubs/pub43975.html
Direct Construction of Compact Context-Dependency Transducers From Data,Interspeech 2010 ISCA,2010,David Rybach Michael Riley,@inproceedings{36834 title = {Direct Construction of Compact Context-Dependency Transducers From Data} author = {David Rybach and Michael Riley} year = 2010 booktitle = {Interspeech 2010} },This paper describes a new method for building compact con-text-dependency transducers for finite-state transducer-based ASR decoders. Instead of the conventional phonetic decision-tree growing followed by FST compilation this approach incorporates the phonetic context splitting directly into the transducer construction. The objective function of the split optimization is augmented with a regularization term that measures the number of transducer states introduced by a split. We give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy. This permits using context sizes and features that might otherwise be unmanageable.,http://research.google.com/pubs/archive/36834.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Direct+Construction+of+Compact+Context-Dependency+Transducers+From+Data+Rybach+Riley,http://research.google.com/pubs/pub36834.html
Palette-based Photo Recoloring,Transactions on Graphics (Proceedings of SIGGRAPH) (2015) (to appear),2015,Huiwen Chang Ohad Fried Yiming Liu Stephen DiVerdi Adam Finkelstein,@article{43475 title = {Palette-based Photo Recoloring} author = {Huiwen Chang and Ohad Fried and Yiming Liu and Stephen DiVerdi and Adam Finkelstein} year = 2015 journal = {Transactions on Graphics (Proceedings of SIGGRAPH)} },Color manipulation is a key process in photo enhancement and professional image editing suites incorporate an array of tools to support it. Some of these tools are easy to understand but offer a limited range of expressiveness. Other more powerful tools are difficult and time consuming to use and inscrutable to novices. Researchers have described a variety of more sophisticated methods but these are typically not interactive which is crucial for creative exploration. This paper introduces a simple intuitive and interactive tool that allows non-experts to recolor an image colors by editing a color palette. This system is comprised of several components: a GUI that is easy to learn and understand a new efficient algorithm for creating a color palette from an image and a new efficient color transfer algorithm that recolors the image based on a user-modified palette. We evaluate our approach via a user study showing that it is faster and easier to use than two alternatives. It also shows that untrained users can quickly achieve results comparable to those of experts using professional software.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Palette-based+Photo+Recoloring+Chang+Fried+Liu+DiVerdi+Finkelstein,http://research.google.com/pubs/pub43475.html
Space-Filling Trees: A New Perspective on Incremental Search for Motion Planning,Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems IEEE (2011),2011,James J. Kuffner Steven M. LaValle,@inproceedings{37063 title = {Space-Filling Trees: A New Perspective on Incremental Search for Motion Planning} author = {James J. Kuffner and Steven M. LaValle} year = 2011 booktitle = {Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems} },This paper introduces space-filling trees and analyzes them in the context of sampling-based motion planning. Space-filling trees are analogous to space-filling curves but have a branching tree-like structure and are defined by an incremental process that results in a tree for which every point in the space has a finite-length path that converges to it. In contrast to space-filling curves individual paths in the tree are short allowing any part of the space to be quickly reached from the root. We compare some basic constructions of space-filling trees to Rapidly-exploring Random Trees (RRTs) which underlie a number of popular algorithms used for sampling-based motion planning. We characterize several key tree properties related to path quality and the overall efficiency of exploration and conclude with a number of open mathematical questions.,http://research.google.com/pubs/archive/37063.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Space-Filling+Trees:+A+New+Perspective+on+Incremental+Search+for+Motion+Planning+Kuffner+LaValle,http://research.google.com/pubs/pub37063.html
D-Nets: Beyond Patch-Based Image Descriptors,IEEE International Conference on Computer Vision and Pattern Recognition (CVPR'12) (2012),2012,Felix von Hundelshausen Rahul Sukthankar,@inproceedings{38090 title = {D-Nets: Beyond Patch-Based Image Descriptors} author = {Felix von Hundelshausen and Rahul Sukthankar} year = 2012 booktitle = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR'12)} },Despite much research on patch-based descriptors SIFT remains the gold standard for finding correspondences across images and recent descriptors focus primarily on improving speed rather than accuracy. In this paper we propose Descriptor-Nets (D-Nets) a computationally efficient method that significantly improves the accuracy of image matching by going beyond patch-based approaches. D-Nets constructs a network in which nodes correspond to traditional sparsely or densely sampled keypoints and where image content is sampled from selected edges in this net. Not only is our proposed representation invariant to cropping translation scale reflection and rotation but it is also significantly more robust to severe perspective and non-linear distortions. We present several variants of our algorithm including one that tunes itself to the image complexity and an efficient parallelized variant that employs a fixed grid. Comprehensive direct comparisons against SIFT and ORB on standard datasets demonstrate that D-Nets dominates existing approaches in terms of precision and recall while retaining computational efficiency.,http://research.google.com/pubs/archive/38090.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=D-Nets:+Beyond+Patch-Based+Image+Descriptors+Hundelshausen+Sukthankar,http://research.google.com/pubs/pub38090.html
Permutation Indexing: Fast Approximate Retrieval from Large Corpora,22nd International Conference on Information and Knowledge Management (CIKM) ACM (2013),2013,Maxim Gurevich Tamas Sarlos,@inproceedings{41468 title = {Permutation Indexing: Fast Approximate Retrieval from Large Corpora} author = {Maxim Gurevich and Tamas Sarlos} year = 2013 booktitle = {22nd International Conference on Information and Knowledge Management (CIKM)} },Inverted indexing is a ubiquitous technique used in retrieval systems including web search. Despite its popularity it has a drawback - query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique permutation indexing where retrieval cost is strictly bounded and has only logarithmic dependence on the corpus size. Our approach is based on two novel techniques: partitioning of the term space into overlapping clusters of terms that frequently co-occur in queries and a data structure for compactly encoding results of all queries composed of terms in a cluster as continuous sequences of document ids. Then query results are retrieved by fetching few small chunks of these sequences. There is a price though: our encoding is lossy and thus returns approximate result sets. The fraction of the true results returned recall is controlled by the level of redundancy. The more space is allocated for the permutation index the higher is the recall. We analyze permutation indexing both theoretically under simplified document and query models and empirically on a realistic document and query collections. We show that although permutation indexing can not replace traditional retrieval methods since high recall cannot be guaranteed on all queries it covers up to 77% of tail queries and can be used to speed up retrieval for these queries.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Permutation+Indexing:+Fast+Approximate+Retrieval+from+Large+Corpora+Gurevich+Sarlos,http://research.google.com/pubs/pub41468.html
Extracting Unambiguous Keywords from Microposts Using Web and Query Logs Data,Making sense of Microposts (at WWW 2012),2012,Davi Reis Felipe Goldstein Frederico Quintao,@inproceedings{40408 title = {Extracting Unambiguous Keywords from Microposts Using Web and Query Logs Data} author = {Davi Reis and Felipe Goldstein and Frederico Quintao} year = 2012 booktitle = {Making sense of Microposts (at WWW 2012)} },In the recent years a new form of content type has become ubiquitous in the web. These are small and noisy text snippets created by users of social networks such as Twitter and Facebook. The full interpretation of those microposts by machines impose tremendous challenges since they strongly rely on context. In this paper we propose a task which is much simpler than full interpretation of microposts: we aim to build classiﬁcation systems to detect keywords that unambiguously refer to a single dominant concept even when taken out of context. For example in the context of this task apple would be classiﬁed as ambiguous whereas microsoft would not. The contribution of this work is twofold. First we formalize this novel classiﬁcation task that can be directly applied for extracting information from microposts. Second we show how high precision classiﬁers for this problem can be built out of Web data and search engine logs combining traditional information retrieval metrics such as inverted document frequency and new ones derived from search query logs. Finally we have proposed and evaluated relevant applications for these classiﬁers which were able to meet precision ≥ 72% and recall ≥ 56% on unambiguous keyword extraction from microposts. We also compare those results with closely related systems none of which could outperform those numbers.,http://research.google.com/pubs/archive/40408.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Extracting+Unambiguous+Keywords+from+Microposts+Using+Web+and+Query+Logs+Data+Reis+Goldstein+Quintao,http://research.google.com/pubs/pub40408.html
Cost-efficient Dragonfly Topology for Large-scale Systems,Optical Fiber Communication Conference (OFC 2009),2009,John Kim William J. Dally Steve Scott Dennis Abts,@inproceedings{35154 title = {Cost-efficient Dragonfly Topology for Large-scale Systems} author = {John Kim and William J. Dally and Steve Scott and Dennis Abts} year = 2009 booktitle = {Optical Fiber Communication Conference (OFC 2009)} },Evolving technology and increasing pin-bandwidth motivate the use of high-radix routers to reduce the diameter latency and cost of interconnection networks. This migration from low-radix to high-radix routers is demonstrated with the recent introduction of high-radix routers and they are expected to impact networks used in large-scale systems such as multicomputers and data centers. As a result a scalable and a cost-efficient topology is needed to properly exploit high-radix routers. High-radix networks require longer cables than their low-radix counterparts. Because cables dominate network cost the number of cables and particularly the number of long global cables should be minimized to realize an efficient network. In this paper we introduce the dragonfly topology which uses a group of high-radix routers as a virtual router to increase the effective radix of the network. With this organization each minimally routed packet traverses at most one global channel. By reducing global channels a dragonfly reduces cost by 20% compared to a flattened butterfly and by 52% compared to a folded Clos network in configurations with at least 16K nodes. The paper also introduces two new variants of global adaptive routing that enable load-balanced routing in the dragonfly. Each router in a dragonfly must make an adaptive routing decision based on the state of a global channel connected to a different router. Because of the indirect nature of this routing decision conventional adaptive routing algorithms give degraded performance. We introduce the use of selective virtual-channel discrimination and the use of credit round-trip latency to both sense and signal channel congestion. The combination of these two methods gives throughput and latency that approaches that of an ideal adaptive routing algorithm.,http://research.google.com/pubs/archive/35154.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cost-efficient+Dragonfly+Topology+for+Large-scale+Systems+Kim+Dally+Scott+Abts,http://research.google.com/pubs/pub35154.html
The Incremental Reach and Cost Efficiency of Online Video Ads over TV Ads,Google Inc (2012) pp. 1-17,2012,Yuxue Jin Sheethal Shobowale Jim Koehler Harry Case,@techreport{40426 title = {The Incremental Reach and Cost Efficiency of Online Video Ads over TV Ads} author = {Yuxue Jin and Sheethal Shobowale and Jim Koehler and Harry Case} year = 2012 },As people spend more time online an increasing number of brand marketers are including video ads in their advertising campaigns. These advertisers would like to know the incremental reach and cost efficiency of their video and display ads compared to their TV ads. In this paper we measure the incremental reach to a target demographic and estimate the cost per incremental reach point of YouTube (YT) and the Google Display Network (GDN) compared to TV ad campaigns. We consider two media planning scenarios: what it would have cost for the TV ad campaign to have delivered the equivalent of the online incremental reach and what saving could have been achieved by having spent less on TV ads and complementing them with online ads for a given reach goal.,http://research.google.com/pubs/archive/40426.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Incremental+Reach+and+Cost+Efficiency+of+Online+Video+Ads+over+TV+Ads+Jin+Shobowale+Koehler+Case,http://research.google.com/pubs/pub40426.html
High-efficiency power supplies for home computers and servers,Google (2006) pp. 1-3,2006,Urs Hölzle Bill Weihl,@techreport{32467 title = {High-efficiency power supplies for home computers and servers} author = {Urs Hölzle and Bill Weihl} year = 2006 URL = {http://services.google.com/blog_resources/PSU_white_paper.pdf} note = {Presented at the Intel Developer Forum September 2006.} institution = {Google} },The focus of our message is efficiency: power efficiency and programming efficiency. There are several hard technical problems surrounding power efficiency of computers but we've found one that is actually not particularly challenging and could have a huge impact on the energy used by home computers and low-end servers: increasing power supply efficiency.,http://services.google.com/blog_resources/PSU_white_paper.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=High-efficiency+power+supplies+for+home+computers+and+servers+Holzle+Weihl,http://research.google.com/pubs/pub32467.html
Reducing Lookups for Invariant Checking,Proceedings of ECOOP 2013 (to appear),2013,Jakob G. Thomsen Christian Clausen Kristoffer J. Andersen Erik Ernst John Danaher,@inproceedings{41099 title = {Reducing Lookups for Invariant Checking} author = {Jakob G. Thomsen and Christian Clausen and Kristoffer J. Andersen and Erik Ernst and John Danaher} year = 2013 booktitle = {Proceedings of ECOOP 2013} },This paper helps reducing the cost of invariant checking in cases where access to data is expensive. Assume that a set of variables satisfy a given invariant and a request is received to update a subset of them. We reduce the set of variables to inspect in order to verify that the invariant is still satised. We present a formal model of this scenario based on a simple query language for the expression of invariants that covers the core of a realistic query language. We present an algorithm which simplies a representation of the invariant along with a mechan- ically veried proof of correctness. We also investigate the underlying invariant checking problem in general and show that it is co-NP hard i.e. that solutions must be approximations to remain tractable. We have seen more than an order of magnitude performance improvement using these techniques in a case study,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+Lookups+for+Invariant+Checking+Thomsen+Clausen+Andersen+Ernst+Danaher,http://research.google.com/pubs/pub41099.html
Language-independent Compound Splitting with Morphological Operations,ACL HLT 2011 pp. 10,2011,Klaus Macherey Andrew M. Dai David Talbot Ashok C. Popat Franz Och,@inproceedings{37093 title = {Language-independent Compound Splitting with Morphological Operations} author = {Klaus Macherey and Andrew M. Dai and David Talbot and Ashok C. Popat and Franz Och} year = 2011 booktitle = {ACL HLT 2011} pages = {10} },Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training they pose a challenge for translation systems. Previous decompounding methods have often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. Furthermore monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach.,http://research.google.com/pubs/archive/37093.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Language-independent+Compound+Splitting+with+Morphological+Operations+Macherey+Dai+Talbot+Popat+Och,http://research.google.com/pubs/pub37093.html
BeyondCorp: A New Approach to Enterprise Security,;login: vol. Vol. 39 No. 6 (2014) pp. 6-11,2014,Rory Ward Betsy Beyer,@article{43231 title = {BeyondCorp: A New Approach to Enterprise Security} author = {Rory Ward and Betsy Beyer} year = 2014 journal = {;login:} pages = {6--11} volume = {Vol. 39 No. 6} },Virtually every company today uses firewalls to enforce perimeter security. However this security model is problematic because when that perimeter is breached an attacker has relatively easy access to a company’s privileged intranet. As companies adopt mobile and cloud technologies the perimeter is becoming increasingly difficult to enforce. Google is taking a different approach to network security. We are removing the requirement for a privileged intranet and moving our corporate applications to the Internet.,http://research.google.com/pubs/archive/43231.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=BeyondCorp:+A+New+Approach+to+Enterprise+Security+Ward+Beyer,http://research.google.com/pubs/pub43231.html
Size Matters: Exhaustive Geometric Verification for Image Retrieval,12th European Conference on Computer Vision (ECCV) Springer (2012) pp. 674-687,2012,Henrik Stewenius Steinar H. Gunderson Julien Pilet,@inproceedings{41923 title = {Size Matters: Exhaustive Geometric Verification for Image Retrieval} author = {Henrik Stewenius and Steinar H. Gunderson and Julien Pilet} year = 2012 booktitle = {12th European Conference on Computer Vision (ECCV)} pages = {674--687} },The overreaching goals in large-scale image retrieval are bigger better and cheaper. For systems based on local features we show how to get both efficient geometric verification of every match and unprecedented speed for the low sparsity situation. Large-scale systems based on quantized local features usually process the index one term at a time forcing two separate scoring steps: First a scoring step to find candidates with enough matches and then a geometric verification step where a subset of the candidates are checked. Our method searches through the index a document at a time verifying the geometry of every candidate in a single pass. We study the behavior of several algorithms with respect to index density---a key element for large-scale databases. In order to further improve the efficiency we also introduce a new new data structure called the counting min-tree which outperforms other approaches when working with low database density a necessary condition for very large-scale systems. We demonstrate the effectiveness of our approach with a proof of concept system that can match an image against a database of more than 90~billion images in just a few seconds.,http://research.google.com/pubs/archive/41923.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Size+Matters:+Exhaustive+Geometric+Verification+for+Image+Retrieval+Stew%C3%A9nius+Gunderson+Pilet,http://research.google.com/pubs/pub41923.html
Value of Targeting,Proceedings of the 7th International Symposium on Algorithmic Game Theory (SAGT) (2014) pp. 194-205,2014,Kshipra Bhawalkar Patrick Hummel Sergei Vassilvitskii,@inproceedings{42895 title = {Value of Targeting} author = {Kshipra Bhawalkar and Patrick Hummel and Sergei Vassilvitskii} year = 2014 URL = {http://rd.springer.com/chapter/10.1007%2F978-3-662-44803-8_17} booktitle = {Proceedings of the 7th International Symposium on Algorithmic Game Theory (SAGT)} pages = {194-205} },We undertake a formal study of the value of targeting data to an advertiser. As expected this value is increasing in the utility difference between realizations of the targeting data and the accuracy of the data and depends on the distribution of competing bids. However this value may vary non-monotonically with an advertiser’s budget. Similarly modeling the values as either private or correlated or allowing other advertisers to also make use of the data leads to unpredictable changes in the value of data. We address questions related to multiple data sources show that utility of additional data may be non-monotonic and provide tradeoffs between the quality and the price of data sources. In a game-theoretic setting we show that advertisers may be worse off than if the data had not been available at all. We also ask whether a publisher can infer the value an advertiser would place on targeting data from the advertiser’s bidding behavior and illustrate that this is impossible.,http://rd.springer.com/chapter/10.1007%2F978-3-662-44803-8_17,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Value+of+Targeting+Bhawalkar+Hummel+Vassilvitskii,http://research.google.com/pubs/pub42895.html
Interpretable groups are definable,Journal of Mathematical Logic vol. 14 (2014),2014,Pantelis Eleftheriou Ya'acov Peterzil Janak Ramakrishnan,@article{42950 title = {Interpretable groups are definable} author = {Pantelis Eleftheriou and Ya'acov Peterzil and Janak Ramakrishnan} year = 2014 URL = {http://www.worldscientific.com/doi/abs/10.1142/S0219061314500020?journalCode=jml&} journal = {Journal of Mathematical Logic} volume = {14} },We prove that in an arbitrary o-minimal structure every interpretable group is definably isomorphic to a definable one. We also prove that every definable group lives in a cartesian product of one-dimensional definable group-intervals (or one-dimensional definable groups). We discuss the general open question of elimination of imaginaries in an o-minimal structure.,http://www.worldscientific.com/doi/abs/10.1142/S0219061314500020?journalCode=jml&,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Interpretable+groups+are+definable+Eleftheriou+Peterzil+Ramakrishnan,http://research.google.com/pubs/pub42950.html
L1 and L2 Regularization for Multiclass Hinge Loss Models,Symposium on Machine Learning in Speech and Natural Language Processing (2011),2011,Robert C. Moore John DeNero,@inproceedings{37362 title = {L1 and L2 Regularization for Multiclass Hinge Loss Models} author = {Robert C. Moore and John DeNero} year = 2011 URL = {http://www.ttic.edu/sigml/symposium2011/papers/Moore+DeNero_Regularization.pdf} booktitle = {Symposium on Machine Learning in Speech and Natural Language Processing} },This paper investigates the relationship between the loss function the type of regularization and the resulting model sparsity of discriminatively-trained multiclass linear models. The effects on sparsity of optimizing log loss are straightforward: L2 regularization produces very dense models while L1 regularization produces much sparser models. However optimizing hinge loss yields more nuanced behavior. We give experimental evidence and theoretical arguments that for a class of problems that arises frequently in natural-language processing both L1- and L2-regularized hinge loss lead to sparser models than L2-regularized log loss but less sparse models than L1-regularized log loss. Furthermore we give evidence and arguments that for models with only indicator features there is a critical threshold on the weight of the regularizer below which L1- and L2-regularized hinge loss tends to produce models of similar sparsity.,http://research.google.com/pubs/archive/37362.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=L1+and+L2+Regularization+for+Multiclass+Hinge+Loss+Models+Moore+DeNero,http://research.google.com/pubs/pub37362.html
High Performance Low Cost Colorless ONU for WDM-PON,Optical Fiber Communication (OFC) Conference 2012 OSA Washington DC,2012,Ryohei Urata Cedric Lam Hong Liu Chris Johnson,@inproceedings{37746 title = {High Performance Low Cost Colorless ONU for WDM-PON} author = {Ryohei Urata and Cedric Lam and Hong Liu and Chris Johnson} year = 2012 booktitle = {Optical Fiber Communication (OFC) Conference 2012} address = {Washington DC} },We give an overview of key technologies for realizing WDM-PON. In particular we highlight promising developments and directions in widely tunable laser technologies for achieving a high performance colorless ONU at the cost points required for access networks.,http://research.google.com/pubs/archive/37746.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=High+Performance+Low+Cost+Colorless+ONU+for+WDM-PON+Urata+Lam+Liu+Johnson,http://research.google.com/pubs/pub37746.html
Capsicum: practical capabilities for UNIX,Proceedings of the 19th USENIX Security Symposium (2010),2010,Robert N. M. Watson Jonathan Anderson Ben Laurie Kris Kennaway,@inproceedings{36736 title = {Capsicum: practical capabilities for UNIX} author = {Robert N. M. Watson and Jonathan Anderson and Ben Laurie and Kris Kennaway} year = 2010 URL = {http://www.cl.cam.ac.uk/research/security/capsicum/papers/2010usenix-security-capsicum-website.pdf} booktitle = {Proceedings of the 19th USENIX Security Symposium} },Capsicum is a lightweight operating system capabil- ity and sandbox framework planned for inclusion in FreeBSD 9. Capsicum extends rather than replaces UNIX APIs providing new kernel primitives (sandboxed capability mode and capabilities) and a userspace sand- box API. These tools support compartmentalisation of monolithic UNIX applications into logical applications an increasingly common goal supported poorly by dis- cretionary and mandatory access control. We demon- strate our approach by adapting core FreeBSD utilities and Google’s Chromium web browser to use Capsicum primitives and compare the complexity and robustness of Capsicum with other sandboxing techniques.,http://research.google.com/pubs/archive/36736.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Capsicum:+practical+capabilities+for+UNIX+Watson+Anderson+Laurie+Kennaway,http://research.google.com/pubs/pub36736.html
Systematic Software Testing: The Korat Approach,Foundations of Software Engineering (FSE) ACM (2012) pp. 1,2012,Chandrasekhar Boyapati Sarfraz Khurshid Darko Marinov,@inproceedings{40660 title = {Systematic Software Testing: The Korat Approach} author = {Chandrasekhar Boyapati and Sarfraz Khurshid and Darko Marinov} year = 2012 URL = {http://web.eecs.umich.edu/~bchandra/publications/fse12.pdf} booktitle = {Foundations of Software Engineering (FSE)} pages = {1} },"At ISSTA 2002 the three authors (then Ph.D. students) published the paper Korat: Automated Testing Based on Java Predicates"" which won one of the first ACM SIGSOFT Distinguished paper awards. In 2012 the paper won the ACM SIGSOFT Impact Paper Award. The authors briefly recount the motivation behind the Korat research the ideas presented in the original paper and some work it inspired.",http://research.google.com/pubs/archive/40660.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Systematic+Software+Testing:+The+Korat+Approach+Boyapati+Khurshid+Marinov,http://research.google.com/pubs/pub40660.html
Vine Pruning for Efficient Multi-Pass Dependency Parsing,The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL '12) Best Paper Award,2012,Alexander Rush Slav Petrov,@inproceedings{37844 title = {Vine Pruning for Efficient Multi-Pass Dependency Parsing} author = {Alexander Rush and Slav Petrov} year = 2012 URL = {http://petrovi.de/data/naacl12.pdf} booktitle = {The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL '12)} pages = {Best Paper Award} },Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first- second- and third-order models achieve accuracies comparable to those of their unpruned counterparts while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.,http://research.google.com/pubs/archive/37844.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Vine+Pruning+for+Efficient+Multi-Pass+Dependency+Parsing+Rush+Petrov,http://research.google.com/pubs/pub37844.html
Do you know your IQ? A research agenda for information quality in systems,HotMETRICS'09 (2009),2009,Kimberley Keeton HP Labs Pankaj Mehra HP Labs John Wilkes,@inproceedings{35209 title = {Do you know your {IQ}? {A} research agenda for information quality in systems} author = {Kimberley Keeton HP Labs and Pankaj Mehra HP Labs and John Wilkes} year = 2009 URL = {http://www.e-wilkes.com/john/papers/2009-QoI-HotMETRICS.pdf} booktitle = {HotMETRICS'09} },Information quality (IQ) is a measure of how fit information is for a purpose. Sometimes called Quality of Information (QoI) by analogy with Quality of Service (QoS) it quantifies whether the right information is being used to make a decision or take an action. Failure to understand whether information is of adequate quality can lead to bad decisions and catastrophic effects. The results can include system outages increased costs lost revenue -- and worse. Quantifying information quality can help improve decision making but the ultimate goal should be to select or construct information sources that have the appropriate balance between information quality and the cost of providing it. In this paper we provide a brief introduction to the field argue the case for applying information quality metrics in the systems domain and propose a research agenda to explore this space.,http://research.google.com/pubs/archive/35209.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Do+you+know+your+%7BIQ%7D%3F+%7BA%7D+research+agenda+for+information+quality+in+systems+Keeton+Mehra+Wilkes,http://research.google.com/pubs/pub35209.html
AAPOR standard definition 8th edition,AAPOR (2015),2015,Tom E. Smith Rob Daves Paul J. Lavrakas Mick P. Couper Timothy P. Johnson Sara Zuckerbraun Katherine Morton David Dutwin Mario Callegaro Mansour Fahimi,none,Background: For a long time survey researchers have needed more comprehensive and reliable diagnostic tools to understand the components of total survey error. Some of those components such as margin of sampling error are relatively easily calculated and familiar to many who use survey research. Other components such as the influence of question wording on responses are more difficult to ascertain. Groves (1989) catalogues error into three other major potential areas in which it can occur in sample surveys. One is coverage where error can result if some members of the population under study do not have a known nonzero chance of being included in the sample. Another is measurement effect such as when the instrument or items on the instrument are constructed in such a way to produce unreliable or invalid data. The third is nonresponse effect where nonrespondents in the sample that researchers originally drew differ from respondents inways that are germane to the objectives of the survey. Defining final disposition codes and calculating survey outcome rates is the topic for this booklet. Often it is assumed — correctly or not — that the lower the response rate the more question there is about the validity of the sample. Although response rate information alone is not sufficient for determining how much nonresponse error exists in a survey or even whether it exists calculating the rates is a critical first step to understanding the presence of this component of potential survey error. By knowing the disposition of every element drawn in a survey sample researchers can assess whether their sample might contain nonresponse error and the potential reasons for that error. With this report AAPOR offers a tool that can be used as a guide to one important aspect of a survey's quality. It is a comprehensive well-delineated way of describing the final disposition of cases and calculating outcome rates for surveys conducted by telephone (landline and cell) for personal interviews in a sample of households for mail surveys of specifically named persons (i.e. a survey in which named persons are the sampled elements) and for Web surveys. AAPOR urges all practitioners to use these standardized sample disposition codes in all reports of survey methods no matter if the project is proprietary work for private sector clients or a public government or academic survey. This will enable researchers to find common ground on which to compare the outcome rates for different surveys. _The eighth edition (2015) was edited by Smith who chaired the committee of Daves Lavrakas Couper and Johnson. The revised section on establishment surveys was developed by Sara Zuckerbraun and Katherine Morton. The new section on dual-frame telephone surveys was prepared by a sub-committee headed by Daves with Smith David Dutwin Mario Callegaro and Mansour Fahimi as members.,http://www.aapor.org/AAPORKentico/Communications/AAPOR-Journals/Standard-Definitions.aspx,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=AAPOR+standard+definition+8th+edition+Smith+Daves+Lavrakas+Couper+Johnson+Zuckerbraun+Morton+Dutwin+Callegaro+Fahimi,http://research.google.com/pubs/pub43457.html
Thwarting Fake OSN Accounts by Predicting their Victims,AI-Sec'2015 ACM (to appear),2015,Yazan Boshmaf Matei Ripeanu Konstantin Beznosov Elizeu Santos-Neto,@inproceedings{43862 title = {Thwarting Fake OSN Accounts by Predicting their Victims} author = {Yazan Boshmaf and Matei Ripeanu and Konstantin Beznosov and Elizeu Santos-Neto} year = 2015 booktitle = {AI-Sec'2015} },Traditional defense mechanisms for fighting against automated fake accounts in online social networks are victim-agnostic. Even though victims of fake accounts play an important role in the viability of subsequent attacks there is no work on utilizing this insight to improve the status quo. In this position paper we take the first step and propose to incorporate predictions about victims of unknown fakes into the workflows of existing defense mechanisms. In particular we investigated how such an integration could lead to more robust fake account defense mechanisms. We also used real world datasets from Facebook and Tuenti to evaluate the feasibility of predicting victims of fake accounts using supervised machine learning.,http://research.google.com/pubs/archive/43862.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Thwarting+Fake+OSN+Accounts+by+Predicting+their+Victims+Boshmaf+Ripeanu+Beznosov+Santos-Neto,http://research.google.com/pubs/pub43862.html
A Gaussian Mixture Model Layer Jointly Optimized with Discriminative Features within A Deep Neural Network Architecture,ICASSP IEEE (2015),2015,Ehsan Variani Erik McDermott Georg Heigold,@inproceedings{43912 title = {A Gaussian Mixture Model Layer Jointly Optimized with Discriminative Features within A Deep Neural Network Architecture} author = {Ehsan Variani and Erik McDermott and Georg Heigold} year = 2015 booktitle = {ICASSP} },This article proposes and evaluates a Gaussian Mixture Model (GMM) represented as the last layer of a Deep Neural Network (DNN) architecture and jointly optimized with all previous layers using Asynchronous Stochastic Gradient Descent (ASGD). The resulting “Deep GMM” architecture was investigated with special attention to the following issues: (1) The extent to which joint optimization improves over separate optimization of the DNN-based feature extraction layers and the GMM layer; (2) The extent to which depth (measured in number of layers for a matched total number of parameters) helps a deep generative model based on the GMM layer compared to a vanilla DNN model; (3) Head-to-head performance of Deep GMM architectures vs. equivalent DNN architectures of comparable depth using the same optimization criterion (frame-level Cross Entropy (CE)) and optimization method (ASGD); (4) Expanded possibilities for modeling offered by the Deep GMM generative model. The proposed Deep GMMs were found to yield Word Error Rates (WERs) competitive with state-of-the-art DNN systems at the cost of pre-training using standard DNNs to initialize the Deep GMM feature extraction layers. An extension to Deep Subspace GMMs is described resulting in additional gains.,http://research.google.com/pubs/archive/43912.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Gaussian+Mixture+Model+Layer+Jointly+Optimized+with+Discriminative+Features+within+A+Deep+Neural+Network+Architecture+Variani+McDermott+Heigold,http://research.google.com/pubs/pub43912.html
Research skills matter: How to teach them,Moonshots in Education: Launching Blended Learning in the Classroom Pacific Research Institute San Francisco CA (2015),2015,Daniel Martin Russell,@inbook{44007 title = {Research skills matter: How to teach them} author = {Daniel Martin Russell} year = 2015 booktitle = {Moonshots in Education: Launching Blended Learning in the Classroom} address = {San Francisco CA} },There’s always been a gap between those who know how to use information resources and those who don’t. Students who knew the ways to leverage a library for research could consistently do better research than those who couldn’t. This chapter is about why teaching research skills is a necessary step in the development of students.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Research+skills+matter:++How+to+teach+them+Russell,http://research.google.com/pubs/pub44007.html
Routing multi-class traffic flows in the plane,Computational Geometry vol. 45 (2012) pp. 99-114,2012,Joondong Kim Joseph S. B. Mitchell Valentin Polishchuk Shang Yang Jingyu Zou,@article{40361 title = {Routing multi-class traffic flows in the plane} author = {Joondong Kim and Joseph S. B. Mitchell and Valentin Polishchuk and Shang Yang and Jingyu Zou} year = 2012 journal = {Computational Geometry} pages = {99--114} volume = {45} },We study a class of multi-commodity flow problems in geometric domains: For a given planar domain P populated with obstacles (holes) of K_2types compute a set of thick paths from a “source” edge of P to a “sink” edge of P for vehicles of K distinct classes. Each class k of vehicle has a given set Ok of obstacles it must avoid and a certain width wk of path it requires. The problem is to determine if it is possible to route Nk width-wk paths for class k vehicles from source to sink with each path avoiding the requisite set Ok of obstacles and no two paths overlapping. This form of multi-commodity flow in two-dimensional domains arises in computing throughput capacity for multiple classes of aircraft in an airspace impacted by different types of constraints such as those arising from weather hazards. We give both algorithmic theory results and experimental results. We show hardness of many versions of the problem by proving that two simple variants are NP-hard even in the case K=2. If w1=w2=1 then the problem is NP-hard even when O1=_. If w1=2 w2=3 then the problem is NP-hard even when O1=O2. In contrast the problem for a single width and a single type of obstacles is polynomially solvable. We present approximation algorithms for the multi-criteria optimization problems that arise when trying to maximize the number of routable paths. We also give a polynomial-time algorithm for the case in which the number of holes in the input domain is bounded. Finally we give experimental results based on an implementation of our methods and experiment with enhanced heuristics for efficient solutions in practice. Our algorithms are being utilized in simulations with NASA_s Future Air traffic management Concepts Evaluation Tool (FACET). We report on experimental results based on applying our algorithms to weather-impacted airspaces comparing heuristic strategies for searching for feasible path orderings and for computing short multi-class routes. Our results show that multi-class routes can feasibly be computed on real weather data instances on the scale required in air traffic management applications.,http://research.google.com/pubs/archive/40361.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Routing+multi-class+traffic+flows+in+the+plane+Kim+Mitchell+Polishchuk+Yang+Zou,http://research.google.com/pubs/pub40361.html
Learning with Deep Cascades,Proceedings of the Twenty-Sixth International Conference on Algorithmic Learning Theory (ALT 2015),2015,Giulia DeSalvo Mehryar Mohri Umar Syed,@inproceedings{43977 title = {Learning with Deep Cascades} author = {Giulia DeSalvo and Mehryar Mohri and Umar Syed} year = 2015 booktitle = {Proceedings of the Twenty-Sixth International Conference on Algorithmic Learning Theory (ALT 2015)} },We introduce a broad learning model formed by cascades of predictors Deep Cascades that is structured as general decision trees in which leaf predictors or node questions may be members of rich function families. We present new detailed data-dependent theoretical guarantees for learning with Deep Cascades with complex leaf predictors or node question in terms of the Rademacher complexities of the sub-families composing these sets of predictors and the fraction of sample points correctly classified at each leaf. These general guarantees can guide the design of a variety of different algorithms for deep cascade models and we give a detailed description of two such algorithms. Our second algorithm uses as node and leaf classifiers SVM predictors and we report the results of experiments comparing its performance with that of SVM combined with polynomial kernels.,http://research.google.com/pubs/archive/43977.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+with+Deep+Cascades+DeSalvo+Mohri+Syed,http://research.google.com/pubs/pub43977.html
ACDC-JS: explorative benchmarking of javascript memory management,Proceedings of the 10th ACM Symposium on Dynamic Languages ACM New York NY USA (2014) pp. 67-78,2014,Martin Aigner Thomas Huetter Christoph M. Kirsch Alexander Miller Hannes Payer Mario Preishuber,@inproceedings{43216 title = {ACDC-JS: explorative benchmarking of javascript memory management} author = {Martin Aigner and Thomas Huetter and Christoph M. Kirsch and Alexander Miller and Hannes Payer and Mario Preishuber} year = 2014 booktitle = {Proceedings of the 10th ACM Symposium on Dynamic Languages} pages = {67--78} address = {New York NY USA} },We present ACDC-JS an open-source JavaScript memory management benchmarking tool. ACDC-JS incorporates a heap model based on real web applications and may be configured to expose virtually any relevant performance characteristics of JavaScript memory management systems. ACDC-JS is based on ACDC a benchmarking tool for C/C++ that models periodic allocation and deallocation behavior (AC) as well as persistent memory (DC). We identify important characteristics of JavaScript mutator behavior and propose a configurable heap model based on typical distributions of these characteristics as foundation for ACDC-JS. We describe heap analyses of 13 real web applications extending existing work on JavaScript behavior analysis. Our experimental results show that ACDC-JS enables performance benchmarking and debugging of state-of-the-art JavaScript virtual machines such as V8 and SpiderMonkey by exposing key aspects of their memory management performance.,http://research.google.com/pubs/archive/43216.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ACDC-JS:+explorative+benchmarking+of+javascript+memory+management+Aigner+Huetter+Kirsch+Miller+Payer+Preishuber,http://research.google.com/pubs/pub43216.html
Reducing dominance in multiple-mouse learning activities,CSCL'09: Proceedings of the 9th international conference on Computer supported collaborative learning International Society of the Learning Sciences (2009) pp. 360-364,2009,Andrea Moed Owen Otto Joyojeet Pal Udai Pawar Singh Matthew Kam Kentaro Toyama,@inproceedings{36234 title = {Reducing dominance in multiple-mouse learning activities} author = {Andrea Moed and Owen Otto and Joyojeet Pal and Udai Pawar Singh and Matthew Kam and Kentaro Toyama} year = 2009 booktitle = {CSCL'09: Proceedings of the 9th international conference on Computer supported collaborative learning} pages = {360--364} },In resource-constrained classrooms in the developing world it is common for several students to share each computer. Unfortunately dominance behavior often naturally emerges in these situations when one child monopolizes the mouse and keyboard. One way to mitigate this phenomenon is by providing each child with a mouse and a corresponding on-screen cursor so that everyone can interact. Though such multiple-mouse configurations reduce the possibility of total domination by one individual they do not automatically eliminate dominance behavior completely. We propose the use of a design for small-group learning on shared computers based on enforced turn-taking in a split-screen multiple-mouse environment. In an evaluation with 104 rural schoolchildren in India we found that dominance behavior was indeed reduced through these design choices.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+dominance+in+multiple-mouse+learning+activities+Moed+Otto+Pal+Singh+Kam+Toyama,http://research.google.com/pubs/pub36234.html
RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,Proceedings of the 21st ACM Conference on Computer and Communications Security ACM Scottsdale Arizona (2014) (to appear),2014,Úlfar Erlingsson Vasyl Pihur Aleksandra Korolova,@inproceedings{42852 title = {RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response} author = {Úlfar Erlingsson and Vasyl Pihur and Aleksandra Korolova} year = 2014 booktitle = {Proceedings of the 21st ACM Conference on Computer and Communications Security} address = {Scottsdale Arizona} },Randomized Aggregatable Privacy-Preserving Ordinal Response or RAPPOR is a technology for crowdsourcing statistics from end-user client software anonymously with strong privacy guarantees. In short RAPPORs allow the forest of client data to be studied without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner RAPPOR provides the mechanisms for such collection as well as for efficient high-utility analysis of the collected data. In particular RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client and without linkability of their reports. This paper describes and motivates RAPPOR details its differential-privacy and utility guarantees discusses its practical deployment and properties in the face of different attack models and finally gives results of its application to both synthetic and real-world data.,http://research.google.com/pubs/archive/42852.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RAPPOR:+Randomized+Aggregatable+Privacy-Preserving+Ordinal+Response+Erlingsson+Pihur+Korolova,http://research.google.com/pubs/pub42852.html
Automatic Gain Control and Multi-style Training for Robust Small-Footprint Keyword Spotting with Deep Neural Networks,Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2015) pp. 4704-4708,2015,Rohit Prabhavalkar Raziel Alvarez Carolina Parada Preetum Nakkiran Tara Sainath,@inproceedings{43289 title = {Automatic Gain Control and Multi-style Training for Robust Small-Footprint Keyword Spotting with Deep Neural Networks} author = {Rohit Prabhavalkar and Raziel Alvarez and Carolina Parada and Preetum Nakkiran and Tara Sainath} year = 2015 booktitle = {Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4704--4708} },We explore techniques to improve the robustness of small-footprint keyword spotting models based on deep neural networks (DNNs) in the presence of background noise and in far-field conditions. We find that system performance can be improved significantly with relative improvements up to 75% in far-field conditions by employing a combination of multi-style training and a proposed novel formulation of automatic gain control (AGC) that estimates the levels of both speech and background noise. Further we find that these techniques allow us to achieve competitive performance even when applied to DNNs with an order of magnitude fewer parameters than our baseline.,http://research.google.com/pubs/archive/43289.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatic+Gain+Control+and+Multi-style+Training+for+Robust+Small-Footprint+Keyword+Spotting+with+Deep+Neural+Networks+Prabhavalkar+Alvarez+Parada+Nakkiran+Sainath,http://research.google.com/pubs/pub43289.html
Fast quantum methods for optimization,The European Physical Journal Special Topics vol. 224 (2015) pp. 35,2015,S. Boixo G. Ortiz R. Somma,@article{43402 title = {Fast quantum methods for optimization} author = {S. Boixo and G. Ortiz and R. Somma} year = 2015 journal = {The European Physical Journal Special Topics} pages = {35} volume = {224} },Discrete combinatorial optimization consists in finding the optimal configuration that minimizes a given discrete objective function. An interpretation of such a function as the energy of a classical system allows us to reduce the optimization problem into the preparation of a low-temperature thermal state of the system. Motivated by the quantum annealing method we present three strategies to prepare the low-temperature state that exploit quantum mechanics in remarkable ways. We focus on implementations without uncontrolled errors induced by the environment. This allows us to rigorously prove a quantum advantage. The first strategy uses a classical-to-quantum mapping where the equilibrium properties of a classical system in d spatial dimensions can be determined from the ground state properties of a quantum system also in d spatial dimensions. We show how such a ground state can be prepared by means of quantum annealing including quantum adiabatic evolutions. This mapping also allows us to unveil some fundamental relations between simulated and quantum annealing. The second strategy builds upon the first one and introduces a technique called spectral gap amplification to reduce the time required to prepare the same quantum state adiabatically. If implemented on a quantum device that exploits quantum coherence this strategy leads to a quadratic improvement in complexity over the well-known bound of the classical simulated annealing method. The third strategy is not purely adiabatic; instead it exploits diabatic processes between the low-energy states of the corresponding quantum system. For some problems it results in an exponential speedup (in the oracle model) over the best classical algorithms.,http://research.google.com/pubs/archive/43402.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+quantum+methods+for+optimization+Boixo+Ortiz+Somma,http://research.google.com/pubs/pub43402.html
Application Of Pretrained Deep Neural Networks To Large Vocabulary Speech Recognition,Proceedings of Interspeech 2012,2012,Navdeep Jaitly Patrick Nguyen Andrew Senior Vincent Vanhoucke,@inproceedings{38130 title = {Application Of Pretrained Deep Neural Networks To Large Vocabulary Speech Recognition} author = {Navdeep Jaitly and Patrick Nguyen and Andrew Senior and Vincent Vanhoucke} year = 2012 booktitle = {Proceedings of Interspeech 2012} },The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artiﬁcial Neural Network - Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems - 5870 hours of Voice Search and 1400 hours of YouTube data. On the ﬁrst dataset the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model - Hidden Markov Model (GMM/HMM) baseline built with a much larger dataset by 3.7% absolute WER while on the second dataset it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) ﬁne tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the ﬁrst dataset and 0.5% and 0.9% absolute on the second dataset.,http://research.google.com/pubs/archive/38130.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Application+Of+Pretrained+Deep+Neural+Networks+To+Large+Vocabulary+Speech+Recognition+Jaitly+Nguyen+Senior+Vanhoucke,http://research.google.com/pubs/pub38130.html
Unsupervised Spatial Event Detection in Targeted Domains with Applications to Civil Unrest Modeling,PLOS ONE vol. 9 (2014) pp. 1-12,2014,Liang Zhao Feng Cheng Jing Dai Ting Hua Chang-Tien Lu Naren Ramakrishnan,@article{43158 title = {Unsupervised Spatial Event Detection in Targeted Domains with Applications to Civil Unrest Modeling} author = {Liang Zhao and Feng Cheng and Jing Dai and Ting Hua and Chang-Tien Lu and Naren Ramakrishnan} year = 2014 URL = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0110206} note = {Open online access peer reviewed.} journal = {PLOS ONE} pages = {1--12} volume = {9} },Twitter has become a popular data source as a surrogate for monitoring and detecting events. Targeted domains such as crime election and social unrest require the creation of algorithms capable of detecting events pertinent to these domains. Due to the unstructured language short-length messages dynamics and heterogeneity typical of Twitter data streams it is technically difficult and labor-intensive to develop and maintain supervised learning systems. We present a novel unsupervised approach for detecting spatial events in targeted domains and illustrate this approach using one specific domain viz. civil unrest modeling. Given a targeted domain we propose a dynamic query expansion algorithm to iteratively expand domain-related terms and generate a tweet homogeneous graph. An anomaly identification method is utilized to detect spatial events over this graph by jointly maximizing local modularity and spatial scan statistics. Extensive experiments conducted in 10 Latin American countries demonstrate the effectiveness of the proposed approach.,http://research.google.com/pubs/archive/43158.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Spatial+Event+Detection+in+Targeted+Domains+with+Applications+to+Civil+Unrest+Modeling+Zhao+Cheng+Dai+Hua+Lu+Ramakrishnan,http://research.google.com/pubs/pub43158.html
Why There's Antifreeze in Your Toothpaste: The Chemistry of Household Ingredients,Chicago Review Press 814 N. Franklin St. 814 N. Franklin St. Chicago IL 60610 (2007) pp. 240,2007,Simon Quellen Field,@book{33331 title = {Why There's Antifreeze in Your Toothpaste: The Chemistry of Household Ingredients} author = {Simon Quellen Field} year = 2007 URL = {http://sci-toys.com/ingredients/ingredients.html} pages = {240} address = {814 N. Franklin St. 814 N. Franklin St. Chicago IL 60610} },Explaining why antifreeze is a component of toothpaste and how salt works in shampoo this fascinating handbook delves into the chemistry of everyday household products. Decoding more than 150 cryptic ingredients the guide explains each component's structural formula offers synonymous names and describes its common uses. This informative resource can serve curious readers as a basic primer to commercial chemistry or as an indexed reference for specific compounds found on a product label. Grouped according to type these chemical descriptions will dissolve common misunderstandings and help make consumers more product savvy.,http://research.google.com/pubs/archive/33331.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+There's+Antifreeze+in+Your+Toothpaste:+The+Chemistry+of+Household+Ingredients+Field,http://research.google.com/pubs/pub33331.html
Similarity-based Clustering by Left-Stochastic Matrix Factorization,Journal Machine Learning Research (JMLR) vol. 14 (2013) pp. 1715-1746,2013,Raman Arora Maya R. Gupta Amol Kapila Maryam Fazel,@article{41697 title = {Similarity-based Clustering by Left-Stochastic Matrix Factorization} author = {Raman Arora and Maya R. Gupta and Amol Kapila and Maryam Fazel} year = 2013 URL = {file://localhost/Users/mayagupta/Downloads/arora13a.pdf} journal = {Journal Machine Learning Research (JMLR)} pages = {1715--1746} volume = {14} },For similarity-based clustering we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given and an error bound is provided. The algorithm is particularly efficient for the case of two clusters which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels and that the efficient hierarchical variant performs surprisingly well.,http://research.google.com/pubs/archive/41697.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Similarity-based+Clustering+by+Left-Stochastic+Matrix+Factorization+Arora+Gupta+Kapila+Fazel,http://research.google.com/pubs/pub41697.html
Programmers’ Build Errors: A Case Study (at Google),International Conference on Software Engineering (ICSE) (2014) (to appear),2014,Hyunmin Seo Caitlin Sadowski Sebastian Elbaum Edward Aftandilian Robert Bowdidge,@inproceedings{42184 title = {Programmers’ Build Errors: A Case Study (at Google)} author = {Hyunmin Seo and Caitlin Sadowski and Sebastian Elbaum and Edward Aftandilian and Robert Bowdidge} year = 2014 booktitle = {International Conference on Software Engineering (ICSE)} },Building is an integral part of the software development process. However little is known about the errors occurring in this process. In this paper we present an empirical study of 26.6 million builds produced during a period of nine months by thousands of developers. We describe the workflow through which those builds are generated and we analyze failure frequency error types and resolution efforts to fix those errors. The results provide insights on how a large organization build process works and pinpoints errors for which further developer support would be most effective.,http://research.google.com/pubs/archive/42184.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Programmers%E2%80%99+Build+Errors:+A+Case+Study+(at+Google)+Seo+Sadowski+Elbaum+Aftandilian+Bowdidge,http://research.google.com/pubs/pub42184.html
Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition,49th Annual Meeting of the Association for Computational Linguistics (ACL-HLT) Association for Computational Linguistics (2011) pp. 965-975,2011,Stefan Rued Massimiliano Ciaramita Jens Mueller Hinrich Schuetze,@inproceedings{37143 title = {Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition} author = {Stefan Rued and Massimiliano Ciaramita and Jens Mueller and Hinrich Schuetze} year = 2011 URL = {http://www.aclweb.org/anthology/P/P11/P11-1097.pdf} booktitle = {49th Annual Meeting of the Association for Computational Linguistics (ACL-HLT)} pages = {965--975} },We use search engine results to address a particularly dif?cult cross-domain language processing task the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news in-domain and out-of-domain and on web queries.,http://research.google.com/pubs/archive/37143.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Piggyback:+Using+Search+Engines+for+Robust+Cross-Domain+Named+Entity+Recognition+Rued+Ciaramita+Mueller+Mueller,http://research.google.com/pubs/pub37143.html
Adolescent search roles,Journal of the American Society for Information Science and Technology vol. 64(1) (2013) pp. 173-189,2013,Elizabeth Foss Hilary Hutchinson Allison Druin Jason Yip Whitney Ford Evan Golub,@article{40667 title = {Adolescent search roles} author = {Elizabeth Foss and Hilary Hutchinson and Allison Druin and Jason Yip and Whitney Ford and Evan Golub} year = 2013 URL = {http://onlinelibrary.wiley.com/doi/10.1002/asi.22809/pdf} journal = {Journal of the American Society for Information Science and Technology} pages = {173-189} volume = {64(1)} },In this article we present an in-home observation and in-context research study investigating how 38 adolescents aged 14-17 search on the Internet. We present the search trends adolescents display and develop a framework of search roles that these trends help define. We compare these trends and roles to similar trends and roles found in prior work with children ages 7 9 and 11. We use these comparisons to make recommendations to adult stakeholders such as researchers designers and information literacy educators about the best ways to design search tools for children and adolescents as well as how to use the framework of searching roles to find better methods of educating youth searchers. Major findings include the seven roles of adolescent searchers and evidence that adolescents are social in their computer use have a greater knowledge of sources than younger children and that adolescents are less frustrated by searching tasks than younger children.,http://onlinelibrary.wiley.com/doi/10.1002/asi.22809/pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adolescent+search+roles+Foss+Hutchinson+Druin+Yip+Ford+Golub,http://research.google.com/pubs/pub40667.html
Scalable Hierarchical Multitask Learning Algorithms for Conversion Optimization in Display Advertising,ACM International Conference on Web Search And Data Mining (WSDM) (2014),2014,Amr Ahmed Abhimanyu Das Alexander J. Smola,@inproceedings{42498 title = {Scalable Hierarchical Multitask Learning Algorithms for Conversion Optimization in Display Advertising} author = {Amr Ahmed and Abhimanyu Das and Alexander J. Smola} year = 2014 booktitle = {ACM International Conference on Web Search And Data Mining (WSDM)} },Many estimation tasks come in groups and hierarchies of related problems. In this paper we propose a hierarchical model and a scalable algorithm to perform inference for multitask learning. It infers task correlation and subtask structure in a joint sparse setting. Implementation is achieved by a distributed subgradient oracle and the successive application of prox-operators pertaining to groups and sub-groups of variables. We apply this algorithm to conversion optimization in display advertising. Experimental results on over 1TB data for up to 1 billion observations and 1 million attributes show that the algorithm provides significantly better prediction accuracy while simultaneously being efficiently scalable by distributed parameter synchronization.,http://research.google.com/pubs/archive/42498.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Hierarchical+Multitask+Learning+Algorithms+for+Conversion+Optimization+in+Display+Advertising+Ahmed+Das+Smola,http://research.google.com/pubs/pub42498.html
Evaluating job packing in warehouse-scale computing,IEEE Cluster Madrid Spain (2014),2014,Abhishek Verma Madhukar Korupolu John Wilkes,@inproceedings{43103 title = {Evaluating job packing in warehouse-scale computing} author = {Abhishek Verma and Madhukar Korupolu and John Wilkes} year = 2014 booktitle = {IEEE Cluster} address = {Madrid Spain} },One of the key factors in selecting a good scheduling algorithm is using an appropriate metric for comparing schedulers. But which metric should be used when evaluating schedulers for warehouse-scale (cloud) clusters which have machines of different types and sizes heterogeneous workloads with dependencies and constraints on task placement and long-running services that consume a large fraction of the total resources? Traditional scheduler evaluations that focus on metrics such as queuing delay makespan and running time fail to capture important behaviors – and ones that rely on workload synthesis and scaling often ignore important factors such as constraints. This paper explains some of the complexities and issues in evaluating warehouse scale schedulers focusing on what we find to be the single most important aspect in practice: how well they pack long-running services into a cluster. We describe and compare four metrics for evaluating the packing efficiency of schedulers in increasing order of sophistication: aggregate utilization hole filling workload inflation and cluster compaction.,http://research.google.com/pubs/archive/43103.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evaluating+job+packing+in+warehouse-scale+computing+Verma+Korupolu+Wilkes,http://research.google.com/pubs/pub43103.html
Whare-Map: Heterogeneity in “Homogeneous” Warehouse-Scale Computers,Proceedings of the 2013 ACM/IEEE International Symposium on Computer Architecture (ISCA) IEEE (to appear),2013,Jason Mars Lingjia Tang Robert Hundt,@inproceedings{41187 title = {Whare-Map: Heterogeneity in “Homogeneous” Warehouse-Scale Computers} author = {Jason Mars and Lingjia Tang and Robert Hundt} year = 2013 booktitle = {Proceedings of the 2013 ACM/IEEE International Symposium on Computer Architecture (ISCA)} },Modern “warehouse scale computers” (WSCs) continue to be embraced as homogeneous computing platforms. However due to frequent machine replacements and upgrades modern WSCs are in fact composed of diverse commodity microarchitectures and machine conﬁgurations. Yet current WSCs are architected with the assumption of homogeneity leaving a potentially signiﬁcant performance opportunity unexplored. In this paper we expose and quantify the performance impact of the “homogeneity assumption” for modern production WSCs using industry-strength large-scale web-service workloads. In addition we argue for and evaluate the beneﬁts of a heterogeneity-aware WSC using commercial web-service production workloads including Google’s websearch. We also identify key factors impacting the available performance opportunity when exploiting heterogeneity and introduce a new metric opportunity factor to quantify an application’s sensitivity to the heterogeneity in a given WSC. To exploit heterogeneity in “homogeneous” WSCs we propose “Whare-Map” the WSC Heterogeneity Aware Mapper that leverages already in-place continuous proﬁling subsystems found in production environments. When employing “Whare-Map” we observe a cluster-wide performance improvement of 15% on average over heterogeneity–oblivious job placement and up to an 80% improvement forweb-service applications that are particularly sensitive to heterogeneity,http://research.google.com/pubs/archive/41187.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Whare-Map:+Heterogeneity+in+%E2%80%9CHomogeneous%E2%80%9D+Warehouse-Scale+Computers+Mars+Tang+Hundt,http://research.google.com/pubs/pub41187.html
Google’s Innovation Factory: Testing Culture And Infrastructure,IEEE International Conference on Software Testing Verification and Validation (2010) pp. 4,2010,Patrick Copeland,@inproceedings{41672 title = {Google’s Innovation Factory: Testing Culture And Infrastructure} author = {Patrick Copeland} year = 2010 booktitle = {IEEE International Conference on Software Testing Verification and Validation} pages = {4} },Google takes quality seriously and is reinventing how software is created tested released and maintained.,http://research.google.com/pubs/archive/41672.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google%E2%80%99s+Innovation+Factory:+Testing+Culture+And+Infrastructure+Copeland,http://research.google.com/pubs/pub41672.html
Online Graph Edge-Coloring in the Random-Order Arrival Model,Theory of Computing vol. 8(1) (2012) pp. 567-595,2012,Bahman Bahmani Aranyak Mehta Rajeev Motwani,@article{40805 title = {Online Graph Edge-Coloring in the Random-Order Arrival Model} author = {Bahman Bahmani and Aranyak Mehta and Rajeev Motwani} year = 2012 URL = {http://theoryofcomputing.org/articles/v008a025/} journal = {Theory of Computing} pages = {567-595} volume = {8(1)} },A classic theorem by Vizing asserts that if the maximum degree of a graph is ? then it is possible to color its edges in polynomial time using at most ?+1 colors. However this algorithm is offline i.e. it assumes the whole graph is known in advance. A natural question then is how well we can do in the online setting where the edges of the graph are revealed one by one and we need to color each edge as soon as it is added to the graph. Online edge coloring has an important application in fast switch scheduling. A natural model is that edges arrive online but in a random permutation. Even in the random permutation model the best proven approximation factor for any algorithm is the factor 2 of the simple greedy algorithm (which holds even in the worst-case online model). The algorithm of Aggarwal et al. (FOCS'03) provides a 1+o(1) factor algorithm for the case of very dense multi-graphs when ?=?(n2) where n is the number of vertices. In this paper we show that for graphs with ?=?(logn) it is possible to color the graph with (1+ee2?1+o(1))??1.43? colors with high probability in the online random-order model. Our algorithm is inspired by a 1.6-approximate distributed offline algorithm of Panconesi and Srinivasan (PODC'92) which we extend by reusing failed colors online. Further we show how we can extend the algorithm to reuse colors multiple times which reduces the approximation factor below 1.43. We conjecture that the algorithm becomes nearly optimal (i.e. uses ?+o(?) colors) with O(log(?/logn)) reuses. We reduce the question to proving the non-negativity of a certain recursively defined sequence which looks true in computer simulations. This non-negativity can be proved explicitly for a small number of reuses giving improved algorithms: e.g. the algorithm which reuses colors 5 times uses 1.26? colors.,http://theoryofcomputing.org/articles/v008a025/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Graph+Edge-Coloring+in+the+Random-Order+Arrival+Model+Bahmani+Mehta+Motwani,http://research.google.com/pubs/pub40805.html
A Crossing-Sensitive Third-Order Factorization for Dependency Parsing,Transactions of the Association for Computational Linguistics vol. 2 (2014) pp. 41-54,2014,Emily Pitler,@article{42214 title = {A Crossing-Sensitive Third-Order Factorization for Dependency Parsing} author = {Emily Pitler} year = 2014 URL = {http://www.transacl.org/wp-content/uploads/2014/02/39.pdf} journal = {Transactions of the Association for Computational Linguistics} pages = {41--54} volume = {2} },Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers wider factorizations have so far implied large increases in the computational complexity of the parsing problem. This paper introduces a “crossing-sensitive” generalization of a third-order factorization that trades off complexity in the model structure (i.e. scoring with features over multiple edges) with complexity in the output structure (i.e. producing crossing edges). Under this model the optimal 1-Endpoint-Crossing tree can be found in O(n^4) time matching the asymptotic run-time of both the third-order projective parser and the edge-factored 1-Endpoint-Crossing parser. The crossing-sensitive third-order parser is significantly more accurate than the third-order projective parser under many experimental settings and significantly less accurate on none.,http://research.google.com/pubs/archive/42214.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Crossing-Sensitive+Third-Order+Factorization+for+Dependency+Parsing+Pitler,http://research.google.com/pubs/pub42214.html
Guarded Program Transformations Using JTL,TOOLS EUROPE 2008 (LNBIP 11) Springer-Verlag Berlin pp. 100-120,2008,Tal Cohen Joseph (Yossi) Gil Itay Maman,@inproceedings{34438 title = {Guarded Program Transformations Using JTL} author = {Tal Cohen and Joseph (Yossi) Gil and Itay Maman} year = 2008 URL = {http://tal.forum2.org/static/cv/JTL-Transformations.pdf} booktitle = {TOOLS EUROPE 2008 (LNBIP 11)} pages = {100--120} address = {Berlin} },There is a growing research interest in employing the logic paradigm for making queries on software in general and OOP software in particular. We describes a side-effect-free technique of using the paradigm for the general task of program transformation. Our technique offers a variety of applications such as implementing generic structures (without erasure) in JAVA a Lint-like program checker and more. By allowing the transformation target to be a different language than the source (program translation) we show how the language can be employed for tasks like the generation of database schemas or XML DTDs that match JAVA classes. The technique is an extension of JTL (Java Tools Language) which is a high-level abstraction over DATALOG. We discuss the JTL-to-DATALOG compilation process and how the program transformation extension can be added to JTL without deviating from the logic paradigm and specifically without introducing side-effects to logic programs.,http://research.google.com/pubs/archive/34438.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Guarded+Program+Transformations+Using+JTL+Cohen+Gil+Maman,http://research.google.com/pubs/pub34438.html
The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines,Morgan & Claypool Publishers (2009),2009,Luiz André Barroso Urs Hölzle,@book{35290 title = {The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines} author = {Luiz André Barroso and Urs Hölzle} year = 2009 booktitle = {The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines} },As computation continues to move into the cloud the computing platform of interest no longer resembles a pizza box or a refrigerator but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance something that can only be achieved by a holistic approach to their design and deployment. In other words we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs the main factors influencing their design operation and cost structure and the characteristics of their software base. We hope it will be useful to architects and programmers of today's WSCs as well as those of future many-core platforms which may one day implement the equivalent of today's WSCs on a single board.,http://dx.doi.org/10.2200/S00193ED1V01Y200905CAC006,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Datacenter+as+a+Computer:+An+Introduction+to+the+Design+of+Warehouse-Scale+Machines+Barroso+Holzle,http://research.google.com/pubs/pub35290.html
Mathematics at Google,Google Inc. (2012) pp. 52,2012,Javier Tordable,@misc{38331 title = {Mathematics at Google} author = {Javier Tordable} year = 2012 URL = {http://www.javiertordable.com/files/MathematicsAtGoogle.pdf} },There is a wide variety of Mathematics used at Google. For example Linear Algebra in the PageRank algorithm used to rank web pages in search results. Or Game Theory used in ad auctions or Graph Theory in Google Maps. At Google there are literally dozens of products which use interesting Mathematics. These are not just research prototypes but real Google products; in which Mathematics play a crucial role. In this presentation I introduce several applications of Mathematics at Google. I begin with a detailed explanation of search on the web and PageRank. Then I show a dozen examples of Google products and the corresponding Mathematics that are used. The presentation has an extensive list of links and references. And it's available in English and Spanish.,http://research.google.com/pubs/archive/38331.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mathematics+at+Google+Tordable,http://research.google.com/pubs/pub38331.html
Effective Perl Programming 2nd Edition,Addison-Wesley Professional (2010) pp. 445,2010,Joshua McAdams brian d foy Joseph Hall,@book{40670 title = {Effective Perl Programming 2nd Edition} author = {Joshua McAdams and brian d foy and Joseph Hall} year = 2010 pages = {445} },The Classic Guide to Solving Real-World Problems with Perl - Now Fully Updated for Today;s Best Idioms! For years experienced programmers have relied on Effective Perl Programming to discover better ways to solve problems with Perl. Now in this long-awaited second edition three renowned Perl programmers bring together today's best idioms techniques and examples: everything you need to write more powerful fluent expressive and succinct code with Perl. Nearly twice the size of the first edition Effective Perl Programming Second Edition offers everything from rules of thumb to avoid common pitfalls to the latest wisdom for using Perl modules. You won't just learn the right ways to use Perl: You'll learn why these approaches work so well. New coverage in this edition includes - Reorganized and expanded material spanning twelve years of Perl evolution - Eight new chapters on CPAN databases distributions files and filehandles production Perl testing Unicode and warnings - Updates for Perl 5.12 the latest version of Perl Systematically updated examples reflecting today's best idioms You'll learn how to work with strings numbers lists arrays strictures namespaces regular expressions subroutines references distributions inline code warnings Perl::Tidy data munging Perl one-liners and a whole lot more. Every technique is organized in the same Items format that helped make the first edition so convenient and popular.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Effective+Perl+Programming+2nd+Edition+McAdams+foy+Hall,http://research.google.com/pubs/pub40670.html
Hokusai | Sketching Streams in Real Time,Proceedings of the 28th International Conference on Conference on Uncertainty in Artificial Intelligence (UAI) (2012),2012,Sergiy Matusevych Alex Smola Amr Ahmed,@inproceedings{38346 title = {Hokusai | Sketching Streams in Real Time} author = {Sergiy Matusevych and Alex Smola and Amr Ahmed} year = 2012 URL = {http://www.cs.cmu.edu/~amahmed/papers/hokusai.pdf} booktitle = {Proceedings of the 28th International Conference on Conference on Uncertainty in Artificial Intelligence (UAI)} },We describe ã Hokusai a real time system which is able to capture frequency information for streams of arbitrary sequences of symbols. The algorithm uses the Count-Min sketch as its basis and exploits the fact that sketching is linear. It provides real time statistics of arbitrary events e.g. streams of queries as a function of time. We use a factorizing approximation to provide point estimates at arbitrary (time item) combinations.,http://research.google.com/pubs/archive/38346.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hokusai+%7C+Sketching+Streams+in+Real+Time+Matusevych+Smola+Ahmed,http://research.google.com/pubs/pub38346.html
Efficient Spectral Neighborhood Blocking for Entity Resolution,International Conference on Data Engineering 2011 (ICDE) IEEE pp. 1-12,2011,Liangcai Shu Aiyou Chen Ming Xiong Weiyi Meng,@inproceedings{36940 title = {Efficient Spectral Neighborhood Blocking for Entity Resolution} author = {Liangcai Shu and Aiyou Chen and Ming Xiong and Weiyi Meng} year = 2011 booktitle = {International Conference on Data Engineering 2011 (ICDE)} pages = {1-12} },In many telecom and web applications there is a need to identify whether data objects in the same source or different sources represent the same entity in the real world. This problem arises for subscribers in multiple services customers in supply chain management and users in social networks when there lacks a unique identifier across multiple data sources to represent a real-world entity. Entity resolution is to identify and discover objects in the data sets that refer to the same entity in the real world. We investigate the entity resolution problem for large data sets where efficient and scalable solutions are needed. We propose a novel unsupervised blocking algorithm namely SPectrAl Neighborhood (SPAN) which constructs a fast bipartition tree for the records based on spectral clustering such that real entities can be identified accurately by neighborhood records in the tree. There are two major novel aspects in our approach: 1) We develop a fast algorithm that performs spectral clustering without computing pairwise similarities explicitly which dramatically improves the scalability of the standard spectral clustering algorithm; 2) We utilize a stopping criterion specified by Newman-Girvan modularity in the bipartition process. Our experimental results with both synthetic and real-world data demonstrate that SPAN is robust and outperforms other blocking algorithms in terms of accuracy while it is efficient and scalable to deal with large data sets.,http://research.google.com/pubs/archive/36940.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Spectral+Neighborhood+Blocking+for+Entity+Resolution+Shu+Chen+Xiong+Meng,http://research.google.com/pubs/pub36940.html
Discriminative learning can succeed where generative learning fails,Information Processing Letters vol. 103(4) (2007) pp. 131-135,2007,Philip M. Long Rocco A. Servedio Hans Ulrich Simon,@article{32733 title = {Discriminative learning can succeed where generative learning fails} author = {Philip M. Long and Rocco A. Servedio and Hans Ulrich Simon} year = 2007 URL = {http://www.phillong.info/publications/LSS07_disc_vs_gen.pdf} journal = {Information Processing Letters} pages = {131-135} volume = {103(4)} },Generative algorithms for learning classifiers use training data to separately estimate a probability model for each class. New items are classified by comparing their probabilities under these models. In contrast discriminative learning algorithms try to find classifiers that perform well on all the training data. We show that there is a learning problem that can be solved by a discriminative learning algorithm but not by any generative learning algorithm. This statement is formalized using a framework inspired by previous work of Goldberg.,http://research.google.com/pubs/archive/32733.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discriminative+learning+can+succeed+where+generative+learning+fails+Long+Servedio+Simon,http://research.google.com/pubs/pub32733.html
On the design of the ECMAScript Reflection API,TOPLAS (Transactions on Programming Languages and Systems) (2012),2012,Tom Van Cutsem Mark S. Miller,@article{37741 title = {On the design of the ECMAScript Reflection API} author = {Tom Van Cutsem and Mark S. Miller} year = 2012 journal = {TOPLAS (Transactions on Programming Languages and Systems)} },We describe in detail the new reflection API of the upcoming Javascript standard. The most prominent feature of this new API is its support for creating proxies: virtual objects that behave as regular objects but whose entire “meta-object protocol” is implemented in Javascript itself. Next to a detailed description of the API we describe a more general set of design principles that helped steer the API’s design and which should be applicable to similar APIs for other languages. We also describe access control abstractions implemented in the new API and provide an operational semantics of an extension of the untyped lambda-calculus featuring proxies.,http://research.google.com/pubs/archive/37741.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+design+of+the+ECMAScript+Reflection+API+Cutsem+Miller,http://research.google.com/pubs/pub37741.html
EXPERTS AND THEIR RECORDS,Economic Inquiry (2013) (to appear),2013,Michael Schwarz Alex Frankel,@article{41682 title = {EXPERTS AND THEIR RECORDS} author = {Michael Schwarz and Alex Frankel} year = 2013 journal = {Economic Inquiry} },A market where short-lived customers interact with long-lived experts is considered. Experts privately observe which treatment best serves a customer but are free to choose more or less profitable treatments. Customers only observe records of experts' past actions. If experts are homogeneous there exists an equilibrium where experts always choose the customer's preferred treatment (play truthfully). Experts are incentivized with the promise of future business: new customers tend to choose experts who performed less profitable treatments in the past. If expert payoffs are private information experts can never always be truthful. But sufficiently patient experts may be truthful almost always.,http://research.google.com/pubs/archive/41682.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=EXPERTS+AND+THEIR+RECORDS+Schwarz+Frankel,http://research.google.com/pubs/pub41682.html
Adversary Lower Bound for the k-sum Problem,Proceeding of 4th Annual ACM Conference on Innovations in Theoretical Computer Science (ITCS'13) (2013) pp. 323-328,2013,Aleksandrs Belovs Robert Spalek,@inproceedings{41471 title = {Adversary Lower Bound for the k-sum Problem} author = {Aleksandrs Belovs and Robert Spalek} year = 2013 URL = {http://arxiv.org/abs/1206.6528} note = {arXiv:1206.6528 [quant-ph]} booktitle = {Proceeding of 4th Annual ACM Conference on Innovations in Theoretical Computer Science (ITCS'13)} pages = {323-328} },We prove a tight quantum query lower bound Ω(n^(k/(k+1))) for the problem of deciding whether there exist k numbers among n that sum up to a prescribed number provided that the alphabet size is sufficiently large.,http://research.google.com/pubs/archive/41471.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adversary+Lower+Bound+for+the+k-sum+Problem+Belovs+Spalek,http://research.google.com/pubs/pub41471.html
Large Scale Online Learning of Image Similarity Through Ranking: Extended Abstract,4th Iberian Conference on Pattern Recognition and Image Analysis IbPRIA (2009),2009,Gal Chechik Varun Sharma Uri Shalit Samy Bengio,@inproceedings{35267 title = {Large Scale Online Learning of Image Similarity Through Ranking: Extended Abstract} author = {Gal Chechik and Varun Sharma and Uri Shalit and Samy Bengio} year = 2009 booktitle = {4th Iberian Conference on Pattern Recognition and Image Analysis IbPRIA} },Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. Pairwise similarity plays a crucial role in classification algorithms like nearest neighbors and is practically important for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks users look for objects that are both visually similar and semantically related to a given object. Unfortunately current approaches for learning semantic similarity are limited to small scale datasets because their complexity grows quadratically with the sample size and because they impose costly positivity constraints on the learned similarity functions. To address real-world large-scale AI problem like learning similarity over all images on the web we need to develop new algorithms that scale to many samples many classes and many features. The current abstract presents OASIS an {\em Online Algorithm for Scalable Image Similarity} learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a dataset with thousands of images it achieves better results than existing state-of-the-art methods while being an order of magnitude faster. Comparing OASIS with different symmetric variants provides unexpected insights into the effect of symmetry on the quality of the similarity. For large web scale datasets OASIS can be trained on more than two million images from 150K text queries within two days on a single CPU. Human evaluations showed that 35\% of the ten top images ranked by OASIS were semantically relevant to a query image. This suggests that query-independent similarity could be accurately learned even for large-scale datasets that could not be handled before.,http://research.google.com/pubs/archive/35267.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Online+Learning++of+Image+Similarity+Through+Ranking:+Extended+Abstract+Chechik+Sharma+Shalit+Bengio,http://research.google.com/pubs/pub35267.html
Model-Based Aligner Combination Using Dual Decomposition,Proceedings of the Association for Computational Linguistics (ACL) 2011,2011,John DeNero Klaus Macherey,@inproceedings{37256 title = {Model-Based Aligner Combination Using Dual Decomposition} author = {John DeNero and Klaus Macherey} year = 2011 URL = {http://dl.acm.org/citation.cfm?id=2002526} booktitle = {Proceedings of the Association for Computational Linguistics (ACL) 2011} },Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.,http://research.google.com/pubs/archive/37256.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Model-Based+Aligner+Combination+Using+Dual+Decomposition+DeNero+Macherey,http://research.google.com/pubs/pub37256.html
Top-k Publish-Subscribe for Social Annotation of News,Proceedings of the 39th International Conference on Very Large Data Bases VLDB Endowment (2013),2013,Alexander Shraer Maxim Gurevich Marcus Fontoura Vanja Josifovski,@inproceedings{40815 title = {Top-k Publish-Subscribe for Social Annotation of News} author = {Alexander Shraer and Maxim Gurevich and Marcus Fontoura and Vanja Josifovski} year = 2013 booktitle = {Proceedings of the 39th International Conference on Very Large Data Bases} },Social content such as Twitter updates often have the quickest first-hand reports of news events as well as numerous commentaries that are indicative of public view of such events. As such social updates provide a good complement to professionally written news articles. In this paper we consider the problem of automatically annotating news stories with social updates (tweets) at a news website serving high volume of pageviews. The high rate of both the pageviews (millions to billions a day) and of the incoming tweets (more than 100 millions a day) make real-time indexing of tweets ineffective as this requires an index that is both queried and updated extremely frequently. The rate of tweet updates makes caching techniques almost unusable since the cache would become stale very quickly. We propose a novel architecture where each story is treated as a subscription for tweets relevant to the story's content and new algorithms that efficiently match tweets to stories proactively maintaining the top-k tweets for each story. Such {\em top-k pub-sub} consumes only a small fraction of the resource cost of alternative solutions and can be applicable to other large scale content-based publish-subscribe problems. We demonstrate the effectiveness of our approach on real-world data: a corpus of news stories from Yahoo! News and a log of Twitter updates.,http://research.google.com/pubs/archive/40815.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Top-k+Publish-Subscribe+for++Social+Annotation+of+News+Shraer+Gurevich+Fontoura+Josifovski,http://research.google.com/pubs/pub40815.html
Google Squared: web scale open domain information extraction and presentation,European Conference on Information Retrieval Industry Day (2010),2010,Dan Crow,@inproceedings{36641 title = {Google Squared: web scale open domain information extraction and presentation} author = {Dan Crow} year = 2010 booktitle = {European Conference on Information Retrieval Industry Day} },Google Squared performs open domain information extraction at massive scale. This paper gives an overview of the techniques used and the user interface developed to help users navigate complex information about multiple comparable entities.,http://research.google.com/pubs/archive/36641.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+Squared:+web+scale+open+domain+information+extraction+and+presentation+Crow,http://research.google.com/pubs/pub36641.html
Inferring semantic mapping between policies and code: the clue is in the language,International Symposium on Engineering Secure Software and Systems Springer (2015),2015,Pauline Anthonysamy Matthew Edwards Chris Weichel Awais Rashid,@inproceedings{44673 title = {Inferring semantic mapping between policies and code: the clue is in the language} author = {Pauline Anthonysamy and Matthew Edwards and Chris Weichel and Awais Rashid} year = 2015 booktitle = {International Symposium on Engineering Secure Software and Systems} },A common misstep in the development of security and privacy solutions is the failure to keep the demands resulting from high-level policies in line with the actual implementation that is supposed to operationalize those policies. This is especially problematic in the domain of social networks where software typically predates policies and then evolves alongside its user base and any changes in policies that arise from their interactions with (and the demands that they place on) the system. Our contribution targets this specific problem drawing together the assurances actually presented to users in the form of policies and the large codebases with which developers work. We demonstrate that a mapping between policies and code can be inferred from the semantics of the natural language. These semantics manifest not only in the policy statements but also coding conventions. Our technique implemented in a tool (CASTOR ) can infer semantic mappings with F1 accuracy of 70% and 78% for two social networks Diaspora and Friendica respectively – as compared with a ground truth mapping established through manual examination of the policies and code.,http://research.google.com/pubs/archive/44673.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Inferring+semantic+mapping+between+policies+and+code:+the+clue+is+in+the+language+Anthonysamy+Edwards+Weichel+Rashid,http://research.google.com/pubs/pub44673.html
Periodic Measurement of Advertising Effectiveness Using Multiple-Test-Period Geo Experiments,Google Inc. (2012),2012,Jon Vaver Jim Koehler,@techreport{38356 title = {Periodic Measurement of Advertising Effectiveness Using Multiple-Test-Period Geo Experiments} author = {Jon Vaver and Jim Koehler} year = 2012 institution = {Google Inc.} },In a previous paper [6] we described the application of geo experiments to the measurement of advertising effectiveness. One reason this method of measurement is attractive is that it provides the rigor of a randomized experiment. However related decisions such as where and how to spend advertising budget are not static. To address this issue we extend this methodology to provide periodic (ongoing) measurement of ad effectiveness. In this approach the test and control assignments of each geographic region rotate across multiple test periods and these rotations provide the opportunity to generate a sequence of measurements of campaign effectiveness. The data across test periods can also be pooled to create a single aggregate measurement of campaign effectiveness. These sequential and pooled measurements have smaller confidence intervals than measurements from a series of geo experiments with a single test period. Alternatively the same confidence interval can be achieved with a reduced magnitude and/or duration of ad spend change thereby lowering the cost of measurement. The net result is a better method for periodic and isolated measurement of ad effectiveness.,http://research.google.com/pubs/archive/38356.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Periodic+Measurement+of+Advertising+Effectiveness+Using+Multiple-Test-Period+Geo+Experiments+Vaver+Koehler,http://research.google.com/pubs/pub38356.html
Large-scale Discriminative Language Model Reranking for Voice Search,Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT Association for Computational Linguistics pp. 41-49,2012,Preethi Jyothi Leif Johnson Ciprian Chelba Brian Strope,@inproceedings{38145 title = {Large-scale Discriminative Language Model Reranking for Voice Search} author = {Preethi Jyothi and Leif Johnson and Ciprian Chelba and Brian Strope} year = 2012 URL = {http://www.aclweb.org/anthology/W12-2706} booktitle = {Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT} pages = {41-49} },We present a distributed framework for large-scale discriminative language models that can be integrated within a large vocabulary continuous speech recognition (LVCSR) system using lattice rescoring. We intentionally use a weakened acoustic model in a baseline LVCSR system to generate candidate hypotheses for voice-search data; this allows us to utilize large amounts of unsupervised data to train our models. We propose an efficient and scalable MapReduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data. We report small but significant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model. We also provide an analysis of the various parameters of our models including model size types of features size of partitions in the MapReduce framework with the help of supporting experiments.,http://research.google.com/pubs/archive/38145.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-scale+Discriminative+Language+Model+Reranking+for+Voice+Search+Jyothi+Johnson+Chelba+Strope,http://research.google.com/pubs/pub38145.html
Pose Embeddings: A Deep Architecture for Learning to Match Human Poses,arXiv (2015),2015,Greg Mori Caroline Pantofaru Nisarg Kothari Thomas Leung George Toderici Alexander Toshev Weilong Yang,@inproceedings{44002 title = {Pose Embeddings: A Deep Architecture for Learning to Match Human Poses} author = {Greg Mori and Caroline Pantofaru and Nisarg Kothari and Thomas Leung and George Toderici and Alexander Toshev and Weilong Yang} year = 2015 URL = {http://arxiv.org/abs/1507.00302} booktitle = {arXiv} },We present a method for learning an embedding that places images of humans in similar poses nearby. This embedding can be used as a direct method of comparing images based on human pose avoiding potential challenges of estimating body joint positions. Pose embedding learning is formulated under a triplet-based distance criterion. A deep architecture is used to allow learning of a representation capable of making distinctions between different poses. Experiments on human pose matching and retrieval from video data demonstrate the potential of the method.,http://research.google.com/pubs/archive/44002.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Pose+Embeddings:+A+Deep+Architecture+for+Learning+to+Match+Human+Poses+Mori+Pantofaru+Kothari+Leung+Toderici+Toshev+Yang,http://research.google.com/pubs/pub44002.html
Why Feedback Implementations Fail: The Importance of Systematic Testing,Workshop on Feedback Control Implementation and Design (at EuroSys) ACM (2010),2010,Joseph L. Hellerstein,@inproceedings{36244 title = {Why Feedback Implementations Fail: The Importance of Systematic Testing} author = {Joseph L. Hellerstein} year = 2010 booktitle = {Workshop on Feedback Control Implementation and Design (at EuroSys)} },Over the last decade there has been great progress in using formal methods from control theory to design closed loops in software systems. Despite this progress formal methods are rarely used by software practitioners. One reason is the substantial risk of making changes to closed loops in software products code that is typically complex and performance sensitive. We argue that broad adoption of formal methods for controller design require addressing how to reduce the risk of making changes in controller implementations. To this end we propose a framework for testing controller implementations that focuses on scenario coverage scenario evaluation and runtime efficiencies. We give examples of applying this framework to the Microsoft .NET Thread Pool the Google Cluster Manager and a Google stream processing system.,http://research.google.com/pubs/archive/36244.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+Feedback+Implementations+Fail:+The+Importance+of+Systematic+Testing+Hellerstein,http://research.google.com/pubs/pub36244.html
Non-Price Equilibria in Markets of Discrete Goods,EC (2011),2011,Avinatan Hassidim Haim Kaplan Yishay Mansour Noam Nisan,@inproceedings{37653 title = {Non-Price Equilibria in Markets of Discrete Goods} author = {Avinatan Hassidim and Haim Kaplan and Yishay Mansour and Noam Nisan} year = 2011 booktitle = {EC} },We study markets of indivisible items in which price-based (Walrasian) equilibria often do not exist due to the discrete non-convex setting. Instead we consider Nash equilibria of the market viewed as a game where players bid for items and where the highest bidder on an item wins it and pays his bid. We first observe that pure Nash-equilibria of this game excatly correspond to price-based equilibiria (and thus need not exist) but that mixed-Nash equilibria always do exist and we analyze their structure in several simple cases where no price-based equilibrium exists. We also undertake an analysis of the welfare properties of these equilibria showing that while pure equilibria are always perfectly efficient (“first welfare theorem”) mixed equilibria need not be and we provide upper and lower bounds on their amount of inefficiency.,http://research.google.com/pubs/archive/37653.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Non-Price+Equilibria+in+Markets+of+Discrete+Goods+Hassidim+Kaplan+Mansour+Nisan,http://research.google.com/pubs/pub37653.html
Buildling adaptive dialogue systems via Bayes-adaptive POMDP,IEEE Journal of Selected Topics in Signal Processing vol. vol.6(8). 2012. (2012) pp. 917-927,2012,Shaowei Png Joelle Pineau B. Chaib-draa,@article{40680 title = {Buildling adaptive dialogue systems via Bayes-adaptive POMDP} author = {Shaowei Png and Joelle Pineau and B. Chaib-draa} year = 2012 journal = {IEEE Journal of Selected Topics in Signal Processing} pages = {917-927} volume = {vol.6(8). 2012.} },Recent research has shown that effective dialogue management can be achieved through the Partially Observable Markov Decision Process (POMDP) framework. However past research on POMDP-based dialogue systems usually assumed the parameters of the decision process were known a priori. The main contribution of this paper is to present a Bayesian reinforcement learning framework for learning the POMDP parameters online from data in a decision-theoretic manner. We discuss various approximations and assumptions which can be leveraged to ensure computational tractability and apply these techniques to learning observation models for several simulated spoken dialogue domains.,http://research.google.com/pubs/archive/40680.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Buildling+adaptive+dialogue+systems+via+Bayes-adaptive+POMDP+Png+Pineau+Chaib-draa,http://research.google.com/pubs/pub40680.html
The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training,Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS) JMLR Workshop and Conference Procedings (2009) pp. 153-160,2009,Dumitru Erhan Pierre-Antoine Manzagol Yoshua Bengio Samy Bengio Pascal Vincent,@inproceedings{34923 title = {The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training} author = {Dumitru Erhan and Pierre-Antoine Manzagol and Yoshua Bengio and Samy Bengio and Pascal Vincent} year = 2009 URL = {http://jmlr.csail.mit.edu/proceedings/papers/v5/erhan09a/erhan09a.pdf} booktitle = {Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS)} pages = {153--160} },Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training. Even though these new algorithms have enabled training deep models many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the influence of pre-training with respect to architecture depth model capacity and number of training examples.,http://research.google.com/pubs/archive/34923.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Difficulty+of+Training+Deep+Architectures+and+the+Effect+of+Unsupervised+Pre-Training+Erhan+Manzagol+Bengio+Bengio+Vincent,http://research.google.com/pubs/pub34923.html
PseudoID: Enhancing Privacy in Federated Login,Hot Topics in Privacy Enhancing Technologies (2010) pp. 95-107,2010,Arkajit Dey Stephen Weis,@inproceedings{36553 title = {PseudoID: Enhancing Privacy in Federated Login} author = {Arkajit Dey and Stephen Weis} year = 2010 URL = {http://www.pseudoid.net} booktitle = {Hot Topics in Privacy Enhancing Technologies} pages = {95--107} },PseudoID is a federated login system that protects users from disclosure of private login data held by identity providers. We offer a proof of concept implementation of PseudoID based on blind digital signatures that is backward-compatible with a popular federated login system named OpenID. We also propose several extensions and discuss some of the practical challenges that must be overcome to further protect user privacy in federated login systems.,http://research.google.com/pubs/archive/36553.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=PseudoID:+Enhancing+Privacy+in+Federated+Login+Dey+Weis,http://research.google.com/pubs/pub36553.html
Directly Modeling Voiced and Unvoiced Components in Speech Waveforms by Neural Networks,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2016) (to appear),2016,Keiichi Tokuda Heiga Zen,@inproceedings{44808 title = {Directly Modeling Voiced and Unvoiced Components in Speech Waveforms by Neural Networks} author = {Keiichi Tokuda and Heiga Zen} year = 2016 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} },This paper proposes a novel acoustic model based on neural networks for statistical parametric speech synthesis. The neural network outputs parameters of a non-zero mean Gaussian process which defines a probability density function of a speech waveform given linguistic features. The mean and covariance functions of the Gaussian process represent deterministic (voiced) and stochastic (unvoiced) components of a speech waveform whereas the previous approach considered the unvoiced component only. Experimental results show that the proposed approach can generate speech waveforms approximating natural speech waveforms.,http://research.google.com/pubs/archive/44808.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Directly+Modeling+Voiced+and+Unvoiced+Components+in+Speech+Waveforms+by+Neural+Networks+Tokuda+Zen,http://research.google.com/pubs/pub44808.html
On Multiway Cut paramterized above lower bounds,IPEC 2011 (to appear),2011,Marek Cygan Marcin Pilipczuk Micha_ Pilipczuk Jakub Onufry Wojtaszczyk,@inproceedings{37467 title = {On Multiway Cut paramterized above lower bounds} author = {Marek Cygan and Marcin Pilipczuk and Micha_ Pilipczuk and Jakub Onufry Wojtaszczyk} year = 2011 URL = {http://arxiv.org/abs/1107.1585} booktitle = {IPEC 2011} },In this paper we consider two above lower bound parameterizations of Node Multiway Cut — above maximum separating cut and above a natural LP-relaxation — and prove them to be fixed-parameter tractable. Our results imply O(4^k) algorithms for Vertex Cover above Maximum Matching and Almost 2-SAT as well as a O*(2^k) algorithm for Node Multiway Cut with a standard parameterization by the solution size improving previous bounds for these problems.,http://arxiv.org/abs/1107.1585,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+Multiway+Cut+paramterized+above+lower+bounds+Cygan+Pilipczuk+Pilipczuk+Wojtaszczyk,http://research.google.com/pubs/pub37467.html
Show and tell: A neural image caption generator,Computer Vision and Pattern Recognition (2015),2015,Oriol Vinyals Alexander Toshev Samy Bengio Dumitru Erhan,@inproceedings{43274 title = {Show and tell: A neural image caption generator} author = {Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan} year = 2015 URL = {http://arxiv.org/abs/1411.4555} booktitle = {Computer Vision and Pattern Recognition} },Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate which we verify both qualitatively and quantitatively. For instance while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25 our approach yields 59 to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k from 56 to 66 and on SBU from 19 to 28. Lastly on the newly released COCO dataset we achieve a BLEU-4 of 27.7 which is the current state-of-the-art.,http://research.google.com/pubs/archive/43274.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Show+and+tell:+A+neural+image+caption+generator+Vinyals+Toshev+Bengio+Erhan,http://research.google.com/pubs/pub43274.html
Non-Parametric Parametricity,Journal of Funcitonal Programming vol. 21 (4 & 5) (2011),2011,Georg Neis Derek Dreyer Andreas Rossberg,@article{43983 title = {Non-Parametric Parametricity} author = {Georg Neis and Derek Dreyer and Andreas Rossberg} year = 2011 URL = {http://journals.cambridge.org/action/displayIssue?decade=2010&jid=JFP&volumeId=21&issueId=4-5&iid=8368537} journal = {Journal of Funcitonal Programming} volume = {21 (4 & 5)} },Type abstraction and intensional type analysis are features seemingly at odds—type abstraction is intended to guarantee parametricity and representation independence while type analysis is inherently non-parametric. Recently however several researchers have proposed and implemented “dynamic type generation” as a way to reconcile these features. The idea is that when one defines an abstract type one should also be able to generate at run time a fresh type name which may be used as a dynamic representative of the abstract type for purposes of type analysis. The question remains: in a language with non-parametric polymorphism does dynamic type generation provide us with the same kinds of abstraction guarantees that we get from parametric polymorphism? Our goal is to provide a rigorous answer to this question. We define a step-indexed Kripke logical relation for a language with both non-parametric polymorphism (in the form of type-safe cast) and dynamic type generation. Our logical relation enables us to establish parametricity and representation independence results even in a non-parametric setting by attaching arbitrary relational interpretations to dynamically-generated type names. In addition we explore how programs that are provably equivalent in a more traditional parametric logical relation may be “wrapped” systematically to produce terms that are related by our non-parametric relation and vice versa. This leads us to develop a “polarized” variant of our logical relation which enables us to distinguish formally between positive and negative notions of parametricity.,http://research.google.com/pubs/archive/43983.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Non-Parametric+Parametricity+Neis+Dreyer+Rossberg,http://research.google.com/pubs/pub43983.html
Combinational Collaborative Filtering for Personalized Community Recommendation,ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining (KDD) ACM (2008) pp. 115-123,2008,Wen-Yen Chen Dong Zhang Edward Chang,@inproceedings{34704 title = {Combinational Collaborative Filtering for Personalized Community Recommendation} author = {Wen-Yen Chen and Dong Zhang and Edward Chang} year = 2008 booktitle = {ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining (KDD)} pages = {115--123} },Rapid growth in the amount of data available on social networking sites has made information retrieval increasingly challenging for users. In this paper we propose a collaborative ltering method Combinational Collaborative Filtering (CCF) to perform personalized community recommendations by considering multiple types of co-occurrences in social data at the same time. This ltering method fuses semantic and user information then applies a hybrid training strategy that combines Gibbs sampling and Expectation-Maximization algorithm. To handle the large-scale dataset parallel computing is used to speed up the model training. Through an empirical study on the Orkut dataset we show,http://research.google.com/pubs/archive/34704.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Combinational+Collaborative+Filtering+for+Personalized+Community+Recommendation+Chen+Zhang+Chang,http://research.google.com/pubs/pub34704.html
MRPSO: MapReduce Particle Swarm Optimization,Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2007) IEEE,2007,Andrew W. McNabb Christopher K. Monson Kevin D. Seppi,@inproceedings{37207 title = {MRPSO: MapReduce Particle Swarm Optimization} author = {Andrew W. McNabb and Christopher K. Monson and Kevin D. Seppi} year = 2007 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2007)} },Abstract— In optimization problems involving large amounts of data such as web content commercial transaction information or bioinformatics data individual function evaluations may take minutes or even hours. Particle Swarm Optimization (PSO) must be parallelized for such functions. However large-scale parallel programs must communicate efﬁciently balance work across all processors and address problems such as failed nodes. We present MapReduce Particle Swarm Optimization (MRPSO) a PSO implementation based on the MapReduce parallel programming model. We describe MapReduce and show how PSO can be naturally expressed in this model without explicitly addressing any of the details of parallelization. We present a benchmark function for evaluating MRPSO and note that MRPSO is not appropriate for optimizing easily evaluated functions. We demonstrate that MRPSO scales to 256 processors on moderately difﬁcult problems and tolerates node failures.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MRPSO:+MapReduce+Particle+Swarm+Optimization+McNabb+Monson+Seppi,http://research.google.com/pubs/pub37207.html
Magda: A New Language for Modularity,Lecture Notes in Computer Science Springer (2012) pp. 560-588,2012,Jarek Kusmierek Viviana Bono Mauro Mulatero,@inproceedings{40595 title = {Magda: A New Language for Modularity} author = {Jarek Kusmierek and Viviana Bono and Mauro Mulatero} year = 2012 booktitle = {Lecture Notes in Computer Science} pages = {560-588} },We introduce Magda a modularity-oriented programming language. The language features lightweight mixins as units of code reuse modular initial- ization protocols and a hygienic approach to identiers. In particular Magda's modularity guarantees that client code of a library written in Magda will never break as a consequence of any addition of members to the library's mixins.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Magda:+A+New+Language+for+Modularity+Kusmierek+Bono+Mulatero,http://research.google.com/pubs/pub40595.html
improper deep learning,The 19th International Conference on Artificial Intelligence and Statist (2016) (to appear),2016,Uri Heinemann Roi Livni Amir Globerson Gal Elidan,@inproceedings{44847 title = {improper deep learning} author = {Uri Heinemann and Roi Livni and Amir Globerson and Gal Elidan} year = 2016 booktitle = {The 19th International Conference on Artificial Intelligence and Statist} },Neural networks have recently re-emerged as a powerful hypothesis class yielding impressive classification accuracy in multiple domains. However their training is a non convex optimization problem. Here we address this difficulty by turning to ”improper learning” of neural nets. In other words we learn a classifier that is not a neural net but is competitive with the best neural net model given a sufficient number of training examples. Our approach relies on a novel kernel which integrates over the set of possible neural models. It turns out that the corresponding integral can be evaluated in closed form via a simple recursion. The learning problem is then an SVM with this kernel and a global optimum can thus be found efficiently. We also provide sample complexity results which depend on the stability of the optimal neural net.,http://research.google.com/pubs/archive/44847.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=improper+deep+learning+Heinemann+Livni+Globerson+Elidan,http://research.google.com/pubs/pub44847.html
Directly Modeling Speech Waveforms by Neural Networks for Statistical Parametric Speech Synthesis,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE (2015) pp. 4215-4219,2015,Keiichi Tokuda Heiga Zen,@inproceedings{43267 title = {Directly Modeling Speech Waveforms by Neural Networks for Statistical Parametric Speech Synthesis} author = {Keiichi Tokuda and Heiga Zen} year = 2015 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4215--4219} },This paper proposes a novel approach for directly-modeling speech at the waveform level using a neural network. This approach uses the neural network-based statistical parametric speech synthesis framework with a specially designed output layer. As acoustic feature extraction is integrated to acoustic model training it can overcome the limitations of conventional approaches such as two-step (feature extraction and acoustic modeling) optimization use of spectra rather than waveforms as targets use of overlapping and shifting frames as unit and fixed decision tree structure. Experimental results show that the proposed approach can directly maximize the likelihood defined at the waveform domain.,http://research.google.com/pubs/archive/43267.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Directly+Modeling+Speech+Waveforms+by+Neural+Networks+for+Statistical+Parametric+Speech+Synthesis+Tokuda+Zen,http://research.google.com/pubs/pub43267.html
A Subjective Study for the Design of Multi-resolution ABR Video Streams with the VP9 Codec,SPIE Electronic Imaging Human Visual Perception (2016) (to appear),2016,Chao Chen Sasi Inguva Andrew Rankin Anil Kokaram,@inproceedings{44278 title = {A Subjective Study for the Design of Multi-resolution ABR Video Streams with the VP9 Codec} author = {Chao Chen and Sasi Inguva and Andrew Rankin and Anil Kokaram} year = 2016 booktitle = {SPIE Electronic Imaging Human Visual Perception} },Adaptive bitrate (ABR) streaming is one enabling technology for video streaming over modern throughput-varying communication networks. A widely used ABR streaming method is to adapt the video bitrate to channel throughput by dynamically changing the video resolution. Since videos have different rate-quality performances at different resolutions such ABR strategy can achieve better rate-quality trade-off than single resolution ABR streaming. The key problem for resolution switched ABR is to work out the bitrate appropriate at each resolution. In this paper we investigate optimal strategies to estimate this bitrate using both quantitative and subjective quality assessment. We use the design of 2K and 4K bitrates as an example of the performance of this strategy. We introduce strategies for selecting an appropriate corpus for subjective assessment and find that at this high resolution there is good agreement between quantitative and subjective analysis.,http://research.google.com/pubs/archive/44278.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Subjective+Study+for+the+Design+of+Multi-resolution+ABR+Video+Streams+with+the+VP9+Codec+Chen+Inguva+Rankin+Kokaram,http://research.google.com/pubs/pub44278.html
Bayesian Dark Knowledge,Advances in Neural Information Processing Systems (2015),2015,Anoop Korattikara Vivek Rathod Kevin Murphy Max Welling,@inproceedings{44678 title = {Bayesian Dark Knowledge} author = {Anoop Korattikara and Vivek Rathod and Kevin Murphy and Max Welling} year = 2015 URL = {http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf} booktitle = {Advances in Neural Information Processing Systems} },"We consider the problem of Bayesian parameter estimation for deep neural networks which is important in problem settings where we may have little data and/ or where we need accurate posterior predictive densities e.g. for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods such as SGLD (stochastic gradient Langevin dynamics). Unfortunately such a method needs to store many copies of the parameters (which wastes memory) and needs to make predictions using many versions of the model (which wastes time). We describe a method for ""distilling"" a Monte Carlo approximation to the posterior predictive density into a more compact form namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks namely an approach based on expectation propagation [Hernandez-Lobato and Adams 2015] and an approach based on variational Bayes [Blundell et al. 2015]. Our method performs better than both of these is much simpler to implement and uses less computation at test time.",http://research.google.com/pubs/archive/44678.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bayesian+Dark+Knowledge+Korattikara+Rathod+Murphy+Welling,http://research.google.com/pubs/pub44678.html
The Maximal Two-Sided Ideals of Nest Algebras,Journal of Operator Theory vol. 73:2 (2015) pp. 407-416,2015,John Lindsay Orr,@article{43797 title = {The Maximal Two-Sided Ideals of Nest Algebras} author = {John Lindsay Orr} year = 2015 journal = {Journal of Operator Theory} pages = {407-416} volume = {73:2} },We give a necessary and sufficient criterion for an operator in a nest algebra to belong to a proper two-sided ideal of that algebra. Using this result we describe the strong radical of a nest algebra and give a general description of the maximal two-sided ideals. This also enables us to provide the final piece in the complete description of epimorphisms of one nest algebra onto another.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Maximal+Two-Sided+Ideals+of+Nest+Algebras+Orr,http://research.google.com/pubs/pub43797.html
Improved Domain Adaptation for Statistical Machine Translation,AMTA-2012 The Association for Machine Translation in the Americas,2012,Wei Wang Klaus Macherey Wolfgang Macherey Franz Och Peng Xu,@inproceedings{40803 title = {Improved Domain Adaptation for Statistical Machine Translation} author = {Wei Wang and Klaus Macherey and Wolfgang Macherey and Franz Och and Peng Xu} year = 2012 URL = {http://amta2012.amtaweb.org/AMTA2012Files/papers/131.pdf} booktitle = {AMTA-2012} },We present a simple and effective infrastructure for domain adaptation for statistical machine translation (MT). To build MT systems for different domains it trains tunes and deploys a single translation system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach uni?es automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability,http://amta2012.amtaweb.org/AMTA2012Files/papers/131.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improved+Domain+Adaptation+for+Statistical+Machine+Translation+Wang+Macherey+Macherey+Och+Xu,http://research.google.com/pubs/pub40803.html
A Biomimetic 4.5 µW 120+dB Log-domain Cochlea Channel with AGC,IEEE JSSC (Journal of Solid-State Circuits) vol. 44 (2009) pp. 1006-1022,2009,Andreas G. Katsiamis Emmanuel M. Drakakis Richard F. Lyon,@article{33485 title = {A Biomimetic 4.5 µW 120+dB Log-domain Cochlea Channel with AGC} author = {Andreas G. Katsiamis and Emmanuel M. Drakakis and Richard F. Lyon} year = 2009 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4787561} journal = {IEEE JSSC (Journal of Solid-State Circuits)} pages = {1006-1022} volume = {44} },This paper deals with the design and performance evaluation of a new analog CMOS cochlea channel of increased biorealism. The design implements a recently proposed transfer function namely the One-Zero Gammatone filter (or OZGF) which provides a robust foundation for modeling a variety of auditory data such as realistic passband asymmetry linear low-frequency tail and level-dependent gain. Moreover the OZGF is attractive because it can be implemented efficiently in any technological medium-analog or digital-using standard building blocks. The channel was synthesized using novel low-power class-AB log-domain biquadratic filters employing MOS transistors operating in their weak inversion regime. Furthermore the paper details the design of a new low-power automatic gain control circuit that adapts the gain of the channel according to the input signal strength thereby extending significantly its input dynamic range. We evaluate the performance of a fourth-order OZGF channel (equivalent to an 8th-order cascaded filter structure) through both detailed simulations and measurements from a fabricated chip using the commercially available 0.35 mum AMS CMOS process. The whole system is tuned at 3 kHz dissipates a mere 4.46 µW of static power accommodates 124 dB (at < 5% THD) of input dynamic range at the center frequency and is set to provide up to 70 dB of amplification for small signals.,http://research.google.com/pubs/archive/33485.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Biomimetic+4.5+%C2%B5W+120%2BdB+Log-domain+Cochlea+Channel+with+AGC+Katsiamis+Drakakis+Lyon,http://research.google.com/pubs/pub33485.html
Fast Algorithms for Finding Extremal Sets,Proc. of the 2011 SIAM Int'l Conf. on Data Mining (to appear),2011,Roberto J. Bayardo Biswanath Panda,@inproceedings{36974 title = {Fast Algorithms for Finding Extremal Sets} author = {Roberto J. Bayardo and Biswanath Panda} year = 2011 URL = {http://bayardo.org/ps/sdm2011.pdf} booktitle = {Proc. of the 2011 SIAM Int'l Conf. on Data Mining} },Identifying the extremal (minimal and maximal) sets from a collection of sets is an important subproblem in the areas of data-mining and satisﬁability checking. For example extremal set ﬁnding algorithms are used in the context of mining maximal frequent itemsets and for simplifying large propositional satisﬁability instances derived from real world tasks such as circuit routing and veriﬁcation. In this paper we describe two new algorithms for the task and detail their performance on real and synthetic data. Each algorithm leverages an entirely di_erent principle – one primarily exploits set cardinality constraints the other lexicographic constraints. Despite the inherent di_culty of this problem (the best known worst-case bounds are nearly quadratic) we show that both these algorithms provide excellent performance in practice and can identify all extremal sets from multi-gigabyte itemset data using only a single processor core. Both algorithms are concise and can be implemented in no more than a few hundred lines of code. Our reference C++ implementations are open source and available for download.,http://bayardo.org/ps/sdm2011.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Algorithms+for+Finding+Extremal+Sets+Bayardo+Panda,http://research.google.com/pubs/pub36974.html
RACEZ: A Lightweight and Non-Invasive Race Detection Tool for Production Applications,ICSE ACM (2011) pp. 401-410,2011,Tianwei Sheng Neil Vachharajani Stephane Eranian Robert Hundt,@inproceedings{37123 title = {RACEZ: A Lightweight and Non-Invasive Race Detection Tool for Production Applications} author = {Tianwei Sheng and Neil Vachharajani and Stephane Eranian and Robert Hundt} year = 2011 URL = {http://portal.acm.org/citation.cfm?doid=1985793.1985848} booktitle = {ICSE} pages = {401-410} },Concurrency bugs particularly data races are notoriously difficult to debug and are a significant source of unreliability in multithreaded applications. Many tools to catch data races rely on program instrumentation to obtain memory instruction traces. Unfortunately this instrumentation introduces significant runtime overhead is extremely invasive or has a limited domain of applicability making these tools unsuitable for many production systems. Consequently these tools are typically used during application testing where many data races go undetected. This paper proposes RACEZ a novel race detection mechanism which uses a sampled memory trace collected by the hardware performance monitoring unit rather than invasive instrumentation. The approach introduces only a modest overhead making it usable in production environments. We validate RACEZ using two open source server applications and the PARSEC benchmarks. Our experiments show that RACEZ catches a set of known bugs with reasonable probability while introducing only 2.8% runtime slow down on average.,http://research.google.com/pubs/archive/37123.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RACEZ:+A+Lightweight+and+Non-Invasive+Race+Detection+Tool+for+Production+Applications+Sheng+Vachharajani+Eranian+Hundt,http://research.google.com/pubs/pub37123.html
The Need for Music Information Retrieval with User-Centered and Multimodal Strategies,MIRUM '11 ACM Scottsdale Arizona (2011) pp. 1-6,2011,Cynthia C.S. Liem Meinard Müller Douglas Eck George Tzanetakis Alan Hanjalic,@inproceedings{37669 title = {The Need for Music Information Retrieval with User-Centered and Multimodal Strategies} author = {Cynthia C.S. Liem and Meinard Müller and Douglas Eck and George Tzanetakis and Alan Hanjalic} year = 2011 booktitle = {MIRUM '11} pages = {1--6} address = {Scottsdale Arizona} },Music is a widely enjoyed content type existing in many multifaceted representations. With the digital information age a lot of digitized music information has theoretically become available at the user’s fingertips. However the abundance of information is too large-scaled and too diverse to annotate oversee and present in a consistent and human manner motivating the development of automated Music Information Retrieval (Music-IR) techniques. In this paper we encourage to consider music content beyond a monomodal audio signal and argue that Music-IR approaches with multimodal and user-centered strategies are necessary to serve reallife usage patterns and maintain and improve accessibility of digital music data. After discussing relevant existing work in these directions we show that the ﬁeld of Music-IR faces similar challenges as neighboring ﬁelds and thus suggest opportunities for joint collaboration and mutual inspiration.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Need+for+Music+Information+Retrieval+with+User-Centered+and+Multimodal+Strategies+Liem+M%C3%BCller+Eck+Tzanetakis+Hanjalic,http://research.google.com/pubs/pub37669.html
Characterizing End-to-End Packet Reordering with UDP Traffic,IEEE Symposium on Computers and Communications (ISCC) (2009) pp. 321-324,2009,Sandra Tinta Alexander Mohr Jennifer Wong,@inproceedings{35247 title = {Characterizing End-to-End Packet Reordering with UDP Traffic} author = {Sandra Tinta and Alexander Mohr and Jennifer Wong} year = 2009 booktitle = {IEEE Symposium on Computers and Communications (ISCC)} pages = {321--324} },Packet reordering is an Internet event that degrades the performance of both TCP and UDP-based applications. In this paper we present an end-to-end measurement study of packet reordering of UDP traffic. The goal of the measurement study performed on PlanetLab was to answer four main questions: how prevalent is reordering across end-to-end paths what are the time scales of reordered packets how correlated is reordering with traffic load and does the size of a transmitted packet affect the likelihood of reordering? Overall our analysis shows that current UDP traffic reordering is consistent to prior 1990's studies on TCP traffic despite increased Internet load and technology advancements and it adds to the previous results by identifying additional reordering characteristics. More specifically we show that packet reordering is asymmetric as well as temporal and site-dependent packet size does influence the likelihood of reordering that there exists a time-of-the-day dependency and reordering primarily exists at two time-scales (a few milliseconds or multiple tens of milliseconds.),http://research.google.com/pubs/archive/35247.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Characterizing+End-to-End+Packet+Reordering+with+UDP+Traffic+Tinta+Mohr+Wong,http://research.google.com/pubs/pub35247.html
HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space,Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL'15) pp. 133-142,2015,Carlos A. Colmenares Marina Litvak Amin Mantrach Fabrizio Silvestri,@inproceedings{44650 title = {HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space} author = {Carlos A. Colmenares and Marina Litvak and Amin Mantrach and Fabrizio Silvestri} year = 2015 URL = {http://www.aclweb.org/anthology/N15-1014} booktitle = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL'15)} pages = {133--142} },Automatic headline generation is a sub-task of document summarization with many reported applications. In this study we present a sequence-prediction technique for learning how editors title their news stories. The introduced technique models the problem as a discrete optimization task in a feature-rich space. In this space the global optimum can be found in polynomial time by means of dynamic programming. We train and test our model on an extensive corpus of financial news and compare it against a number of baselines by using standard metrics from the document summarization domain as well as some new ones proposed in this work. We also assess the readability and informativeness of the generated titles through human evaluation. The obtained results are very appealing and substantiate the soundness of the approach.,http://research.google.com/pubs/archive/44650.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HEADS:+Headline+Generation+as+Sequence+Prediction+Using+an+Abstract+Feature-Rich+Space+Colmenares+Litvak+Mantrach+Silvestri,http://research.google.com/pubs/pub44650.html
Asymptotic Performance of the Non-Forced Idle Time Scheduling Policies in the Presence of Variable Demand for Resources,Proceedings of the 46th Annual Conference on Communication Control and Computing (2008) pp. 499-503,2008,Ana Radovanovic Cliff Stein,@inproceedings{34721 title = {Asymptotic Performance of the Non-Forced Idle Time Scheduling Policies in the Presence of Variable Demand for Resources} author = {Ana Radovanovic and Cliff Stein} year = 2008 booktitle = {Proceedings of the 46th Annual Conference on Communication Control and Computing} pages = {499-503} },http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4797599&tag=1,http://research.google.com/pubs/archive/34721.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Asymptotic+Performance+of+the+Non-Forced+Idle+Time+Scheduling+Policies+in+the+Presence+of+Variable+Demand+for+Resources+Radovanovic+Stein,http://research.google.com/pubs/pub34721.html
Posterior Sparsity in Dependency Grammar Induction,Journal of Machine Learning Research vol. 12 (2011) pp. 455-490,2011,Jennifer Gillenwater Kuzman Ganchev Joao Graca Fernando Pereira Ben Taskar,@article{38281 title = {Posterior Sparsity in Dependency Grammar Induction} author = {Jennifer Gillenwater and Kuzman Ganchev and Joao Graca and Fernando Pereira and Ben Taskar} year = 2011 URL = {http://jmlr.csail.mit.edu/papers/v12/gillenwater11a.html} journal = {Journal of Machine Learning Research} pages = {455--490} volume = {12} },A strong inductive bias is essential in unsupervised grammar induction. In this paper we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with 12 different languages we achieve significant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline with an average accuracy improvement of 6.5% outperforming EM by at least 1% for 9 out of 12 languages. Furthermore the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular we show that our approach improves performance over other state-of-the-art techniques.,http://research.google.com/pubs/archive/38281.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Posterior+Sparsity+in+Dependency+Grammar+Induction+Gillenwater+Ganchev+Graca+Pereira+Taskar,http://research.google.com/pubs/pub38281.html
Adapting Online Advertising Techniques to Television,Online Multimedia Advertising: Techniques and Technologies Information Science Reference Hershey PA (2011) pp. 148-165,2011,Sundar Dorai-Raj Yannet Interian Igor Naverniouk Dan Zigmond,@inbook{37038 title = {Adapting Online Advertising Techniques to Television} author = {Sundar Dorai-Raj and Yannet Interian and Igor Naverniouk and Dan Zigmond} year = 2011 booktitle = {Online Multimedia Advertising: Techniques and Technologies} pages = {148-165} address = {Hershey PA} },The availability of precise data on TV ad consumption fundamentally changes this advertising medium and allows many techniques developed for analyzing online ads to be adapted for TV. This chapter looks in particular at how results from the emerging field of online ad quality analysis can now be applied to TV.,http://research.google.com/pubs/archive/37038.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adapting+Online+Advertising+Techniques+to+Television+Dorai-Raj+Interian+Naverniouk+Zigmond,http://research.google.com/pubs/pub37038.html
A Database for Measuring Linguistic Information Content.,Language Resources and Evaluation Conference ELDA 330 W 58th St (2014),2014,Richard Sproat Bruno Cartoni HyunJeong Choe David Huynh Linne Ha Ravindran Rajakumar Evelyn Wenzel-Grondie,@inproceedings{42526 title = {A Database for Measuring Linguistic Information Content.} author = {Richard Sproat and Bruno Cartoni and HyunJeong Choe and David Huynh and Linne Ha and Ravindran Rajakumar and Evelyn Wenzel-Grondie} year = 2014 booktitle = {Language Resources and Evaluation Conference} address = {330 W 58th St} },Which languages convey the most information in a given amount of space? This is a question often asked of linguists especially by engineers who often have some information theoretic measure of ``information'' in mind but rarely define exactly how they would measure that information. The question is in fact remarkably hard to answer and many linguists consider it unanswerable. But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically diverse languages with detailed marking of morphosyntactic and morphosemantic features one could hope to quantify the differences between how these different languages convey information. Since no appropriate database exists we decided to construct one. The purpose of this paper is to present our work on the database along with some preliminary results. We plan to release the dataset once complete.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Database+for+Measuring+Linguistic+Information+Content.+Sproat+Cartoni+Choe+Huynh+Ha+Rajakumar+Wenzel-Grondie,http://research.google.com/pubs/pub42526.html
Mobile Music Modeling Analysis and Recognition,International Conference on Acoustics Speech and Signal Processing (ICASSP) (2012),2012,Pavel Golik Boulos Harb Ananya Misra Michael Riley Alex Rudnick Eugene Weinstein,@inproceedings{37754 title = {Mobile Music Modeling Analysis and Recognition} author = {Pavel Golik and Boulos Harb and Ananya Misra and Michael Riley and Alex Rudnick and Eugene Weinstein} year = 2012 booktitle = {International Conference on Acoustics Speech and Signal Processing (ICASSP)} },We present an analysis of music modeling and recognition techniques in the context of mobile music matching substantially improving on the techniques presented in [Mohri et al. 2010]. We accomplish this by adapting the features specifically to this task and by introducing new modeling techniques that enable using a corpus of noisy and channel-distorted data to improve mobile music recognition quality. We report the results of an extensive empirical investigation of the system's robustness under realistic channel effects and distortions. We show an improvement of recognition accuracy by explicit duration modeling of music phonemes and by integrating the expected noise environment into the training process. Finally we propose the use of frame-to-phoneme alignment for high-level structure analysis of polyphonic music.,http://research.google.com/pubs/archive/37754.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mobile+Music+Modeling+Analysis+and+Recognition+Golik+Harb+Misra+Riley+Rudnick+Weinstein,http://research.google.com/pubs/pub37754.html
Near-Data Processing: Insights from a MICRO-46 Workshop,IEEE Micro (Special Issue on Big Data) vol. 34 (2014) pp. 36-43,2014,Rajeev Balasubramonian Jichuan Chang Troy Manning Jaime H. Moreno Richard Murphy Ravi Nair Steven Swanson,@article{42899 title = {Near-Data Processing: Insights from a MICRO-46 Workshop} author = {Rajeev Balasubramonian and Jichuan Chang and Troy Manning and Jaime H. Moreno and Richard Murphy and Ravi Nair and Steven Swanson} year = 2014 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6871738&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A6871686%29} journal = {IEEE Micro (Special Issue on Big Data)} pages = {36-43} volume = {34} },The cost of data movement in big-data systems motivates careful examination of near-data processing (NDP) frameworks. The concept of NDP was actively researched in the 1990s but gained little commercial traction. After a decade-long dormancy interest in this topic has spiked. A workshop on NDP was organized at MICRO-46 and was well attended. Given the interest the organizers and keynote speakers have attempted to capture the key insights from the workshop into an article that can be widely disseminated. This article describes the many reasons why NDP is compelling today and identifies key upcoming challenges in realizing the potential of NDP.,http://research.google.com/pubs/archive/42899.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Near-Data+Processing:+Insights+from+a+MICRO-46+Workshop+Balasubramonian+Chang+Manning+Moreno+Murphy+Nair+Swanson,http://research.google.com/pubs/pub42899.html
Machine Hearing: An Emerging Field,IEEE Signal Processing Magazine vol. 27 (2010) pp. 131-139,2010,Richard F. Lyon,@article{36608 title = {Machine Hearing: An Emerging Field} author = {Richard F. Lyon} year = 2010 journal = {IEEE Signal Processing Magazine} pages = {131--139} volume = {27} },(intro paragraph in lieu of abstract) If we had machines that could hear as humans do we would expect them to be able to easily distinguish speech from music and background noises to pull out the speech and music parts for special treatment to know what direction sounds are coming from to learn which noises are typical and which are noteworthy. Hearing machines should be able to organize what they hear; learn names for recognizable objects actions events places musical styles instruments and speakers; and retrieve sounds by reference to those names. These machines should be able to listen and react in real time to take appropriate action on hearing noteworthy events to participate in ongoing activities whether in factories in musical performances or in phone conversations.,http://research.google.com/pubs/archive/36608.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+Hearing:+An+Emerging+Field+Lyon,http://research.google.com/pubs/pub36608.html
Clustering Query Refinements by User Intent,Proceedings of the International World Wide Web Conference (WWW) (2010),2010,Eldar Sadikov Jayant Madhavan Lu Wang Alon Halevy,@inproceedings{36242 title = {Clustering Query Refinements by User Intent} author = {Eldar Sadikov and Jayant Madhavan and Lu Wang and Alon Halevy} year = 2010 booktitle = {Proceedings of the International World Wide Web Conference (WWW)} },We address the problem of clustering the reﬁnements of a user search query. The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine and can also serve to summarize the different aspects of information relevant to the original user query. Our algorithm clusters reﬁnements based on their likely underlying user intents by combining document click and session co-occurrence information. At its core our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior. A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Clustering+Query+Refinements+by+User+Intent+Sadikov+Madhavan+Wang+Halevy,http://research.google.com/pubs/pub36242.html
A rational deconstruction of Landin's SECD machine with the J operator,Logical Methods in Computer Science vol. 4 (2008) pp. 1-67,2008,Olivier Danvy Kevin Millikin,@article{40335 title = {A rational deconstruction of Landin's SECD machine with the J operator} author = {Olivier Danvy and Kevin Millikin} year = 2008 URL = {http://www.lmcs-online.org/ojs/viewarticle.php?id=258=abstract} journal = {Logical Methods in Computer Science} pages = {1--67} volume = {4} },Landin's SECD machine was the first abstract machine for applicative expressions ie functional programs. Landin's J operator was the first control operator for functional languages and was specified by an extension of the SECD machine. We present a family of evaluation functions corresponding to this extension of the SECD machine using a series of elementary transformations (transformation into continu-ation-passing style (CPS) and defunctionalization chiefly) and their left inverses (transformation into direct style and refunctionalization). To this end we modernize the SECD machine into a bisimilar one that operates in lockstep with the original one but that (1) does not use a data stack and (2) uses the caller-save rather than the callee-save convention for environments. We also identify that the dump component of the SECD machine is managed in a callee-save way. The caller-save counterpart of the modernized SECD machine precisely corresponds to Thielecke's double-barrelled continuations and to Felleisen's encoding of J in terms of call/cc. We then variously characterize the J operator in terms of CPS and in terms of delimited-control operators in the CPS hierarchy. As a byproduct we also present several reduction semantics for applicative expressions with the J operator based on Curien's original calculus of explicit substitutions. These reduction semantics mechanically correspond to the modernized versions of the SECD machine and to the best of our knowledge they provide the first syntactic theories of applicative expressions with the J operator. The present work is concluded by a motivated wish to see Landin's name added to the list of co-discoverers of continuations. Methodologically however it mainly illustrates the value of Reynolds's defunctionalization and of refunctionalization as well as the expressive power of the CPS hierarchy (1) to account for the first control operator and the first abstract machine for functional languages and (2) to connect them to their successors. Our work also illustrates the value of Danvy and Nielsen's refocusing technique to connect environment-based abstract machines and syntactic theories in the form of reduction semantics for calculi of explicit substitutions.,http://research.google.com/pubs/archive/40335.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+rational+deconstruction+of+Landin's+SECD+machine+with+the+J+operator+Danvy+Millikin,http://research.google.com/pubs/pub40335.html
Weakly Supervised Learning of Object Segmentations from Web-Scale Video,ECCV'12 Proceedings of the 12th international conference on Computer Vision - Volume Part I Springer-Verlag Berlin Heidelberg (2012) pp. 198-208,2012,Glenn Hartmann Matthias Grundmann Judy Hoffman David Tsai Vivek Kwatra Omid Madani Sudheendra Vijayanarasimhan Irfan Essa James Rehg Rahul Sukthankar,@inproceedings{40735 title = {Weakly Supervised Learning of Object Segmentations from Web-Scale Video} author = {Glenn Hartmann and Matthias Grundmann and Judy Hoffman and David Tsai and Vivek Kwatra and Omid Madani and Sudheendra Vijayanarasimhan and Irfan Essa and James Rehg and Rahul Sukthankar} year = 2012 booktitle = {ECCV'12 Proceedings of the 12th international conference on Computer Vision - Volume Part I} pages = {198--208} address = {Berlin Heidelberg} },"We propose to learn pixel-level segmentations of objects from weakly labeled (tagged) internet videos. Specifically given a large collection of raw YouTube content along with potentially noisy tags our goal is to automatically generate spatiotemporal masks for each object such as ""dog"" without employing any pre-trained object detectors. We formulate this problem as learning weakly supervised classifiers for a set of independent spatio-temporal segments. The object seeds obtained using segment-level classifiers are further refined using graphcuts to generate high-precision object masks. Our results obtained by training on a dataset of 20000 YouTube videos weakly tagged into 15 classes demonstrate automatic extraction of pixel-level object masks. Evaluated against a ground-truthed subset of 50000 frames with pixel-level annotations we confirm that our proposed methods can learn good object masks just by watching YouTube.",http://research.google.com/pubs/archive/40735.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Weakly+Supervised+Learning+of+Object+Segmentations+from+Web-Scale+Video+Hartmann+Grundmann+Hoffman+Cai+Kwatra+Madani+Vijayanarasimhan+Essa+Rehg+Sukthankar,http://research.google.com/pubs/pub40735.html
Optimizing Utilization of Resource Pools in Web Application Servers,Concurrency and Computation: Practice and Experience vol. 22 (2010) pp. 2421-2444,2010,Alexander Totok Vijay Karamcheti,@article{36941 title = {Optimizing Utilization of Resource Pools in Web Application Servers} author = {Alexander Totok and Vijay Karamcheti} year = 2010 URL = {http://dx.doi.org/10.1002/cpe.1572} journal = {Concurrency and Computation: Practice and Experience} pages = {2421--2444} volume = {22} },Among the web application server resources most critical for its performance are those that are held exclusively by a service request for the duration of its execution (or some significant part of it). Such exclusively-held server resources become performance bottleneck points with failures to obtain such a resource constituting a major portion of request rejections under server overload conditions. In this paper we propose a methodology that computes the optimal pool sizes for two such critical resources: web server threads and database connections. Our methodology uses information about incoming request flow and about fine-grained server resource utilization by service requests of different types obtained through offline and online request profiling. In our methodology we advocate (and show its benefits) the use of a database connection pooling mechanism that caches database connections for the duration of a service request execution (so-called request-wide database connection caching). We evaluate our methodology by testing it on the TPC-W web application. Our method is able to accurately compute the optimal number of server threads and database connections and the value of sustainable request throughput computed by the method always lies within a 5% margin of the actual value determined experimentally.,http://research.google.com/pubs/archive/36941.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+Utilization+of+Resource+Pools+in+Web+Application+Servers+Totok+Karamcheti,http://research.google.com/pubs/pub36941.html
An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL) Association for Computational Linguistics 209 N. Eighth Street East Stroudsburg PA USA pp. 986-995,2007,Wolfgang Macherey Franz J. Och,@inproceedings{34506 title = {An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems} author = {Wolfgang Macherey and Franz J. Och} year = 2007 URL = {http://www.aclweb.org/anthology-new/D/D07/D07-1105.pdf} booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)} pages = {986--995} address = {209 N. Eighth Street East Stroudsburg PA USA} },This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the best-ranked machine translation engines in the 2006 NIST machine translation evaluation.,http://research.google.com/pubs/archive/34506.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Empirical+Study+on+Computing+Consensus+Translations+from+Multiple+Machine+Translation+Systems+Macherey+Och,http://research.google.com/pubs/pub34506.html
Take me to your leader! Online Optimization of Distributed Storage Configurations,Proceedings of the 41st International Conference on Very Large Data Bases VLDB Endowment (2015) pp. 1490-1501,2015,Artyom Sharov Alexander Shraer Arif Merchant Murray Stokely,@inproceedings{43999 title = {Take me to your leader! Online Optimization of Distributed Storage Configurations} author = {Artyom Sharov and Alexander Shraer and Arif Merchant and Murray Stokely} year = 2015 booktitle = {Proceedings of the 41st International Conference on Very Large Data Bases} pages = {1490--1501} },The configuration of a distributed storage system typically includes among other parameters the set of servers and their roles in the replication protocol. Although mechanisms for changing the configuration at runtime exist it is usually left to system administrators to manually determine the “best” configuration and periodically reconfigure the system often by trial and error. This paper describes a new workload-driven optimization framework that dynamically determines the optimal configuration at runtime. We focus on optimizing leader and quorum based replication schemes and divide the framework into three optimization tiers dynamically optimizing different configuration aspects: 1) leader placement 2) roles of different servers in the replication protocol and 3) replica locations. We showcase our optimization framework by applying it to a large-scale distributed storage system used internally in Google and demonstrate that most client applications significantly benefit from using our framework reducing average operation latency by up to 94%.,http://research.google.com/pubs/archive/43999.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Take+me+to+your+leader!+Online+Optimization+of+Distributed+Storage+Configurations+Sharov+Shraer+Merchant+Stokely,http://research.google.com/pubs/pub43999.html
Scalable Thread Scheduling and Global Power Management for Heterogeneous Many-Core Architectures,Proceedings of the Nineteenth International Conference on Parallel Architectures and Compilation Techniques (PACT) Association for Computing Machinery 2 Penn Plaza Suite 701 New York NY 10121-0701 (2010) pp. 29-39,2010,Jonathan A. Winter David H. Albonesi Christine A. Shoemaker,@inproceedings{36755 title = {Scalable Thread Scheduling and Global Power Management for Heterogeneous Many-Core Architectures} author = {Jonathan A. Winter and David H. Albonesi and Christine A. Shoemaker} year = 2010 URL = {http://www.csl.cornell.edu/~albonesi/research/papers/pact10_2.pdf} booktitle = {Proceedings of the Nineteenth International Conference on Parallel Architectures and Compilation Techniques (PACT)} pages = {29--39} address = {2 Penn Plaza Suite 701 New York NY 10121-0701} },Future many-core microprocessors are likely to be heterogeneous by design or due to variability and defects. The latter type of heterogeneity is especially challenging due to its unpredictability. To minimize the performance and power impact of these hardware imperfections the runtime thread scheduler and global power manager must be nimble enough to handle such random heterogeneity. With hundreds of cores expected on a single die in the future these algorithms must provide high power-performance efficiency yet remain scalable with low runtime overhead. This paper presents a range of scheduling and power management algorithms and performs a detailed evaluation of their effectiveness and scalability on heterogeneous many-core architectures with up to 256 cores. We also conduct a limit study on the potential benefits of coordinating scheduling and power management and demonstrate that coordination yields little benefit. We highlight the scalability limitations of previously proposed thread scheduling algorithms that were designed for small-scale chip multiprocessors and propose a Hierarchical Hungarian Scheduling Algorithm that dramatically reduces the scheduling overhead without loss of accuracy. Finally we show that the high computational requirements of prior global power management algorithms based on linear programming make them infeasible for many-core chips and that an algorithm that we call Steepest Drop achieves orders of magnitude lower execution time without sacrificing power-performance efficiency.,http://research.google.com/pubs/archive/36755.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Thread+Scheduling+and+Global+Power+Management+for+Heterogeneous+Many-Core+Architectures+Winter+Albonesi+Shoemaker,http://research.google.com/pubs/pub36755.html
Brief Announcement: Consistency and Complexity Tradeoffs for Highly-Available Multi-Cloud Store,The International Symposium on Distributed Computing (DISC) (2013),2013,Gregory Chockler Dan Dobre Alexander Shraer,@inproceedings{41216 title = {Brief Announcement: Consistency and Complexity Tradeoffs for Highly-Available Multi-Cloud Store} author = {Gregory Chockler and Dan Dobre and Alexander Shraer} year = 2013 booktitle = {The International Symposium on Distributed Computing (DISC)} },An emerging multi-cloud storage paradigm suggests replicating data across multiple cloud storage services potentially operated by distinct providers. In this paper we study the impact of the storage interfaces and consistency semantics exposed by individual clouds on the complexity of the reliable multi-cloud storage implementation. Our results establish several inherent space and time tradeoffs associated with emulating reliable objects over a collection of unreliable storage services with varied interfaces and consistency guarantees.,http://research.google.com/pubs/archive/41216.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Brief+Announcement:+Consistency+and+Complexity+Tradeoffs+for+Highly-Available+Multi-Cloud+Store+Chockler+Dobre+Shraer,http://research.google.com/pubs/pub41216.html
RFC6928 - Increasing TCP's Initial Window,Internet Engineering Task Force (IETF) (2013),2013,H.K. Jerry Chu Nandita Dukkipati Yuchung Cheng Matt Mathis,@misc{41330 title = {RFC6928 - Increasing TCP's Initial Window} author = {H.K. Jerry Chu and Nandita Dukkipati and Yuchung Cheng and Matt Mathis} year = 2013 URL = {http://www.rfc-editor.org/rfc/rfc6928.txt} },This document proposes an experiment to increase the permitted TCP initial window (IW) from between 2 and 4 segments as specified in RFC 3390 to 10 segments with a fallback to the existing recommendation when performance issues are detected. It discusses the motivation behind the increase the advantages and disadvantages of the higher initial window and presents results from several large-scale experiments showing that the higher initial window improves the overall performance of many web services without resulting in a congestion collapse. The document closes with a discussion of usage and deployment for further experimental purposes recommended by the IETF TCP Maintenance and Minor Extensions (TCPM) working group.,http://research.google.com/pubs/archive/41330.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC6928+-+Increasing+TCP's+Initial+Window+Chu+Dukkipati+Cheng+Mathis,http://research.google.com/pubs/pub41330.html
Interpreting the Data: Parallel Analysis with Sawzall,Scientific Programming Journal vol. 13 (2005) pp. 277-298,2005,Rob Pike Sean Dorward Robert Griesemer Sean Quinlan,@article{61 title = {Interpreting the Data: Parallel Analysis with Sawzall} author = {Rob Pike and Sean Dorward and Robert Griesemer and Sean Quinlan} year = 2005 URL = {http://research.google.com/archive/sawzall.html} journal = {Scientific Programming Journal} pages = {277--298} volume = {13} },Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records network logs and web document repositories. These large data sets are not amenable to study using traditional database techniques if only because they can be too large to fit in a single relational database. On the other hand many of the analyses done on them can be expressed using simple easily distributed computations: filtering aggregation extraction of statistics and so on. We present a system for automating such analyses. A filtering phase in which a query is expressed using a new programming language emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design -- including the separation into two phases the form of the programming language and the properties of the aggregators -- exploits the parallelism inherent in having data and computation distributed across many machines. Animation: The paper references this movie showing how the distribution of requests to google.com around the world changed through the day on August 14 2003.,http://research.google.com/pubs/archive/61.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Interpreting+the+Data:+Parallel+Analysis+with+Sawzall+Pike+Dorward+Griesemer+Quinlan,http://research.google.com/pubs/pub61.html
Large-Scale Automatic Classification of Phishing Pages,NDSS '10 (2010),2010,Colin Whittaker Brian Ryner Marria Nazif,@inproceedings{35580 title = {Large-Scale Automatic Classification of Phishing Pages} author = {Colin Whittaker and Brian Ryner and Marria Nazif} year = 2010 URL = {http://www.isoc.org/isoc/conferences/ndss/10/pdf/08.pdf} booktitle = {NDSS '10} },Phishing websites fraudulent sites that trick viewers into interacting with them continue to cost Internet users over a billion dollars each year. In this paper we describe the design and performance characteristics of a scalable machine learning classifier we developed to detect phishing web sites. We use this classifier to maintain Google's phishing blacklist automatically. Our classifier analyzes millions of pages a day examining the URL and the contents of a page to determine whether or not a page is phishing. Unlike previous work in this field we train the classifier on a noisy dataset consisting of millions of samples from previously collected live classification data. Despite the noise in the training data our classifier learns a robust model for identifying phishing pages which correctly classifies more than 90% of phishing pages several weeks after training concludes.,http://research.google.com/pubs/archive/35580.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Automatic+Classification+of+Phishing+Pages+Whittaker+Ryner+Nazif,http://research.google.com/pubs/pub35580.html
Using Web Co-occurrence Statistics for Improving Image Categorization,arXiv (2013),2013,Samy Bengio Jeffrey Dean Dumitru Erhan Eugene Ie Quoc Le Andrew Rabinovich Jonathon Shlens Yoram Singer,@techreport{42244 title = {Using Web Co-occurrence Statistics for Improving Image Categorization} author = {Samy Bengio and Jeffrey Dean and Dumitru Erhan and Eugene Ie and Quoc Le and Andrew Rabinovich and Jonathon Shlens and Yoram Singer} year = 2013 URL = {http://arxiv.org/abs/1312.5697} institution = {arXiv} },Object recognition and localization are important tasks in computer vision. The focus of this work is the incorporation of contextual information in order to improve object recognition and localization. For instance it is natural to expect not to see an elephant to appear in the middle of an ocean. We consider a simple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants sharks oceans etc.) co-occur in web documents we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining textual co-occurrence statistics with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem it is effective in improving both recognition and localization accuracy. Concretely we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets.,http://research.google.com/pubs/archive/42244.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Web+Co-occurrence+Statistics+for+Improving+Image+Categorization+Bengio+Dean+Erhan+Ie+Le+Rabinovich+Shlens+Singer,http://research.google.com/pubs/pub42244.html
Improved Consistent Sampling Weighted Minhash and L1 Sketching,ICDM (2010) (to appear),2010,Sergey Ioffe,@inproceedings{36928 title = {Improved Consistent Sampling Weighted Minhash and L1 Sketching} author = {Sergey Ioffe} year = 2010 booktitle = {ICDM} },We propose a new Consistent Weighted Sampling method where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings apply it to Weighted Minhash and achieve more accurate distance estimates from sketches than existing methods as long as the inputs are sufficiently distinct. We show how to choose the optimal number of bits per hash for sketching and demonstrate experimental results which agree with the theoretical analysis.,http://research.google.com/pubs/archive/36928.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improved+Consistent+Sampling+Weighted+Minhash+and+L1+Sketching+Ioffe,http://research.google.com/pubs/pub36928.html
Capturing Mobile Experience in the Wild: A Tale of Two Apps,8th International Conference on emerging Networking EXperiments and Technologies (CoNEXT) ACM (Association for Computing Machinery) (2013) NA (to appear),2013,Ashish Patro Shravan Rayanchu Michael Griepentrog,@inproceedings{41590 title = {Capturing Mobile Experience in the Wild: A Tale of Two Apps} author = {Ashish Patro and Shravan Rayanchu and Michael Griepentrog} year = 2013 note = {The paper has been accepted to CONEXT 2013. We are working on the final draft of the paper. Please note that the draft has an older title I will upload the final draft once we have it.} booktitle = {8th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)} pages = {NA} },We present Insight a framework that collects mobile application analytics with minimal overhead on the application and the developers. Insight offers information about application usage device and platform statistics application footprint user behavior and retention properties and factors affecting application revenues. Further Insight leverages the vast and diverse mobile user base of the applications to continuously crowd-source network measurements from across the world. This allows us to carry out interesting longitudinal studies about the long term trends in usage and performance characteristics of these networks. Further by coupling network measurements along with application analytics Insight also helps understand how network performance can impact application usage performance and revenues. We deployed Insight on two applications in Apple’s AppStore and Google’s Android Market. One of them Parallel Kingdom (PK) is a popular Massively Multiplayer Online Role Playing Game (MMORPG) which has over 600000 unique users distributed across 118 countries. The second application was more recently released and currently has a few thousand users. Our measurements span almost the entire life of the PK game starting from its inception on October 31 2008 to Nov 10 2011 (1104 days in total). Through deployment of Insight on this game we also perform the ﬁrst study analyzing the characteristics of a mobile MMORPG.,http://research.google.com/pubs/archive/41590.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Capturing+Mobile+Experience+in+the+Wild:+A+Tale+of+Two+Apps+Patro+Rayanchu+Griepentrog,http://research.google.com/pubs/pub41590.html
On Big Data Algorithmics,Algorithms – ESA Springer (2012) pp. 1,2012,Yossi Matias,@inproceedings{41691 title = {On Big Data Algorithmics} author = {Yossi Matias} year = 2012 booktitle = {Algorithms – ESA} pages = {1} },The extensive use of Big Data has now become common in plethora of technologies and industries. From massive data bases to business intelligence and datamining applications; from search engines to recommendation systems; advancing the state of the art of voice recognition translation and more. The design analysis and engineering of Big Data algorithms has multiple flavors including massive parallelism streaming algorithms sketches and synopses cloud technologies and more. We will discuss some of these aspects and reflect on their evolution and on the interplay between the theory and practice of Big Data algorithmics.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+Big+Data+Algorithmics+Matias,http://research.google.com/pubs/pub41691.html
Unified and contrasting cuts in multiple graphs: application to medical imaging segmentation,KDD (2015) pp. 617-626,2015,Chia-Tung Kuo Xiang Wang Peter Walker Owen Carmichael Jieping Ye Ian Davidson,@inproceedings{44266 title = {Unified and contrasting cuts in multiple graphs: application to medical imaging segmentation} author = {Chia-Tung Kuo and Xiang Wang and Peter Walker and Owen Carmichael and Jieping Ye and Ian Davidson} year = 2015 URL = {http://dx.doi.org/10.1145/2783258.2783318} booktitle = {KDD} pages = {617--626} },"The analysis of data represented as graphs is common having wide scale applications from social networks to medical imaging. A popular analysis is to cut the graph so that the disjoint subgraphs can represent communities (for social network) or background and foreground cognitive activity (for medical imaging). An emerging setting is when multiple data sets (graphs) exist which opens up the opportunity for many new questions. In this paper we study two such questions: i) For a collection of graphs find a single cut that is good for all the graphs and ii) For two collections of graphs find a single cut that is good for one collection but poor for the other. We show that existing formulations of multiview consensus and alternative clustering cannot address these questions and instead we provide novel formulations in the spectral clustering framework. We evaluate our approaches on functional magnetic resonance imaging (fMRI) data to address questions such as: ""What common cognitive network does this group of individuals have?"" and ""What are the differences in the cognitive networks for these two groups?"" We obtain useful results without the need for strong domain knowledge.",http://research.google.com/pubs/archive/44266.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unified+and+contrasting+cuts+in+multiple+graphs:+application+to+medical+imaging+segmentation+Kuo+Wang+Walker+Carmichael+Ye+Davidson,http://research.google.com/pubs/pub44266.html
Associating Locations with Healthcare Events,Defensive Publications Series Technical Disclosure Commons (2015),2015,Daniel V. Klein Dean Jackson,@incollection{43419 title = {Associating Locations with Healthcare Events} author = {Daniel V. Klein and Dean Jackson} year = 2015 URL = {http://www.tdcommons.org/dpubs_series/31/} booktitle = {Defensive Publications Series} },The disclosed subject matter relates to computer implemented methods for associating locations with healthcare events. In one aspect a method includes receiving location data from a location-aware client device. The location data includes latitude and longitude information. The method further includes determining based on the received location data a routine travel pattern of a user associated with the location-aware client device. The method further includes detecting an anomaly in the routine travel pattern. The method further includes detecting a healthcare event. The healthcare event can be a visit to a healthcare facility and/or a healthcare transaction. The method further includes correlating the anomaly in the routine travel pattern of the user with the healthcare event. The method further includes associating one or more healthcare event locations to the healthcare event based on the correlation.,http://www.tdcommons.org/dpubs_series/31/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Associating+Locations+with+Healthcare+Events+Klein+Jackson,http://research.google.com/pubs/pub43419.html
Efficient and Accurate Label Propagation on Large Graphs and Label Sets,Proceedings International Conference on Advances in Multimedia IARIA (2013),2013,Michele Covell Shumeet Baluja,@inproceedings{41144 title = {Efficient and Accurate Label Propagation on Large Graphs and Label Sets} author = {Michele Covell and Shumeet Baluja} year = 2013 URL = {http://www.mangolassi.org/covell/pubs/Covell_MMEDIA_2013.pdf} booktitle = {Proceedings International Conference on Advances in Multimedia} },Many web-based application areas must infer label distributions starting from a small set of sparse noisy labels. Examples include searching for recommending and advertising against image audio and video content. These labeling problems must handle millions of interconnected entities (users domains content segments) and thousands of competing labels (interests tags recommendations topics). Previous work has shown that graph-based propagation can be very effective at finding the best label distribution across nodes starting from partial information and a weighted-connection graph. In their work on video recommendations Baluja et al. [1] showed high-quality results using Adsorption a normalized propagation process. An important step in the original formulation of Adsorption was re-normalization of the label vectors associated with each node between every propagation step. That interleaved normalization forced computation of all label distributions in synchrony in order to allow the normalization to be correctly determined. Interleaved normalization also prevented use of standard linear-algebra methods like stabilized bi-conjugate gradient descent (BiCGStab) and Gaussian elimination. This paper presents a method that replaces the interleaved normalization with a single pre-normalization done once before the main propagation process starts allowing use of selective label computation (label slicing) as well as large-matrix-solution methods. As a result much larger graphs and label sets can be handled than in the original formulation and more accurate solutions can be found in fewer propagation steps. We also report results from using pre-normalized Adsorption in topic labeling for web domains using label slicing and BiCGStab.,http://research.google.com/pubs/archive/41144.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+and+Accurate+Label+Propagation+on+Large+Graphs+and+Label+Sets+Covell+Baluja,http://research.google.com/pubs/pub41144.html
GOOGLE DISEASE TRENDS: AN UPDATE,International Society of Neglected Tropical Diseases 2013 International Society of Neglected Tropical Diseases pp. 3,2013,Patrick Copeland Raquel Romano Tom Zhang Greg Hecht Dan Zigmond Christian Stefansen,@inproceedings{41763 title = {GOOGLE DISEASE TRENDS: AN UPDATE} author = {Patrick Copeland and Raquel Romano and Tom Zhang and Greg Hecht and Dan Zigmond and Christian Stefansen} year = 2013 booktitle = {International Society of Neglected Tropical Diseases 2013} pages = {3} },The purpose of Google Flu Trends (GFT) is to use search keyword trends from Google.com to produce a daily estimate or nowcast of the occurrence of flu two weeks in advance of publication of official surveillance data. While not covered in detail in this paper Google Dengue Trends launched in June 2011 is a service that uses similar techniques to track Dengue fever. During the 2012 flu season we observed our algorithm overestimating influenza-like illness (ILI). We have concluded that our algorithm for Flu and Dengue were susceptible to heightened media coverage and have since developed several improvements.,http://research.google.com/pubs/archive/41763.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=GOOGLE+DISEASE+TRENDS:++AN+UPDATE+Copeland+Romano+Zhang+Hecht+Zigmond+Stefansen,http://research.google.com/pubs/pub41763.html
Large-Scale Manifold Learning,Computer Vision and Pattern Recognition (CVPR) (2008),2008,Ameet Talwalkar Sanjiv Kumar Henry A. Rowley,@inproceedings{34395 title = {Large-Scale Manifold Learning} author = {Ameet Talwalkar and Sanjiv Kumar and Henry A. Rowley} year = 2008 URL = {http://www.sanjivk.com/largeManifold.pdf} booktitle = {Computer Vision and Pattern Recognition (CVPR)} },This paper examines the problem of extracting low-dimensional manifold structure given millions of high-dimensional face images. Specifically we address the computational challenges of nonlinear dimensionality reduction via Isomap and Laplacian Eigenmaps using a graph containing about 18 million nodes and 65 million edges. Since most manifold learning techniques rely on spectral decomposition we first analyze two approximate spectral decomposition techniques for large dense matrices (Nystrom and Column-sampling) providing the first direct theoretical and empirical comparison between these techniques. We next show extensive experiments on learning low-dimensional embeddings for two large face datasets: CMU-PIE (35 thousand faces) and a web dataset (18 million faces). Our comparisons show that the Nystrom approximation is superior to the Column-sampling method. Furthermore approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classification with the labeled CMU-PIE dataset.,http://research.google.com/pubs/archive/34395.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Manifold+Learning+Talwalkar+Kumar+Rowley,http://research.google.com/pubs/pub34395.html
Reducing Photon Mapping Bandwidth by Query Reordering,IEEE Transactions on Visualization and Computer Graphics vol. 14 (2008),2008,Joshua Steinhurst Greg Coombe Anselmo Lastra,@article{33468 title = {Reducing Photon Mapping Bandwidth by Query Reordering} author = {Joshua Steinhurst and Greg Coombe and Anselmo Lastra} year = 2008 URL = {http://doi.ieeecomputersociety.org/10.1109/TVCG.2007.70413} journal = {IEEE Transactions on Visualization and Computer Graphics} volume = {14} },Photon mapping places an enormous burden on the memory hierarchy. Rendering a 512_512 image of a simple scene can require more than 196GB of raw bandwidth to the photon map data structure. This bandwidth is a major obstacle to real time photon mapping. This paper investigates two approaches for reducing the required bandwidth: 1) reordering the kNN searches; and 2) cache conscious data structures. Using a Hilbert curve reordering we demonstrate an experimental lower bound of 15MB of bandwidth for the same scene. Unfortunately this improvement of four orders of magnitude requires a prohibitive amount of intermediate storage. We introduce two novel cost-effective algorithms that reduce the bandwidth by one order of magnitude. Scenes of different complexities are shown to exhibit similar reductions in bandwidth. We explain why the choice of data structure does not achieve similar reductions. We also examine the interaction of query reordering with two photon map acceleration techniques importance sampling and the irradiance cache. Query reordering exploits the additional coherence that arises from the use of importance sampling in scenes with glossy surfaces. Irradiance caching also benefits from query reordering even when complex surface geometry reduces the effectiveness of the irradiance cache.,http://doi.ieeecomputersociety.org/10.1109/TVCG.2007.70413,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reducing+Photon+Mapping+Bandwidth+by+Query+Reordering+Steinhurst+Coombe+Lastra,http://research.google.com/pubs/pub33468.html
Emotional Memory and Adaptive Personalities,Handbook of Synthetic Emotions and Sociable Robotics Information Science Reference an imprint of IGI Global www.info-sci-ref.com (2009) pp. 391-412,2009,Anthony Francis Manish Mehta Ashwin Ram,@inbook{34434 title = {Emotional Memory and Adaptive Personalities} author = {Anthony Francis and Manish Mehta and Ashwin Ram} year = 2009 URL = {http://www.igi-global.com/bookstore/chapter.aspx?titleid=21518} booktitle = {Handbook of Synthetic Emotions and Sociable Robotics} pages = {391-412} address = {www.info-sci-ref.com} },Believable agents designed for long-term interaction with human users need to adapt to them in a way which appears emotionally plausible while maintaining a consistent personality. For short-term interactions in restricted environments scripting and state machine techniques can create agents with emotion and personality but these methods are labor intensive hard to extend and brittle in new environments. Fortunately research in memory emotion and personality in humans and animals points to a solution to this problem. Emotions focus an animal’s attention on things it needs to care about and strong emotions trigger enhanced formation of memory enabling the animal to adapt its emotional response to the objects and situations in its environment. In humans this process becomes reflective: emotional stress or frustration can trigger re-evaluating past behavior with respect to personal standards which in turn can lead to setting new strategies or goals. To aid the authoring of adaptive agents we present an artificial intelligence model inspired by these psychological results in which an emotion model triggers case-based emotional preference learning and behavioral adaptation guided by personality models. Our tests of this model on robot pets and embodied characters show that emotional adaptation can extend the range and increase the behavioral sophistication of an agent without the need for authoring additional hand-crafted behaviors.,http://www.igi-global.com/bookstore/chapter.aspx?titleid=21518,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Emotional+Memory+and+Adaptive+Personalities+Francis+Mehta+Ram,http://research.google.com/pubs/pub34434.html
The non-adaptive query complexity of testing k-parities,Chicago Journal of Theoretical Computer Science vol. 2013 (2013) pp. 1-11,2013,Harry Buhrman David Garcia Arie Matsliah Ronald de Wolf,@article{41544 title = {The non-adaptive query complexity of testing k-parities} author = {Harry Buhrman and David Garcia and Arie Matsliah and Ronald de Wolf} year = 2013 URL = {http://cjtcs.cs.uchicago.edu/articles/2013/6/contents.html} journal = {Chicago Journal of Theoretical Computer Science} pages = {1-11} volume = {2013} },We prove tight bounds of _(klogk) queries for non-adaptively testing whether a function f:{01}^n_{01} is a k-parity or far from any k-parity. The lower bound combines a recent method of Blais Brody and Matulef to get lower bounds for testing from communication complexity with an Ω(klogk) lower bound for the one-way communication complexity of k-disjointness.,http://research.google.com/pubs/archive/41544.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+non-adaptive+query+complexity+of+testing+k-parities+Buhrman+Garcia+Matsliah+Wolf,http://research.google.com/pubs/pub41544.html
Video Quality Assessment for Web Content Mirroring,Imaging and Multimedia Analytics in a Web and Mobile World 2014 IS&T/SPIE Electronic Imaging San Francisco California pp. 9027-11,2014,Ye He Kevin Fei Gus Fernandez Edward J. Delp,@inproceedings{42110 title = {Video Quality Assessment for Web Content Mirroring} author = {Ye He and Kevin Fei and Gus Fernandez and Edward J. Delp} year = 2014 booktitle = {Imaging and Multimedia Analytics in a Web and Mobile World 2014} pages = {9027-11} address = {San Francisco California} },Due to the increasing user expectation on watching experience moving web high quality video streaming content from the small screen in mobile devices to the larger TV screen has become popular. It is crucial to develop video quality metrics to measure the quality change for various devices or network conditions. In this paper we propose an automated scoring system to quantify user satisfaction. We compare the quality of local videos with the videos transmitted to a TV. Four video quality metrics namely Image Quality Rendering Quality Freeze Time Ratio and Rate of Freeze Events are used to measure video quality change during web content mirroring. To measure image quality and rendering quality we compare the matched frames between the source video and the destination video using barcode tools. Freeze time ratio and rate of freeze events are measured after extracting video timestamps. Several user studies are conducted to evaluate the impact of each objective video quality metric on the subjective user watching experience.,http://research.google.com/pubs/archive/42110.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Video+Quality+Assessment+for+Web+Content+Mirroring+He+Fei+Fernandez+Delp,http://research.google.com/pubs/pub42110.html
Algorithms for Secretary Problems on Graphs and Hypergraphs,ICALP 2009,2009,Nitish Korula Martin Pál,@inproceedings{35260 title = {Algorithms for Secretary Problems on Graphs and Hypergraphs} author = {Nitish Korula and Martin Pál} year = 2009 URL = {http://front.math.ucdavis.edu/0807.1139} booktitle = {ICALP 2009} },We examine several online matching problems with applications to Internet advertising reservation systems. Consider an edge-weighted bipartite graph G with partite sets L R. We develop an 8-competitive algorithm for the following secretary problem: Initially given R and the size of L the algorithm receives the vertices of L sequentially in a random order. When a vertex l \in L is seen all edges incident to l are revealed together with their weights. The algorithm must immediately either match l to an available vertex of R or decide that l will remain unmatched. Dimitrov and Plaxton show a 16-competitive algorithm for the transversal matroid secretary problem which is the special case with weights on vertices not edges. (Equivalently one may assume that for each l \in L the weights on all edges incident to l are identical.) We use a similar algorithm but simplify and improve the analysis to obtain a better competitive ratio for the more general problem. Perhaps of more interest is the fact that our analysis is easily extended to obtain competitive algorithms for similar problems such as to find disjoint sets of edges in hypergraphs where edges arrive online. We also introduce secretary problems with adversarially chosen groups. Finally we give a 2e-competitive algorithm for the secretary problem on graphic matroids where with edges appearing online the goal is to find a maximum-weight acyclic subgraph of a given graph.,http://front.math.ucdavis.edu/0807.1139,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Algorithms+for+Secretary+Problems+on+Graphs+and+Hypergraphs+Korula+P%C3%A1l,http://research.google.com/pubs/pub35260.html
Feature Seeding for Action Recognition,International Conference on Computer Vision (ICCV) (2011),2011,Pyry Matikainen Rahul Sukthankar Martial Hebert,@inproceedings{40359 title = {Feature Seeding for Action Recognition} author = {Pyry Matikainen and Rahul Sukthankar and Martial Hebert} year = 2011 booktitle = {International Conference on Computer Vision (ICCV)} },Progress in action recognition has been in large part due to advances in the features that drive learning-based methods. However the relative sparsity of training data and the risk of overfitting have made it difficult to directly search for good features. In this paper we suggest using synthetic data to search for robust features that can more easily take advantage of limited data rather than using the synthetic data directly as a substitute for real data. We demonstrate that the features discovered by our selection method which we call seeding improve performance on an action classification task on real data even though the synthetic data from which our features are seeded differs significantly from the real data both in terms of appearance and the set of action classes.,http://research.google.com/pubs/archive/40359.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Feature+Seeding+for+Action+Recognition+Matikainen+Sukthankar+Hebert,http://research.google.com/pubs/pub40359.html
Programming Google App Engine,O'Reilly Media Inc. 1005 Gravenstein Highway North Sebastopol CA 95472 (2009) pp. 367,2009,Dan Sanderson,@book{36601 title = {Programming Google App Engine} author = {Dan Sanderson} year = 2009 URL = {http://oreilly.com/catalog/9780596522735/} booktitle = {Programming Google App Engine} pages = {367} address = {1005 Gravenstein Highway North Sebastopol CA 95472} },Google App Engine is a cloud computing service unlike any other: it provides a simple model for building applications that scale automatically to accommodate millions of users. With Programming Google App Engine you'll get expert practical guidance that will help you make the best use of this powerful platform. Google engineer Dan Sanderson shows you how to design your applications for scalability and how to perform common development tasks using App Engine's APIs and scalable services.,http://oreilly.com/catalog/9780596522735/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Programming+Google+App+Engine+Sanderson,http://research.google.com/pubs/pub36601.html
Incremental Clicks Impact of Mobile Search Advertising,Google Inc. (2013),2013,Shaun Lysen,@techreport{41334 title = {Incremental Clicks Impact of Mobile Search Advertising} author = {Shaun Lysen} year = 2013 institution = {Google Inc.} },In this research we examine how the number of mobile organic clicks changes when advertisers significantly change their mobile ad spend. This continues the line of research of search ads pause by applying it to the mobile platform. We utilize a statistical model to estimate the fraction of clicks that can be attributed to mobile search advertising. A metastudy of 327 advertisers reveals that 88% of ad clicks are incremental in the sense that the visits to the advertiser’s site would not have occurred without the mobile ad campaigns.,http://research.google.com/pubs/archive/41334.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Incremental+Clicks+Impact+of+Mobile+Search+Advertising+Lysen,http://research.google.com/pubs/pub41334.html
Example-based Image Compression,International Conference on Image Processing (ICIP 2010),2010,Jing-Yu Cui Saurabh Mathur Michele Covell Vivek Kwatra Mei Han,@inproceedings{36415 title = {Example-based Image Compression} author = {Jing-Yu Cui and Saurabh Mathur and Michele Covell and Vivek Kwatra and Mei Han} year = 2010 URL = {http://cs.unc.edu/~kwatra/publications.html#compression} booktitle = {International Conference on Image Processing (ICIP 2010)} },The current standard image-compression approaches rely on fairly simple predictions using either block- or wavelet-based methods. While many more sophisticated texture-modeling approaches have been proposed most do not provide a significant improvement in compression rate over the current standards at a workable encoding complexity level. We re-examine this area using example-based texture prediction. We find that we can provide consistent and significant improvements over JPEG reducing the bit rate by more than 20% for many PSNR levels. These improvements require consideration of the differences between residual energy and prediction/residual compressibility when selecting a texture prediction as well as careful control of the computational complexity in encoding.,http://research.google.com/pubs/archive/36415.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Example-based+Image+Compression+Cui+Mathur+Covell+Kwatra+Han,http://research.google.com/pubs/pub36415.html
Practical Memory Checking with Dr. Memory,Proceedings of the IEEE/ACM International Symposium on Code Generation and Optimization IEEE Computer Society Los Alamitos CA USA (2011) pp. 213-223,2011,Derek Bruening Qin Zhao,@inproceedings{37274 title = {Practical Memory Checking with Dr. Memory} author = {Derek Bruening and Qin Zhao} year = 2011 booktitle = {Proceedings of the IEEE/ACM International Symposium on Code Generation and Optimization} pages = {213-223} address = {Los Alamitos CA USA} },Memory corruption reading uninitialized memory using freed memory and other memory-related errors are among the most difficult programming bugs to identify and fix due to the delay and non-determinism linking the error to an observable symptom. Dedicated memory checking tools are invaluable for finding these errors. However such tools are difficult to build and because they must monitor all memory accesses by the application they incur significant overhead. Accuracy is another challenge: memory errors are not always straightforward to identify and numerous false positive error reports can make a tool unusable. A third obstacle to creating such a tool is that it depends on low-level operating system and architectural details making it difficult to port to other platforms and difficult to target proprietary systems like Windows. This paper presents Dr. Memory a memory checking tool that operates on both Windows and Linux applications. Dr. Memory handles the complex and not fully documented Windows environment and avoids reporting false positive memory leaks that plague traditional leak locating algorithms. Dr. Memory employs efficient instrumentation techniques; a direct comparison with the state-of-the-art Valgrind Memcheck tool reveals that Dr. Memory is twice as fast as Memcheck on average and up to four times faster on individual benchmarks.,http://research.google.com/pubs/archive/37274.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Practical+Memory+Checking+with+Dr.+Memory+Bruening+Zhao,http://research.google.com/pubs/pub37274.html
Active Tuples-based Scheme for Bounding Posterior Beliefs,JAIR vol. 39 (2010) pp. 335-371,2010,Bozhena Bidyuk Rina Dechte Emma Rollon,@article{37518 title = {Active Tuples-based Scheme for Bounding Posterior Beliefs} author = {Bozhena Bidyuk and Rina Dechte and Emma Rollon} year = 2010 URL = {http://www.jair.org/media/2945/live-2945-5215-jair.pdf} journal = {JAIR} pages = {335--371} volume = {39} },The paper presents a scheme for computing lower and upper bounds on the posterior marginals in Bayesian networks with discrete variables. Its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. The scheme uses the cutset conditioning principle to tighten existing bounding schemes and to facilitate anytime behavior utilizing a fixed number of cutset tuples. The accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. We demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme.,http://www.jair.org/media/2945/live-2945-5215-jair.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Active+Tuples-based+Scheme+for+Bounding+Posterior+Beliefs+Bidyuk+Dechter+Rollon,http://research.google.com/pubs/pub37518.html
Quantum query complexity of state conversion,Proceeding of 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS'11) (2011) pp. 344-353,2011,Troy Lee Rajat Mittal Ben Reichardt Robert Spalek Mario Szegedy,@inproceedings{41472 title = {Quantum query complexity of state conversion} author = {Troy Lee and Rajat Mittal and Ben Reichardt and Robert Spalek and Mario Szegedy} year = 2011 URL = {http://arxiv.org/abs/1011.3020} note = {arXiv:1011.3020 [quant-ph]} booktitle = {Proceeding of 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS'11)} pages = {344-353} },State conversion generalizes query complexity to the problem of converting between two input-dependent quantum states by making queries to the input. We characterize the complexity of this problem by introducing a natural information-theoretic norm that extends the Schur product operator norm. The complexity of converting between two systems of states is given by the distance between them as measured by this norm. In the special case of function evaluation the norm is closely related to the general adversary bound a semi-definite program that lower-bounds the number of input queries needed by a quantum algorithm to evaluate a function. We thus obtain that the general adversary bound characterizes the quantum query complexity of any function whatsoever. This generalizes and simplifies the proof of the same result in the case of boolean input and output. Also in the case of function evaluation we show that our norm satisfies a remarkable composition property implying that the quantum query complexity of the composition of two functions is at most the product of the query complexities of the functions up to a constant. Finally our result implies that discrete and continuous-time query models are equivalent in the bounded-error setting even for the general state-conversion problem.,http://research.google.com/pubs/archive/41472.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Quantum+query+complexity+of+state+conversion+Lee+Mittal+Reichardt+Spalek+Szegedy,http://research.google.com/pubs/pub41472.html
A Discriminative Kernel-based Approach to Retrieval Images from Text Queries,IEEE Transactions on Pattern Analysis and Machine Intelligence vol. 30 (2008) pp. 1371-1384,2008,David Grangier Samy Bengio,@article{33027 title = {A Discriminative Kernel-based Approach to Retrieval Images from Text Queries} author = {David Grangier and Samy Bengio} year = 2008 URL = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2007.70791} journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence} pages = {1371--1384} volume = {30} },This paper proposes a discriminative model for the retrieval of images from text queries. Contrary to previous research this approach does not rely on an intermediate annotation task. Instead it addresses the retrieval problem directly and learns from a criterion related to the final ranking performance of the retrieval model. Moreover our learning procedure builds upon recent work on the online learning of kernel-based classifiers yielding an efficient scalable training algorithm. The experiments performed over stock photography data show the advantage of our discriminative ranking approach over state-of-the-art alternatives (e.g. our model yields $26.3\%$ average precision over the standard Corel benchmark which should be compared to $22.0\%$ for the best alternative model evaluated).,http://research.google.com/pubs/archive/33027.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Discriminative+Kernel-based+Approach+to+Retrieval+Images+from+Text+Queries+Grangier+Bengio,http://research.google.com/pubs/pub33027.html
Web Surveys for the General Population: How why and when?,Natcen (2014) pp. 22,2014,Gerri Nicolaas Lisa Calderwood Peter Lynn Caroline Roberts Mario Callegaro,@techreport{42522 title = {Web Surveys for the General Population: How why and when?} author = {Gerri Nicolaas and Lisa Calderwood and Peter Lynn and Caroline Roberts and Mario Callegaro} year = 2014 URL = {http://www.natcenweb.co.uk/genpopweb/documents/GenPopWeb-final-report.pdf} note = {Mario Callegaro was member of the UK Core Group and provided input on drafts of this report} institution = {Natcen} },Cultural and technological change has made the web a possible and even desirable mode for complex social surveys but the financial challenges faced by the Research Councils and the UK Government has accelerated this shift creating an urgent need to explore both its potential and hazards for a range of studies. While some progress in carrying out large-scale complex social surveys on the web has been made there is still no consensus about how this can best be achieved while maintaining population representativeness and preserving data quality. To address this problem the NCRM funded a network of methodological innovation “Web Surveys for the General Population: How Why and When?” (also known by its acronym GenPopWeb). A key objective of the network’s activities was to review and synthesise existing knowledge about the use of web-based data collection for general population samples and to identify areas where new research is needed. The network “Web Surveys for the General Population: Why How and When?” was supported with funding from the ESRC National Centre for Research Methods under the initiative Networks for Methodological Innovation 2012. We are also grateful to the Institute of Education and the University of Essex for hosting the two main events of the network. We would like to thank all of the presenters at the events as well as the participants for their contribution. Particular thanks are due to the UK Core Group for their time advice and support: Bill Blyth TNS Global Mario Callegaro Google UK Ed Dunn & Laura Wilson ONS Rory Fitzgerald City University London Joanna Lake ESRC Carli Lessof & Joel Williams TNS BMRB Nick Moon GfK NOP Patten Smith Ipsos MORI Professor Patrick Sturgis NCRM Joe Twyman & Michael Wagstaff YouGov UK,http://www.natcenweb.co.uk/genpopweb/documents/GenPopWeb-final-report.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Surveys+for+the++General+Population:++How+why+and+when%3F+Nicolaas+Calderwood+Lynn+Roberts+Callegaro,http://research.google.com/pubs/pub42522.html
Practical Privacy Concerns in a Real World Browser,W3C Workshop on Privacy for Advanced Web APIs W3C (2010) pp. 4,2010,Ian Fette Jochen Eisinger,@incollection{36731 title = {Practical Privacy Concerns in a Real World Browser} author = {Ian Fette and Jochen Eisinger} year = 2010 URL = {http://www.w3.org/2010/api-privacy-ws/papers/privacy-ws-24.pdf} booktitle = {W3C Workshop on Privacy for Advanced Web APIs} pages = {4} },Google Chrome has implemented a number of HTML5 APIs including the Geolocation API and various storage APIs. In this paper we discuss some of our experiences on the Google Chrome team in implementing these APIs as well as our thoughts around privacy for new APIs we are considering implementing. Specifically we discuss our ideas of how providing access to things such as speech web cameras and filesystems can be done in ways that are understandable and in the natural flow of users.,http://research.google.com/pubs/archive/36731.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Practical+Privacy+Concerns+in+a+Real+World+Browser+Fette+Eisinger,http://research.google.com/pubs/pub36731.html
An Extension of BLANC to System Mentions,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) (2014) pp. 24-29,2014,Xiaoqiang Luo Sameer Pradhan Marta Recasens Eduard Hovy,@inproceedings{42559 title = {An Extension of BLANC to System Mentions} author = {Xiaoqiang Luo and Sameer Pradhan and Marta Recasens and Eduard Hovy} year = 2014 URL = {http://69.195.124.161/~aclwebor/anthology/P/P14/P14-2005.pdf} booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)} pages = {24--29} },BLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions. This paper extends the original BLANC (“BLANC-gold” henceforth) to system mentions removing the gold mention assumption. The proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data.,http://research.google.com/pubs/archive/42559.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Extension+of+BLANC+to+System+Mentions+Luo+Pradhan+Recasens+Hovy,http://research.google.com/pubs/pub42559.html
Power Provisioning for a Warehouse-sized Computer,The 34th ACM International Symposium on Computer Architecture (2007),2007,Xiaobo Fan Wolf-Dietrich Weber Luiz André Barroso,@inproceedings{32980 title = {Power Provisioning for a Warehouse-sized Computer} author = {Xiaobo Fan and Wolf-Dietrich Weber and Luiz André Barroso} year = 2007 URL = {http://research.google.com/archive/power_provisioning.pdf} booktitle = {The 34th ACM International Symposium on Computer Architecture} },Large-scale Internet services require a computing infrastructure that can be appropriately described as a warehouse-sized computing system. The cost of building datacenter facilities capable of delivering a given power capacity to such a computer can rival the recurring energy consumption costs themselves. Therefore there are strong economic incentives to operate facilities as close as possible to maximum capacity so that the non-recurring facility costs can be best amortized. That is difficult to achieve in practice because of uncertainties in equipment power ratings and because power consumption tends to vary significantly with the actual computing activity. Effective power provisioning strategies are needed to determine how much computing equipment can be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of large collections of servers (up to 15 thousand) for different classes of applications over a period of approximately six months. Those observations allow us to evaluate opportunities for maximizing the use of the deployed power capacity of datacenters and assess the risks of over-subscribing it. We find that even in well-tuned applications there is a noticeable gap (7 - 16%) between achieved and theoretical aggregate peak power usage at the cluster level (thousands of servers). The gap grows to almost 40% in whole datacenters. This headroom can be used to deploy additional compute equipment within the same power budget with minimal risk of exceeding it. We use our modeling framework to estimate the potential of power management schemes to reduce peak power and energy usage. We find that the opportunities for power and energy savings are significant but greater at the cluster-level (thousands of servers) than at the rack-level (tens). Finally we argue that systems need to be power efficient across the activity range and not only at peak performance levels.,http://research.google.com/archive/power_provisioning.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Power+Provisioning+for+a+Warehouse-sized+Computer+Fan+Weber+Barroso,http://research.google.com/pubs/pub32980.html
Making “Push On Green” a Reality: Issues & Actions Involved in Maintaining a Production Service,;login: vol. 39 number 5 (2014) pp. 26-32,2014,Daniel V. Klein Dina M. Betser Mathew G. Monroe,@article{42576 title = {Making “Push On Green” a Reality: Issues & Actions Involved in Maintaining a Production Service} author = {Daniel V. Klein and Dina M. Betser and Mathew G. Monroe} year = 2014 journal = {;login:} pages = {26-32} volume = {39 number 5} },Updating production software is a process that may require dozens if not hundreds of steps. These include creating and testing the new code building new binaries and packages associating the packages with a versioned release updating the jobs in production datacenters possibly modifying database schemata and testing and verifying the results. There are boxes to check and approvals to seek and the more automated the process the easier it becomes. When releases can be made faster it is possible to release more often and organizationally one becomes less afraid to “release early release often”. This is the fundamental driving force behind the work described in this paper – making rollouts as easy and as automated as possible so that when a “green” condition (defined below) is detected we can more quickly perform a new rollout. Humans may still be needed somewhere in the loop but we strive to reduce the purely mechanical toil they need to perform. This paper describes how we as Site Reliability Engineers working on several different Ads and Commerce services at Google do this and shares information on how to enable other organizations to do the same. We define Push On Green and describe the development and deployment of best practices that serve as a foundation for this kind of undertaking. Using a “sample service” at Google as an example we look at the historical development of the mechanization of the rollout process and discuss the steps taken to further automate it. We then examine the steps remaining both near and long-term as we continue to gain experience and advance the process towards full automation. We conclude with a set of concrete recommendations for other groups wishing to implement a Push On Green system that keeps production systems not only up-and-running but also updated with as little engineer-involvement and user-visible downtime as possible.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Making+%E2%80%9CPush+On+Green%E2%80%9D+a+Reality:+Issues+%26+Actions+Involved+in+Maintaining+a+Production+Service+Klein+Betser+Monroe,http://research.google.com/pubs/pub42576.html
Privacy Mediators: Helping IoT Cross the Chasm,Hot Topics in Mobile Computing (Hot Mobile) ACM (2016) (to appear),2016,Nigel Davies Nina Taft Mahadev Satyanarayanan Sarah Clinch Brandon Amos,@inproceedings{44691 title = {Privacy Mediators: Helping IoT Cross the Chasm} author = {Nigel Davies and Nina Taft and Mahadev Satyanarayanan and Sarah Clinch and Brandon Amos} year = 2016 booktitle = {Hot Topics in Mobile Computing (Hot Mobile)} },Unease over data privacy will retard consumer acceptance of IoT deployments. The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud. This is a direct consequence of the over-centralization of today’s cloud-based IoT hub designs. We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream. Each mediator is in the same administrative domain as the sensors whose data is being collected and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain. This solution ne- cessitates a logical point of presence for mediators within the admin- istrative boundaries of each organization. Such points of presence are provided by cloudlets which are small locally-administered data centers at the edge of the Internet that can support code mobility. The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility.,http://research.google.com/pubs/archive/44691.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Privacy+Mediators:+Helping+IoT+Cross+the+Chasm+Davies+Taft+Satyanarayanan+Clinch+Amos,http://research.google.com/pubs/pub44691.html
Advertising on YouTube and TV: A Meta-analysis of Optimal Media-mix Planning,Google Inc. (2015) pp. 1-28 (to appear),2015,Georg M. Goerg Christoph Best Sheethal Shobowale Jim Koehler Nicolas Remy,@techreport{44291 title = {Advertising on YouTube and TV: A Meta-analysis of Optimal Media-mix Planning} author = {Georg M. Goerg and Christoph Best and Sheethal Shobowale and Jim Koehler and Nicolas Remy} year = 2015 institution = {Google Inc.} },In this work we investigate under what circumstances a TV campaign should be complemented with online advertising to increase combined reach. First we use probabilistic models to derive necessary and sufficient conditions. We then test these optimality conditions on empirical findings of a large collection of TV campaigns to answer two important questions: i) which characteristics of a TV campaign make it favorable to shift part of its budget to online advertising?; and ii) if it should shift how much cost savings and additional reach can advertisers expect? First we use classification methods such as linear discriminant analysis logistic regression and decision trees to decide whether a TV campaign should add online advertising; secondly we train linear and support vector regression models to predict optimal budget allocation cost savings or additional reach. To train these models we use optimization results on roughly 26000 campaigns. We do not only achieve excellent out-of-sample predictive power but also obtain simple interpretable and actionable rules that improve the understanding of media mix advertising.,http://research.google.com/pubs/archive/44291.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Advertising+on+YouTube+and+TV:+A+Meta-analysis+of+Optimal+Media-mix+Planning+Goerg+Best+Shobowale+Koehler+Remy,http://research.google.com/pubs/pub44291.html
Experience report: Haskell as a reagent: results and observations on the use of Haskell in a python project,Proceedings of the 15th ACM SIGPLAN international conference on Functional programming ACM New York NY USA (2010) pp. 369-374,2010,Iustin Pop,@inproceedings{36970 title = {Experience report: Haskell as a reagent: results and observations on the use of Haskell in a python project} author = {Iustin Pop} year = 2010 URL = {http://dx.doi.org/10.1145/1863543.1863595} booktitle = {Proceedings of the 15th ACM SIGPLAN international conference on Functional programming} pages = {369--374} address = {New York NY USA} },In system administration the languages of choice for solving automation tasks are scripting languages owing to their flexibility extensive library support and quick development cycle. Functional programming is more likely to be found in software development teams and the academic world. This separation means that system administrators cannot use the most effective tool for a given problem; in an ideal world we should be able to mix and match different languages based on the problem at hand. This experience report details our initial introduction and use of Haskell in a mature medium size project implemented in Python. We also analyse the interaction between the two languages and show how Haskell has excelled at solving a particular type of real-world problems.,http://dx.doi.org/10.1145/1863543.1863595,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Experience+report:+Haskell+as+a+reagent:+results+and+observations+on+the+use+of+Haskell+in+a+python+project+Pop,http://research.google.com/pubs/pub36970.html
Up Next: Retrieval Methods for Large Scale Related Video Suggestion,Proceedings of KDD 2014 New York NY USA pp. 1769-1778,2014,Michael Bendersky Lluis Garcia Pueyo Vanja Josifovski Jeremiah J. Harmsen Dima Lepikhin,@inproceedings{42623 title = {Up Next: Retrieval Methods for Large Scale Related Video Suggestion} author = {Michael Bendersky and Lluis Garcia Pueyo and Vanja Josifovski and Jeremiah J. Harmsen and Dima Lepikhin} year = 2014 booktitle = {Proceedings of KDD 2014} pages = {1769-1778} address = {New York NY USA} },The explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advances in video retrieval recommendation and discovery. In this paper we focus on the task of video suggestion commonly found in many online applications. The current state-of-the-art video suggestion techniques are based on the collaborative filtering analysis and suggest videos that are likely to be co-viewed with the watched video. In this paper we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related videos. We propose two novel methods for topical video representation. The first method uses information retrieval heuristics such as tf-idf while the second method learns the optimal topical representations based on the implicit user feedback available in the online scenario. We conduct a large scale live experiment on YouTube traffic and demonstrate that augmenting collaborative filtering with topical representations significantly improves the quality of the related video suggestions in a live setting especially for categories with fresh and topically-rich video content such as news videos. In addition we show that employing user feedback for learning the optimal topical video representations can increase the user engagement by more than 80% over the standard information retrieval representation when compared to the collaborative filtering baseline.,http://research.google.com/pubs/archive/42623.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Up+Next:+Retrieval+Methods+for+Large+Scale+Related+Video+Suggestion+Bendersky+Garcia-Pueyo+Josifovski+Harmsen+Lepikhin,http://research.google.com/pubs/pub42623.html
Efficient Top-Down BTG Parsing for Machine Translation Preordering,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) Association for Computational Linguistics (2015) pp. 208-218,2015,Tetsuji Nakagawa,@inproceedings{43848 title = {Efficient Top-Down BTG Parsing for Machine Translation Preordering} author = {Tetsuji Nakagawa} year = 2015 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)} pages = {208-218} },We present an efficient incremental top-down parsing method for preordering based on Bracketing Transduction Grammar (BTG). The BTG-based preordering framework (Neubig et al. 2012) can be applied to any language using only parallel text but has the problem of computational efficiency. Our top-down parsing algorithm allows us to use the early update technique easily for the latent variable structured Perceptron algorithm with beam search and solves the problem. Experimental results showed that the top-down method is more than 10 times faster than a method using the CYK algorithm. A phrase-based machine translation system with the top-down method had statistically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers compared to baseline systems using existing preordering methods.,http://research.google.com/pubs/archive/43848.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Top-Down+BTG+Parsing+for+Machine+Translation+Preordering+Nakagawa,http://research.google.com/pubs/pub43848.html
On the Prospects for Building a Working Model of the Visual Cortex,Proceedings of AAAI-07 MIT Press Cambridge Massachusetts (2007) pp. 1597-1600,2007,Thomas Dean Glenn Carroll Richard Washington,@inproceedings{34771 title = {On the Prospects for Building a Working Model of the Visual Cortex} author = {Thomas Dean and Glenn Carroll and Richard Washington} year = 2007 booktitle = {Proceedings of AAAI-07} pages = {1597-1600} address = {Cambridge Massachusetts} },Human-level visual performance has remained largely beyond the reach of engineered systems despite decades of research and significant advances in problem formulation algorithms and computing power. We posit that significant progress can be made by combining existing technologies from machine vision insights from theoretical neuroscience and large-scale distributed computing. Such claims have been made before and so it is quite reasonable to ask what are the new ideas we bring to the table that might make a difference this time around. From a theoretical standpoint our primary point of departure from current practice is our reliance on exploiting time in order to turn an otherwise intractable unsupervised problem into a locally semi-supervised and plausibly tractable learning problem. From a pragmatic perspective our system architecture follows what we know of cortical neuroanatomy and provides a solid foundation for scalable hierarchical inference. This combination of features provides the framework for implementing a wide range of robust object-recognition capabilities.,http://research.google.com/pubs/archive/34771.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+Prospects+for+Building+a+Working+Model+of+the+Visual+Cortex+Dean+Carroll+Washington,http://research.google.com/pubs/pub34771.html
A New ELF Linker,Proceedings of the GCC Developers' Summit (2008),2008,Ian Lance Taylor,@inproceedings{34417 title = {A New ELF Linker} author = {Ian Lance Taylor} year = 2008 URL = {http://ols.fedoraproject.org/GCC/Reprints-2008/taylor-reprint.pdf} booktitle = {Proceedings of the GCC Developers' Summit} },gold is a new ELF linker recently added to the GNU binutils. I discuss why it made sense to write a new linker rather than extend the existing one. I describe the architecture of the linker and new features. I present performance measurements. I discuss future plans for the linker. I discuss the use of C++ in writing system tools.,http://research.google.com/pubs/archive/34417.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+New+ELF+Linker+Taylor,http://research.google.com/pubs/pub34417.html
Neither Snow Nor Rain Nor MITM ... An Empirical Analysis of Email Delivery Security,Proceedings of the Internet Measurement Conferene (2015),2015,Zakir Durumeric David Adrian Ariana Mirian James Kasten Elie Bursztein Nicolas Lidzborski Kurt Thomas Vijay Eranti Michael Bailey J. Alex Halderman,@inproceedings{43962 title = {Neither Snow Nor Rain Nor MITM ... An Empirical Analysis of Email Delivery Security} author = {Zakir Durumeric and David Adrian and Ariana Mirian and James Kasten and Elie Bursztein and Nicolas Lidzborski and Kurt Thomas and Vijay Eranti and Michael Bailey and J. Alex Halderman} year = 2015 booktitle = {Proceedings of the Internet Measurement Conferene} },The SMTP protocol is responsible for carrying some of users most intimate communication but like other Internet protocols authentication and confidentiality were added only as an afterthought. In this work we present the first report on global adoption rates of SMTP security extensions including: STARTTLS SPF DKIM and DMARC. We present data from two perspectives: SMTP server configurations for the Alexa Top Million domains and over a year of SMTP connections to and from Gmail. We find that the top mail providers (e.g. Gmail Yahoo and Outlook) all proactively encrypt and authenticate messages. However these best practices have yet to reach widespread adoption in a long tail of over 700000 SMTP servers of which only 35% successfully configure encryption and 1.1% specify a DMARC authentication policy. This security patchwork -- paired with SMTP policies that favor failing open to allow gradual deployment -- exposes users to attackers who downgrade TLS connections in favor of cleartext and who falsify MX records to reroute messages. We present evidence of such attacks in the wild highlighting seven countries where more than 20% of inbound Gmail messages arrive in cleartext due to network attackers.,http://research.google.com/pubs/archive/43962.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Neither+Snow+Nor+Rain+Nor+MITM+...+An+Empirical+Analysis+of+Email+Delivery+Security+Durumeric+Adrian+Mirian+Kasten+Bursztein+Lidzborski+Thomas+Eranti+Bailey+Halderman,http://research.google.com/pubs/pub43962.html
Learning to Search Efficiently in High Dimensions,Neural Information Processing Systems (2011),2011,Zhen Li Huazhong Ning Liangliang Cao Tong Zhan Yihong Gong Thomas S. Huang,@inproceedings{37686 title = {Learning to Search Efficiently in High Dimensions} author = {Zhen Li and Huazhong Ning and Liangliang Cao and Tong Zhan and Yihong Gong and Thomas S. Huang} year = 2011 booktitle = {Neural Information Processing Systems} },High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications specialized data structures are required to achieve computational efﬁciency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees k-means trees). While supervised learning algorithms have been applied to related problems those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efﬁciency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efﬁciency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efﬁcient large scale search. Our approach takes both search quality and computational cost into consideration. Speciﬁcally we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efﬁciently converted into an inverted indexing data structure which can leverage modern text search infrastructure to achieve both scalability and efﬁciency. Experimental results show that our approach signiﬁcantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing) as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).,http://research.google.com/pubs/archive/37686.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Search+Efficiently+in+High+Dimensions+Li+Ning+Cao+Zhang+Gong+Huang,http://research.google.com/pubs/pub37686.html
An Integrated Framework for Spatio-Temporal-Textual Search and Mining,20th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS 2012) ACM 2 Penn Plaza Suite 701 New York NY 10121 pp. 570-573,2012,Bingsheng Wang Haili Dong Arnold Boedihardjo Chang-Tien Lu Harland Yu Ing-Ray Chen Jing Dai,@inproceedings{40571 title = {An Integrated Framework for Spatio-Temporal-Textual Search and Mining} author = {Bingsheng Wang and Haili Dong and Arnold Boedihardjo and Chang-Tien Lu and Harland Yu and Ing-Ray Chen and Jing Dai} year = 2012 booktitle = {20th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS 2012)} pages = {570--573} address = {2 Penn Plaza Suite 701 New York NY 10121} },This paper presents an integrated framework for Spatio-Temporal-Textual (STT) information retrieval and knowledge discovery system. The proposed ensemble framework contains an efficient STT search engine with multiple indexing ranking and scoring schemes an effective STT pattern miner with Spatio-Temporal (ST) analytics and novel STT topic modeling. Specifically we design an effective prediction prototype with a third-order linear regression model and present an innovative STT topic modeling relevance ranker to score documents based on inherent STT features under topical space. We demonstrate the framework with a crime dataset from the Washington DC area from 2006 to 2010 and a global terrorism dataset from 2004 to 2010.,http://research.google.com/pubs/archive/40571.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Integrated+Framework+for+Spatio-Temporal-Textual+Search+and+Mining+Wang+Dong+Boedihardjo+Lu+Yu+Chen+Dai,http://research.google.com/pubs/pub40571.html
The Optical Mouse: Early Biomimetic Embedded Vision,Advnances in Embedded Computer Vision Springer (2014) pp. 3-22,2014,Richard F. Lyon,@inbook{43260 title = {The Optical Mouse: Early Biomimetic Embedded Vision} author = {Richard F. Lyon} year = 2014 URL = {http://dicklyon.com/tech/OMouse/Lyon_embedded_vision.pdf} booktitle = {Advnances in Embedded Computer Vision} pages = {3--22} },The 1980 Xerox optical mouse invention and subsequent product was a successful deployment of embedded vision as well as of the Mead–Conway VLSI design methodology that we developed at Xerox PARC in the late 1970s. The design incorporated an interpretation of visual lateral inhibition essentially mimicking biology to achieve a wide dynamic range or light-level-independent operation. Conceived in the context of a research group developing VLSI design methodologies the optical mouse chip represented an approach to self-timed semi-digital design with the analog image-sensing nodes connecting directly to otherwise digital logic using a switch-network methodology. Using only a few hundred gates and pass transistors in 5-micron nMOS technology the optical mouse chip tracked the motion of light dots in its field of view and reported motion with a pair of 2-bit Gray codes for x and y relative position—just like the mechanical mice of the time. Besides the chip the only other electronic components in the mouse were the LED illuminators.,http://research.google.com/pubs/archive/43260.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Optical+Mouse:+Early+Biomimetic+Embedded+Vision+Lyon,http://research.google.com/pubs/pub43260.html
Assessing a New Advertising Effect: Measurement of the Impact of Television Commercials on Internet Search Queries,Journal of Advertising Research vol. 50 (2010) pp. 162-168,2010,Dan Zigmond Horst Stipp,@article{39974 title = {Assessing a New Advertising Effect: Measurement of the Impact of Television Commercials on Internet Search Queries} author = {Dan Zigmond and Horst Stipp} year = 2010 journal = {Journal of Advertising Research} pages = {162--168} volume = {50} },Most Americans today use both television and the Internet on a daily basis and studies have shown that many are frequently online or in proximity of a computer while they are watching television. One result of these multi-platform media use patterns is a new television advertising effect: Today’s consumer can easily obtain more information on an advertised product by searching for more information on the Web. This article demonstrates the measurement of such an effect by introducing a new metric—a measure of changes in Google search queries—that can show how TV commercials or sponsorships can trigger Internet searches by consumers. We believe this metric is a valuable addition to the researcher’s toolkit for assessing advertising effects and regions of interest as it measures an actual behavioral advertising response.,http://research.google.com/pubs/archive/39974.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Assessing+a+New+Advertising+Effect:+Measurement+of+the+Impact+of+Television+Commercials+on+Internet+Search+Queries+Zigmond+Stipp,http://research.google.com/pubs/pub39974.html
Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution,Journal of Artificial Intelligence Research vol. 52 (2015) pp. 445-475,2015,Marie-Catherine de Marneffe Marta Recasens Christopher Potts,@article{43407 title = {Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution} author = {Marie-Catherine de Marneffe and Marta Recasens and Christopher Potts} year = 2015 URL = {http://www.jair.org/media/4565/live-4565-8580-jair.pdf} journal = {Journal of Artificial Intelligence Research} pages = {445-475} volume = {52} },A discourse typically involves numerous entities but few are mentioned more than once. Distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models leading to increased performance. To realize these gains we build a classifier for predicting the singleton/coreferent distinction. The model’s feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation modality and attitude predication) with existing results about the benefits of “surface” (part-of-speech and n-gram-based) features for coreference resolution. The model is effective in its own right and the feature representations help to identify the anchor phrases in bridging anaphora as well. Furthermore incorporating the model into two very different state-of-the-art coreference resolution systems one rule-based and the other learning-based yields significant performance improvements.,http://research.google.com/pubs/archive/43407.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Modeling+the+Lifespan+of+Discourse+Entities++with+Application+to+Coreference+Resolution+Marneffe+Recasens+Potts,http://research.google.com/pubs/pub43407.html
Boosting the area under the ROC curve,NIPS (2007),2007,Philip M. Long Rocco A. Servedio,@inproceedings{33317 title = {Boosting the area under the ROC curve} author = {Philip M. Long and Rocco A. Servedio} year = 2007 URL = {http://www.phillong.info/publications/LS07_weak_rankers.pdf} booktitle = {NIPS} },We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efficiently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassification noise given access to a noise-tolerant weak ranker.,http://research.google.com/pubs/archive/33317.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Boosting+the+area+under+the+ROC+curve+Long+Servedio,http://research.google.com/pubs/pub33317.html
The One-Way Communication Complexity of Hamming Distance,Theory of Computing vol. 4 (2008) pp. 129-135,2008,T. S. Jayram Ravi Kumar D. Sivakumar,@article{35370 title = {The One-Way Communication Complexity of Hamming Distance} author = {T. S. Jayram and Ravi Kumar and D. Sivakumar} year = 2008 URL = {http://www.theoryofcomputing.org/articles/v004a006/} journal = {Theory of Computing} pages = {129--135} volume = {4} },Consider the following version of the Hamming distance problem for {1-1}-vectors of length n: the promise is that the distance is either at least (n/2)+sqrt{n} or at most (n/2)-sqrt{n} and the goal is to find out which of these two cases occurs. Woodruff (Proc. ACM-SIAM Symposium on Discrete Algorithms 2004) gave a linear lower bound for the randomized one-way communication complexity of this problem. In this note we give a simple proof of this result. Our proof uses a simple reduction from the indexing problem and avoids the VC-dimension arguments used in the previous paper. As shown by Woodruff (loc. cit.) this implies an Omega(1/epsilon^2)-space lower bound for approximating frequency moments within a factor 1+epsilon in the data stream model.,http://www.theoryofcomputing.org/articles/v004a006/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+One-Way+Communication+Complexity+of+Hamming+Distance+Jayram+Kumar+Sivakumar,http://research.google.com/pubs/pub35370.html
Crowd-Sourced Call Identification and Suppression,Federal Trade Commission Robocall Challenge (2013),2013,Daniel V. Klein Dean K. Jackson,@misc{41119 title = {Crowd-Sourced Call Identification and Suppression} author = {Daniel V. Klein and Dean K. Jackson} year = 2013 URL = {http://ftc.gov/os/comments/robocallchallenge/index.shtm} note = {Google submission to the FTC Robocall Challenge} },We recommend the creation of a system that allows users to report to an online database system the originating telephone number of unwanted solicitations advertisements or robotically placed calls (henceforth called 'spammers'). We also recommend that users' telephones or external hardware may automatically query the database about the telephone number of an incoming call (before the call is answered or even before the telephone rings) to determine if the caller has been flagged as a spammer by other users and optionally block the call or otherwise handle it differently from a non-spam call. The recommended system thereby would provide a means whereby users can make reports of spam calls as well as ask if others have reported a caller as a spammer. While the first few people called would get spammed after a sufficient number of reports are made further calls would be blocked. The recommended system would work on most types of telephonic platforms - smartphones some feature phones POTS lines VoIP PBX and telephony providers - through the use of software and optional inline hardware. In addition to crowd-sourced blacklisting we also recommend a means to whitelist specific numbers so that for example emergency calls will always go through.,http://research.google.com/pubs/archive/41119.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Crowd-Sourced+Call+Identification+and+Suppression+Klein+Jackson,http://research.google.com/pubs/pub41119.html
Multimedia Semantics: Interactions Between Content and Community,Proceedings of the IEEE vol. 100 no. 9 (2012),2012,Hari Sundaram Lexing Xie Munmun De Choudhury Yu-Ru Lin Apostol Natsev,@article{38105 title = {Multimedia Semantics: Interactions Between Content and Community} author = {Hari Sundaram and Lexing Xie and Munmun De Choudhury and Yu-Ru Lin and Apostol Natsev} year = 2012 journal = {Proceedings of the IEEE} volume = {100 no. 9} },This paper reviews the state of the art and some emerging issues in research areas related to pattern analysis and monitoring of web-based social communities. This research area is important for several reasons. First the presence of near-ubiquitous low-cost computing and communication technologies has enabled people to access and share information at an unprecedented scale. The scale of the data necessitates new research for making sense of such content. Furthermore popular websites with sophisticated media sharing and notification features allow users to stay in touch with friends and loved ones; these sites also help to form explicit and implicit social groups. These social groups are an important source of information to organize and to manage multimedia data. In this article we study how media-rich social networks provide additional insight into familiar multimedia research problems including tagging and video ranking. In particular we advance the idea that the contextual and social aspects of media are as important for successful multimedia applications as is the media content. We examine the interrelationship between content and social context through the prism of three key questions. First how do we extract the context in which social interactions occur? Second does social interaction provide value to the media object? Finally how do social media facilitate the repurposing of shared content and engender cultural memes? We present three case studies to examine these questions in detail. In the first case study we show how to discover structure latent in the social media data and use the discovered structure to organize Flickr photo streams. In the second case study we discuss how to determine the interestingness of conversations---and of participants---around videos uploaded to YouTube. Finally we show how the analysis of visual content in particular tracing of content remixes can help us understand the relationship among YouTube participants. For each case we present an overview of recent work and review the state of the art. We also discuss two emerging issues related to the analysis of social networks---robust data sampling and scalable data analysis.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multimedia+Semantics:+Interactions+Between+Content+and+Community+Sundaram+Xie+Choudhury+Lin+Natsev,http://research.google.com/pubs/pub38105.html
Norming to Performing: Failure Analysis and Deployment Automation of Big Data Software Developed by Highly Iterative Models,IEEE International Symposium on Software Reliability Engineering IEEE International Symposium on Software Reliability Engineering (2014) pp. 144-155,2014,Keun Soo Yim,@inproceedings{43149 title = {Norming to Performing: Failure Analysis and Deployment Automation of Big Data Software Developed by Highly Iterative Models} author = {Keun Soo Yim} year = 2014 booktitle = {IEEE International Symposium on Software Reliability Engineering} pages = {144-155} },We observe many interesting failure characteristics from Big Data software developed and released using some kinds of highly iterative development models (e.g. agile). ~16% of failures occur due to faults in software deployments (e.g. packaging and pushing to production). Our analysis shows that many such production outages are at least partially due to some human errors rooted in the high frequency and complexity of software deployments. ~51% of the observed human errors (e.g. transcription education and communication error types) are avoidable through automation. We thus develop a fault-tolerant automation framework to make it efficient to automate end-to-end software deployment procedures. We apply the framework to two Big Data products. Our case studies show the complexity of the deployment procedures of multi-homed Big Data applications and help us to study the effectiveness of the validation and verification techniques for user-provided automation programs. We analyze the production failures of the two products again after the automation. Our experimental data shows how the automation and the associated procedure improvements reduce the deployment faults and overall failure rate and improve the feature launch velocity. Automation facilitates more formal procedure-driven software engineering practices which not only reduce the manual work and human-oriented avoidable production outages but also help engineers to better understand overall software engineering procedures making them more auditable predictable reliable and efficient. We discuss two novel metrics to evaluate progress in mitigating human errors and the conditions indicating points to start such transition from owner-driven deployment practice.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Norming+to+Performing:+Failure+Analysis+and+Deployment+Automation+of+Big+Data+Software+Developed+by+Highly+Iterative+Models+Yim,http://research.google.com/pubs/pub43149.html
Good Abandonment in Mobile and PC Internet Search,32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ACM (Association for Computing Machinery) 2 Penn Plaza Suite 701 New York 10121-0701 (2009) pp. 43-50,2009,Jane Li Scott Huffman Akihito Tokuda,@inproceedings{35486 title = {Good Abandonment in Mobile and PC Internet Search} author = {Jane Li and Scott Huffman and Akihito Tokuda} year = 2009 URL = {http://portal.acm.org/citation.cfm?id=1571941.1571951} booktitle = {32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval} pages = {43--50} address = {2 Penn Plaza Suite 701 New York 10121-0701} },Query abandonment by search engine users is generally considered to be a negative signal. In this paper we explore the concept of good abandonment. We define a good abandonment as an abandoned query for which the user's information need was successfully addressed by the search results page with no need to click on a result or refine the query. We present an analysis of abandoned internet search queries across two modalities (PC and mobile) in three locales. The goal is to approximate the prevalence of good abandonment and to identify types of information needs that may lead to good abandonment across different locales and modalities. Our study has three key findings: First queries potentially indicating good abandonment make up a significant portion of all abandoned queries. Second the good abandonment rate from mobile search is significantly higher than that from PC search across all locales tested. Third classified by type of information need the major classes of good abandonment vary dramatically by both locale and modality. Our findings imply that it is a mistake to uniformly consider query abandonment as a negative signal. Further there is a potential opportunity for search engines to drive additional good abandonment especially for mobile search users by improving search features and result snippets.,http://research.google.com/pubs/archive/35486.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Good+Abandonment+in+Mobile+and+PC+Internet+Search+Li+Huffman+Tokuda,http://research.google.com/pubs/pub35486.html
Analysis of a Mixed-Use Urban WiFi Network: When Metropolitan becomes Neapolitan,IMC '08 Proceedings of the 8th ACM SIGCOMM conference on Internet measurement ACM New York NY USA (2008) pp. 85-98,2008,Mikhail Afanasyev Tsuwei Chen Geoffrey M. Voelker Alex C. Snoeren,@inproceedings{34430 title = {Analysis of a Mixed-Use Urban WiFi Network: When Metropolitan becomes Neapolitan} author = {Mikhail Afanasyev and Tsuwei Chen and Geoffrey M. Voelker and Alex C. Snoeren} year = 2008 booktitle = {IMC '08 Proceedings of the 8th ACM SIGCOMM conference on Internet measurement} pages = {85-98} address = {New York NY USA} },In this paper we study the usage of the Google WiFi network deployed in Mountain View California. We find that usage naturally falls into three categories based almost entirely on client device type. Moreover each of these classes of use has significant geographical and transportation areas of the city. Finally we find a diverse set of mobility patterns that map well to the archetypal use cases for traditional access technologies.,http://research.google.com/pubs/archive/34430.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Analysis+of+a+Mixed-Use+Urban+WiFi+Network:+When+Metropolitan+becomes+Neapolitan+Afanasyev+Chen+Voelker+Snoeren,http://research.google.com/pubs/pub34430.html
A New Entity Salience Task with Millions of Training Examples,Proceedings of the European Association for Computational Linguistics Association for Computational Linguistics (2014) (to appear),2014,Dan Gillick Jesse Dunietz,@inproceedings{42235 title = {A New Entity Salience Task with Millions of Training Examples} author = {Dan Gillick and Jesse Dunietz} year = 2014 booktitle = {Proceedings of the European Association for Computational Linguistics} },Although many NLP systems are moving toward entity-based processing most still identify important phrases using classical keyword-based approaches. To bridge this gap we introduce the task of entity salience: assigning a relevance score to each entity in a document. We demonstrate how a labeled corpus for the task can be automatically generated from a corpus of documents and accompanying abstracts. We then show how a classifier with features derived from a standard NLP pipeline outperforms a strong baseline by 34%. Finally we outline initial experiments on further improving accuracy by leveraging background knowledge about the relationships between entities.,http://research.google.com/pubs/archive/42235.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+New+Entity+Salience+Task+with+Millions+of+Training+Examples+Gillick+Dunietz,http://research.google.com/pubs/pub42235.html
Latent Structured Ranking,UAI (2012),2012,Jason Weston John Blitzer,@inproceedings{40574 title = {Latent Structured Ranking} author = {Jason Weston and John Blitzer} year = 2012 URL = {http://www.thespermwhale.com/jaseweston/papers/lsr.pdf} booktitle = {UAI} },Many latent (factorized) models have been proposed for recommendation tasks like collaborative ﬁltering and for ranking tasks like document or image retrieval and annotation. Common to all those methods is that during inference the items are scored independently by their similarity to the query in the latent embedding space. The structure of the ranked list (i.e. considering the set of items returned as a whole) is not taken into account. This can be a problem because the set of top predictions can be either too diverse (contain results that contradict each other) or are not diverse enough. In this paper we introduce a method for learning latent structured rankings that improves over existing methods by providing the right blend of predictions at the top of the ranked list. Particular emphasis is put on making this method scalable. Empirical results on large scale image annotation and music recommendation tasks show improvements over existing approaches.,http://www.thespermwhale.com/jaseweston/papers/lsr.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Latent+Structured+Ranking+Weston+Blitzer,http://research.google.com/pubs/pub40574.html
DaMN – Discriminative and Mutually Nearest: Exploiting Pairwise Category Proximity for Video Action Recognition,Proceedings of European Conference on Computer Vision (2014),2014,Rui Hou Amir Roshan Zamir Rahul Sukthankar Mubarak Shah,@inproceedings{42960 title = {DaMN – Discriminative and Mutually Nearest: Exploiting Pairwise Category Proximity for Video Action Recognition} author = {Rui Hou and Amir Roshan Zamir and Rahul Sukthankar and Mubarak Shah} year = 2014 booktitle = {Proceedings of European Conference on Computer Vision} },"We propose a method for learning discriminative category-level features and demonstrate state-of-the-art results on large-scale action recognition in video. The key observation is that one-vs-rest classifiers which are ubiquitously employed for this task face challenges in separating very similar categories (such as running vs. jogging). Our proposed method automatically identifies such pairs of categories using a criterion of mutual pairwise proximity in the (kernelized) feature space using a category-level similarity matrix where each entry corresponds to the one-vs-one SVM margin for pairs of categories. We then exploit the observation that while splitting such ""Siamese Twin"" categories may be difficult separating them from the remaining categories in a two-vs-rest framework is not. This enables us to augment one-vs-rest classifiers with a judicious selection of ""two-vs-rest"" classifier outputs formed from such discriminative and mutually nearest (DaMN) pairs. By combining one-vs-rest and two-vs-rest features in a principled probabilistic manner we achieve state-of-the-art results on the UCF101 and HMDB51 datasets. More importantly the same DaMN features when treated as a mid-level representation also outperform existing methods in knowledge transfer experiments both cross-dataset from UCF101 to HMDB51 and to new categories with limited training data (one-shot and few-shot learning). Finally we study the generality of the proposed approach by applying DaMN to other classification tasks; our experiments show that DaMN outperforms related approaches in direct comparisons not only on video action recognition but also on their original image dataset tasks.",http://research.google.com/pubs/archive/42960.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=DaMN+%E2%80%93+Discriminative+and+Mutually+Nearest:+Exploiting+Pairwise+Category+Proximity+for+Video+Action+Recognition+Hou+Zamir+Sukthankar+Shah,http://research.google.com/pubs/pub42960.html
Mining Arabic Business Reviews,IEEE pp. 1108-1113,2010,Mohamed Elhawary Mohamed Elfeky,@proceedings{36759 title = {Mining Arabic Business Reviews} editor = {Mohamed Elhawary and Mohamed Elfeky} year = 2010 booktitle = {2010 IEEE International Conference on Data Mining Workshops} pages = {1108-1113} },For languages with rich content over the web business reviews are easily accessible via many known websites e.g. Yelp.com. For languages with poor content over the web like Arabic there are very few websites (we are actually aware of only one that is indeed unpopular) that provide business reviews. However this does not mean that such reviews do not exist. They indeed exist unstructured in websites not originally intended for reviews e.g. Forums and Blogs. Hence there is a need to mine for those Arabic reviews from the web in order to provide them in the search results when a user searches for a business or a category of businesses. In this paper we show how to extract the business reviews scattered on the web written in the Arabic language. The mined reviews are analyzed to also provide their sentiments (positive negative or neutral). This way we provide our users the information they need about the local businesses in the language they understand and therefore provide a better search experience for the Middle East region which mostly speaks Arabic.,http://research.google.com/pubs/archive/36759.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Mining+Arabic+Business+Reviews+Elhawary+Elfeky,http://research.google.com/pubs/pub36759.html
An Overview of the Tesseract OCR Engine,Proc. Ninth Int. Conference on Document Analysis and Recognition (ICDAR) IEEE Computer Society (2007) pp. 629-633,2007,Ray Smith,@inproceedings{33418 title = {An Overview of the Tesseract OCR Engine} author = {Ray Smith} year = 2007 booktitle = {Proc. Ninth Int. Conference on Document Analysis and Recognition (ICDAR)} pages = {629--633} },The Tesseract OCR engine as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy[1] is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine including in particular the line finding features/classification methods and the adaptive classifier.,http://research.google.com/pubs/archive/33418.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Overview+of+the+Tesseract+OCR+Engine+Smith,http://research.google.com/pubs/pub33418.html
RFC6472 - Recommendation for Not Using AS_SET and AS_CONFED_SET in BGP,IETF (2011),2011,Warren Kumari Kotikalapudi Sriram,@misc{37645 title = {RFC6472 - Recommendation for Not Using AS_SET and AS_CONFED_SET in BGP} author = {Warren Kumari and Kotikalapudi Sriram} year = 2011 URL = {http://www.rfc-editor.org/rfc/rfc6472.txt} },This document recommends against the use of the AS_SET and AS_CONFED_SET types of the AS_PATH in BGPv4. This is done to simplify the design and implementation of BGP and to make the semantics of the originator of a route more clear. This will also simplify the design implementation and deployment of ongoing work in the Secure Inter-Domain Routing Working Group.,http://research.google.com/pubs/archive/37645.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC6472+-+Recommendation+for+Not+Using+AS_SET+and+AS_CONFED_SET+in+BGP+Kumari+Sriram,http://research.google.com/pubs/pub37645.html
Recent Books and Journals Articles in Public Opinion Survey Methods and Survey Statistics. 2015 Update,Survey Practice vol. 8 (2015),2015,Mario Callegaro,@article{44262 title = {Recent Books and Journals Articles in Public Opinion Survey Methods and Survey Statistics. 2015 Update} author = {Mario Callegaro} year = 2015 URL = {http://www.surveypractice.org/index.php/SurveyPractice/article/view/313/html_35} journal = {Survey Practice} volume = {8} },Welcome to the 7th edition of this column on recent books and journal articles in the field of public opinion survey methods and survey statistics. This year I had the chance to visit the London book fair so I was able actually to see some of the new books in our field. This article is an update of the April 2014 article. Like the previous year the books are organized by topic; this should help the readers to focus on their interests. It is unlikely to list all new books in the field; I did my best scouting different resources and websites but I take full responsibility for any omission. The list is also focusing only on books published in English language and available for purchase (as an Ebook or in print) at the time of this review (June 2015). Books are listed based on the relevance to the topic and no judgment is made in terms of quality of the content. We let the readers do so. Given our field is becoming more and more interdisciplinary this year I added a new section called “big data social media and other relevant books” to capture areas that are overlapping more and more with public opinion survey research and survey statistics.,http://www.surveypractice.org/index.php/SurveyPractice/article/view/313/html_35,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recent+Books+and+Journals+Articles+in+Public+Opinion+Survey+Methods+and+Survey+Statistics.+2015+Update+Callegaro,http://research.google.com/pubs/pub44262.html
Preference-Based Learning to Rank,Machine Learning Journal vol. 8 (2010) pp. 189-211,2010,Nir Ailon Mehryar Mohri,@article{36945 title = {Preference-Based Learning to Rank} author = {Nir Ailon and Mehryar Mohri} year = 2010 URL = {http://www.cs.nyu.edu/%7Emohri/pub/pref.pdf} journal = {Machine Learning Journal} pages = {189-211} volume = {8} },This paper presents an e_cient preference-based ranking algorithm running in two stages. In the ﬁrst stage the algorithm learns a preference function deﬁned over pairs as in a standard binary classification problem. In the second stage it makes use of that preference function to produce an accurate ranking thereby reducing the learning problem of ranking to binary classiﬁcation. This reduction is based on the familiar QuickSort and guarantees an expected pairwise misranking loss of at most twice that of the binary classiﬁer derived in the ﬁrst stage. Furthermore in the important special case of bipartite ranking the factor of two in loss is reduced to one. This improved bound also applies to the regret achieved by our ranking and that of the binary classifier obtained. Our algorithm is randomized but we prove a lower bound for any deterministic reduction of ranking to binary classiﬁcation showing that randomization is necessary to achieve our guarantees. This and a recent result by Balcan et al. who show a regret bound of two for a deterministic algorithm in the bipartite case suggest a trade-off between achieving low regret and determinism in this context. Our reduction also admits an improved running time guarantee with respect to that deterministic algorithm. In particular the number of calls to the preference function in the reduction is improved from Ω(n^2) to O(n log n). In addition when the top k ranked elements only are required (k_n) as in many applications in information extraction or search engine design the time complexity of our algorithm can be further reduced to O(k log k+n). Our algorithm is thus practical for realistic applications where the number of points to rank exceeds several thousand.,http://www.cs.nyu.edu/%7Emohri/pub/pref.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Preference-Based+Learning+to+Rank+Ailon+Mohri,http://research.google.com/pubs/pub36945.html
Robust Trait Composition for JavaScript,Science of Computer Programming: Special Issue on Advances in Dynamic Languages (2012),2012,Tom Van Cutsem Mark S. Miller,@article{37733 title = {Robust Trait Composition for JavaScript} author = {Tom Van Cutsem and Mark S. Miller} year = 2012 URL = {http://soft.vub.ac.be/Publications/2012/vub-soft-tr-12-19.pdf} journal = {Science of Computer Programming: Special Issue on Advances in Dynamic Languages} },"We introduce traits.js a small portable trait composition library for Javascript. Traits are a more robust alternative to multiple inheritance and enable object composition and reuse. traits.js is motivated by two goals: first it is an experiment in using and extending Javascript's recently added meta-level object description format. By reusing this standard description format traits.js can be made more interoperable with similar libraries and even with built-in primitives. Second traits.js makes it convenient to create ""high-integrity"" objects whose integrity cannot be violated by clients an important property in the context of mash-ups composed from mutually suspicious scripts. We describe the design of traits.js and provide an operational semantics for TRAITS-JS a minimal calculus that models the core functionality of the library.",http://research.google.com/pubs/archive/37733.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Robust+Trait+Composition+for+JavaScript+Cutsem+Miller,http://research.google.com/pubs/pub37733.html
Predictive Models for Music,Connection Science vol. 21 (2009) pp. 253-272,2009,Jean-Francois Paiement Yves Grandvalet Samy Bengio,@article{34342 title = {Predictive Models for Music} author = {Jean-Francois Paiement and Yves Grandvalet and Samy Bengio} year = 2009 journal = {Connection Science} pages = {253--272} volume = {21} },Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper we introduce predictive models for melodies. We decompose melodic modeling into two subtasks. We first propose a rhythm model based on the distributions of distances between subsequences. Then we define a generative model for melodies given chords and rhythms based on modeling sequences of Narmour features. The rhythm model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases. Using a similar evaluation procedure the proposed melodic model consistently outperforms an Input/Output Hidden Markov Model. Furthermore these models are able to generate realistic melodies given appropriate musical contexts.,http://research.google.com/pubs/archive/34342.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Predictive+Models+for+Music+Paiement+Grandvalet+Bengio,http://research.google.com/pubs/pub34342.html
Proceedings of the 2013 Conference on the Theory of Information Retrieval,ACM (2013),2013,Oren Kurland Donald Metzler Christina Lioma Birger Larsen Peter Ingwersen,@proceedings{41860 title = {Proceedings of the 2013 Conference on the Theory of Information Retrieval} editor = {Oren Kurland and Donald Metzler and Christina Lioma and Birger Larsen and Peter Ingwersen} year = 2013 },These proceedings contain the refereed papers posters and abstracts of keynotes tutorials and panel discussion presented at the Fourth International Conference on the Theory of Information Retrieval (ICTIR13) held in Copenhagen Denmark during September 29-October 2 2013.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Proceedings+of+the+2013+Conference+on+the+Theory+of+Information+Retrieval+Kurland+Metzler+Lioma+Larsen+Ingwersen,http://research.google.com/pubs/pub41860.html
Autoregressive Product of Multi-frame Predictions Can Improve the Accuracy of Hybrid Models,Proceedings of Interspeech 2014,2014,Navdeep Jaitly Vincent Vanhoucke Geoffrey Hinton,@inproceedings{42947 title = {Autoregressive Product of Multi-frame Predictions Can Improve the Accuracy of Hybrid Models} author = {Navdeep Jaitly and Vincent Vanhoucke and Geoffrey Hinton} year = 2014 booktitle = {Proceedings of Interspeech 2014} },We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural Network- Hidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax unit for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However if we average the predic- tions for each frame - from the different contexts it is associated with - we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional archi- tectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set (test- dev93) and 9.3% on test set (test-eval92).,http://research.google.com/pubs/archive/42947.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Autoregressive+Product+of+Multi-frame+Predictions+Can+Improve+the+Accuracy+of+Hybrid+Models+Jaitly+Vanhoucke+Hinton,http://research.google.com/pubs/pub42947.html
“WTH..!?!” Experiences reactions and expectations related to online privacy panic situations,Eleventh Symposium On Usable Privacy and Security (SOUPS 2015) USENIX Association,2015,Julio Angulo Martin Ortlieb,@inproceedings{44641 title = {“WTH..!?!” Experiences reactions and expectations related to online privacy panic situations} author = {Julio Angulo and Martin Ortlieb} year = 2015 URL = {https://www.usenix.org/system/files/conference/soups2015/soups15-paper-angulo.pdf} booktitle = {Eleventh Symposium On Usable Privacy and Security (SOUPS 2015)} },There are moments in which users might find themselves experiencing feelings of panic with the realization that their privacy or personal information on the Internet might be at risk. We present an exploratory study on common experiences of online privacy-related panic and on users’ reactions to frequently occurring privacy incidents. By using the metaphor of a privacy panic button we also gather users’ expectations on the type of help that they would like to obtain in such situations. Through user interviews (n = 16) and a survey (n = 549) we identify 18 scenarios of privacy panic situations. We ranked these scenarios according to their frequency of occurrence and to the concerns of users to become victims of these incidents. We explore users’ underlying worries of falling pray for these incidents and other contextual factors common to privacy panic experiences. Based on our findings we present implications for the design of a help system for users experiencing privacy panic situations.,http://research.google.com/pubs/archive/44641.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%E2%80%9CWTH..!%3F!%E2%80%9D+Experiences+reactions+and+expectations+related+to+online+privacy+panic+situations+Angulo+Ortlieb,http://research.google.com/pubs/pub44641.html
Video2Text: Learning to Annotate Video Content,ICDM Workshop on Internet Multimedia Mining (2009),2009,Hrishikesh Aradhye George Toderici Jay Yagnik,@inproceedings{35638 title = {Video2Text: Learning to Annotate Video Content} author = {Hrishikesh Aradhye and George Toderici and Jay Yagnik} year = 2009 booktitle = {ICDM Workshop on Internet Multimedia Mining} },This paper discusses a new method for automatic discovery and organization of descriptive concepts (labels) within large real-world corpora of user-uploaded multimedia such as YouTube.com. Conversely it also provides validation of existing labels if any. While training our method does not assume any explicit manual annotation other than the weak labels already available in the form of video title descrip- tion and tags. Prior work related to such auto-annotation assumed that a vocabulary of labels of interest (e.g. indoor outdoor city landscape) is speciﬁed a priori. In contrast the proposed method begins with an empty vocabulary. It analyzes audiovisual features of 25 million YouTube.com videos – nearly 150 years of video data – effectively searching for consistent correlation between these features and text metadata. It autonomously extends the label vocabulary as and when it discovers concepts it can reliably identify eventually leading to a vocabulary with thousands of labels and growing. We believe that this work signiﬁcantly extends the state of the art in multimedia data mining discovery and organization based on the technical merit of the proposed ideas as well as the enormous scale of the mining exercise in a very challenging unconstrained noisy domain.,http://research.google.com/pubs/archive/35638.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Video2Text:+Learning+to+Annotate+Video+Content+Aradhye+Toderici+Yagnik,http://research.google.com/pubs/pub35638.html
Runtime adaptation: a case for reactive code alignment,Proceedings of the 2nd International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era ACM New York NY USA (2012) pp. 1-11,2012,Michelle McDaniel Kim Hazelwood,@inproceedings{38226 title = {Runtime adaptation: a case for reactive code alignment} author = {Michelle McDaniel and Kim Hazelwood} year = 2012 URL = {http://doi.acm.org/10.1145/2185475.2185476} booktitle = {Proceedings of the 2nd International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era} pages = {1--11} address = {New York NY USA} },Static alignment techniques are well studied and have been incorporated into compilers in order to optimize code locality for the instruction fetch unit in modern processors. However current static alignment techniques have several limitations that cannot be overcome. In the exascale era it becomes even more important to break from static techniques and develop adaptive algorithms in order to maximize the utilization of every processor cycle. In this paper we explore those limitations and show that reactive realignment a method where we dynamically monitor running applications react to symptoms of poor alignment and adapt alignment to the current execution environment and program input is more scalable than static alignment. We present fetches-per-instruction as a runtime indicator of poor alignment. Additionally we discuss three main opportunities that static alignment techniques cannot leverage but which are increasingly important in large scale computing systems: microarchitectural differences of cores dynamic program inputs that exercise different and sometimes alternating code paths and dynamic branch behavior including indirect branch behavior and phase changes. Finally we will present several instances where our trigger for reactive realignment may be incorporated in practice and discuss the limitations of dynamic alignment.,http://doi.acm.org/10.1145/2185475.2185476,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Runtime+adaptation:+a+case+for+reactive+code+alignment+McDaniel+Hazelwood,http://research.google.com/pubs/pub38226.html
Semantic Multimodal Compression for Wearable sensing Systems,In Proceedings of the 9th Annual IEEE Conference on Sensors IEEE (2010) pp. 1149-1453,2010,Saro Meguerdichian Hyduke Noshadi Foad Dabiri Miodrag Potkonjak,@inproceedings{41367 title = {Semantic Multimodal Compression for Wearable sensing Systems} author = {Saro Meguerdichian and Hyduke Noshadi and Foad Dabiri and Miodrag Potkonjak} year = 2010 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5690381} booktitle = {In Proceedings of the 9th Annual IEEE Conference on Sensors} pages = {1149-1453} },Wearable sensing systems (WSS's) are emerging as an important class of distributed embedded systems in application domains ranging from medical to military. Such systems can be expensive and power hungry due to their multi sensor implementations that require constant use yet by nature they demand low-cost and low-power implementations. Semantic multimodal compression (SMC) mitigates these metrics in terms of data size by leveraging the natural tendency of signals in many types of embedded sensing systems to be composed of phases. In our driving example of a medical shoe with an insole lined with pressure sensors we find that the natural airborne landing and take-off segments have sharply different and repetitive properties. SMC models and compresses each segment independently selecting the best compression scheme for each segment and thus reducing total transmission energy.,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5690381,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Multimodal+Compression+for+Wearable+sensing+Systems+Meguerdichian+Noshadi+Dabiri+Potkonjak,http://research.google.com/pubs/pub41367.html
SpecTrans: Versatile Material Classification for Interaction with Textureless Specular and Transparent Surfaces,SIGCHI Conference on Human Factors in Computing Systems ACM (2015) pp. 2191-2200,2015,Munehiko Sato Shigeo Yoshida Alex Olwal Boxin Shi Atsushi Hiyama Tomohiro Tanikawa Michitaka Hirose Ramesh Raskar,@inproceedings{43828 title = {SpecTrans: Versatile Material Classification for Interaction with Textureless Specular and Transparent Surfaces} author = {Munehiko Sato and Shigeo Yoshida and Alex Olwal and Boxin Shi and Atsushi Hiyama and Tomohiro Tanikawa and Michitaka Hirose and Ramesh Raskar} year = 2015 URL = {http://olwal.com/projects/research/spectrans/sato_spectrans_chi_2015.pdf} booktitle = {SIGCHI Conference on Human Factors in Computing Systems} pages = {2191-2200} },Surface and object recognition is of significant importance in ubiquitous and wearable computing. While various techniques exist to infer context from material properties and appearance they are typically neither designed for real-time applications nor for optically complex surfaces that may be specular textureless and even transparent. These materials are however becoming increasingly relevant in HCI for transparent displays interactive surfaces and ubiquitous computing. We present SpecTrans a new sensing technology for surface classification of exotic materials such as glass transparent plastic and metal. The proposed technique extracts optical features by employing laser and multi-directional multispectral LED illumination that leverages the material’s optical properties. The sensor hardware is small in size and the proposed classification method requires significantly lower computational cost than conventional image-based methods which use texture features or reflectance analysis thereby providing real-time performance for ubiquitous computing. Our evaluation of the sensing technique for nine different transparent materials including air shows a promising recognition rate of 99.0%. We demonstrate a variety of possible applications using SpecTrans’ capabilities.,http://research.google.com/pubs/archive/43828.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SpecTrans:+Versatile+Material+Classification+for+Interaction+with+Textureless+Specular+and+Transparent+Surfaces+Sato+Yoshida+Olwal+Shi+Hiyama+Tanikawa+Hirose+Raskar,http://research.google.com/pubs/pub43828.html
A Tale of Two (Similar) Cities: Inferring City Similarity Through Geo-Spatial Query Log Analysis,Proceedings of the International Conference on Knowledge Discovery and Information Retrieval (2011),2011,Rohan Seth Michele Covell Deepak Ravichandran D. Sivakumar Shumeet Baluja,@inproceedings{37632 title = {A Tale of Two (Similar) Cities: Inferring City Similarity Through Geo-Spatial Query Log Analysis} author = {Rohan Seth and Michele Covell and Deepak Ravichandran and D. Sivakumar and Shumeet Baluja} year = 2011 booktitle = {Proceedings of the International Conference on Knowledge Discovery and Information Retrieval} },Understanding the backgrounds and interest of the people who are consuming a piece of content such as a news story video or music is vital for the content producer as well the advertisers who rely on the content to provide a channel on which to advertise. We extend traditional search-engine query log analysis which has primarily concentrated on analyzing either single or small groups of queries or users to examining the complete query stream of very large groups of users – the inhabitants of 13377 cities across the United States. Query logs can be a good representation of the interests of the city’s inhabitants and a useful characterization of the city itself. Further we demonstrate how query logs can be effectively used to gather city-level statistics sufficient for providing insights into the similarities and differences between cities. Cities that are found to be similar through the use of query analysis correspond well to the similar cities as determined through other large-scale and time-consuming direct measurement studies such as those undertaken by the Census Bureau.,http://research.google.com/pubs/archive/37632.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Tale+of+Two+(Similar)+Cities:+Inferring+City+Similarity+Through+Geo-Spatial+Query+Log+Analysis+Seth+Covell+Ravichandran+Sivakumar+Baluja,http://research.google.com/pubs/pub37632.html
Fast Orthogonal Projection Based on Kronecker Product,International Conference on Computer Vision (ICCV) (2015),2015,Xu Zhang Felix X. Yu Ruiqi Guo Sanjiv Kumar Shengjin Wang Shih-Fu Chang,@inproceedings{43992 title = {Fast Orthogonal Projection Based on Kronecker Product} author = {Xu Zhang and Felix X. Yu and Ruiqi Guo and Sanjiv Kumar and Shengjin Wang and Shih-Fu Chang} year = 2015 booktitle = {International Conference on Computer Vision (ICCV)} },We propose a family of structured matrices to speed up orthogonal projections for high-dimensional data commonly seen in computer vision applications. In this a structured matrix is formed by the Kronecker product of a series of smaller orthogonal matrices. This achieves O(dlogd) computational complexity and O(logd) space complexity for d-dimensional data a drastic improvement over the standard unstructured projections whose computational and space complexities are both O(d^2). We also introduce an efficient learning procedure for optimizing such matrices in a data dependent fashion. We demonstrate the significant advantages of the proposed approach in solving the approximate nearest neighbor (ANN) image search problem with both binary embedding and quantization. Comprehensive experiments show that the proposed approach can achieve similar or better accuracy as the existing state-of-the-art but with significantly less time and memory.,http://research.google.com/pubs/archive/43992.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Orthogonal+Projection+Based+on+Kronecker+Product+Zhang+Yu+Guo+Kumar+Wang+Chang,http://research.google.com/pubs/pub43992.html
Securing the Tangled Web,Communications of the ACM vol. 57 no. 9 (2014) pp. 38-47,2014,Christoph Kern,@article{42934 title = {Securing the Tangled Web} author = {Christoph Kern} year = 2014 URL = {http://dx.doi.org/10.1145/2643134} journal = {Communications of the ACM} pages = {38--47} volume = {57 no. 9} },Preventing script injection vulnerabilities through software design. Script injection vulnerabilities are a bane of Web application development: deceptively simple in cause and remedy they are nevertheless surprisingly difficult to prevent in large-scale Web development. Cross-site scripting (XSS) arises when insufficient data validation sanitization or escaping within a Web application allow an attacker to cause browser-side execution of malicious JavaScript in the application's context. This injected code can then do whatever the attacker wants using the privileges of the victim. Exploitation of XSS bugs results in complete (though not necessarily persistent) compromise of the victim's session with the vulnerable application. This article provides an overview of how XSS vulnerabilities arise and why it is so difficult to avoid them in real-world Web application software development. Software design patterns developed at Google to address the problem are then described.,http://research.google.com/pubs/archive/42934.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Securing+the+Tangled+Web+Kern,http://research.google.com/pubs/pub42934.html
Ensuring Connectivity via Data Plane Mechanisms,10th USENIX Symposium on Networked Systems Design and Implementation (2013),2013,Junda Liu,@inproceedings{40812 title = {Ensuring Connectivity via Data Plane Mechanisms} author = {Junda Liu} year = 2013 booktitle = {10th USENIX Symposium on Networked Systems Design and Implementation} },We typically think of network architectures as having two basic components: a data plane responsible for forwarding packets at line-speed and a control plane that instantiates the forwarding state the data plane needs. With this separation of concerns ensuring connectivity is the responsibility of the control plane. However the control plane typically operates at timescales several orders of magnitude slower than the data plane which means that failure recovery will always be slow compared to dataplane forwarding rates. In this paper we propose moving the responsibility for connectivity to the data plane. Our design called Data-Driven Connectivity (DDC) ensures routing connectivity via data plane mechanisms. We believe this new separation of concerns basic connectivity on the data plane optimal paths on the control plane will allow networks to provide a much higher degree of availability while still providing flexible routing control.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Ensuring+Connectivity+via+Data+Plane+Mechanisms+Liu,http://research.google.com/pubs/pub40812.html
Bespoke infrastructures,IEEE Software vol. 31 (2014) pp. 12-14,2014,Diomidis Spinellis,@article{41868 title = {Bespoke infrastructures} author = {Diomidis Spinellis} year = 2014 journal = {IEEE Software} pages = {12--14} volume = {31} },Infrastructure developed within an organization for its own internal use can take many forms. The obvious reason for creating a bespoke solution is that it can be tailored to fit the organization's unique needs. This can offer many advantages: better performance increased flexibility and tactical or strategic advantages over the competition. However such solutions are associated with a steep learning curve for newcomers maintenance and support costs and the risk of hijacking by groups with vested interests. Given that investment in bespoke infrastructures is a sunk cost and that these polarize the types of employees that stay in the organization rational approaches for building an organization's infrastructure include customizing a general-purpose solution or adopting an open-source tool and improving it to address the organization's requirements.,http://research.google.com/pubs/archive/41868.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Bespoke+infrastructures+Spinellis,http://research.google.com/pubs/pub41868.html
JSWhiz - Static Analysis for JavaScript Memory Leaks,Proceedings of the 10th annual IEEE/ACM international symposium on Code generation and optimization IEEE (2013),2013,Jacques Pienaar Robert Hundt,@inproceedings{40738 title = {JSWhiz - Static Analysis for JavaScript Memory Leaks} author = {Jacques Pienaar and Robert Hundt} year = 2013 booktitle = {Proceedings of the 10th annual IEEE/ACM international symposium on Code generation and optimization} },JavaScript is the dominant language for implementing dynamic web pages in browsers. Even though it is standardized many browsers implement language and browser bindings in different and incompatible ways. As a result a plethora of web development frameworks were developed to hide cross-browser issues and to ease development of large web applications. An unwelcome side-effect of these frameworks is that they can introduce memory leaks despite the fact that JavaScript is garbage collected. Memory bloat is a major issue for web applications as it affects user perceived latency and may even prevent large web applications from running on devices with limited resources. In this paper we present JSWhiz an extension to the open-source Closure JavaScript compiler. Based on experiences analyzing memory leaks in Gmail JSWhiz detects five identified common problem patterns. JSWhiz found a total of 89 memory leaks across Google's Gmail Docs Spreadsheets Books and Closure itself. It contributed significantly in a recent effort to reduce Gmail memory footprint which resulted in bloat reduction of 75% at the 99th percentile and by roughly 50% at the median.,http://research.google.com/pubs/archive/40738.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=JSWhiz+-+Static+Analysis+for+JavaScript+Memory+Leaks+Pienaar+Hundt,http://research.google.com/pubs/pub40738.html
Discrete Graph Hashing,Neural Information Processing Systems (2014),2014,Wei Liu Cun Mu Sanjiv Kumar Shih-Fu Chang,@inproceedings{43145 title = {Discrete Graph Hashing} author = {Wei Liu and Cun Mu and Sanjiv Kumar and Shih-Fu Chang} year = 2014 URL = {http://sanjivk.com/NIPS14_dgh_embed.pdf} booktitle = {Neural Information Processing Systems} },Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular learning based hashing has received considerable attention due to its appealing storage and search efficiency. However the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods especially for longer codes.,http://research.google.com/pubs/archive/43145.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discrete+Graph+Hashing+Liu+Mu+Kumar+Chang,http://research.google.com/pubs/pub43145.html
Your Reputation Precedes You: History Reputation and the Chrome Malware Warning,Proceedings of the Symposium On Usable Privacy and Security: SOUPS '14 USENIX (2014),2014,Hazim Almuhimedi Adrienne Porter Felt Robert W. Reeder Sunny Consolvo,@inproceedings{42546 title = {Your Reputation Precedes You: History Reputation and the Chrome Malware Warning} author = {Hazim Almuhimedi and Adrienne Porter Felt and Robert W. Reeder and Sunny Consolvo} year = 2014 booktitle = {Proceedings of the Symposium On Usable Privacy and Security: SOUPS '14} },Several web browsers including Google Chrome and Mozilla Firefox use malware warnings to stop people from visiting infectious websites. However users can choose to click through (i.e. ignore) these malware warnings. In Google Chrome users click through a fifth of malware warnings on average. We investigate factors that may contribute to why people ignore such warnings. First we examine field data to see how browsing history affects click-through rates. We find that users consistently heed warnings about websites that they have not visited before. However users respond unpredictably to warnings about websites that they have previously visited. On some days users ignore more than half of warnings about websites they've visited in the past. Next we present results of an online survey-based experiment that we ran to gain more insight into the effects of reputation on warning adherence. Participants said that they trusted high-reputation websites more than the warnings; however their responses suggest that a notable minority of people could be swayed by providing more information. We provide recommendations for warning designers and pose open questions about the design of malware warnings.,http://research.google.com/pubs/archive/42546.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Your+Reputation+Precedes+You:+History+Reputation+and+the+Chrome+Malware+Warning+Almuhimedi+Felt+Reeder+Consolvo,http://research.google.com/pubs/pub42546.html
Label Transition and Selection Pruning and Automatic Decoding Parameter Optimization for Time-Synchronous Viterbi Decoding,13th International Conference on Document Analysis and Recognition (ICDAR) IEEE (2015) pp. 756-760,2015,Yasuhisa Fujii Dmitriy Genzel Ashok C. Popat Remco Teunen,@inproceedings{44823 title = {Label Transition and Selection Pruning and Automatic Decoding Parameter Optimization for Time-Synchronous Viterbi Decoding} author = {Yasuhisa Fujii and Dmitriy Genzel and Ashok C. Popat and Remco Teunen} year = 2015 booktitle = {13th International Conference on Document Analysis and Recognition (ICDAR)} pages = {756--760} },Hidden Markov Model (HMM)-based classifiers have been successfully used for sequential labeling problems such as speech recognition and optical character recognition for decades. They have been especially successful in the domains where the segmentation is not known or difficult to obtain since in principle all possible segmentation points can be taken into account. However the benefit comes with a non-negligible computational cost. In this paper we propose simple yet effective new pruning algorithms to speed up decoding with HMM-based classifiers of up to 95% relative over a baseline. As the number of tunable decoding parameters increases it becomes more difficult to optimize the parameters for each configuration. We also propose a novel technique to estimate the parameters based on a loss value without relying on a grid search.,http://research.google.com/pubs/archive/44823.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Label+Transition+and+Selection+Pruning+and+Automatic+Decoding+Parameter+Optimization+for+Time-Synchronous+Viterbi+Decoding+Fujii+Genzel+Popat+Teunen,http://research.google.com/pubs/pub44823.html
Definably extending partial orders in totally ordered structures,Mathematical Logic Quarterly vol. 60 (2014) pp. 205-210,2014,Janak Ramakrishnan Charles Steinhorn,@article{42951 title = {Definably extending partial orders in totally ordered structures} author = {Janak Ramakrishnan and Charles Steinhorn} year = 2014 URL = {http://onlinelibrary.wiley.com/doi/10.1002/malq.201300047/abstract} journal = {Mathematical Logic Quarterly} pages = {205-210} volume = {60} },We show for various classes of totally ordered structures \mathcal M=(M<...)_ including o-minimal and weakly o-minimal structures that every definable partial order on a subset of _M^n extends definably in \mathcal M_ to a total order. This extends the result proved in [5] for n=1_ and _ \mathcal M o-minimal.,http://onlinelibrary.wiley.com/doi/10.1002/malq.201300047/abstract,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Definably+extending+partial+orders+in+totally+ordered+structures+Ramakrishnan+Steinhorn,http://research.google.com/pubs/pub42951.html
Image Annotation in Presence of Noisy Labels,International Conference on Pattern Recognition and Machine Intelligence (2013) (to appear),2013,Chandrashekhar V. Shailesh Kumar C. V. Jawahar,@inproceedings{41435 title = {Image Annotation in Presence of Noisy Labels} author = {Chandrashekhar V. and Shailesh Kumar and C. V. Jawahar} year = 2013 booktitle = {International Conference on Pattern Recognition and Machine Intelligence} },Labels associated with social images are valuable source of information for tasks of image annotation understanding and retrieval. These labels are often found to be noisy mainly due to the collaborative tagging activities of users. Existing methods on annotation have been developed and verified on noise free labels of images. In this paper we propose a novel and generic framework that exploits the collective knowledge embedded in noisy label co-occurrence pairs to derive robust annotations. We compare our method with a well-known image annotation algorithm and show its superiority in terms of annotation accuracy on benchmark Corel5K and ESP datasets in presence of noisy labels.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Image+Annotation+in+Presence+of+Noisy+Labels+V.+Kumar+Jawahar,http://research.google.com/pubs/pub41435.html
Project Intelligence,Pacific Northwest Software Quality Conference 2007 Proceedings PNSQC PO Box 10733 Portland OR 97296-0733 pp. 267-274,2007,Rong Ou,@inproceedings{33481 title = {Project Intelligence} author = {Rong Ou} year = 2007 URL = {http://www.pnsqc.org/proceedings/pnsqc2007.pdf} booktitle = {Pacific Northwest Software Quality Conference 2007 Proceedings} pages = {267--274} address = {PO Box 10733 Portland OR 97296-0733} },Modern software development is mostly a cooperative team effort generating large amount of data in disparate tools built around the development lifecycle. Making sense of this data to gain a clear understanding of the project status and direction has become a time-consuming highoverhead and messy process. In this paper we show how we have applied Business Intelligence (BI) techniques to address some of these issues. We built a real-time data warehouse to host project-related data from different systems. The data is cleansed transformed and sometimes rolled up to facilitate easier analytics operations. We built a web-based data visualization and dashboard system to give project stakeholders an accurate real-time view of the project status. In practice we saw participating teams gained better understanding of their corresponding projects and improved their project quality over time.,http://research.google.com/pubs/archive/33481.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Project+Intelligence+Ou,http://research.google.com/pubs/pub33481.html
Optimal Traversal Planning in Road Networks with Navigational Constraints,ACM GIS ACM (2007),2007,Leyla Kazemi Cyrus Shahabi Mehdi Sharifzadeh Luc Vincent,@inproceedings{35206 title = {Optimal Traversal Planning in Road Networks with Navigational Constraints} author = {Leyla Kazemi and Cyrus Shahabi and Mehdi Sharifzadeh and Luc Vincent} year = 2007 booktitle = {ACM GIS} },A frequent query in geospatial planning and decision making domains (e.g. emergency response data acquisition street cleaning) is to ¯nd an optimal traversal plan (OTP) that traverses an entire area (e.g. a city) by navigating through all its streets. The optimality is de¯ned in terms of the time it takes to complete the traversal. This time depends on the number of times each street segment is traversed as well as the navigation time such as the time spent on changing direction at each intersection. While the problem roots in the classic problems of graph theory real-world geospatial constraints of road network introduce new application-speci¯c challenges. In this paper we propose two algorithms to ¯nd OTP of a directed road network. Our greedy algorithm employs a classic graph traversal algorithm. During the traversal it utilizes a set of heuristics at each intersection to minimize the total travel time. Our near-optimal algorithm however reduces an OTP problem to an Asymmetric Traveling Salesman Problem (ATSP) by extracting the dual graph of the original network in which each edge is represented by a graph node. Using an approximate solution for ATSP our algorithm finds a near optimal answer. Our experiments using real-world road networks verify that our near-optimal algorithm outperforms the greedy algorithm in terms of the overall cost of its generated traversal by a factor of two while its complexity is tolerable in real-world cases.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimal+Traversal+Planning+in+Road+Networks+with+Navigational+Constraints+Kazemi+Shahabi+Sharifzadeh+Vincent,http://research.google.com/pubs/pub35206.html
Tenzing A SQL Implementation On The MapReduce Framework,Proceedings of VLDB VLDB Endowment (2011) pp. 1318-1327,2011,Biswapesh Chattopadhyay Liang Lin Weiran Liu Sagar Mittal Prathyusha Aragonda Vera Lychagina Younghee Kwon Michael Wong,@inproceedings{37200 title = {Tenzing A SQL Implementation On The MapReduce Framework} author = {Biswapesh Chattopadhyay and Liang Lin and Weiran Liu and Sagar Mittal and Prathyusha Aragonda and Vera Lychagina and Younghee Kwon and Michael Wong} year = 2011 booktitle = {Proceedings of VLDB} pages = {1318--1327} },Tenzing is a query engine built on top of MapReduce for ad hoc analysis of Google data. Tenzing supports a mostly complete SQL implementation (with several extensions) combined with several key characteristics such as heterogeneity high performance scalability reliability metadata awareness low latency support for columnar storage and structured data and easy extensibility. Tenzing is currently used internally at Google by 1000+ employees and serves 10000+ queries per day over 1.5 petabytes of compressed data. In this paper we describe the architecture and implementation of Tenzing and present benchmarks of typical analytical queries.,http://research.google.com/pubs/archive/37200.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Tenzing++A+SQL+Implementation+On+The+MapReduce+Framework+Chattopadhyay+Lin+Liu+Mittal+Aragonda+Lychagina+Kwon+Wong,http://research.google.com/pubs/pub37200.html
What devices do data centers need,OFC 2014 Technical Digest OSA,2014,Cedric F. Lam Ryohei Urata Hong Liu,@inproceedings{42180 title = {What devices do data centers need} author = {Cedric F. Lam and Ryohei Urata and Hong Liu} year = 2014 booktitle = {OFC 2014 Technical Digest} },We discuss the trend in fiber optic technology developments to fulfill the scaling requirements of datacenter networks.,http://research.google.com/pubs/archive/42180.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=What+devices+do+data+centers+need+Lam+Urata+Liu,http://research.google.com/pubs/pub42180.html
Still All On One Server: Perforce at Scale,2011 Perforce User Conference,2011,Dan Bloch,@inproceedings{39983 title = {Still All On One Server: Perforce at Scale} author = {Dan Bloch} year = 2011 URL = {http://info.perforce.com/rs/perforce/images/GoogleWhitePaper-StillAllonOneServer-PerforceatScale.pdf} booktitle = {2011 Perforce User Conference} },Google runs the busiest single Perforce server on the planet and one of the largest repositories in any source control system. From that high-water mark this paper looks at server performance and other issues of scale with digressions into where we are how we got here and how we continue to stay one step ahead of our users.,http://research.google.com/pubs/archive/39983.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Still+All+On+One+Server:+Perforce+at+Scale+Bloch,http://research.google.com/pubs/pub39983.html
Identifying Phrasal Verbs Using Many Bilingual Corpora,Proceedings of Empirical Methods in Natural Language Processing (2013),2013,Karl Pichotta John DeNero,@inproceedings{41851 title = {Identifying Phrasal Verbs Using Many Bilingual Corpora} author = {Karl Pichotta and John DeNero} year = 2013 URL = {http://aclweb.org/anthology//D/D13/D13-1060.pdf} booktitle = {Proceedings of Empirical Methods in Natural Language Processing} },We address the problem of identifying multiword expressions in a language focusing on English phrasal verbs. Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs achieving performance comparable to a human-curated set.,http://research.google.com/pubs/archive/41851.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Identifying+Phrasal+Verbs+Using+Many+Bilingual+Corpora+Pichotta+DeNero,http://research.google.com/pubs/pub41851.html
Yedalog: Exploring Knowledge at Scale,1st Summit on Advances in Programming Languages (SNAPL 2015) Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik Dagstuhl Germany pp. 63-78,2015,Brian Chin Daniel von Dincklage Vuk Ercegovac Peter Hawkins Mark S. Miller Franz Och Chris Olston Fernando Pereira,@inproceedings{43462 title = {Yedalog: Exploring Knowledge at Scale} author = {Brian Chin and Daniel von Dincklage and Vuk Ercegovac and Peter Hawkins and Mark S. Miller and Franz Och and Chris Olston and Fernando Pereira} year = 2015 URL = {http://drops.dagstuhl.de/opus/frontdoor.php?source_opus=5017} booktitle = {1st Summit on Advances in Programming Languages (SNAPL 2015)} pages = {63--78} address = {Dagstuhl Germany} },With huge progress on data processing frameworks human programmers are frequently the bottleneck when analyzing large repositories of data. We introduce Yedalog a declarative programming language that allows programmers to mix data-parallel pipelines and computation seamlessly in a single language. By contrast most existing tools for data-parallel computation embed a sublanguage of data-parallel pipelines in a general-purpose language or vice versa. Yedalog extends Datalog incorporating not only computational features from logic programming but also features for working with data structured as nested records. Yedalog programs can run both on a single machine and distributed across a cluster in batch and interactive modes allowing programmers to mix different modes of execution easily.,http://research.google.com/pubs/archive/43462.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Yedalog:+Exploring+Knowledge+at+Scale+Chin+von+Dincklage+Ercegovac+Hawkins+Miller+Och+Olston+Pereira,http://research.google.com/pubs/pub43462.html
Large-Scale Speaker Identification,Proc. ICASSP IEEE (2014),2014,Ludwig Schmidt Matthew Sharifi Ignacio Lopez-Moreno,@inproceedings{42535 title = {Large-Scale Speaker Identification} author = {Ludwig Schmidt and Matthew Sharifi and Ignacio Lopez-Moreno} year = 2014 booktitle = {Proc. ICASSP} address = {IEEE} },Speaker identification is one of the main tasks in speech processing. In addition to identification accuracy large-scale applications of speaker identification give rise to another challenge: fast search in the database of speakers. In this paper we propose a system based on i-vectors a current approach for speaker identification and locality sensitive hashing an algorithm for fast nearest-neighbor search in high dimensions. The connection between the two techniques is the cosine distance: one the one hand we use the cosine distance to compare i-vectors on the other hand locality sensitive hashing allows us to quickly approximate the cosine distance in our retrieval procedure. We evaluate our approach on a realistic data set from YouTube with about 1000 speakers. The results show that our algorithm is approximately one to two orders of magnitude faster than a linear search while maintaining the identification accuracy of an i-vector-based system.,http://research.google.com/pubs/archive/42535.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Speaker+Identification+Schmidt+Sharifi+Lopez+Moreno,http://research.google.com/pubs/pub42535.html
HEADY: News headline abstraction through event pattern clustering,Proceedings of ACL-2013,2013,Enrique Alfonseca Daniele Pighin Guillermo Garrido,@inproceedings{41185 title = {HEADY: News headline abstraction through event pattern clustering} author = {Enrique Alfonseca and Daniele Pighin and Guillermo Garrido} year = 2013 booktitle = {Proceedings of ACL-2013} },This paper presents HEADY: a novel ab- stractive approach for headline generation from news collections. From a web-scale corpus of English news we mine syntactic patterns that a Noisy-OR model generalizes into event descriptions. At inference time we query the model with the patterns observed in an unseen news collection identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headline. HEADY improves over a state-of-the- art open-domain title abstraction method bridging half of the gap that separates it from extractive methods using human-generated titles in manual evaluations and performs comparably to human-generated headlines as evaluated with ROUGE.,http://research.google.com/pubs/archive/41185.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HEADY:+News+headline+abstraction+through+event+pattern+clustering+Alfonseca+Pighin+Garrido,http://research.google.com/pubs/pub41185.html
Making Privacy a Fundamental Component of Web Resources,W3C Workshop on Privacy for Advanced Web APIs W3C (2010) pp. 5,2010,Thomas Duebendorfer Christoph Renner Tyrone Grandison Michael Maximilien Mark Weitzel,@inproceedings{36497 title = {Making Privacy a Fundamental Component of Web Resources} author = {Thomas Duebendorfer and Christoph Renner and Tyrone Grandison and Michael Maximilien and Mark Weitzel} year = 2010 URL = {http://www.w3.org/2010/api-privacy-ws/papers/privacy-ws-26.pdf} booktitle = {W3C Workshop on Privacy for Advanced Web APIs} pages = {5} },We present a social network inspired and access control list based sharing model for web resources. We have specified it as an extension for OpenSocial 1.0 and implemented a proof of concept in Orkut as well as a mobile social photo sharing application using it. The paper explains important design decisions and how the model can be leveraged to make privacy a core component and enabler for sharing resources on the web and beyond using capabilities of mobile devices.,http://research.google.com/pubs/archive/36497.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Making+Privacy+a+Fundamental+Component+of+Web+Resources+Duebendorfer+Renner+Grandison+Maximilien+Weitzel,http://research.google.com/pubs/pub36497.html
RFC7646 -Definition and Use of DNSSEC Negative Trust Anchors,IETF RFCs Internet Engineering Task Force (2015) pp. 15,2015,Warren Kumari Jason Livingood Chris Griffiths,@incollection{44313 title = {RFC7646 -Definition and Use of DNSSEC Negative Trust Anchors} author = {Warren Kumari and Jason Livingood and Chris Griffiths} year = 2015 booktitle = {IETF RFCs} pages = {15} },DNS Security Extensions (DNSSEC) is now entering widespread deployment. However domain signing tools and processes are not yet as mature and reliable as those for non-DNSSEC-related domain administration tools and processes. This document defines Negative Trust Anchors (NTAs) which can be used to mitigate DNSSEC validation failures by disabling DNSSEC validation at specified domains.,http://research.google.com/pubs/archive/44313.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7646+-Definition+and+Use+of+DNSSEC+Negative+Trust+Anchors+Kumari+Livingood+Griffiths,http://research.google.com/pubs/pub44313.html
The Emerging Optical Data Center,OFC 2011 OTuH2,2011,Amin Vahdat Hong Liu Xiaoxue Zhao Chris Johnson,@inproceedings{37047 title = {The Emerging Optical Data Center} author = {Amin Vahdat and Hong Liu and Xiaoxue Zhao and Chris Johnson} year = 2011 booktitle = {OFC 2011} pages = {OTuH2} },We review the architecture of modern datacenter networks as well as their scaling challenges; then present high-level requirements for deploying optical technologies in datacenters particularly focusing on optical circuit switching and WDM transceivers.,http://research.google.com/pubs/archive/37047.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Emerging+Optical+Data+Center+Vahdat+Liu+Zhao+Johnson,http://research.google.com/pubs/pub37047.html
Joint Image and Word Sense Discrimination For Image Retrieval,ECCV (2012),2012,Aurelien Lucchi Jason Weston,@inproceedings{40575 title = {Joint Image and Word Sense Discrimination For Image Retrieval} author = {Aurelien Lucchi and Jason Weston} year = 2012 URL = {http://www.thespermwhale.com/jaseweston/papers/imax.pdf} booktitle = {ECCV} },We study the task of learning to rank images given a text query a problem that is complicated by the issue of multiple senses. That is the senses of interest are typically the visually distinct concepts that a user wishes to retrieve. In this paper we propose to learn a ranking function that optimizes the ranking cost of interest and simultaneously discovers the disambiguated senses of the query that are optimal for the supervised task. Note that no supervised information is given about the senses. Experiments performed on web images and the ImageNet dataset show that using our approach leads to a clear gain in performance.,http://www.thespermwhale.com/jaseweston/papers/imax.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Joint+Image+and+Word+Sense+Discrimination+For+Image+Retrieval+Lucchi+Weston,http://research.google.com/pubs/pub40575.html
Structured Transforms for Small-footprint Deep Learning,Neural Information Processing Systems (NIPS) (2015),2015,Vikas Sindhwani Tara N. Sainath Sanjiv Kumar,@inproceedings{44671 title = {Structured Transforms for Small-footprint Deep Learning} author = {Vikas Sindhwani and Tara N. Sainath and Sanjiv Kumar} year = 2015 URL = {http://arxiv.org/pdf/1510.01722v1.pdf} booktitle = {Neural Information Processing Systems (NIPS)} },We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a uni- fied framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models while providing more than 3.5-fold compression.,http://arxiv.org/pdf/1510.01722v1.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Structured+Transforms+for+Small-footprint+Deep+Learning+Sindhwani+Sainath+Kumar,http://research.google.com/pubs/pub44671.html
“...no one can hack my mind”: Comparing Expert and Non-Expert Security Practices,Proceedings of the Eleventh Symposium On Usable Privacy and Security USENIX (2015) pp. 327-346,2015,Iulia Ion Rob Reeder Sunny Consolvo,@inproceedings{43963 title = {“...no one can hack my mind”: Comparing Expert and Non-Expert Security Practices} author = {Iulia Ion and Rob Reeder and Sunny Consolvo} year = 2015 URL = {https://www.usenix.org/system/files/conference/soups2015/soups15-paper-ion.pdf} booktitle = {Proceedings of the Eleventh Symposium On Usable Privacy and Security} pages = {327--346} },The state of advice given to people today on how to stay safe online has plenty of room for improvement. Too many things are asked of them which may be unrealistic time consuming or not really worth the effort. To improve the security advice our community must find out what practices people use and what recommendations if messaged well are likely to bring the highest benefit while being realistic to ask of people. In this paper we present the results of a study which aims to identify which practices people do that they consider most important at protecting their security online. We compare self-reported security practices of non-experts to those of security experts (i.e. participants who reported having five or more years of experience working in computer security). We report on the results of two online surveys—one with 231 security experts and one with 294 MTurk participants—on what the practices and attitudes of each group are. Our findings show a discrepancy between the security practices that experts and non-experts report taking. For instance while experts most frequently report installing software updates using two-factor authentication and using a password manager to stay safe online non-experts report using antivirus software visiting only known websites and changing passwords frequently.,http://research.google.com/pubs/archive/43963.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%E2%80%9C...no+one+can+hack+my+mind%E2%80%9D:+Comparing+Expert+and+Non-Expert+Security+Practices+Ion+Reeder+Consolvo,http://research.google.com/pubs/pub43963.html
Maestro: Quality-of-Service in Large Disk Arrays,Proceedings of the 8th ACM international conference on Autonomic computing (ICAC) ACM New York NY USA (2011) pp. 245-254,2011,Arif Merchant Mustafa Uysal Pradeep Padala Xiaoyun Zhu Sharad Singhal Kang Shin,@inproceedings{37233 title = {Maestro: Quality-of-Service in Large Disk Arrays} author = {Arif Merchant and Mustafa Uysal and Pradeep Padala and Xiaoyun Zhu and Sharad Singhal and Kang Shin} year = 2011 URL = {http://kabru.eecs.umich.edu//papers/publications/2011/icac11.pdf} booktitle = {Proceedings of the 8th ACM international conference on Autonomic computing (ICAC)} pages = {245--254} address = {New York NY USA} },Provisioning storage in disk arrays is a difficult problem because many applications with different workload characteristics and priorities share resources provided by the array. Currently storage in disk arrays is statically partitioned leading to difficult choices between over-provisioning to meet peak demands and resource sharing to meet efficiency targets. In this paper we present Maestro a feedback controller that can manage resources on large disk arrays to provide performance differentiation among multiple applications. Maestro monitors the performance of each application and dynamically allocates the array resources so that diverse performance requirements can be met without static partitioning. It supports multiple performance metrics (e.g. latency and throughput) and application priorities so that important applications receive better performance in case of resource contention. By ensuring that high-priority applications sharing storage with other applications obtain the performance levels they require Maestro makes it possible to use storage resources efficiently. We evaluate Maestro using both synthetic and real-world workloads on a large commercial disk array. Our experiments indicate that Maestro can reliably adjust the allocation of disk array resources to achieve application performance targets.,http://kabru.eecs.umich.edu//papers/publications/2011/icac11.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Maestro:+Quality-of-Service+in+Large+Disk+Arrays+Merchant+Uysal+Padala+Zhu+Singhal+Shin,http://research.google.com/pubs/pub37233.html
Subset Feedback Vertex Set is fixed parameter tractable,ICALP 2011 (to appear),2011,Marek Cygan Marcin Pilipczuk Micha_ Pilipczuk Jakub Onufry Wojtaszczyk,@inproceedings{37375 title = {Subset Feedback Vertex Set is fixed parameter tractable} author = {Marek Cygan and Marcin Pilipczuk and Micha_ Pilipczuk and Jakub Onufry Wojtaszczyk} year = 2011 booktitle = {ICALP 2011} },The classical Feedback Vertex Set problem asks for a given undirected graph G and an integer k to find a set of at most k vertices that hits all the cycles in the graph G. Feedback Vertex Set has attracted a large amount of research in the parameterized setting and subsequent kernelization and fixed-parameter algorithms have been a rich source of ideas in the field. In this paper we consider a more general and difficult version of the problem named Subset Feedback Vertex Set (SFVS in short) where an instance comes additionally with a set S of vertices and we ask for a set of at most k vertices that hits all simple cycles passing through S. Because of its applications in circuit testing and genetic linkage analysis SFVS was studied from the approximation algorithms perspective by Even et al. [SICOMP'00 SIDMA'00]. The question whether the SFVS problem is fixed parameter tractable was posed independently by Kawarabayashi and Saurabh in 2009. We answer this question affirmatively. We begin by showing that this problem is fixed-parameter tractable when parametrized by |S|. Next we present an algorithm which reduces the size of S to O(k^3) in 2^{O(k\log k)}n^{O(1)} time using kernelization techniques such as the 2-Expansion Lemma Menger's theorem and Gallai's theorem. These two facts allow us to give a 2^{O(k\log k)} n^{O(1)} time algorithm solving the SFVS problem proving that it is indeed fixed parameter tractable.,http://research.google.com/pubs/archive/37375.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Subset+Feedback+Vertex+Set+is+fixed+parameter+tractable+Cygan+Pilipczuk+Pilipczuk+Wojtaszczyk,http://research.google.com/pubs/pub37375.html
How Google is using Linked Data Today and Vision For Tomorrow,Proceedings of Linked Data in the Future Internet at the Future Internet Assembly (FIA 2010) Ghent December 2010,2010,Thomas Steiner Raphael Troncy Michael Hausenblas,@inproceedings{37430 title = {How Google is using Linked Data Today and Vision For Tomorrow} author = {Thomas Steiner and Raphael Troncy and Michael Hausenblas} year = 2010 URL = {http://CEUR-WS.org/Vol-700/Paper5.pdf} booktitle = {Proceedings of Linked Data in the Future Internet at the Future Internet Assembly (FIA 2010) Ghent December 2010} },In this position paper we first discuss how modern search engines such as Google make use of Linked Data spread in Web pages for displaying Rich Snippets. We present an example of the technology and we analyze its current uptake. We then sketch some ideas on how Rich Snippets could be extended in the future in particular for multimedia documents. We outline bottlenecks in the current Internet architecture that require fixing in order to enable our vision to work at Web scale.,http://research.google.com/pubs/archive/37430.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Google+is+using+Linked+Data+Today+and+Vision+For+Tomorrow+Steiner+Troncy+Hausenblas,http://research.google.com/pubs/pub37430.html
Computer-aided quality assurance of an Icelandic pronunciation dictionary,LREC 2014 Reykjavik,2014,Martin Jansche,@inproceedings{42502 title = {Computer-aided quality assurance of an Icelandic pronunciation dictionary} author = {Martin Jansche} year = 2014 URL = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/339_Paper.pdf} booktitle = {LREC 2014} address = {Reykjavik} },We propose a model-driven method for ensuring the quality of pronunciation dictionaries. The key ingredient is computing an alignment between letter strings and phoneme strings a standard technique in pronunciation modeling. The novel aspect of our method is the use of informative parametric alignment models which are refined iteratively as they are tested against the data. We discuss the use of alignment failures as a signal for detecting and correcting problematic dictionary entries. We illustrate this method using an existing pronunciation dictionary for Icelandic. Our method is completely general and has been applied in the construction of pronunciation dictionaries for commercially deployed speech recognition systems in several languages.,http://research.google.com/pubs/archive/42502.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Computer-aided+quality+assurance+of+an+Icelandic+pronunciation+dictionary+Jansche,http://research.google.com/pubs/pub42502.html
Acoustic Modelling with CD-CTC-SMBR LSTM RNNS,ASRU (2015) (to appear),2015,Andrew Senior Hasim Sak Felix de Chaumont Quitry Tara N. Sainath Kanishka Rao,@inproceedings{44269 title = {Acoustic Modelling with CD-CTC-SMBR LSTM RNNS} author = {Andrew Senior and Hasim Sak and Felix de Chaumont Quitry and Tara N. Sainath and Kanishka Rao} year = 2015 booktitle = {ASRU} },This paper describes a series of experiments to extend the application of Context-Dependent (CD) long short-term memory (LSTM) recurrent neural networks (RNNs) trained with Connectionist Temporal Classification (CTC) and sMBR loss. Our experiments on a noisy reverberant voice search task include training with alternative pronunciations and the application to child speech recognition; combination of multiple models and convolutional input layers. We also investigate the latency of CTC models and show that constraining forward-backward alignment in training can reduce the delay for a real-time streaming speech recognition system. Finally we investigate transferring knowledge from one network to another through alignments,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Acoustic+Modelling+with+CD-CTC-SMBR+LSTM+RNNS+Senior+Sak+Quitry+Sainath+Rao,http://research.google.com/pubs/pub44269.html
On the k-atomicity-verification problem,The 33rd International Conference on Distributed Computing Systems IEEE (2013),2013,Wojciech Golab Jeremy Hurwitz Xiaozhou Li,@inproceedings{41097 title = {On the k-atomicity-verification problem} author = {Wojciech Golab and Jeremy Hurwitz and Xiaozhou Li} year = 2013 booktitle = {The 33rd International Conference on Distributed Computing Systems} },Modern Internet-scale storage systems often provide weak consistency in exchange for better perfor- mance and resilience. An important weak consistency prop- erty is k-atomicity which bounds the staleness of values returned by read operations. The k-atomicity-verification problem (or k-AV for short) is the problem of deciding whether a given history of operations is k-atomic. The 1-AV problem is equivalent to verifying atomicity/linearizability a well-known and solved problem. However for k ? 2 no polynomial-time k-AV algorithm is known. This paper makes the following contributions towards solving the k-AV problem. First we present a simple 2- AV algorithm called LBT which is likely to be efficient (quasilinear) for histories that arise in practice although it is less efficient (quadratic) in the worst case. Second we present a more involved 2-AV algorithm called FZF which runs efficiently (quasilinear) even in the worst case. To our knowledge these are the first algorithms that solve the 2-AV problem fully. Third we show that the weighted k-AV problem a natural extension of the k-AV problem is NP-complete.,http://research.google.com/pubs/archive/41097.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+k-atomicity-verification+problem+Golab+Hurwitz+Li,http://research.google.com/pubs/pub41097.html
Continuous Pipelines at Google,SRECon Europe 2015 USENIX Dublin Ireland pp. 12,2015,Dan Dennison,@inproceedings{43790 title = {Continuous Pipelines at Google} author = {Dan Dennison} year = 2015 URL = {https://www.usenix.org/system/files/conference/srecon15europe/continuouspipelinesatgooglefinal.pdf} booktitle = {SRECon Europe 2015} pages = {12} address = {Dublin Ireland} },This article focuses on the real life challenges of managing data processing pipelines of depth and complexity. It considers the frequency continuum between periodic pipelines that run very infrequently through continuous pipelines that never stop running and discusses the discontinuities that can produce significant operational problems. A fresh take on the master_slave model is presented as a more reliable and better scaling alternative to the periodic pipeline for processing Big Data.,http://research.google.com/pubs/archive/43790.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Continuous+Pipelines+at+Google+Dennison,http://research.google.com/pubs/pub43790.html
The Semantic Vectors Package: New Algorithms and Public Tools for Distributional Semantics,Fourth IEEE International Conference on Semantic Computing (IEEE ICSC2010) IEEE,2010,Dominic Widdows Trevor Cohen,@inproceedings{36654 title = {The Semantic Vectors Package: New Algorithms and Public Tools for Distributional Semantics} author = {Dominic Widdows and Trevor Cohen} year = 2010 booktitle = {Fourth IEEE International Conference on Semantic Computing (IEEE ICSC2010)} },Distributional semantics is the branch of natural language processing that attempts to model the meanings of words phrases and documents from the distribution and usage of words in a corpus of text. In the past three years research in this area has been accelerated by the availability of the Semantic Vectors package a stable fast scalable and free software package for creating and exploring concepts in distributional models. This paper introduces the broad field of distributional semantics the role of vector models within this field and describes some of the results that have been made possible by the Semantic Vectors package. These applications of Semantic Vectors have so far included contributions to medical informatics and knowledge discovery analysis of scientific articles and even Biblical scholarship. Of particular interest is the recent emergence of models that take word order and other ordered structures into account using permutation of coordinates to model directional relationships and semantic predicates.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Semantic+Vectors+Package:+New+Algorithms+and+Public+Tools+for+Distributional+Semantics+Widdows+Cohen,http://research.google.com/pubs/pub36654.html
Finding Meaning on YouTube: Tag Recommendation and Category Discovery,Computer Vision and Pattern Recognition IEEE (2010),2010,George Toderici Hrishikesh Aradhye Marius Pasca Luciano Sbaiz Jay Yagnik,@inproceedings{35651 title = {Finding Meaning on YouTube: Tag Recommendation and Category Discovery} author = {George Toderici and Hrishikesh Aradhye and Marius Pasca and Luciano Sbaiz and Jay Yagnik} year = 2010 booktitle = {Computer Vision and Pattern Recognition} },We present a system that automatically recommends tags for YouTube videos solely based on their audiovisual content. We also propose a novel framework for unsupervised discovery of video categories that exploits knowledge mined from the World-Wide Web text documents/searches. First video content to tag association is learned by training classifiers that map audiovisual content-based features from millions of videos on YouTube.com to existing uploader-supplied tags for these videos. When a new video is uploaded the labels provided by these classifiers are used to automatically suggest tags deemed relevant to the video. Our system has learned a vocabulary of over 20000 tags. Secondly we mined large volumes of Web pages and search queries to discover a set of possible text entity categories and a set of associated is-A relationships that map individual text entities to categories. Finally we apply these is-A relationships mined from web text on the tags learned from audiovisual content of videos to automatically synthesize a reliable set of categories most relevant to videos -- along with a mechanism to predict these categories for new uploads. We then present rigorous rating studies that establish that: (a) the average relevance of tags automatically recommended by our system matches the average relevance of the uploader-supplied tags at the same or better coverage and (b) the average precision@K of video categories discovered by our system is 70% with K=5.,http://research.google.com/pubs/archive/35651.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Finding+Meaning+on+YouTube:+Tag+Recommendation+and+Category+Discovery+Toderici+Aradhye+Pasca+Sbaiz+Yagnik,http://research.google.com/pubs/pub35651.html
Authentication at Scale,IEEE Security and Privacy vol. 11 (2013) pp. 15-22,2013,Eric Grosse Mayank Upadhyay,@article{40692 title = {Authentication at Scale} author = {Eric Grosse and Mayank Upadhyay} year = 2013 URL = {http://www.computer.org/cms/Computer.org/ComputingNow/pdfs/AuthenticationAtScale.pdf} journal = {IEEE Security and Privacy} pages = {15-22} volume = {11} },In working to keep cloud computing users' data safe we observe many threats---malware on the client attacks on ssl vulnerabilities in web applications rogue insiders espionage---but authentication related issues stand out amongst the biggest. When trying to help hundreds of millions of people from an unbelievable variety of endpoints attitudes and skill levels what can possibly displace plain old passwords? No single thing nothing overnight and nothing perfect. A combination of risk-based checks second-factor options privacy-enhanced client certificates and different forms of delegation is starting to find adoption towards making a discernible difference.,http://www.computer.org/cms/Computer.org/ComputingNow/pdfs/AuthenticationAtScale.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Authentication+at+Scale+Grosse+Upadhyay,http://research.google.com/pubs/pub40692.html
CQIC: Revisiting Cross-Layer Congestion Control f or Cellular Networks,Proceedings of The 16th International Workshop on Mobile Computing Systems and Applications (HotMobile) ACM (2015) pp. 45-50,2015,Feng Lu Hao Du Ankur Jain Geoffrey M. Voelker Alex C. Snoeren Andreas Terzis,@inproceedings{43311 title = {CQIC: Revisiting Cross-Layer Congestion Control f or Cellular Networks} author = {Feng Lu and Hao Du and Ankur Jain and Geoffrey M. Voelker and Alex C. Snoeren and Andreas Terzis} year = 2015 booktitle = {Proceedings of The 16th International Workshop on Mobile Computing Systems and Applications (HotMobile)} pages = {45-50} },With the advent of high-speed cellular access and the overwhelming popularity of smartphones a large percent of today’s Internet content is being delivered via cellular links. Due to the nature of long-range wireless signal propagation the capacity of the last hop cellular link can vary by orders of magnitude within a short period of time (e.g. a few seconds). Unfortunately TCP does not perform well in such fast-changing environments potentially leading to poor spectrum utilization and high end-to-end packet delay. In this paper we revisit seminal work in cross-layer optimization the context of 4G cellular networks. Specifically we leverage the rich physical layer information exchanged between base stations (NodeB) and mobile phones (UE) to predict the capacity of the underlying cellular link and propose CQIC a cross-layer congestion control design. Experiments on real cellular networks confirm that our capacity estimation method is both accurate and precise. A CQIC sender uses these capacity estimates to adjust its packet sending behavior. Our preliminary evaluation reveals that CQIC improves throughput over TCP by 1.08–2.89 _ for small and medium flows. For large flows CQIC attains throughput comparable to TCP while reducing the average RTT by 2.38–2.65x.,http://research.google.com/pubs/archive/43311.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=CQIC:+Revisiting+Cross-Layer+Congestion+Control+f+or+Cellular+Networks+Lu+Du+Jain+Voelker+Snoeren+Terzis,http://research.google.com/pubs/pub43311.html
Google Correlate Whitepaper,Google (2011),2011,Matt Mohebbi Dan Vanderkam Julia Kodysh Rob Schonberger Hyunyoung Choi Sanjiv Kumar,@techreport{41695 title = {Google Correlate Whitepaper} author = {Matt Mohebbi and Dan Vanderkam and Julia Kodysh and Rob Schonberger and Hyunyoung Choi and Sanjiv Kumar} year = 2011 URL = {http://www.google.com/trends/correlate/whitepaper.pdf} institution = {Google} },Trends in online web search query data have been shown useful in providing models of real world phenomena. However many of these results rely on the careful choice of queries that prior knowledge suggests should correspond with the phenomenon. Here we present an online automated method for query selection that does not require such prior knowledge. Instead given a temporal or spatial pattern of interest we determine which queries best mimic the data. These search queries can then serve to build an estimate of the true value of the phenomenon. We present the application of this method to produce accurate models of influenza activity and home refinance rate in the United States. We additionally show that spatial patterns in real world activity and temporal patterns in web search query activity can both surface interesting and useful correlations.,http://research.google.com/pubs/archive/41695.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google+Correlate+Whitepaper+Mohebbi+Vanderkam+Kodysh+Schonberger+Choi+Kumar,http://research.google.com/pubs/pub41695.html
Media agenda setting and online search traffic: Influences of online and traditional media,American Political Science Association American Political Science Association (2010),2010,Laura Ann Granka,@inproceedings{36915 title = {Media agenda setting and online search traffic: Influences of online and traditional media} author = {Laura Ann Granka} year = 2010 URL = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1658172} booktitle = {American Political Science Association} },This paper addresses the patterns of influence between the news media and the public by specifically targeting breaking stories or shocks to a news system. Specifically we assess media agenda setting and selective exposure by looking at the relative public attention spans to hard and soft news (as measured by query volume) in comparison with the volume of news coverage (in print broadcast and Web content) for these selected news events. We measure the dynamic distribution of issue coverage in the news media and how this volume of coverage ultimately influences online search traffic. In order to assess sustained interest in a given topic distributions of query volume and news coverage were fit with Gamma distributions of appropriate parameters. Findings indicate that there are significant differences in the public attention spans for hard and soft news issues particularly relative to what news coverage might predict. Soft news events produced a slower rate of decline in query volume matching the slow tapering off of issue coverage found in Web content. Conversely for hard substantive news issues query volume drops off quite quickly more closely paralleling the distribution of coverage in broadcast news.,http://research.google.com/pubs/archive/36915.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Media+agenda+setting+and+online+search+traffic:+Influences+of+online+and+traditional+media+Granka,http://research.google.com/pubs/pub36915.html
Efficient Closed-Form Solution to Generalized Boundary Detection,Proceedings of European Conference on Computer Vision (ECCV'12) (2012),2012,Marius Leordeanu Rahul Sukthankar Crisitian Sminchisescu,@inproceedings{41111 title = {Efficient Closed-Form Solution to Generalized Boundary Detection} author = {Marius Leordeanu and Rahul Sukthankar and Crisitian Sminchisescu} year = 2012 booktitle = {Proceedings of European Conference on Computer Vision (ECCV'12)} },Boundary detection is essential for a variety of computer vision tasks such as segmentation and recognition. We propose a unified formulation for boundary detection with closed-form solution which is applicable to the localization of different types of boundaries such as intensity edges and occlusion boundaries from video and RGB-D cameras. Our algorithm simultaneously combines low- and mid-level image representations in a single eigenvalue problem and we solve over an infinite set of putative boundary orientations. Moreover our method achieves state of the art results at a significantly lower computational cost than current methods. We also propose a novel method for soft-segmentation that can be used in conjunction with our boundary detection algorithm and improve its accuracy at a negligible extra computational cost.,http://research.google.com/pubs/archive/41111.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Efficient+Closed-Form+Solution+to+Generalized+Boundary+Detection+Leordeanu+Sukthankar+Sminchisescu,http://research.google.com/pubs/pub41111.html
Blognoon: Exploring a Topic in the Blogosphere,WWW 2011 ACM New York NY USA pp. 213-216,2011,Maria Grineva Maxim Grinev Dmitry Lizorkin Alexander Boldakov Denis Turdakov Andrey Sysoe Alexander Kiyko,@inproceedings{37126 title = {Blognoon: Exploring a Topic in the Blogosphere} author = {Maria Grineva and Maxim Grinev and Dmitry Lizorkin and Alexander Boldakov and Denis Turdakov and Andrey Sysoe and Alexander Kiyko} year = 2011 URL = {http://www.www2011india.com/proceeding/companion/p213.pdf} note = {Demo session} booktitle = {WWW 2011} pages = {213-216} address = {New York NY USA} },We demonstrate Blognoon a semantic blog search engine with the focus on topic exploration and navigation. Blognoon provides concept search instead of traditional keywords search and improves ranking by identifying main topics of posts. It enhances navigation over the Blogosphere with faceted interfaces and recommendations.,http://research.google.com/pubs/archive/37126.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Blognoon:+Exploring+a+Topic+in+the+Blogosphere+Grineva+Grinev+Lizorkin+Boldakov+Turdakov+Sysoev+Kiyko,http://research.google.com/pubs/pub37126.html
Semantic Queries by Example,Proceedings of the 16th International Conference on Extending Database Technology (EDBT 2013) (to appear),2013,Lipyeow Lim Haixun Wang Min Wang,@inproceedings{40761 title = {Semantic Queries by Example} author = {Lipyeow Lim and Haixun Wang and Min Wang} year = 2013 booktitle = {Proceedings of the 16th International Conference on Extending Database Technology (EDBT 2013)} },With the ever increasing quantities of electronic data there is a growing need to make sense out of the data. Many advanced database applications are beginning to support this need by integrating domain knowledge encoded as ontologies into queries over relational data. However it is extremely difficult to express queries against graph structured ontology in the relational SQL query language or its extensions. Moreover semantic queries are usually not precise especially when data and its related ontology are complicated. Users often only have a vague notion of their information needs and are not able to specify queries precisely. In this paper we address these challenges by introducing a novel method to support semantic queries in relational databases with ease. Instead of casting ontology into relational form and creating new language constructs to express such queries we ask the user to provide a small number of examples that satisfy the query she has in mind. Using those examples as seeds the system infers the exact query automatically and the user is therefore shielded from the complexity of interfacing with the ontology. Our approach consists of three steps. In the first step the user provides several examples that satisfy the query. In the second step we use machine learning techniques to mine the semantics of the query from the given examples and related ontologies. Finally we apply the query semantics on the data to generate the full query result. We also implement an optional active learning mechanism to find the query semantics accurately and quickly. Our experiments validate the effectiveness of our approach.,http://research.google.com/pubs/archive/40761.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Queries+by+Example+Lim+Wang+Wang,http://research.google.com/pubs/pub40761.html
One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling,ArXiv Google (2013),2013,Ciprian Chelba Tomas Mikolov Mike Schuster Qi Ge Thorsten Brants Phillipp Koehn Tony Robinson,@techreport{41880 title = {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling} author = {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson} year = 2013 URL = {http://arxiv.org/abs/1312.3005} institution = {Google} },We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data we hope this benchmark will be useful to quickly evaluate novel language modeling techniques and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity or 10% reduction in cross-entropy (bits) over that baseline. The benchmark is available as a code.google.com project at https://code.google.com/p/1-billion-word-language-modeling-benchmark/; besides the scripts needed to rebuild the training/held-out data it also makes available log-probability values for each word in each of ten held-out data sets for each of the baseline n-gram models.,http://research.google.com/pubs/archive/41880.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=One+Billion+Word+Benchmark+for+Measuring+Progress+in+Statistical+Language+Modeling+Chelba+Mikolov+Schuster+Ge+Brants+Koehn+Robinson,http://research.google.com/pubs/pub41880.html
Adding Meaning to Facebook Microposts via a Mash-up API and Tracking Its Data Provenance,The 7th International Conference on Next Generation Web Services Practices (NWeSP 2011),2011,Thomas Steiner Ruben Verborgh Joaquim Gabarro Rik Van de Walle,@inproceedings{37426 title = {Adding Meaning to Facebook Microposts via a Mash-up API and Tracking Its Data Provenance} author = {Thomas Steiner and Ruben Verborgh and Joaquim Gabarro and Rik Van de Walle} year = 2011 URL = {http://www.lsi.upc.edu/~tsteiner/papers/2011/adding-meaning-to-facebook-microposts-nwesp2011.pdf} booktitle = {The 7th International Conference on Next Generation Web Services Practices (NWeSP 2011)} },The social networking website Facebook offers to its users a feature called “status updates” (or just “status”) which allows users to create microposts directed to all their contacts or a subset thereof. Readers can respond to microposts or in addition to that also click a “Like” button to show their appreciation for a certain micropost. Adding semantic meaning in the sense of unambiguous intended ideas to such microposts can for example be achieved via Natural Language Processing (NLP). Therefore we have implemented a RESTful mash-up NLP API which is based on a combination of several third party NLP APIs in order to retrieve more accurate results in the sense of emergence. In consequence our API uses third party APIs opaquely in the background in order to deliver its output. In this paper we describe how one can keep track of provenance and credit back the contributions of each single API to the combined result of all APIs. In addition to that we show how the existence of provenance metadata can help understand the way a combined result is formed and optimize the result combination process. Therefore we use the HTTP Vocabulary in RDF and the Provenance Vocabulary. The main contribution of our work is a description of how provenance metadata can be automatically added to the output of mash-up APIs like the one presented here.,http://research.google.com/pubs/archive/37426.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Adding+Meaning+to+Facebook+Microposts+via+a+Mash-up+API+and+Tracking+Its+Data+Provenance+Steiner+Verborgh+Gabarro+Van+de+Walle,http://research.google.com/pubs/pub37426.html
Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding,IEEE/ACM Transactions on Audio Speech and Language Processing vol. 23 (2015) pp. 530-539,2015,Grégoire Mesnil Yann Dauphin Kaisheng Yao Yoshua Bengio Li Deng Dilek Hakkani-Tür Xiaodong He Larry Heck Gokhan Tur Dong Yu Geoffrey Zweig,@article{44628 title = {Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding} author = {Grégoire Mesnil and Yann Dauphin and Kaisheng Yao and Yoshua Bengio and Li Deng and Dilek Hakkani-Tür and Xiaodong He and Larry Heck and Gokhan Tur and Dong Yu and Geoffrey Zweig} year = 2015 journal = {IEEE/ACM Transactions on Audio Speech and Language Processing} pages = {530-539} volume = {23} },Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU). In this paper we propose to use recurrent neural networks (RNNs) for this task and present several novel architectures designed to efficiently model past and future temporal dependencies. Specifically we implemented and compared several important RNN architectures including Elman Jordan and hybrid variants. To facilitate reproducibility we implemented these networks with the publicly available Theano neural network toolkit and completed experiments on the well-known airline travel information system (ATIS) benchmark. In addition we compared the approaches on two custom SLU data sets from the entertainment and movies domains. Our results show that the RNN-based models outperform the conditional random field (CRF) baseline by 2% in absolute error reduction on the ATIS benchmark. We improve the state-of-the-art by 0.5% in the Entertainment domain and 6.7% for the movies domain.,http://research.google.com/pubs/archive/44628.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Using+Recurrent+Neural+Networks+for+Slot+Filling+in+Spoken+Language+Understanding+Mesnil+Dauphin+Yao+Bengio+Deng+Hakkani-T%C3%BCr+He+Heck+Tur+Yu+Zweig,http://research.google.com/pubs/pub44628.html
Two-Stage Learning Kernel Algorithms,Proceedings of the 27th Annual International Conference on Machine Learning (ICML 2010),2010,Corinna Cortes Mehryar Mohri Afshin Rostamizadeh,@inproceedings{36468 title = {Two-Stage Learning Kernel Algorithms} author = {Corinna Cortes and Mehryar Mohri and Afshin Rostamizadeh} year = 2010 URL = {http://www.cs.nyu.edu/~mohri/pub/align.pdf} booktitle = {Proceedings of the 27th Annual International Conference on Machine Learning (ICML 2010)} },This paper examines two-stage techniques for learning kernels based on a notion of alignment. It presents a number of novel theoretical algorithmic and empirical results for alignmentbased techniques. Our results build on previous work by Cristianini et al. (2001) but we adopt a different definition of kernel alignment and significantly extend that work in several directions: we give a novel and simple concentration bound for alignment between kernel matrices; show the existence of good predictors for kernels with high alignment both for classification and for regression; give algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP; and report the results of extensive experimentswith this alignment-based method in classification and regression tasks which show an improvement both over the uniformcombination of kernels and over other state-of-the-art learning kernel methods.,http://research.google.com/pubs/archive/36468.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Two-Stage+Learning+Kernel+Algorithms+Cortes+Mohri+Rostamizadeh,http://research.google.com/pubs/pub36468.html
Best-Buddies Similarity for Robust Template Matching,IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2015),2015,Tali Dekel Shaul Oron Michael Rubinstein Shai Avidan William T. Freeman,@inproceedings{43841 title = {Best-Buddies Similarity for Robust Template Matching} author = {Tali Dekel and Shaul Oron and Michael Rubinstein and Shai Avidan and William T. Freeman} year = 2015 URL = {http://people.csail.mit.edu/talidekel/Best-Buddies%20Similarity.html} booktitle = {IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)} },We propose a novel method for template matching in unconstrained environments. Its essence is the Best Buddies Similarity (BBS) a useful robust and parameter-free similarity measure between two sets of points. BBS is based on a count of Best Buddies Pairs (BBPs)—pairs of points in which each one is the nearest neighbor of the other. BBS has several key features that make it robust against complex geometric deformations and high levels of outliers such as those arising from background clutter and occlusions. We study these properties provide a statistical analysis that justifies them and demonstrate the consistent success of BBS on a challenging real-world dataset.,http://research.google.com/pubs/archive/43841.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Best-Buddies+Similarity+for+Robust+Template+Matching+Dekel+Oron+Rubinstein+Avidan+Freeman,http://research.google.com/pubs/pub43841.html
Table Detection in Heterogeneous Documents,Document Analysis Systems 2010 ACM International Conference Proceedings series,2010,Faisal Shafait Ray Smith,@inproceedings{35652 title = {Table Detection in Heterogeneous Documents} author = {Faisal Shafait and Ray Smith} year = 2010 URL = {http://doi.acm.org/10.1145/1815330.1815339} booktitle = {Document Analysis Systems 2010} },Detecting tables in document images is important since not only do tables contain important information but also most of the layout analysis methods fail in the presence of tables in the document image. Existing approaches for table de- tection mainly focus on detecting tables in single columns of text and do not work reliably on documents with varying layouts. This paper presents a practical algorithm for table detection that works with a high accuracy on documents with varying layouts (company reports newspaper articles magazine pages . . . ). An open source implementation of the algorithm is provided as part of the Tesseract OCR engine. Evaluation of the algorithm on document images from pub- licly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system.,http://doi.acm.org/10.1145/1815330.1815339,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Table+Detection+in+Heterogeneous+Documents+Shafait+Smith,http://research.google.com/pubs/pub35652.html
Recursive Sparse Spatiotemporal Coding,Proceedings of the Fifth IEEE International Workshop on Multimedia Information Processing and Retrieval IEEE Computer Society (2009),2009,Thomas Dean Greg Corrado Richard Washington,@inproceedings{41118 title = {Recursive Sparse Spatiotemporal Coding} author = {Thomas Dean and Greg Corrado and Richard Washington} year = 2009 booktitle = {Proceedings of the Fifth IEEE International Workshop on Multimedia Information Processing and Retrieval} },We present a new approach to learning sparse spatiotemporal codes in which the number of basis vectors their orientations velocities and the size of their receptive fields change over the duration of unsupervised training. The algorithm starts with a relatively small initial basis with minimal temporal extent. This initial basis is obtained through conventional sparse coding techniques and is expanded over time by recursively constructing a new basis consisting of basis vectors with larger temporal extent that proportionally conserve regions of previously trained weights. These proportionally conserved weights are combined with the result of adjusting newly added weights to represent a greater range of primitive motion features. The size of the current basis is determined probabilistically by sampling from existing basis vectors according to their activation on the training set. The resulting algorithm produces bases consisting of filters that are bandpass spatially oriented and temporally diverse in terms of their transformations and velocities. The basic methodology borrows inspiration from the layer-by-layer learning of multiple-layer restricted Boltzmann machines developed by Geoff Hinton and his students. Indeed we can learn multiple-layer sparse codes by training a stack of denoising autoencoders but we have had greater success using L1 regularized regression in a variation on Olshausen and Field's original SPARSENET. To accelerate learning and focus attention we apply a space-time interest-point operator that selects for periodic motion. This attentional mechanism enables us to efficiently compute and compactly represent a broad range of interesting motion. We demonstrate the utility of our approach by using it to recognize human activity in video. Our algorithm meets or exceeds the performance of state-of-the-art activity-recognition methods.,http://research.google.com/pubs/archive/41118.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Recursive+Sparse+Spatiotemporal+Coding+Dean+Corrado+Washington,http://research.google.com/pubs/pub41118.html
Functional and Logic Programming,Springer LNCS (2010),2010,Matthias Blume Naoki Kobayashi Germán Vidal,@proceedings{36663 title = {Functional and Logic Programming} editor = {Matthias Blume and Naoki Kobayashi and Germán Vidal} year = 2010 },Functional and Logic Programming 10th International Symposium FLOPS 2010 Sendai Japan April 19-21 2010 Proceedings,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Functional+and+Logic+Programming+Blume+Kobayashi+Vidal,http://research.google.com/pubs/pub36663.html
Beyond Heuristics: Learning to Classify Vulnerabilities and Predict Exploits,Proceedings of the Sixteenth ACM Conference on Knowledge Discovery and Data Mining (KDD-2010) pp. 105-113,2010,Mehran Bozorgi Lawrence Saul Stefan Savage Geoffrey M. Voelker,@inproceedings{36738 title = {Beyond Heuristics: Learning to Classify Vulnerabilities and Predict Exploits} author = {Mehran Bozorgi and Lawrence Saul and Stefan Savage and Geoffrey M. Voelker} year = 2010 URL = {http://www.sysnet.ucsd.edu/projects/exploit-learn/} booktitle = {Proceedings of the Sixteenth ACM Conference on Knowledge Discovery and Data Mining (KDD-2010)} pages = {105-113} },The security demands on modern system administration are enormous and getting worse. Chief among these demands administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way. Such vulnerabilities include buffer overflow errors improperly validated inputs and other unanticipated attack modalities. In 2008 over 7400 new vulnerabilities were disclosed—well over 100 per week. While no enterprise is affected by all of these disclosures administrators commonly face many outstanding vulnerabilities across the software systems they manage. A key question for systems administrators is which vulnerabilities to prioritize. From publicly available databases that document past vulnerabilities we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited. As input our classifiers operate on high dimensional feature vectors that we extract from the text fields time stamps cross-references and other entries in existing vulnerability disclosure reports. Compared to current industry-standard heuristics based on expert knowledge and static formulas our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited.,http://research.google.com/pubs/archive/36738.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Beyond+Heuristics:+Learning+to+Classify+Vulnerabilities+and+Predict+Exploits+Bozorgi+Saul+Savage+Voelker,http://research.google.com/pubs/pub36738.html
Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search,Mobile Speech and Advanced Natural Language Solutions Springer Science+Business Media New York (2013) pp. 197-229,2013,Ciprian Chelba Johan Schalkwyk,@inbook{41096 title = {Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search} author = {Ciprian Chelba and Johan Schalkwyk} year = 2013 URL = {http://www.springer.com/engineering/signals/book/978-1-4614-6017-6} booktitle = {Mobile Speech and Advanced Natural Language Solutions} pages = {197--229} address = {New York} },Mobile is poised to become the predominant platform over which people are accessing the World Wide Web. Recent developments in speech recognition and understanding backed by high bandwidth coverage and high quality speech signal acquisition on smartphones and tablets are presenting the users with the choice of speaking their web search queries instead of typing them. A critical component of a speech recognition system targeting web search is the language model. The chapter presents an empirical exploration of the google.com query stream with the end goal of high quality statistical language modeling for mobile voice search. Our experiments show that after text normalization the query stream is not as ``wild'' as it seems at first sight. One can achieve out-of-vocabulary rates below 1% using a one million word vocabulary and excellent n-gram hit ratios of 77/88% even at high orders such as n=5/4 respectively. A more careful analysis shows that a significantly larger vocabulary (approx. 10 million words) may be required to guarantee at most 1% out-of-vocabulary rate for a large percentage (95%) of users. Using large scale distributed language models can improve performance significantly---up to 10% relative reductions in word-error-rate over conventional models used in speech recognition. We also find that the query stream is non-stationary which means that adding more past training data beyond a certain point provides diminishing returns and may even degrade performance slightly. Perhaps less surprisingly we have shown that locale matters significantly for English query data across USA Great Britain and Australia. In an attempt to leverage the speech data in voice search logs we successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance.,http://www.springer.com/engineering/signals/book/978-1-4614-6017-6,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Empirical+Exploration+of+Language+Modeling+for+the+google.com+Query+Stream+as+Applied+to+Mobile+Voice+Search+Chelba+Schalkwyk,http://research.google.com/pubs/pub41096.html
A 6 µW per Channel Analog Biomimetic Cochlear Implant Processor Filterbank Architecture With Across Channels AGC,IEEE Transactions on Biomedical Circuits and Systems vol. 9 (2015) pp. 72-86,2015,Guang Wang Richard F. Lyon Emmanuel M. Drakakis,@article{43280 title = {A 6 µW per Channel Analog Biomimetic Cochlear Implant Processor Filterbank Architecture With Across Channels AGC} author = {Guang Wang and Richard F. Lyon and Emmanuel M. Drakakis} year = 2015 journal = {IEEE Transactions on Biomedical Circuits and Systems} pages = {72--86} volume = {9} },A new analog cochlear implant processor filterbank architecture of increased biofidelity enhanced across-channel contrast and very low power consumption has been designed and prototyped. Each channel implements a biomimetic asymmetric bandpass-like One-Zero-Gammatone-Filter (OZGF) transfer function using class-AB log-domain techniques. Each channel's quality factor and suppression are controlled by means of a new low power Automatic Gain Control (AGC) scheme which is coupled across the neighboring channels and emulates lateral inhibition (LI) phenomena in the auditory system. Detailed measurements from a five-channel silicon IC prototype fabricated in a 0.35 µm AMS technology confirm the operation of the coupled AGC scheme and its ability to enhance contrast among channel outputs. The prototype is characterized by an input dynamic range of 92 dB while consuming only 28 µW of power in total ~6 µW per channel) under a 1.8 V power supply. The architecture is well-suited for fully-implantable cochlear implants.,http://research.google.com/pubs/archive/43280.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+6+%C2%B5W+per+Channel+Analog+Biomimetic+Cochlear+Implant+Processor+Filterbank+Architecture+With+Across+Channels+AGC+Wang+Lyon+Drakakis,http://research.google.com/pubs/pub43280.html
Unsupervised Translation Sense Clustering,the North American Association of Computational Linguistics (2012),2012,Mohit Bansal John DeNero Dekang Lin,@inproceedings{38138 title = {Unsupervised Translation Sense Clustering} author = {Mohit Bansal and John DeNero and Dekang Lin} year = 2012 booktitle = {the North American Association of Computational Linguistics} },We propose an unsupervised method for clustering the translations of a word such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm. In addition to describing our approach we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally we describe a method for annotating clusters with usage examples.,http://research.google.com/pubs/archive/38138.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Unsupervised+Translation+Sense+Clustering+Bansal+DeNero+Lin,http://research.google.com/pubs/pub38138.html
RFC7706 - Decreasing Access Time to Root Servers by Running One on Loopback,IETF RFCs Internet Engineering Task Force (2015) pp. 12,2015,Warren Kumari Paul Hoffman,@incollection{44314 title = {RFC7706 - Decreasing Access Time to Root Servers by Running One on Loopback} author = {Warren Kumari and Paul Hoffman} year = 2015 booktitle = {IETF RFCs} pages = {12} },Some DNS recursive resolvers have longer-than-desired round-trip times to the closest DNS root server. Some DNS recursive resolver operators want to prevent snooping of requests sent to DNS root servers by third parties. Such resolvers can greatly decrease the round-trip time and prevent observation of requests by running a copy of the full root zone on a loopback address (such as 127.0.0.1). This document shows how to start and maintain such a copy of the root zone that does not pose a threat to other users of the DNS at the cost of adding some operational fragility for the operator.,http://research.google.com/pubs/archive/44314.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC7706+-+Decreasing+Access+Time+to+Root+Servers+by+Running+One+on+Loopback+Kumari+Hoffman,http://research.google.com/pubs/pub44314.html
RDRP: Reward-Driven Request Prioritization for e-Commerce Web Sites,Electronic Commerce Research and Applications vol. 9 (2010) pp. 549-561,2010,Alexander Totok Vijay Karamcheti,@article{36924 title = {RDRP: Reward-Driven Request Prioritization for e-Commerce Web Sites} author = {Alexander Totok and Vijay Karamcheti} year = 2010 URL = {http://dx.doi.org/10.1016/j.elerap.2010.03.001} journal = {Electronic Commerce Research and Applications} pages = {549-561} volume = {9} },Meeting client Quality-of-Service (QoS) expectations proves to be a difficult task for the providers of e-Commerce services especially when web servers experience overload conditions which cause increased response times and request rejections leading to user frustration lowered usage of the service and reduced revenues. In this paper we propose a server-side request scheduling mechanism that addresses these problems. Our Reward-Driven Request Prioritization (RDRP) algorithm gives higher execution priority to client web sessions that are likely to bring more service profit (or any other application-specific reward). The method works by predicting future session structure by comparing its requests seen so far with aggregated information about recent client behavior and using these predictions to preferentially allocate web server resources. Our experiments using the TPC-W benchmark application with an implementation of the RDRP techniques in the JBoss web application server show that RDRP can significantly boost profit attained by the service while providing better QoS to clients that bring more profit.,http://research.google.com/pubs/archive/36924.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RDRP:+Reward-Driven+Request+Prioritization+for+e-Commerce+Web+Sites+Totok+Karamcheti,http://research.google.com/pubs/pub36924.html
Hardware Requirements for Optical Circuit Switched Data Center Networks,IEEE Communication Society IEEE Photonic Society Optical Society of America OFC/NFOEC 2011 Los Angeles (2010) OTuH3,2010,Nathan Farrington Yeshaiahu Fainman Hong Liu George Papen Amin Vahdat,@proceedings{36840 title = {Hardware Requirements for Optical Circuit Switched Data Center Networks} editor = {Nathan Farrington and Yeshaiahu Fainman and Hong Liu and George Papen and Amin Vahdat} year = 2010 booktitle = {Optical Fiber Communications Conference} pages = {OTuH3} address = {OFC/NFOEC 2011 Los Angeles} },Based on measurements of a prototype we identify hardware requirements for improving the performance of hybrid electrical-packet-switched/optical-circuit-switched data center networks.,http://research.google.com/pubs/archive/36840.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hardware+Requirements+for+Optical+Circuit+Switched+Data+Center+Networks+Farrington+Fainman+Liu+Papen+Vahdat,http://research.google.com/pubs/pub36840.html
The Google Technical Interview,XRDS vol. Volume 20 No. 2 (2013) pp. 12-14,2013,Dean Jackson,@article{41881 title = {The Google Technical Interview} author = {Dean Jackson} year = 2013 URL = {http://xrds.acm.org/article.cfm?aid=2539270f} journal = {XRDS} pages = {12-14} volume = {Volume 20 No. 2} },A review of the Google technical interviews intended for students of computer science and related disciplines.,http://research.google.com/pubs/archive/41881.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Google+Technical+Interview+Jackson,http://research.google.com/pubs/pub41881.html
Distributed Authorization With Distributed Grammars,Programming Languages with Applications to Biology and Security Springer International Publishing Switzerland Gewerbestrasse 11 CH-6330 Cham (ZG) Switzerland (2015) (to appear),2015,Martin Abadi Mike Burrows Himabindu Pucha Adam Sadovsky Asim Shankar Ankur Taly,@inproceedings{43979 title = {Distributed Authorization With Distributed Grammars} author = {Martin Abadi and Mike Burrows and Himabindu Pucha and Adam Sadovsky and Asim Shankar and Ankur Taly} year = 2015 booktitle = {Programming Languages with Applications to Biology and Security} address = {Gewerbestrasse 11 CH-6330 Cham (ZG) Switzerland} },While groups are generally helpful for the definition of authorization policies their use in distributed systems is not straightforward. This paper describes a design for authorization in distributed systems that treats groups as formal languages. The design supports forms of delegation and negative clauses in authorization policies. It also considers the wish for privacy and efficiency in group-membership checks and the possibility that group definitions may not all be available and may contain cycles.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Authorization+With+Distributed+Grammars+Abadi+Burrows+Pucha+Sadovsky+Shankar+Taly,http://research.google.com/pubs/pub43979.html
Building Web Apps for Google TV,o'Reilly Media 1005 Gravenstein Hwy N Sebastopol CA 95472 (2011) pp. 116,2011,Amanda Surya Andres Ferrate Daniels Lee Maile Ohye Paul Carff Shawn Shen Steven Hines,@book{37253 title = {Building Web Apps for Google TV} author = {Amanda Surya and Andres Ferrate and Daniels Lee and Maile Ohye and Paul Carff and Shawn Shen and Steven Hines} year = 2011 URL = {http://oreilly.com/catalog/0636920019886/} pages = {116} address = {1005 Gravenstein Hwy N Sebastopol CA 95472} },By integrating the Web with traditional TV Google TV offers developers an important new channel for content. But creating apps for Google TV requires learning some new skills—in fact what you may already know about mobile or desktop web apps isn't entirely applicable. Building Web Apps for Google TV will help you make the transition to Google TV as you learn the tools and techniques necessary to build sophisticated web apps for this platform.,http://oreilly.com/catalog/0636920019886/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Building+Web+Apps+for+Google+TV+Surya+Ferrate+Lee+Ohye+Carff+Shen+Hines,http://research.google.com/pubs/pub37253.html
Google's C/C++ toolchain for smart handheld devices,VLSI Design Automation and Test (VLSI-DAT) 2012 International Symposium on IEEE,2012,Doug Kwan Jing Yu Bhaskar Janakiraman,@inproceedings{40355 title = {Google's C/C++ toolchain for smart handheld devices} author = {Doug Kwan and Jing Yu and Bhaskar Janakiraman} year = 2012 note = {Invited paper} booktitle = {VLSI Design Automation and Test (VLSI-DAT) 2012 International Symposium on} },Smart handheld devices are ubiquitous today and software plays an important role on them. Therefore a compiler and related tools can improve devices by generating efficient compact and secure code. In this paper we share our experience of applying various compilation techniques at Google to improve software running on smart handheld devices using our mobile platforms as examples. At Google we use the GNU toolchain for generating code on different platforms and for conducting compiler research and development. We have developed new techniques added features and functionality in the GNU tools. Some of these results are now used for smart handheld devices.,http://research.google.com/pubs/archive/40355.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google's+C/C%2B%2B+toolchain+for+smart+handheld+devices+Kwan+Yu+Janakiraman,http://research.google.com/pubs/pub40355.html
Design of user interfaces for selective editing of digital photos on touchscreen devices,Proceedings SPIE 8667 (Multimedia Content and Mobile Devices) SPIE (2013),2013,Thomas Binder Meikel Steiding Manuel Wille Nils Kokemohr,@inproceedings{41338 title = {Design of user interfaces for selective editing of digital photos on touchscreen devices} author = {Thomas Binder and Meikel Steiding and Manuel Wille and Nils Kokemohr} year = 2013 URL = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1662496} booktitle = {Proceedings SPIE 8667 (Multimedia Content and Mobile Devices)} },When editing images it is often desirable to apply a filter with a spatially varying strength. With the usual selection tools like gradient lasso brush or quick selection tools creating masks containing such spatially varying strength values is time-consuming and cumbersome. We present an interactive filtering approach which allows to process photos selectively without the intermediate step of creating a mask containing strength values. In using this approach the user only needs to place reference points (called control points) on the image and to adjust the spatial influence and filter strength for each control point. The filter is then applied selectively to the image with strength values interpolated for each pixel between control points. The interpolation is based on a mixture of distances in space luminance and color; it is therefore a low-level operation. Since the main goal of the approach is to make selective image editing intuitive easy and playful emphasis is put on the user interface: We describe the process of developing an existing mouse-driven user interface into a touch-driven one. Many question needed to be answered anew such as how to present a slider widget on a touchscreen. Several variants are discussed and compared.,http://research.google.com/pubs/archive/41338.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Design+of+user+interfaces+for+selective+editing+of+digital+photos+on+touchscreen+devices+Binder+Steiding+Wille+Kokemohr,http://research.google.com/pubs/pub41338.html
Reporting Neighbors in High-Dimensional Euclidean Space,SIAM journal of computing vol. 43 (2014) pp. 1239-1511,2014,Dror Aiger Haim Kaplan Micha Sharir,@article{42457 title = {Reporting Neighbors in High-Dimensional Euclidean Space} author = {Dror Aiger and Haim Kaplan and Micha Sharir} year = 2014 note = {A preliminary version of the paper has appeared in Proc. 24th ACM-SIAM Sympos. Discrete Algorithm 2013} journal = {SIAM journal of computing} pages = {pp. 1239-1511} volume = {43} },We consider the following problem which arises in many database and web-based applications: Given a set P of n points in a high-dimensional space Rd and a distance r we want to report all pairs of points of P at Euclidean distance at most r. We present two randomized algorithms one based on randomly shifted grids and the other on randomly shifted and rotated grids. The running time of both algorithms is of the form C(d)(n + k)log n where k is the output size and C(d) is a constant that depends on the dimension d. The log n factor is needed to guarantee with high probability that all neighbor pairs are reported and can be dropped if it suffices to report in expectation an arbitrarily large fraction of the pairs. When only translations are used C(d) is of the form (a√d)d for some (small) absolute constant a≈0.484; this bound is worst-case tight up to an exponential factor of about 2d. When both rotations and translations are used C(d) can be improved to roughly 6.74d getting rid of the super-exponential factor √dd. When the input set (lies in a subset of d-space that) has low doubling dimension the performance of the first algorithm improves to C(d_)(n + k)log n (or to C(d_)(n + k)) where C(d_)=O((ed/_)_) for _≤ √d. Otherwise (d_)=O(e√d√d_. We also present experimental results on several large datasets demonstrating that our algorithms run significantly faster than all the leading existing algorithms for reporting neighbors.,http://research.google.com/pubs/archive/42457.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reporting+Neighbors+in+High-Dimensional+Euclidean+Space+Aiger+Kaplan+Sharir,http://research.google.com/pubs/pub42457.html
Optimizing Programs with Intended Semantics,Proceedings of OOPSLA ACM Press (2009) (to appear),2009,Daniel von Dincklage Amer Diwan,@inproceedings{35475 title = {Optimizing Programs with Intended Semantics} author = {Daniel von Dincklage and Amer Diwan} year = 2009 booktitle = {Proceedings of OOPSLA} },Modern object-oriented languages have complex features that cause programmers to overspecify their programs. This overspecification hinders automatic optimizers since they must preserve the overspecified semantics. If an optimizer knew which semantics the programmer intended it could do a better job. Making a programmer clarify his intentions by placing assumptions into the program is rarely practical. This is because the programmer does not know which parts of the programs' overspecified semantics hinder the optimizer. Therefore the programmer has to guess which assumption to add. Since the programmer can add many different assumptions to a large program he will need to place many such assumptions before he guesses right and helps the optimizer. We present IOpt a practical optimizer that uses a specification of the programmers' intended semantics to enable additional optimizations. That way our optimizer can significantly improve the performance of a program. We present case studies in which we use IOpt to speed up two programs by over 50%. To make specifying the intended semantics practical IOpt communicates with the programmer. IOpt identifies which assumptions the programmer textit{should} place and where he should place them. IOpt ranks each assumption by (i) the likelyhood that the assumption conforms to the programmers' intended semantics and (ii) how much the assumption will help IOpt improve the programs' performance. IOpt proposes ranked assumptions to the programmer who just picks those that conform to his intended semantics. With this approach IOpt keeps the programmers' specification burden low. Our case studies show that the programmer just needs to add a few assumptions to realize the 50% speedup.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Optimizing+Programs+with+Intended+Semantics+von+Dincklage+Diwan,http://research.google.com/pubs/pub35475.html
Diagnosing Automatic Whitelisting for Dynamic Remarketing Ads Using Hybrid ASP,Francesco Calimeri Giovambattista Ianni Miroslaw Truszczynski. Logic Programming and Nonmonotonic Reasoning 13th International Conference LPNMR 2015 Lexington September 27-30 2015. Proceedings. Springer International Publishing AG Gewerbestrasse 11 CH-6330 Cham (ZG) Switzerland t.b.d. (to appear),2015,Alex Brik Jeffrey Remmel,@inproceedings{43812 title = {Diagnosing Automatic Whitelisting for Dynamic Remarketing Ads Using Hybrid ASP} author = {Alex Brik and Jeffrey Remmel} year = 2015 booktitle = {Francesco Calimeri Giovambattista Ianni Miroslaw Truszczynski. Logic Programming and Nonmonotonic Reasoning 13th International Conference LPNMR 2015 Lexington September 27-30 2015. Proceedings.} pages = {t.b.d.} address = {Gewerbestrasse 11 CH-6330 Cham (ZG) Switzerland} },Hybrid ASP (H-ASP) is an extension of ASP that allows users to combine ASP type rules and numerical algorithms. Dynamic Remarketing Ads is Google’s platform for serving customized ads based on past interactions with a user. In this paper we will describe the use of H-ASP to diagnose failures of the automatic whitelisting system for Dynamic Remarketing Ads. We will show that the diagnosing task is an instance of a computational pattern that we call the Branching Computational Pattern (BCP). We will then describe a Python H-ASP library (H-ASP PL) that allows to perform computations using a BCP and we will describe a H-ASP PL program that solves the diagnosing problem.,http://research.google.com/pubs/archive/43812.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Diagnosing+Automatic+Whitelisting+for+Dynamic+Remarketing+Ads+Using+Hybrid+ASP+Brik+Remmel,http://research.google.com/pubs/pub43812.html
Web Workers Multithreaded Programs in JavaScript,O'Reilly 1005 Gravenstein Hwy N Sebastopol CA 95472 (2013) pp. 62,2013,Ido Green,@book{41524 title = {Web Workers Multithreaded Programs in JavaScript} author = {Ido Green} year = 2013 URL = {http://shop.oreilly.com/product/0636920024446.do} note = {All the examples of the book are on GitHub - https://github.com/greenido/Web-Workers-Examples-} pages = {62} address = {1005 Gravenstein Hwy N Sebastopol CA 95472} },Web apps would run much better if heavy calculations could be performed in the background rather than compete with the user interface. With this book you’ll learn how to use Web Workers to run computationally intensive JavaScript code in a thread parallel to the UI. Yes multi-threaded programing is complicated but Web Workers provide a simple API that helps you be productive without the complex algorithms. If you have an intermediate to advanced understanding of JavaScript— especially event handling and callbacks—you’re ready to tackle Web Workers with the tools in this example-driven guide.,http://shop.oreilly.com/product/0636920024446.do,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Web+Workers+Multithreaded+Programs+in+JavaScript+Green,http://research.google.com/pubs/pub41524.html
Hardware Trojan Detection Solutions and Design-for-Trust Challenges,IEEE Computer (2011) pp. 64-72,2011,Kurt Rosenfeld,@article{37395 title = {Hardware Trojan Detection Solutions and Design-for-Trust Challenges} author = {Kurt Rosenfeld} year = 2011 journal = {IEEE Computer} pages = {64--72} },Globalization of the semiconductor industry and evolving fabrication processes have made integrated circuits increasingly vulnerable to Trojans. Researchers must expand efforts to verify trust in intellectual property cores and ICs.,http://research.google.com/pubs/archive/37395.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hardware++Trojan+Detection++Solutions+and++Design-for-Trust++Challenges+Rosenfeld,http://research.google.com/pubs/pub37395.html
Handcrafted Fraud and Extortion: Manual Account Hijacking in the Wild,IMC '14 Proceedings of the 2014 Conference on Internet Measurement Conference ACM 1600 Amphitheatre Parkway pp. 347-358,2014,Elie Bursztein Borbala Benko Daniel Margolis Tadek Pietraszek Andy Archer Allan Aquino Andreas Pitsillidis Stefan Savage,@inproceedings{43469 title = {Handcrafted Fraud and Extortion: Manual Account Hijacking in the Wild} author = {Elie Bursztein and Borbala Benko and Daniel Margolis and Tadek Pietraszek and Andy Archer and Allan Aquino and Andreas Pitsillidis and Stefan Savage} year = 2014 URL = {https://www.elie.net/publication/handcrafted-fraud-and-extortion-manual-account-hijacking-in-the-wild} booktitle = {IMC '14 Proceedings of the 2014 Conference on Internet Measurement Conference} pages = {347-358} address = {1600 Amphitheatre Parkway} },Online accounts are inherently valuable resources---both for the data they contain and the reputation they accrue over time. Unsurprisingly this value drives criminals to steal or hijack such accounts. In this paper we focus on manual account hijacking---account hijacking performed manually by humans instead of botnets. We describe the details of the hijacking workflow: the attack vectors the exploitation phase and post-hijacking remediation. Finally we share as a large online company which defense strategies we found effective to curb manual hijacking.,http://research.google.com/pubs/archive/43469.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Handcrafted+Fraud+and+Extortion:+Manual+Account+Hijacking+in+the+Wild+Bursztein+Benko+Margolis+Pietraszek+Archer+Aquino+Pitsillidis+Savage,http://research.google.com/pubs/pub43469.html
Statistical Parametric Speech Synthesis,UKSpeech Conference Edinburgh UK (2014),2014,Heiga Zen,@misc{42624 title = {Statistical Parametric Speech Synthesis} author = {Heiga Zen} year = 2014 note = {Tutorial} },Statistical parametric speech synthesis has grown in popularity over the last years. In this tutorial its system architecture is outlined and then basic techniques used in the system including algorithms for speech parameter generation are described with simple examples.,http://research.google.com/pubs/archive/42624.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+Parametric+Speech+Synthesis+Zen,http://research.google.com/pubs/pub42624.html
ESOFTCHECK: REMOVAL OF NON-VITAL CHECKS FOR FAULT TOLERANCE,Proceedings of the CGO 2009 The Seventh International Symposium on Code Generation and Optimization IEEE Computer Society pp. 35-46,2009,Jing Yu Maria Jesus Garzaran Marc Snir,@inproceedings{36600 title = {ESOFTCHECK: REMOVAL OF NON-VITAL CHECKS FOR FAULT TOLERANCE} author = {Jing Yu and Maria Jesus Garzaran and Marc Snir} year = 2009 booktitle = {Proceedings of the CGO 2009 The Seventh International Symposium on Code Generation and Optimization} pages = {35-46} },As semiconductor technology scales into the deep submicron regime the occurrence of transient or soft errors will increase. This will require new approaches to error detection. Software checking approaches are attractive because they require little hardware modification and can be easily adjusted to fit different reliability and performance requirements. Unfortunately software checking adds a significant performance overhead.In this paper we present ESoftCheck a set of compiler optimization techniques to determine which are the vital checks that is the minimum number of checks that are necessary to detect an error and roll back to a correct program state. ESoftCheck identifies the vital checks on platforms where registers are hardware-protected with parity or ECC when there are redundant checks and when checks appear in loops. ESoftCheck also provides knobs to trade reliability for performance based on the support for recovery and the degree of trustiness of the operations. Our experimental results on a Pentium 4 show that ESoftCheck can obtain 27.1% performance improvement without losing fault coverage.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ESOFTCHECK:+REMOVAL+OF+NON-VITAL+CHECKS+FOR+FAULT+TOLERANCE+Yu+Garzaran+Snir,http://research.google.com/pubs/pub36600.html
Address space randomization for mobile devices,WiSec '11 - Proceedings of the fourth ACM conference on wireless network security ACM New York NY (2011),2011,Hristo Bojinov Dan Boneh Rich Cannings Iliyan Malchev,@inproceedings{37656 title = {Address space randomization for mobile devices} author = {Hristo Bojinov and Dan Boneh and Rich Cannings and Iliyan Malchev} year = 2011 booktitle = {WiSec '11 - Proceedings of the fourth ACM conference on wireless network security} address = {New York NY} },Address Space Layout Randomization (ASLR) is a defensive technique supported by many desktop and server operating systems. While smartphone vendors wish to make it available on their platforms there are technical challenges in implementing ASLR on these devices. Pre-linking limited processing power and restrictive update processes make it dicult to use existing ASLR implementation strategies even on the latest generation of smartphones. In this paper we introduce retouching a mechanism for executable ASLR that requires no kernel modications and is suitable for mobile devices. We have implemented ASLR for the Android operating system and evaluated its eectiveness and performance. In addition we introduce crash stack analysis a technique that uses crash reports locally on the device or in aggregate in the cloud to reliably detect attempts to brute-force ASLR protection. We expect that retouching and crash stack analysis will become standard techniques in mobile ASLR implementations.,http://research.google.com/pubs/archive/37656.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Address+space+randomization+for+mobile+devices+Bojinov+Boneh+Cannings+Malchev,http://research.google.com/pubs/pub37656.html
C/C++ Thread Safety Analysis,2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation IEEE,2014,DeLesley Hutchins Aaron Ballman Dean Sutherland,@inproceedings{42958 title = {C/C++ Thread Safety Analysis} author = {DeLesley Hutchins and Aaron Ballman and Dean Sutherland} year = 2014 booktitle = {2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation} },Writing multithreaded programs is hard. Static analysis tools can help developers by allowing threading policies to be formally specified and mechanically checked. They essentially provide a static type system for threads and can detect potential race conditions and deadlocks. This paper describes Clang Thread Safety Analysis a tool which uses annotations to declare and enforce thread safety policies in C and C++ programs. Clang is a production-quality C++ compiler which is available on most platforms and the analysis can be enabled for any build with a simple warning flag: _Wthread_safety. The analysis is deployed on a large scale at Google where it has provided sufficient value in practice to drive widespread voluntary adoption. Contrary to popular belief the need for annotations has not been a liability and even confers some benefits with respect to software evolution and maintenance.,http://research.google.com/pubs/archive/42958.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=C/C%2B%2B+Thread+Safety+Analysis+Hutchins+Ballman+Sutherland,http://research.google.com/pubs/pub42958.html
Statistical Machine Translation for Query Expansion in Answer Retrieval,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL'07) Prague Czech Republic (2007),2007,Stefan Riezler Alexander Vasserman Ioannis Tsochantaridis Vibhu Mittal Yi Liu,@inproceedings{32707 title = {Statistical Machine Translation for Query Expansion in Answer Retrieval} author = {Stefan Riezler and Alexander Vasserman and Ioannis Tsochantaridis and Vibhu Mittal and Yi Liu} year = 2007 URL = {http://www.stefanriezler.com/PAPERS/ACL07.pdf} booktitle = {Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL'07)} address = {Prague Czech Republic} },This paper presents a novel approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is performed on the one hand by using a SMT-based full-sentence paraphraser to introduce synonyms in the context the full query and on the other hand by training an SMT model on question-answer pairs and expanding queries by answer terms taken from translations of full queries. We compare these global context-aware query expansion techniques with a baseline tfidf model and local query expansion on a database of 10 million question-answer pairs extracted from FAQ pages. Experimental results show a significant improvement of SMT-based query expansion over both baselines.,http://www.stefanriezler.com/PAPERS/ACL07.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Statistical+Machine+Translation+for+Query+Expansion+in+Answer+Retrieval+Riezler+Vasserman+Tsochantaridis+Mittal+Liu,http://research.google.com/pubs/pub32707.html
Atoms Bits and Cells,Applied and Translational Genomics (2015),2015,David Glazer,@article{43885 title = {Atoms Bits and Cells} author = {David Glazer} year = 2015 URL = {http://www.sciencedirect.com/science/article/pii/S2212066115300314} journal = {Applied and Translational Genomics} },Biology is entering the world of data. Data brings great potential for understanding all of us and improving the health of each of us. But making sense of data also brings challenges. To realize the potential we must build on advances in data science and learn from the experiences of other fields.,http://www.sciencedirect.com/science/article/pii/S2212066115300314,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Atoms+Bits+and+Cells+Glazer,http://research.google.com/pubs/pub43885.html
Borg Omega and Kubernetes,ACM Queue vol. 14 (2016) pp. 70-93,2016,Brendan Burns Brian Grant David Oppenheimer Eric Brewer John Wilkes,@article{44843 title = {Borg Omega and Kubernetes} author = {Brendan Burns and Brian Grant and David Oppenheimer and Eric Brewer and John Wilkes} year = 2016 URL = {http://queue.acm.org/detail.cfm?id=2898444} journal = {ACM Queue} pages = {70--93} volume = {14} },Lessons learned from three container management systems over a decade.,http://research.google.com/pubs/archive/44843.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Borg+Omega+and+++Kubernetes+Burns+Grant+Oppenheimer+Brewer+Wilkes,http://research.google.com/pubs/pub44843.html
Logical Leaps and Quantum Connectives: Forging Paths through Predication Space,AAAI-Fall 2010 Symposium on Quantum Informatics for Cognitive Social and Semantic Processes. (to appear),2010,Trevor Cohen Dominic Widdows Roger W. Schvaneveldt Thomas C. Rindflesch,@inproceedings{36657 title = {Logical Leaps and Quantum Connectives: Forging Paths through Predication Space} author = {Trevor Cohen and Dominic Widdows and Roger W. Schvaneveldt and Thomas C. Rindflesch} year = 2010 booktitle = {AAAI-Fall 2010 Symposium on Quantum Informatics for Cognitive Social and Semantic Processes.} },The Predication-based Semantic Indexing (PSI) approach encodes both symbolic and distributional information into a semantic space using a permutation-based variant of Random Indexing. In this paper we develop and evaluate a computational model of abductive reasoning based on PSI. Using distributional information we identify pairs of concepts that are likely to be predicated about a common third concept or middle term. As this occurs without the explicit identification of the middle term concerned we refer to this process as a “logical leap”. Subsequently we use further operations in the PSI space to retrieve this middle term and identify the predicate types involved. On evaluation using a set of 1000 randomly selected cue concepts the model is shown to retrieve with accuracy concepts that can be connected to a cue concept by a middle term as well as the middle term concerned using nearest-neighbor search in the PSI space. The utility of quantum logical operators as a means to identify alternative paths through this space is also explored.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Logical+Leaps+and+Quantum+Connectives:+Forging+Paths+through+Predication+Space+Cohen+Widdows+Schvaneveldt+Rindflesch,http://research.google.com/pubs/pub36657.html
Does Bug Prediction Support Human Developers? Findings from a Google Case Study,International Conference on Software Engineering (ICSE) (2013),2013,Chris Lewis Zhongpeng Lin Caitlin Sadowski Xiaoyan Zhu Rong Ou E. James Whitehead Jr.,@inproceedings{41145 title = {Does Bug Prediction Support Human Developers? Findings from a Google Case Study} author = {Chris Lewis and Zhongpeng Lin and Caitlin Sadowski and Xiaoyan Zhu and Rong Ou and E. James Whitehead Jr.} year = 2013 booktitle = {International Conference on Software Engineering (ICSE)} },While many bug prediction algorithms have been developed by academia they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google and found no identifiable change in developer behavior. Using our experience we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.,http://research.google.com/pubs/archive/41145.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Does+Bug+Prediction+Support+Human+Developers%3F+Findings+from+a+Google+Case+Study+Lewis+Lin+Sadowski+Zhu+Ou+Jr.,http://research.google.com/pubs/pub41145.html
Filtering: a method for solving graph problems in MapReduce.,SPAA 2011: Proceedings of the 23rd Annual ACM Symposium on Parallelism in Algorithms and Architectures pp. 85-94,2011,Silvio Lattanzi Benjamin Moseley Siddharth Suri Sergei Vassilvitskii,@inproceedings{37240 title = {Filtering: a method for solving graph problems in MapReduce.} author = {Silvio Lattanzi and Benjamin Moseley and Siddharth Suri and Sergei Vassilvitskii} year = 2011 booktitle = {SPAA 2011: Proceedings of the 23rd Annual ACM Symposium on Parallelism in Algorithms and Architectures} pages = {85-94} },The MapReduce framework is currently the de facto standard used throughout both industry and academia for petabyte scale data analysis. As the input to a typical MapReduce computation is large one of the key requirements of the framework is that the input cannot be stored on a single machine and must be processed in parallel. In this paper we describe a general algorithmic design technique in the MapReduce framework called filtering. The main idea behind filtering is to reduce the size of the input in a distributed fashion so that the resulting much smaller problem instance can be solved on a single machine. Using this approach we give new algorithms in the MapReduce framework for a variety of fundamental graph problems. Specifically we present algorithms for minimum spanning trees maximal matchings approximate weighted matchings approximate vertex and edge covers and minimum cuts. In all of these cases we will parameterize our algorithms by the amount of memory available on the machines allowing us to show tradeoffs between the memory available and the number of MapReduce rounds. For each setting we will show that even if the machines are only given substantially sublinear memory our algorithms run in a constant number of MapReduce rounds. To demonstrate the practical viability of our algorithms we implement the maximal matching algorithm that lies at the core of our analysis and show that it achieves a significant speedup over the sequential version.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Filtering:+a+method+for+solving+graph+problems+in+MapReduce.+Lattanzi+Moseley+Suri+Vassilvitskii,http://research.google.com/pubs/pub37240.html
Autonomous spectrum balancing for digital subscriber lines,IEEE Transactions on Signal Processing vol. 8 (2007) pp. 4241-4257,2007,Raphael Cendrillon Jianwei Huang Mung Chiang Marc Moonen,@article{40748 title = {Autonomous spectrum balancing for digital subscriber lines} author = {Raphael Cendrillon and Jianwei Huang and Mung Chiang and Marc Moonen} year = 2007 URL = {http://espace.library.uq.edu.au/eserv.php?pid=UQ:7857&dsID=asb_journal.pdf} journal = {IEEE Transactions on Signal Processing} pages = {4241-4257} volume = {8} },"The main performance bottleneck of modern digital subscriber line (DSL) networks is the crosstalk among different lines (i.e. users). By deploying dynamic spectrum management (DSM) techniques and reducing excess crosstalk among users a network operator can dramatically increase the data rates and service reach of broadband access. However current DSM algorithms suffer from either substantial suboptimality in typical deployment scenarios or prohibitively high complexity due to centralized computation. This paper develops analyzes and simulates a new suite of DSM algorithms for DSL interference-channel models called autonomous spectrum balancing (ASB). The ASB algorithms utilize the concept of a ""reference line"" which mimics a typical victim line in the interference channel. In ASB each modem tries to minimize the harm it causes to the reference line under the constraint of achieving its own target data-rate. Since the reference line is based on the statistics of the entire network rather than any specific knowledge of the binder a modem operates in ASB can be implemented autonomously without the need for a centralized spectrum management center. ASB has a low complexity and simulations using a realistic simulator show that it achieves large performance gains over existing autonomous algorithms coming close to the optimal rate region in some typical scenarios. Sufficient conditions for convergence of ASB are also proved.",http://research.google.com/pubs/archive/40748.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Autonomous+spectrum+balancing+for+digital+subscriber+lines+Cendrillon+Huang+Chiang+Moonen,http://research.google.com/pubs/pub40748.html
Why Silent Updates Boost Security,ETH Zurich (2009) pp. 1-9,2009,Thomas Duebendorfer Stefan Frei,@techreport{35246 title = {Why Silent Updates Boost Security} author = {Thomas Duebendorfer and Stefan Frei} year = 2009 URL = {http://www.techzoom.net/publications/silent-updates/} institution = {ETH Zurich} },Security fixes and feature improvements don't benefit the end user of software if the update mechanism and strategy is not effective. In this paper we analyze the effectiveness of different Web browsers update mechanisms; from Chrome's silent update mechanism to Opera's update requiring a full re-installation. We use anonymized logs from Google's world wide distributed Web servers. An analysis of the logged HTTP user-agent string that Web browsers report when requesting any Web page is used to measure the daily browser version shares in active use. To the best of our knowledge this is the first global scale measurement of Web browser update effectiveness comparing four different Web browser update strategies. Our measurements prove that silent updates and little dependency on the underlying operating system are most effective to get users of Web browsers to surf the Web with the latest browser version. However there is still room for improvement as we found. Chrome's advantageous silent update mechanism has been open sourced in April 2009. We recommend any software vendor to seriously consider deploying silent updates as this benefits both the vendor and the user especially for widely used attack-exposed applications like Web browsers and browser plug-ins.,http://www.techzoom.net/publications/silent-updates/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Why+Silent+Updates+Boost+Security+Duebendorfer+Frei,http://research.google.com/pubs/pub35246.html
Verifying Cloud Services: Present and Future,Operating Systems Review (2013),2013,Sara Bouchenak Gregory Chockler Hana Chockler Gabriela Gheorghe Nuno Santos Alexander Shraer,@article{40816 title = {Verifying Cloud Services: Present and Future} author = {Sara Bouchenak and Gregory Chockler and Hana Chockler and Gabriela Gheorghe and Nuno Santos and Alexander Shraer} year = 2013 journal = {Operating Systems Review} },As cloud-based services gain popularity in both private and enterprise domains cloud consumers are still lacking in tools to verify that these services work as expected. Such tools should consider properties such as functional correctness service availability reliability performance and security guar- antees. In this paper we survey existing work in these ar- eas and identify gaps in existing cloud technology in terms of the verication tools provided to users. We also discuss challenges and new research directions that can help bridge these gaps.,http://research.google.com/pubs/archive/40816.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Verifying+Cloud+Services:+Present+and+Future+Bouchenak+Chockler+Chockler+Gheorghe+Santos+Shraer,http://research.google.com/pubs/pub40816.html
Google's Cross-Dialect Arabic Voice Search,IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP 2012) pp. 4441-4444,2012,Fadi Biadsy Pedro J. Moreno Martin Jansche,@inproceedings{38079 title = {Google's Cross-Dialect Arabic Voice Search} author = {Fadi Biadsy and Pedro J. Moreno and Martin Jansche} year = 2012 URL = {http://www.icassp2012.com/Papers/ViewPapers_MS.asp?PaperNum=2416} booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP 2012)} pages = {4441--4444} },We present a large scale effort to build a commercial Automatic Speech Recognition (ASR) product for Arabic. Our goal is to support voice search dictation and voice control for the general Arabic-speaking public including support for multiple Arabic dialects. We describe our ASR system design and compare recognizers for five Arabic dialects with the potential to reach more than 125 million people in Egypt Jordan Lebanon Saudi Arabia and the United Arab Emirates (UAE). We compare systems built on diacritized vs. non-diacritized text. We also conduct cross-dialect experiments where we train on one dialect and test on the others. Our average word error rate (WER) is 24.8% for voice search.,http://research.google.com/pubs/archive/38079.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Google's+Cross-Dialect+Arabic+Voice+Search+Biadsy+Moreno+Jansche,http://research.google.com/pubs/pub38079.html
Large Scale Distributed Deep Networks,NIPS (2012),2012,Jeffrey Dean Greg S. Corrado Rajat Monga Kai Chen Matthieu Devin Quoc V. Le Mark Z. Mao Marc’Aurelio Ranzato Andrew Senior Paul Tucker Ke Yang Andrew Y. Ng,@inproceedings{40565 title = {Large Scale Distributed Deep Networks} author = {Jeffrey Dean and Greg S. Corrado and Rajat Monga and Kai Chen and Matthieu Devin and Quoc V. Le and Mark Z. Mao and Marc’Aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Andrew Y. Ng} year = 2012 booktitle = {NIPS} },Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework we have developed two algorithms for large-scale distributed training: (i) Downpour SGD an asynchronous stochastic gradient descent procedure supporting a large number of model replicas and (ii) Sandblaster a framework that supports a variety of distributed batch optimization procedures including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature and achieves state-of-the-art performance on ImageNet a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks the underlying algorithms are applicable to any gradient-based machine learning algorithm.,http://research.google.com/pubs/archive/40565.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large+Scale+Distributed+Deep+Networks+Dean+Corrado+Monga+Chen+Devin+Le+Mao+Ranzato+Senior+Tucker+Yang+Ng,http://research.google.com/pubs/pub40565.html
Helping You Protect You,IEEE (2014) pp. 39-42,2014,M. Angela Sasse Charles C. Palmer Markus Jakobsson Sunny Consolvo Rick Wash L. Jean Camp,@misc{43406 title = {Helping You Protect You} author = {M. Angela Sasse and Charles C. Palmer and Markus Jakobsson and Sunny Consolvo and Rick Wash and L. Jean Camp} year = 2014 },Guest editors M. Angela Sasse and Charles C. Palmer speak with security practitioners (L. Jean Camp Sunny Consolvo Markus Jakobsson and Rick Wash) about what companies are doing to keep customers secure and what users can do to stay safe.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Helping+You+Protect+You+Sasse+Palmer+Jakobsson+Consolvo+Wash+Camp,http://research.google.com/pubs/pub43406.html
RFC 6937 - Proportional Rate Reduction for TCP,Internet Engineering Task Force (IETF) (2013),2013,Matt Mathis Nandita Dukkipati Yuchung Cheng,@misc{41429 title = {RFC 6937 - Proportional Rate Reduction for TCP} author = {Matt Mathis and Nandita Dukkipati and Yuchung Cheng} year = 2013 URL = {http://www.rfc-editor.org/rfc/rfc6937.txt} },This document describes an experimental Proportional Rate Reduction (PRR) algorithm as an alternative to the widely deployed Fast Recovery and Rate-Halving algorithms. These algorithms determine the amount of data sent by TCP during loss recovery. PRR minimizes excess window adjustments and the actual window size at the end of recovery will be as close as possible to the ssthresh as determined by the congestion control algorithm.,http://research.google.com/pubs/archive/41429.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC+6937+-+Proportional+Rate+Reduction+for+TCP+Mathis+Dukkipati+Cheng,http://research.google.com/pubs/pub41429.html
Wikipedia Tools for Google Spreadsheets,Wiki Workshop @ WWW (2016),2016,Thomas Steiner,@inproceedings{44817 title = {Wikipedia Tools for Google Spreadsheets} author = {Thomas Steiner} year = 2016 booktitle = {Wiki Workshop @ WWW} },In this paper we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online while collaborating with other users in realtime. Wikipedia is a free-access free-content Internet encyclopedia whose content and data is available among other means through an API. With the Wikipedia Tools for Google Spreadsheets we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets] released under the permissive Apache 2.0 license.,http://research.google.com/pubs/archive/44817.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Wikipedia+Tools+for+Google+Spreadsheets+Steiner,http://research.google.com/pubs/pub44817.html
A Systematic Comparison of Phrase Table Pruning Techniques,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning Association for Computational Linguistics Jeju Island Korea pp. 972-983,2012,Richard Zens Daisy Stanton Peng Xu,@inproceedings{38279 title = {A Systematic Comparison of Phrase Table Pruning Techniques} author = {Richard Zens and Daisy Stanton and Peng Xu} year = 2012 booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning} pages = {972--983} address = {Jeju Island Korea} },When trained on very large parallel corpora the phrase table component of a machine translation system grows to consume vast computational resources. In this paper we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.,http://research.google.com/pubs/archive/38279.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Systematic+Comparison+of+Phrase+Table+Pruning+Techniques+Zens+Stanton+Xu,http://research.google.com/pubs/pub38279.html
Distinct counting with a self-learning bitmap,Journal of American Statistical Association vol. 106 (2011) 879–890,2011,Aiyou Chen Jin Cao Larry Shepp Tuan Nguyen,@article{37169 title = {Distinct counting with a self-learning bitmap} author = {Aiyou Chen and Jin Cao and Larry Shepp and Tuan Nguyen} year = 2011 journal = {Journal of American Statistical Association} pages = {879–890} volume = {106} },Counting the number of distinct elements (cardinality) in a dataset is a fundamental problem in database management. In recent years due to many of its modern applications there has been signiﬁcant interest to address the distinct counting problem in a data stream setting where each incoming data can be seen only once and cannot be stored for long periods of time. Many probabilistic approaches based on either sampling or sketching have been proposed in the computer science literature that only require limited computing and memory resources. However the performances of these methods are not scale invariant in the sense that their relative root mean square estimation errors (RRMSE) depend on the unknown cardinalities. This is not desirable in many applications where cardinalities can be very dynamic or inhomogeneous and many cardinalities need to be estimated. In this article we develop a novel approach called self-learning bitmap (S-bitmap) that is scale invariant for cardinalities in a speciﬁed range. S-bitmap uses a binary vector whose entries are updated from 0 to 1 by an adaptive sampling process for inferring the unknown cardinality where the sampling rates are reduced sequentially as more and more entries change from 0 to 1. We prove rigorously that the S-bitmap estimate is not only unbiased but scale invariant. We demonstrate that to achieve a small RRMSE value of _ or less our approach requires significantly less memory and consumes similar or less operations than state-of-the-art methods for many common practice cardinality scales. Both simulation and experimental studies are reported.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distinct+counting+with+a+self-learning+bitmap+Chen+Cao+Shepp+Nguyen,http://research.google.com/pubs/pub37169.html
Discriminative Segment Annotation in Weakly Labeled Video,Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013),2013,Kevin Tang Rahul Sukthankar Jay Yagnik Li Fei-Fei,@inproceedings{40751 title = {Discriminative Segment Annotation in Weakly Labeled Video} author = {Kevin Tang and Rahul Sukthankar and Jay Yagnik and Li Fei-Fei} year = 2013 booktitle = {Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR 2013)} },paper tackles the problem of segment annotation in complex Internet videos. Given a weakly labeled video we automatically generate spatiotemporal masks for each of the concepts with which it is labeled. This is a particularly relevant problem in the video domain as large numbers of YouTube videos are now available tagged with the visual concepts that they contain. Given such weakly labeled videos we focus on the problem of spatiotemporal segment classification. We propose a straightforward algorithm CRANE that utilizes large amounts of weakly labeled video to rank spatiotemporal segments by the likelihood that they correspond to a given visual concept. We make publicly available segment-level annotations for a subset of the Prest et al. dataset and show convincing results. We also show state-of-the-art results on Hartmann et al.'s more difficult large-scale object segmentation dataset.,http://research.google.com/pubs/archive/40751.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discriminative+Segment+Annotation+in+Weakly+Labeled+Video+Tang+Sukthankar+Yagnik+Fei-Fei,http://research.google.com/pubs/pub40751.html
Precentile-Based Approach to Forecasting Workload Growth,IT Performance and Capacity by CMG 41st International Conference (CMG2015) Computer Measurement Group 3501 Route 42 Suite 130 #121 Turnersville NJ 08012-1734 USA,2015,Alex Gilgur Stephen Gunn Douglas Browning Xiaojun Di Wei Chen Rajesh Krishnaswamy,@inproceedings{44279 title = {Precentile-Based Approach to Forecasting Workload Growth} author = {Alex Gilgur and Stephen Gunn and Douglas Browning and Xiaojun Di and Wei Chen and Rajesh Krishnaswamy} year = 2015 booktitle = {IT Performance and Capacity by CMG 41st International Conference (CMG2015)} address = {3501 Route 42 Suite 130 #121 Turnersville NJ 08012-1734 USA} },When forecasting resource workloads (traffic CPU load memory usage etc.) we often extrapolate from the upper percentiles of data distributions. This works very well when the resource is far enough from its saturation point. However when the resource utilization gets closer to the workload-carrying capacity of the resource upper percentiles level off (the phenomenon is colloquially known as flat-topping or clipping) leading to underpredictions of future workload and potentially to undersized resources. This paper explains the phenomenon and proposes a new approach that can be used for making useful forecasts of workload when historical data for the forecast are collected from a resource approaching saturation.,http://research.google.com/pubs/archive/44279.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Precentile-Based+Approach+to+Forecasting+Workload+Growth+Gilgur+Gunn+Browning+Di+Chen+Krishnaswamy,http://research.google.com/pubs/pub44279.html
Reading the Markets: Forecasting Public Opinion of Political Candidates by News Analysis,Conference on Computational Linguistics (Coling) (2008),2008,Kevin Lerman Ari Gilder Mark Dredze Fernando Pereira,@inproceedings{34666 title = {Reading the Markets: Forecasting Public Opinion of Political Candidates by News Analysis} author = {Kevin Lerman and Ari Gilder and Mark Dredze and Fernando Pereira} year = 2008 booktitle = {Conference on Computational Linguistics (Coling)} },Media reporting shapes public opinion which can in turn influence events particularly in political elections in which candidates both respond to and shape public perception of their campaigns. We use computational linguistics to automatically predict the impact of news on public perception of political candidates. Our system uses daily newspaper articles to predict shifts in public opinion as reflected in prediction markets. We discuss various types of features designed for this problem. The news system improves market prediction over baseline market systems.,http://research.google.com/pubs/archive/34666.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Reading+the+Markets:+Forecasting+Public+Opinion+of+Political+Candidates+by+News+Analysis+Lerman+Gilder+Dredze+Pereira,http://research.google.com/pubs/pub34666.html
Machine Learning Applications for Data Center Optimization,Google (2014),2014,Jim Gao,@misc{42542 title = {Machine Learning Applications for Data Center Optimization} author = {Jim Gao} year = 2014 },The modern data center (DC) is a complex interaction of multiple mechanical electrical and controls systems. The sheer number of possible operating configurations and nonlinear interdependencies make it difficult to understand and optimize energy efficiency. We develop a neural network framework that learns from actual operations data to model plant performance and predict PUE within a range of 0.004 +/0.005 (mean absolute error +/- 1 standard deviation) or 0.4% error for a PUE of 1.1. The model has been extensively tested and validated at Google DCs. The results demonstrate that machine learning is an effective way of leveraging existing sensor data to model DC performance and improve energy efficiency.,http://research.google.com/pubs/archive/42542.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+Learning+Applications+for+Data+Center+Optimization+Gao,http://research.google.com/pubs/pub42542.html
Translating Queries into Snippets for Improved Query Expansion,Proceedings of the 22nd International Conference on Computational Linguistics (COLING'08) Manchester England (2008),2008,Stefan Riezler Yi Liu Alexander Vasserman,@inproceedings{34382 title = {Translating Queries into Snippets for Improved Query Expansion} author = {Stefan Riezler and Yi Liu and Alexander Vasserman} year = 2008 URL = {http://www.stefanriezler.com/PAPERS/COLING08.pdf} booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (COLING'08)} address = {Manchester England} },User logs of search engines have recently been applied successfully to improve various aspects of web search quality. In this paper we will apply pairs of user queries and snippets of clicked results to train a machine translation model to bridge the ``lexical gap'' between query and document space. We show that the combination of a query-to-snippet translation model with a large n-gram language model trained on queries achieves improved contextual query expansion compared to a system based on term correlations.,http://research.google.com/pubs/archive/34382.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Translating+Queries+into+Snippets+for+Improved+Query+Expansion+Riezler+Liu+Vasserman,http://research.google.com/pubs/pub34382.html
A Guided Tour of Datacenter Networking,Communications of the ACM - ACM Queue vol. 55 number 6 (2012) pp. 44-51,2012,Dennis Abts Bob Felderman,@article{40404 title = {A Guided Tour of Datacenter Networking} author = {Dennis Abts and Bob Felderman} year = 2012 URL = {http://dx.doi.org/10.1145/2184319.2184335} note = {DOI:10.1145/2184319.2184335} journal = {Communications of the ACM -- ACM Queue} pages = {44--51} volume = {55 number 6} },The magic of the cloud is that it is always on and always available from anywhere. Users have come to expect that services are there when they need them. A data center (or warehouse-scale computer) is the nexus from which all the services flow. It is often housed in a nondescript warehouse-sized building bearing no indication of what lies inside. Amidst the whirring fans and refrigerator-sized computer racks is a tapestry of electrical cables and fiber optics weaving everything together—the data-center network. This article provides a “guided tour” through the principles and central ideas surrounding the network at the heart of a data center—the modern-day loom that weaves the digital fabric of the Internet.,http://research.google.com/pubs/archive/40404.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Guided+Tour+of+Datacenter+Networking+Abts+Felderman,http://research.google.com/pubs/pub40404.html
Rogue Femtocell Owners: How Mallory Can Monitor My Devices,2013 Proceedings IEEE INFOCOM IEEE New Jersey USA pp. 3553-3558,2013,David Malone Darren F Kavanagh Niall Richard Murphy,@inproceedings{41331 title = {Rogue Femtocell Owners: How Mallory Can Monitor My Devices} author = {David Malone and Darren F Kavanagh and Niall Richard Murphy} year = 2013 booktitle = {2013 Proceedings IEEE INFOCOM} pages = {3553-3558} address = {New Jersey USA} },Femtocells are small cellular telecommunication base stations that provide improved cellular coverage. These devices provide important improvements in coverage battery life and throughput they also present security challenges. We identify a problem which has not been identified in previous studies of femtocell security: rogue owners of femtocells can secretly monitor third-party mobile devices by using the femtocell's access control features. We present traffic analysis of real femtocell traces are presented and demonstrate the ability to monitor mobile devices through classification of the femtocell's encrypted backhaul traffic. We also consider the femtocell's power usage and status LEDs as other side channels that provide information on the femtocell's operation. We conclude by presenting suitable solutions to overcome this problem.,http://research.google.com/pubs/archive/41331.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Rogue+Femtocell+Owners:+How+Mallory+Can+Monitor+My+Devices+Malone+Kavanagh+Murphy,http://research.google.com/pubs/pub41331.html
An estimation-theoretic approach to video denoising,2015 IEEE International Conference on Image Processing IEEE pp. 4273-4277,2015,Jingning Han Timothy Kopp Yaowu Xu,@inproceedings{44680 title = {An estimation-theoretic approach to video denoising} author = {Jingning Han and Timothy Kopp and Yaowu Xu} year = 2015 booktitle = {2015 IEEE International Conference on Image Processing} pages = {4273-4277} },A novel denoising scheme is proposed to fully exploit the spatio-temporal correlations of the video signal for efficient enhancement. Unlike conventional pixel domain approaches that directly connect motion compensated reference pixels and spatially neighboring pixels to build statistical models for noise filtering this work first removes spatial correlations by applying transformations to both pixel blocks and performs estimation in the frequency domain. It is premised on the realization that the precise nature of temporal dependencies which is entirely masked in the pixel domain by the statistics of the dominant low frequency components emerges after signal decomposition and varies considerably across the spectrum. We derive an optimal non-linear estimator that accounts for both motion compensated reference and the noisy observations to resemble the original video signal per transform coefficient. It departs from other transform domain approaches that employ linear filters over a sizable reference set to reduce the uncertainty due to the random noise term. Instead it jointly exploits this precise statistical property appeared in the transform domain and the noise probability model in an estimation-theoretic framework that works on a compact support region. Experimental results provide evidence for substantial denoising performance improvement.,http://research.google.com/pubs/archive/44680.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+estimation-theoretic+approach+to+video+denoising+Han+Kopp+Xu,http://research.google.com/pubs/pub44680.html
Fast Bilateral-Space Stereo for Synthetic Defocus,CVPR (2015),2015,Jonathan T Barron Andrew Adams YiChang Shih Carlos Hernández,@inproceedings{44003 title = {Fast Bilateral-Space Stereo for Synthetic Defocus} author = {Jonathan T Barron and Andrew Adams and YiChang Shih and Carlos Hernández} year = 2015 booktitle = {CVPR} },"Given a stereo pair it is possible to recover a depth map and use that depth to render a synthetically defocused image. Though stereo algorithms are well-studied rarely are those algorithms considered solely in the context of producing these defocused renderings. In this paper we present a technique for efficiently producing disparity maps using a novel optimization framework in which inference is performed in ""bilateral-space"". Our approach produces higher-quality ""defocus"" results than other stereo algorithms while also being 10-100 times faster than comparable techniques.",http://research.google.com/pubs/archive/44003.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+Bilateral-Space+Stereo+for+Synthetic+Defocus+Barron+Adams+Shih+Hernandez+Esteban,http://research.google.com/pubs/pub44003.html
Models for Neural Spike Computation and Cognition,CreateSpace Seattle WA (2011) pp. 142,2011,David H. Staelin Carl H. Staelin,@book{37538 title = {Models for Neural Spike Computation and Cognition} author = {David H. Staelin and Carl H. Staelin} year = 2011 URL = {http://cognon.net/Neural_Spike_Computation.pdf} pages = {142} address = {Seattle WA} },This monograph addresses the intertwined mathematical neurological and cognitive mysteries of the brain. It first evaluates the mathematical performance limits of simple spiking neuron models that both learn and later recognize complex spike excitation patterns in less than one second without using training signals unique to each pattern. Simulations validate these models while theoretical expressions validate their simpler performance parameters. These single-neuron models are then qualitatively related to the training and performance of multi-layer neural networks that may have significant feedback. The advantages of feedback are then qualitatively explained and related to a model for cognition. This model is then compared to observed mild hallucinations that arguably include accelerated time-reversed video memories. The learning mechanism for these binary threshold-firing “cognon” neurons is spike-timing-dependent plasticity (STDP) that depends only on whether the spike excitation pattern presented to a given single “learning-ready” neuron within a period of milliseconds causes that neuron to fire or “spike”. The “false-alarm” probability that a trained neuron will fire for a random unlearned pattern can be made almost arbitrarily low by reducing the number of patterns learned by each neuron. Models that use and that do not use spike timing within patterns are evaluated. A Shannon mutual information metric (recoverable bits/neuron) is derived for binary neuron models that are characterized only by their probability of learning a random input excitation pattern presented to that neuron during learning readiness and by their false-alarm probability for random unlearned patterns. Based on simulations the upper bounds to recoverable information are ~0.1 bits per neuron for optimized neuron parameters and training. This information metric assumes that: 1) each neural spike indicates only that the responsible neuron input excitation pattern (a pattern lasts less than the time between consecutive patterns say 30 milliseconds) had probably been seen earlier while that neuron was “learning ready” and 2) information is stored in the binary synapse strengths. This focus on recallable learned information differs from most prior metrics such as pattern classification performance and metrics relying on pattern-specific training signals other than the normal input spikes. This metric also shows that neuron models can recall useful Shannon information only if their probability of firing randomly is lowered between learning and recall. Also discussed are: 1) how rich feedback might permit improved noise immunity learning and recognition of pattern sequences compression of data associative or content-addressable memory and development of communications links through white matter 2) extensions of cognon models that use spike timing dendrite compartments and new learning mechanisms in addition to spike timing- dependent plasticity (STDP) 3) simulations that show how simple optimized neuron models can have optimum numbers of binary synapses in the range between 200 and 10000 depending on neural parameters and 4) simulation results for parameters like the average bits/spike bits/neuron/second maximum number of learnable patterns optimum ratios between the strengths of weak and strong synapses and probabilities of false alarms.,http://research.google.com/pubs/archive/37538.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Models+for+Neural+Spike+Computation+and+Cognition+Staelin+Staelin,http://research.google.com/pubs/pub37538.html
Stability Bounds for Stationary $\phi$-mixing and $\beta$-mixing Processes,Journal of Machine Learning Research (JMLR) vol. 11 (2010) pp. 798-814,2010,Mehryar Mohri Afshin Rostamizadeh,@article{36944 title = {Stability Bounds for Stationary $\phi$-mixing and $\beta$-mixing Processes} author = {Mehryar Mohri and Afshin Rostamizadeh} year = 2010 URL = {http://www.cs.nyu.edu/%7Emohri/pub/niidj.pdf} journal = {Journal of Machine Learning Research (JMLR)} pages = {798-814} volume = {11} },Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used independently of any algorithm. In contrast the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However as in much of learning theory existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications however this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence.,http://research.google.com/pubs/archive/36944.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Stability+Bounds+for+Stationary+$%5Cphi$-mixing+and+$%5Cbeta$-mixing+Processes+Mohri+Rostamizadeh,http://research.google.com/pubs/pub36944.html
Fast k-best Sentence Compression,arXiv (2015),2015,Katja Filippova Enrique Alfonseca,@techreport{44820 title = {Fast k-best Sentence Compression} author = {Katja Filippova and Enrique Alfonseca} year = 2015 URL = {http://arxiv.org/abs/1510.08418} },A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately dependence on ILP may make the compressor prohibitively slow and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.,http://research.google.com/pubs/archive/44820.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+k-best+Sentence+Compression+Filippova+Alfonseca,http://research.google.com/pubs/pub44820.html
How to grow more pairs: suggesting review targets for comparison-friendly review ecosystems,WWW (2013) pp. 237-248,2013,James Cook Alex Fabrikant Avinatan Hassidim,@inproceedings{42474 title = {How to grow more pairs: suggesting review targets for comparison-friendly review ecosystems} author = {James Cook and Alex Fabrikant and Avinatan Hassidim} year = 2013 booktitle = {WWW} pages = {237--248} },We consider the algorithmic challenges behind a novel interface that simplifies consumer research of online reviews by surfacing relevant comparable review bundles: reviews for two or more of the items being researched all generated in similar enough circumstances to provide for easy comparison. This can be reviews by the same reviewer or by the same demographic category of reviewer or reviews focusing on the same aspect of the items. But such an interface will work only if the review ecosystem often has comparable review bundles for common research tasks. Here we develop and evaluate practical algorithms for suggesting additional review targets to reviewers to maximize comparable pair coverage the fraction of co-researched pairs of items that have both been reviewed by the same reviewer (or more generally are comparable in one of several ways). We show the exact problem and many subcases to be intractable and give a greedy online linear-time 2-approximation for a very general setting and an offline 1.583-approximation for a narrower setting. We evaluate the algorithms on the Google+ Local reviews dataset yielding more than 10x gain in pair coverage from six months of simulated replacement of existing reviews by suggested reviews. Even allowing for 90% of reviewers ignoring the suggestions the pair coverage grows more than 2x in the simulation. To explore other parts of the parameter space we also evaluate the algorithms on synthetic models.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+to+grow+more+pairs:+suggesting+review+targets+for+comparison-friendly+review+ecosystems+Cook+Fabrikant+Hassidim,http://research.google.com/pubs/pub42474.html
HMM-based script identification for OCR,Proceedings of the 4th International Workshop on Multilingual OCR ACM New York NY US (2013) 2:1-2:5,2013,Dmitriy Genzel Ashok Popat Remco Teunen Yasuhisa Fujii,@inproceedings{43996 title = {HMM-based script identification for OCR} author = {Dmitriy Genzel and Ashok Popat and Remco Teunen and Yasuhisa Fujii} year = 2013 booktitle = {Proceedings of the 4th International Workshop on Multilingual OCR} pages = {2:1--2:5} address = {New York NY US} },While current OCR systems are able to recognize text in an increasing number of scripts and languages typically they still need to be told in advance what those scripts and languages are. We propose an approach that repurposes the same HMM-based system used for OCR to the task of script/language ID by replacing character labels with script class labels. We apply it in a multi-pass overall OCR process which achieves “universal” OCR over 54 tested languages in 18 distinct scripts over a wide variety of typefaces in each. For comparison we also consider a brute-force approach wherein a singe HMM-based OCR system is trained to recognize all considered scripts. Results are presented on a large and diverse evaluation set extracted from book images both for script identification accuracy and for overall OCR accuracy. On this evaluation data the script ID system provided a script ID error rate of 1.73% for 18 distinct scripts. The end-to-end OCR system with the script ID system achieved a character error rate of 4.05% an increase of 0.77% over the case where the languages are known a priori.,http://research.google.com/pubs/archive/43996.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=HMM-based+script+identification+for+OCR+Genzel+Popat+Teunen+Fujii,http://research.google.com/pubs/pub43996.html
CrowdForge: Crowdsourcing Complex Work,Proceedings of UIST 2011 Santa Barbara CA,2011,Aniket Kittur Boris Smus Susheel Khamkar Robert Kraut,@inproceedings{39980 title = {CrowdForge: Crowdsourcing Complex Work} author = {Aniket Kittur and Boris Smus and Susheel Khamkar and Robert Kraut} year = 2011 URL = {http://crowdforge.com} booktitle = {Proceedings of UIST 2011} address = {Santa Barbara CA} },Micro-task markets such as Amazon’s Mechanical Turk represent a new paradigm for accomplishing work in which employers can tap into a large population of workers around the globe to accomplish tasks in a fraction of the time and money of more traditional methods. However such markets have been primarily used for simple independent tasks such as labeling an image or judging the relevance of a search result. Here we present a general purpose framework for accomplishing complex and interdependent tasks using micro-task markets. We describe our framework a web-based prototype and case studies on article writing decision making and science journalism that demonstrate the benefits and limitations of the approach.,http://research.google.com/pubs/archive/39980.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=CrowdForge:+Crowdsourcing+Complex+Work+Kittur+Smus+Khamkar+Kraut,http://research.google.com/pubs/pub39980.html
Scaling Distributed Machine Learning with the Parameter Server,Operating Systems Design and Implementation (OSDI) USENIX (2014) pp. 583-598,2014,Mu Li David G. Anderson Jun Woo Park Alexander J. Smola Amr Ahmed Vanja Josifovski James Long Eugene J. Shekita Bor-Yiing Su,@inproceedings{44634 title = {Scaling Distributed Machine Learning with the Parameter Server} author = {Mu Li and David G. Anderson and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su} year = 2014 booktitle = {Operating Systems Design and Implementation (OSDI)} pages = {583--598} },We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes while the server nodes maintain globally shared parameters represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes and supports flexible consistency models elastic scalability and continuous fault tolerance. To demonstrate the scalability of the proposed framework we show experimental results on petabytes of real data with billions of examples and parameters on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scaling+Distributed+Machine+Learning+with+the+Parameter+Server+Li+Anderson+Park+Smola+Ahmed+Josifovski+Long+Shekita+Su,http://research.google.com/pubs/pub44634.html
Language-Independent Discriminative Parsing of Temporal Expressions,The 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013) (to appear),2013,Gabor Angeli Jakob Uszkoreit,@inproceedings{41188 title = {Language-Independent Discriminative Parsing of Temporal Expressions} author = {Gabor Angeli and Jakob Uszkoreit} year = 2013 booktitle = {The 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)} },Temporal resolution systems are traditionally tuned to a particular language requiring significant human effort to translate them to new languages. We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach without lexical cues or language-specific tuning. We achieve state-of-the-art accuracy on all languages in the TempEval-2 temporal normalization task reporting a 4% improvement in both English and Spanish accuracy and to our knowledge the first results for four other languages.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Language-Independent+Discriminative+Parsing+of+Temporal+Expressions+Angeli+Uszkoreit,http://research.google.com/pubs/pub41188.html
Semantic Segmentation Using Regions and Parts,Computer Vision and Pattern Recognition IEEE Computer Society Washington DC USA (2012) pp. 3378-3385,2012,Pablo Arbelaez Bharath Hariharan Chunhui Gu Saurabh Gupta Lubomir Bourdev Jitendra Malik,@inproceedings{40666 title = {Semantic Segmentation Using Regions and Parts} author = {Pablo Arbelaez and Bharath Hariharan and Chunhui Gu and Saurabh Gupta and Lubomir Bourdev and Jitendra Malik} year = 2012 URL = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6248077} booktitle = {Computer Vision and Pattern Recognition} pages = {3378--3385} },We address the problem of segmenting and recognizing objects in real world images focusing on challenging articulated categories such as humans and other animals. For this purpose we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge and report competitive performance with respect to current leading techniques. On VOC2010 our method obtains the best results in 6/20 categories and the highest performance on articulated objects.,http://research.google.com/pubs/archive/40666.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Segmentation+Using+Regions+and+Parts+Arbelaez+Hariharan+Gu+Gupta+Bourdev+Malik,http://research.google.com/pubs/pub40666.html
Sound Retrieval and Ranking Using Sparse Auditory Representations,Neural Computation vol. 22 (2010) pp. 2390-2416,2010,Richard F Lyon Martin Rehn Samy Bengio Thomas C. Walters Gal Chechik,@article{35479 title = {Sound Retrieval and Ranking Using Sparse Auditory Representations} author = {Richard F Lyon and Martin Rehn and Samy Bengio and Thomas C. Walters and Gal Chechik} year = 2010 URL = {http://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00011} journal = {Neural Computation} pages = {2390--2416} volume = {22} },To create systems that understand the sounds that humans are exposed to in everyday life we need to represent sounds with features that can discriminate among many different sound classes. Here we use a sound-ranking framework to quantitatively evaluate such representations in a large scale task. We have adapted a machine-vision method the ``passive-aggressive model for image retrieval'' (PAMIR) which efficiently learns a linear mapping from a very large sparse feature space to a large query-term space. Using this approach we compare different auditory front ends and different ways of extracting sparse features from high-dimensional auditory images. We tested auditory models that use adaptive pole--zero filter cascade (PZFC) auditory filterbank and sparse-code feature extraction from stabilized auditory images via multiple vector quantizers. In addition to auditory image models we also compare a family of more conventional Mel-Frequency Cepstral Coefficient (MFCC) front ends. The experimental results show a significant advantage for the auditory models over vector-quantized MFCCs. Ranking thousands of sound files with a query vocabulary of thousands of words the best precision at top-1 was 73% and the average precision was 35% reflecting a 18% improvement over the best competing MFCC.,http://research.google.com/pubs/archive/35479.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sound+Retrieval+and+Ranking+Using+Sparse+Auditory+Representations+Lyon+Rehn+Bengio+Walters+Chechik,http://research.google.com/pubs/pub35479.html
Combining landline and mobile phone samples A dual frame approach,Gesis working paper 2011/13 (2011),2011,Mario Callegaro Oztas Ayhan Siegfried Gabler Sabine Haeder Ana Villar,@techreport{44677 title = {Combining landline and mobile phone samples A dual frame approach} author = {Mario Callegaro and Oztas Ayhan and Siegfried Gabler and Sabine Haeder and Ana Villar} year = 2011 URL = {http://www.gesis.org/fileadmin/upload/forschung/publikationen/gesis_reihen/gesis_arbeitsberichte/WorkingPapers_2011_13.pdf} },More and more households abandon their landline phones and rely solely on cell phones. This implies a challenge for survey researchers: since the cell phone only households are not included in the frames for landline telephone surveys samples based on these frames are in danger to be seriously biased due to undercoverage if respondents who do not have a landline are systematically different from respondents who have a landline. Thus strategies for combining samples from different frames need to be developed. In this paper we give theoretical foundations for a dual frame approach to sampling explain how samples can be optimally allocated from these two frames and describe an empirical application of a survey conducted in Germany that used a dual frame approach.,http://www.gesis.org/fileadmin/upload/forschung/publikationen/gesis_reihen/gesis_arbeitsberichte/WorkingPapers_2011_13.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Combining+landline+and+mobile+phone+samples+A+dual+frame+approach+Callegaro+Ayhan+Gabler+Haeder+Villar,http://research.google.com/pubs/pub44677.html
Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks,ICLR2014 ICLR2014 (to appear),2014,Ian Goodfellow Yaroslav Bulatov Julian Ibarz Sacha Arnoud Vinay Shet,@inproceedings{42241 title = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks} author = {Ian Goodfellow and Yaroslav Bulatov and Julian Ibarz and Sacha Arnoud and Vinay Shet} year = 2014 booktitle = {ICLR2014} },Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localiza- tion segmentation and recognition steps. In this paper we propose a unified ap- proach that integrates these three steps via the use of a deep convolutional neu- ral network that operates directly on the image pixels. We employ the DistBe- lief (Dean et al. 2012) implementation of deep neural networks in order to train large distributed neural networks on high quality images. We find that the per- formance of this approach increases with the depth of the convolutional network with the best performance occurring in the deepest architecture we trained with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task we improve upon the state-of-the-art and achieve 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. Our evaluations further indicate that at specific operating thresholds the performance of the proposed system is comparable to that of human operators. To date our system has helped us extract close to 100 million physical street numbers from Street View imagery worldwide.,http://research.google.com/pubs/archive/42241.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multi-digit+Number+Recognition+from+Street+View+Imagery+using+Deep+Convolutional+Neural+Networks+Goodfellow+Bulatov+Ibarz+Arnoud+Shet,http://research.google.com/pubs/pub42241.html
Biperpedia: An Ontology for Search Applications,Proc. 40th Int'l Conf. on Very Large Data Bases (PVLDB) (2014),2014,Rahul Gupta Alon Halevy Xuezhi Wang Steven Whang Fei Wu,@inproceedings{41894 title = {Biperpedia: An Ontology for Search Applications} author = {Rahul Gupta and Alon Halevy and Xuezhi Wang and Steven Whang and Fei Wu} year = 2014 booktitle = {Proc. 40th Int'l Conf. on Very Large Data Bases (PVLDB)} },Search engines make significant efforts to recognize queries that can be answered by structured data and invest heavily in creating and maintaining high-precision databases. While these databases have a relatively wide coverage of entities the number of attributes they model (e.g. gdp capital anthem) is relatively small. Extending the number of attributes known to the search engine can enable it to more precisely answer queries from the long and heavy tail extract a broader range of facts from the Web and recover the semantics of tables on the Web. We describe Biperpedia an ontology with 1.6M (class attribute) pairs and 67K distinct attribute names. Biperpedia extracts attributes from the query stream and then uses the best extractions to seed attribute extraction from text. For every attribute Biperpedia saves a set of synonyms and text patterns in which it appears thereby enabling it to recognize the attribute in more contexts. In addition to a detailed analysis of the quality of Biperpedia we show that it can increase the number of Web tables whose semantics we can recover by more than a factor of 4 compared with Freebase.,http://research.google.com/pubs/archive/41894.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Biperpedia:+An+Ontology+for+Search+Applications+Gupta+Halevy+Wang+Whang+Wu,http://research.google.com/pubs/pub41894.html
How Developers Search for Code: A Case Study,Joint Meeting of the European Software Engineering Conference and the Symposium on the Foundations of Software Engineering (ESEC/FSE ) 1600 Amphitheatre Parkway (2015) (to appear),2015,Caitlin Sadowski Kathryn T. Stolee Sebastian Elbaum,@inproceedings{43835 title = {How Developers Search for Code: A Case Study} author = {Caitlin Sadowski and Kathryn T. Stolee and Sebastian Elbaum} year = 2015 booktitle = {Joint Meeting of the European Software Engineering Conference and the Symposium on the Foundations of Software Engineering (ESEC/FSE )} address = {1600 Amphitheatre Parkway} },With the advent of large code repositories and sophisticated search capabilities code search is increasingly becoming a key software development activity. In this work we shed some light into how developers search for code through a case study performed at Google using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when performing a search search scope query properties and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently conducting an average of five search sessions with 12 total queries each workday. The search queries are often targeted at a particular code location and programmers are typically looking for code with which they are somewhat familiar. Further programmers are generally seeking answers to questions about how to use an API what code does why something is failing or where code is located.,http://research.google.com/pubs/archive/43835.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Developers+Search+for+Code:+A+Case+Study+Sadowski+Stolee+Elbaum,http://research.google.com/pubs/pub43835.html
Sequence to Sequence Learning with Neural Networks,Proc. NIPS Montreal CA (2014),2014,Ilya Sutskever Oriol Vinyals Quoc V. Le,@inproceedings{43155 title = {Sequence to Sequence Learning with Neural Networks} author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le} year = 2014 URL = {http://arxiv.org/abs/1409.3215} booktitle = {Proc. NIPS} address = {Montreal CA} },Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available they cannot be used to map sequences to sequences. In this paper we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally the LSTM did not have difficulty on long sentences. For comparison a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system its BLEU score increases to 36.5 which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.,http://arxiv.org/abs/1409.3215,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Sequence+to+Sequence+Learning+with+Neural+Networks+Sutskever+Vinyals+Le,http://research.google.com/pubs/pub43155.html
Context-Dependent Fine-Grained Entity Type Tagging,arXiv.org (2014),2014,Dan Gillick Nevena Lazic Kuzman Ganchev Jesse Kirchner David Huynh,@misc{43240 title = {Context-Dependent Fine-Grained Entity Type Tagging} author = {Dan Gillick and Nevena Lazic and Kuzman Ganchev and Jesse Kirchner and David Huynh} year = 2014 URL = {http://arxiv.org/abs/1412.1820} },Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types recent work (Ling and Weld 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data existing fine-grained tagging systems obtain examples automatically using resolved entities and their types extracted from a knowledge base. However since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government) this procedure can result in spurious labels leading to poorer generalization. We propose the task of context-dependent fine type tagging where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11304 mentions annotated with their context-dependent fine types and we provide baseline experimental results on this data.,http://research.google.com/pubs/archive/43240.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Context-Dependent+Fine-Grained+Entity+Type+Tagging+Gillick+Lazic+Ganchev+Kirchner+Huynh,http://research.google.com/pubs/pub43240.html
Investigations on Exemplar-Based Features for Speech Recognition Towards Thousands of Hours of Unsupervised Noisy Data,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE Kyoto Japan (2012) pp. 4437-4440,2012,Georg Heigold Patrick Nguyen Mitchel Weintraub Vincent Vanhoucke,@inproceedings{37825 title = {Investigations on Exemplar-Based Features for Speech Recognition Towards Thousands of Hours of Unsupervised Noisy Data} author = {Georg Heigold and Patrick Nguyen and Mitchel Weintraub and Vincent Vanhoucke} year = 2012 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} pages = {4437--4440} address = {Kyoto Japan} },The acoustic models in state-of-the-art speech recognition systems are based on phones in context that are represented by hidden Markov models. This modeling approach may be limited in that it is hard to incorporate long-span acoustic context. Exemplar-based approaches are an attractive alternative in particular if massive data and computational power are available. Yet most of the data at Google are unsupervised and noisy. This paper investigates an exemplar-based approach under this yet not well understood data regime. A log-linear rescoring framework is used to combine the exemplar-based features on the word level with the first-pass model. This approach guarantees at least baseline performance and focuses on the refined modeling of words with sufficient data. Experimental results for the Voice Search and the YouTube tasks are presented.,http://research.google.com/pubs/archive/37825.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Investigations+on+Exemplar-Based+Features+for+Speech+Recognition+Towards+Thousands+of+Hours+of+Unsupervised+Noisy+Data+Heigold+Nguyen+Weintraub+Vanhoucke,http://research.google.com/pubs/pub37825.html
ZARATHUSTRA: Extracting WebInject Signatures from Banking Trojans,Twelfth Annual International Conference on Privacy Security and Trust IEEE (2014) pp. 139-148,2014,Claudio Criscione Fabio Bosatelli Stefano Zanero Federico Maggi,@inproceedings{43021 title = {ZARATHUSTRA: Extracting WebInject Signatures from Banking Trojans} author = {Claudio Criscione and Fabio Bosatelli and Stefano Zanero and Federico Maggi} year = 2014 note = {Privacy Security and Trust (PST) 2014 Twelfth Annual International Conference on} booktitle = {Twelfth Annual International Conference on Privacy Security and Trust} pages = {139-148} },Modern trojans are equipped with a functionality called WebInject that can be used to silently modify a web page on the infected end host. Given its flexibility WebInject-based malware is becoming a popular information-stealing mechanism. In addition the structured and well-organized malware-as-a-service model makes revenue out of customization kits which in turns leads to high volumes of binary variants. Analysis approaches based on memory carving to extract the decrypted webinject.txt and config.bin files at runtime make the strong assumption that the malware will never change the way such files are handled internally and therefore are not future proof by design. In addition developers of sensitive web applications (e.g. online banking) have no tools that they can possibly use to even mitigate the effect of WebInjects. WebInject-based trojans insert client-side code (e.g. HTML JavaScript) while the targeted web pages (e.g. online banking website search engine) are rendered on the browser. This additional code will capture sensitive information entered by the victim (e.g. one-time passwords) or perform other nefarious actions (e.g. click fraud or search engine result poisoning). The visible effect of a WebInject is that a web page rendered on infected clients differs from the very same page rendered on clean machines. We leverage this key observation and propose an approach to automatically characterize the WebInject behavior. Ultimately our system can be applied to analyze a sample automatically against a set of target websites without requiring any manual action or to generate fingerprints that are useful to determine whether a client is infected. Differently from the state of the art our method works regardless of how the WebInject module is implemented and requires no reverse engineering. We implemented and evaluated our approach against live online websites and a dataset of distinct variants of WebInject-based financial trojans. The results show that our approach correctly recognize known variants of WebInject-based malware with negligible false positives. Throughout the paper we describe some use cases that describe how our method can be applied in practice,http://research.google.com/pubs/archive/43021.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=ZARATHUSTRA:+Extracting+WebInject+Signatures+from+Banking+Trojans+Criscione+Bosatelli+Zanero+Maggi,http://research.google.com/pubs/pub43021.html
Learning the Inter-frame Distance for Discriminative Template-based Keyword Detection,Proceedings of the International Conference Interspeech-Eurospeech (2007),2007,David Grangier Samy Bengio,@inproceedings{32832 title = {Learning the Inter-frame Distance for Discriminative Template-based Keyword Detection} author = {David Grangier and Samy Bengio} year = 2007 URL = {http://bengio.abracadoudou.com/cv/publications/pdf/grangier_2007_eurospeech.pdf} booktitle = {Proceedings of the International Conference Interspeech-Eurospeech} },This paper proposes a discriminative approach to template-based keyword detection. We introduce a method to learn the distance used to compare acoustic frames a crucial element for template matching approaches. The proposed algorithm estimates the distance from data with the objective to produce a detector maximizing the Area Under the receiver operating Curve (AUC) i.e. the standard evaluation measure for the keyword detection problem. The experiments performed over a large corpus SpeechDatII suggest that our model is effective compared to an HMM system e.g. the proposed approach reaches 93.8\% of averaged AUC compared to 87.9\% for the HMM.,http://research.google.com/pubs/archive/32832.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+the+Inter-frame+Distance+for+Discriminative+Template-based+Keyword+Detection+Grangier+Bengio,http://research.google.com/pubs/pub32832.html
The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus Distribution,Large-Scale Exploits and Emergent Threats USENIX (2010),2010,Moheeb Abu Rajab Lucas Ballard Panayiotis Marvrommatis Niels Provos Xin Zhao,@inproceedings{36346 title = {The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus Distribution} author = {Moheeb Abu Rajab and Lucas Ballard and Panayiotis Marvrommatis and Niels Provos and Xin Zhao} year = 2010 URL = {http://www.usenix.org/event/leet10/tech/full_papers/Rajab.pdf} booktitle = {Large-Scale Exploits and Emergent Threats} },We present a study of Fake Anti-Virus attacks on the web. Fake AV software masquerades as a legitimate security product with the goal of deceiving victims into paying registration fees to seemingly remove malware from their computers. Our analysis of 240 million web pages collected by Google's malware detection infrastructure over a 13 month period discovered over 11000 domains involved in Fake AV distribution. We show that the Fake AV threat is rising in prevalence both absolutely and relative to other forms of web-based malware. Fake AV currently accounts for 15% of all malware we detect on the web. Our investigation reveals several characteristics that distinguish Fake AVs from other forms of web-based malware and shows how these characteristics have changed over time. For instance Fake AV attacks occur frequently via web sites likely to reach more users including spam web sites and on-line Ads. These attacks account for 60% of the malware discovered on domains that include trending keywords. As of this writing Fake AV is responsible for 50% of all malware delivered via Ads which represents a five-fold increase from just a year ago.,http://research.google.com/pubs/archive/36346.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Nocebo+Effect+on+the+Web:+An+Analysis+of+Fake+Anti-Virus+Distribution+Abu+Rajab+Ballard+Mavrommatis+Provos+Zhao,http://research.google.com/pubs/pub36346.html
Semantic Frame Identification with Distributed Word Representations,Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (2014),2014,Karl Moritz Hermann Dipanjan Das Jason Weston Kuzman Ganchev,@inproceedings{42245 title = {Semantic Frame Identification with Distributed Word Representations} author = {Karl Moritz Hermann and Dipanjan Das and Jason Weston and Kuzman Ganchev} year = 2014 booktitle = {Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics} },We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work we achieve state-of-the-art results on FrameNet-style frame-semantic analysis. Additionally we report strong results on PropBank-style semantic role labeling in comparison to prior work.,http://research.google.com/pubs/archive/42245.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Semantic+Frame+Identification+with+Distributed+Word+Representations+Hermann+Das+Weston+Ganchev,http://research.google.com/pubs/pub42245.html
Perflint: A Context Sensitive Performance Advisor for C++ Programs,Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization IEEE Computer Society Washington DC USA (2009) pp. 265-274,2009,Lixia Liu Silvius Rus,@inproceedings{36599 title = {Perflint: A Context Sensitive Performance Advisor for C++ Programs} author = {Lixia Liu and Silvius Rus} year = 2009 URL = {http://portal.acm.org/citation.cfm?id=1545006.1545076} booktitle = {Proceedings of the 7th annual IEEE/ACM International Symposium on Code Generation and Optimization} pages = {265--274} address = {Washington DC USA} },We present perflint a new industrial strength open source analysis tool that identifies suboptimal use patterns of the C++ standard library. Simply by recompiling and running on a representative input set programmers receive context sensitive performance advice on their use of standard library data structures and algorithms. Our solution consists of collecting traces of relevant library operations and state during program execution and then recognizing patterns for which there is a faster alternative based on a model made of performance guarantees in the C++ language standard and machine knowledge. perflint has already found hundreds of suboptimal patterns in a set of large C++ benchmarks. In one case following the advice and changing one line of code resulted in 17% program run time reduction.,http://portal.acm.org/citation.cfm?id=1545006.1545076,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Perflint:+A+Context+Sensitive+Performance+Advisor+for+C%2B%2B+Programs+Liu+Rus,http://research.google.com/pubs/pub36599.html
A practical comparison of the bivariate probit and linear IV estimators,Economics Letters vol. 117 (2012) pp. 762-766,2012,Richard C. Chiburis Jishnu Das Michael Lokshin,@article{40392 title = {A practical comparison of the bivariate probit and linear IV estimators} author = {Richard C. Chiburis and Jishnu Das and Michael Lokshin} year = 2012 URL = {http://dx.doi.org/10.1016/j.econlet.2012.08.037} journal = {Economics Letters} pages = {762--766} volume = {117} },This paper compares asymptotic and finite sample properties of linear IV and bivariate probit in models with an endogenous binary treatment and binary outcome. The results provide guidance on the choice of model specification and help to explain large differences in the estimates depending on the specification chosen.,http://dx.doi.org/10.1016/j.econlet.2012.08.037,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+practical+comparison+of+the+bivariate+probit+and+linear+IV+estimators+Chiburis+Das+Lokshin,http://research.google.com/pubs/pub40392.html
Training Deep Neural Networks on Noisy Labels with Bootstrapping,ICLR 2015,2015,Scott E. Reed Honglak Lee Dragomir Anguelov Christian Szegedy Dumitru Erhan Andrew Rabinovich,@inproceedings{43273 title = {Training Deep Neural Networks on Noisy Labels with Bootstrapping} author = {Scott E. Reed and Honglak Lee and Dragomir Anguelov and Christian Szegedy and Dumitru Erhan and Andrew Rabinovich} year = 2015 URL = {http://arxiv.org/abs/1412.6596} booktitle = {ICLR 2015} },Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples and in current practice the labels are assumed to be unambiguous and accurate. However this assumption often does not hold; e.g. in recognition class labels may be missing; in detection objects in the image may not be localized; and in general the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits we show that our model is robust to label corruption. On the Toronto Face Database we show that our model handles well the case of subjective labels in emotion recognition achieving state-of-theart results and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data we show that our approach extends to very deep networks high resolution images and structured outputs and results in improved scalable detection.,http://research.google.com/pubs/archive/43273.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Training+Deep+Neural+Networks+on+Noisy+Labels+with+Bootstrapping+Reed+Lee+Anguelov+Szegedy+Erhan+Rabinovich,http://research.google.com/pubs/pub43273.html
Programming Interviews Exposed,Wrox an imprint of John Wiley & Sons Inc. 111 River Street Hoboken NJ 07030-5774 (2012),2012,John Mongan Eric Giguere Noah Kindler,@book{40561 title = {Programming Interviews Exposed} author = {John Mongan and Eric Giguere and Noah Kindler} year = 2012 URL = {http://www.piexposed.com} address = {111 River Street Hoboken NJ 07030-5774} },Landing a great programming job isn't a matter of luck; it's a matter of being prepared for the unique challenges of the technical job search. Programming interviews require a different set of skills than day-to-day programming so even expert programmers often struggle if they don't know what to expect. This thoroughly revised and expanded third edition teaches you the skills you need to apply your programming expertise to the types of problems most frequently encountered in interviews at top tech companies today. Step-by-step solutions to an extensive set of sample interview questions simulate the interview experience to hone the skills you've learned. After you've worked through this book you'll approach your interviews with confidence knowing you can solve any problem that stands between you and the job you really want.,http://www.piexposed.com,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Programming+Interviews+Exposed+Mongan+Giguere+Kindler,http://research.google.com/pubs/pub40561.html
Biometric Person Authentication IS A Multiple Classifier Problem,7th International Workshop on Multiple Classifier Systems (2007),2007,Samy Bengio Johnny Mariéthoz,@inproceedings{32829 title = {Biometric Person Authentication IS A Multiple Classifier Problem} author = {Samy Bengio and Johnny Mariéthoz} year = 2007 URL = {http://bengio.abracadoudou.com/cv/publications/pdf/bengio_2007_mcs.pdf} booktitle = {7th International Workshop on Multiple Classifier Systems} },Several papers have already shown the interest of using multiple classifiers in order to enhance the performance of biometric person authentication systems. In this paper we would like to argue that the core task of Biometric Person Authentication is actually a multiple classifier problem as such: indeed in order to reach state-of-the-art performance we argue that all current systems  in one way or another try to solve several tasks simultaneously and that without such joint training (or sharing) they would not succeed as well. We explain hereafter this perspective and according to it we propose some ways to take advantage of it ranging from more parameter sharing to similarity learning.,http://research.google.com/pubs/archive/32829.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Biometric+Person+Authentication+IS+A+Multiple+Classifier+Problem+Bengio+Mari%C3%A9thoz,http://research.google.com/pubs/pub32829.html
Rate-Distortion Optimization for Multichannel Audio Compression,2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),2013,Minyue Li Jan Skoglund W. Bastiaan Kleijn,@inproceedings{41648 title = {Rate-Distortion Optimization for Multichannel Audio Compression} author = {Minyue Li and Jan Skoglund and W. Bastiaan Kleijn} year = 2013 booktitle = {2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)} },Multichannel audio coding is studied from a rate-distortion theoret- ical viewpoint. Two practical coding techniques both of which are based on rate-distortion optimization are also proposed. The first technique decorrelates a multichannel signal hierarchically using el- ementary unitary transforms. The second method rearranges a mul- tichannel signal into sub-signals and compresses them at optimized bit rates using a conventional codec. Both objective and subjective tests were conducted to illustrate the efficiency of the methods.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Rate-Distortion+Optimization+for+Multichannel+Audio+Compression+Li+Skoglund+Kleijn,http://research.google.com/pubs/pub41648.html
Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It,Proceedings of the 4th Joint Conference on Lexical and Computational Semantics (*SEM 2015) pp. 299-308,2015,Sujay Kumar Jauhar Raul D. Guerra Edgar Gonzàlez Pellicer Marta Recasens,@inproceedings{43433 title = {Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It} author = {Sujay Kumar Jauhar and Raul D. Guerra and Edgar Gonzàlez Pellicer and Marta Recasens} year = 2015 URL = {https://aclweb.org/anthology/S/S15/S15-1035.pdf} booktitle = {Proceedings of the 4th Joint Conference on Lexical and Computational Semantics (*SEM 2015)} pages = {299--308} },Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal rather than nominal antecedents. Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deictic. However current coreference resolution systems ignore this phenomenon. This paper presents an automatic system for the detection and resolution of discourse-deictic pronouns. We introduce a two-step approach that first recognizes instances of discourse-deictic pronouns and then resolves them to their verbal antecedent. Both components rely on linguistically motivated features. We evaluate the components in isolation and in combination with two state-of-the-art coreference resolvers. Results show that our system outperforms several baselines including the only comparable discourse deixis system and leads to small but statistically significant improvements over the full coreference resolution systems. An error analysis lays bare the need for a less strict evaluation of this task.,http://research.google.com/pubs/archive/43433.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Resolving+Discourse-Deictic+Pronouns:+A+Two-Stage+Approach+to+Do+%3Cem%3EIt%3C/em%3E+Jauhar+Guerra+Gonz%C3%A0lez+Pellicer+Recasens,http://research.google.com/pubs/pub43433.html
Learning to Rank Answers to Non-Factoid Questions from Web Collections,Computational Linguistics vol. 37 (2011) pp. 351-383,2011,Mihai Surdeanu Massimiliano Ciaramita Hugo Zaragoza,@article{37119 title = {Learning to Rank Answers to Non-Factoid Questions from Web Collections} author = {Mihai Surdeanu and Massimiliano Ciaramita and Hugo Zaragoza} year = 2011 URL = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00051} journal = {Computational Linguistics} pages = {351--383} volume = {37} },This work investigates the use of linguistically motivated features to improve search in particular for ranking answers to non-factoid questions. We show that it is possible to exploit existing large collections of question–answer pairs (from online social Question Answering sites) to extract such features and train ranking models which combine them effectively. We investigate a wide range of feature types some exploiting natural language processing such as coarse word sense disambiguation named-entity identiﬁcation syntactic parsing and semantic role labeling. Our experiments demonstrate that linguistic features in combination yield considerable improvements in accuracy. Depending on the system settings we measure relative improvements of 14% to 21% in Mean Reciprocal Rank and Precision@1 providing one of the most compelling evidence to date that complex linguistic features such as word senses and semantic roles can have a signiﬁcant impact on large-scale information retrieval tasks.,http://research.google.com/pubs/archive/37119.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+to+Rank+Answers+to+Non-Factoid+Questions+from+Web+Collections+Surdeanu+Ciaramita+Zaragoza,http://research.google.com/pubs/pub37119.html
Minimizing weighted flowtime on capacitated machines,ACM-SIAM Symposium on Discrete Algorithms (SODA) (2013),2013,Kyle Fox Madhukar Korupolu,@inproceedings{40493 title = {Minimizing weighted flowtime on capacitated machines} author = {Kyle Fox and Madhukar Korupolu} year = 2013 URL = {http://knowledgecenter.siam.org/0236-000131/1} booktitle = {ACM-SIAM Symposium on Discrete Algorithms (SODA)} },It is well-known that SRPT is optimal for minimizing flow time on machines that run one job at a time. However running one job at a time is a big under- utilization for modern systems where sharing simultane- ous execution and virtualization-enabled consolidation are a common trend to boost utilization. Such machines used in modern large data centers and clouds are powerful enough to run multiple jobs/VMs at a time subject to overall CPU memory network and disk capacity constraints. Motivated by this prominent trend and need in this work we give the first scheduling algorithms to minimize weighted flow time on such capacitated machines. To capture the difficulty of the problem we show that without resource augmentation no online algorithm can achieve a bounded competitive ratio. We then investigate algorithms with a small resource augmentation in speed and/or capacity. Our first result is a simple (2 + _)- capacity O(1/_)-competitive greedy algorithm. Using only speed augmentation we then obtain a 1.75-speed O(1)-competitive algorithm. Our main technical result is a near-optimal (1 + _)-speed (1 + _)-capacity O(1/_3 )- competitive algorithm using a novel combination of knapsacks densities job classification into categories and potential function methods. We show that our results also extend to the multiple unrelated capacitated machines setting.,http://research.google.com/pubs/archive/40493.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Minimizing+weighted+flowtime+on+capacitated+machines+Fox+Korupolu,http://research.google.com/pubs/pub40493.html
Isolating Web Programs in Modern Browser Architectures,Eurosys Nuremburg (2009),2009,Charles Reis Steven D. Gribble,@inproceedings{34924 title = {Isolating Web Programs in Modern Browser Architectures} author = {Charles Reis and Steven D. Gribble} year = 2009 URL = {http://portal.acm.org/citation.cfm?doid=1519065.1519090} booktitle = {Eurosys} address = {Nuremburg} },Many of today's web sites contain substantial amounts of client-side code and consequently they act more like programs than simple documents. This creates robustness and performance challenges for web browsers. To give users a robust and responsive platform the browser must identify program boundaries and provide isolation between them. We provide three contributions in this paper. First we present abstractions of web programs and program instances and we show that these abstractions clarify how browser components interact and how appropriate program boundaries can be identified. Second we identify backwards compatibility tradeoffs that constrain how web content can be divided into programs without disrupting existing web sites. Third we present a multi-process browser architecture that isolates these web program instances from each other improving fault tolerance resource management and performance. We discuss how this architecture is implemented in Google Chrome and we provide a quantitative performance evaluation examining its benefits and costs.,http://research.google.com/pubs/archive/34924.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Isolating+Web+Programs+in+Modern+Browser+Architectures+Reis+Gribble,http://research.google.com/pubs/pub34924.html
Megastore: Providing Scalable Highly Available Storage for Interactive Services,Proceedings of the Conference on Innovative Data system Research (CIDR) (2011) pp. 223-234,2011,Jason Baker Chris Bond James C. Corbett JJ Furman Andrey Khorlin James Larson Jean-Michel Leon Yawei Li Alexander Lloyd Vadim Yushprakh,@inproceedings{36971 title = {Megastore: Providing Scalable Highly Available Storage for Interactive Services} author = {Jason Baker and Chris Bond and James C. Corbett and JJ Furman and Andrey Khorlin and James Larson and Jean-Michel Leon and Yawei Li and Alexander Lloyd and Vadim Yushprakh} year = 2011 URL = {http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf} booktitle = {Proceedings of the Conference on Innovative Data system Research (CIDR)} pages = {223--234} },Megastore is a storage system developed to meet the requirements of today's interactive online services. Megastore blends the scalability of a NoSQL datastore with the convenience of a traditional RDBMS in a novel way and provides both strong consistency guarantees and high availability. We provide fully serializable ACID semantics within fine-grained partitions of data. This partitioning allows us to synchronously replicate each write across a wide area network with reasonable latency and support seamless failover between datacenters. This paper describes Megastore's semantics and replication algorithm. It also describes our experience supporting a wide range of Google production services built with Megastore.,http://research.google.com/pubs/archive/36971.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Megastore:+Providing+Scalable+Highly+Available+Storage+for+Interactive+Services+Baker+Bond+Corbett+Furman+Khorlin+Larson+Leon+Li+Lloyd+Yushprakh,http://research.google.com/pubs/pub36971.html
Scheduling partially ordered jobs faster than 2^n,ESA (2011) (to appear),2011,Marek Cygan Marcin Pilipczuk Micha_ Pilipczuk Jakub Onufry Wojtaszczyk,@inproceedings{37466 title = {Scheduling partially ordered jobs faster than 2^n} author = {Marek Cygan and Marcin Pilipczuk and Micha_ Pilipczuk and Jakub Onufry Wojtaszczyk} year = 2011 booktitle = {ESA} },In the SCHED problem we are given a set of n jobs together with their processing times and precedence constraints. The task is to order the jobs so that their total completion time is minimized. SCHED is a special case of the Traveling Repairman Problem with precedences. A natural dynamic programming algorithm solves both these problems in 2^n n^O(1) time and whether there exists an algorithms solving SCHED in O(c^n) time for some constant c < 2 was an open problem posted in 2004 by Woeginger. In this paper we answer this question positively.,http://research.google.com/pubs/archive/37466.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scheduling+partially+ordered+jobs+faster+than+2%5En+Cygan+Pilipczuk+Pilipczuk+Wojtaszczyk,http://research.google.com/pubs/pub37466.html
Deep Neural Networks for Acoustic Modeling in Speech Recognition,Signal Processing Magazine (2012),2012,Geoffrey Hinton Li Deng Dong Yu George Dahl Abdel-rahman Mohamed Navdeep Jaitly Andrew Senior Vincent Vanhoucke Patrick Nguyen Tara Sainath Brian Kingsbury,@article{38131 title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition} author = {Geoffrey Hinton and Li Deng and Dong Yu and George Dahl and Abdel-rahman Mohamed and Navdeep Jaitly and Andrew Senior and Vincent Vanhoucke and Patrick Nguyen and Tara Sainath and Brian Kingsbury} year = 2012 journal = {Signal Processing Magazine} },Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM ﬁts a frame or a short window of frames of coefﬁcients that represents the acoustic input. An alternative way to evaluate the ﬁt is to use a feedforward neural network that takes several frames of coefﬁcients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.,http://research.google.com/pubs/archive/38131.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Deep+Neural+Networks+for+Acoustic+Modeling+in+Speech+Recognition+Hinton+Deng+Yu+Dahl+Mohamed+Jaitly+Senior+Vanhoucke+Nguyen+Sainath+Kingsbury,http://research.google.com/pubs/pub38131.html
Public-Key Encryption in the Bounded-Retrieval Model,Advances in Cryptology - EUROCRYPT 2010 29th Annual International Conference on the Theory and Applications of Cryptographic Techniques French Riviera May 30 - June 3 2010. Proceedings Springer pp. 113-134,2010,Joel Alwen Yevgeniy Dodis Moni Naor Gil Segev Shabsi Walfish Daniel Wichs,@inproceedings{36624 title = {Public-Key Encryption in the Bounded-Retrieval Model} author = {Joel Alwen and Yevgeniy Dodis and Moni Naor and Gil Segev and Shabsi Walfish and Daniel Wichs} year = 2010 booktitle = {Advances in Cryptology - EUROCRYPT 2010 29th Annual International Conference on the Theory and Applications of Cryptographic Techniques French Riviera May 30 - June 3 2010. Proceedings} pages = {113-134} },"We construct the first public-key encryption scheme in the Bounded-Retrieval Model (BRM) providing security against various forms of adversarial ""key leakage"" attacks. In this model the adversary is allowed to learn arbitrary information about the decryption key subject only to the constraint that the overall amount of ""leakage"" is bounded by at most L bits. The goal of the BRM is to design cryptographic schemes that can flexibly tolerate arbitrarily leakage bounds L (few bits or many Gigabytes) by only increasing the size of secret key proportionally but keeping all the other parameters -- including the size of the public key ciphertext encryption/decryption time and the number of secret-key bits accessed during decryption -— small and independent of L. As our main technical tool we introduce the concept of an Identity-Based Hash Proof System (IB-HPS) which generalizes the notion of hash proof systems of Cramer and Shoup [CS02] to the identity-based setting. We give three different constructions of this primitive based on: (1) bilinear groups (2) lattices and (3) quadratic residuosity. As a result of independent interest we show that an IB-HPS almost immediately yields an Identity-Based Encryption (IBE) scheme which is secure against (small) partial leakage of the target identity’s decryption key. As our main result we use IB-HPS to construct public-key encryption (and IBE) schemes in the Bounded-Retrieval Model.",http://research.google.com/pubs/archive/36624.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Public-Key+Encryption+in+the+Bounded-Retrieval+Model+Alwen+Dodis+Naor+Segev+Walfish+Wichs,http://research.google.com/pubs/pub36624.html
Predicting Accurate and Actionable Static Analysis Warnings: An Experimental Approach,Proceedings of the International Conference on Software Engineering ACM (2008) pp. 341-350,2008,Joseph Ruthruff John Penix J. David Morgenthaler Sebastian Elbaum Gregg Rothermel,@inproceedings{33330 title = {Predicting Accurate and Actionable Static Analysis Warnings: An Experimental Approach} author = {Joseph Ruthruff and John Penix and J. David Morgenthaler and Sebastian Elbaum and Gregg Rothermel} year = 2008 URL = {http://cse.unl.edu/~grother/papers/icse08.pdf} booktitle = {Proceedings of the International Conference on Software Engineering} pages = {341-350} },Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings and suggests that the models are competitive with alternative models built without screening.,http://research.google.com/pubs/archive/33330.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Predicting+Accurate+and+Actionable+Static+Analysis+Warnings:+An+Experimental+Approach+Ruthruff+Penix+Morgenthaler+Elbaum+Rothermel,http://research.google.com/pubs/pub33330.html
Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams,SIGMOD '13: Proceedings of the 2013 international conference on Management of data ACM New York NY USA pp. 577-588,2013,Rajagopal Ananthanarayanan Venkatesh Basker Sumit Das Ashish Gupta Haifeng Jiang Tianhao Qiu Alexey Reznichenko Deomid Ryabkov Manpreet Singh Shivakumar Venkataraman,@inproceedings{41318 title = {Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams} author = {Rajagopal Ananthanarayanan and Venkatesh Basker and Sumit Das and Ashish Gupta and Haifeng Jiang and Tianhao Qiu and Alexey Reznichenko and Deomid Ryabkov and Manpreet Singh and Shivakumar Venkataraman} year = 2013 URL = {http://dl.acm.org/citation.cfm?doid=2463676.2465272} booktitle = {SIGMOD '13: Proceedings of the 2013 international conference on Management of data} pages = {577-588} address = {New York NY USA} },Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value. In this paper we describe the architecture of Photon a geographically distributed system for joining multiple continuously flowing streams of data in real-time with high scalability and low latency where the streams may be unordered or delayed. The system fully tolerates infrastructure degradation and datacenter-level outages without any manual intervention. Photon guarantees that there will be no duplicates in the joined output (at-most-once semantics) at any point in time that most joinable events will be present in the output in real-time (near-exact semantics) and exactly-once semantics eventually. Photon is deployed within Google Advertising System to join data streams such as web search queries and user clicks on advertisements. It produces joined logs that are used to derive key business metrics including billing for advertisers. Our production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds. We also present challenges and solutions in maintaining large persistent state across geographically distant locations and highlight the design principles that emerged from our experience.,http://research.google.com/pubs/archive/41318.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Photon:+Fault-tolerant+and+Scalable+Joining+of+Continuous+Data+Streams+Ananthanarayanan+Basker+Das+Gupta+Jiang+Qiu+Reznichenko+Ryabkov+Singh+Venkataraman,http://research.google.com/pubs/pub41318.html
Scalable K-Means by ranked retrieval,Proceedings of the 7th ACM international conference on Web search and data mining ACM New York NY USA (2014) pp. 233-242,2014,Andrei Broder Lluis Garcia-Pueyo Vanja Josifovski Sergei Vassilvitskii Srihari Venkatesan,@inproceedings{42853 title = {Scalable K-Means by ranked retrieval} author = {Andrei Broder and Lluis Garcia-Pueyo and Vanja Josifovski and Sergei Vassilvitskii and Srihari Venkatesan} year = 2014 booktitle = {Proceedings of the 7th ACM international conference on Web search and data mining} pages = {233-242} address = {New York NY USA} },"The k-means clustering algorithm has a long history and a proven practical performance however it does not scale to clustering millions of data points into thousands of clusters in high dimensional spaces. The main computational bottleneck is the need to recompute the nearest centroid for every data point at every iteration aprohibitive cost when the number of clusters is large. In this paper we show how to reduce the cost of the k-means algorithm by large factors by adapting ranked retrieval techniques. Using a combination of heuristics on two real life data sets the wall clock time per iteration is reduced from 445 minutes to less than 4 and from 705 minutes to 1.4 while the clustering quality remains within 0.5% of the k-means quality. The key insight is to invert the process of point-to-centroid assignment by creating an inverted index over all the points and then using the current centroids as queries to this index to decide on cluster membership. In other words rather than each iteration consisting of ""points picking centroids"" each iteration now consists of ""centroids picking points"". This is much more efficient but comes at the cost of leaving some points unassigned to any centroid. We show experimentally that the number of such points is low and thus they can be separately assigned once the final centroids are decided. To speed up the computation we sparsify the centroids by pruning low weight features. Finally to further reduce the running time and the number of unassigned points we propose a variant of the WAND algorithm that uses the results of the intermediate results of nearest neighbor computations to improve performance.",http://research.google.com/pubs/archive/42853.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+K-Means+by+ranked+retrieval+Broder+Garcia-Pueyo+Josifovski+Vassilvitskii+Venkatesan,http://research.google.com/pubs/pub42853.html
Multicore Bundle Adjustment,Proc. IEEE Conf. on Computer Vision and Pattern Recognition (2011) pp. 3057-3064,2011,Changchang Wu Sameer Agarwal Brian Curless Steven Seitz,@inproceedings{37112 title = {Multicore Bundle Adjustment} author = {Changchang Wu and Sameer Agarwal and Brian Curless and Steven Seitz} year = 2011 URL = {http://grail.cs.washington.edu/projects/mcba/pba.pdf} booktitle = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition} pages = {3057--3064} },The emergence of multi-core computers represents a fundamental shift with major implications for the design of computer vision algorithms. Most computers sold today have a multicore CPU with 2-16 cores and a GPU with anywhere from 4 to 128 cores. Exploiting this hardware parallelism will be key to the success and scalability of computer vision algorithms in the future. In this project we consider the design and implementation of new inexact Newton type Bundle Adjustment algorithms that exploit hardware parallelism for efficiently solving large scale 3D scene reconstruction problems. We explore the use of multicore CPU as well as multicore GPUs for this purpose. We show that overcoming the severe memory and bandwidth limitations of current generation GPUs not only leads to more space efficient algorithms but also to surprising savings in runtime. Our CPU based system is up to ten times and our GPU based system is up to thirty times faster than the current state of the art methods while maintaining comparable convergence behavior.,http://grail.cs.washington.edu/projects/mcba/pba.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multicore+Bundle+Adjustment+Wu+Agarwal+Curless+Seitz,http://research.google.com/pubs/pub37112.html
Learning improved linear transforms for speech recognition,ICASSP IEEE (2012),2012,Andrew Senior Youngmin Cho Jason Weston,@inproceedings{36901 title = {Learning improved linear transforms for speech recognition} author = {Andrew Senior and Youngmin Cho and Jason Weston} year = 2012 booktitle = {ICASSP} },This paper explores a large margin approach to learning a linear transform for dimensionality reduction. The method assumes a trained Gaussian mixture model for the each class to be discriminated and trains a linear transform with respect to the model using stochastic gradient descent. Results are presented showing improvements in state classification for individual frames and reduced word error rate in a large vocabulary speech recognition problem after maximum likelihood training and boosted maximum mutual information training.,http://research.google.com/pubs/archive/36901.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+improved+linear+transforms+for+speech+recognition+Senior+Cho+Weston,http://research.google.com/pubs/pub36901.html
The War Against Spam: A report from the front line,NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security,2007,Brad Taylor Dan Fingal Douglas Aberdeen,@inproceedings{36954 title = {The War Against Spam: A report from the front line} author = {Brad Taylor and Dan Fingal and Douglas Aberdeen} year = 2007 booktitle = {NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security} },Fighting spam is a success story of real-world machine learning. Despite the occasional spam that does reach our inboxes the overwhelming majority of spam — and there is a lot of it — is positively identified. At the same time the rarity with which users feel the need to check their spam box for false positives demonstrates a high precision of classification. This paper is an overview of Google’s approach to ﬁghting email abuse with machine learning and a discussion of some lessons learned.,http://research.google.com/pubs/archive/36954.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+War+Against+Spam:+A+report+from+the+front+line+Taylor+Fingal+Aberdeen,http://research.google.com/pubs/pub36954.html
Differential Synchronization,DocEng'09 Proceedings of the 2009 ACM Symposium on Document Engineering The Association for Computing Machinery 2 Penn Plaza Suite 701 New York New York 10121-0701 pp. 13-20,2009,Neil Fraser,@inproceedings{35605 title = {Differential Synchronization} author = {Neil Fraser} year = 2009 URL = {http://neil.fraser.name/writing/sync/eng047-fraser.pdf} booktitle = {DocEng'09 Proceedings of the 2009 ACM Symposium on Document Engineering} pages = {13--20} address = {2 Penn Plaza Suite 701 New York New York 10121-0701} },This paper describes the Differential Synchronization (DS) method for keeping documents synchronized. The key feature of DS is that it is simple and well suited for use in both novel and existing state-based applications without requiring application redesign. DS uses deltas to make efficient use of bandwidth and is fault-tolerant allowing copies to converge in spite of occasional errors. We consider practical implementation of DS and describe some techniques to improve its performance in a browser environment.,http://research.google.com/pubs/archive/35605.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Differential+Synchronization+Fraser,http://research.google.com/pubs/pub35605.html
Scalable Object Detection using Deep Neural Networks,Computer Vision and Pattern Recognition IEEE (2014) pp. 2155- 2162,2014,Dumitru Erhan Christian Szegedy Alexander Toshev Dragomir Anguelov,@inproceedings{42238 title = {Scalable Object Detection using Deep Neural Networks} author = {Dumitru Erhan and Christian Szegedy and Alexander Toshev and Dragomir Anguelov} year = 2014 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6909673&tag=1} booktitle = {Computer Vision and Pattern Recognition} pages = {2155- 2162} },Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work we propose a saliency-inspired neural network model for detection which predicts a set of class-agnostic bounding boxes along with a single score for each box corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012 while using only the top few predicted locations in each image and a small number of neural network evaluations.,http://research.google.com/pubs/archive/42238.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Object+Detection+using+Deep+Neural+Networks+Erhan+Szegedy+Toshev+Anguelov,http://research.google.com/pubs/pub42238.html
The Case for Energy-Proportional Computing,IEEE Computer vol. 40 (2007),2007,Luiz André Barroso Urs Hölzle,@article{33387 title = {The Case for Energy-Proportional Computing} author = {Luiz André Barroso and Urs Hölzle} year = 2007 URL = {http://www.computer.org/portal/site/computer/index.jsp?pageID=computer_level1&path=computer/homepage/Dec07&file=feature.xml&xsl=article.xsl} journal = {IEEE Computer} volume = {40} },In current servers the lowest energy-efficiency region corresponds to their most common operating mode. Addressing this perfect mismatch will require significant rethinking of components and systems. To that end we propose that energy proportionality should become a primary design goal. Energy-proportional designs would enable large energy savings in servers potentially doubling their efficiency in real-life use. Achieving energy proportionality will require significant improvements in the energy usage profile of every system component particularly the memory and disk subsystems. Although our experience in the server space motivates these observations we believe that energy-proportional computing also will benefit other types of computing devices.,http://research.google.com/pubs/archive/33387.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Case+for+Energy-Proportional+Computing+Barroso+Holzle,http://research.google.com/pubs/pub33387.html
Learning Hierarchical Bag of Words Using Naive Bayes Clustering,Asian Conference on Computer Vision (2012) pp. 382-395,2012,Siddhartha Chandra Shailesh Kumar C. V. Jawahar,@inproceedings{41321 title = {Learning Hierarchical Bag of Words Using Naive Bayes Clustering} author = {Siddhartha Chandra and Shailesh Kumar and C. V. Jawahar} year = 2012 URL = {http://cvit.iiit.ac.in/papers/Siddhartha2012Learning.pdf} booktitle = {Asian Conference on Computer Vision} pages = {382-395} },"Image analysis tasks such as classication clustering detection and retrieval are only as good as the feature representation of the images they use. Much research in computer vision is focused on finding better or semantically richer image representations. Bag of visual Words (BoW) is a representation that has emerged as an eective one for a variety of computer vision tasks. BoW methods traditionally use low level features. We have devised a strategy to use these low level features to create \higher level"" features by making use of the spatial context in images. In this paper we propose a novel hierarchical feature learning framework that uses a Naive Bayes Clustering algorithm to convert a 2-D symbolic image at one level to a 2-D symbolic image at the next level with richer features. On two popular datasets Pascal VOC 2007 and Caltech 101 we empirically show that classication accuracy obtained from the hierarchical features computed using our approach is signicantly higher than the traditional SIFT based BoW representation of images even though our image representations are more compact.",http://research.google.com/pubs/archive/41321.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+Hierarchical+Bag+of+Words+Using+Naive+Bayes+Clustering+Chandra+Kumar+Jawahar,http://research.google.com/pubs/pub41321.html
Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction,ACL 2013,2013,Wei Xu Raphael Hoffmann Le Zhao Ralph Grishman,@inproceedings{41671 title = {Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction} author = {Wei Xu and Raphael Hoffmann and Le Zhao and Ralph Grishman} year = 2013 booktitle = {ACL 2013} },(first author email should be xuwei@cs.nyu.edu) Abstract: Distant supervision has attracted recent in- terest for training information extraction systems because it does not require any human annotation but rather employs ex- isting knowledge bases to heuristically la- bel a training corpus. However previous work has failed to address the problem of false negative training examples misla- beled due to the incompleteness of knowl- edge bases. To tackle this problem we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art rela- tion extractor using multi-instance learn- ing with ﬁne features. We adapt the in- formation retrieval technique of pseudo- relevance feedback to expand knowledge bases assuming entity pairs in top-ranked passages are more likely to express a rela- tion. Our proposed technique signiﬁcantly improves the quality of distantly super- vised relation extraction boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Filling+Knowledge+Base+Gaps+for+Distant+Supervision+of+Relation+Extraction+Xu+Hoffmann+Zhao+Grishman,http://research.google.com/pubs/pub41671.html
Collaboration in the Cloud at Google,research.google.com (2014) pp. 1-13,2014,Yunting Sun Diane Lambert Makoto Uchida Nicolas Remy,@techreport{41926 title = {Collaboration in the Cloud at Google} author = {Yunting Sun and Diane Lambert and Makoto Uchida and Nicolas Remy} year = 2014 institution = {research.google.com} },Through a detailed analysis of logs of activity for all Google employees this paper shows how the Google Docs suite (documents spreadsheets and slides) enables and increases collaboration within Google. In particular visualization and analysis of the evolution of Google’s collaboration network show that new employees have started collaborating more quickly and with more people as usage of Docs has grown. Over the last two years the percentage of new employees who collaborate on Docs per month has risen from 70% to 90% and the percentage who collaborate with more than two people has doubled from 35% to 70%. Moreover the culture of collaboration has become more open with public sharing within Google overtaking private sharing.,http://research.google.com/pubs/archive/41926.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Collaboration+in+the+Cloud+at+Google+Sun+Lambert+Uchida+Remy,http://research.google.com/pubs/pub41926.html
S-links: Why distributed security policy requires secure introduction,Web 2.0 Security & Privacy 2013 IEEE,2013,Joseph Bonneau,@inproceedings{41138 title = {S-links: Why distributed security policy requires secure introduction} author = {Joseph Bonneau} year = 2013 booktitle = {Web 2.0 Security & Privacy 2013} },"In this paper we argue that secure introduction via hyperlinks will be essential for distributing security policies on the web. The ""strict transport security"" policy which makes HTTPS mandatory for a given domain can already be expressed by links with an https URL. We propose s-links a set of lightweight HTML extensions to express more complex security policies in links such as key pinning. This is the simplest and most efficient way to secure connections to new domains before persistent security policy can be negotiated directly requiring no changes to the user experience and aligning trust decisions with the user's mental model. We show how s-links can benefit a variety of proposed protocols and discuss implications for the browser's same-origin policy.",http://research.google.com/pubs/archive/41138.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=S-links:+Why+distributed+security+policy+requires+secure+introduction+Bonneau,http://research.google.com/pubs/pub41138.html
Improved Approximation Algorithms for (Budgeted) Node-weighted Steiner Problems,ICALP Springer (2013),2013,Mohammadhossein Bateni MohammadTaghi Hajiaghayi Vahid Liaghat,@inproceedings{41135 title = {Improved Approximation Algorithms for (Budgeted) Node-weighted Steiner Problems} author = {Mohammadhossein Bateni and MohammadTaghi Hajiaghayi and Vahid Liaghat} year = 2013 booktitle = {ICALP} },Moss and Rabani [12] study constrained node-weighted Steiner tree problems with two independent weight values associated with each node namely cost and prize (or penalty). They give an O(logn)-approximation algorithm for the prize-collecting node-weighted Steiner tree problem (PCST),http://research.google.com/pubs/archive/41135.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Improved+Approximation+Algorithms+for+(Budgeted)+Node-weighted+Steiner+Problems+Bateni+Hajiaghayi+Liaghat,http://research.google.com/pubs/pub41135.html
On the Technology Prospects and Investment Opportunities for Scalable Neuroscience,ArXiv (2013),2013,Thomas Dean Biafra Ahanonu Mainak Chowdhury Anjali Datta Andre Esteva Daniel Eth Nobie Redmon Oleg Rumyantsev Ysis Tarter,@article{41324 title = {On the Technology Prospects and Investment Opportunities for Scalable Neuroscience} author = {Thomas Dean and Biafra Ahanonu and Mainak Chowdhury and Anjali Datta and Andre Esteva and Daniel Eth and Nobie Redmon and Oleg Rumyantsev and Ysis Tarter} year = 2013 URL = {http://arxiv.org/abs/1307.7302} journal = {ArXiv} },Two major initiatives to accelerate research in the brain sciences have focused attention on developing a new generation of scientific instruments for neuroscience. These instruments will be used to record static (structural) and dynamic (behavioral) information at unprecedented spatial and temporal resolution and report out that information in a form suitable for computational analysis. We distinguish between recording — taking measurements of individual cells and the extracellular matrix — and reporting — transcoding packaging and transmitting the resulting information for subsequent analysis — as these represent very different challenges as we scale the relevant technologies to support simultaneously tracking the many neurons that comprise neural circuits of interest. We investigate a diverse set of technologies with the purpose of anticipating their development over the span of the next 10 years and categorizing their impact in terms of short-term [1-2 years] medium-term [2-5 years] and longer-term [5-10 years] deliverables.,http://research.google.com/pubs/archive/41324.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+Technology+Prospects+and+Investment+Opportunities+for+Scalable+Neuroscience+Dean+Ahanonu+Chowdhury+Datta+Esteva+Eth+Redmon+Rumyantsev+Tarter,http://research.google.com/pubs/pub41324.html
Large-Scale Music Annotation and Retrieval: Learning to Rank in Joint Semantic Spaces.,Journal of New Music Research (2011),2011,Jason Weston Samy Bengio Philippe Hamel,@article{37179 title = {Large-Scale Music Annotation and Retrieval: Learning to Rank in Joint Semantic Spaces.} author = {Jason Weston and Samy Bengio and Philippe Hamel} year = 2011 journal = {Journal of New Music Research} },Music prediction tasks range from predicting tags given a song or clip of audio predicting the name of the artist or predicting related songs given a song clip artist name or tag. That is we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases the number of songs is measured in the hundreds of thousands or more and the number of artists in the tens of thousands or more providing a considerable challenge to standard machine learning techniques. In this work we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio artist names and tags in a single low-dimensional semantic embedding space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our single model learnt by training on the joint objective function is shown experimentally to have improved accuracy over training on each task alone. Our method also outperforms the baseline methods tried and in comparison to them is faster and consumes less memory. We also demonstrate how our method learns an interpretable model where the semantic space captures well the similarities of interest.,http://research.google.com/pubs/archive/37179.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Large-Scale+Music+Annotation+and+Retrieval:+Learning+to+Rank+in+Joint+Semantic+Spaces.+Weston+Bengio+Hamel,http://research.google.com/pubs/pub37179.html
Human Computation Must Be Reproducible,WWW 2012 Lyon.,2012,Praveen Paritosh,@inproceedings{40741 title = {Human Computation Must Be Reproducible} author = {Praveen Paritosh} year = 2012 booktitle = {WWW 2012 Lyon.} },Human computation is the technique of performing a computational process by outsourcing some of the difficult-to-automate steps to humans. In the social and behavioral sciences when using humans as measuring instruments reproducibility guides the design and evaluation of experiments. We argue that human computation has similar properties and that the results of human computation must be reproducible in the least in order to be informative. We might additionally require the results of human computation to have high validity or high utility but the results must be reproducible in order to measure the validity or utility to a degree better than chance. Additionally a focus on reproducibility has implications for design of task and instructions as well as for the communication of the results. It is humbling how often the initial understanding of the task and guidelines turns out to lack reproducibility. We suggest ensuring measuring and communicating reproducibility of human computation tasks.,http://research.google.com/pubs/archive/40741.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Human+Computation+Must+Be+Reproducible+Paritosh,http://research.google.com/pubs/pub40741.html
Collaborative Environmental In Situ Data Collection: Experiences and Opportunities for Ambient Data Integration,On the Move to Meaningful Internet Systems: OTM 2010 Workshops Lecture Notes in Computer Science pp. 119,2010,David Thau,@inproceedings{37355 title = {Collaborative Environmental In Situ Data Collection: Experiences and Opportunities for Ambient Data Integration} author = {David Thau} year = 2010 URL = {https://springerlink3.metapress.com/content/n45w70220848559m/resource-secured/?target=fulltext.pdf&sid=vombe52ypocvnlldog5b4ntt&sh=www.springerlink.com} booktitle = {On the Move to Meaningful Internet Systems: OTM 2010 Workshops} pages = {119} },Collaborative environmental in situ data collection occurs when a team of investigators goes into the field together to collect environmental data. These data might be necessary e.g. for a biodiversity inventory compilation of a soil density map or to estimate above-ground forest carbon stocks. Investigators will often arrive at a location and disperse collecting data and then compiling it either in the field or at a later time. Typically an area will be divided into a set of plots and within those subplots. Teams of investigators will visit each of these plots with standardized forms and specialized equipment for collecting the data of interest. For example in a forest inventory investigators might collect data about the diameter and species of the trees in the forest the trees’ health fire damage and soil quality at the plot proximity to roads and whether any logging has taken place.,https://springerlink3.metapress.com/content/n45w70220848559m/resource-secured/?target=fulltext.pdf&sid=vombe52ypocvnlldog5b4ntt&sh=www.springerlink.com,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Collaborative+Environmental+In+Situ+Data+Collection:+Experiences+and+Opportunities+for+Ambient+Data+Integration+Thau,http://research.google.com/pubs/pub37355.html
Flexible Network Bandwidth and Latency Provisioning in the Datacenter,arxiv.org (2014),2014,Vimalkumar Jeyakumar Abdul Kabbani Jeffrey C. Mogul Amin Vahdat,@techreport{43871 title = {Flexible Network Bandwidth and Latency Provisioning in the Datacenter} author = {Vimalkumar Jeyakumar and Abdul Kabbani and Jeffrey C. Mogul and Amin Vahdat} year = 2014 URL = {http://arxiv.org/abs/1405.0631} },Predictably sharing the network is critical to achieving high utilization in the datacenter. Past work has focussed on providing bandwidth to endpoints but often we want to allocate resources among multi-node services. In this paper we present Parley which provides service-centric minimum bandwidth guarantees which can be composed hierarchically. Parley also supports service-centric weighted sharing of bandwidth in excess of these guarantees. Further we show how to configure these policies so services can get low latencies even at high network load. We evaluate Parley on a multi-tiered oversubscribed network connecting 90 machines each with a 10Gb/s network interface and demonstrate that Parley is able to meet its goals.,http://research.google.com/pubs/archive/43871.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Flexible+Network+Bandwidth+and+Latency+Provisioning+in+the+Datacenter+Jeyakumar+Kabbani+Mogul+Vahdat,http://research.google.com/pubs/pub43871.html
Erlang for Concurrent Programming,Communications of the ACM vol. 52 (2009) pp. 48-56,2009,Jim Larson,@article{35168 title = {Erlang for Concurrent Programming} author = {Jim Larson} year = 2009 URL = {http://cacm.acm.org/magazines/2009/3/21784-erlang-for-concurrent-programming/fulltext} journal = {Communications of the ACM} pages = {48--56} volume = {52} },Designed for concurrency from the ground up the Erlang language can be a valuable too to help solve concurrent problems.,http://cacm.acm.org/magazines/2009/3/21784-erlang-for-concurrent-programming/fulltext,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Erlang+for+Concurrent+Programming+Larson,http://research.google.com/pubs/pub35168.html
Multi-Class Deep Boosting,Advances in Neural Information Processing Systems (2014),2014,Vitaly Kuznetsov Mehryar Mohri Umar Syed,@inproceedings{43247 title = {Multi-Class Deep Boosting} author = {Vitaly Kuznetsov and Mehryar Mohri and Umar Syed} year = 2014 booktitle = {Advances in Neural Information Processing Systems (2014)} },We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multiclass classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and more crucially by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble’s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees prove positive results for the H-consistency of several of them and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.,http://research.google.com/pubs/archive/43247.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Multi-Class+Deep+Boosting+Kuznetsov+Mohri+Syed,http://research.google.com/pubs/pub43247.html
Dynamic adjustment of video quality,Patent (2015),2015,Thomas Steiner,none,A video quality module receives data indicating a visibility status of a tab of a web browser running on a user device. The video quality module determines based on the data indicating the visibility status of the tab whether the tab of the web browser is currently visible to a user of the user device the tab of the web browser comprising a streaming media player. If the tab of the web browser is not currently visible to the user the video quality module decreases a quality of a video component of a streaming media file playing in the streaming media player.,http://www.google.com/patents/US8966370,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+adjustment+of+video+quality+Steiner,http://research.google.com/pubs/pub44013.html
Accurate Online Power Estimation and Automatic Battery Behavior Based Power Model Generation for Smartphones,Proceeding of Internation Conference on Hardware/Software Codesign and System Synthesis (2010) pp. 105-114,2010,Lide Zhang Birjodh Tiwana Zhiyun Qian Zhaoguang Wang Robert P. Dick Z. Morley Mao Lei Yang,@inproceedings{39990 title = {Accurate Online Power Estimation and Automatic Battery Behavior Based Power Model Generation for Smartphones} author = {Lide Zhang and Birjodh Tiwana and Zhiyun Qian and Zhaoguang Wang and Robert P. Dick and Z. Morley Mao and Lei Yang} year = 2010 booktitle = {Proceeding of Internation Conference on Hardware/Software Codesign and System Synthesis} pages = {105-114} },This paper describes PowerBooter an automated power model construction technique that uses built-in battery voltage sensors and knowledge of battery discharge behavior to monitor power consumption while explicitly controlling the power management and activity states of individual components. It requires no external measurement equipment. We also describe PowerTutor a component power management and activity state introspection based tool that uses the model generated by PowerBooter for online power estimation. PowerBooter is intended to make it quick and easy for application developers and end users to generate power models for new smartphone variants which each have different power consumption properties and therefore require different power models. PowerTutor is intended to ease the design and selection of power efficient software for embedded systems. Combined PowerBooter and PowerTutor have the goal of opening power modeling and analysis for more smartphone variants and their users.,http://research.google.com/pubs/archive/39990.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Accurate+Online+Power+Estimation+and+Automatic+Battery+Behavior+Based+Power+Model+Generation+for+Smartphones+Zhang+Tiwana+Qian+Wang+Dick+Mao+Yang,http://research.google.com/pubs/pub39990.html
MapReduce/Bigtable for Distributed Optimization,Neural Information Processing Systems Workshop on Leaning on Cores Clusters and Clouds (2010),2010,Keith B. Hall Scott Gilpin Gideon Mann,@inproceedings{36948 title = {MapReduce/Bigtable for Distributed Optimization} author = {Keith B. Hall and Scott Gilpin and Gideon Mann} year = 2010 note = {http://lccc.eecs.berkeley.edu/} booktitle = {Neural Information Processing Systems Workshop on Leaning on Cores Clusters and Clouds} },For large data it can be very time consuming to run gradient based optimizat ionfor example to minimize the log-likelihood for maximum entropy models.Distributed methods are therefore appealing and a number of distributed gradientoptimization strategies have been proposed including: distributed gradient asynchronousupdates and iterative parameter mixtures. In this paper we evaluatethese various strategies with regards to their accuracy and speed over MapReduce/Bigtable and discuss the techniques needed for high performance.,http://research.google.com/pubs/archive/36948.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=MapReduce/Bigtable+for+Distributed+Optimization+Hall+Gilpin+Mann,http://research.google.com/pubs/pub36948.html
SAC056 - ICANN SSAC Advisory on Impacts of Content Blocking via the Domain Name System,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (Internet Corporation for Assigned Names and Numbers) (2012),2012,Warren Kumari Alain Aina Jaap Akkerhuis Don Blumenthal KC Claffy David Conrad Patrik Fältström James Galvin Jason Livingood Danny McPherson Ram Mohan Paul Vixie,@incollection{41397 title = {SAC056 - ICANN SSAC Advisory on Impacts of Content Blocking via the Domain Name System} author = {Warren Kumari and Alain Aina and Jaap Akkerhuis and Don Blumenthal and KC Claffy and David Conrad and Patrik Fältström and James Galvin and Jason Livingood and Danny McPherson and Ram Mohan and Paul Vixie} year = 2012 URL = {http://www.icann.org/en/groups/ssac/documents/sac-056-en.pdf} booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} },The use of Domain Name System (DNS) blocking to limit access to resources on the Internet has become a topic of interest in numerousInternet governance venues. Several governments around the world whether by law treaty court order law enforcement action or other actions or agreements have either implemented DNS blocking or are actively considering doing so. However due to the Internet’s architecture blocking by domain name can be easily bypassed by end users and is thus likely to be largely ineffective in the long term and fraught with unanticipated consequences in the near term. In addition DNS blocking can present conflicts with the adoption of DNS Security Extensions(DNSSEC) and could promote balkanization of the Internet into a country-by-country view of the Internet’s name space.,http://research.google.com/pubs/archive/41397.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC056+-+ICANN+SSAC+Advisory+on+Impacts+of+Content+Blocking+via+the+Domain+Name+System+Kumari+Aina+Akkerhuis+Blumenthal+Claffy+Conrad+F%C3%A4ltstr%C3%B6m+Galvin+Livingood+McPherson+Mohan+Vixie,http://research.google.com/pubs/pub41397.html
Dapper a Large-Scale Distributed Systems Tracing Infrastructure,Google Inc. (2010),2010,Benjamin H. Sigelman Luiz André Barroso Mike Burrows Pat Stephenson Manoj Plakal Donald Beaver Saul Jaspan Chandan Shanbhag,@techreport{36356 title = {Dapper a Large-Scale Distributed Systems Tracing Infrastructure} author = {Benjamin H. Sigelman and Luiz André Barroso and Mike Burrows and Pat Stephenson and Manoj Plakal and Donald Beaver and Saul Jaspan and Chandan Shanbhag} year = 2010 URL = {http://research.google.com/archive/papers/dapper-2010-1.pdf} institution = {Google Inc.} },Modern Internet services are often implemented as complex large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams perhaps in different programming languages and could span many thousands of machines across multiple physical facili- ties. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment. Here we introduce the design of Dapper Google’s production distributed systems tracing infrastructure and describe how our design goals of low overhead application-level transparency and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems particularly Magpie [3] and X-Trace [12] but certain design choices were made that have been key to its success in our environment such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The main goal of this paper is to report on our experience building deploying and using the system for over two years since Dapper’s foremost measure of success has been its usefulness to developer and operations teams. Dapper began as a self-contained tracing tool but evolved into a monitoring platform which has enabled the creation of many different tools some of which were not anticipated by its designers. We describe a few of the analysis tools that have been built using Dapper share statistics about its usage within Google present some example use cases and discuss lessons learned so far.,http://research.google.com/pubs/archive/36356.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dapper+a+Large-Scale+Distributed+Systems+Tracing+Infrastructure+Sigelman+Barroso+Burrows+Stephenson+Plakal+Beaver+Jaspan+Shanbhag,http://research.google.com/pubs/pub36356.html
Loop Recognition in C++/Java/Go/Scala,Proceedings of Scala Days 2011,2011,Robert Hundt,@inproceedings{37122 title = {Loop Recognition in C++/Java/Go/Scala} author = {Robert Hundt} year = 2011 URL = {https://days2011.scala-lang.org/sites/days2011/files/ws3-1-Hundt.pdf} booktitle = {Proceedings of Scala Days 2011} },In this experience report we encode a well speciﬁed compact benchmark in four programming languages namely C++ Java Go and Scala. The implementations each use the languages’ idiomatic container classes looping constructs and memory/object allocation schemes. It does not attempt to exploit speciﬁc language and runtime features to achieve maximum performance. This approach allows an almost fair comparison of language features code complexity compilers and compile time binary sizes runtimes and memory footprint. While the benchmark itself is simple and compact it employs many language features in particular higher-level data structures (lists maps lists and arrays of sets and lists) a few algorithms (union/ﬁnd dfs / deep recursion and loop recognition based on Tarjan) iterations over collection types some object oriented features and interesting memory allocation patterns. We do not explore any aspects of multi-threading or higher level type mechanisms which vary greatly between the languages. The benchmark points to very large differences in all examined dimensions of the language implementations. After publication of the benchmark internally at Google several engineers produced highly optimized versions of the benchmark. While this whole effort is an anectodal comparison only the benchmark and subsequent tuning effort might be indicatie of typical performance pain points in the respective languages.,http://research.google.com/pubs/archive/37122.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Loop+Recognition+in+C%2B%2B/Java/Go/Scala+Hundt,http://research.google.com/pubs/pub37122.html
TCP Option to Denote Packet Mood,Internet Engineering Task Force http://ietf.org (2010) pp. 8,2010,Richard Hay Warren Turkal,@misc{36604 title = {TCP Option to Denote Packet Mood} author = {Richard Hay and Warren Turkal} year = 2010 URL = {http://datatracker.ietf.org/doc/rfc5841/} note = {RFC 5841} },"In an attempt to anthropomorphize the bit streams on countless physical layer networks throughout the world we propose a TCP option to express packet mood. This can be addressed by adding TCP Options [RFC793] to the TCP header using ASCII characters that encode commonly used ""emoticons"" to convey packet mood.",http://research.google.com/pubs/archive/36604.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=TCP+Option+to+Denote+Packet+Mood+Hay+Turkal,http://research.google.com/pubs/pub36604.html
Scalable Attribute-Value Extraction from Semi-Structured Text,ICDM Workshop on Large-scale Data Mining: Theory and Applications (2009),2009,Yuk Wah Wong Dominic Widdows Tom Lokovic Kamal Nigam,@inproceedings{34460 title = {Scalable Attribute-Value Extraction from Semi-Structured Text} author = {Yuk Wah Wong and Dominic Widdows and Tom Lokovic and Kamal Nigam} year = 2009 URL = {http://www.computer.org/portal/web/csdl/doi/10.1109/ICDMW.2009.81} booktitle = {ICDM Workshop on Large-scale Data Mining: Theory and Applications} },This paper describes a general methodology for extracting attribute-value pairs from web pages. It consists of two phases: candidate generation in which syntactically likely attribute-value pairs are annotated; and candidate filtering in which semantically improbable annotations are removed. We describe three types of candidate generators and two types of candidate filters all of which are designed to be massively parallelizable. Our methods can handle 1 billion web pages in less than 6 hours with 1000 machines. The best generator and filter combination achieves 70% F-measure compared to a hand-annotated corpus.,http://research.google.com/pubs/archive/34460.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scalable+Attribute-Value+Extraction+from+Semi-Structured+Text+Wong+Widdows+Lokovic+Nigam,http://research.google.com/pubs/pub34460.html
Max-Cover in Map-Reduce,Proceedings of the 19th international conference on World Wide Web ACM Raleigh North Carolina (2010) pp. 231-240,2010,Flavio Chierichetti Ravi Kumar Andrew Tomkins,@inproceedings{36582 title = {Max-Cover in Map-Reduce} author = {Flavio Chierichetti and Ravi Kumar and Andrew Tomkins} year = 2010 booktitle = {Proceedings of the 19th international conference on World Wide Web} pages = {231-240} address = {Raleigh North Carolina} },The NP-hard Max-k-cover problem requires selecting k sets from a collection so as to maximize the size of the union. This classic problem occurs commonly in many settings in web search and advertising. For moderately-sized instances a greedy algorithm gives an approximation of (1-1/e). However the greedy algorithm requires updating scores of arbitrary elements after each step and hence becomes intractable for large datasets. We give the first max cover algorithm designed for today's large-scale commodity clusters. Our algorithm has provably almost the same approximation as greedy but runs much faster. Furthermore it can be easily expressed in the MapReduce programming paradigm and requires only polylogarithmically many passes over the data. Our experiments on five large problem instances show that our algorithm is practical and can achieve good speedups compared to the sequential greedy algorithm.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Max-Cover+in+Map-Reduce+Chierichetti+Kumar+Tomkins,http://research.google.com/pubs/pub36582.html
Capturing Indoor Scenes with Smartphones,Proc. UIST 651 N. 34th St. (2012) (to appear),2012,Aditya Sankar Steve Seitz,@inproceedings{39967 title = {Capturing Indoor Scenes with Smartphones} author = {Aditya Sankar and Steve Seitz} year = 2012 URL = {http://grail.cs.washington.edu/videotour/videotour_final.pdf} booktitle = {Proc. UIST} address = {651 N. 34th St.} },In this paper we present a novel smartphone application designed to easily capture visualize and reconstruct homes ofﬁces and other indoor scenes. Our application leverages data from smartphone sensors such as the camera accelerometer gyroscope and magnetometer to help model the indoor scene. The output of the system is two-fold; ﬁrst an interactive visual tour of the scene is generated in real time that allows the user to explore each room and transition between connected rooms. Second with some basic interactive photogrammetric modeling the system generates a 2D ﬂoor plan and accompanying 3D model of the scene under a Manhattan-world assumption. The approach does not require any specialized equipment or training and is able to produce accurate ﬂoor plans.,http://grail.cs.washington.edu/videotour/videotour_final.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Capturing+Indoor+Scenes+with+Smartphones+Sankar+Seitz,http://research.google.com/pubs/pub39967.html
Automatically Discovering Talented Musicians with Acoustic Analysis of YouTube Videos,Proceedings of the 2012 IEEE 12th International Conference on Data Mining (ICDM) IEEE Computer Society Washington DC USA pp. 559-565,2012,Eric Nichols Charles DuHadway Hrishikesh Aradhye Richard F. Lyon,@inproceedings{41422 title = {Automatically Discovering Talented Musicians with Acoustic Analysis of YouTube Videos} author = {Eric Nichols and Charles DuHadway and Hrishikesh Aradhye and Richard F. Lyon} year = 2012 URL = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413870} booktitle = {Proceedings of the 2012 IEEE 12th International Conference on Data Mining (ICDM)} pages = {559--565} address = {Washington DC USA} },"Online video presents a great opportunity for up-and-coming singers and artists to be visible to a worldwide audience. However the sheer quantity of video makes it difficult to discover promising musicians. We present a novel algorithm to automatically identify talented musicians using machine learning and acoustic analysis on a large set of ""home singing"" videos. We describe how candidate musician videos are identified and ranked by singing quality. To this end we present new audio features specifically designed to directly capture singing quality. We evaluate these vis-a-vis a large set of generic audio features and demonstrate that the proposed features have good predictive performance. We also show that this algorithm performs well when videos are normalized for production quality.",http://research.google.com/pubs/archive/41422.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatically+Discovering+Talented+Musicians+with+Acoustic+Analysis+of+YouTube+Videos+Nichols+DuHadway+Aradhye+Lyon,http://research.google.com/pubs/pub41422.html
Beyond “Near-Duplicates”: Learning Hash Codes for Efficient Similar-Image Retrieval,20th International Conference on Pattern Recognition 2010,2010,Shumeet Baluja Michele Covell,@inproceedings{36579 title = {Beyond “Near-Duplicates”: Learning Hash Codes for Efficient Similar-Image Retrieval} author = {Shumeet Baluja and Michele Covell} year = 2010 URL = {http://www.esprockets.com/papers/icpr2010-baluja-covell.pdf} booktitle = {20th International Conference on Pattern Recognition 2010} },Finding similar images in a large database is an important but often computationally expensive task. In this paper we present a two-tier similar-image retrieval system with the efficiency characteristics found in simpler systems designed to recognize near-duplicates. We compare the efficiency of lookups based on random projections and learned hashes to 100-times-more-frequent exemplar sampling. Both approaches significantly improve on the results from exemplar sampling despite having significantly lower computational costs. Learned-hash keys provide the best result in terms of both recall and efficiency.,http://research.google.com/pubs/archive/36579.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Beyond+%E2%80%9CNear-Duplicates%E2%80%9D:+Learning+Hash+Codes+for+Efficient+Similar-Image+Retrieval+Baluja+Covell,http://research.google.com/pubs/pub36579.html
Asynchronous Stochastic Optimization for Sequence Training of Deep Neural Networks,Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP) IEEE Firenze Italy (2014),2014,Georg Heigold Erik McDermott Vincent Vanhoucke Andrew Senior Michiel Bacchiani,@inproceedings{42248 title = {Asynchronous Stochastic Optimization for Sequence Training of Deep Neural Networks} author = {Georg Heigold and Erik McDermott and Vincent Vanhoucke and Andrew Senior and Michiel Bacchiani} year = 2014 booktitle = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)} address = {Firenze Italy} },This paper explores asynchronous stochastic optimization for sequence training of deep neural networks. Sequence training requires more computation than frame-level training using pre-computed frame data. This leads to several complications for stochastic optimization arising from signiﬁcant asynchrony in model updates under massive parallelization and limited data shufﬂing due to utterance-chunked processing. We analyze the impact of these two issues on the efﬁciency and performance of sequence training. In particular we suggest a framework to formalize the reasoning about the asynchrony and present experimental results on both small and large scale Voice Search tasks to validate the effectiveness and efﬁciency of asynchronous stochastic optimization.,http://research.google.com/pubs/archive/42248.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Asynchronous+Stochastic+Optimization+for+Sequence+Training+of+Deep+Neural+Networks+Heigold+McDermott+Vanhoucke+Senior+Bacchiani,http://research.google.com/pubs/pub42248.html
A big data approach to acoustic model training corpus selection,Conference of the International Speech Communication Association (Interspeech) (2014),2014,Olga Kapralova John Alex Eugene Weinstein Pedro Moreno Olivier Siohan,@inproceedings{43230 title = {A big data approach to acoustic model training corpus selection} author = {Olga Kapralova and John Alex and Eugene Weinstein and Pedro Moreno and Olivier Siohan} year = 2014 booktitle = {Conference of the International Speech Communication Association (Interspeech)} },Deep neural networks (DNNs) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese French Italian and Russian languages.,http://research.google.com/pubs/archive/43230.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+big+data+approach+to+acoustic+model+training+corpus+selection+Kapralova+Alex+Weinstein+Moreno+Siohan,http://research.google.com/pubs/pub43230.html
A Computationally Efficient Algorithm for Learning Topical Collocation Models,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing Association for Computational Linguistics Beijing China (2015) pp. 1460-1469,2015,Zhendong Zhao Lan Du Benjamin Borschinger John K Pate Massimiliano Ciaramita Mark Steedman Mark Johnson,@inproceedings{43891 title = {A Computationally Efficient Algorithm for Learning Topical Collocation Models} author = {Zhendong Zhao and Lan Du and Benjamin Borschinger and John K Pate and Massimiliano Ciaramita and Mark Steedman and Mark Johnson} year = 2015 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing} pages = {1460--1469} address = {Beijing China} },Most existing topic models make the bagof-words assumption that words are generated independently and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach restricted attention to bigrams or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson 2010) and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Computationally+Efficient+Algorithm+for+Learning+Topical+Collocation+Models+Zhao+Du+B%C3%B6rschinger+Pate+Ciaramita+Steedman+Johnson,http://research.google.com/pubs/pub43891.html
Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP) Association for Computational Linguistics,2011,Ashish Venugopal Jakob Uszkoreit David Talbot Franz Och Juri Ganitkevitch,@inproceedings{37162 title = {Watermarking the Outputs of Structured Prediction with an application in Statistical Machine Translation} author = {Ashish Venugopal and Jakob Uszkoreit and David Talbot and Franz Och and Juri Ganitkevitch} year = 2011 URL = {http://www.aclweb.org/anthology/D11-1126.pdf} booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)} },We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms. Our method is robust to local editing operations and provides well deﬁned trade-o_s between the ability to identify algorithm outputs and the quality of the watermarked output. Unlike previous work in the ﬁeld our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one’s own algorithm. We present an application in statistical machine translation where machine translated output is watermarked at minimal loss in translation quality and detected with high recall.,http://research.google.com/pubs/archive/37162.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Watermarking+the+Outputs+of+Structured+Prediction+with+an+application+in+Statistical+Machine+Translation+Venugopal+Uszkoreit+Talbot+Och+Ganitkevitch,http://research.google.com/pubs/pub37162.html
A transcription factor affinity-based code for mammalian transcription initiation,Genome Research vol. 19 (2009) pp. 644-56,2009,M Megraw F Pereira ST Jensen U Ohler AG Hatzigeorgiou,@article{36366 title = {A transcription factor affinity-based code for mammalian transcription initiation} author = {M Megraw and F Pereira and ST Jensen and U Ohler and AG Hatzigeorgiou} year = 2009 URL = {http://genome.cshlp.org/content/19/4/644.abstract} journal = {Genome Research} pages = {644--56} volume = {19} },The recent arrival of large-scale cap analysis of gene expression (CAGE) data sets in mammals provides a wealth of quantitative information on coding and noncoding RNA polymerase II transcription start sites (TSS). Genome-wide CAGE studies reveal that a large fraction of TSS exhibit peaks where the vast majority of associated tags map to a particular location ( approximately 45%) whereas other active regions contain a broader distribution of initiation events. The presence of a strong single peak suggests that transcription at these locations may be mediated by position-specific sequence features. We therefore propose a new model for single-peaked TSS based solely on known transcription factors (TFs) and their respective regions of positional enrichment. This probabilistic model leads to near-perfect classification results in cross-validation (auROC = 0.98) and performance in genomic scans demonstrates that TSS prediction with both high accuracy and spatial resolution is achievable for a specific but large subgroup of mammalian promoters. The interpretable model structure suggests a DNA code in which canonical sequence features such as TATA-box Initiator and GC content do play a significant role but many additional TFs show distinct spatial biases with respect to TSS location and are important contributors to the accurate prediction of single-peak transcription initiation sites. The model structure also reveals that CAGE tag clusters distal from annotated gene starts have distinct characteristics compared to those close to gene 5'-ends. Using this high-resolution single-peak model we predict TSS for approximately 70% of mammalian microRNAs based on currently available data.,http://genome.cshlp.org/content/19/4/644.abstract,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+transcription+factor+affinity-based+code+for+mammalian+transcription+initiation+Megraw+Pereira+Jensen+Ohler+Hatzigeorgiou,http://research.google.com/pubs/pub36366.html
The Cray XT4 and Seastar 3-D Torus Interconnect,Encyclopedia of Parallel Computing Springer (2011),2011,Dennis Abts,@inbook{36896 title = {The Cray XT4 and Seastar 3-D Torus Interconnect} author = {Dennis Abts} year = 2011 booktitle = {Encyclopedia of Parallel Computing} },The Cray XT4 system is a distributed memory multiprocessor combining an aggressive superscalar processor (AMD64) with a bandwidth-rich 3-D torus interconnection network that scales up to 32K processing nodes. This chapter provides an overview of the Cray XT4 system architecture and a detailed discussion of its interconnection network.,http://research.google.com/pubs/archive/36896.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Cray+XT4+and+Seastar+3-D+Torus+Interconnect+Abts,http://research.google.com/pubs/pub36896.html
Energy-Efﬁcient Protocol for Cooperative Networks,IEEE/ACM TRANSACTIONS ON NETWORKING vol. 19 (2011) pp. 561-574,2011,Mohamed Elhawary Zygmunt J. Haas,@article{36951 title = {Energy-Efﬁcient Protocol for Cooperative Networks} author = {Mohamed Elhawary and Zygmunt J. Haas} year = 2011 journal = {IEEE/ACM TRANSACTIONS ON NETWORKING} pages = {561--574} volume = {19} },In cooperative networks transmitting and receiving nodes recruit neighboring nodes to assist in communication. We model a cooperative transmission link in wireless networks as a transmitter cluster and a receiver cluster. We then propose a cooperative communication protocol for establishment of these clusters and for cooperative transmission of data. We derive the upper bound of the capacity of the protocol and we analyze the end-to-end robustness of the protocol to data-packet loss along with the tradeoff between energy consumption and error rate. The analysis results are used to compare the energy savings and the end-to-end robustness of our protocol to two non- cooperative schemes as well as to another cooperative protocol published in the technical literature. The comparison results show that when nodes are positioned on a grid there is a reduction in the probability of packet delivery failure by two orders of magnitude for the values of parameters considered. Up to 80% in energy savings can be achieved for a grid topology while for random node placement our cooperative protocol can save up to 40% in energy consumption relative to the other protocols. The reduction in error rate and the energy savings translate into increased life time of cooperative sensor networks.,http://research.google.com/pubs/archive/36951.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Energy-Ef%EF%AC%81cient+Protocol+for+Cooperative+Networks+Elhawary+Haas,http://research.google.com/pubs/pub36951.html
Performance tournaments with crowdsourced judges,Proceedings of the American Statistical Association section on marketing statistics American Statistical Association 732 North Washtington Street Alexandria VA 22314-1943 (2013),2013,Daryl Pregibon Williiam D Heavlin,@inproceedings{41884 title = {Performance tournaments with crowdsourced judges} author = {Daryl Pregibon and Williiam D Heavlin} year = 2013 booktitle = {Proceedings of the American Statistical Association section on marketing statistics} address = {732 North Washtington Street Alexandria VA 22314-1943} },A performance slam is a competition among a fixed set of performances whereby pairs of performances are judged by audience participants. When performances are recorded on electronic media performance slams become amenable to audiences that watch on_line and judge asynchronously (“crowdsourced”). In order to better entertain the audience we want to show the better performances (“exploitation”). In order to identify the good videos we want to glean a least some information about all videos (“exploration”). Our approach has three elements: (1) We take our preference model from Bradley and Terry (1952). (2) Its parameters we calculate by rewriting the likelihood gradient into a fixed point estimate one which mimics the estimate of Mantel and Haenszel (1959). (3) Each pair of performances is chosen sequentially always chosen to minimize the weighted variance of (the logarithms of) the Bradley-Terry parameter estimates. Our preferred weights consist of the log_rank weights proposed by Savage (1956).,http://research.google.com/pubs/archive/41884.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Performance+tournaments+with+crowdsourced+judges+Pregibon+Heavlin,http://research.google.com/pubs/pub41884.html
How Friendships Form,The Quarterly Journal of Economics vol. 121 (2006),2006,David Marmaros Bruce Sacerdote,@article{27880 title = {How Friendships Form} author = {David Marmaros and Bruce Sacerdote} year = 2006 journal = {The Quarterly Journal of Economics} volume = {121} },We examine how people form social networks among their peers. We use a unique data set that tells us the volume of email between any two people in the sample. The data are from students and recent graduates of Dartmouth College. First-year students interact with peers in their immediate proximity and form long-term friendships with a subset of these people. This result is consistent with a model in which the expected value of interacting with an unknown person is low (making traveling solely to meet new people unlikely) while the benefits from interacting with the same person repeatedly are high. Geographic proximity and race are greater determinants of social interaction than are common interests majors or family background.,http://research.google.com/pubs/archive/27880.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Friendships+Form+Marmaros+Sacerdote,http://research.google.com/pubs/pub27880.html
EventWave: Programming Model and Runtime Support for Tightly-Coupled Elastic Cloud Applications,Proceedings of the 2013 ACM Symposium on Cloud Computing ACM Santa Clara CA USA,2013,Wei-Chiu Chuang Bo Sang Sunghwan Yoo Rui Gu Charles Killian Milind Kulkarni,@inproceedings{41668 title = {EventWave: Programming Model and Runtime Support for Tightly-Coupled Elastic Cloud Applications} author = {Wei-Chiu Chuang and Bo Sang and Sunghwan Yoo and Rui Gu and Charles Killian and Milind Kulkarni} year = 2013 URL = {http://www.socc2013.org/home/program/a21-chuang.pdf?attredirects=0} booktitle = {Proceedings of the 2013 ACM Symposium on Cloud Computing} address = {Santa Clara CA USA} },An attractive approach to leveraging the ability of cloud-computing platforms to provide resources on demand is to build elastic applications which can dynamically scale up or down based on resource requirements. To ease the development of elastic applications it is useful for programmers to write applications with simple sequential semantics without considering elasticity and rely on runtime support to provide that elasticity. While this approach has been useful in restricted domains such as MapReduce existing programming models for general distributed applications do not expose enough information about their inherent organization of state and computation to provide such transparent elasticity. We introduce EVENTWAVE an event-driven programming model that allows developers to design elastic programs with inelastic semantics while naturally exposing isolated state and computation with programmatic parallelism. In addition we describe the runtime mechanism which takes the exposed parallelism to provide elasticity. Finally we evaluate our implementation through microbenchmarks and case studies to demonstrate that EVENTWAVE can provide efﬁcient scalable transparent elasticity for applications run in the cloud.,http://research.google.com/pubs/archive/41668.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=EventWave:+Programming+Model+and+Runtime+Support+for++Tightly-Coupled+Elastic+Cloud+Applications+Chuang+Sang+Yoo+Gu+Killian+Kulkarni,http://research.google.com/pubs/pub41668.html
Machine Learning in an Auction Environment,Proceedings of the 23rd International Conference on the World Wide Web (WWW) (2014) pp. 7-18,2014,Patrick Hummel Preston McAfee,@inproceedings{42434 title = {Machine Learning in an Auction Environment} author = {Patrick Hummel and Preston McAfee} year = 2014 URL = {http://wwwconference.org/proceedings/www2014/proceedings/p7.pdf} booktitle = {Proceedings of the 23rd International Conference on the World Wide Web (WWW)} pages = {7-18} },We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration into a machine learning system in an auction environment is exceedingly small.,http://wwwconference.org/proceedings/www2014/proceedings/p7.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Machine+Learning+in+an+Auction+Environment+Hummel+McAfee,http://research.google.com/pubs/pub42434.html
Metric Embeddings with Relaxed Guarantees,SIAM Journal on Computing vol. 38 (2009) pp. 2303-2329,2009,T H Hubert Chan Kedar Dhamdhere Anupam Gupta Jon m Kleinberg Aleksandrs Slivkins,@article{36235 title = {Metric Embeddings with Relaxed Guarantees} author = {T H Hubert Chan and Kedar Dhamdhere and Anupam Gupta and Jon m Kleinberg and Aleksandrs Slivkins} year = 2009 journal = {SIAM Journal on Computing} pages = {2303--2329} volume = {38} },We consider the problem of embedding finite metrics with slack: We seek to produce embeddings with small dimension and distortion while allowing a (small) constant fraction of all distances to be arbitrarily distorted. This definition is motivated by recent research in the networking community which achieved striking empirical success at embedding Internet latencies with low distortion into low-dimensional Euclidean space provided that some small slack is allowed. Answering an open question of Kleinberg Slivkins and Wexler [in Proceedings of the 45th IEEE Symposium on Foundations of Computer Science 2004] we show that provable guarantees of this type can in fact be achieved in general: Any finite metric space can be embedded with constant slack and constant distortion into constant-dimensional Euclidean space. We then show that there exist stronger embeddings into $\ell_1$ which exhibit gracefully degrading distortion: There is a single embedding into $\ell_1$ that achieves distortion at most $O(\log\frac{1}{\epsilon})$ on all but at most-1.5pt an $\epsilon$ fraction of distances simultaneously for all $\epsilon>0$. We extend this with distortion1pt $O(\log\frac{1}{\epsilon})^{1/p}$ to maps into general $\ell_p$ $p\geq1$ for several classes of metrics including those with bounded doubling dimension and those arising from the shortest-path metric of a graph with an excluded minor. Finally we show that many of our constructions are tight and give a general technique to obtain lower bounds for $\epsilon$-slack embeddings from lower bounds for low-distortion embeddings.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Metric+Embeddings+with+Relaxed+Guarantees+Chan+Dhamdhere+Gupta+Kleinberg+Slivkins,http://research.google.com/pubs/pub36235.html
YouTube's Collaborative Annotations,Webcentives (2009) pp. 18-19,2009,Michael Fink Sigalit Bar Aviad Bazilai Nir Kerem Isaac Elias Julian Frumar Herb Ho Ryan Junee Simon Ratner Jasson Schrock Ran Tavory,@inproceedings{36735 title = {YouTube's Collaborative Annotations} author = {Michael Fink and Sigalit Bar and Aviad Bazilai and Nir Kerem and Isaac Elias and Julian Frumar and Herb Ho and Ryan Junee and Simon Ratner and Jasson Schrock and Ran Tavory} year = 2009 booktitle = {Webcentives} pages = {18--19} },More and more YouTube videos no longer provide a passive viewing experience but rather entice the viewer to interact with the video by clicking on objects with embedded links. These links are part of YouTube’s Annotations system which enables content owners to add active overlays on top of their videos. YouTube Annotation overlays also enable adding dynamic speech bubbles and pop-ups which can function as an ever-changing layer of supplementary information and entertainment augmenting the video experience. This paper addresses the question of whether the ability to add annotation overlays on a given video should be opened to the YouTube public. The basic dilemma in opening a video to collaborative annotations is derived from the tension between the benefits of collaboration and the risks of visual clutter and spam. We term the degree to which a video is open to external contributions as the collaboration spectrum and describe several models that let content owners to explore this spectrum in order to find the optimal way to harness the power of the masses.,http://research.google.com/pubs/archive/36735.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=YouTube's+Collaborative+Annotations+Fink+Bar+Bazilai+Kerem+Elias+Frumar+Ho+Junee+Ratner+Schrock+Tavory,http://research.google.com/pubs/pub36735.html
Origin-Bound Certificates: A Fresh Approach to Strong Client Authentication for the Web,21st USENIX Security Symposium The USENIX Association (2012) pp. 317-332,2012,Michael Dietz Alexei Czeskis Dirk Balfanz Dan Wallach,@inproceedings{38357 title = {Origin-Bound Certificates: A Fresh Approach to Strong Client Authentication for the Web} author = {Michael Dietz and Alexei Czeskis and Dirk Balfanz and Dan Wallach} year = 2012 URL = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final162.pdf} booktitle = {21st USENIX Security Symposium} pages = {317-332} },Client authentication on the web has remained in the internet-equivalent of the stone ages for the last two decades. Instead of adopting modern public-key-based authentication mechanisms we seem to be stuck with passwords and cookies. In this paper we propose to break this stalemate by presenting a fresh approach to public-key-based client authentication on the web. We describe a simple TLS extension that allows clients to establish strong authenti- cated channels with servers and to bind existing authen- tication tokens like HTTP cookies to such channels. This allows much of the existing infrastructure of the web to remain unchanged while at the same time strengthening client authentication considerably against a wide range of attacks. We implemented our system in Google Chrome and Google’s web serving infrastructure and provide a per- formance evaluation of this implementation.,http://research.google.com/pubs/archive/38357.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Origin-Bound+Certificates:+A+Fresh+Approach+to+Strong+Client+Authentication+for+the+Web+Dietz+Czeskis+Balfanz+Wallach,http://research.google.com/pubs/pub38357.html
Beyond Short Snippets: Deep Networks for Video Classification,Computer Vision and Pattern Recognition (2015),2015,Joe Yue-Hei Ng Matthew Hausknecht Sudheendra Vijayanarasimhan Oriol Vinyals Rajat Monga George Toderici,@inproceedings{43793 title = {Beyond Short Snippets: Deep Networks for Video Classification} author = {Joe Yue-Hei Ng and Matthew Hausknecht and Sudheendra Vijayanarasimhan and Oriol Vinyals and Rajat Monga and George Toderici} year = 2015 booktitle = {Computer Vision and Pattern Recognition} },Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition detection segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).,http://research.google.com/pubs/archive/43793.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Beyond+Short+Snippets:+Deep+Networks+for+Video+Classification+Ng+Hausknecht+Vijayanarasimhan+Vinyals+Monga+Toderici,http://research.google.com/pubs/pub43793.html
Data Enriched Linear Regression,Electronic Journal of Statistics vol. 9 (2015) pp. 1078-1112 (to appear),2015,Aiyou Chen Art Owen Minghui Shi,@article{41010 title = {Data Enriched Linear Regression} author = {Aiyou Chen and Art Owen and Minghui Shi} year = 2015 URL = {http://projecteuclid.org/euclid.ejs/1432732305} journal = {Electronic Journal of Statistics} pages = {1078-1112} volume = {9} },We present a linear regression method for predictions on a small data set making use of a second possibly biased data set that may be much larger. Our method fits linear regressions to the two data sets while penalizing the difference between predictions made by those two models. The resulting algorithm is a shrinkage method similar to those used in small area estimation. We find a Stein-type result for Gaussian responses: when the model has 5 or more coefficients and 10 or more error degrees of freedom it becomes inadmissible to use only the small data set no matter how large the bias is. We also present both plug-in and AICc-based methods to tune our penalty parameter. Most of our results use an L2 penalty but we obtain formulas for L1 penalized estimates when the model is specialized to the location setting. Ordinary Stein shrinkage provides an inadmissibility result for only 3 or more coefficients but we find that our shrinkage method typically produces much lower squared errors in as few as 5 or 10 dimensions when the bias is small and essentially equivalent squared errors when the bias is large.,http://research.google.com/pubs/archive/41010.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Data+Enriched+Linear+Regression+Chen+Owen+Shi,http://research.google.com/pubs/pub41010.html
Auto-Rectification of User Photos,Proceedings of International Conference on Image Processing ICIP IEEE (2014) pp. 3479-3483,2014,Krishnendu Chaudhury (aka Krish Chaudhury) Stephen DiVerdi Sergey Ioffe,@inproceedings{42532 title = {Auto-Rectification of User Photos} author = {Krishnendu Chaudhury (aka Krish Chaudhury) and Stephen DiVerdi and Sergey Ioffe} year = 2014 booktitle = {Proceedings of International Conference on Image Processing ICIP} pages = {3479-3483} },The image auto rectification project at Google aims to create a pleasanter version of user photos by correcting the small involuntary camera rotations (roll / pitch/ yaw) that often occur in non-professional photographs. Our system takes the image closer to the fronto-parallel view by performing an affine rectification on the image that restores parallelism of lines that are parallel in the fronto-parallel image view. This partially corrects perspective distortions but falls short of full metric rectification which also restores angles between lines. On the other hand the 2D homography for our rectification can be computed from only two (as opposed to three) estimated vanishing points allowing us to fire upon many more images. A new RANSAC based approach to vanishing point estimation has been developed. The main strength of our vanishing point detector is that it is line-less thereby avoiding the hard binary (line/no-line) upstream decisions that cause traditional algorithm to ignore much supporting evidence and/or admit noisy evidence for vanishing points. A robust RANSAC based technique for detecting horizon lines in an image is also proposed for analyzing correctness of the estimated rectification. We post-multiply our affine rectification homography with a 2D rotation which aligns the closer vanishing point with the image Y axis.,http://research.google.com/pubs/archive/42532.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Auto-Rectification+of+User+Photos+Chaudhury+DiVerdi+Ioffe,http://research.google.com/pubs/pub42532.html
Inferring causal impact using Bayesian structural time-series models,Annals of Applied Statistics vol. 9 (2015) pp. 247-274,2015,Kay H. Brodersen Fabian Gallusser Jim Koehler Nicolas Remy Steven L. Scott,@article{41854 title = {Inferring causal impact using Bayesian structural time-series models} author = {Kay H. Brodersen and Fabian Gallusser and Jim Koehler and Nicolas Remy and Steven L. Scott} year = 2015 journal = {Annals of Applied Statistics} pages = {247--274} volume = {9} },An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. In order to allocate a given budget optimally for example an advertiser must assess to what extent different campaigns have contributed to an incremental lift in web searches product installs or sales. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes state-space models make it possible to (i) infer the temporal evolution of attributable impact (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment and (iii) flexibly accommodate multiple sources of variation including the time-varying influence of contemporaneous covariates i.e. synthetic controls. Using a Markov chain Monte Carlo algorithm for model inversion we illustrate the statistical properties of our approach on synthetic data. We then demonstrate its practical utility by evaluating the effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach.,http://research.google.com/pubs/archive/41854.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Inferring+causal+impact+using+Bayesian+structural+time-series+models+Brodersen+Gallusser+Koehler+Remy+Scott,http://research.google.com/pubs/pub41854.html
A Discriminative Latent Variable Model for Online Clustering,International Conference on Machine Learning (2014) (to appear),2014,Rajhans Samdani Kai-Wei Chang Dan Roth,@inproceedings{42183 title = {A Discriminative Latent Variable Model for Online Clustering} author = {Rajhans Samdani and Kai-Wei Chang and Dan Roth} year = 2014 URL = {http://jmlr.org/proceedings/papers/v32/samdani14.pdf} booktitle = {International Conference on Machine Learning} },This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering L3M outperforms several existing online as well as batch supervised clustering techniques.,http://research.google.com/pubs/archive/42183.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Discriminative+Latent+Variable+Model+for+Online+Clustering+Samdani+Chang+Roth,http://research.google.com/pubs/pub42183.html
Position Auctions with Dynamic Resizing,International Journal of Industrial Organization vol. 45 (2016) pp. 38-46,2016,Patrick Hummel,@article{44807 title = {Position Auctions with Dynamic Resizing} author = {Patrick Hummel} year = 2016 URL = {http://www.sciencedirect.com/science/article/pii/S016771871500140X} journal = {International Journal of Industrial Organization} pages = {38-46} volume = {45} },This paper analyzes mechanisms for selling advertising inventory in a position auction in which displaying less than the maximal number of ads means the ads that are shown can be dynamically resized and displayed more prominently. I characterize the optimal mechanism with and without dynamic resizing and illustrate how the optimal reserve prices in a Vickrey–Clarke–Groves mechanism vary with the amount of dynamic resizing and the number of bidders.,http://www.sciencedirect.com/science/article/pii/S016771871500140X,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Position+Auctions+with+Dynamic+Resizing+Hummel,http://research.google.com/pubs/pub44807.html
Federated Optimization: Distributed Optimization Beyond the Datacenter,NIPS Optimization for Machine Learning Workshop (2015) pp. 5,2015,Jakub Kone_n_ H. Brendan McMahan Daniel Ramage,@misc{44310 title = {Federated Optimization: Distributed Optimization Beyond the Datacenter} author = {Jakub Kone_n_ and H. Brendan McMahan and Daniel Ramage} year = 2015 URL = {http://arxiv.org/pdf/1511.03575v1.pdf} },We introduce a new and increasingly relevant setting for distributed optimization in machine learning where the data defining the optimization are distributed (unevenly) over an extremely large number of nodes but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network each of which has only a tiny fraction of data available totally; in particular we expect the number of data points available locally to be much smaller than the number of devices. Additionally since different users generate data with different patterns we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization.,http://arxiv.org/pdf/1511.03575v1.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Federated+Optimization:+Distributed+Optimization+Beyond+the+Datacenter+Kone%C4%8Dn%C3%BD+McMahan+Ramage,http://research.google.com/pubs/pub44310.html
Evaluating Online Ad Campaigns in a Pipeline: Causal Models at Scale,Proceedings of ACM SIGKDD 2010 pp. 7-15,2010,David Chan Rong Ge Ori Gershony Tim Hesterberg Diane Lambert,@inproceedings{36552 title = {Evaluating Online Ad Campaigns in a Pipeline: Causal Models at Scale} author = {David Chan and Rong Ge and Ori Gershony and Tim Hesterberg and Diane Lambert} year = 2010 booktitle = {Proceedings of ACM SIGKDD 2010} pages = {7-15} },Display ads proliferate on the web but are they effective? Or are they irrelevant in light of all the other advertising that people see? We describe a way to answer these questions quickly and accurately without randomized experiments surveys focus groups or expert data analysts. Doubly robust estimation protects against the selection bias that is inherent in observational data and a nonparametric test that is based on irrelevant outcomes provides further defense. Simulations based on realistic scenarios show that the resulting estimates are more robust to selection bias than traditional alternatives such as regression modeling or propensity scoring. Moreover computations are fast enough that all processing from data retrieval through estimation testing validation and report generation proceeds in an automated pipeline without anyone needing to see the raw data.,http://research.google.com/pubs/archive/36552.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Evaluating+Online+Ad+Campaigns+in+a+Pipeline:+Causal+Models+at+Scale+Chan+Ge+Gershony+Hesterberg+Lambert,http://research.google.com/pubs/pub36552.html
A Green Display for the Internet,Renewable Energy and the Environment Optical Society of America (2013),2013,Ken Foo Bill Hamburgen Jim Zhuang,@inproceedings{41605 title = {A Green Display for the Internet} author = {Ken Foo and Bill Hamburgen and Jim Zhuang} year = 2013 URL = {https://docs.google.com/a/google.com/file/d/0BySPthXLikL2T2d2aUZKSDdMcVE/edit} booktitle = {Renewable Energy and the Environment} },In typical use a liquid crystal display (LCD) with high resolution brightness and color saturation can consume over half the total system power in a modern mobile device. This paper examines LCD optical transmittance and system electrical design to extend battery run time. By applying a solid understanding of critical optical parameters and complementary system design a low power “Green” display can be achieved. The LCD in the Pixel Chromebook [1] will be used as a baseline for discussion.,http://research.google.com/pubs/archive/41605.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Green+Display+for+the+Internet+Foo+Hamburgen+Zhuang,http://research.google.com/pubs/pub41605.html
Design Implementation and Verification of an eXtensible and Modular Hypervisor Framework,IEEE Symposium on Security and Privacy (2013) (to appear),2013,Amit Vasudevan Sagar Chaki Limin Jia Jonathan McCune James Newsome Anupam Datta,@inproceedings{40806 title = {Design Implementation and Verification of an eXtensible and Modular Hypervisor Framework} author = {Amit Vasudevan and Sagar Chaki and Limin Jia and Jonathan McCune and James Newsome and Anupam Datta} year = 2013 booktitle = {IEEE Symposium on Security and Privacy} },We present the design implementation and verification of XMHF - an eXtensible and Modular Hypervisor Framework. XMHF is designed to achieve three goals - modular extensibility automated verification and high performance. XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or functional properties while preserving the fundamental hypervisor security property of memory integrity (i.e. ensuring that the hypervisor's memory is not modified by software running at a lower privilege level). We verify the memory integrity of the XMHF core - 6018 lines of code - using a combination of automated and manual techniques. The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM. We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds. Our experiments indicate that XMHF's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Design+Implementation+and+Verification+of+an+eXtensible+and+Modular+Hypervisor+Framework+Vasudevan+Chaki+Jia+McCune+Newsome+Datta,http://research.google.com/pubs/pub40806.html
The Case Against Data Lock-in,Communications of the ACM vol. 53 No.11 (2010) pp. 42-46,2010,Brian W. Fitzpatrick JJ Lueck,@article{37230 title = {The Case Against Data Lock-in} author = {Brian W. Fitzpatrick and JJ Lueck} year = 2010 URL = {http://queue.acm.org/detail.cfm?id=1868432} journal = {Communications of the ACM} pages = {42-46} volume = {53 No.11} },Want to keep your users? Just make it easy for them to leave.,http://queue.acm.org/detail.cfm?id=1868432,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Case+Against+Data+Lock-in+Fitzpatrick+Lueck,http://research.google.com/pubs/pub37230.html
How Much Software Testing is Enough,Communications of the ACM vol. 53. No 9 (2010) pp. 9,2010,Ruben Ortega,@article{36667 title = {How Much Software Testing is Enough} author = {Ruben Ortega} year = 2010 journal = {Communications of the ACM} pages = {9} volume = {53. No 9} },Software and Test-Driven Development,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Much+Software+Testing+is+Enough+Ortega,http://research.google.com/pubs/pub36667.html
Dremel: Interactive Analysis of Web-Scale Datasets,Communications of the ACM vol. 54 (2011) pp. 114-123,2011,Sergey Melnik Andrey Gubarev Jing Jing Long Geoffrey Romer Shiva Shivakumar Matt Tolton Theodore Vassilakis,@article{37217 title = {Dremel: Interactive Analysis of Web-Scale Datasets} author = {Sergey Melnik and Andrey Gubarev and Jing Jing Long and Geoffrey Romer and Shiva Shivakumar and Matt Tolton and Theodore Vassilakis} year = 2011 URL = {http://cacm.acm.org/magazines/2011/6/108648-dremel-interactive-analysis-of-web-scale-datasets/fulltext} journal = {Communications of the ACM} pages = {114-123} volume = {54} },Dremel is a scalable interactive ad hoc query system for analysis of read-only nested data. By combining multilevel execution trees and columnar data layout it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data and has thousands of users at Google. In this paper we describe the architecture and implementation of Dremel and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.,http://research.google.com/pubs/archive/37217.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dremel:+Interactive+Analysis+of+Web-Scale+Datasets+Melnik+Gubarev+Long+Romer+Shivakumar+Tolton+Vassilakis,http://research.google.com/pubs/pub37217.html
Team Geek: A Software Developer's Guide to Working Well with Others,O'Reilly Media 1005 Gravenstein Highway North Sebastopol CA 95472 (2012) pp. 180,2012,Brian W. Fitzpatrick Ben Collins-Sussman,@book{38144 title = {Team Geek: A Software Developer's Guide to Working Well with Others} author = {Brian W. Fitzpatrick and Ben Collins-Sussman} year = 2012 URL = {http://shop.oreilly.com/product/0636920018025.do} pages = {180} address = {1005 Gravenstein Highway North Sebastopol CA 95472} },"As a software engineer you’re great with computer languages compilers debuggers and algorithms. And in a perfect world those who produce the best code are the most successful. But in our perfectly messy world success also depends on how you work with people to get your job done. In this highly entertaining book Brian Fitzpatrick and Ben Collins-Sussman cover basic patterns and anti-patterns for working with other people teams and users while trying to develop software. It’s valuable information from two respected software engineers whose popular video series ""Working with Poisonous People"" has attracted hundreds of thousands of viewers. You’ll learn how to deal with imperfect people—those irrational and unpredictable beings—in the course of your work. And you’ll discover why playing well with others is at least as important as having great technical skills. By internalizing the techniques in this book you’ll get more software written be more influential be happier in your career.",http://shop.oreilly.com/product/0636920018025.do,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Team+Geek:+A+Software+Developer's+Guide+to+Working+Well+with+Others+Fitzpatrick+Collins-Sussman,http://research.google.com/pubs/pub38144.html
Online Python Tutor: Embeddable Web-Based Program Visualization for CS Education,Proceedings of the ACM Technical Symposium on Computer Science Education (SIGCSE) ACM (2013) (to appear),2013,Philip Guo,@inproceedings{40591 title = {Online Python Tutor: Embeddable Web-Based Program Visualization for CS Education} author = {Philip Guo} year = 2013 booktitle = {Proceedings of the ACM Technical Symposium on Computer Science Education (SIGCSE)} },This paper presents Online Python Tutor a web-based program visualization tool for Python which is becoming a popular language for teaching introductory CS courses. Using this tool teachers and students can write Python programs directly in the web browser (without installing any plugins) step forwards and backwards through execution to view the run-time state of data structures and share their program visualizations on the web. In the past three years over 200000 people have used Online Python Tutor to visualize their programs. In addition instructors in a dozen universities such as UC Berkeley MIT the University of Washington and the University of Waterloo have used it in their CS1 courses. Finally Online Python Tutor visualizations have been embedded within three web-based digital Python textbook projects which collectively attract around 16000 viewers per month and are being used in at least 25 universities. Online Python Tutor is free and open source software available at pythontutor.com,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Online+Python+Tutor:+Embeddable+Web-Based+Program+Visualization+for+CS+Education+Guo,http://research.google.com/pubs/pub40591.html
Training a Parser for Machine Translation Reordering,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP '11),2011,Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och David Talbot Hiroshi Ichikawa Masakazu Seno,@inproceedings{37159 title = {Training a Parser for Machine Translation Reordering} author = {Jason Katz-Brown and Slav Petrov and Ryan McDonald and Franz Och and David Talbot and Hiroshi Ichikawa and Masakazu Seno} year = 2011 URL = {http://petrovi.de/data/emnlp11b.pdf} booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP '11)} },We propose a simple training regime that can improve the extrinsic performance of a parser given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.,http://research.google.com/pubs/archive/37159.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Training+a+Parser+for+Machine+Translation+Reordering+Katz-Brown+Petrov+McDonald+Och+Talbot+Ichikawa+Seno,http://research.google.com/pubs/pub37159.html
On the necessity of irrelevant variables,ICML (2011),2011,David P. Helmbold Philip M. Long,@inproceedings{37074 title = {On the necessity of irrelevant variables} author = {David P. Helmbold and Philip M. Long} year = 2011 URL = {http://www.phillong.info/publications/HL11_features.pdf} note = {An updated version of the paper is available at http://www.phillong.info/publications/HL12_features.pdf.} booktitle = {ICML} },This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.,http://www.phillong.info/publications/HL11_features.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+necessity+of+irrelevant+variables+Helmbold+Long,http://research.google.com/pubs/pub37074.html
Quantitative Analysis of Culture Using Millions of Digitized Books,Science (2010),2010,Jean-Baptiste Michel Yuan Kui Shen Aviva Presser Aiden Adrian Veres Matthew K. Gray The Google Books Team Joseph P. Pickett Dale Holberg Dan Clancy Peter Norvig Jon Orwant Steven Pinker Martin A. Nowak Erez Lieberman Aiden,@article{37388 title = {Quantitative Analysis of Culture Using Millions of Digitized Books} author = {Jean-Baptiste Michel and Yuan Kui Shen and Aviva Presser Aiden and Adrian Veres and Matthew K. Gray and The Google Books Team and Joseph P. Pickett and Dale Holberg and Dan Clancy and Peter Norvig and Jon Orwant and Steven Pinker and Martin A. Nowak and Erez Lieberman Aiden} year = 2010 URL = {http://www.sciencemag.org/content/331/6014/176.full} journal = {Science} },We constructed a corpus of digitized texts containing about 4% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of ‘culturomics’ focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography the evolution of grammar collective memory the adoption of technology the pursuit of fame censorship and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.,http://www.sciencemag.org/content/331/6014/176.full,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Quantitative+Analysis+of+Culture+Using+Millions+of+Digitized+Books+Michel+Shen+Aiden+Veres+Gray+Team+Pickett+Holberg+Clancy+Norvig+Orwant+Pinker+Nowak+Aiden,http://research.google.com/pubs/pub37388.html
RFC6583 - Operational Neighbor Discovery Problems,IETF RFCs Internet Engineering Task Force (2012),2012,Warren Kumari Igor Gashinsky Yahoo! Joel Jaeggli Zynga,@incollection{38101 title = {RFC6583 - Operational Neighbor Discovery Problems} author = {Warren Kumari and Igor Gashinsky Yahoo! and Joel Jaeggli Zynga} year = 2012 URL = {http://tools.ietf.org/rfc/rfc6583.txt} booktitle = {IETF RFCs} },"In IPv4 subnets are generally small made just large enough to cover the actual number of machines on the subnet. In contrast the default IPv6 subnet size is a /64 a number so large it covers trillions of addresses the overwhelming number of which will be unassigned. Consequently simplistic implementations of Neighbor Discovery (ND) can be vulnerable to deliberate or accidental denial of service (DoS) whereby they attempt to perform address resolution for large numbers of unassigned addresses. Such denial-of-service attacks can be launched intentionally (by an attacker) or result from legitimate operational tools or accident conditions. As a result of these vulnerabilities new devices may not be able to ""join"" a network it may be impossible to establish new IPv6 flows and existing IPv6 transported flows may be interrupted. This document describes the potential for DoS in detail and suggests possible implementation improvements as well as operational mitigation techniques that can in some cases be used to protect against or at least alleviate the impact of such attacks.",http://research.google.com/pubs/archive/38101.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=RFC6583+-+Operational+Neighbor+Discovery+Problems+Kumari+Yahoo!+Zynga,http://research.google.com/pubs/pub38101.html
Disks for Data Centers,Google (2016) pp. 1-16,2016,Eric Brewer Lawrence Ying Lawrence Greenfield Robert Cypher Theodore T'so,@techreport{44830 title = {Disks for Data Centers} author = {Eric Brewer and Lawrence Ying and Lawrence Greenfield and Robert Cypher and Theodore T'so} year = 2016 institution = {Google} },Disks form the central element of Cloud-based storage whose demand far outpaces the considerable rate of innovation in disks. Exponential growth in demand already in progress for 15+ years implies that most future disks will be in data centers and thus part of a large collection of disks. We describe the “collection view” of disks and how it and the focus on tail latency driven by live services place new and different requirements on disks. Beyond defining key metrics for data-center disks we explore a range of new physical design options and changes to firmware that could improve these metrics. We hope this is the beginning of a new era of “data center” disks and a new broad and open discussion about how to evolve disks for data centers. The ideas presented here provide some guidance and some options but we believe the best solutions will come from the combined efforts of industry academia and other large customers.,http://research.google.com/pubs/archive/44830.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Disks+for+Data+Centers+Brewer+Ying+Greenfield+Cypher+Ts'o,http://research.google.com/pubs/pub44830.html
Theoretical Convergence Guarantees for Cooperative Coevolutionary Algorithms,Evolutionary Computation Journal (2010),2010,Liviu Panait,@article{33013 title = {Theoretical Convergence Guarantees for Cooperative Coevolutionary Algorithms} author = {Liviu Panait} year = 2010 journal = {Evolutionary Computation Journal} },Cooperative coevolutionary algorithms have the potential to significantly speed up the search process by dividing the space into parts that can be each conquered separately. Unfortunately recent research presented theoretical and empirical arguments that these algorithms might not be fit for optimization tasks as they might tend to drift to suboptimal solutions in the search space. This paper details an extended formal model for cooperative coevolutionary algorithms and uses it to demonstrate that these algorithms will converge to the globally optimal solution if properly set and if given enough resources. We also present an intuitive graphical visualization for the basins of attraction to optimal and suboptimal solutions in the search space.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Theoretical+Convergence+Guarantees+for+Cooperative+Coevolutionary+Algorithms+Panait,http://research.google.com/pubs/pub33013.html
From Assets to Stories via the Google Cultural Institute Platform,IEEE BigData'13 Big Data and the Humanities (2013) pp. 6 (to appear),2013,W. Brent Seales Steve Crossan Sertan Girgin Mark Yoshitake,@inproceedings{41442 title = {From Assets to Stories via the Google Cultural Institute Platform} author = {W. Brent Seales and Steve Crossan and Sertan Girgin and Mark Yoshitake} year = 2013 booktitle = {IEEE BigData'13 Big Data and the Humanities} pages = {6} },The Google Cultural Institute Platform scale system for ingesting archiving organizing and interacting with digital assets of cultural material. This paper explains the components through which the platform contextualizes individual assets in order to enable storytelling. Contextualization is an inverse problem: given assets that are instances of cultural material infer their precise context and use that as a way to support the storytelling process. The approach is based on three components: extraction knowledge and scale. Extraction is the inference of context from two sources of information: explicitly provided metadata and automatically extracted features. Knowledge is the use of a large refer- ence fact database for further contextualizing an asset based on its descriptors. And scale achieved through global self-serve enables massively expanded coverage of the knowledge database and crowdsource potential for metadata reﬁnement. Together these components sustain a storytelling framework and a compelling user experience that has the potential to become the largest repository of cultural information and coherent narrative in history.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=From+Assets+to+Stories+via+the+Google+Cultural+Institute+Platform+Seales+Crossan+Girgin+Yoshitake,http://research.google.com/pubs/pub41442.html
Translation-Inspired OCR,ICDAR-2011,2011,Dmitriy Genzel Ashok C. Popat Nemanja Spasojevic Michael Jahr Andrew Senior Eugene Ie Frank Yung-Fong Tang,@inproceedings{37260 title = {Translation-Inspired OCR} author = {Dmitriy Genzel and Ashok C. Popat and Nemanja Spasojevic and Michael Jahr and Andrew Senior and Eugene Ie and Frank Yung-Fong Tang} year = 2011 booktitle = {ICDAR-2011} },Optical character recognition is carried out using techniques borrowed from statistical machine translation. In particular the use of multiple simple feature functions in linear combination along with minimum-error-rate training integrated decoding and $N$-gram language modeling is found to be remarkably effective across several scripts and languages. Results are presented using both synthetic and real data in five languages.,http://research.google.com/pubs/archive/37260.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Translation-Inspired+OCR+Genzel+Popat+Spasojevic+Jahr+Senior+Ie+Tang,http://research.google.com/pubs/pub37260.html
Distributed Discriminative Language Models for Google Voice Search,Proceedings of ICASSP 2012 IEEE pp. 5017-5021,2012,Preethi Jyothi Leif Johnson Ciprian Chelba Brian Strope,@inproceedings{37682 title = {Distributed Discriminative Language Models for Google Voice Search} author = {Preethi Jyothi and Leif Johnson and Ciprian Chelba and Brian Strope} year = 2012 booktitle = {Proceedings of ICASSP 2012} pages = {5017--5021} },This paper considers large-scale linear discriminative language models trained using a distributed perceptron algorithm. The algorithm is implemented efficiently using a MapReduce/SSTable framework. This work also introduces the use of large amounts of unsupervised data (confidence filtered Google voice-search logs) in conjunction with a novel training procedure that regenerates word lattices for the given data with a weaker acoustic model than the one used to generate the unsupervised transcriptions for the logged data. We observe small but statistically significant improvements in recognition performance after reranking N-best lists of a standard Google voice-search data set.,http://research.google.com/pubs/archive/37682.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+Discriminative+Language+Models+for+Google+Voice+Search+Jyothi+Johnson+Chelba+Strope,http://research.google.com/pubs/pub37682.html
Quizz: Targeted Crowdsourcing with a Billion (Potential) Users,WWW (2014) (to appear),2014,Panos Ipeirotis Evgeniy Gabrilovich,@inproceedings{42022 title = {Quizz: Targeted Crowdsourcing with a Billion (Potential) Users} author = {Panos Ipeirotis and Evgeniy Gabrilovich} year = 2014 booktitle = {WWW} },We describe Quizz a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them. Quizz operates by asking users to complete short quizzes on specific topics; as a user answers the quiz questions Quizz estimates the user's competence. To acquire new knowledge Quizz also incorporates questions for which we do not have a known answer; the answers given by competent users provide useful signals for selecting the correct answers for these questions. Quizz actively tries to identify knowledgeable users on the Internet by running advertising campaigns effectively leveraging the targeting capabilities of existing publicly available ad placement services. Quizz quantifies the contributions of the users using information theory and sends feedback to the advertising system about each user. The feedback allows the ad targeting mechanism to further optimize ad placement. Our experiments which involve over ten thousand users confirm that we can crowdsource knowledge curation for niche and specialized topics as the advertising network can automatically identify users with the desired expertise and interest in the given topic. We present controlled experiments that examine the effect of various incentive mechanisms highlighting the need for having short-term rewards as goals which incentivize the users to contribute. Finally our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms while offering the additional advantage of giving access to billions of potential users all over the planet and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces.,http://research.google.com/pubs/archive/42022.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Quizz:+Targeted+Crowdsourcing+with+a+Billion+(Potential)+Users+Ipeirotis+Gabrilovich,http://research.google.com/pubs/pub42022.html
Discriminative Tag Learning on YouTube Videos with Latent Sub-tags,Computer Vision and Pattern Recognition IEEE (2011),2011,Weilong Yang George Toderici,@inproceedings{36931 title = {Discriminative Tag Learning on YouTube Videos with Latent Sub-tags} author = {Weilong Yang and George Toderici} year = 2011 booktitle = {Computer Vision and Pattern Recognition} },We consider the problem of content-based automated tag learning. In particular we address semantic varia- tions (sub-tags) of the tag. Each video in the training set is assumed to be associated with a sub-tag label and we treat this sub-tag label as latent information. A latent learning framework based on LogitBoost is proposed which jointly considers both tag label and the latent sub-tag label. The latent sub-tag information is exploited in our frame- work to assist the learning of our end goal i.e. tag predic- tion. We use the cowatch information to initialize the learn- ing process. In experiments we show that the proposed method achieves signiﬁcantly better results over baselines on a large-scale testing video set which contains about 50 million YouTube videos.,http://research.google.com/pubs/archive/36931.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Discriminative+Tag+Learning+on+YouTube+Videos+with+Latent+Sub-tags+Yang+Toderici,http://research.google.com/pubs/pub36931.html
Limits on the Application of Frequency-based Language Models to OCR,ICDAR IEEE (2011) pp. 538-542,2011,Ray Smith,@inproceedings{36984 title = {Limits on the Application of Frequency-based Language Models to OCR} author = {Ray Smith} year = 2011 note = {Won Best Industrial Paper Award} booktitle = {ICDAR} pages = {538-542} },Although large language models are used in speech recognition and machine translation applications OCR systems are “far behind” in their use of language models. The reason for this is not the laggardness of the OCR community but the fact that at high accuracies a frequency-based language model can do more damage than good unless carefully applied. This paper presents an analysis of this discrepancy with the help of the Google Books n-gram Corpus and concludes that noisy-channel models that closely model the underlying classifier and segmentation errors are required.,http://research.google.com/pubs/archive/36984.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Limits+on+the+Application+of+Frequency-based+Language+Models+to+OCR+Smith,http://research.google.com/pubs/pub36984.html
How Developers Use Data Race Detection Tools,Evaluation and Usability of Programming Languages and Tools (PLATEAU) ACM (2014),2014,Caitlin Sadowski Jaeheon Yi,@inproceedings{43217 title = {How Developers Use Data Race Detection Tools} author = {Caitlin Sadowski and Jaeheon Yi} year = 2014 URL = {http://dx.doi.org/10.1145/2688204.2688205} booktitle = {Evaluation and Usability of Programming Languages and Tools (PLATEAU)} },Developers need help with multithreaded programming. We investigate how two program analysis tools are used by developers at Google: ThreadSafety an annotation-based static data race analysis and TSan a dynamic data race detector. The data was collected by interviewing seven veteran industry developers at Google and provides unique insight into how four different teams use tooling in different ways to help with multithreaded programming. The result is a collection of perceived pros and cons of using ThreadSafety and TSan as well as general issues with multithreading.,http://research.google.com/pubs/archive/43217.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=How+Developers+Use+Data+Race+Detection+Tools+Sadowski+Yi,http://research.google.com/pubs/pub43217.html
The Virtues of Peer Pressure: A Simple Method for Discovering High-Value Mistakes,International Conference on Computer Analysis of Images and Patterns (2015),2015,Shumeet Baluja Michele Covell Rahul Sukthankar,@inproceedings{43986 title = {The Virtues of Peer Pressure: A Simple Method for Discovering High-Value Mistakes} author = {Shumeet Baluja and Michele Covell and Rahul Sukthankar} year = 2015 URL = {http://www.esprockets.com/papers/caip2015.pdf} booktitle = {International Conference on Computer Analysis of Images and Patterns} },Much of the recent success of neural networks can be attributed to the deeper architectures that have become prevalent. However the deeper architectures often yield unintelligible solutions require enormous amounts of labeled data and still remain brittle and easily broken. In this paper we present a method to efficiently and intuitively discover input instances that are misclassified by well-trained neural networks. As in previous studies we can identify instances that are so similar to previously seen examples such that the transformation is visually imperceptible. Additionally unlike in previous studies we can also generate mistakes that are significantly different from any training sample while importantly still remaining in the space of samples that the network should be able to classify correctly. This is achieved by training a basket of N “peer networks” rather than a single network. These are similarly trained networks that serve to provide consistency pressure on each other. When an example is found for which a single network S disagrees with all of the other N _ 1 networks which are consistent in their prediction that example is a potential mistake for S. We present a simple method to find such examples and demonstrate it on two visual tasks. The examples discovered yield realistic images that clearly illuminate the weaknesses of the trained models as well as provide a source of numerous diverse labeled-training samples.,http://research.google.com/pubs/archive/43986.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=The+Virtues+of+Peer+Pressure:+A+Simple+Method+for+Discovering+High-Value+Mistakes+Baluja+Covell+Sukthankar,http://research.google.com/pubs/pub43986.html
A Generative Model for Rhythms,Neural Information Processing Systems Workshop on Brain Music and Cognition (2008),2008,Jean-Francois Paiement Samy Bengio Yves Grandvalet Doug Eck,@inproceedings{34447 title = {A Generative Model for Rhythms} author = {Jean-Francois Paiement and Samy Bengio and Yves Grandvalet and Doug Eck} year = 2008 URL = {http://bengio.abracadoudou.com/cv/publications/pdf/paiement_2007_nips.pdf} booktitle = {Neural Information Processing Systems Workshop on Brain Music and Cognition} },Modeling music involves capturing long-term dependencies in time series which has proved very difficult to achieve with traditional statistical methods. The same problem occurs when only considering rhythms. In this paper we introduce a generative model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.,http://research.google.com/pubs/archive/34447.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=A+Generative+Model+for+Rhythms+Paiement+Bengio+Grandvalet+Eck,http://research.google.com/pubs/pub34447.html
Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) (2014) pp. 30-35,2014,Sameer Pradhan Xiaoqiang Luo Marta Recasens Eduard Hovy Vincent Ng Michael Strube,@inproceedings{42562 title = {Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation} author = {Sameer Pradhan and Xiaoqiang Luo and Marta Recasens and Eduard Hovy and Vincent Ng and Michael Strube} year = 2014 URL = {http://69.195.124.161/~aclwebor/anthology//P/P14/P14-2006.pdf} booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)} pages = {30--35} },The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either or both the key and predicted mentions in order to get a one-to-one mapping. On the other hand the metric BLANC was until recently limited to scoring partitions of key mentions. In this paper we (i) argue that mention manipulation for scoring predicted mentions is unnecessary and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open source thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms.,http://research.google.com/pubs/archive/42562.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scoring+Coreference+Partitions+of+Predicted+Mentions:+A+Reference+Implementation+Pradhan+Luo+Recasens+Hovy+Ng+Strube,http://research.google.com/pubs/pub42562.html
App Inventor,O'Reilly Media Inc 1005 Gravenstein Hwy N Sebastopol CA 95472 (2011) pp. 384,2011,David Wolber Hal Abelson Ellen Spertus Liz Looney,@book{37227 title = {App Inventor} author = {David Wolber and Hal Abelson and Ellen Spertus and Liz Looney} year = 2011 booktitle = {App Inventor} pages = {384} address = {1005 Gravenstein Hwy N Sebastopol CA 95472} },Yes you can create your own apps for Android phones—and it's easy to do. This extraordinary book introduces App Inventor for Android a powerful visual tool that lets anyone build apps for Android-based devices. Learn the basics of App Inventor with step-by-step instructions for more than a dozen fun projects such as creating location-aware apps data storage and apps that include decision-making logic. The second half of the book features an Inventor's manual to help you understand the fundamentals of app building and computer science. App Inventor makes an excellent textbook for beginners and experienced developers alike. Design games and other apps with 2D graphics and animation Create custom multi-media quizzes and study guides Create a custom tour of your city school or workplace Use an Android phone to control a LEGO® MINDSTORMS® NXT robot Build location-aware apps by working with your phone’s sensors Explore apps that incorporate information from the Web Learn computer science as you build your apps,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=App+Inventor+Wolber+Abelson+Spertus+Looney,http://research.google.com/pubs/pub37227.html
Learning kernels using local rademacher complexity,Advances in Neural Information Processing Systems (NIPS 2013) MIT Press.,2013,Corinna Cortes Marius Kloft Mehryar Mohri,@inproceedings{42029 title = {Learning kernels using local rademacher complexity} author = {Corinna Cortes and Marius Kloft and Mehryar Mohri} year = 2013 URL = {http://www.cs.nyu.edu/~mohri/pub/loc.pdf} booktitle = {Advances in Neural Information Processing Systems (NIPS 2013)} },We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which under certain general conditions guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also re- port the results of experiments with both algorithms in both binary and multi-class classification tasks.,http://research.google.com/pubs/archive/42029.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Learning+kernels+using+local+rademacher+complexity+Cortes+Kloft+Mohri,http://research.google.com/pubs/pub42029.html
What’s Cookin’? Interpreting Cooking Videos using Text Speech and Vision,North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT 2015) (to appear),2015,Jonathan Malmaud Jonathan Huang Vivek Rathod Nicholas Johnston Andrew Rabinovich Kevin Murphy,@inproceedings{43403 title = {What’s Cookin’? Interpreting Cooking Videos using Text Speech and Vision} author = {Jonathan Malmaud and Jonathan Huang and Vivek Rathod and Nicholas Johnston and Andrew Rabinovich and Kevin Murphy} year = 2015 booktitle = {North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT 2015)} },We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular we focus on the cooking domain where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications such as automatically illustrating recipes with keyframes and searching within a video for events of interest.,http://research.google.com/pubs/archive/43403.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=What%E2%80%99s+Cookin%E2%80%99%3F+Interpreting+Cooking+Videos+using+Text+Speech+and+Vision+Malmaud+Huang+Rathod+Johnston+Rabinovich+Murphy,http://research.google.com/pubs/pub43403.html
Scaling Optical Interconnects in Datacenter Networks Opportunities and Challenges for WDM,2010 18th IEEE Symposium on High Performance Interconnects IEEE pp. 113-116,2010,Hong Liu Cedric F. Lam Chris Johnson,@inproceedings{36670 title = {Scaling Optical Interconnects in Datacenter Networks Opportunities and Challenges for WDM} author = {Hong Liu and Cedric F. Lam and Chris Johnson} year = 2010 booktitle = {2010 18th IEEE Symposium on High Performance Interconnects} pages = {113-116} },We review the growing need for optical interconnect bandwidth in datacenter networks and the opportunities and challenges for wavelength division multiplexing (WDM) to sustain the “last 2km” bandwidth growth inside datacenter networks.,http://research.google.com/pubs/archive/36670.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Scaling+Optical+Interconnects+in+Datacenter+Networks+Opportunities+and+Challenges+for+WDM+Liu+Lam+Johnson,http://research.google.com/pubs/pub36670.html
Speech/Nonspeech Segmentation in Web Videos,Proceedings of InterSpeech 2012,2012,Ananya Misra,@inproceedings{40362 title = {Speech/Nonspeech Segmentation in Web Videos} author = {Ananya Misra} year = 2012 booktitle = {Proceedings of InterSpeech 2012} },Speech transcription of web videos requires ﬁrst detecting segments with transcribable speech. We refer to this as segmentation. Commonly used segmentation techniques are inadequate for domains such as YouTube where videos may have a large variety of background and recording conditions. In this work we investigate alternative audio features and a discriminative classiﬁer which together yield a lower frame error rate (25.3%) on YouTube videos compared to the commonly used Gaussian mixture models trained on cepstral features (30.6%). The alternative audio features perform particularly well in noisy conditions.,http://research.google.com/pubs/archive/40362.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Speech/Nonspeech+Segmentation+in+Web+Videos+Misra,http://research.google.com/pubs/pub40362.html
LatLong: Diagnosing Wide-Area Latency Changes for CDNs,IEEE Transactions on Network and Service Management vol. 9 (2012) (to appear),2012,Yaping Zhu Benjamin Helsley Jennifer Rexford Aspi Siganporia Sridhar Srinivasan,@article{38239 title = {LatLong: Diagnosing Wide-Area Latency Changes for CDNs} author = {Yaping Zhu and Benjamin Helsley and Jennifer Rexford and Aspi Siganporia and Sridhar Srinivasan} year = 2012 journal = {IEEE Transactions on Network and Service Management} volume = {9} },Minimizing user-perceived latency is crucial for Content Distribution Networks (CDNs) hosting interactive services. Latency may increase for many reasons such as interdomain routing changes and the CDN's own load-balancing policies. CDNs need greater visibility into the causes of latency increases so they can adapt by directing traffic to different servers or paths. In this paper we propose techniques for CDNs to diagnose large latency increases based on passive measurements of performance traffic and routing. Separating the many causes from the effects is challenging. We propose a decision tree for classifying latency changes and determine how to distinguish traffic shifts from increases in latency for existing servers routers and paths. Another challenge is that network operators group related clients to reduce measurement and control overhead but the clients in a region may use multiple servers and paths during a measurement interval. We propose metrics that quantify the latency contributions across sets of servers and routers. Analyzing a month of data from Google's CDN we find that nearly 1% of the daily latency changes increase delay by more than 100 msec. More than 40% of these increases coincide with interdomain routing changes and more than one-third involve a shift in traffic to different servers. This is the first work to diagnose latency problems in a large operational CDN from purely passive measurements. Through case studies of individual events we identify research challenges for measuring and managing wide-area latency for CDNs.,http://research.google.com/pubs/archive/38239.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=LatLong:+Diagnosing+Wide-Area+Latency+Changes+for+CDNs+Zhu+Helsley+Rexford+Siganporia+Srinivasan,http://research.google.com/pubs/pub38239.html
Cloud Data Protection for the Masses,Computer vol. 45 no. 1 (2012) pp. 39-45,2012,Dawn Song Elaine Shi Ian Fischer Umesh Shankar,@article{37672 title = {Cloud Data Protection for the Masses} author = {Dawn Song and Elaine Shi and Ian Fischer and Umesh Shankar} year = 2012 URL = {http://doi.ieeecomputersociety.org/10.1109/MC.2012.1} journal = {Computer} pages = {39--45} volume = {45 no. 1} },Offering strong data protection to cloud users while enabling rich applications is a challenging task. Researchers explore a new cloud platform architecture called Data Protection as a Service which dramatically reduces the per-application development effort required to offer data protection while still allowing rapid development and maintenance.,http://research.google.com/pubs/archive/37672.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Cloud+Data+Protection+for+the+Masses+Song+Shi+Fischer+Shankar,http://research.google.com/pubs/pub37672.html
Silicon Photonics for Optical Access Networks,Group IV Photonics IEEE Washington DC (2012) pp. 3,2012,Ryohei Urata Hong Liu Cedric Lam Pedram Dashti Chris Johnson,@inproceedings{39960 title = {Silicon Photonics for Optical Access Networks} author = {Ryohei Urata and Hong Liu and Cedric Lam and Pedram Dashti and Chris Johnson} year = 2012 booktitle = {Group IV Photonics} pages = {3} address = {Washington DC} },We highlight promising developments and directions in silicon photonics for realizing cost effective WDM-PON: photonic integration for integrated WDM transceivers at the OLT and widely tunable laser technologies for achieving a high performance colorless ONU.,http://research.google.com/pubs/archive/39960.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Silicon+Photonics+for+Optical+Access+Networks+Urata+Liu+Lam+Dashti+Johnson,http://research.google.com/pubs/pub39960.html
An Online Algorithm for Large Scale Image Similarity Learning,Advances in Neural Information Processing Systems (2009),2009,Gal Chechik Varun Sharma Uri Shalit Samy Bengio,@inproceedings{35311 title = {An Online Algorithm for Large Scale Image Similarity Learning} author = {Gal Chechik and Varun Sharma and Uri Shalit and Samy Bengio} year = 2009 booktitle = {Advances in Neural Information Processing Systems} },Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. It stands in the core of classifications methods like kernel machines and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately current approaches for learning similarity do not scale to large datasets especially when imposing metric constraints on the learned similarity. We describe OASIS a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a standard benchmark with thousands of images it is more precise than state-of-the-art methods and faster by orders of magnitude. On 2 millions images collected from the web OASIS can be trained within 3 days on a single CPU. The non-metric similarities learned by OASIS can be transformed into metric similarities achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by two orders of magnitude than was handled before.,http://research.google.com/pubs/archive/35311.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Online+Algorithm+for+Large+Scale+Image+Similarity+Learning+Chechik+Sharma+Shalit+Bengio,http://research.google.com/pubs/pub35311.html
Projecting Disk Usage Based on Historical Trends in a Cloud Environment,ScienceCloud 2012 Proceedings of the 3rd International Workshop on Scientific Cloud Computing ACM pp. 63-70,2012,Murray Stokely Amaan Mehrabian Christoph Albrecht Francois Labelle Arif Merchant,@inproceedings{37747 title = {Projecting Disk Usage Based on Historical Trends in a Cloud Environment} author = {Murray Stokely and Amaan Mehrabian and Christoph Albrecht and Francois Labelle and Arif Merchant} year = 2012 booktitle = {ScienceCloud 2012 Proceedings of the 3rd International Workshop on Scientific Cloud Computing} pages = {63--70} },Provisioning scarce resources among competing users and jobs remains one of the primary challenges of operating large-scale distributed computing environments. Distributed storage systems in particular typically rely on hard operator-set quotas to control disk allocation and enforce isolation for space and I/O bandwidth among disparate users. However users and operators are very poor at predicting future requirements and as a result tend to over-provision grossly. For three years we collected detailed usage information for data stored in distributed filesystems in a large private cloud spanning dozens of clusters on multiple continents. Specifically we measured the disk space usage I/O rate and age of stored data for thousands of different engineering users and teams. We find that although the individual timeseries often have non-stable usage trends regional aggregations user classification and ensemble forecasting methods can be combined to provide a more accurate prediction of future use for the majority of users. We applied this methodology for the storage users in one geographic region and back-tested these techniques over the past three years to compare our forecasts against actual usage. We find that by classifying a small subset of users with unforecastable trend changes due to known product launches we can generate three-month out forecasts with mean absolute errors of less than ~12%. This compares favorably to the amount of allocated but unused quota that is generally wasted with manual operator-set quotas.,http://research.google.com/pubs/archive/37747.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Projecting+Disk+Usage+Based+on+Historical+Trends+in+a+Cloud+Environment+Stokely+Mehrabian+Albrecht+Labelle+Merchant,http://research.google.com/pubs/pub37747.html
PRESS: PRedictive Elastic ReSource Scaling for cloud systems,6th IEEE/IFIP International Conference on Network and Service Management (CNSM 2010) Niagara Falls Canada,2010,Zhenhuan Gong Xiaohui Gu John Wilkes,@inproceedings{41688 title = {PRESS: PRedictive Elastic ReSource Scaling for cloud systems} author = {Zhenhuan Gong and Xiaohui Gu and John Wilkes} year = 2010 URL = {http://www.e-wilkes.com/john/papers/2010-PRESS-CNSM.pdf} booktitle = {6th IEEE/IFIP International Conference on Network and Service Management (CNSM 2010)} address = {Niagara Falls Canada} },Cloud systems require elastic resource allocation to minimize resource provisioning costs while meeting service level objectives (SLOs). In this paper we present a novel PRedictive Elastic reSource Scaling (PRESS) scheme for cloud systems. PRESS unobtrusively extracts ﬁne-grained dynamic patterns in application resource demands and adjust their resource allocations automatically. Our approach leverages light-weight signal processing and statistical learning algorithms to achieve online predictions of dynamic application resource requirements. We have implemented the PRESS system on Xen and tested it using RUBiS and an application load trace from Google. Our experiments show that we can achieve good resource prediction accuracy with less than 5% over-estimation error and near zero under-estimation error and elastic resource scaling can both signiﬁcantly reduce resource waste and SLO violations.,http://research.google.com/pubs/archive/41688.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=PRESS:+PRedictive+Elastic+ReSource+Scaling+for+cloud+systems+Gong+Gu+Wilkes,http://research.google.com/pubs/pub41688.html
Revenue Maximization for Selling Multiple Correlated Items,23rd Annual European Symposium on Algorithms (ESA) Springer-Verlag (2015),2015,Mohammadhossein Bateni Sina Dehghani MohammadTaghi Hajiaghayi Saeed Seddighin,@inproceedings{43815 title = {Revenue Maximization for Selling Multiple Correlated Items} author = {Mohammadhossein Bateni and Sina Dehghani and MohammadTaghi Hajiaghayi and Saeed Seddighin} year = 2015 booktitle = {23rd Annual European Symposium on Algorithms (ESA)} },"We study the problem of selling $n$ items to a single buyer with an additive valuation function. We consider the valuation of the items to be correlated i.e. desirabilities of the buyer for the items are not drawn independently. Ideally the goal is to design a mechanism to maximize the revenue. However it has been shown that a revenue optimal mechanism might be very complicated and as a result inapplicable to real-world auctions. Therefore our focus is on designing a simple mechanism that achieves a constant fraction of the optimal revenue. Babaioff et al. (FOCS'14) propose a simple mechanism that achieves a constant fraction of the optimal revenue for independent setting with a single additive buyer. However they leave the following problem as an open question: ""Is there a simple approximately optimal mechanism for a single additive buyer whose value for $n$ items is sampled from a common base-value distribution?"" Babaioff et al. show a constant approximation factor of the optimal revenue can be achieved by either selling the items separately or as a whole bundle in the independent setting. We show a similar result for the correlated setting when the desirabilities of the buyer are drawn from a common base-value distribution. It is worth mentioning that the core decomposition lemma which is mainly the heart of the proofs for efficiency of the mechanisms does not hold for correlated settings. Therefore we propose a modified version of this lemma which is applicable to the correlated settings as well. Although we apply this technique to show the proposed mechanism can guarantee a constant fraction of the optimal revenue in a very weak correlation this method alone can not directly show the efficiency of the mechanism in stronger correlations. Therefore via a combinatorial approach we reduce the problem to an auction with a weak correlation to which the core decomposition technique is applicable. In addition we introduce a generalized model of correlation for items and show the proposed mechanism achieves an $O(\log k)$ approximation factor of the optimal revenue in that setting.",http://research.google.com/pubs/archive/43815.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Revenue+Maximization+for+Selling+Multiple+Correlated+Items+Bateni+Dehghani+Hajiaghayi+Seddighin,http://research.google.com/pubs/pub43815.html
Structural maxent models,Proceedings of the Thirty-Second International Conference on Machine Learning (ICML 2015),2015,Corinna Cortes Vitaly Kuznetsov Mehryar Mohri Umar Syed,@inproceedings{43976 title = {Structural maxent models} author = {Corinna Cortes and Vitaly Kuznetsov and Mehryar Mohri and Umar Syed} year = 2015 booktitle = {Proceedings of the Thirty-Second International Conference on Machine Learning (ICML 2015)} },We present a new class of density estimation models Structural Maxent models with feature functions selected from a union of possibly very complex sub-families and yet benefiting from strong learning guarantees. The design of our models is based on a new principle supported by uniform convergence bounds and taking into consideration the complexity of the different sub-families composing the full set of features. We prove new data-dependent learning bounds for our models expressed in terms of the Rademacher complexities of these sub-families. We also prove a duality theorem which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm including the details of its derivation and report the results of several experiments demonstrating that its performance improves on that of existing L1-norm regularized Maxent algorithms. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models also making use of a union of possibly complex feature subfamilies. We prove a duality theorem for these models as well which reveals their connection with existing binary and multi-class deep boosting algorithms.,http://research.google.com/pubs/archive/43976.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Structural+maxent+models+Cortes+Kuznetsov+Mohri+Syed,http://research.google.com/pubs/pub43976.html
On the Convergence of Regret Minimization Dynamics in Concave Games,41st Annual ACM Symposium on Theory of Computing STOC ACM (2009) pp. 523-532,2009,Eyal Even-Dar Yishay Mansour Uri Nadav,@inproceedings{35384 title = {On the Convergence of Regret Minimization Dynamics in Concave Games} author = {Eyal Even-Dar and Yishay Mansour and Uri Nadav} year = 2009 URL = {http://portal.acm.org/citation.cfm?doid=1536414.1536486} booktitle = {41st Annual ACM Symposium on Theory of Computing STOC} pages = {523--532} },We consider standard regret minimization setting where at each time step the decision maker has to choose a distribution over k alternatives and then observes the loss of each alternative. The setting is very similar to the classical online job scheduling setting with three major differences: Information model: in the regret minimization setting losses are only observed after the actions (assigning the job to a machine) is performed and not observed before the action selection as assumed in the classical online job scheduling setting The comparison class: in regret minimization the comparison class is the best static algorithm (i.e. distribution over alternatives) and not the optimal offline solution. Performance measure: In regret minimization we measure the additive difference to the optimal solution in the comparison class in contrast to the ratio used in online job scheduling setting. Motivated by load balancing and job scheduling we consider a global cost function (over the losses incur by each alternative/machine) rather than simply a summation of the instantaneous losses as done traditionally in regret minimization. Such global cost functions include the makespan (the maximum over the alternatives/machines) and the Ld norm (over the alternatives/machines). The major contribution of this work is to design a novel regret minimization algorithm based on calibration that guarantees a vanishing average regret where the regret is measured with respect to the best static decision maker who selects the same distribution over alternatives at every time step. Our results hold for a wide class of global cost functions. which include the makespan and the Ld norms for d>1. In contrast we show that for concave global cost functions such as Ld norms for d<1 the worst-case average regret does not vanish. In addition to the general calibration based algorithm we provide simple and efficient algorithms for special interesting cases.,http://portal.acm.org/citation.cfm?doid=1536414.1536486,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=On+the+Convergence+of+Regret+Minimization+Dynamics+in+Concave+Games+Even-Dar+Mansour+Nadav,http://research.google.com/pubs/pub35384.html
Voice Search for Development,Interspeech 2010,2010,Etienne Barnard Johan Schalkwyk Charl van Heerden Pedro J. Moreno,@inproceedings{36833 title = {Voice Search for Development} author = {Etienne Barnard and Johan Schalkwyk and Charl van Heerden and Pedro J. Moreno} year = 2010 URL = {http://www.interspeech2010.org/program/session.php?id=4140} booktitle = {Interspeech 2010} },In light of the serious problems with both illiteracy and information access in the developing world there is a widespread belief that speech technology can play a significant role in improving the quality of life of developing-world citizens. We review the main reasons why this impact has not occurred to date and propose that voice-search systems may be a useful tool in delivering on the original promise. The challenges that must be addressed to realize this vision are analyzed and initial experimental results in developing voice search for two languages of South Africa (Zulu and Afrikaans) are summarized,http://research.google.com/pubs/archive/36833.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Voice+Search+for+Development+Barnard+Schalkwyk+Heerden+Moreno,http://research.google.com/pubs/pub36833.html
Trustworthy Hardware: Identifying and Classifying Hardware Trojans,IEEE Design and Test of Computers (2010) pp. 39-46,2010,Kurt Rosenfeld,@article{37398 title = {Trustworthy Hardware: Identifying and Classifying Hardware Trojans} author = {Kurt Rosenfeld} year = 2010 journal = {IEEE Design and Test of Computers} pages = {39--46} },For reasons of economy critical systems will inevitably depend on electronics made in untrusted factories. A proposed new hardware Trojan taxonomy provides a first step in better understanding existing and potential threats.,http://research.google.com/pubs/archive/37398.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Trustworthy+Hardware:+Identifying+and+Classifying+Hardware+Trojans+Rosenfeld,http://research.google.com/pubs/pub37398.html
Visualizing Statistical Mix Effects and Simpson's Paradox,Proceedings of IEEE InfoVis 2014 IEEE (to appear),2014,Zan Armstrong Martin Wattenberg,@inproceedings{42901 title = {Visualizing Statistical Mix Effects and Simpson's Paradox} author = {Zan Armstrong and Martin Wattenberg} year = 2014 booktitle = {Proceedings of IEEE InfoVis 2014} },We discuss how “mix effects” can surprise users of visualizations and potentially lead them to incorrect conclusions. This statistical issue (also known as “omitted variable bias” or in extreme cases as “Simpson’s paradox”) is widespread and can affect any visualization in which the quantity of interest is an aggregated value such as a weighted sum or average. Our first contribution is to document how mix effects can be a serious issue for visualizations and we analyze how mix effects can cause problems in a variety of popular visualization techniques from bar charts to treemaps. Our second contribution is a new technique the “comet chart” that is meant to ameliorate some of these issues.,http://research.google.com/pubs/archive/42901.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Visualizing+Statistical+Mix+Effects+and+Simpson's+Paradox+Armstrong+Wattenberg,http://research.google.com/pubs/pub42901.html
Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation,COLING-2010,2010,Dmitriy Genzel,@inproceedings{36484 title = {Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation} author = {Dmitriy Genzel} year = 2010 booktitle = {COLING-2010} },We describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation. We learn rules for 8 different language pairs showing BLEU improvements for all of them and demonstrate that many important order transformations (SVO to SOV or VSO head-modifier verb movement) can be captured by this approach.,http://research.google.com/pubs/archive/36484.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Automatically+Learning+Source-side+Reordering+Rules+for+Large+Scale+Machine+Translation+Genzel,http://research.google.com/pubs/pub36484.html
Hiring a secretary from a poset.,Proceedings 12th ACM Conference on Electronic Commerce (EC-2011) pp. 39-48,2011,Ravi Kumar Silvio Lattanzi Sergei Vassilvitskii Andrea Vattani,@inproceedings{37250 title = {Hiring a secretary from a poset.} author = {Ravi Kumar and Silvio Lattanzi and Sergei Vassilvitskii and Andrea Vattani} year = 2011 booktitle = {Proceedings 12th ACM Conference on Electronic Commerce (EC-2011)} pages = {39-48} },The secretary problem lies at the core of mechanism design for online auctions. In this work we study the generalization of the classical secretary problem in a setting where there is only a partial order be- tween the elements and the goal of the algorithm is to return one of the maximal elements of the poset. This is equivalent to the setting where the seller has a multidimensional objective function with only a partial order among the outcomes. We obtain an algorithm that succeeds with probability at least?1 + l k^{_k/(k_1)} ((1+log^{-1/(k-1)} k)^k -1) where k is the number of maximal elements in the poset and is the only information about the poset that is known to the algorithm. On the other hand we prove an almost matching upper bound of k^{_1/(k_1)} on the success probability of any algorithm for this problem; this upper bound holds even if the algorithm knows the complete structure of the poset.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Hiring+a+secretary+from+a+poset.+Kumar+Lattanzi+Vassilvitskii+Vattani,http://research.google.com/pubs/pub37250.html
Dynamic cache contention detection in multi-threaded applications,VEE 2011; Proceedings of the 7th ACM SIGPLAN/SIGOPS International conference on virtual execution environments ACM New York NY pp. 27-37,2011,Qin Zhao David Koh Syed Raza Derek Bruening Weng-Fai Wong,@inproceedings{37659 title = {Dynamic cache contention detection in multi-threaded applications} author = {Qin Zhao and David Koh and Syed Raza and Derek Bruening and Weng-Fai Wong} year = 2011 URL = {http://dx.doi.org/10.1145/2007477.1952688} booktitle = {VEE 2011; Proceedings of the 7th ACM SIGPLAN/SIGOPS International conference on virtual execution environments} pages = {27--37} address = {New York NY} },In today's multi-core systems cache contention due to true and false sharing can cause unexpected and significant performance degradation. A detailed understanding of a given multi-threaded application's behavior is required to precisely identify such performance bottlenecks. Traditionally however such diagnostic information can only be obtained after lengthy simulation of the memory hierarchy. In this paper we present a novel approach that efficiently analyzes interactions between threads to determine thread correlation and detect true and false sharing. It is based on the following key insight: although the slowdown caused by cache contention depends on factors including the thread-to-core binding and parameters of the memory hierarchy the amount of data sharing is primarily a function of the cache line size and application behavior. Using memory shadowing and dynamic instrumentation we implemented a tool that obtains detailed sharing information between threads without simulating the full complexity of the memory hierarchy. The runtime overhead of our approach --- a 5x slowdown on average relative to native execution --- is significantly less than that of detailed cache simulation. The information collected allows programmers to identify the degree of cache contention in an application the correlation among its threads and the sources of significant false sharing. Using our approach we were able to improve the performance of some applications up to a factor of 12x. For other contention-intensive applications we were able to shed light on the obstacles that prevent their performance from scaling to many cores.,http://research.google.com/pubs/archive/37659.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Dynamic+cache+contention+detection+in+multi-threaded+applications+Zhao+Koh+Raza+Bruening+Wong,http://research.google.com/pubs/pub37659.html
Characterization and Comparison of Cloud versus Grid Workloads,IEEE Cluster 2012,2012,Sheng Di Derrick Kondo Walfredo Cirne,@inproceedings{42552 title = {Characterization and Comparison of Cloud versus Grid Workloads} author = {Sheng Di and Derrick Kondo and Walfredo Cirne} year = 2012 booktitle = {IEEE Cluster 2012} },A new era of Cloud Computing has emerged but the characteristics of Cloud load in data centers is not perfectly clear. Yet this characterization is critical for the design of novel Cloud job and resource management systems. In this paper we comprehensively characterize the job/task load and host load in a real-world production data center at Google Inc. We use a detailed trace of over 25 million tasks across over 12500 hosts. We study the differences between a Google data center and other Grid/HPC systems from the perspective of both work load (w.r.t. jobs and tasks) and host load (w.r.t. machines). In particular we study the job length job submission frequency and the resource utilization of jobs in the different systems and also investigate valuable statistics of machine’s maximum load queue state and relative usage levels with different job priorities and resource attributes. We find that the Google data center exhibits finer resource allocation with respect to CPU and memory than that of Grid/HPC systems. Google jobs are always submitted with much higher frequency and they are much shorter than Grid jobs. As such Google host load exhibits higher variance and noise.,,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Characterization+and+Comparison+of+Cloud+versus+Grid+Workloads+Di+Kondo+Cirne,http://research.google.com/pubs/pub42552.html
Temporal Synchronization of Multiple Audio Signals,Proceedings of the International Conference on Signal Processing (ICASSP) Florence Italy (2014),2014,Julius Kammerl Neil Birkbeck Sasi Inguva Damien Kelly Andy Crawford Hugh Denman Anil Kokaram Caroline Pantofaru,@inproceedings{42193 title = {Temporal Synchronization of Multiple Audio Signals} author = {Julius Kammerl and Neil Birkbeck and Sasi Inguva and Damien Kelly and Andy Crawford and Hugh Denman and Anil Kokaram and Caroline Pantofaru} year = 2014 booktitle = {Proceedings of the International Conference on Signal Processing (ICASSP)} address = {Florence Italy} },Given the proliferation of consumer media recording devices events often give rise to a large number of recordings. These recordings are taken from different spatial positions and do not have reliable timestamp information. In this paper we present two robust graph-based approaches for synchronizing multiple audio signals. The graphs are constructed atop the over-determined system resulting from pairwise signal comparison using cross-correlation of audio features. The first approach uses a Minimum Spanning Tree (MST) technique while the second uses Belief Propagation (BP) to solve the system. Both approaches can provide excellent solutions and robustness to pairwise outliers however the MST approach is much less complex than BP. In addition an experimental comparison of audio features-based synchronization shows that spectral flatness outperforms the zero-crossing rate and signal energy.,http://research.google.com/pubs/archive/42193.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Temporal+Synchronization+of+Multiple+Audio+Signals+Kammerl+Birkbeck+Inguva+Kelly+Crawford+Denman+Kokaram+Pantofaru,http://research.google.com/pubs/pub42193.html
Distributed forensics and incident response in the enterprise,Journal of Digital Investigation vol. 8 (2011) S101-S110,2011,Michael Cohen Darren Bilby Germano Caronni,@article{37237 title = {Distributed forensics and incident response in the enterprise} author = {Michael Cohen and Darren Bilby and Germano Caronni} year = 2011 URL = {http://www.dfrws.org/2011/proceedings/16-348.pdf} journal = {Journal of Digital Investigation} pages = {S101--S110} volume = {8} },Remote live forensics has recently been increasingly used in order to facilitate rapid remote access to enterprise machines. We present the GRR Rapid Response Framework (GRR) a new multi-platform open source tool for enterprise forensic investigations enabling remote raw disk and memory access. GRR is designed to be scalable opening the door for continuous enterprise wide forensic analysis. This paper describes the architecture used by GRR and illustrates how it is used routinely to expedite enterprise forensic investigations.,http://research.google.com/pubs/archive/37237.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Distributed+forensics+and+incident+response+in+the+enterprise+Cohen+Bilby+Caronni,http://research.google.com/pubs/pub37237.html
“Poetic” Statistical Machine Translation: Rhyme and Meter,EMNLP (2010) pp. 158-166,2010,Dmitriy Genzel Jakob Uszkoreit Franz Och,@inproceedings{36745 title = {“Poetic” Statistical Machine Translation: Rhyme and Meter} author = {Dmitriy Genzel and Jakob Uszkoreit and Franz Och} year = 2010 booktitle = {EMNLP} pages = {158--166} },As a prerequisite to translation of poetry we implement the ability to produce translations with meter and rhyme for phrase-based MT examine whether the hypothesis space of such a system is flexible enough to accommodate such constraints and investigate the impact of such constraints on translation quality.,http://research.google.com/pubs/archive/36745.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=%E2%80%9CPoetic%E2%80%9D+Statistical+Machine+Translation:+Rhyme+and+Meter+Genzel+Uszkoreit+Och,http://research.google.com/pubs/pub36745.html
Storing and Querying Tree-Structured Records in Dremel,Proceedings of the VLDB Endowment vol. 7 (2014) pp. 1131-1142,2014,Foto N Afrati Dan Delorey Mosha Pasumansky Jeffrey D. Ullman,@article{43119 title = {Storing and Querying Tree-Structured Records in Dremel} author = {Foto N Afrati and Dan Delorey and Mosha Pasumansky and Jeffrey D. Ullman} year = 2014 URL = {http://www.vldb.org/pvldb/vol7/p1131-afrati.pdf} journal = {Proceedings of the VLDB Endowment} pages = {1131-1142} volume = {7} },In Dremel data is stored as nested relations. The schema for a relation is a tree all of whose nodes are attributes and whose leaf attributes hold values. We explore filter and aggregate queries that are given in the Dremel dialect of SQL. Complications arise because of repeated attributes i.e. attributes that are allowed to have more than one value. We focus on the common class of Dremel queries that are processed on column-stored data in a way that results in query processing time that is linear on the size of the relevant data i.e. data in the columns that participate in the query. We formally define the data model the query language and the algorithms for query processing in column-stored data. The concepts of repetition context and semi-flattening are introduced here and play a central role in understanding this class of queries and their algorithms.,http://research.google.com/pubs/archive/43119.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Storing+and+Querying+Tree-Structured+Records+in+Dremel+Afrati+Delorey+Pasumansky+Ullman,http://research.google.com/pubs/pub43119.html
Fast and Secure Three-party Computation: The Garbled Circuit Approach,The 22nd ACM Conference on Computer and Communications Security ACM (2015),2015,Payman Mohassel Mike Rosulek Ye Zhang,@inproceedings{43888 title = {Fast and Secure Three-party Computation: The Garbled Circuit Approach} author = {Payman Mohassel and Mike Rosulek and Ye Zhang} year = 2015 booktitle = {The 22nd ACM Conference on Computer and Communications Security} },Many deployments of secure multi-party computation (MPC) in practice have used information-theoretic three-party protocols that tolerate a single semi-honest corrupt party since these protocols enjoy very high efficiency. We propose a new approach for secure three-party computation (3PC) that improves security while maintaining practical efficiency that is competitive with traditional information theoretic protocols. Our protocol is based on garbled circuits and provides security against a single malicious corrupt party. Unlike information-theoretic 3PC protocols ours uses a constant number of rounds. Our protocol only uses inexpensive symmetric-key cryptography: hash functions block ciphers pseudorandom generators (in particular no oblivious transfers) and has performance that is comparable to that of Yao’s (semi-honest) 2PC protocol. We demonstrate the practicality of our protocol with an implementation based on the JustGarble framework of Bellare et al. (S&P 2013). The implementation incorporates various optimizations including the most recent techniques for efficient circuit garbling. We perform experiments on several benchmarking circuits in different setups. Our experiments confirm that despite providing a more demanding security guarantee our protocol has performance comparable to existing information-theoretic 3PC.,http://research.google.com/pubs/archive/43888.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Fast+and+Secure+Three-party+Computation:+The+Garbled+Circuit+Approach+Mohassel+Rosulek+Zhang,http://research.google.com/pubs/pub43888.html
Proportional Rate Reduction for TCP,Proceedings of the 11th ACM SIGCOMM Conference on Internet Measurement 2011 Berlin Germany - November 2-4 2011,2011,Nandita Dukkipati Matt Mathis Yuchung Cheng Monia Ghobadi,@inproceedings{37486 title = {Proportional Rate Reduction for TCP} author = {Nandita Dukkipati and Matt Mathis and Yuchung Cheng and Monia Ghobadi} year = 2011 URL = {http://conferences.sigcomm.org/imc/2011/program.htm} booktitle = {Proceedings of the 11th ACM SIGCOMM Conference on Internet Measurement 2011 Berlin Germany - November 2-4 2011} },Packet losses increase latency for Web users. Fast recovery is a key mechanism for TCP to recover from packet losses. In this paper we explore some of the weaknesses of the standard algorithm described in RFC 3517 and the non-standard algorithms implemented in Linux. We ﬁnd that these algorithms deviate from their intended behavior in the real world due to the combined e_ect of short ﬂows application stalls burst losses acknowledgment (ACK) loss and reordering and stretch ACKs. Linux su_ers from excessive congestion window reductions while RFC 3517 transmits large bursts under high losses both of which harm the rest of the ﬂow and increase Web latency. Our primary contribution is a new design to control transmission in fast recovery called proportional rate reduction (PRR). PRR recovers from losses quickly smoothly and accurately by pacing out retransmissions across received ACKs. In addition to PRR we evaluate the TCP early retransmit (ER) algorithm which lowers the duplicate acknowledgment threshold for short transfers and show that delaying early retransmissions for a short interval is e_ective in avoiding spurious retransmissions in the presence of a small degree of reordering. PRR and ER reduce the TCP latency of connections experiencing losses by 3-10% depending on the response size. Based on our instrumentation on Google Web and YouTube servers in U.S. and India we also present key statistics on the nature of TCP retransmissions.,http://research.google.com/pubs/archive/37486.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Proportional+Rate+Reduction+for+TCP+Dukkipati+Mathis+Cheng+Ghobadi,http://research.google.com/pubs/pub37486.html
Version Control with Subversion Second Edition,O'Reilly Media 1003 Gravenstein Highway North Sebastopol CA 95472 (2008) pp. 430,2008,Brian Fitzpatrick Ben Collins-Sussman C. Michael Pilato,@book{34879 title = {Version Control with Subversion Second Edition} author = {Brian Fitzpatrick and Ben Collins-Sussman and C. Michael Pilato} year = 2008 URL = {http://svnbook.red-bean.com/} note = {This book is released under a Creative Commons license.} booktitle = {Version Control with Subversion} pages = {430} address = {1003 Gravenstein Highway North Sebastopol CA 95472} },The official guide and reference manual for the popular open source revision control technology.,http://svnbook.red-bean.com/,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Version+Control+with+Subversion+Second+Edition+Fitzpatrick+Collins-Sussman+Pilato,http://research.google.com/pubs/pub34879.html
Performance Trade-offs Implementing Refactoring Support for Objective-C,Workshop on Refactoring Tools (2009),2009,Robert Bowdidge,@inproceedings{36572 title = {Performance Trade-offs Implementing Refactoring Support for Objective-C} author = {Robert Bowdidge} year = 2009 URL = {http://homepage.mac.com/rbowdidge/research_assets/bowdidgeTradeoffs.pdf} booktitle = {Workshop on Refactoring Tools} },When we started implementing a refactoring tool for real-world C programs we recognized that preprocessing and parsing in straightforward and accurate ways would result in unacceptably slow analysis times and an overly-complex parsing system. Instead we traded some accuracy so we could parse analyze and change large real programs while still making the refactoring experience feel interactive and fast. Our tradeoffs fell into three categories: using different levels of accuracy in different parts of the analysis recognizing that the collected wisdom about C programs didn't hold for Objective-C programs and finding ways to exploit delays in typical interaction with the tool.,http://research.google.com/pubs/archive/36572.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Performance+Trade-offs+Implementing+Refactoring+Support+for+Objective-C+Bowdidge,http://research.google.com/pubs/pub36572.html
Painting with Triangles,Proceedings of the Workshop on Non-Photorealistic Animation and Rendering NPAR ACM New York NY USA (2014) pp. 13-20,2014,Mark D. Benjamin Stephen DiVerdi Adam Finkelstein,@inproceedings{43466 title = {Painting with Triangles} author = {Mark D. Benjamin and Stephen DiVerdi and Adam Finkelstein} year = 2014 URL = {http://gfx.cs.princeton.edu/gfx/pubs/Benjamin_2014_PWT/index.php} booktitle = {Proceedings of the Workshop on Non-Photorealistic Animation and Rendering NPAR} pages = {13-20} address = {New York NY USA} },Although vector graphics offer a number of benefits conventional vector painting programs offer only limited support for the traditional painting metaphor. We propose a new algorithm that translates a user's mouse motion into a triangle mesh representation. This triangle mesh can then be composited onto a canvas containing an existing mesh representation of earlier strokes. This representation allows the algorithm to render solid colors and linear gradients. It also enables painting at any resolution. This paradigm allows artists to create complex multi-scale drawings with gradients and sharp features while avoiding pixel sampling artifacts.,http://research.google.com/pubs/archive/43466.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=Painting+with+Triangles+Benjamin+DiVerdi+Finkelstein,http://research.google.com/pubs/pub43466.html
SAC073 - SSAC Comments on Root Zone Key Signing Key Rollover Plan,ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories ICANN (2016) pp. 41,2016,Warren Kumari Patrik Fältström,@incollection{44840 title = {SAC073 - SSAC Comments on Root Zone Key Signing Key Rollover Plan} author = {Warren Kumari and Patrik Fältström} year = 2016 booktitle = {ICANN Security and Stability Advisory Committee (SSAC) Reports and Advisories} pages = {41} },SSAC Comments on the Design Teams Draft Report on the Root Zone Key Signing Key Rollover Plan.,http://research.google.com/pubs/archive/44840.pdf,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=SAC073+-+SSAC+Comments+on+Root+Zone+Key+Signing+Key+Rollover+Plan+Kumari+F%C3%A4ltstr%C3%B6m,http://research.google.com/pubs/pub44840.html
An Assertional Correctness Proof of a Self-Stabilizing l-Exclusion Algorithm,11th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS'06) IEEE CS (2006) pp. 199-208,2006,Milos Besta Frank Stomp,@inproceedings{32704 title = {An Assertional Correctness Proof of a Self-Stabilizing l-Exclusion Algorithm} author = {Milos Besta and Frank Stomp} year = 2006 URL = {http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/iceccs/&toc=comp/proceedings/iceccs/2006/2530/00/2530toc.xml&PageNumber=199} booktitle = {11th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS'06)} pages = {199--208} },A formal correctness proof of a self-stabilizing l-exclusion algorithm (SLEX) is described. The analyzed algorithm is an improvement of the SLEX due to Abraham Dolev Herman and Koll since our version satisfies a stronger liveness property. The proof is formulated in Linear-Time Temporal Logic and utilizes a history variable to model access to regular registers. The proof consists of a safety part and a liveness part. Our analysis provides some new insight in the correctness of the algorithm: (1) Our proof is constructive. That is we explicitly formulate auxiliary quantities required to establish some of the properties. This contrasts with the operational arguments of Abraham et al. where many quantities are not explicitly formulated and the validity of the above mentioned properties is established by disproving their non-existence. (2) We characterize processes (and their minimum number) identified by some process as attempting to enter the critical section. (3) A novel proof rule for reasoning about programs in the presence of disabled processes is presented to structure the liveness proof.,http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/iceccs/&toc=comp/proceedings/iceccs/2006/2530/00/2530toc.xml&PageNumber=199,http://www.google.com/search?lr&ie=UTF-8&oe=UTF-8&q=An+Assertional+Correctness+Proof+of+a+Self-Stabilizing+l-Exclusion+Algorithm+Besta+Stomp,http://research.google.com/pubs/pub32704.html