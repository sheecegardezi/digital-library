id,Volume,vol_link,title,authors,Details,Pdf link,vol_link,Addition url,date,data of the conference,conference_name,conference_name_long,Place,editors,id,abs_link,abstracts
1,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Increasing Feature Selection Accuracy for L1 Regularized Linear Models,Abhishek Jaiantilal and Gregory Grudic,"10:86-96, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/jaiantilal10a/jaiantilal10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_jaiantilal10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/jaiantilal10a.html,L1 (also referred to as the 1-norm or Lasso) penalty based formulations have been shown to be eective in problem domains when noisy features are present. However the L1 penalty does not give favorable asymptotic properties with respect to feature selection and has been shown to be inconsistent as a feature selection estimator; e.g. when noisy features are correlated with the relevant features. This can aect the estimation of the correct feature set in certain domains like robotics when both the number of examples and the number of features are large. The weighted lasso penalty by (Zou 2006) has been proposed to rectify this problem of correct estimation of the feature set. This paper proposes a novel method for identifying problem specific L1 feature weights by utilizing the results from (Zou 2006) and (Rocha et al. 2009) and is applicable to regression and classification algorithms. Our method increases the accuracy of L1 penalized algorithms through randomized experiments on subsets of the training data as a fast pre-processing step. We show experimental and theoretical results supporting the ecacy of the proposed method on two L1 penalized classification algorithms.
2,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,A Statistical Implicative Analysis Based Algorithm and MMPC Algorithm for Detecting Multiple Dependencies,"Elham Salehi, Jayashree Nyayachavadi and Robin Gras","10:22-34, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/salehi10a/salehi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_salehi10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/salehi10a.html,Discovering the dependencies among the variables of a domain from examples is an important problem in optimization. Many methods have been proposed for this purpose but few large-scale evaluations were conducted. Most of these methods are based on measurements of conditional probability. The statistical implicative analysis offers another perspective of dependencies. It is important to compare the results obtained using this approach with one of the best methods currently available for this task: the MMPC heuristic. As the SIA is not used directly to address this problem we designed an extension of it for our purpose. We conducted a large number of experiments by varying parameters such as the number of dependencies the number of variables involved or the type of their distribution to compare the two approaches. The results show strong complementarities of the two methods.
3,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Feature Selection for Text Classification Based on Gini Coefficient of Inequality,"Ranbir Sanasam, Hema Murthy and Timothy Gonsalves","10:76-85, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/sanasam10a/sanasam10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_sanasam10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/sanasam10a.html,A number of feature selection mechanisms have been explored in text categorization among which mutual information information gain and chi-square are considered most effective. In this paper we study another method known as {\it within class popularity} to deal with feature selection based on the concept {\it Gini coefficient of inequality} (a commonly used measure of inequality of \textit{income}). The proposed measure explores the relative distribution of a feature among different classes. From extensive experiments with four text classifiers over three datasets of different levels of heterogeneity we observe that the proposed measure outperforms the mutual information information gain and chi-square static with an average improvement of approximately 28.5% 19% and 9.2% respectively.
4,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Hierarchical Convex NMF for Clustering Massive Data,"Kristian Kersting, Mirwaes Wahabzada, Christian Thurau, and Christian Bauckhage","13:253-268, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/kersting10a/kersting10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_kersting10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/kersting10a.html,We present an extension of convex-hull non-negative matrix factorization (CH-NMF) which was recently proposed as a large scale variant of convex non-negative matrix factorization or Archetypal Analysis. CHNMF factorizes a non-negative data matrix V into two non-negative matrix factors V = WH such that the columns of W are convex combinations of certain data points so that they are readily interpretable to data analysts. There is however no free lunch: imposing convexity constraints on W typically prevents adaptation to intrinsic low dimensional structures in the data. Alas in cases where the data is distributed in a non-convex manner or consists of mixtures of lower dimensional convex distributions the cluster representatives obtained from CH-NMF will be less meaningful. In this paper we present a hierarchical CH-NMF that automatically adapts to internal structures of a dataset hence it yields meaningful and interpretable clusters for non-convex datasets. This is also confirmed by our extensive evaluation on DBLP publication records of 760000 authors 4000000 images harvested from the web and 150000000 votes on World of Warcraft guilds.
5,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Decision Tree for Dynamic and Uncertain Data Streams,"Chunquan Liang, Yang Zhang, and Qun Song","13:209-224, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/liang10a/liang10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_liang10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/liang10a.html,Current research on data stream classification mainly focuses on certain data in which precise and definite value is usually assumed. However data with uncertainty is quite natural in real-world application due to various causes including imprecise measurement repeated sampling and network errors. In this paper we focus on uncertain data stream classification. Based on CVFDT and DTU we propose our UCVFDT (Uncertainty-handling and Concept-adapting Very Fast Decision Tree) algorithm which not only maintains the ability of CVFDT to cope with concept drift with high speed but also adds the ability to handle data with uncertain attribute. Experimental study shows that the proposed UCVFDT algorithm is efficient in classifying dynamic data stream with uncertain numerical attribute and it is computationally efficient.
6,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Ellipsoidal Support Vector Machines,"Michinari Momma, Kohei Hatano, and Hiroki Nakayama","13:31-46, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/momma10a/momma10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_momma10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/momma10a.html,This paper proposes the ellipsoidal SVM (e-SVM) that uses an ellipsoid center in the version space to approximate the Bayes point. Since SVM approximates it by a sphere center e-SVM provides an extension to SVM for better approximation of the Bayes point. Although the idea has been mentioned before (Rujan 1997) no work has been done for formulating and kernelizing the method. Starting from the maximum volume ellipsoid problem we successfully formulate and kernelize it by employing relaxations. The resulting e-SVM optimization framework has much similarity to SVM; it is naturally extendable to other loss functions and other problems. A variant of the sequential minimal optimization is provided for efficient batch implementation. Moreover we provide an online version of linear or primal e-SVM to be applicable for large-scale datasets.
7,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Generative Models of Information Diffusion with Asynchronous Timedelay,"Kazumi Saito, Masahiro Kimura, Kouzou Ohara, and Hiroshi Motoda","13:193-208, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/saito10a/saito10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_saito10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/saito10a.html,We address the problem of formalizing an information diffusion process on a social network as a generative model in the machine learning framework so that we can learn model parameters from the observation. Time delay plays an important role in formulating the likelihood function as well as for the analyses of information diffusion. We identified that there are two different types of time delay: link delay and node delay. The former corresponds to the delay associated with information propagation and the latter corresponds to the delay due to human action. We further identified that there are two distinctions of the way the activation from the multiple parents is updated: nonoverride and override. The former sticks to the initial activation and the latter can decide to update the time to activate multiple times. We formulated the likelihood function of the well known diffusion models: independent cascade and linear threshold both enhanced with asynchronous time delay distinguishing the difference in two types of delay and two types of update scheme. Simulation using four real world networks reveals that there are differences in the spread of information diffusion and they strongly depend on the choice of the parameter values and the denseness of the network.
8,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Ranking by calibrated AdaBoost,"R. Busa-Fekete, B. K _ gl, T. € ltet _ & G. Szarvas","14:37_48, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/busa-fekete11a/busa-fekete11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_busa-fekete11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/busa-fekete11a.html,This paper describes the ideas and methodologies that we used in the Yahoo learning-to-rank challenge      1    . Our technique is essentially pointwise with a listwise touch at the last combination step. The main ingredients of our approach are 1) preprocessing (querywise normalization) 2) multi-class A d  a B o  o  s  t .MH 3) regression calibration and 4) an exponentially weighted forecaster for model combination. In post-challenge analysis we found that preprocessing and training AdaBoost with a wide variety of hyperparameters improved individual models signi_cantly the _nal listwise ensemble step was crucial whereas calibration helped only in creating diversity.   Page last modified on Wed Jan 26 10:36:50 2011.
9,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Unsupervised Supervised Learning II: Margin-Based Classification without Labels,"Krishnakumar Balasubramanian, Pinar Donmez, Guy Lebanon","15:137-145, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/balasubramanian11a/balasubramanian11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_balasubramanian11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/balasubramanian11a.html,Many popular linear classifiers such as logistic regression boosting or SVM are trained by optimizing margin-based risk functions. Traditionally these risk functions are computed based on a labeled dataset. We develop a novel technique for estimating such risks using only unlabeled data and knowledge of p(y). We prove that the proposed risk estimator is consistent on high-dimensional datasets and demonstrate it on synthetic and real-world data. In particular we show how the estimate is used for evaluating classifiers in transfer learning and for training classifiers using exclusively unlabeled data.   [pdf]
10,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Concave Gaussian Variational Approximations for Inference in Large-Scale Bayesian Linear Models,"Edward Challis, David Barber","15:199-207, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/challis11a/challis11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_challis11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/challis11a.html,Two popular approaches to forming bounds in approximate Bayesian inference are local variational methods and minimal Kullback-Leibler divergence methods. For a large class of models we explicitly relate the two approaches showing that the local variational method is equivalent to a weakened form of Kullback-Leibler Gaussian approximation. This gives a strong motivation to develop efficient methods for KL minimisation. An important and previously unproven property of the KL variational Gaussian bound is that it is a concave function in the parameters of the Gaussian for log concave sites. This observation along with compact concave parametrisations of the covariance enables us to develop fast scalable optimisation procedures to obtain lower bounds on the marginal likelihood in large scale Bayesian linear models.   [pdf]  [supplementary]
11,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A novel greedy algorithm for Nystr_m approximation,"Ahmed Farahat, Ali Ghodsi, Mohamed Kamel","15:269-277, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/farahat11a/farahat11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_farahat11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/farahat11a.html,The Nystr_m method is an efficient technique for obtaining a low-rank approximation of a large kernel matrix based on a subset of its columns. The quality of the Nystr_m approximation highly depends on the subset of columns used which are usually selected using random sampling. This paper presents a novel recursive algorithm for calculating the Nystr_m approximation and an effective greedy criterion for column selection. Further a very efficient variant is proposed for greedy sampling which works on random partitions of data instances. Experiments on benchmark data sets show that the proposed greedy algorithms achieve significant improvements in approximating kernel matrices with minimum overhead in run time.   [pdf]  [supplementary]
12,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Learning from positive and unlabeled examples by enforcing statistical significance,Pierre Geurts,"15:305-314, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/geurts11a/geurts11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_geurts11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/geurts11a.html,"Given a finite but large set of objects described by a vector of features only a small subset of which have been labeled as """"positive"""" with respect to a class of interest we consider the problem of characterizing the positive class. We formalize this as the problem of learning a feature based score function that minimizes the p-value of a non parametric statistical hypothesis test. For linear score functions over the original feature space or over one of its kernelized versions we provide a solution of this problem computed by a one-class SVM applied on a surrogate dataset obtained by sampling subsets of the overall set of objects and representing them by their average feature-vector shifted by the average feature-vector of the original sample of positive examples. We carry out experiments with this method on the prediction of targets of transcription factors in two different organisms E. Coli and S. Cererevisiae. Our method extends enrichment analysis commonly carried out in Bioinformatics and its results outperform common solutions to this problem.   [pdf]  [supplementary]"
13,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Multiscale Community Blockmodel for Network Exploration,"Qirong Ho, Ankur Parikh, Le Song, Eric Xing","15:333-341, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ho11a/ho11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ho11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/ho11a.html,Real world networks exhibit a complex set of phenomena such as underlying hierarchical organization multiscale interaction and varying topologies of communities. Most existing methods do not adequately capture the intrinsic interplay among such phenomena. We propose a nonparametric Multiscale Community Blockmodel (MSCB) to model the generation of hierarchies in social communities selective membership of actors to subsets of these communities and the resultant networks due to within- and cross- community interactions. By using the nested Chinese Restaurant Process our model automatically infers the hierarchy structure from the data. We develop a collapsed Gibbs sampling algorithm for posterior inference conduct extensive validation using synthetic networks and demonstrate the utility of our model in real-world datasets such as predator-prey networks and citation networks.   [pdf]  [supplementary]
14,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Online Learning of Structured Predictors with Multiple Kernels,"Andre Filipe Torres Martins, Noah Smith, Eric Xing, Pedro Aguiar, Mario Figueiredo","15:507-515, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/martins11a/martins11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_martins11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/martins11a.html,Training structured predictors often requires a considerable time selecting features or tweaking the kernel. Multiple kernel learning (MKL) sidesteps this issue by embedding the kernel learning into the training procedure. Despite the recent progress towards efficiency of MKL algorithms the structured output case remains an open research front. We propose a family of online algorithms able to tackle variants of MKL and group-LASSO for which we show regret convergence and generalization bounds. Experiments on handwriting recognition and dependency parsing attest the success of the approach.   [pdf]  [supplementary]
15,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Maximum Volume Clustering,"Gang Niu, Bo Dai, Lin Shang, Masashi Sugiyama","15:561-569, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/niu11b/niu11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_niu11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/niu11b.html,The large volume principle proposed by Vladimir Vapnik which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable is a useful alternative to the large margin principle. In this paper we introduce a clustering model based on the large volume principle called maximum volume clustering (MVC) and propose two algorithms to solve it approximately: a soft-label and a hard-label MVC algorithms based on sequential quadratic programming and semi-definite programming respectively. Our MVC model includes spectral clustering and maximum margin clustering as special cases and is substantially more general. We also establish the finite sample stability and an error bound for the soft-label MVC method. Experiments show that the proposed MVC approach compares favorably with state-of-the-art clustering algorithms.   [pdf]  [supplementary]
16,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,On NDCG Consistency of Listwise Ranking Methods,"Pradeep Ravikumar, Ambuj Tewari, Eunho Yang","15:618-626, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ravikumar11a/ravikumar11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ravikumar11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/ravikumar11a.html,"We examine the consistency of listwise ranking methods with respect to the popular Normalized Discounted Cumulative Gain (NDCG) criterion. The most successful listwise approaches replace NDCG with a surrogate that is easier to optimize. We characterize NDCG consistent surrogates to discover a surprising fact: several commonly used surrogates are NDCG inconsistent. We then show how to change them so that they become NDCG consistent in a strong but natural sense. An explicit characterization of strong NDCG consistency is provided. Going beyond qualitative consistency considerations we also give quantitive statements that enable us to transform the excess error as measured in the surrogate to the excess error in comparison to the Bayes optimal ranking function for NDCG. Finally we also derive improved results if a certain natural ``low noise"""" or ``large margin"""" condition holds. Our experiments demonstrate that ensuring NDCG consistency does improve the performance of listwise ranking methods on real-world datasets. Moreover a novel surrogate function suggested by our theoretical results leads to further improvements over NDCG consistent versions of existing surrogates.   [pdf]  [supplementary]"
17,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Spectral Clustering on a Budget,"Ohad Shamir, Naftali Tishby","15:661-669, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/shamir11a/shamir11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_shamir11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/shamir11a.html,Spectral clustering is a modern and well known method for performing data clustering. However it depends on the availability of a similarity matrix which in many applications can be non-trivial to obtain. In this paper we focus on the problem of performing spectral clustering under a budget constraint where there is a limit on the number of entries which can be queried from the similarity matrix. We propose two algorithms for this problem and study them theoretically and experimentally. These algorithms allow a tradeoff between computational efficiency and actual performance and are also relevant for the problem of speeding up standard spectral clustering.   [pdf]  [supplementary]
18,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Machine Learning Markets,Amos Storkey,"15:716-724, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/storkey11a/storkey11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_storkey11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/storkey11a.html,Prediction markets show considerable promise for developing flexible mechanisms for machine learning. Here machine learning markets for multivariate systems are defined and a utility-based framework is established for their analysis. It is shown that such markets can implement model combination methods used in machine learning such as product of expert and mixture of expert approaches as equilibrium pricing models by varying agent utility functions. They can implement models composed of local potentials and message passing methods. Prediction markets also allow for more flexible combinations by combining multiple different utility functions.   [pdf]
19,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Active Boosted Learning (ActBoost),"Kirill Trapeznikov, Venkatesh Saligrama, David Castanon","15:743-751, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/trapeznikov11a/trapeznikov11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_trapeznikov11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/trapeznikov11a.html,Active learning deals with the problem of selecting a small subset of examples to label from a pool of unlabeled data for training a good classifier. We develop an active learning algorithm algorithm in the boosting framework. In contrast to much of the recent efforts which has focused on selecting the most ambiguous unlabeled example to label based on the current learned classifier our algorithm selects examples to maximally reduce the volume of the version space of feasible boosted classifiers. We show that under suitable sparsity assumptions this strategy achieves the generalization error performance of a boosted classifier trained on the entire data set while only selecting logarithmically many unlabeled samples to label. We also establish a partial negative result in that with out imposing structural assumptions it is difficult to guarantee generalization error performance. We explicitly characterize our convergence rate in terms of the sign pattern differences produced by the weak learners on the unlabeled data. We also present a convex relaxation to account for the non-convex sparse structure and show that the computational complexity of the resulting algorithm scales polynomially in the number of weak learners. We test ActBoost on several datasets to illustrate its performance and demonstrate its robustness to initialization.   [pdf]  [supplementary]
20,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Cross-Domain Object Matching with Model Selection,"Makoto Yamada, Masashi Sugiyama","15:807-815, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/yamada11a/yamada11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_yamada11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/yamada11a.html,The goal of cross-domain object matching (CDOM) is to find correspondence between two sets of objects in different domains in an unsupervised way. Photo album summarization is a typical application of CDOM where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system. CDOM is usually formulated as finding a mapping from objects in one domain (photos) to objects in the other domain (frame) so that the pairwise dependency is maximized. A state-of-the-art CDOM method employs a kernel-based dependency measure but it has a drawback that the kernel parameter needs to be determined manually. In this paper we propose alternative CDOM methods that can naturally address the model selection problem. Through experiments on image matching unpaired voice conversion and photo album summarization tasks the effectiveness of the proposed methods is demonstrated.   [pdf]
21,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Multi-Label Output Codes using Canonical Correlation Analysis,"Yi Zhang, Jeff Schneider","15:873-882, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhang11c/zhang11c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhang11c,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhang11c.html,Traditional error-correcting output codes (ECOCs) decompose a multi-class classification problem into many binary problems. Although it seems natural to use ECOCs for multi-label problems as well doing so naively creates issues related to: the validity of the encoding the efficiency of the decoding the predictability of the generated codeword and the exploitation of the label dependency. Using canonical correlation analysis we propose an error-correcting code for multi-label classification. Label dependency is characterized as the most predictable directions in the label space which are extracted as canonical output variates and encoded into the codeword. Predictions for the codeword define a graphical model of labels with both Bernoulli potentials (from classifiers on the labels) and Gaussian potentials (from regression on the canonical output variates). Decoding is performed by efficient mean-field approximation. We establish connections between the proposed code and research areas such as compressed sensing and ensemble learning. Some of these connections contribute to better understanding of the new code and others lead to practical improvements in code design. In our empirical study the proposed code leads to substantial improvements compared to various competitors in music emotion classification and outdoor scene recognition.   [pdf]
22,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Semi-supervised Learning by Higher Order Regularization,"Xueyuan Zhou, Mikhail Belkin","15:892-900, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11b/zhou11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhou11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11b.html,In semi-supervised learning at the limit of infinite unlabeled points while fixing labeled ones the solutions of several graph Laplacian regularization based algorithms were shown by Nadler et al. (2009) to degenerate to constant functions with ``spikes'' at labeled points in $\R^d$ for $d\ge 2$. These optimization problems all use the graph Laplacian regularizer as a common penalty term. In this paper we address this problem by using regularization based on an iterated Laplacian which is equivalent to a higher order Sobolev semi-norm. Alternatively it can be viewed as a generalization of the thin plate spline to an unknown submanifold in high dimensions. We also discuss relationships between Reproducing Kernel Hilbert Spaces and Green's functions. Experimental results support our analysis by showing consistently improved results using iterated Laplacians.   [pdf]
23,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Error Analysis of Laplacian Eigenmaps for Semi-supervised Learning,"Xueyuan Zhou, Nathan Srebro","15:901-908, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11c/zhou11c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhou11c,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11c.html,We study the error and sample complexity of semi-supervised learning by Laplacian Eignmaps at the limit of infinite unlabeled data. We provide a bound on the error and show that it is controlled by the graph Laplacian regularizer. Our analysis also gives guidance to the choice of the number of eigenvectors $k$ to use: when the data lies on a $d$-dimensional domain the optimal choice of $k$ is of order $(n/\log(n))^{\frac{d}{d+2}}$ yielding an asymptotic error rate of $(n/\log(n))^{-\frac{2}{2+d}}$.   [pdf]
24,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Active Learning with Clustering,"Z. Bod„, Z. Minier & L. Csat„ ; 16:127_139, 2011. [ abs ] [ pdf ]","16:127_139, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/bodo11a/bodo11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,16-May-10,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_bodo11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/bodo11a.html,Active learning is an important _eld of machine learning and it is becoming more widely used in case of problems where labeling the examples in the training data set is expensive. In this paper we present a clustering-based algorithm used in the Active Learning Challenge (  http://www.causality.inf.ethz.ch/activelearning.php  ). The algorithm is based on graph clustering with normalized cuts and uses k -means to extract representative points from the data and approximate spectral clustering for e_ciently performing the computations.   Page last modified on Wed Mar 30 11:10:15 2011.
25,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Managing Uncertainty within KTD,"M. Geist & O. Pietquin ; 16:157_168, 2011. [ abs ] [ pdf ]","16:157_168, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/geist11a/geist11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,16-May-10,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_geist11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/geist11a.html,The dilemma between exploration and exploitation is an important topic in reinforcement learning (RL). Most successful approaches in addressing this problem tend to use some uncertainty information about values estimated during learning. On another hand scalability is known as being a lack of RL algorithms and value function approximation has become a major topic of research. Both problems arise in real-world applications however few approaches allow approximating the value function while maintaining uncertainty information about estimates. Even fewer use this information in the purpose of addressing the exploration/exploitation dilemma. In this paper we show how such an uncertainty information can be derived from a Kalman-based Temporal Di_erences (KTD) framework and how it can be used.   Page last modified on Wed Mar 30 11:10:34 2011.
26,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Personalized Ranking for Non-Uniformly Sampled Items,"Z. Gantner, L. Drumond, C. Freudenthaler & L. Schmidt-Thieme","18:231_247, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/gantner12a/gantner12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,21-Aug-11,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_gantner12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/gantner12a.html,We develop an adapted version of the Bayesian Personalized Ranking ( BPR ) optimization criterion (Rendle et al. 2009) that takes the non-uniform sampling of negative test items „ as in track 2 of the KDD Cup 2011 „ into account. Furthermore we present a modi_ed version of the generic BPR learning algorithm that maximizes the new criterion. We use it to train ranking matrix factorization models as components of an ensemble. Additionally we combine the ranking predictions with rating prediction models to also take into account rating data. With an ensemble of such combined models we ranked 8th (out of more than 300 teams) in track 2 of the KDD Cup 2011 without using the additional taxonomic information o_ered by the competition organizers.   Page last modified on Tue May 29 10:23:38 2012.
27,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Feature Engineering in Userês Music Preference Prediction,"J. Xie, S. Leishman, L. Tian, D. Lisuk, S. Koo & M. Blume","18:183_197, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/xie12a/xie12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,21-Aug-11,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_xie12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/xie12a.html,The second track of this yearÍs KDD Cup asked contestants to separate a userÍs highly rated songs from unrated songs for a large set of Yahoo! Music listeners. We cast this task as a binary classi_cation problem and addressed it utilizing gradient boosted decision trees. We created a set of highly predictive features each with a clear explanation. These features were grouped into _ve categories: hierarchical linkage features track-based statistical features user-based statistical features features derived from the k -nearest neighbors of the users and features derived from the k -nearest neighbors of the items. No music domain knowledge was needed to create these features. We demonstrate that each group of features improved the prediction accuracy of the classi_cation model. We also discuss the top predictive features of each category in this paper.   Page last modified on Tue May 29 10:23:29 2012.
28,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Blackwell Approachability and No-Regret Learning are Equivalent,"Jacob Abernethy, Peter L. Bartlett, Elad Hazan","19:27-46, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/abernethy11b/abernethy11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_abernethy11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/abernethy11b.html,We consider the celebrated Blackwell Approachability Theorem for two-player games with vector payoffs. Blackwell himself previously showed that the theorem implies the existence of a ``no-regret'' algorithm for a simple online learning problem. We show that this relationship is in fact much stronger that Blackwell's result is equivalent to in a very strong sense the problem of regret minimization for Online Linear Optimization. We show that any algorithm for one such problem can be efficiently converted into an algorithm for the other. We provide one novel application of this reduction: the first \emph{efficient} algorithm for calibrated forecasting.
29,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Multiclass Learnability and the ERM principle,"Amit Daniely, Sivan Sabato, Shai Ben-David, Shai Shalev-Shwartz","19:207-232, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/daniely11a/daniely11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_daniely11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/daniely11a.html,Multiclass learning is an area of growing practical relevance for which thecurrently available theory is still far from providing satisfactoryunderstanding. We study the learnability of multiclass prediction and deriveupper and lower bounds on the sample complexity of multiclass hypothesisclasses in different learning models: batch/online realizable/unrealizablefull information/bandit feedback. Our analysis reveals a surprisingphenomenon: In the multiclass setting in sharp contrast to binaryclassification not all Empirical Risk Minimization (ERM) algorithms areequally successful. We show that there exist hypotheses classes for which someERM learners have lower sample complexity than others. Furthermore there areclasses that are learnable by some ERM learners while other ERM learner willfail to learn them. We propose a principle for designing good ERM learners anduse this principle to prove tight bounds on the sample complexity of learning{\em symmetric} multiclass hypothesis classes (that is classes that areinvariant under any permutation of label names). We demonstrate the relevanceof the theory by analyzing the sample complexity of two widely used hypothesisclasses: generalized linear multiclass models and reduction trees. We also obtainsome practically relevant conclusions.
30,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond,"Aur_lien Garivier, Olivier Capp_","19:359-376, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/garivier11a/garivier11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_garivier11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/garivier11a.html,This paper presents a finite-time analysis of the KL-UCB algorithm an online horizon-free index policy for stochastic bandit problems. We prove two distinct results: first for arbitrary bounded rewards the KL-UCB algorithm satisfies a uniformly better regret bound than UCB and its variants; second in the special case of Bernoulli rewards it reaches the lower bound of Lai and Robbins. Furthermore we show that simple adaptations of the KL-UCB algorithm are also optimal for specific classes of (possibly unbounded) rewards including those generated from exponential families of distributions. A large-scale numerical study comparing KL-UCB with its main competitors (UCB MOSS UCB-Tuned UCB-V DMED) shows that KL-UCB is remarkably efficient and stable including for short time horizons. KL-UCB is also the only method that always performs better than the basic UCB policy. Our regret bounds rely on deviations results of independent interest which are stated and proved in the Appendix. As a by-product we also obtain an improved regret bound for the standard UCB algorithm.
31,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Sparsity Regret Bounds for Individual Sequences in Online Linear Regression,S_bastien Gerchinovitz,"19:377-396, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/gerchinovitz11a/gerchinovitz11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_gerchinovitz11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/gerchinovitz11a.html,We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension $d$ can be much larger than the number of time rounds $T$. We introduce the notion of sparsity regret bound which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm on {i.i.d.}\ data and derive risk bounds of the same flavor as in \citet{DaTsy08SEWDaTsy10MirrorAveraging} but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian.
32,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Bounds on Individual Risk for Log-loss Predictors,"Peter D. Grônwald, Wojciech Kot_owski","19:815-818, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/grunwald11b/grunwald11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_grunwald11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/grunwald11b.html,In sequential prediction with log-loss as well as density estimationwith risk measured by KL divergence one is often interested in the{\em expected instantaneous loss} or equivalently the {\em individual risk\/} at a given fixed sample size $n$. For Bayesianprediction and estimation methods it is often easy to obtain boundson the {\em cumulative risk}. Such results are based on bounding theindividual sequence regret a technique that is very well known in theCOLT community. Motivated by the easiness of proofs for the cumulativerisk our open problem is to use the results on cumulative risk to prove corresponding individual-risk bounds.
33,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Robust approachability and regret minimization in games with partial monitoring,"Shie Mannor, Vianney Perchet, Gilles Stoltz","19:515-536, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/mannor11a/mannor11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_mannor11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/mannor11a.html,Approachability has become a standard tool in analyzing learning algorithms in the adversarial online learning setup.We develop a variant of approachability for games where there is ambiguity in the obtained reward that belongs to a set rather than being a single vector. Using this variant we tackle the problem of approachability in games with partial monitoring and develop simple and efficientalgorithms (i.e. with constant per-step complexity) for this setup.We finally consider external and internal regret in repeated games with partial monitoring for which we deriveregret-minimizing strategies based on approachability theory.
34,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Agnostic KWIK learning and efficient approximate reinforcement learning,"Istvˆn Szita, Csaba Szepesvˆri","19:739-772, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/szita11a/szita11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_szita11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/szita11a.html,A popular approach in reinforcement learning is to use a model-based algorithm i.e. an algorithm that utilizes a model learner to learn an approximate model to the environment.It has been shown that such a model-based learner is efficient if the model learner is efficient in the so-called ``knows what it knows'' (KWIK) framework.A major limitation of the standard KWIK framework is that by its very definition it covers only the case when the (model) learner can represent the actual environment with no errors.In this paper we study the agnostic KWIK learning model where we relax this assumption by allowing nonzero approximation errors. We show that with the new definition an efficient model learner still leads to an efficient reinforcement learning algorithm.At the same time though we find that learning within the new framework can be substantially slower as compared to the standard framework even in the case of simple learning problems.
35,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Nonlinear Online Classi_cation Algorithm with Probability Margin,"M. Chi, H. He & W. Zhang","20:33_46, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/chi11/chi11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_chi11,http://jmlr.csail.mit.edu/proceedings/papers/v20/chi11.html,Usually it is necessary for nonlinear online learning algorithms to store a set of misclassi_ed observed examples for computing kernel values. For large-scale problems this is not only time consuming but leads also to an out-of-memory problem. In the paper a nonlinear online classi_cation algorithm is proposed with a probability margin to address the problem. In particular the discriminant function is de_ned by the Gaussian mixture model with the statistical information of all the observed examples instead of data points. Then the learnt model is used to train a nonlinear online classi_cation algorithm with con_dence such that the corresponding margin is de_ned by probability. When doing so the internal memory is signi_cantly reduced while the classi_cation performance is kept. Also we prove mistake bounds in terms of the generative model. Experiments carried out on one synthesis and two real large-scale data sets validate the e_ectiveness of the proposed approach.   Page last modified on Sun Nov 6 15:42:08 2011.
36,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Summarization of Yes/No Questions Using a Feature Function Model,J. He & D. Dai,"20:351_366, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/he11/he11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_he11,http://jmlr.csail.mit.edu/proceedings/papers/v20/he11.html,Answer summarization is an important problem in the study of Question and Answering. In this paper we deal with the general questions with ñYes _ Noî answers in English. We design 1) a model to score the relevance of the answers and the questions and 2) a feature function combining the relevance and opinion scores to classify each answer to be ñYesî ñNoî or ñNeutralî. We combine the opinion features together with two weighting scores to solve this problem and conduct experiments on a real word dataset. Given an input question the system _rstly detects if it can be simply answered by ñYes/Noî or not and then outputs the resulting voting numbers of ñYesî answers and ñNoî answers to this question. We also _rst proposed the accuracy precision and recall to the ñYes/Noî answer detection.   Page last modified on Sun Nov 6 15:44:26 2011.
37,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Multi-label Active Learning with Auxiliary Learner,C.-W. Hung & H.-T. Lin,"20:315_332, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/hung11/hung11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_hung11,http://jmlr.csail.mit.edu/proceedings/papers/v20/hung11.html,Multi-label active learning is an important problem because of the expensive labeling cost in multi-label classi_cation applications. A state-of-the-art approach for multi-label active learning maximum loss reduction with maximum con_dence (MMC) heavily depends on the binary relevance support vector machine in both learning and querying. Nevertheless it is not clear whether the heavy dependence is necessary or unrivaled. In this work we extend MMC to a more general framework that removes the heavy dependence and clari_es the roles of each component in MMC. In particular the framework is characterized by a major learner for making predictions an auxiliary learner for helping with query decisions and a query criterion based on the disagreement between the two learners. The framework takes MMC and several baseline multi-label active learning algorithms as special cases. With the §exibility of the general framework we design two criteria other than the one used by MMC. We also explore the possibility of using learners other than the binary relevance support vector machine for multi-label active learning. Experimental results demonstrate that a new criterion soft Hamming loss reduction is usually better than the original MMC criterion across di_erent pairs of major/auxiliary learners and validate the usefulness of the proposed framework.   Page last modified on Sun Nov 6 15:44:12 2011.
38,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Unsupervised Multiple Kernel Learning,"J. Zhuang, J. Wang, S.C.H. Hoi & X. Lan","20:129_144, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/zhuang11/zhuang11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_zhuang11,http://jmlr.csail.mit.edu/proceedings/papers/v20/zhuang11.html,Traditional multiple kernel learning (MKL) algorithms are essentially supervised learning in the sense that the kernel learning task requires the class labels of training data. However class labels may not always be available prior to the kernel learning task in some real world scenarios e.g. an early preprocessing step of a classi_cation task or an unsupervised learning task such as dimension reduction. In this paper we investigate a problem of Unsupervised Multiple Kernel Learning (UMKL) which does not require class labels of training data as needed in a conventional multiple kernel learning task. Since a kernel essentially de_nes pairwise similarity between any two examples our unsupervised kernel learning method mainly follows two intuitive principles: (1) a good kernel should allow every example to be well reconstructed from its localized bases weighted by the kernel values; (2) a good kernel should induce kernel values that are coincided with the local geometry of the data. We formulate the unsupervised multiple kernel learning problem as an optimization task and propose an e_cient alternating optimization algorithm to solve it. Empirical results on both classi_cation and dimension reductions tasks validate the e_cacy of the proposed UMKL algorithm.    Page last modified on Sun Nov 6 15:42:54 2011.
39,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Clearing Restarting Automata and Grammatical Inference,Peter _erno,"21:54-68, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/cerno12a/cerno12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_cerno12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/cerno12a.html,Clearing and subword-clearing restarting automata are linguistically motivated models of automata. We investigate the problem of grammatical inference for such automata based on the given set of positive and negative samples. We show that it is possible to identify these models in the limit. In this way we can learn a large class of languages. On the other hand we prove that the task of finding a clearing restarting automaton consistent with a given set of positive and negative samples is NP-hard provided that we impose an upper bound on the width of its instructions.
40,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Beyond Semilinearity: Distributional Learning of Parallel Multiple Context-free Grammars,Alexander Clark and Ryo Yoshinaka,"21:84-96, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/clark12a/clark12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_clark12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/clark12a.html,Semilinearity is widely held to be a linguistic invariant but controversially some linguistic phenomena in languages like Old Georgian and Yoruba seem to violate this constraint. In this paper we extend distributional learning to the class of parallel multiple context-free grammars a class which as far as is known includes all attested natural languages even taking an extreme view on these examples. These grammars may have a copying operation that can recursively copy constituents allowing them to generate non-semilinear languages. We generalise the notion of a context to a class of functions that include copying operations. The congruential approach is ineffective at this level of the hierarchy; accordingly we extend this using dual approaches defining nonterminals using sets of these generalised contexts. As a corollary we also extend the multiple context free grammars using the lattice based approaches.
41,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Learning of Bi-_ Languages from Factors,"M. Jayasrirani, M. H. Begam, D. G. Thomas D.G and J. D. Emerald","21:139-144, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/jayasri12a/jayasri12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_jayasri12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/jayasri12a.html,Higuera and Janodet (2001) gave a polynomial algorithm that identifies the class of safe _-languages which is a subclass of deterministic _-languages from positive and negative prefixes. As an extension of this work we study the learning of the family of bi-_ languages.
42,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Inducing Partially Observable Markov Decision Processes,Michael L. Littman,"21:145-148, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/littman12a/littman12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_littman12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/littman12a.html,The partially observable Markov decision process (POMDP) model plays an important role in the field of reinforcement learning. It captures the problem of decision making when some important features of the environment are not visible to the decision maker. A number of approaches have been proposed for inducing POMDP models from data a problem that has important parallels with grammar induction.
43,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,A Lattice of Sets of Alignments Built on the Common Subwords in a Finite Language,"Laurent Miclet, Nelly Barbot and Baptiste Jeudy","21:164-176, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/miclet12a/miclet12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_miclet12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/miclet12a.html,We define the locally maximal subwords and locally minimal superwords common to a finite set of words. We also define the corresponding sets of alignments. We give a partial order relation between such sets of alignments as well as two operations between them. We show that the constructed family of sets of alignments has the lattice structure. We give hints to use this structure as a machine learning basis for inducing a generalization of the set of words.
44,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Bootstrapping Dependency Grammar Inducers from Incomplete Sentence Fragments via Austere Models,"Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Jurafsky","21:189-194, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/spitkovsky12a/spitkovsky12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_spitkovsky12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/spitkovsky12a.html,Modern grammar induction systems often employ curriculum learning strategies that begin by training on a subset of all available input that is considered simpler than the full data. Traditionally filtering has been at granularities of whole input units e.g. discarding entire sentences with too many words or punctuation marks. We propose instead viewing inter-punctuation fragments as atoms initially thus making some simple phrases and clauses of complex sentences available to training sooner. Splitting input text at punctuation in this way improved our state-of-the-art grammar induction pipeline. We observe that resulting partial data i.e. mostly incomplete sentence fragments can be analyzed using reduced parsing models which we show can be easier to bootstrap than more nuanced grammars. Starting with a new bare dependency-and-boundary model (DBM-0) our grammar inducer attained 61.2% directed dependency accuracy on Section 23 (all sentences) of the Wall Street Journal corpus: more than 2% higher than previous published results for this task.
45,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Family of MCMC Methods on Implicitly Defined Manifolds,"Marcus Brubaker, Mathieu Salzmann, Raquel Urtasun",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/brubaker12/brubaker12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_brubaker12,http://jmlr.csail.mit.edu/proceedings/papers/v22/brubaker12.html,Traditional MCMC methods are only applicable to distributions which can be defined on $\mathbb{R}^n$. However there exist many application domains where the distributions cannot easily be defined on a Euclidean space. To address this limitation we propose a general constrained version of Hamiltonian Monte Carlo and give conditions under which the Markov chain is convergent. Based on this general framework we define a family of MCMC methods which can be applied to sample from distributions on non-linear manifolds. We demonstrate the effectiveness of our approach on a variety of problems including sampling from the Bingham-von Mises-Fisher distribution collaborative filtering and human pose estimation.
46,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Classifier Cascade for Minimizing Feature Evaluation Cost,"Minmin Chen, Zhixiang Xu, Kilian Weinberger, Olivier Chapelle, Dor Kedem",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12c/chen12c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_chen12c,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12c.html,Machine learning algorithms are increasingly used in large-scale industrial settings. Here the operational cost during test-time has to be taken into account when an algorithm is designed. This operational cost is affected by the average running time and the computation time required for feature extraction. When a diverse set of features is used the latter can vary drastically. In this paper we propose an algorithm that constructs a cascade of classifiers that explicitly trades-off operational cost and classifier accuracy while accounting for on-demand feature extraction costs. Different from previous work our algorithm re-optimizes trained classifiers and allows expensive features to be scheduled at any stage within the cascade to minimize overall cost. Experiments on actual web-search ranking data sets demonstrate that our framework leads to drastic test-time improvements.
47,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A metric learning perspective of SVM: on the relation of LMNN and SVM,"Huyen Do, Alexandros Kalousis, Jun Wang, Adam Woznica",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/do12/do12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_do12,http://jmlr.csail.mit.edu/proceedings/papers/v22/do12.html,Support Vector Machines SVMs and the Large Margin Nearest Neighbor algorithm LMNN are two very popular learning algorithms with quite different learning biases. In this paper we bring them into a unified view and show that they have a much stronger relation than what is commonly thought. We analyze SVMs from a metric learning perspective and cast them as a metric learning problem a view which helps us uncover the relations of the two algorithms. We show that LMNN can be seen as learning a set of local SVM-like models in a quadratic space. Along the way and inspired by the metric-based interpretation of SVMs we derive a novel variant of SVMs \epsilon-SVM to which LMNN is even more similar. We give a unified view of LMNN and the different SVM variants. Finally we provide some preliminary experiments on a number of benchmark datasets in which show that \epsilon-SVM compares favorably both with respect to LMNN and SVM.
48,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Lifted coordinate descent for learning with trace-norm regularization,"Miroslav Dudik, Zaid Harchaoui, Jerome Malick",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/dudik12/dudik12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_dudik12,http://jmlr.csail.mit.edu/proceedings/papers/v22/dudik12.html,We consider the minimization of a smooth loss with trace-norm regularization which is a natural objective in multi-class and multi-task learning. Even though the problem is convex existing approaches rely on optimizing a non-convex variational bound which is not guaranteed to converge or repeatedly perform singular-value decomposition which prevents scaling beyond moderate matrix sizes. We lift the non-smooth convex problem into an infinitely dimensional smooth problem and apply coordinate descent to solve it. We prove that our approach converges to the optimum and is competitive or outperforms state of the art.
49,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,No Internal Regret via Neighborhood Watch,"Dean Foster, Alexander Rakhlin",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/foster12/foster12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_foster12,http://jmlr.csail.mit.edu/proceedings/papers/v22/foster12.html,We present an algorithm which attains O(\sqrt{T}) internal (and thus external) regret for finite games with partial monitoring under the local observability condition. Recently this condition has been shown by Bartok Pal and Szepesvari (2011) to imply the O(\sqrt{T}) rate for partial monitoring games against an i.i.d. opponent and the authors conjectured that the same holds for non-stochastic adversaries. Our result is in the affirmative and it completes the characterization of possible rates for finite partial-monitoring games an open question stated by Cesa-Bianchi Lugosi and Stoltz (2006). Our regret guarantees also hold for the more general model of partial monitoring with random signals.
50,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Semiparametric Pseudo-Likelihood Estimation in Markov Random Fields,Antonino Freno,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/freno12/freno12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_freno12,http://jmlr.csail.mit.edu/proceedings/papers/v22/freno12.html,Probabilistic graphical models for continuous variables can be built out of either parametric or nonparametric conditional density estimators. While several research efforts have been focusing on parametric approaches (such as Gaussian models) kernel-based estimators are still the only viable and well-understood option for nonparametric density estimation. This paper develops a semiparametric estimator of probability density functions based on the nonparanormal transformation which has been recently proposed for mapping arbitrarily distributed data samples onto normally distributed datasets. Pointwise and uniform consistency properties are established for the developed method. The resulting density model is then applied to pseudo-likelihood estimation in Markov random fields. An experimental evaluation on data distributed according to a variety of density functions indicates that such semiparametric Markov random field models significantly outperform both their Gaussian and kernel-based alternatives in terms of prediction accuracy.
51,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian regularization of non-homogeneous dynamic Bayesian networks by globally coupling interaction parameters,"Marco Grzegorzyk, Dirk Husmeier",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/grzegorzyk12/grzegorzyk12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_grzegorzyk12,http://jmlr.csail.mit.edu/proceedings/papers/v22/grzegorzyk12.html,To relax the homogeneity assumption of classical dynamic Bayesian networks (DBNs) various recent studies have combined DBNs with multiple changepoint processes. The underlying assumption is that the parameters associated with time series segments delimited by multiple changepoints are a priori independent. Under weak regularity conditions the parameters can be integrated out in the likelihood leading to a closed-form expression of the marginal likelihood. However the assumption of prior independence is unrealistic in many real-world applications where the segment-specific regulatory relationships among the interdependent quantities tend to undergo gradual evolutionary adaptations. We therefore propose a Bayesian coupling scheme to introduce systematic information sharing among the segment-specific interaction parameters. We investigate the effect this model improvement has on the network reconstruction accuracy in a reverse engineering context where the objective is to learn the structure of a gene regulatory network from temporal gene expression profiles.
52,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Information Theoretic Model Validation for Spectral Clustering,"Morteza Haghir Chehreghani, Alberto Giovanni Busetto, Joachim M. Buhmann",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/haghir12/haghir12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_haghir12,http://jmlr.csail.mit.edu/proceedings/papers/v22/haghir12.html,Model validation constitutes a fundamental step in data clustering. The central question is: Which cluster model and how many clusters are most appropriate for a certain application? In this study we introduce a method for the validation of spectral clustering based upon approximation set coding. In particular we compare correlation and pairwise clustering to analyze the correlations of temporal gene expression profiles. To evaluate and select clustering models we calculate their reliable informativeness. Experimental results in the context of gene expression analysis show that pairwise clustering yields superior amounts of reliable information. The analysis results are consistent with the Bayesian Information Criterion (BIC) and exhibit higher generality than BIC.
53,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Stochastic Bandit Based on Empirical Moments,"Junya Honda, Akimichi Takemura",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/honda12/honda12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_honda12,http://jmlr.csail.mit.edu/proceedings/papers/v22/honda12.html,In the multiarmed bandit problem a gambler chooses an arm of a slot machine to pull considering a tradeoff between exploration and exploitation. We study the stochastic bandit problem where each arm has a reward distribution supported in [01]. For this model there exists a policy which achieves the theoretical bound asymptotically. However the optimal policy requires a computation of a convex optimization which involves the empirical distribution of each arm. In this paper we propose a policy which exploits the first d empirical moments for arbitrary d fixed in advance. We show that the performance of the policy approaches the theoretical bound as d increases. This policy can be implemented by solving polynomial equations which we derive the explicit solution for d smaller than 5. By choosing appropriate d the proposed policy realizes a tradeoff between the computational complexity and the expected regret.
54,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Detecting Network Cliques with Radon Basis Pursuit,"Xiaoye Jiang, Yuan Yao, Han Liu, Leonidas Guibas",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/jiang12/jiang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_jiang12,http://jmlr.csail.mit.edu/proceedings/papers/v22/jiang12.html,In this paper we propose a novel formulation of the network clique detection problem by introducing a general network data representation framework. We show connections between our formulation with a new algebraic tool namely Radon basis pursuit in homogeneous spaces. Such a connection allows us to identify rigorous recovery conditions for clique detection problems. Practical approximation algorithms are also developed for solving empirical problems and their usefulness is demonstrated on real-world datasets. Our work connects two seemingly different areas: network data analysis and compressed sensing which helps to bridge the gap between the research of network data and the classical theory of statistical learning and signal processing.
55,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,High-dimensional Sparse Inverse Covariance Estimation using Greedy Methods,"Christopher Johnson, Ali Jalali, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/johnson12/johnson12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_johnson12,http://jmlr.csail.mit.edu/proceedings/papers/v22/johnson12.html,In this paper we consider the task of estimating the non-zero pattern of the sparse inverse covariance matrix of a zero-mean Gaussian random vector from a set of iid samples. Note that this is also equivalent to recovering the underlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We present two novel greedy approaches to solving this problem. The first estimates the non-zero covariates of the overall inverse covariance matrix using a series of global forward and backward greedy steps. The second estimates the neighborhood of each node in the graph separately again using greedy forward and backward steps and combines the intermediate neighborhoods to form an overall estimate. The principal contribution of this paper is a rigorous analysis of the sparsistency or consistency in recovering the sparsity pattern of the inverse covariance matrix. Surprisingly we show that both the local and global greedy methods learn the full structure of the model with high probability given just O(d log(p)) samples which is a significant improvement over state of the art L1-regularized Gaussian MLE (Graphical Lasso) that requires O(d^2 log(p)) samples. Moreover the restricted eigenvalue and smoothness conditions imposed by our greedy methods are much weaker than the strong irrepresentable conditions required by the L1-regularization based methods. We corroborate our results with extensive simulations and examples comparing our local and global greedy methods to the L1-regularized Gaussian MLE as well as the nodewise L1-regularized linear regression (Neighborhood Lasso).
56,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian Classifier Combination,"Hyun-Chul Kim, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kim12/kim12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kim12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kim12.html,Bayesian model averaging linearly mixes the probabilistic predictions of multiple models each weighted by its posterior probability. This is the coherent Bayesian way of combining multiple models only under certain restrictive assumptions which we outline. We explore a general framework for Bayesian model combination (which differs from model averaging) in the context of classification. This framework explicitly models the relationship between each model's output and the unknown true label. The framework does not require that the models be probabilistic (they can even be human assessors) that they share prior information or receive the same training data or that they be independent in their errors. Finally the Bayesian combiner does not need to believe any of the models is in fact correct. We test several variants of this classifier combination procedure starting from a classic statistical model proposed by Dawid and Skene (1979) and using MCMC to add more complex but important features to the model. Comparisons on several data sets to simpler methods like majority voting show that the Bayesian methods not only perform well but result in interpretable diagnostics on the data points and the models.
57,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Marginal Regression For Multitask Learning,"Mladen Kolar, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kolar12/kolar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kolar12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kolar12.html,Variable selection is an important practical problem that arises in analysis of many high-dimensional datasets. Convex optimization procedures that arise from relaxing the NP-hard subset selection procedure e.g. the Lasso or Dantzig selector have become the focus of intense theoretical investigations. Although many efficient algorithms exist that solve these problems finding a solution when the number of variables is large e.g. several hundreds of thousands in problems arising in genome-wide association analysis is still computationally challenging. A practical solution for these high-dimensional problems is the marginal regression where the output is regressed on each variable separately. We investigate theoretical properties of the marginal regression in a multitask framework. Our contribution include: i) sharp analysis for the marginal regression in a single task setting with random design ii) sufficient conditions for the multitask screening to select the relevant variables iii) a lower bound on the Hamming distance convergence for multitask variable selection problems. A simulation study further demonstrates the performance of the marginal regression.
58,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian Comparison of Machine Learning Algorithms on Single and Multiple Datasets,"Alexandre Lacoste, Francois Laviolette, Mario Marchand",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/lacoste12/lacoste12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_lacoste12,http://jmlr.csail.mit.edu/proceedings/papers/v22/lacoste12.html,We propose a new method for comparing learning algorithms on multiple tasks which is based on a novel non-parametric test that we call the Poisson binomial test. The key aspect of this work is that we provide a formal definition for what is meant to have an algorithm that is better than another. Also we are able to take into account the dependencies induced when evaluating classifiers on the same test set. Finally we make optimal use (in the Bayesian sense) of all the testing data we have. We demonstrate empirically that our approach is more reliable than the sign test and the Wilcoxon signed rank test the current state of the art for algorithm comparisons.
59,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Closed-Form Entropy Limits - A Tool to Monitor Likelihood Optimization of Probabilistic Generative Models,"Jorg Lucke, Marc Henniges",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/lucke12/lucke12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_lucke12,http://jmlr.csail.mit.edu/proceedings/papers/v22/lucke12.html,The maximization of the data likelihood under a given probabilistic generative model is the essential goal of many algorithms for unsupervised learning. If expectation maximization is used for optimization a lower bound on the data likelihood the free-energy is optimized. The parameter-dependent part of the free-energy (the difference between free-energy and posterior entropy) is the essential entity in the derivation of learning algorithms. Here we show that for many common generative models the optimal values of the parameter-dependent part can be derived in closed-form. These closed-form expressions are hereby given as sums of the negative (differential) entropies of the individual model distributions. We apply our theoretical results to derive such closed-form expressions for a number of common and recent models including probabilistic PCA factor analysis different versions of sparse coding and Linear Dynamical Systems. The main contribution of this work is theoretical but we show how the derived results can be used to efficiently compute free-energies and how they can be used for consistency checks of learning algorithms.
60,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,The adversarial stochastic shortest path problem with unknown transition probabilities,"Gergely Neu, Andras Gyorgy, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/neu12/neu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_neu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/neu12.html,We consider online learning in a special class of episodic Markovian decision processes namely loop-free stochastic shortest path problems. In this problem an agent has to traverse through a finite directed acyclic graph with random transitions while maximizing the obtained rewards along the way. We assume that the reward function can change arbitrarily between consecutive episodes and is entirely revealed to the agent at the end of each episode. Previous work was concerned with the case when the stochastic dynamics is known ahead of time whereas the main novelty of this paper is that this assumption is lifted. We propose an algorithm called ``follow the perturbed optimistic policy'' that combines ideas from the ``follow the perturbed leader'' method for online learning of arbitrary sequences and ``upper confidence reinforcement learning'' an algorithm for regret minimization in Markovian decision processes (with a fixed reward function). We prove that the expected cumulative regret of our algorithm is of order $L X A\sqrt{T}$ up to logarithmic factors where $L$ is the length of the longest path in the graph $\X$ is the state space $\A$ is the action space and $T$ is the number of episodes. To our knowledge this is the first algorithm that learns and controls stochastic and adversarial components in an online fashion at the same time.
61,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Nonparametric Bayesian Model for Multiple Clustering with Overlapping Feature Views,"Donglin Niu, Jennifer Dy, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/niu12/niu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_niu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/niu12.html,Most clustering algorithms produce a single clustering solution. This is inadequate for many data sets that are multi-faceted and can be grouped and interpreted in many different ways. Moreover for high-dimensional data different features may be relevant or irrelevant to each clustering solution suggesting the need for feature selection in clustering. Features relevant to one clustering interpretation may be different from the ones relevant for an alternative interpretation or view of the data. In this paper we introduce a probabilistic nonparametric Bayesian model that can discover multiple clustering solutions from data and the feature subsets that are relevant for the clusters in each view. In our model the features in different views may be shared and therefore the sets of relevant features are allowed to overlap. We model feature relevance to each view using an Indian Buffet Process and the cluster membership in each view using a Chinese Restaurant Process. We provide an inference approach to learn the latent parameters corresponding to this multiple partitioning problem. Our model not only learns the features and clusters in each view but also automatically learns the number of clusters number of views and number of features in each view.
62,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Part & Clamp: Efficient Structured Output Learning,"Patrick Pletscher, Cheng Soon Ong",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/pletscher12a/pletscher12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_pletscher12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/pletscher12a.html,Discriminative training for general graphical models is a challenging task due to the intractability of the partition function. We propose a computationally efficient approach to estimate the partition sum in a structured learning problem. The key idea is a lower bound of the partition sum that can be evaluated in a fixed number of message passing iterations. The bound makes use of a subset of the variables a feedback vertex set which allows us to decompose the graph into tractable parts. Furthermore a tightening strategy for the bound is presented which finds the states of the feedback vertex set that maximally increase the bound and clamps them. Based on this lower bound we derive batch and online learning algorithms and demonstrate their effectiveness on a computer vision problem.
63,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Learning Low-order Models for Enforcing High-order Statistics,"Patrick Pletscher, Pushmeet Kohli",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/pletscher12b/pletscher12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_pletscher12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/pletscher12b.html,Models such as pairwise conditional random fields (CRFs) are extremely popular in computer vision and various other machine learning disciplines. However they have limited expressive power and often cannot represent the posterior distribution correctly. While learning the parameters of such models which have insufficient expressivity researchers use loss functions to penalize certain misrepresentations of the solution space. Till now researchers have used only simplistic loss functions such as the hamming loss to enable efficient inference. The paper shows how sophisticated and useful higher order loss functions can be incorporated in the learning process. These loss functions ensure that the MAP solution does not deviate much from the ground truth in terms of certain \emph{higher order statistics}. We propose a learning algorithm which uses the recently proposed lower-envelope representation of higher order functions to transform them to pairwise functions which allow efficient inference. We test the efficacy of our method on the problem of foreground-background image segmentation. Experimental results show that the incorporation of higher order loss functions in the learning formulation using our method leads to much better results compared to those obtained by using the traditional hamming loss.
64,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Exploiting Unrelated Tasks in Multi-Task Learning,"Bernardino Romera Paredes, Andreas Argyriou, Nadia Berthouze, Massimiliano Pontil",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/romera12/romera12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_romera12,http://jmlr.csail.mit.edu/proceedings/papers/v22/romera12.html,We study the problem of learning a group of principal tasks using a group of auxiliary tasks unrelated to the principal ones. In many applications joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task essentially screening out idiosyncrasies of the data distribution. We propose a novel method which builds on a prior multitask methodology by favoring a shared low dimensional representation within each group of tasks. In addition we impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. We further discuss a condition which ensures convexity of the optimization problem and argue that it can be solved by alternating minimization. We present experiments on synthetic and real data which indicate that incorporating unrelated tasks can improve significantly over standard multi-task learning methods.
65,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Domain Adaptation: A Small Sample Statistical Approach,"Ruslan Salakhutdinov, Sham Kakade, Dean Foster",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/salakhutdinov12/salakhutdinov12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_salakhutdinov12,http://jmlr.csail.mit.edu/proceedings/papers/v22/salakhutdinov12.html,We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains but where we have many samples in each domain. Our goal is to generalize to a new domain. For example we may want to learn a similarity function using only certain classes of objects but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that ``dogs are similar to dogs'' even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.
66,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Local Anomaly Detection,"Venkatesh Saligrama, Manqi Zhao",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/saligrama12/saligrama12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_saligrama12,http://jmlr.csail.mit.edu/proceedings/papers/v22/saligrama12.html,Anomalies with spatial and temporal stamps arise in a number of applications including communication networks traffic monitoring and video analysis. In these applications anomalies are temporally or spatially localized but otherwise unknown. We propose a novel graph-based statistical notion that unifies the idea of temporal and spatial locality. This notion lends itself to an elegant characterization of optimal decision rules and in turn suggests corresponding empirical rules based on local nearest neighbor distances. We compute a single composite score for the entire spatio-temporal data sample based on the local neighborhood distances. We declare data samples as containing local anomalies based on the composite score. We show that such rules not only asymptotically guarantee desired false alarm control but are also asymptotically optimal. We also show that our composite scoring scheme overcomes the inherent resolution issues of alternative multi-comparison approaches that are based on fusing the outcomes of location-by-location comparisons. We then verify our algorithms on synthetic and real data sets.
67,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Fast Variational Bayesian Inference for Non-Conjugate Matrix Factorization Models,"Matthias Seeger, Guillaume Bouchard",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/seeger12/seeger12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_seeger12,http://jmlr.csail.mit.edu/proceedings/papers/v22/seeger12.html,Probabilistic matrix factorization methods aim to extract meaningful correlation structure from an incomplete data matrix by postulating low rank constraints. Recently variational Bayesian (VB) inference techniques have successfully been applied to such large scale bilinear models. However current algorithms are of the alternate updating or stochastic gradient descent type slow to converge and prone to getting stuck in shallow local minima. While for MAP or maximum margin estimation singular value shrinkage algorithms have been proposed which can far outperform alternate updating this methodological avenue remains unexplored for Bayesian techniques. In this paper we show how to combine a recent singular value shrinkage characterization of fully observed spherical Gaussian VB matrix factorization with augmented Lagrangian techniques in order to obtain efficient VB inference for general MF models with arbitrary likelihood potentials. In particular we show how to handle Poisson and Bernoulli potentials far more suited for most MF applications than Gaussian likelihoods. Our algorithm can be run even for very large models and is easily implemented in {\em Matlab}. It outperforms MAP estimation on a range of real-world datasets.
68,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Using More Data to Speed-up Training Time,"Shai Shalev-Shwartz, Ohad Shamir, Eran Tromer",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/shalev-shwartz12/shalev-shwartz12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_shalev-shwartz12,http://jmlr.csail.mit.edu/proceedings/papers/v22/shalev-shwartz12.html,In many recent applications data is plentiful. By now we have a rather clear understanding of how more data can be used to improve the accuracy of learning algorithms. Recently there has been a growing interest in understanding how more data can be leveraged to reduce the required training runtime. In this paper we study the runtime of learning as a function of the number of available training examples and underscore the main high-level techniques. We provide the first formal positive result showing that even in the unrealizable case the runtime can decrease exponentially while only requiring a polynomial growth of the number of examples. Our construction corresponds to a synthetic learning problem and an interesting open question is whether the tradeoff can be shown for more natural learning problems. We spell out several interesting candidates of natural learning problems for which we conjecture that there is a tradeoff between computational and sample complexity.
69,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Complexity of Bethe Approximation,Jinwoo Shin,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/shin12/shin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_shin12,http://jmlr.csail.mit.edu/proceedings/papers/v22/shin12.html,This paper resolves a common complexity issue in the Bethe approximation of statistical physics and the sum-product Belief Propagation (BP) algorithm of artificial intelligence. The Bethe approximation reduces the problem of computing the partition function in a graphical model to that of solving a set of non-linear equations so-called the Bethe equation. On the other hand the BP algorithm is a popular heuristic method for estimating marginal distribution in a graphical model. Although they are inspired and developed from different directions Yedidia Freeman and Weiss (2004) established a somewhat surprising connection: the BP algorithm solves the Bethe equation if it converges (however it often does not). This naturally motivates the following important question to understand their limitations and empirical successes: the Bethe equation is computationally easy to solve? We present a message passing algorithm solving the Bethe equation in polynomial number of bitwise operations for arbitrary binary graphical models of n nodes where the maximum degree in the underlying graph is O(log n). Our algorithm an alternative to BP fixing its convergence issue is the first fully polynomial-time approximation scheme for the BP fixed point computation in such a large class of graphical models. Moreover we believe that our technique is of broader interest to understand the computational complexity of the cavity method in statistical physics.
70,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,On Bisubmodular Maximization,"Ajit Singh, Andrew Guillory, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/singh12/singh12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_singh12,http://jmlr.csail.mit.edu/proceedings/papers/v22/singh12.html,Bisubmodularity extends the concept of submodularity to set functions with two arguments. We show how bisubmodular maximization leads to richer value-of-information problems using examples in sensor placement and feature selection. We present the first constant-factor approximation algorithm for a wide class of bisubmodular maximizations.
71,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,On Nonparametric Guidance for Learning Autoencoder Representations,"Jasper Snoek, Ryan Adams, Hugo Larochelle",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/snoek12/snoek12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_snoek12,http://jmlr.csail.mit.edu/proceedings/papers/v22/snoek12.html,Unsupervised discovery of latent representations in addition to being useful for density modeling visualisation and exploratory data analysis is also increasingly important for learning features relevant to discriminative tasks. Autoencoders in particular have proven to be an effective way to learn latent codes that reflect meaningful variations in data. A continuing challenge however is guiding an autoencoder toward representations that are useful for particular tasks. A complementary challenge is to find codes that are invariant to irrelevant transformations of the data. The most common way of introducing such problem-specific guidance in autoencoders has been through the incorporation of a parametric component that ties the latent representation to the label information. In this work we argue that a preferable approach relies instead on a nonparametric guidance mechanism. Conceptually it ensures that there exists a function that can predict the label information without explicitly instantiating that function. The superiority of this guidance mechanism is confirmed on two datasets. In particular this approach is able to incorporate invariance information (lighting elevation etc.) from the small NORB object recognition dataset and yields state-of-the-art performance for a single layer non-convolutional network.
72,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Fast Learning Rate of Multiple Kernel Learning: Trade-Off between Sparsity and Smoothness,"Taiji Suzuki, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/suzuki12/suzuki12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_suzuki12,http://jmlr.csail.mit.edu/proceedings/papers/v22/suzuki12.html,We investigate the learning rate of multiple kernel leaning (MKL) with L1 and elastic-net regularizations. The elastic-net regularization is a composition of an L1-regularizer for inducing the sparsity and an L2-regularizer for controlling the smoothness. We focus on a sparse setting where the total number of kernels is large but the number of non-zero components of the ground truth is relatively small and show sharper convergence rates than the learning rates ever shown for both L1 and elastic-net regularizations. Our analysis shows there appears a trade-off between the sparsity and the smoothness when it comes to selecting which of L1 and elastic-net regularizations to use; if the ground truth is smooth the elastic-net regularization is preferred otherwise the L1 regularization is preferred.
73,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Multiresolution Deep Belief Networks,"Yichuan Tang, Abdel-Rahman Mohamed",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/tang12/tang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_tang12,http://jmlr.csail.mit.edu/proceedings/papers/v22/tang12.html,Motivated by the observation that coarse and fine resolutions of an image reveal different structures in the underlying visual phenomenon we present a model based on the Deep Belief Network (DBN) which learns features from the multiscale representation of images. A Laplacian Pyramid is first constructed for each image. DBNs are then trained separately at each level of the pyramid. Finally a top level RBM combines these DBNs into a single network we call the Multiresolution Deep Belief Network (MrDBN). Experiments show that MrDBNs generalize better than standard DBNs on NORB classification and TIMIT phone recognition. In the domain of generative learning we demonstrate the superiority of MrDBNs at modeling face images.
74,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Generalized Optimal Reverse Prediction,"Martha White, Dale Schuurmans",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/white12/white12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_white12,http://jmlr.csail.mit.edu/proceedings/papers/v22/white12.html,"Recently it has been shown that classical supervised and unsupervised training methods can be unified as special cases of so-called ""optimal reverse prediction"": predicting inputs from target labels while optimizing over both model parameters and missing labels. Although this perspective establishes links between classical training principles the existing formulation only applies to linear predictors under squared loss hence is extremely limited. We generalize the formulation of optimal reverse prediction to arbitrary Bregman divergences and more importantly to nonlinear predictors. This extension is achieved by establishing a new generalized form of forward-reverse minimization equivalence that holds for arbitrary matching losses. Several benefits follow. First a new variant of Bregman divergence clustering can be recovered that incorporates a non-linear data reconstruction model. Second normalized-cut and kernel-based extensions can be formulated coherently. Finally a new semi-supervised training principle can be recovered for classification problems that demonstrates advantages over the state of the art."
75,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Statistical Optimization in High Dimensions,"Huan Xu, Constantine Caramanis, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/xu12a/xu12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_xu12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/xu12a.html,We consider optimization problems whose parameters are known only approximately based on a noisy sample. Of particular interest is the high-dimensional regime where the number of samples is roughly equal to the dimensionality of the problem and the noise magnitude may greatly exceed the magnitude of the signal itself. This setup falls far outside the traditional scope of Robust and Stochastic optimization. We propose three algorithms to address this setting combining ideas from statistics machine learning and robust optimization. In the important case where noise artificially increases the dimensionality of the parameters we show that combining robust optimization and dimensionality reduction can result in high-quality solutions at greatly reduced computational cost.
76,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Robust Multi-task Regression with Grossly Corrupted Observations,"Huan Xu, Chenlei Leng",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/xu12b/xu12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_xu12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/xu12b.html,We consider the multiple-response regression problem where the response is subject to *sparse gross errors* in the high-dimensional setup. We propose a tractable regularized M-estimator that is robust to such error where the sum of two individual regularization terms are used: the first one encourages row-sparse regression parameters and the second one encourages a sparse error term. We obtain non-asymptotical estimation error bounds of the proposed method. To the best of our knowledge this is the first analysis of the robust multi-task regression problem with gross errors.
77,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Active Learning from Multiple Knowledge Sources,"Yan Yan, Romer Rosales, Glenn Fung, Faisal Farooq, Bharat Rao, Jennifer Dy",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/yan12/yan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_yan12,http://jmlr.csail.mit.edu/proceedings/papers/v22/yan12.html,Some supervised learning tasks do not fit the usual single annotator scenario. In these problems ground-truth may not exist and multiple annotators are generally available. A few approaches have been proposed to address this learning problem. In this setting active learning (AL) the problem of optimally selecting unlabeled samples for labeling offers new challenges and has received little attention. In multiple annotator AL it is not sufficient to select a sample for labeling since in addition an optimal annotator must also be selected. This setting is of great interest as annotators' expertise generally varies and could depend on the given sample itself; additionally some annotators may be adversarial. Thus clearly the information provided by some annotators should be more valuable than that provided by others and it could vary across data points. We propose an AL approach for this new scenario motivated by information theoretic principles. Specifically we focus on maximizing the information that an annotator label provides about the true (but unknown) label of the data point. We develop this concept propose an algorithm for active learning and experimentally validate the proposed approach.
78,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Perturbation based Large Margin Approach for Ranking,"Eunho Yang, Ambuj Tewari, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/yang12/yang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_yang12,http://jmlr.csail.mit.edu/proceedings/papers/v22/yang12.html,The use of the standard hinge loss for structured outputs for the learning to rank problem faces two main caveats: (a) the label space the set of all possible permutations of items to be ranked is too large and also less amenable to the usual dynamic-programming based techniques used for structured outputs and (b) the supervision or training data consists of instances with multiple labels per input instead of just a single label. The most natural way to deal with such multiple labels leads unfortunately to a non-convex surrogate. In this paper we propose a general class of perturbation-based surrogates that leverage the large margin approach and are convex. We show that the standard hinge surrogate for classification actually falls within this class. We also find a surrogate within this class for the ranking problem that does not suffer from the caveats mentioned above. Indeed our experiments demonstrate that it performs better than other candidate large margin proposals on both synthetic and real world ranking datasets.
79,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Beta-Negative Binomial Process and Poisson Factor Analysis,"Mingyuan Zhou, Lauren Hannah, David Dunson, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12c/zhou12c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhou12c,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12c.html,"A beta-negative binomial (BNB) process is proposed leading to a beta-gamma-Poisson process which may be viewed as a ""multi-scoop"" generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization."
80,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Predicting the functions of proteins in Protein-Protein Interaction networks from global information,"Hossein Rahmani, Hendrik Blockeel, Andreas Bender","8:82-97, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/rahmani10a/rahmani10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_rahmani10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/rahmani10a.html,In this work we present a novel approach to predict the function of proteins in protein-protein interaction (PPI) networks. We classify existing approaches into inductive and transductive approaches and into local and global approaches. As of yet among the group of inductive approaches only local ones have been proposed for protein function prediction. We here introduce a protein description formalism that also includes global information namely information that locates a protein relative to specific important proteins in the network. We analyze the effect on function prediction accuracy of selecting a different number of important proteins. With around 70 important proteins even in large graphs our method makes good and stable predictions. Furthermore we investigate whether our method also classifies proteins accurately on more detailed function levels. We examined up to five different function levels. The method is benchmarked on four datasets where we found classification performance according to F-measure values indeed improves by 9 percent over the benchmark methods employed.
81,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Focused Belief Propagation for Query-Specific Inference,"Anton Chechetka, Carlos Guestrin","9:89-96, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/chechetka10a/chechetka10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_chechetka10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/chechetka10a.html,"With the increasing popularity of large-scale probabilistic graphical models even ""lightweight"" approximate inference methods are becoming infeasible. Fortunately often large parts of the model are of no immediate interest to the end user. Given the variable that the user actually cares about we show how to quantify edge importance in graphical models and to significantly speed up inference by focusing computation on important parts of the model. Our algorithm empirically demonstrates convergence speedup by multiple times over state of the art"
82,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines,"Guillaume Desjardins, Aaron Courville, Yoshua Bengio, Pascal Vincent, Olivier Delalleau","9:145-152, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/desjardins10a/desjardins10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_desjardins10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/desjardins10a.html,Alternating Gibbs sampling is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM) a crucial component in deep architectures such as Deep Belief Networks. However we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this hinders the advantage that could in principle be brought by training algorithms relying on Gibbs sampling for uncovering spurious modes such as the Persistent Contrastive Divergence algorithm. To alleviate this problem we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs. We find both through visualization of samples and measures of likelihood on a toy dataset that it helps both sampling and learning.
83,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Nonparametric Bayesian Matrix Factorization by Power-EP,"Nan Ding, Yuan Qi, Rongjing Xiang, Ian Molloy, Ninghui Li","9:169-176, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ding10a/ding10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ding10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ding10a.html,Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix as the product of two latent matrices matrix factorization can reveal hidden structures embedded in data. A common challenge to use matrix factorization is determining the dimensionality of the latent matrices from data. Indian Buffet Processes (IBPs) enable us to apply the nonparametric Bayesian machinery to address this challenge. However it remains a difficult task to learn nonparametric Bayesian matrix factorization models. In this paper we propose a novel variational Bayesian method based on new equivalence classes of infinite matrices for learning these models. Furthermore inspired by the success of nonnegative matrix factorization on many learning problems we impose nonnegativity constraints on the latent matrices and mix variational inference with expectation propagation. This mixed inference method is unified in a power expectation propagation framework. Experimental results on image decomposition demonstrate the superior computational efficiency and the higher prediction accuracy of our methods compared to alternative Monte Carlo and variational inference methods for IBP models. We also apply the new methods to collaborative filtering and role mining and show the improved predictive performance over other matrix factorization methods.
84,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Semi-Supervised Learning via Generalized Maximum Entropy,"Ayse Erkan, Yasemin Altun","9:209-216, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/erkan10a/erkan10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_erkan10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/erkan10a.html,Various supervised inference methods can be analyzed as convex duals of the generalized maximum entropy (MaxEnt) framework. Generalized MaxEnt aims to find a distribution that maximizes an entropy function while respecting prior information represented as potential functions in miscellaneous forms of constraints and/or penalties. We extend this framework to semi-supervised learning by incorporating unlabeled data via modifications to these potential functions reflecting structural assumptions on the data geometry. The proposed approach leads to a family of discriminative semi-supervised algorithms that are convex scalable inherently multi-class easy to implement and that can be kernelized naturally. Experimental evaluation of special cases shows the competitiveness of our methodology.
85,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Regret Bounds for Gaussian Process Bandit Problems,"Steffen Grônew_lder, Jean_Yves Audibert, Manfred Opper, John Shawe_Taylor","9:273-280, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/grunewalder10a/grunewalder10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_grunewalder10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/grunewalder10a.html,Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modeled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function defining the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds.
86,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Sufficient covariates and linear propensity analysis,"Hui Guo, Philip Dawid","9:281-288, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/guo10a/guo10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_guo10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/guo10a.html,"Working within the decision-theoretic framework for causal inference we study the properties of ""sufficient covariates"" which support causal inference from observational data and possibilities for their reduction. In particular we illustrate the role of a propensity variable by means of a simple model and explain why such a reduction typically does not increase (and may reduce) estimation efficiency."
87,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Dirichlet Process Mixtures of Generalized Linear Models,"Lauren Hannah, David Blei, Warren Powell","9:313-320, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/hannah10a/hannah10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_hannah10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/hannah10a.html,We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLMs) a new method of nonparametric regression that accommodates continuous and categorical inputs models a response variable locally by a generalized linear model. We give conditions for the existence and asymptotic unbiasedness of the DP-GLM regression mean function estimate; we then give a practical example for when those conditions hold. We evaluate DP-GLM on several data sets comparing it to modern methods of nonparametric regression including regression trees and Gaussian processes.
88,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Nonlinear functional regression: a functional RKHS approach,"Hachem Kadri, Emmanuel Duflos, Philippe Preux, St_phane Canu, Manuel Davy","9:374-380, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kadri10a/kadri10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kadri10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kadri10a.html,This paper deals with functional regression in which the input attributes as well as the response are functions. To deal with this problem we develop a functional reproducing kernel Hilbert space approach; here a kernel is an operator acting on a function and yielding a function. We demonstrate basic properties of these functional RKHS as well as a representer theorem for this setting; we investigate the construction of kernels; we provide some experimental insight.
89,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity,"Sham Kakade, Ohad Shamir, Karthik Sindharan, Ambuj Tewari","9:381-388, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kakade10a/kakade10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kakade10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kakade10a.html,The versatility of exponential families along with their attendant convexity properties make them a popular and effective statistical model. A central issue is learning these models in high-dimensions when the optimal parameter vector is sparse. This work characterizes a certain strong convexity property of general exponential families which allows their generalization ability to be quantified. In particular we show how this property can be used to analyze generic exponential families under L1 regularization.
90,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Semi-Supervised Learning with Max-Margin Graph Cuts,"Branislav Kveton, Michal Valko, Ali Rahimi, Ling Huang","9:421-428, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kveton10a/kveton10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kveton10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kveton10a.html,This paper proposes a novel algorithm for semi-supervised learning. This algorithm learns graph cuts that maximize the margin with respect to the labels induced by the harmonic function solution. We motivate the approach compare it to existing work and prove a bound on its generalization error. The quality of our solutions is evaluated on a synthetic problem and three UCI ML repository datasets. In most cases we outperform manifold regularization of support vector machines which is a state-of-the-art approach to semi-supervised max-margin learning.
91,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Solving the Uncapacitated Facility Location Problem Using Message Passing Algorithms,"Nevena Lazic, Brendan Frey, Parham Aarabi","9:429-436, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/lazic10a/lazic10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_lazic10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/lazic10a.html,The Uncapacitated Facility Location Problem (UFLP) is one of the most widely studied discrete location problems whose applications arise in a variety of settings. We tackle the UFLP using probabilistic inference in a graphical model - an approach that has received little attention in the past. We show that the fixed points of max-product linear programming (MPLP) a convexified version of the max-product algorithm can be used to construct a solution with a 3-approximation guarantee for metric UFLP instances. In addition we characterize some scenarios under which the MPLP solution is guaranteed to be globally optimal. We evaluate the performance of both max-sum and MPLP empirically on metric and non-metric problems demonstrating the advantages of the 3-approximation construction and algorithm applicability to non-metric instances.
92,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Exploiting Covariate Similarity in Sparse Regression via the Pairwise Elastic Net,"Alexander Lorbert, David Eis, Victoria Kostina, David Blei, Peter Ramadge","9:477-484, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/lorbert10b/lorbert10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_lorbert10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/lorbert10b.html,A new approach to regression regularization called the Pairwise Elastic Net is proposed. Like the Elastic Net it simultaneously performs automatic variable selection and continuous shrinkage. In addition the Pairwise Elastic Net encourages the grouping of strongly correlated predictors based on a pairwise similarity measure. We give examples of how the Pairwise Elastic Net can be used to achieve the objectives of Ridge regression the Lasso the Elastic Net and Group Lasso. Finally we present a coordinate descent algorithm to solve the Pairwise Elastic Net.
93,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A generalization of the Multiple-try Metropolis algorithm for Bayesian estimation and model selection,"Silvia Pandolfi, Francesco Bartolucci, Nial Friel","9:581-588, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/pandolfi10a/pandolfi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_pandolfi10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/pandolfi10a.html,We propose a generalization of the Multiple-try Metropolis (MTM) algorithm of Liu et al. (2000) which is based on drawing several proposals at each step and randomly choosing one of them on the basis of weights that may be arbitrary chosen. In particular for Bayesian estimation we also introduce a method based on weights depending on a quadratic approximation of the posterior distribution. The resulting algorithm cannot be reformulated as an MTM algorithm and leads to a comparable gain of efficiency with a lower computational effort. We also outline the extension of the proposed strategy and then of the MTM strategy to Bayesian model selection casting it in a Reversible Jump framework. The approach is illustrated by real examples.
94,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,REGO: Rank-based Estimation of Renyi Information using Euclidean Graph Optimization,"Barnabas Poczos, Sergey Kirshner, Csaba Szepesvˆri","9:605-612, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/poczos10a/poczos10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_poczos10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/poczos10a.html,We propose a new method for a non-parametric estimation of Renyi and Shannon information for a multivariate distribution using a corresponding copula a multivariate distribution over normalized ranks of the data. As the information of the distribution is the same as the negative entropy of its copula our method estimates this information by solving a Euclidean graph optimization problem on the empirical estimate of the distribution's copula. Owing to the properties of the copula we show that the resulting estimator of Renyi information is strongly consistent and robust. Further we demonstrate its applicability in the image registration in addition to simulated experiments.
95,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images,"Marc'Aurelio Ranzato, Alex Krizhevsky, Geoffrey Hinton","9:621-628, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ranzato10a/ranzato10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ranzato10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ranzato10a.html,"Deep belief nets have been successful in modeling handwritten characters but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the ""tiny images"" data set. Even better features are obtained by then using standard binary RBM's to learn a deeper model."
96,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Gaussian processes with monotonicity information,"Jaakko Riihim_ki, Aki Vehtari","9:645-652, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/riihimaki10a/riihimaki10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_riihimaki10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/riihimaki10a.html,A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.
97,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A Regularization Approach to Nonlinear Variable Selection,"Lorenzo Rosasco, Matteo Santoro, Sofia Mosci, Alessandro Verri, Silvia Villa","9:653-660, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/rosasco10a/rosasco10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_rosasco10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/rosasco10a.html,In this paper we consider a regularization approach to variable selection when the regression function depends nonlinearly on a few input variables. The proposed method is based on a regularized least square estimator penalizing large values of the partial derivatives. An efficient iterative procedure is proposed to solve the underlying variational problem and its convergence is proved. The empirical properties of the obtained estimator are tested both for prediction and variable selection. The algorithm compares favorably to more standard ridge regression and L1 regularization schemes.
98,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Reducing Label Complexity by Learning From Bags,"Sivan Sabato, Nathan Srebro, Naftali Tishby","9:685-692, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sabato10a/sabato10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sabato10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sabato10a.html,We consider a supervised learning setting in which the main cost of learning is the number of training labels and one can obtain a single label for a bag of examples indicating only if a positive example exists in the bag as in Multi-Instance Learning. We thus propose to create a training sample of bags and to use the obtained labels to learn to classify individual examples. We provide a theoretical analysis showing how to select the bag size as a function of the problem parameters and prove that if the original labels are distributed unevenly the number of required labels drops considerably when learning from bags. We demonstrate that finding a low-error separating hyperplane from bags is feasible in this setting using a simple iterative procedure similar to latent SVM. Experiments on synthetic and real data sets demonstrate the success of the approach.
99,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Nonparametric Tree Graphical Models,"Le Song, Arthur Gretton, Carlos Guestrin","9:765-772, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/song10a/song10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_song10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/song10a.html,We introduce a nonparametric representation for graphical model on trees which expresses marginals as Hilbert space embeddings and conditionals as embedding operators. This formulation allows us to define a graphical model solely on the basis of the feature space representation of its variables. Thus this nonparametric model can be applied to general domains where kernels are defined handling challenging cases such as discrete variables whose domains are huge or very complex non-Gaussian continuous distributions. We also derive \emph{kernel belief propagation} a Hilbert-space algorithm for performing inference in our model. We show that our method outperforms state-of-the-art techniques in a cross-lingual document retrieval task and a camera rotation estimation problem.
100,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Sufficient Dimension Reduction via Squared-loss Mutual Information Estimation,"Taiji Suzuki, Masashi Sugiyama","9:804-811, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/suzuki10a/suzuki10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_suzuki10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/suzuki10a.html,The goal of sufficient dimension reduction in supervised learning is to find the lowdimensional subspace of input features that is sufficient for predicting output values. In this paper we propose a novel sufficient dimension reduction method using a squaredloss variant of mutual information as a dependency measure. We utilize an analytic approximator of squared-loss mutual information based on density ratio estimation which is shown to possess suitable convergence properties. We then develop a natural gradient algorithm for sufficient subspace search. Numerical experiments show that the proposed method compares favorably with existing dimension reduction approaches.
101,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A Markov-Chain Monte Carlo Approach to Simultaneous Localization and Mapping,"Peter Torma, Andrˆs Gy_rgy, Csaba Szepesvˆri","9:852-859, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/torma10a/torma10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_torma10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/torma10a.html,A Markov-Chain Monte Carlo based algorithm is provided to solve the simultaneous localization and mapping (SLAM) problem with general dynamical and observation models under open-loop control and provided that the map-representation is finite dimensional. To our knowledge this is the first provably consistent yet (close-to) practical solution to this problem. The superiority of our algorithm over alternative SLAM algorithms is demonstrated in a difficult loop closing situation.
102,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning Causal Structure from Overlapping Variable Sets,"Sofia Triantafillou, Ioannis Tsamardinos, Ioannis Tollis","9:860-867, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/triantafillou10a/triantafillou10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_triantafillou10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/triantafillou10a.html,We present an algorithm name cSAT+ for learning the causal structure in a domain from datasets measuring different variables sets. The algorithm outputs a graph with edges corresponding to all possible pairwise causal relations between two variables named Pairwise Causal Graph (PCG). Examples of interesting inferences include the induction of the absence or presence of some causal relation between two variables never measured together. cSAT+ converts the problem to a series of SAT problems obtaining leverage from the efficiency of state-of-the-art solvers. In our empirical evaluation it is shown to outperform ION the first algorithm solving a similar but more general problem by two orders of magnitude.
103,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Online Passive-Aggressive Algorithms on a Budget,"Zhuang Wang, Slobodan Vucetic","9:908-915, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/wang10b/wang10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_wang10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/wang10b.html,In this paper a kernel-based online learning algorithm which has both constant space and update time is proposed. The approach is based on the popular online Passive-Aggressive (PA) algorithm. When used in conjunction with kernel function the number of support vectors in PA grows without bounds when learning from noisy data streams. This implies unlimited memory and ever increasing model update and prediction time. To address this issue the proposed budgeted PA algorithm maintains only a fixed number of support vectors. By introducing an additional constraint to the original PA optimization problem a closed-form solution was derived for the support vector removal and model update. Using the hinge loss we developed several budgeted PA algorithms that can trade between accuracy and update cost. We also developed the ramp loss versions of both original and budgeted PA and showed that the resulting algorithms can be interpreted as the combination of active learning and hinge loss PA. All proposed algorithms were comprehensively tested on 7 benchmark data sets. The experiments showed that they are superior to the existing budgeted online algorithms. Even with modest budgets the budgeted PA achieved very competitive accuracies to the non-budgeted PA and kernel perceptron algorithms.
104,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Multi-Task Learning using Generalized t Process,"Yu Zhang, Dit_Yan Yeung","9:964-971, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10c/zhang10c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhang10c,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10c.html,Multi-task learning seeks to improve the generalization performance of a learning task with the help of other related learning tasks. Among the multi-task learning methods proposed thus far Bonilla et al.'s method provides a novel multi-task extension of Gaussian process (GP) by using a task covariance matrix to model the relationships between tasks. However learning the task covariance matrix directly has both computational and representational drawbacks. In this paper we propose a Bayesian extension by modeling the task covariance matrix as a random matrix with an inverse-Wishart prior and integrating it out to achieve Bayesian model averaging. To make the computation feasible we first give an alternative weight-space view of Bonilla et al.'s multi-task GP model and then integrate out the task covariance matrix in the model leading to a multi-task generalized t process (MTGTP). For the likelihood we use a generalized t noise model which together with the generalized t process prior brings about the robustness advantage as well as an analytical form for the marginal likelihood. In order to specify the inverse-Wishart prior we use the maximum mean discrepancy (MMD) statistic to estimate the parameter matrix of the inverse-Wishart prior. Moreover we investigate some theoretical properties of MTGTP such as its asymptotic analysis and learning curve. Comparative experimental studies on two common multi-task learning applications show very promising results.
105,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An embarrassingly simple approach to zero-shot learning,"Bernardino Romera-Paredes, Philip Torr",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/romera-paredes15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/romera-paredes15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_romera-paredes15,http://jmlr.csail.mit.edu/proceedings/papers/v37/romera-paredes15.html,"Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to \(17\%\) ."
106,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,HMMPayl: an application of HMM to the analysis of the HTTP Payload,Davide Ariu and Giorgio Giacinto,"11:81-87, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/ariu10a/ariu10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_ariu10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/ariu10a.html,"Zero-days attacks are one of the most dangerous threats against computer networks. These, by definition, are attacks never seen before. Thus, defense tools based on a database of rules (usually referred as –signatures”) that describe known attacks cannot do anything against them. Recently, defense tools based on machine learning algorithms have gained an increasing popularity as they offer the possibility to fight off also zero-days attacks. In this paper we propose HMMPayl, an anomaly based Intrusion Detection System for the protection of a web server and of the applications the server hosts. HMMPayl analyzes the network traffic toward the web server and it is based on Hidden Markov Models. With this paper we provide for several contributions. First, the algorithm implemented by HMMPayl allows to carefully model the payload increasing the classification accuracy with respect to previously proposed solutions. Second, we show that an approach based on multiple classifiers leads to an increased classification accuracy with respect to the case where a single classifier is used. Third, exploiting the redundancy within the information extracted from the payload we propose a solution to reduce the computational cost of the algorithm."
107,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Gap Between Theory and Practice: Noise Sensitive Word Alignment in Machine Translation,"Tsuyoshi Okita, Yvette Graham and Andy Way","11:119-126, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/okita10a/okita10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_okita10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/okita10a.html,"Word alignment is to estimate a lexical translation probability p ( e | f ), or to estimate the correspondence g ( e , f ) where a function g outputs either 0 or 1, between a source word f and a target word e for given bilingual sentences. In practice, this formulation does not consider the existence of 'noise' (or outlier) which may cause problems depending on the corpus. N -to- m mapping objects, such as paraphrases, non-literal translations, and multi-word expressions, may appear as both noise and also as valid training data. From this perspective, this paper tries to answer the following two questions: 1) how to detect stable patterns where noise seems legitimate, and 2) how to reduce such noise, where applicable, by supplying extra information as prior knowledge to a word aligner."
108,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning Policy Improvements with Path Integrals,"Evangelos Theodorou, Jonas Buchli, Stefan Schaal","9:828-835, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/theodorou10a/theodorou10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_theodorou10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/theodorou10a.html,With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parametrized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations policy improvements can be transformed into an approximation problem of a path integral which has no open parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based semi-model-based or even model free depending on how the learning problem is structured. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. We believe that Policy Improvement with Path Integrals PI^2 offers currently one of the most efficient numerically robust and easy to implement algorithms for RL based on trajectory roll-outs.
109,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,The sample complexity of agnostic learning under deterministic labels,"Shai Ben-David, Ruth Urner","JMLR W&CP 35 :527-542, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/ben-david14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_ben-david14,http://jmlr.csail.mit.edu/proceedings/papers/v35/ben-david14.html,"With the emergence of Machine Learning tools that allow handling data with a huge number of features, it becomes reasonable to assume that, over the full set of features, the true labeling is (almost) fully determined. That is, the labeling function is deterministic, but not necessarily a member of some known hypothesis class. However, agnostic learning of deterministic labels has so far received little research attention. We investigate this setting and show that it displays a behavior that is quite different from that of the fundamental results of the common (PAC) learning setups. First, we show that the sample complexity of learning a binary hypothesis class (with respect to deterministic labeling functions) is not fully determined by the VC-dimension of the class. For any \(d\) , we present classes of VC-dimension \(d\) that are learnable from \({\tilde O}(d/\epsilon)\) -many samples and classes that require samples of size \(\Omega(d/\epsilon^2)\) . Furthermore, we show that in this setup, there are classes for which any proper learner has suboptimal sample complexity. While the class can be learned with sample complexity \({\tilde O}(d/\epsilon)\) , any proper (and therefore, any ERM) algorithm requires \(\Omega(d/\epsilon^2)\) samples. We provide combinatorial characterizations of both phenomena, and further analyze the utility of unlabeled samples in this setting. Lastly, we discuss the error rates of nearest neighbor algorithms under deterministic labels and additional niceness-of-data assumptions."
110,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Anytime Concurrent Clustering of Multiple Streams with an Indexing Tree,"Zhinoos Razavi Hesabi, Timos Sellis, Xiuzhen Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/razavi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_razavi15,http://jmlr.csail.mit.edu/proceedings/papers/v41/razavi15.html,"With the advancement of data generation technologies such as sensor networks, multiple data streams are continuously generated. Clustering multiple data streams is challenging as the requirement of clustering at anytime becomes more critical. We aim to cluster multiple data streams concurrently and in this paper we report our work in progress. ClusTree is an anytime clustering algorithm for a single stream. It uses a hierarchical tree structure to index micro-clusters, which are summary statistics for streaming data objects. We design a dynamic, concurrent indexing tree structure that extends the ClusTree structure to achieve more granular micro clusters (summaries) of multiple streams at any time. We devised algorithms to search, expand and update the hierarchical tree structure of storing micro clusters concurrently, along with an algorithm for anytime concurrent clustering of multiple streams. As this is work in progress, we plan to test our proposed algorithms, on sensor data sets, and evaluate the space and time complexity of creating and accessing micro-clusters. We will also evaluate the quality of clustering in terms of number of created clusters and compare our technique with other approaches."
111,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Classification of Imbalanced Marketing Data with Balanced Random Sets,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/nikulin09/nikulin09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_nikulin09,http://jmlr.csail.mit.edu/proceedings/papers/v7/nikulin09.html,With imbalanced data a classifier built using all of the data has the tendency to ignore the minority class. To overcome this problem we propose to use an ensemble classifier constructed on the basis of a large number of relatively small and balanced subsets where representatives from both patterns are to be selected randomly. As an outcome the system produces the matrix of linear regression coefficients whose rows represent the random sub- sets and the columns represent the features. Based on this matrix we make an assessment of how stable the influence of a particular feature is. It is proposed to keep in the model only features with stable influence. The final model represents an average of the base-learners which is not necessarily a linear regression. Proper data pre-processing is very important for the effectiveness of the whole system and it is proposed to reduce the original data to the most simple binary sparse format which is particularly convenient for the construction of decision trees. As a result any particular feature will be represented by several binary variables or bins which are absolutely equivalent in terms of data structure. This property is very important and may be used for feature selection. The proposed method exploits not only contributions of particular variables to the base-learners but also the diversity of such contributions. Test results against KDD-2009 competition datasets are presented.
112,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scalable Model Selection for Large-Scale Factorial Relational Models,"Chunchen Liu, Lu Feng, Ryohei Fujimaki, Yusuke Muraoka",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/liub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_liub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/liub15.html,"With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models."
113,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables,"Robert Tillman, Peter Spirtes","15:3-15, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/tillman11a/tillman11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_tillman11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/tillman11a.html,While there has been considerable research in learning probabilistic graphical models from data for predictive and causal inference almost all existing algorithms assume a single dataset of i.i.d. observations for all variables. For many applications it may be impossible or impractical to obtain such datasets but multiple datasets of i.i.d. observations for different subsets of these variables may be available. Tillman et al. (2009) showed how directed graphical models learned from such datasets can be integrated to construct an equivalence class of structures over all variables. While their procedure is correct it assumes that the structures integrated do not entail contradictory conditional independences and dependences for variables in their intersections. While this assumption is reasonable asymptotically it rarely holds in practice with finite samples due to the frequency of statistical errors. We propose a new correct procedure for learning such equivalence classes directly from the multiple datasets which avoids this problem and is thus more practically useful. Empirical results indicate our method is not only more accurate but also faster and requires less memory.
114,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Algorithms for Robust One-bit Compressive Sensing,"Lijun Zhang, Jinfeng Yi, Rong Jin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhangc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangc14.html,"While the conventional compressive sensing assumes measurements of infinite precision, one-bit compressive sensing considers an extreme setting where each measurement is quantized to just a single bit. In this paper, we study the vector recovery problem from noisy one-bit measurements, and develop two novel algorithms with formal theoretical guarantees. First, we propose a passive algorithm, which is very efficient in the sense it only needs to solve a convex optimization problem that has a closed-form solution. Despite the apparent simplicity, our theoretical analysis reveals that the proposed algorithm can recover both the exactly sparse and the approximately sparse vectors. In particular, for a sparse vector with \(s\) nonzero elements, the sample complexity is \(O(s \log n/\epsilon^2)\) , where \(n\) is the dimensionality and \(\epsilon\) is the recovery error. This result improves significantly over the previously best known sample complexity in the noisy setting, which is \(O(s \log n/\epsilon^4)\) . Second, in the case that the noise model is known, we develop an adaptive algorithm based on the principle of active learning. The key idea is to solicit the sign information only when it cannot be inferred from the current estimator. Compared with the passive algorithm, the adaptive one has a lower sample complexity if a high-precision solution is desired."
115,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Learning Substitutable Binary Plane Graph Grammars,"R_mi Eyraud, Jean-Christophe Janodet and Tim Oates","21:114-128, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/eyraud12a/eyraud12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_eyraud12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/eyraud12a.html,While some heuristics exist for the learning of graph grammars few has been done on the theoretical side. Due to complexity issues the class of graphs has to be restricted: this paper deals with the subclass of plane graphs which correspond to drawings of planar graphs. This allows us to introduce a new kind of graph grammars using a face-replacement mechanism. To learn them we extend recent successful techniques developed for string grammars and based on a property on target languages: the substitutability property. We show how this property can be extended to plane graph languages and finally state the first identification in the limit result for a class of graph grammars as far as we know.
116,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Collaborative Filtering via Rating Concentration,"Bert Huang, Tony Jebara","9:334-341, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10a/huang10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_huang10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10a.html,While most popular collaborative filtering methods use low-rank matrix factorization and parametric density assumptions this article proposes an approach based on distribution-free concentration inequalities. Using agnostic hierarchical sampling assumptions functions of observed ratings are provably close to their expectations over query ratings on average. A joint probability distribution over queries of interest is estimated using maximum entropy regularization. The distribution resides in a convex hull of allowable candidate distributions which satisfy concentration inequalities that stem from the sampling assumptions. The method accurately estimates rating distributions on synthetic and real data and is competitive with low rank and parametric methods which make more aggressive assumptions about the problem.
117,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,"Gamma Processes, Stick-Breaking, and Variational Inference","Anirban Roychowdhury, Brian Kulis",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/roychowdhury15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/roychowdhury15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_roychowdhury15,http://jmlr.csail.mit.edu/proceedings/papers/v38/roychowdhury15.html,"While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithm on non-negative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors, as well as a direct DP-based construction of the gamma process."
118,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Globally Convergent Parallel MAP LP Relaxation Solver using the Frank-Wolfe Algorithm,"Alexander Schwing, Tamir Hazan, Marc Pollefeys, Raquel Urtasun",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/schwing14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_schwing14,http://jmlr.csail.mit.edu/proceedings/papers/v32/schwing14.html,"While MAP inference is typically intractable for many real-world applications, linear programming relaxations have been proven very effective. Dual block-coordinate descent methods are among the most efficient solvers, however, they are prone to get stuck in sub-optimal points. Although subgradient approaches achieve global convergence, they are typically slower in practice. To improve convergence speed, algorithms which compute the steepest \(\epsilon\) -descent direction by solving a quadratic program have been proposed. In this paper we suggest to decouple the quadratic program based on the Frank-Wolfe approach. This allows us to obtain an efficient and easy to parallelize algorithm while retaining the global convergence properties. Our method proves superior when compared to existing algorithms on a set of spin-glass models and protein design tasks."
119,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Deep Sparse Rectifier Neural Networks,"Xavier Glorot, Antoine Bordes, Yoshua Bengio","15:315-323, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a/glorot11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_glorot11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/glorot11a.html,While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks and closing the performance gap between neural networks learnt with and without unsupervised pre-training.
120,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Autonomous Exploration For Navigating In MDPs,Shiau Hong Lim and Peter Auer,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/lim12/lim12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_lim12,http://jmlr.csail.mit.edu/proceedings/papers/v23/lim12.html,"While intrinsically motivated learning agents hold considerable promise to overcome limitations of more supervised learning systems, quantitative evaluation and theoretical analysis of such agents are difficult. We propose to consider a restricted setting for autonomous learning where systematic evaluation of learning performance is possible. In this setting the agent needs to learn to navigate in a Markov Decision Process where extrinsic rewards are not present or are ignored. We present a learning algorithm for this scenario and evaluate it by the amount of exploration it uses to learn the environment."
121,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Dimensionality estimation without distances,"Matth_us Kleindessner, Ulrike von Luxburg",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kleindessner15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kleindessner15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kleindessner15.html,"While existing methods for estimating the intrinsic dimension of datasets require to know distances between data points, we consider a situation where one has considerably less information. Given a sample of points, all we get to see is who are the k nearest neighbors of every point. In other words, we get the adjacency matrix of the directed, unweighted k-nearest neighbor graph on the sample, but do not know any point coordinates or distances between the points. We provide two estimators for this situation, a naive one and a more elaborate one. Both of them can be proved to be statistically consistent. However, further theoretical and experimental evidence shows that the naive estimator performs rather poorly, whereas the elaborate one achieves results comparable to those of estimators based on distance information."
122,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Online Incremental Feature Learning with Denoising Autoencoders,"Guanyu Zhou, Kihyuk Sohn, Honglak Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12b/zhou12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhou12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12b.html,While determining model complexity is an important problem in machine learning many feature learning algorithms rely on cross-validation to choose an optimal number of features which is usually infeasible for online learning from a massive stream of data. In this paper we propose an incremental feature learning algorithm to determine the optimal model complexity for large-scale online datasets based on the denoising autoencoder. This algorithm is composed of two processes: adding features and merging features. Specifically it adds new features to minimize the objective function's residual and merges similar features to obtain a compact feature representation and prevent over-fitting. Our experiments show that the model quickly converges to the optimal number of features in a large-scale online setting and outperforms the (non-incremental) denoising autoencoder as well as deep belief networks and stacked denoising autoencoders for classification tasks. Further the algorithm is particularly effective in recognizing new patterns when the data distribution changes over time in the massive online data stream.
123,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Algorithms for Direct 0_1 Loss Optimization in Binary Classification,"Tan Nguyen, Scott Sanner",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_nguyen13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13a.html,"While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers. On the other hand, while the non-convex 0_1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice. In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0_1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0_1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0_1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers. To this end, we believe this work reiterates the importance of 0_1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize."
124,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Active Nearest Neighbors in Changing Environments,"Christopher Berlind, Ruth Urner",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/berlind15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/berlind15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_berlind15,http://jmlr.csail.mit.edu/proceedings/papers/v37/berlind15.html,"While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization."
125,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training,"Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, Pascal Vincent","5:153-160, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/erhan09a/erhan09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_erhan09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/erhan09a.html,Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization the positive effect of pre-training in terms of optimization and its role as a kind of regularizer. We show the influence of architecture depth model capacity and number of training examples.
126,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Unsupervised Variable Selection: when random rankings sound as irrelevancy,S_bastien Gu_rif,"4:163-177, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/guerif08a/guerif08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_guerif08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/guerif08a.html,Whereas the variable selection has been extensively studied in the context of supervised learning the unsupervised variable selection has attracted attention of researchers more recently as the available amount of unlabeled data has exploded. Many unsupervised variable ranking criteria were proposed and their relevance is usually demonstrated using either external cluster validity indexes or the accuracy of a classifier which are both supervised criteria. Actually the major issue of the variable subset selection according to a ranking measure has been adressed only by few authors in the unsupervised learning context. In this paper we propose to combine multiple ranking to go ahead toward a stable consensus variable subset in a totally unsupervised fashion.
127,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Understanding the difficulty of training deep feedforward neural networks,"Xavier Glorot, Yoshua Bengio","9:249-256, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a/glorot10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_glorot10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a.html,Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained since then several algorithms have been shown to successfully train them with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value which can drive especially the top hidden layer into saturation. Surprisingly we find that saturated units can move out of saturation by themselves albeit slowly and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally we study how activations and gradients vary across layers and during training with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations we propose a new initialization scheme that brings substantially faster convergence.
128,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization,"Jiangwen Sun, Jin Lu, Tingyang Xu, Jinbo Bi",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sunb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/sunb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sunb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sunb15.html,"When multiple views of data are available for a set of subjects, co-clustering aims to identify subject clusters that agree across the different views. We explore the problem of co-clustering when the underlying clusters exist in different subspaces of each view. We propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors. This approach is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters. The proposed algorithm can globally converge to a critical point of the problem. A simulation study validates that the proposed algorithm can identify the hypothesized clusters and their associated features. Comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed approach."
129,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Scalable Optimization of Randomized Operational Decisions in Adversarial Classification Settings,"Bo Li, Yevgeniy Vorobeychik",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15a-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15a.html,"When learning, such as classification, is used in adversarial settings, such as intrusion detection, intelligent adversaries will attempt to evade the resulting policies. The literature on adversarial machine learning aims to develop learning algorithms which are robust to such adversarial evasion, but exhibits two significant limitations: a) failure to account for operational constraints and b) a restriction that decisions are deterministic. To overcome these limitations, we introduce a conceptual separation between learning, used to infer attacker preferences, and operational decisions, which account for adversarial evasion, enforce operational constraints, and naturally admit randomization. Our approach gives rise to an intractably large linear program. To overcome scalability limitations, we introduce a novel method for estimating a compact parity basis representation for the operational decision function. Additionally, we develop an iterative constraint generation approach which embeds adversaryês best response calculation, to arrive at a scalable algorithm for computing near-optimal randomized operational decisions. Extensive experiments demonstrate the efficacy of our approach."
130,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Nonlinear low-dimensional regression using auxiliary coordinates,"Weiran Wang, Miguel Carreira-Perpinan",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/wang12/wang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_wang12,http://jmlr.csail.mit.edu/proceedings/papers/v22/wang12.html,When doing regression with inputs and outputs that are high-dimensional it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs. Much work in statistics and machine learning such as reduced-rank regression slice inverse regression and their variants has focused on linear dimensionality reduction or on estimating the dimensionality reduction first and then the mapping. We propose a method where both the dimensionality reduction and the mapping can be nonlinear and are estimated jointly. Our key idea is to define an objective function where the low-dimensional coordinates are free parameters in addition to the dimensionality reduction and the mapping. This has the effect of decoupling many groups of parameters from each other affording a far more effective optimization than if using a deep network with nested mappings and to use a good initialization from slice inverse regression or spectral methods. Our experiments with image and robot applications show our approach to improve over direct regression and various existing approaches.
131,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Feature Multi-Selection among Subjective Features,"Sivan Sabato, Adam Kalai",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sabato13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sabato13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sabato13.html,"When dealing with subjective, noisy, or otherwise nebulous features, the –wisdom of crowds” suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated –”feature multi-selection–” algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting peopleês height and weight from photos, using features such as –”gender–” and –”estimated weight–” as well as culturally fraught ones such as –”attractive–”."
132,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Model Merging versus Model Splitting Context-Free Grammar Induction,Menno van Zaanen and Nanne van Noord,"21:224-236, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/vanzaanen12b/vanzaanen12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_vanzaanen12b,http://jmlr.csail.mit.edu/proceedings/papers/v21/vanzaanen12b.html,When comparing different grammatical inference algorithms it becomes evident that generic techniques have been used in different systems. Several finite-state learning algorithms use state-merging as their underlying technique and a collection of grammatical inference algorithms that aim to learn context-free grammars build on the concept of substitutability to identify potential grammar rules. When learning context-free grammars there are essentially two approaches: model merging which generalizes with more data and model splitting which specializes with more data. Both approaches can be combined sequentially in a generic framework. In this article we investigate the impact of different approaches within the first phase of the framework on system performance.
133,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Flexible Martingale Priors for Deep Hierarchies,"Jacob Steinhardt, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/steinhardt12/steinhardt12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_steinhardt12,http://jmlr.csail.mit.edu/proceedings/papers/v22/steinhardt12.html,When building priors over trees for Bayesian hierarchical models there is a tension between maintaining desirable theoretical properties such as infinite exchangeability and important practical properties such as the ability to increase the depth of the tree to accommodate new data. We resolve this tension by presenting a family of infinitely exchangeable priors over discrete tree structures that allows the depth of the tree to grow with the data and then showing that our family contains all hierarchical models with certain mild symmetry properties. We also show that deep hierarchical models are in general intimately tied to a process called a martingale and use Doob's martingale convergence theorem to demonstrate some unexpected properties of deep hierarchies.
134,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades,"Mehrdad Farajtabar, Manuel Gomez Rodriguez, Mohammad Zamani, Nan Du, Hongyuan Zha, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/farajtabar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/farajtabar15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_farajtabar15,http://jmlr.csail.mit.edu/proceedings/papers/v38/farajtabar15.html,"When a piece of malicious information becomes rampant in an information diffusion network, can we identify the source node that originally introduced the piece into the network and infer the time when it initiated this? Being able to do so is critical for curtailing the spread of malicious information, and reducing the potential losses incurred. This is a very challenging problem since typically only incomplete traces are observed and we need to unroll the incomplete traces into the past in order to pinpoint the source. In this paper, we tackle this problem by developing a two-stage framework, which first learns a continuous-time diffusion network based on historical diffusion traces and then identifies the source of an incomplete diffusion trace by maximizing the likelihood of the trace under the learned model. Experiments on both large synthetic and real-world data show that our framework can effectively go back to the past, and pinpoint the source node and its initiation time significantly more accurately than previous state-of-the-arts."
135,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Cortical Learning via Prediction,"Christos H. Papadimitriou, Santosh S. Vempala",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Papadimitriou15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Papadimitriou15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Papadimitriou15.html,"What is the mechanism of learning in the brain? Despite breathtaking advances in neuroscience, and in machine learning, we do not seem close to an answer. Using Valiantês neuronal model as a foundation, we introduce PJOIN (for ``predictive join–), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiantês conservative, formal model of cortical computation. Using PJOIN ã and almost nothing else ã we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiantês work). This algorithm relies crucially on prediction, and entails significant downward traffic (``feedback”) while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the first theoretical prediction of their essential role in learning."
136,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Detection of Server-side Web Attacks,Igino Corona and Giorgio Giacinto,"11:160-166, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/corona10a/corona10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_corona10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/corona10a.html,"Web servers and server-side applications constitute the key components of modern Internet services. We present a pattern recognition system to the detection of intrusion attempts that target such components. Our system is anomaly-based, i.e., we model the normal (legitimate) traffic and intrusion attempts are identified as anomalous traffic. In order to address the presence of attacks (noise) inside the training set we employ an ad-hoc outlier detection technique. This approach does not require supervision and allows us to accurately detect both known and unknown attacks against web services."
137,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Sketching the Support of a Probability Measure,"Joachim Giesen, Soeren Laue, Lars Kuehne","JMLR W&CP 33 :257-265, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/giesen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_giesen14,http://jmlr.csail.mit.edu/proceedings/papers/v33/giesen14.html,"We want to sketch the support of a probability measure on Euclidean space from samples that have been drawn from the measure. This problem is closely related to certain manifold learning problems, where one assumes that the sample points are drawn from a manifold that is embedded in Euclidean space. Here we propose to sketch the support of the probability measure (that does not need to be a manifold) by some gradient flow complex, or more precisely by its Hasse diagram. The gradient flow is defined with respect to the distance function to the sample points. We prove that a gradient flow complex (that can be computed) is homotopy equivalent to the support of the measure for sufficiently dense samplings, and demonstrate the feasibility of our approach on real world data sets."
138,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Unsupervised Learning of Video Representations using LSTMs,"Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/srivastava15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_srivastava15,http://jmlr.csail.mit.edu/proceedings/papers/v37/srivastava15.html,"We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences _ patches of image pixels and high-level representations (``percepts"") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem _ human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance."
139,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,"Algebraic classifiers: a generic approach to fast cross-validation, online training, and parallel training",Michael Izbicki,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/izbicki13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/izbicki13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_izbicki13,http://jmlr.csail.mit.edu/proceedings/papers/v28/izbicki13.html,"We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning. To use these algorithms on a classification model, we must show that the model has appropriate algebraic structure. It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classifiers and a novel variation of decision stumps called HomStumps. But not all classifiers have an obvious structure, so we introduce the Free HomTrainer. This can be used to give a –generic” algebraic structure to any classifier. We use the Free HomTrainer to give algebraic structure to bagging and boosting. In so doing, we derive novel online and parallel algorithms, and present the first fast cross-validation schemes for these classifiers."
140,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Deep Learning Made Easier by Linear Transformations in Perceptrons,"Tapani Raiko, Harri Valpola, Yann Lecun",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/raiko12/raiko12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_raiko12,http://jmlr.csail.mit.edu/proceedings/papers/v22/raiko12.html,We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero activation and zero slope on average and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases with and without regularization and with networks from two to five hidden layers.
141,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Competitive Closeness Testing,"Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan","19:47-68, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/acharya11a/acharya11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_acharya11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/acharya11a.html,We test whether two sequences are generated by the same distributionor by two different ones.Unlike previous work we make no assumptions on the distributions'support size. Additionally we compare our performance to thatof the best possible test.We describe an efficiently-computable algorithm based on\emph{pattern} maximum likelihood that is near optimal whenever the bestpossible error probability is $\le\exp(-14n^{2/3})$using length-$n$ sequences.
142,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Unconfused Ultraconservative Multiclass Algorithms,"Ugo Louche, Liva Ralaivola","JMLR W&CP 29 :309-324, 2013",http://jmlr.org/proceedings/papers/v29/Louche13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Louche13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Louche13.html,"We tackle the problem of learning linear classifiers from noisy datasets in a multiclass setting. The two-class version of this problem was studied a few years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these contributions, the proposed approaches to fight the noise revolve around a Perceptron learning scheme fed with peculiar examples computed through a weighted average of points from the noisy training set. We propose to build upon these approaches and we introduce a new algorithm called (for Unconfused Multiclass additive Algorithm) which may be seen as a generalization to the multiclass setting of the previous approaches. In order to characterize the noise we use the confusion matrix as a multiclass extension of the classification noise studied in the aforementioned literature. Theoretically well-founded, furthermore displays very good empirical noise robustness, as evidenced by numerical simulations conducted on both synthetic and real data."
143,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Joint Inference of Multiple Label Types in Large Networks,"Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus Macskassy",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chakrabarti14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chakrabarti14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chakrabarti14.html,"We tackle the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers, for users connected by a social network. Standard label propagation fails to consider the properties of the label types and the interactions between them. Our proposed method, called EdgeExplain, explicitly models these, while still enabling scalable inference under a distributed message-passing architecture. On a billion-node subset of the Facebook social network, EdgeExplain significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3."
144,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,A Study of Approximate Inference in Probabilistic Relational Models,Fabian Kaelin and Doina Precup,"13:315-330, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/kaelin10a/kaelin10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_kaelin10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/kaelin10a.html,We tackle the problem of approximate inference in Probabilistic Relational Models (PRMs) and propose the Lazy Aggregation Block Gibbs (LABG) algorithm. The LABG algorithm makes use of the inherent relational structure of the ground Bayesian network corresponding to a PRM. We evaluate our approach on artificial and real data and show that it scales well with the size of the data set.
145,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Prediction with Limited Advice and Multiarmed Bandits with Paid Observations,"Yevgeny Seldin, Peter Bartlett, Koby Crammer, Yasin Abbasi-Yadkori",none,http://jmlr.org/proceedings/papers/v32/seldin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/seldin14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_seldin14,http://jmlr.csail.mit.edu/proceedings/papers/v32/seldin14.html,"We study two problems of online learning under restricted information access. In the first problem, prediction with limited advice , we consider a game of prediction with expert advice, where on each round of the game we query the advice of a subset of \(M\) out of \(N\) experts. We present an algorithm that achieves \(O(\sqrt{(N/M)T\ln N})\) regret on \(T\) rounds of this game. The second problem, the multiarmed bandit with paid observations , is a variant of the adversarial \(N\) -armed bandit game, where on round \(t\) of the game we can observe the reward of any number of arms, but each observation has a cost \(c\) . We present an algorithm that achieves \(O((cN\ln N)^{1/3} T^{2/3} + \sqrt{T \ln N})\) regret on \(T\) rounds of this game in the worst case. Furthermore, we present a number of refinements that treat arm- and time-dependent observation costs and achieve lower regret under benign conditions. We present lower bounds that show that, apart from the logarithmic factors, the worst-case regret bounds cannot be improved."
146,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Compressed Sensing with Very Sparse Gaussian Random Projections,"Ping Li, Cun-Hui Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15c,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15c.html,"We study the use of very sparse random projections for compressed sensing (sparse signal recovery) when the nonzero coordinates of signals can be either positive or negative. In our setting, the entries of a Gaussian design matrix are randomly sparsified so that only a very small fraction of entries are nonzero. Our proposed decoding algorithm is simple and efficient in that the major cost is one linear scan of the coordinates. Using our proposed –tie estimator” , we are able to recover a \(K\) -sparse signal of length \(N\) using \(1.551 eK \log K/\delta\) measurements (where \(\delta\leq 0.05\) is the confidence) in one scan. The practical performance of our method, however, can be substantially better than this bound. The Gaussian design assumption is not essential although it simplifies the analysis. Prior studies have shown that existing one-scan (or roughly one-scan) recovery algorithms using sparse matrices would require substantially (e.g., one order of magnitude) more measurements than L1 decoding by linear programming, when the nonzero coordinates of signals can be either negative or positive. In this paper, following a well-known experimental setup , we show that, at the same number of measurements, the recovery accuracies of our proposed method are similar to the standard L1 decoding."
147,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Convex Total Least Squares,"Dmitry Malioutov, Nikolai Slavov",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/malioutov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_malioutov14,http://jmlr.csail.mit.edu/proceedings/papers/v32/malioutov14.html,"We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identifi cation and econometrics. The special case when all dependent and independent variables have the same level of uncorrelated Gaussian noise, known as ordinary TLS, can be solved by singular value decomposition (SVD). However, SVD cannot solve many important practical TLS problems with realistic noise structure, such as having varying measurement noise, known structure on the errors, or large outliers requiring robust error-norms. To solve such problems, we develop convex relaxation approaches for a general class of structured TLS (STLS). We show both theoretically and experimentally, that while the plain nuclear norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear norm approach is very effective, and achieves better accuracy on challenging STLS problems than popular non-convex solvers. We describe a fast solution based on augmented Lagrangian formulation, and apply our approach to an important class of biological problems that use population average measurements to infer cell-type and physiological-state specific expression levels that are very hard to measure directly."
148,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning the Consistent Behavior of Common Users for Target Node Prediction across Social Networks,"Shan-Hung Wu, Hao-Heng Chien, Kuan-Hua Lin, Philip Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wu14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wu14.html,"We study the target node prediction problem: given two social networks, identify those nodes/users from one network (called the source network) who are likely to join another (called the target network, with nodes called target nodes). Although this problem can be solved using existing techniques in the field of cross domain classification, we observe that in many real-world situations the cross-domain classifiers perform sub-optimally due to the heterogeneity between source and target networks that prevents the knowledge from being transferred. In this paper, we propose learning the consistent behavior of common users to help the knowledge transfer. We first present the Consistent Incidence Co-Factorization (CICF) for identifying the consistent users, i.e., common users that behave consistently across networks. Then we introduce the Domain-UnBiased (DUB) classifiers that transfer knowledge only through those consistent users. Extensive experiments are conducted and the results show that our proposal copes with heterogeneity and improves prediction accuracy."
149,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Bounded regret in stochastic multi-armed bandits,"S_bastien Bubeck, Vianney Perchet, Philippe Rigollet",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bubeck13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Bubeck13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bubeck13.html,"We study the stochastic multi-armed bandit problem when one knows the value \(\mu^{(\star)}\) of an optimal arm, as a well as a positive lower bound on the smallest positive gap \(\Delta\) . We propose a new randomized policy that attains a regret uniformly bounded over time in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows \(\Delta\) , and bounded regret of order \(1/\Delta\) is not possible if one only knows \(\mu^{(\star)}\) ."
150,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Statistical analysis of stochastic gradient methods for generalized linear models,"Panagiotis Toulis, Edoardo Airoldi, Jason Rennie",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/toulis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_toulis14,http://jmlr.csail.mit.edu/proceedings/papers/v32/toulis14.html,"We study the statistical properties of stochastic gradient descent (SGD) using explicit and implicit updates for fitting generalized linear models (GLMs). Initially, we develop a computationally efficient algorithm to implement implicit SGD learning of GLMs. Next, we obtain exact formulas for the bias and variance of both updates which leads to two important observations on their comparative statistical properties. First, in small samples, the estimates from the implicit procedure are more biased than the estimates from the explicit one, but their empirical variance is smaller and they are more robust to learning rate misspecification. Second, the two procedures are statistically identical in the limit: they are both unbiased, converge at the same rate and have the same asymptotic variance. Our set of experiments confirm our theory and more broadly suggest that the implicit procedure can be a competitive choice for fitting large-scale models, especially when robustness is a concern."
151,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Half Transductive Ranking,"Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Corinna Cortes, Mehryar Mohri","9:49-56, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/bai10a/bai10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_bai10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/bai10a.html,We study the standard retrieval task of ranking a fixed set of items given a previously unseen query and pose it as the half transductive ranking problem. The task is transductive as the set of items is fixed. Transductive representations (where the vector representation of each example is learned) allow the generation of highly nonlinear embeddings that capture object relationships without relying on a specific choice of features and require only relatively simple optimization. Unfortunately they have no direct out-of-sample extension. Inductive approaches on the other hand allow for the representation of unknown queries. We describe algorithms for this setting which have the advantages of both transductive and inductive approaches and can be applied in unsupervised (either reconstruction-based or graph-based) and supervised ranking setups. We show empirically that our methods give strong performance on all three tasks.
152,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Stability of Multi-Task Kernel Regression Algorithms,"Julien Audiffren, Hachem Kadri","JMLR W&CP 29 :1-16, 2013",http://jmlr.org/proceedings/papers/v29/Audiffren13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Audiffren13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Audiffren13.html,"We study the stability properties of nonlinear multi-task regression in reproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a. multi-task kernels, are appropriate for learning problems with nonscalar outputs like multi-task learning and structured output prediction. We show that multi-task kernel regression algorithms are uniformly stable in the general case of infinite-dimensional output spaces. We then derive under mild assumption on the kernel generalization bounds of such algorithms, and we show their consistency even with non Hilbert-Schmidt operator-valued kernels. We demonstrate how to apply the results to various multi-task kernel regression methods such as vector-valued SVR and functional ridge regression."
153,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Sample-Complexity of General Reinforcement Learning,"Tor Lattimore, Marcus Hutter, Peter Sunehag",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lattimore13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lattimore13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lattimore13.html,"We study the sample-complexity of reinforcement learning in a general setting without assuming ergodicity or finiteness of the environment. Instead, we define a topology on the space of environments and show that if an environment class is compact with respect to this topology then finite sample-complexity bounds are possible and give an algorithm achieving these bounds. We also show the existence of environment classes that are non-compact where finite sample-complexity bounds are not achievable. A lower bound is presented that matches the upper bound except for logarithmic factors."
154,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On the convergence of no-regret learning in selfish routing,"Walid Krichene, Benjamin Drighès, Alexandre Bayen",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/krichene14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/krichene14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_krichene14,http://jmlr.csail.mit.edu/proceedings/papers/v32/krichene14.html,"We study the repeated, non-atomic routing game, in which selfish players make a sequence of routing decisions. We consider a model in which players use regret-minimizing algorithms as the learning mechanism, and study the resulting dynamics. We are concerned in particular with the convergence to the set of Nash equilibria of the routing game. No-regret learning algorithms are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence. We show that convergence holds for a large class of online learning algorithms, inspired from the continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to belong to this class, which guarantees its convergence."
155,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,SVM versus Least Squares SVM,"Jieping Ye, Tao Xiong","2:644-651, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/ye07a/ye07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_ye07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/ye07a.html,We study the relationship between Support Vector Machines (SVM) and Least Squares SVM (LS-SVM). Our main result shows that under mild conditions LS-SVM for binaryclass classifications is equivalent to the hard margin SVM based on the well-known Mahalanobis distance measure. We further study the asymptotics of the hard margin SVM when the data dimensionality tends to infinity with a fixed sample size. Using recently developed theory on the asymptotics of the distribution of the eigenvalues of the covariance matrix we show that under mild conditions the equivalence result holds for the traditional Euclidean distance measure. These equivalence results are further extended to the multi-class case. Experimental results confirm the presented theoretical analysis.
156,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Competitive Classification and Closeness Testing,"Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan and Ananda Suresh",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/acharya12/acharya12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_acharya12,http://jmlr.csail.mit.edu/proceedings/papers/v23/acharya12.html,"We study the problems of classification and closeness testing. A classifier associates a test sequence with the one of two training sequences that was generated by the same distribution. A closeness test determines whether two sequences were generated by the same or by different distributions. For both problems all natural algorithms are symmetric -- they make the same decision under all symbol relabelings. With no assumptions on the distributions' support size or relative distance, we construct a classifier and closeness test that require at most ê(n 3/2 ) samples to attain the n-sample accuracy of the best symmetric classifier or closeness test designed with knowledge of the underlying distributions. Both algorithms run in time linear in the number of samples. Conversely we also show that for any classifier or closeness test, there are distributions that require _(n 7/6 ) samples to achieve the n -sample accuracy of the best symmetric algorithm that knows the underlying distributions."
157,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Variational Approximation for Topic Modeling of Hierarchical Corpora,"Do-kyum Kim, Geoffrey Voelker, Lawrence Saul",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kim13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13.html,"We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For these models we show that there exists a simple variational approximation for probabilistic inference. The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the modelês hierarchy. We compare our approach to existing implementations of nonparametric HDPs. On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods. Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security_one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy."
158,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Generalized Kernel Approach to Structured Output Learning,"Hachem Kadri, Mohammad Ghavamzadeh, Philippe Preux",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kadri13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kadri13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kadri13.html,"We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on three structured output problems, and compare it to the state-of-the art kernel-based structured output regression methods."
159,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Submodularity in Data Subset Selection and Active Learning,"Kai Wei, Rishabh Iyer, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wei15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wei15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wei15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wei15.html,"We study the problem of selecting a subset of big data to train a classifier while incurring minimal performance loss. We show the connection of submodularity to the data likelihood functions for Naive Bayes (NB) and Nearest Neighbor (NN) classifiers, and formulate the data subset selection problems for these classifiers as constrained submodular maximization. Furthermore, we apply this framework to active learning and propose a novel scheme filtering active submodular selection (FASS), where we combine the uncertainty sampling method with a submodular data subset selection framework. We extensively evaluate the proposed framework on text categorization and handwritten digit recognition tasks with four different classifiers, including Deep Neural Network (DNN) based classifiers. Empirical results indicate that the proposed framework yields significant improvement over the state-of-the-art algorithms on all classifiers."
160,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Optimal PAC Multiple Arm Identification with Applications to Crowdsourcing,"Yuan Zhou, Xi Chen, Jian Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhoub14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhoub14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhoub14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhoub14.html,"We study the problem of selecting \(K\) arms with the highest expected rewards in a stochastic \(N\) -armed bandit game. Instead of using existing evaluation metrics (e.g., misidentification probability or the metric in EXPLORE-K), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which, with probability at least \(1-\delta\) , identifies a set of \(K\) arms with regret at most \(\epsilon\) . We provide the sample complexity bound of our algorithm. To complement, we establish the lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm."
161,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Local Ordinal Embedding,"Yoshikazu Terada, Ulrike von Luxburg",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/terada14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/terada14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_terada14,http://jmlr.csail.mit.edu/proceedings/papers/v32/terada14.html,"We study the problem of ordinal embedding: given a set of ordinal constraints of the form \(distance(i,j) _ distance(k,l)\) for some q uadruples \((i,j,k,l)\) of indices, the goal is to construct a point configuration \(\hat{\bm{x}}_1, ..., \hat{\bm{x}}_n\) in \(\R^p\) that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embedding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know –local ordinal information” in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing."
162,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A Potential-based Framework for Online Multi-class Learning with Partial Feedback,"Shijun Wang, Rong Jin, Hamed Valizadegan","9:900-907, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/wang10a/wang10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_wang10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/wang10a.html,We study the problem of online multi-class learning with partial feedback: in each trial of online learning instead of providing the true class label for a given instance the oracle will only reveal to the learner if the predicted class label is correct. We present a general framework for online multi-class learning with partial feedback that adapts the potential-based gradient descent approaches. The generality of the proposed framework is verified by the fact that Banditron is indeed a special case of our work if the potential function is set to be the squared $L_2$ norm of the weight vector. We propose an exponential gradient algorithm for online multi-class learning with partial feedback. Compared to the Banditron algorithm the exponential gradient algorithm is advantageous in that its mistake bound is independent from the dimension of data making it suitable for classifying high dimensional data. Our empirical study with four data sets show that the proposed algorithm for online learning with partial feedback is more effective than the Banditron algorithm.
163,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Competing With Strategies,"Wei Han, Alexander Rakhlin, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Han13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Han13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Han13.html,"We study the problem of online learning with a notion of regret defined with respect to a set of strategies. We develop tools for analyzing the minimax rates and for deriving regret-minimization algorithms in this scenario. While the standard methods for minimizing the usual notion of regret fail, through our analysis we demonstrate existence of regret-minimization methods that compete with such sets of strategies as: autoregressive algorithms, strategies based on statistical models, regularized least squares, and follow the regularized leader strategies. In several cases we also derive efficient learning algorithms."
164,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,\(\propto\)SVM for Learning with Label Proportions,"Felix Yu, Dong Liu, Sanjiv Kumar, Jebara Tony, Shih-Fu Chang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yu13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/yu13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yu13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/yu13a.html,"We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or \(\propto\) SVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The \(\propto\) SVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that \(\propto\) SVM outperforms the state-of-the-art, especially for larger group sizes."
165,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,On Learning Discrete Graphical Models using Group-Sparse Regularization,"Ali Jalali, Pradeep Ravikumar, Vishvas Vasuki, Sujay Sanghavi","15:378-387, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/jalali11a/jalali11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_jalali11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/jalali11a.html,We study the problem of learning the graph structure associated with a general discrete graphical models (each variable can take any of m _ 1 values the clique factors have maximum size c _= 2) from samples under high-dimensional scaling where the number of variables p could be larger than the number of samples n. We provide a quantitative consistency analysis of a procedure based on node-wise multi-class logistic regression with group-sparse regularization. We first consider general m-ary pairwise models -- where each factor depends on at most two variables. We show that when the number of samples scale as n _ K(m-1)^2 d^2 log ((m-1)^2(p-1)) -- where d is the maximum degree and K a fixed constant -- the procedure succeeds in recovering the graph with high probability. For general models with c-way factors the natural multi-way extension of the pairwise method quickly becomes very computationally complex. So we studied the effectiveness of using the pairwise method even while the true model has higher order factors. Surprisingly we show that under slightly more stringent conditions the pairwise procedure still recovers the graph structure when the samples scale as n _ K (m-1)^2 d^{3/2c - 1} \log ( (m-1)^c (p-1)^{c-1} ).
166,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Learning Rules from Incomplete Examples via Implicit Mention Models,"J.R. Doppa, M.S. Sorower, M. Nasresfahani, J. Irvine, W. Orr, T.G. Dietterich, X. Fern & P. Tadepalli","20:197_212, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/doppa11/doppa11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_doppa11,http://jmlr.csail.mit.edu/proceedings/papers/v20/doppa11.html,We study the problem of learning general rules from concrete facts extracted from natural data sources such as the newspaper stories and medical histories. Natural data sources present two challenges to automated learning namely radical incompleteness and systematic bias . In this paper we propose an approach that combines simultaneous learning of multiple predictive rules with di_erential scoring of evidence which adapts to a presumed model of data generation. Learning multiple predicates simultaneously mitigates the problem of radical incompleteness while the di_erential scoring would help reduce the e_ects of systematic bias. We evaluate our approach empirically on both textual and non-textual sources. We further present a theoretical analysis that elucidates our approach and explains the empirical results.   Page last modified on Sun Nov 6 15:43:23 2011.
167,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,The Multi-Task Learning View of Multimodal Data,"Hachem Kadri, Stephane Ayache, C_cile Capponi, Sokol Koço, François-Xavier Dup_, Emilie Morvant","JMLR W&CP 29 :261-276, 2013",http://jmlr.org/proceedings/papers/v29/Kadri13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Kadri13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Kadri13.html,"We study the problem of learning from multiple views using kernel methods in a supervised setting. We approach this problem from a multi-task learning point of view and illustrate how to capture the interesting multimodal structure of the data using multi-task kernels. Our analysis shows that the multi-task perspective offers the flexibility to design more efficient multiple-source learning algorithms, and hence the ability to exploit multiple descriptions of the data. In particular, we formulate the multimodal learning framework using vector-valued reproducing kernel Hilbert spaces, and we derive specific multi-task kernels that can operate over multiple modalities. Finally, we analyze the vector-valued regularized least squares algorithm in this context, and demonstrate its potential in a series of experiments with a real-world multimodal data set."
168,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Finding a most biased coin with fewest flips,"Karthekeyan Chandrasekaran, Richard Karp","JMLR W&CP 35 :394-407, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/chandrasekaran14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_chandrasekaran14,http://jmlr.csail.mit.edu/proceedings/papers/v35/chandrasekaran14.html,"We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin whose posterior probability of being most biased is at least \(1-\delta\) for a given \(\delta\) . Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategyãa strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any given starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs mathematical tools from the area of Markov games."
169,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Kernel Learning by Unconstrained Optimization,"Fuxin Li, Yunshan Fu, Yu-Hong Dai, Cristian Sminchisescu, wang jue","5:328-335, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/li09a/li09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_li09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/li09a.html,We study the problem of learning a kernel matrix from an apriori kernel and training data. In this context we propose a new unconstrained convex optimization formulation with an arbitrary convex second-order differentiable loss function on kernel entries and a LogDet divergence term for regularization. Since the number of variables is of order $O(n^2)$ the computational cost of standard Newton and quasi-Newton methods for learning a kernel matrix is prohibitive. Here an operator form Hessian is used to develop an $O(n^3)$ trust-region inexact Newton method where the Newton direction is computed using several conjugate gradient steps on the Hessian operator equation. On the uspst dataset our algorithm can handle 2 million optimization variables within one hour. Experiments are shown for both linear (Mahalanobis) metric learning and for kernel learning. The convergence rate speed and performance of several loss functions and algorithms are discussed.
170,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Learning Mixtures of Discrete Product Distributions using Spectral Decompositions,"Prateek Jain, Sewoong Oh","JMLR W&CP 35 :824-856, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/jain14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_jain14,http://jmlr.csail.mit.edu/proceedings/papers/v35/jain14.html,"We study the problem of learning a distribution from samples, when the underlying distribution is a mixture of product distributions over discrete domains. This problem is motivated by several practical applications such as crowdsourcing, recommendation systems, and learning Boolean functions. The existing solutions either heavily rely on the fact that the number of mixtures is finite or have sample/time complexity that is exponential in the number of mixtures. In this paper, we introduce a polynomial time/sample complexity method for learning a mixture of \(r\) discrete product distributions over \(\{1, 2, \dots, \ell\}^n\) , for general \(\ell\) and \(r\) . We show that our approach is consistent and further provide finite sample guarantees. We use recently developed techniques from tensor decompositions for moment matching. A crucial step in these approaches is to construct certain tensors with low-rank spectral decompositions. These tensors are typically estimated from the sample moments. The main challenge in learning mixtures of discrete product distributions is that the corresponding low-rank tensors cannot be obtained directly from the sample moments. Instead, we need to estimate a low-rank matrix using only off-diagonal entries, and estimate a tensor using a few linear measurements. We give an alternating minimization based method to estimate the low-rank matrix, and formulate the tensor estimation problem as a least-squares problem."
171,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Locally Linear Denoising on Image Manifolds,"Dian Gong, Fei Sha, G_rard Medioni","9:265-272, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/gong10a/gong10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_gong10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/gong10a.html,We study the problem of image denoising where images are assumed to be samples from low dimensional (sub)manifolds. We propose the algorithm of locally linear denoising. The algorithm approximates manifolds with locally linear patches by constructing nearest neighbor graphs. Each image is then locally denoised within its neighborhoods. A global optimal denoising result is then identified by aligning those local estimates. The algorithm has a closed-form solution that is efficient to compute. We evaluated and compared the algorithm to alternative methods on two image data sets. We demonstrated the effectiveness of the proposed algorithm which yields visually appealing denoising results incurs smaller reconstruction errors and results in lower error rates when the denoised data are used in supervised learning tasks.
172,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multiple Identifications in Multi-Armed Bandits,"S_ebastian Bubeck, Tengyao Wang, Nitin Viswanathan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bubeck13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bubeck13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bubeck13.html,"We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem."
173,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bat Call Identification with Gaussian Process Multinomial Probit Regression and a Dynamic Time Warping Kernel,"Vassilios Stathopoulos, Veronica Zamora-Gutierrez, Kate Jones, Mark Girolami","JMLR W&CP 33 :913-921, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/stathopoulos14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_stathopoulos14,http://jmlr.csail.mit.edu/proceedings/papers/v33/stathopoulos14.html,We study the problem of identifying bat species from echolocation calls in order to build automated bioacoustic monitoring algorithms. We employ the Dynamic Time Warping algorithm which has been successfully applied for bird flight calls identification and show that classification performance is superior to hand crafted call shape parameters used in previous research. This highlights that generic bioacoustic software with good classification rates can be constructed with little domain knowledge. We conduct a study with field data of 21 bat species from the north and central Mexico using a multinomial probit regression model with Gaussian process prior and a full EP approximation of the posterior of latent function values. Results indicate high classification accuracy across almost all classes while misclassification rate across families of species is low highlighting the common evolutionary path of echolocation in bats.
174,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stochastic Simultaneous Optimistic Optimization,"Michal Valko, Alexandra Carpentier, R_mi Munos",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/valko13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_valko13,http://jmlr.csail.mit.edu/proceedings/papers/v28/valko13.html,"We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known."
175,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Most Correlated Arms Identification,"Che-Yu Liu, S _ bastien Bubeck","JMLR W&CP 35 :623-637, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/liu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_liu14,http://jmlr.csail.mit.edu/proceedings/papers/v35/liu14.html,We study the problem of finding the most mutually correlated arms among many arms. We show that adaptive arms sampling strategies can have significant advantages over the non-adaptive uniform sampling strategy. Our proposed algorithms rely on a novel correlation estimator. The use of this accurate estimator allows us to get improved results for a wide range of problem instances.
176,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Almost Optimal Exploration in Multi-Armed Bandits,"Zohar Karnin, Tomer Koren, Oren Somekh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/karnin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_karnin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/karnin13.html,"We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases."
177,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Fast Distribution To Real Regression,"Junier Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, Eric Xing","JMLR W&CP 33 :706-714, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14a-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_oliva14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14a.html,"We study the problem of distribution to real regression, where one aims to regress a mapping \(f\) that takes in a distribution input covariate \(P\in \mathcal{I}\) (for a non-parametric family of distributions \(\mathcal{I}\) ) and outputs a real-valued response \(Y=f(P) + \epsilon\) . This setting was recently studied in P„zcos et al. (2013), where the –Kernel-Kernel” estimator was introduced and shown to have a polynomial rate of convergence. However, evaluating a new prediction with the Kernel-Kernel estimator scales as \(\Omega(N)\) . This causes the difficult situation where a large amount of data may be necessary for a low estimation risk, but the computation cost of estimation becomes infeasible when the data-set is too large. To this end, we propose the Double-Basis estimator, which looks to alleviate this big data problem in two ways: first, the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances \(N\) when evaluating new predictions after training; secondly, the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings \(f\in\mathcal{F}\) ."
178,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Structured Generative Models of Natural Source Code,"Chris Maddison, Daniel Tarlow",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/maddison14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/maddison14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_maddison14,http://jmlr.csail.mit.edu/proceedings/papers/v32/maddison14.html,"We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have two key properties: First, they incorporate both sequential and hierarchical structure. Second, they are capable of integrating closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the probability of generating test programs."
179,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Learning Coverage Functions and Private Release of Marginals,"Vitaly Feldman, Pravesh Kothari","JMLR W&CP 35 :679-702, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_feldman14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14a.html,"We study the problem of approximating and learning coverage functions. A function \(c: 2^{[n]} \rightarrow \mathbf{R}^{+}\) is a coverage function, if there exists a universe \(U\) with non-negative weights \(w(u)\) for each \(u \in U\) and subsets \(A_1, A_2, \ldots, A_n\) of \(U\) such that \(c(S) = \sum_{u \in \cup_{i \in S} A_i} w(u)\) . Alternatively, coverage functions can be described as non-negative linear combinations of monotone disjunctions. They are a natural subclass of submodular functions and arise in a number of applications. We give an algorithm that for any \(\gamma,\delta_0\) , given random and uniform examples of an unknown coverage function \(c\) , finds a function \(h\) that approximates \(c\) within factor \(1+\gamma\) on all but \(\delta\) -fraction of the points in time \(poly(n,1/\gamma,1/\delta)\) . This is the first fully-polynomial algorithm for learning an interesting class of functions in the demanding PMAC model of Balcan and Harvey (2011). Our algorithms are based on several new structural properties of coverage functions. Using the results in (Feldman and Kothari, 2014), we also show that coverage functions are learnable agnostically with excess \(\ell_1\) -error \(\epsilon\) over all product and symmetric distributions in time \(n^{\log(1/\epsilon)}\) . In contrast, we show that, without assumptions on the distribution, learning coverage functions is at least as hard as learning polynomial-size disjoint DNF formulas, a class of functions for which the best known algorithm runs in time \(2^{\tilde{O}(n^{1/3})}\) (Klivans and Servedio, 2004). As an application of our learning results, we give simple differentially-private algorithms for releasing monotone conjunction counting queries with low average error. In particular, for any \(k \leq n\) , we obtain private release of \(k\) -way marginals with average error \(\bar{\alpha}\) in time \(n^{O(\log(1/\bar{\alpha}))}\) ."
180,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Approximate Inference in Collective Graphical Models,"Daniel Sheldon, Tao Sun, Akshat Kumar, Tom Dietterich",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sheldon13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sheldon13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sheldon13.html,"We study the problem of approximate inference in collective graphical models (CGMs), which were recently introduced to model the problem of learning and inference with noisy aggregate observations. We first analyze the complexity of inference in CGMs: unlike inference in conventional graphical models, exact inference in CGMs is NP-hard even for tree-structured models. We then develop a tractable convex approximation to the NP-hard MAP inference problem in CGMs, and show how to use MAP inference for approximate marginal inference within the EM framework. We demonstrate empirically that these approximation techniques can reduce the computational cost of inference by two orders of magnitude and the cost of learning by at least an order of magnitude while providing solutions of equal or better quality."
181,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Optimal Allocation Strategies for the Dark Pool Problem,"Alekh Agarwal, Peter Bartlett, Max Dama","9:9-16, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/agarwal10a/agarwal10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_agarwal10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/agarwal10a.html,We study the problem of allocating stocks to dark pools. We propose and analyze an optimal approach for allocations if continuous-valued allocations are allowed. We also propose a modification for the case when only integer-valued allocations are possible. We extend the previous work on this problem by Ganchev et al (UAI 2009) to adversarial scenarios while also improving over their results in the iid setup. The resulting algorithms are efficient and are tested on extensive simulations under stochastic and adversarial inputs. Our work also has consequences for other perishable inventory control problems extending their analyses to adversarial models too.
182,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data,"Naiyan Wang, Dit-Yan Yeung",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangg14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangg14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangg14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangg14.html,"We study the problem of aggregating the contributions of multiple contributors in a crowdsourcing setting. The data involved is in a form not typically considered in most crowdsourcing tasks, in that the data is structured and has a temporal dimension. In particular, we study the visual tracking problem in which the unknown data to be estimated is in the form of a sequence of bounding boxes representing the trajectory of the target object being tracked. We propose a factorial hidden Markov model (FHMM) for ensemble-based tracking by learning jointly the unknown trajectory of the target and the reliability of each tracker in the ensemble. For efficient online inference of the FHMM, we devise a conditional particle filter algorithm by exploiting the structure of the joint posterior distribution of the hidden variables. Using the largest open benchmark for visual tracking, we empirically compare two ensemble methods constructed from five state-of-the-art trackers with the individual trackers. The promising experimental results provide empirical evidence for our ensemble approach to –get the best of all worlds”."
183,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Negative Results for Active Learning with Convex Losses,"Steve Hanneke, Liu Yang","9:321-325, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/hanneke10a/hanneke10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_hanneke10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/hanneke10a.html,We study the problem of active learning with convex loss functions. We prove that even under bounded noise constraints the minimax rates for proper active learning are often no better than passive learning.
184,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Online Optimization with Gradual Variations,"Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin and Shenghuo Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/chiang12/chiang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_chiang12,http://jmlr.csail.mit.edu/proceedings/papers/v23/chiang12.html,"We study the online convex optimization problem, in which an online algorithm has to make repeated decisions with convex loss functions and hopes to achieve a small regret. We consider a natural restriction of this problem in which the loss functions have a small deviation, measured by the sum of the distances between every two consecutive loss functions, according to some distance metrics. We show that for the linear and general smooth convex loss functions, an online algorithm modified from the gradient descend algorithm can achieve a regret which only scales as the square root of the deviation. For the closely related problem of prediction with expert advice, we show that an online algorithm modified from the multiplicative update algorithm can also achieve a similar regret bound for a different measure of deviation. Finally, for loss functions which are strictly convex, we show that an online algorithm modified from the online Newton step algorithm can achieve a regret which is only logarithmic in terms of the deviation, and as an application, we can also have such a logarithmic regret for the portfolio management problem."
185,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Min-Max Problems on Factor Graphs,"Siamak Ravanbakhsh, Christopher Srinivasa, Brendan Frey, Russell Greiner",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ravanbakhsh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ravanbakhsh14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ravanbakhsh14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ravanbakhsh14.html,"We study the min-max problem in factor graphs, which seeks the assignment that minimizes the maximum value over all factors. We reduce this problem to both min-sum and sum-product inference, and focus on the later. This approach reduces the min-max inference problem to a sequence of constraint satisfaction problems (CSPs) which allows us to sample from a uniform distribution over the set of solutions. We demonstrate how this scheme provides a message passing solution to several NP-hard combinatorial problems, such as min-max clustering (a.k.a. K-clustering), the asymmetric K-center problem, K-packing and the bottleneck traveling salesman problem. Furthermore we theoretically relate the min-max reductions to several NP hard decision problems, such as clique cover, set cover, maximum clique and Hamiltonian cycle, therefore also providing message passing solutions for these problems. Experimental results suggest that message passing often provides near optimal min-max solutions for moderate size instances."
186,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Efficient Learning of Linear Separators under Bounded Noise,"Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Ruth Urner",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Awasthi15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Awasthi15b,http://jmlr.csail.mit.edu/proceedings/papers/v40/Awasthi15b.html,"We study the learnability of linear separators in \(\Re^d\) in the presence of bounded (a.k.a Massart) noise. This is a realistic generalization of the random classification noise model, where the adversary can flip each example \(x\) with probability \(\eta(x) \leq \eta\) . We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit sphere in \(\Re^d\) , for some constant value of \(\eta\) . While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design algorithms achieving arbitrarily small excess error in polynomial time under this realistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work, instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only logarithmic in the desired excess error \(\epsilon\) ."
187,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits,"Pratik Gajane, Tanguy Urvoy, Fabrice Cl_rot",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gajane15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/gajane15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gajane15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gajane15.html,"We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications."
188,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Weighted Graph Clustering with Non-Uniform Uncertainties,"Yudong Chen, Shiau Hong Lim, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenh14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chenh14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenh14.html,"We study the graph clustering problem where each observation (edge or no-edge between a pair of nodes) may have a different level of confidence/uncertainty. We propose a clustering algorithm that is based on optimizing an appropriate weighted objective, where larger weights are given to observations with lower uncertainty. Our approach leads to a convex optimization problem that is efficiently solvable. We analyze our approach under a natural generative model, and establish theoretical guarantees for recovering the underlying clusters. Our main result is a general theorem that applies to any given weight and distribution for the uncertainty. By optimizing over the weights, we derive a provably optimal weighting scheme, which matches the information theoretic lower bound up to logarithmic factors and leads to strong performance bounds in several specific settings. By optimizing over the uncertainty distribution, we show that non-uniform uncertainties can actually help. In particular, if the graph is built by spending a limited amount of resource to take measurement on each node pair, then it is beneficial to allocate the resource in a non-uniform fashion to obtain accurate measurements on a few pairs of nodes, rather than obtaining inaccurate measurements on many pairs. We provide simulation results that validate our theoretical findings."
189,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds,"Yuchen Zhang, Martin Wainwright, Michael Jordan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhangc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangc15.html,"We study the following generalized matrix rank estimation problem: given an n-by-n matrix and a constant \(c _ 0\) , estimate the number of eigenvalues that are greater than \(c\) . In the distributed setting, the matrix of interest is the sum of \(m\) matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate \(\Omega(n^2)\) bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only \(O(n)\) bits. The upper bound is matched by an \(\Omega(n)\) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments."
190,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Polynomials with Neural Networks,"Alexandr Andoni, Rina Panigrahy, Gregory Valiant, Li Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/andoni14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_andoni14,http://jmlr.csail.mit.edu/proceedings/papers/v32/andoni14.html,"We study the effectiveness of learning low degree polynomials using neural networks by the gradient descent method. While neural networks have been shown to have great expressive power, and gradient descent has been widely used in practice for learning neural networks, few theoretical guarantees are known for such methods. In particular, it is well known that gradient descent can get stuck at local minima, even for simple classes of target functions. In this paper, we present several positive theoretical results to support the effectiveness of neural networks. We focus on two-layer neural networks (i.e. one hidden layer) where the top layer node is a linear function, similar to . First we show that for a randomly initialized neural network with sufficiently many hidden units, the gradient descent method can learn any low degree polynomial. Secondly, we show that if we use complex-valued weights (the target function can still be real), then under suitable conditions, there are no –robust local minima”: the neural network can always escape a local minimum by performing a random perturbation. This property does not hold for real-valued weights. Thirdly, we discuss whether sparse polynomials can be learned with small neural networks, where the size is dependent on the sparsity of the target function."
191,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Local algorithms for interactive clustering,"Pranjal Awasthi, Maria Balcan, Konstantin Voevodski",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/awasthi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_awasthi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/awasthi14.html,We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.
192,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Pseudo-reward Algorithms for Contextual Bandits with Linear Payoff Functions,"Ku-Chun Chou, Hsuan-Tien Lin, Chao-Kai Chiang, Chi-Jen Lu",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/chou14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_chou14,http://jmlr.csail.mit.edu/proceedings/papers/v39/chou14.html,"We study the contextual bandit problem with linear payoff functions, which is a generalization of the traditional multi-armed bandit problem. In the contextual bandit problem, the learner needs to iteratively select an action based on an observed context, and receives a linear score on only the selected action as the reward feedback. Motivated by the observation that better performance is achievable if the other rewards on the non-selected actions can also be revealed to the learner, we propose a new framework that feeds the learner with pseudo-rewards, which are estimates of the rewards on the non-selected actions. We argue that the pseudo-rewards should better contain over-estimates of the true rewards, and propose a forgetting mechanism to decrease the negative influence of the over-estimation in the long run. Then, we couple the two key ideas above with the linear upper confidence bound (LinUCB) algorithm to design a novel algorithm called linear pseudo-reward upper confidence bound (LinPRUCB). We prove that LinPRUCB shares the same order of regret bound to LinUCB, while enjoying the practical observation of faster reward-gathering in the earlier iterations. Experiments on artificial and real-world data sets justify that LinPRUCB is competitive to and sometimes even better than LinUCB. Furthermore, we couple LinPRUCB with a special parameter to formalize a new algorithm that yields faster computation in updating the internal models while keeping the promising practical performance. The two properties match the real-world needs of the contextual bandit problem and make the new algorithm a favorable choice in practice."
193,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,The Loss Surfaces of Multilayer Networks,"Anna Choromanska, MIkael Henaff, Michael Mathieu, Gerard Ben Arous, Yann LeCun",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/choromanska15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_choromanska15,http://jmlr.csail.mit.edu/proceedings/papers/v38/choromanska15.html,"We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting."
194,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Computational Bounds on Statistical Query Learning,Vitaly Feldman and Varun Kanade,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/feldman12a/feldman12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_feldman12a,http://jmlr.csail.mit.edu/proceedings/papers/v23/feldman12a.html,"We study the complexity of learning in Kearns' well-known statistical query (SQ) learning model (Kearns, 1993). A number of previous works have addressed the definition and estimation of the information-theoretic bounds on the SQ learning complexity, in other words, bounds on the query complexity. Here we give the first strictly computational upper and lower bounds on the complexity of several types of learning in the SQ model. As it was already observed, the known characterization of distribution-specific SQ learning (Blum, et al. 1994) implies that for weak learning over a fixed distribution, the query complexity and computational complexity are essentially the same. In contrast, we show that for both distribution-specific and distribution-independent (strong) learning there exists a concept class of polynomial query complexity that is not efficiently learnable unless RP = NP. We then prove that our distribution-specific lower bound is essentially tight by showing that for every concept class C of polynomial query complexity there exists a polynomial time algorithm that given access to random points from any distribution D and an NP oracle, can SQ learn C over D . We also consider a restriction of the SQ model, the correlational statistical query (CSQ) model (Bshouty and Feldman, 2001; Feldman, 2008) of learning which is closely-related to Valiant's model of evolvability (Valiant, 2007). We show a similar separation result for distribution-independent CSQ learning under a stronger assumption: there exists a concept class of polynomial CSQ query complexity which is not efficiently learnable unless every problem in W[P] has a randomized fixed parameter tractable algorithm."
195,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,"Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees","Vitaly Feldman, Pravesh Kothari, Jan Vondrˆk",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Feldman13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Feldman13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Feldman13.html,"We study the complexity of approximate representation and learning of submodular functions over the uniform distribution on the Boolean hypercube \(\{0,1\}^n\) . Our main result is the following structural theorem: any submodular function is \(\epsilon\) -close in \(\ell_2\) to a real-valued decision tree (DT) of depth \(O(1/\epsilon^2)\) . This immediately implies that any submodular function is \(\epsilon\) -close to a function of at most \(2^{O(1/\epsilon^2)}\) variables and has a spectral \(\ell_1\) norm of \(2^{O(1/\epsilon^2)}\) . It also implies the closest previous result that states that submodular functions can be approximated by polynomials of degree \(O(1/\epsilon^2)\) (Cheraghchi et al., 2012). Our result is proved by constructing an approximation of a submodular function by a DT of rank \(4/\epsilon^2\) and a proof that any rank- \(r\) DT can be \(\epsilon\) -approximated by a DT of depth \(\frac{5}{2}(r+\log(1/\epsilon))\) . We show that these structural results can be exploited to give an attribute-efficient PAC learning algorithm for submodular functions running in time \(\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}\) . The best previous algorithm for the problem requires \(n^{O(1/\epsilon^{2})}\) time and examples (Cheraghchi et al., 2012) but works also in the agnostic setting. In addition, we give improved learning algorithms for a number of related settings. We also prove that our PAC and agnostic learning algorithms are essentially optimal via two lower bounds: (1) an information-theoretic lower bound of \(2^{\Omega(1/\epsilon^{2/3})}\) on the complexity of learning monotone submodular functions in any reasonable model (including learning with value queries); (2) computational lower bound of \(n^{\Omega(1/\epsilon^{2/3})}\) based on a reduction to learning of sparse parities with noise, widely-believed to be intractable. These are the first lower bounds for learning of submodular functions over the uniform distribution."
196,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Achieving All with No Parameters: AdaNormalHedge,"Haipeng Luo, Robert E. Schapire",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Luo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Luo15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Luo15.html,"We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm (Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. (2009) and Chernov and Vovk (2010). Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen (2014) on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses."
197,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Attribute-Efficient Learning and Weight-Degree Tradeoffs for Polynomial Threshold Functions,"Rocco Servedio, Li-Yang Tan and Justin Thaler",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/servedio12/servedio12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_servedio12,http://jmlr.csail.mit.edu/proceedings/papers/v23/servedio12.html,"We study the challenging problem of learning decision lists attribute-efficiently, giving both positive and negative results. Our main positive result is a new tradeoff between the running time and mistake bound for learning length- k decision lists over n Boolean variables. When the allowed running time is relatively high, our new mistake bound improves significantly on the mistake bound of the best previous algorithm of Klivans and Servedio (Klivans and Servedio, 2006). Our main negative result is a new lower bound on the weight of any degree- d polynomial threshold function (PTF) that computes a particular decision list over k variables (the ""ODD-MAX-BIT"" function). The main result of Beigel (Beigel, 1994) is a weight lower bound of 2 _( k / d 2 ) , which was shown to be essentially optimal for d _ k 1/3 by Klivans and Servedio. Here we prove a 2 _(í k/d ) lower bound, which improves on Beigel's lower bound for d _ k 1/3 . This lower bound establishes strong limitations on the effectiveness of the Klivans and Servedio approach and suggests that it may be difficult to improve on our positive result. The main tool used in our lower bound is a new variant of Markov's classical inequality which may be of independent interest; it provides a bound on the derivative of a univariate polynomial in terms of both its degree and the size of its coefficients."
198,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Budgeted Bandit Problems with Continuous Random Costs,"Yingce Xia, Wenkui Ding, Xu-Dong Zhang, Nenghai Yu, Tao Qin",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Xia15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Xia15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Xia15.html,"We study the budgeted bandit problem, where each arm is associated with both a reward and a cost. In a budgeted bandit problem, the objective is to design an arm pulling algorithm in order to maximize the total reward before the budget runs out. In this work, we study both multi-armed bandits and linear bandits, and focus on the setting with continuous random costs. We propose an upper confidence bound based algorithm for multi-armed bandits and a confidence ball based algorithm for linear bandits, and prove logarithmic regret bounds for both algorithms. We conduct simulations on the proposed algorithms, which verify the effectiveness of our proposed algorithms."
199,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Regret Bounds for the Adaptive Control of Linear Quadratic Systems,"Yasin Abbasi-Yadkori, Csaba Szepesvˆri","19:1-26, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/abbasi-yadkori11a/abbasi-yadkori11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_abbasi-yadkori11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/abbasi-yadkori11a.html,We study the average cost Linear Quadratic (LQ) control problem with unknown model parameters also known as the adaptive control problem in the control community. We design an algorithm and prove that apart from logarithmic factors its regret up to time $T$ is $O(\sqrt{T})$.Unlike previous approaches that use a forced-exploration scheme we construct a high-probability confidence set around the model parameters and design an algorithm that plays optimistically with respect to this confidence set. The construction of the confidence set is based on the recent results from online least-squares estimation and leads to improved worst-case regret bound for the proposed algorithm.To the best of our knowledge this is the the first time that a regret bound is derived for the LQ control problem. %(That $O(\sqrt{T})$ is a minimax optimal rate follows from the existing lower bounds for linear bandit problems.)
200,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On the Complexity of Bandit Linear Optimization,Ohad Shamir,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Shamir15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Shamir15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Shamir15.html,"We study the attainable regret for online linear optimization problems with bandit feedback, where unlike the full-information setting, the player can only observe its own loss rather than the full loss vector. We show that the price of bandit information in this setting can be as large as \(d\) , disproving the well-known conjecture (Danie et al. (2007)) that the regret for bandit linear optimization is at most \(\sqrt{d}\) times the full-information regret. Surprisingly, this is shown using –trivial” modifications of standard domains, which have no effect in the full-information setting. This and other results we present highlight some interesting differences between full-information and bandit learning, which were not considered in previous literature."
201,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem,"Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Komiyama15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Komiyama15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Komiyama15.html,"We study the \(K\) -armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. We introduce a tight asymptotic regret lower bound that is based on the information divergence. An algorithm that is inspired by the Deterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010) is proposed, and its regret is analyzed. The proposed algorithm is found to be the first one with a regret upper bound that matches the lower bound. Experimental comparisons of dueling bandit algorithms show that the proposed algorithm significantly outperforms existing ones."
202,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,There's a Hole in My Data Space: Piecewise Predictors for Heterogeneous Learning Problems,"Ofer Dekel, Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/dekel12/dekel12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_dekel12,http://jmlr.csail.mit.edu/proceedings/papers/v22/dekel12.html,We study statistical learning problems where the data space is multimodal and heterogeneous and constructing a single global predictor is difficult. We address such problems by iteratively identifying high-error regions in the data space and learning specialized predictors for these regions. While the idea of composing localized predictors is not new our approach is unique in that we actively seek out predictors that clump errors together making it easier to isolate the problematic regions. When errors are clumped together they are also easier to interpret and resolve through appropriate feature engineering and data preprocessing. We present an error-clumping classification algorithm based on a convex optimization problem and an efficient stochastic optimization algorithm for this problem. We theoretically motivate our approach with a novel sample complexity analysis for piecewise predictors and empirically demonstrate its behavior on an illustrative classification problem.
203,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Minimax Rates of Estimation for Sparse PCA in High Dimensions,"Vincent Vu, Jing Lei",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/vu12/vu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_vu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/vu12.html,We study sparse principal components analysis in the high-dimensional setting where $p$ (the number of variables) can be much larger than $n$ (the number of observations). We prove optimal minimax lower and upper bounds on the estimation error for the first leading eigenvector when it belongs to an $\ell_q$ ball for $q \in [01]$. Our bounds are tight in $p$ and $n$ for all $q \in [0 1]$ over a wide class of distributions. The upper bound is obtained by analyzing the performance of $\ell_q$-constrained PCA. In particular our results provide convergence rates for $\ell_1$-constrained PCA. Our methods and arguments are also extendable to multi-dimensional subspace estimation.
204,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Sparse Principal Component Analysis for High Dimensional Multivariate Time Series,"Zhaoran Wang, Fang Han, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_wang13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13a.html,"We study sparse principal component analysis (sparse PCA) for high dimensional multivariate vector autoregressive (VAR) time series. By treating the transition matrix as a nuisance parameter, we show that sparse PCA can be directly applied on analyzing multivariate time series as if the data are i.i.d. generated. Under a double asymptotic framework in which both the length of the sample period \(T\) and dimensionality \(d\) of the time series can increase (with possibly \(d\gg T\) ), we provide explicit rates of convergence of the angle between the estimated and population leading eigenvectors of the time series covariance matrix. Our results suggest that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. Implications of such a general result is further illustrated using concrete examples. The results of this paper have impacts on different applications, including financial time series, biomedical imaging, and social media, etc."
205,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Distribution-independent Reliable Learning,"Varun Kanade, Justin Thaler","JMLR W&CP 35 :3-24, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kanade14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kanade14,http://jmlr.csail.mit.edu/proceedings/papers/v35/kanade14.html,"We study several questions in the reliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than other types. A positive reliable classifier is one that makes no false positive errors. The goal in the positive reliable agnostic framework is to output a hypothesis with the following properties: (i) its false positive error rate is at most \(\epsilon\) , (ii) its false negative error rate is at most \(\epsilon\) more than that of the best positive reliable classifier from the class. A closely related notion is fully reliable agnostic learning, which considers partial classifiers that are allowed to predict –unknown” on some inputs. The best fully reliable partial classifier is one that makes no errors and minimizes the probability of predicting –unknown”, and the goal in fully reliable learning is to output a hypothesis that is almost as good as the best fully reliable partial classifier from a class. For distribution-independent learning, the best known algorithms for PAC learning typically utilize polynomial threshold representations, while the state of the art agnostic learning algorithms use point-wise polynomial approximations. We show that one-sided polynomial approximations , an intermediate notion between polynomial threshold representations and point-wise polynomial approximations, suffice for learning in the reliable agnostic settings. We then show that majorities can be fully reliably learned and disjunctions of majorities can be positive reliably learned, through constructions of appropriate one-sided polynomial approximations. Our fully reliable algorithm for majorities provides the first evidence that fully reliable learning may be strictly easier than agnostic learning. Our algorithms also satisfy strong attribute-efficiency properties, and in many cases they provide smooth tradeoffs between sample complexity and running time."
206,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Multi-Manifold Semi-Supervised Learning,"Andrew Goldberg, Xiaojin Zhu, Aarti Singh, Zhiting Xu, Robert Nowak","5:169-176, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/goldberg09a/goldberg09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_goldberg09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/goldberg09a.html,We study semi-supervised learning when the data consists of multiple intersecting manifolds. We give a finite sample analysis to quantify the potential gain of using unlabeled data in this multi-manifold setting. We then propose a semi-supervised learning algorithm that separates different manifolds into decision sets and performs supervised learning within each set. Our algorithm involves a novel application of Hellinger distance and size-constrained spectral clustering. Experiments demonstrate the benefit of our multi-manifold semi-supervised learning approach.
207,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Towards an optimal stochastic alternating direction method of multipliers,"Samaneh Azadi, Suvrit Sra",none,http://jmlr.org/proceedings/papers/v32/azadi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/azadi14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_azadi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/azadi14.html,"We study regularized stochastic convex optimization subject to linear equality constraints. This class of problems was recently also studied by Ouyang et al. (2013) and Suzuki (2013); both introduced similar stochastic alternating direction method of multipliers (SADMM) algorithms. However, the analysis of both papers led to suboptimal convergence rates. This paper presents two new SADMM methods: (i) the first attains the minimax optimal rate of O(1/k) for nonsmooth strongly-convex stochastic problems; while (ii) the second progresses towards an optimal rate by exhibiting an O(1/k 2 ) rate for the smooth part. We present several experiments with our new methods; the results indicate improved performance over competing ADMM methods."
208,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Regret Minimization for Branching Experts,"Eyal Gofer, NicolÖ Cesa-Bianchi, Claudio Gentile, Yishay Mansour",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Gofer13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Gofer13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Gofer13.html,"We study regret minimization bounds in which the dependence on the number of experts is replaced by measures of the realized complexity of the expert class. The measures we consider are defined in retrospect given the realized losses. We concentrate on two interesting cases. In the first, our measure of complexity is the number of different –leading experts”, namely, experts that were best at some point in time. We derive regret bounds that depend only on this measure, independent of the total number of experts. We also consider a case where all experts remain grouped in just a few clusters in terms of their realized cumulative losses. Here too, our regret bounds depend only on the number of clusters determined in retrospect, which serves as a measure of complexity. Our results are obtained as special cases of a more general analysis for a setting of branching experts,where the set of experts may grow over time according to a tree-like structure, determined by an adversary. For this setting of branching experts, we give algorithms and analysis that cover both the full information and the bandit scenarios."
209,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Bayesian Games for Adversarial Regression Problems,"Michael Gro_hans, Christoph Sawade, Michael Brôckner, Tobias Scheffer",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/grosshans13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/grosshans13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_grosshans13,http://jmlr.csail.mit.edu/proceedings/papers/v28/grosshans13.html,"We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversaryês objective; instead, any knowledge of the learner about parameters of the adversaryês goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression."
210,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Active Learning of Halfspaces: an Aggressive Approach,"Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gonen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gonen13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gonen13.html,"We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings."
211,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Algebraic Reconstruction Bounds and Explicit Inversion for Phase Retrieval at the Identifiability Threshold,"Franz Kirˆly, Martin Ehler","JMLR W&CP 33 :503-511, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kiraly14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kiraly14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kiraly14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kiraly14.html,"We study phase retrieval from magnitude measurements of an unknown signal as an algebraic estimation problem. Indeed, phase retrieval from rank-one and more general linear measurements can be treated in an algebraic way. It is verified that a certain number of generic rank-one or generic linear measurements are sufficient to enable signal reconstruction for generic signals, and slightly more generic measurements yield reconstructability for all signals. Our results solve few open problems stated in the recent literature. Furthermore, we show how the algebraic estimation problem can be solved by a closed-form algebraic estimation technique, termed ideal regression, providing non-asymptotic success guarantees."
212,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Information Geometry and Minimum Description Length Networks,"Ke Sun, Jun Wang, Alexandros Kalousis, Stephan Marchand-Maillet",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/suna15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/suna15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_suna15,http://jmlr.csail.mit.edu/proceedings/papers/v37/suna15.html,"We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations."
213,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints,"Alexander Rakhlin, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Rakhlin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Rakhlin15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Rakhlin15.html,"We study online prediction where regret of the algorithm is measured against a benchmark defined via evolving constraints. This framework captures online prediction on graphs, as well as other prediction problems with combinatorial structure. A key aspect here is that finding the optimal benchmark predictor (even in hindsight, given all the data) might be computationally hard due to the combinatorial nature of the constraints. Despite this, we provide polynomial-time prediction algorithms that achieve low regret against combinatorial benchmark sets. We do so by building improper learning algorithms based on two ideas that work together. The first is to alleviate part of the computational burden through random playout, and the second is to employ Lasserre semidefinite hierarchies to approximate the resulting integer program. Interestingly, for our prediction algorithms, we only need to compute the values of the semidefinite programs and not the rounded solutions. However, the integrality gap for Lasserre hierarchy does enter the generic regret bound in terms of Rademacher complexity of the benchmark set. This establishes a trade-off between the computation time and the regret bound of the algorithm."
214,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Exchangeability Characterizes Optimality of Sequential Normalized Maximum Likelihood and Bayesian Prediction with Jeffreys Prior,"Fares Hedayati, Peter Bartlett",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/hedayati12/hedayati12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_hedayati12,http://jmlr.csail.mit.edu/proceedings/papers/v22/hedayati12.html,We study online prediction of individual sequences under logarithmic loss with parametric constant experts. The optimal strategy normalized maximum likelihood (NML) is computationally demanding and requires the length of the game to be known. We consider two simpler strategies: sequential normalized maximum likelihood (SNML) which computes the NML forecasts at each round as if it were the last round and Bayesian prediction. Under appropriate conditions both are known to achieve near-optimal regret. In this paper we investigate when these strategies are optimal. We show that SNML is optimal iff the joint distribution on sequences defined by SNML is exchangeable. In the case of exponential families this is equivalent to the optimality of any Bayesian prediction strategy and the optimal prior is Jeffreys prior.
215,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,The Optimality of Jeffreys Prior for Online Density Estimation and the Asymptotic Normality of Maximum Likelihood Estimators,Fares Hedayati and Peter L. Bartlett,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/hedayati12/hedayati12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_hedayati12,http://jmlr.csail.mit.edu/proceedings/papers/v23/hedayati12.html,"We study online learning under logarithmic loss with regular parametric models. We show that a Bayesian strategy predicts optimally only if it uses Jeffreys prior. This result was known for canonical exponential families; we extend it to parametric models for which the maximum likelihood estimator is asymptotically normal. The optimal prediction strategy, normalized maximum likelihood, depends on the number n of rounds of the game, in general. However, when a Bayesian strategy is optimal, normalized maximum likelihood becomes independent of n . Our proof uses this to exploit the asymptotics of normalized maximum likelihood. The asymptotic normality of the maximum likelihood estimator is responsible for the necessity of Jeffreys prior."
216,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families,"Peter Bartlett, Peter Grônwald, Peter Harremoïs, Fares Hedayati, Wojciech Kotlowski",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bartlett13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Bartlett13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bartlett13.html,"We study online learning under logarithmic loss with regular parametric models. Hedayati and Bartlett (2012) showed that a Bayesian prediction strategy with Jeffreys prior and sequential normalized maximum likelihood (SNML) coincide and are optimal if and only if the latter is exchangeable, which occurs if and only if the optimal strategy can be calculated without knowing the time horizon in advance. They put forward the question what families have exchangeable SNML strategies. We answer this question for one-dimensional exponential families: SNML is exchangeable only for three classes of natural exponential family distributions,namely the Gaussian, the gamma, and the Tweedie exponential family of order \(3/2\) ."
217,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Online Learning: Beyond Regret,"Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari","19:559-594, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/rakhlin11a/rakhlin11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_rakhlin11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/rakhlin11a.html,We study online learnability of a wide class of problems extending the results of \cite{RakSriTew10} to general notions of performance measure well beyond external regret. Our framework simultaneously captures such well-known notions as internal and general $\Phi$-regret learning with non-additive global cost functions Blackwell's approachability calibration of forecasters and more. We show that learnability in all these situations is due to control of the same three quantities: a martingale convergence term a term describing the ability to perform well if future is known and a generalization of sequential Rademacher complexity studied in \cite{RakSriTew10}. Since we directly study complexity of the problem instead of focusing on efficient algorithms we are able to improve and extend many known results which have been previously derived via an algorithmic construction.
218,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Optimal and Adaptive Algorithms for Online Boosting,"Alina Beygelzimer, Satyen Kale, Haipeng Luo",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/beygelzimer15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/beygelzimer15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_beygelzimer15,http://jmlr.csail.mit.edu/proceedings/papers/v37/beygelzimer15.html,"We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal."
219,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,A second-order bound with excess losses,"Pierre Gaillard, Gilles Stoltz, Tim van Erven","JMLR W&CP 35 :176-196, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/gaillard14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_gaillard14,http://jmlr.csail.mit.edu/proceedings/papers/v35/gaillard14.html,"We study online aggregation of the predictions of experts, and first show new second-order regret bounds in the standard setting, which are obtained via a version of the Prod algorithm (and also a version of the polynomially weighted average algorithm) with multiple learning rates. These bounds are in terms of excess losses, the differences between the instantaneous losses suffered by the algorithm and the ones of a given expert. We then demonstrate the interest of these bounds in the context of experts that report their confidences as a number in the interval \([0,1]\) using a generic reduction to the standard setting. We conclude by two other applications in the standard setting, which improve the known bounds in case of small excess losses and show a bounded regret against i.i.d. sequences of losses."
220,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Data modeling with the elliptical gamma distribution,"Suvrit Sra, Reshad Hosseini, Lucas Theis, Matthias Bethge",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/sra15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_sra15,http://jmlr.csail.mit.edu/proceedings/papers/v38/sra15.html,"We study mixture modeling using the elliptical gamma (EG) distribution, a non-Gaussian distribution that allows heavy and light tail and peak behaviors. We first consider maximum likelihood parameter estimation, a task that turns out to be very challenging: we must handle positive definiteness constraints, and more crucially, we must handle possibly nonconcave log-likelihoods, which makes maximization hard. We overcome these difficulties by developing algorithms based on fixed-point theory; our methods respect the psd constraint, while also efficiently solving the (possibly) nonconcave maximization to global optimality. Subsequently, we focus on mixture modeling using EG distributions: we present a closed-form expression of the KL-divergence between two EG distributions, which we then combine with our ML estimation methods to obtain an efficient split-and-merge expectation maximization algorithm. We illustrate the use of our model and algorithms on a dataset of natural image patches."
221,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Maximum Entropy Correlated Equilibria,"Luis E. Ortiz, Robert E. Schapire, Sham M. Kakade","2:347-354, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/ortiz07a/ortiz07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_ortiz07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/ortiz07a.html,We study maximum entropy correlated equilibria (Maxent CE) in multi-player games. After motivating and deriving some interesting important properties of Maxent CE we provide two gradient-based algorithms that are guaranteed to converge to it. The proposed algorithms have strong connections to algorithms for statistical estimation (e.g. iterative scaling) and permit a distributed learning-dynamics interpretation. We also briefly discuss possible connections of this work and more generally of the Maximum Entropy Principle in statistics to the work on learning in games and the problem of equilibrium selection.
222,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Tracking Adversarial Targets,"Yasin Abbasi-Yadkori, Peter Bartlett, Varun Kanade",none,http://jmlr.org/proceedings/papers/v32/abbasi-yadkori14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/abbasi-yadkori14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_abbasi-yadkori14,http://jmlr.csail.mit.edu/proceedings/papers/v32/abbasi-yadkori14.html,"We study linear control problems with quadratic losses and adversarially chosen tracking targets. We present an efficient algorithm for this problem and show that, under standard conditions on the linear system, its regret with respect to an optimal linear policy grows as \(O(\log^2 T)\) , where \(T\) is the number of rounds of the game. We also study a problem with adversarially chosen transition dynamics; we present an exponentially-weighted average algorithm for this problem, and we give regret bounds that grow as \(O(\sqrt T)\) ."
223,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,New Bounds for Learning Intervals with Implications for Semi-Supervised Learning,David P. Helmbold and Philip M. Long,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/helmbold12/helmbold12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_helmbold12,http://jmlr.csail.mit.edu/proceedings/papers/v23/helmbold12.html,"We study learning of initial intervals in the prediction model. We show that for each distribution D over the domain, there is an algorithm A D , whose probability of a mistake in round m is at most (_ + o(1))/m . We also show that the best possible bound that can be achieved in the case in which the same algorithm A must be applied for all distributions D is at least ( 1 ˜ í e - o(1)) 1 ˜ m _ ( 3 ˜ 5 -o(1)) 1 ˜ m . Informally, ""knowing"" the distribution D enables an algorithm to reduce its error rate by a constant factor strictly greater than 1. As advocated by Ben-David et al. (2008), knowledge of D can be viewed as an idealized proxy for a large number of unlabeled examples."
224,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Deterministic Independent Component Analysis,"Ruitong Huang, Andras Gyorgy, Csaba Szepesvˆri",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/huangb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_huangb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/huangb15.html,"We study independent component analysis with noisy observations. We present, for the first time in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals and the mixing matrix with a reconstruction error that vanishes at a \(1/\sqrt{T}\) rate using \(T\) observations and scales only polynomially with the natural parameters of the problem. Our algorithms and analysis also extend to deterministic source signals whose empirical distributions are approximately independent."
225,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Robust Multi-objective Learning with Mentor Feedback,"Alekh Agarwal, Ashwinkumar Badanidiyuru, Miroslav DudÍk, Robert E. Schapire, Aleksandrs Slivkins","JMLR W&CP 35 :726-741, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/agarwal14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_agarwal14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/agarwal14b.html,"We study decision making when each action is described by a set of objectives, all of which are to be maximized. During the training phase, we have access to the actions of an outside agent (–mentor”). In the test phase, our goal is to maximally improve upon the mentorês (unobserved) actions across all objectives. We present an algorithm with a vanishing regret compared with the optimal possible improvement, and show that our regret bound is the best possible. The bound is independent of the number of actions, and scales only as the logarithm of the number of objectives."
226,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Contextual Multi-Armed Bandits,"Tyler Lu, David Pal, Martin Pal","9:485-492, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/lu10a/lu10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_lu10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/lu10a.html,We study contextual multi-armed bandit problems where the context comes from a metric space and the payoff satisfies a Lipschitz condition with respect to the metric. Abstractly a contextual multi-armed bandit problem models a situation where in a sequence of independent trials an online algorithm chooses based on a given context (side information) an action from a set of possible actions so as to maximize the total payoff of the chosen actions. The payoff depends on both the action chosen and the context. In contrast context-free multi-armed bandit problems a focus of much previous research model situations where no side information is available and the payoff depends only on the action chosen. Our problem is motivated by sponsored web search where the task is to display ads to a user of an Internet search engine based on her search query so as to maximize the click-through rate (CTR) of the ads displayed. We cast this problem as a contextual multi-armed bandit problem where queries and ads form metric spaces and the payoff function is Lipschitz with respect to both the metrics. For any $\epsilon _ 0$ we present an algorithm with regret $O(T^{\frac{a+b+1}{a+b+2} + \epsilon})$ where $ab$ are the covering dimensions of the query space and the ad space respectively. We prove a lower bound $\Omega(T^{\frac{\tilde{a}+\tilde{b}+1}{\tilde{a}+\tilde{b}+2} \epsilon})$ for the regret of any algorithm where $\tilde{a} \tilde{b}$ are packing dimensions of the query spaces and the ad space respectively. For finite spaces or convex bounded subsets of Euclidean spaces this gives an almost matching upper and lower bound.
227,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Resourceful Contextual Bandits,"Ashwinkumar Badanidiyuru, John Langford, Aleksandrs Slivkins","JMLR W&CP 35 :1109-1134, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/badanidiyuru14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_badanidiyuru14,http://jmlr.csail.mit.edu/proceedings/papers/v35/badanidiyuru14.html,"We study contextual bandits with ancillary constraints on resources, which are common in real-world applications such as choosing ads or dynamic pricing of items. We design the first algorithm for solving these problems that improves over a trivial reduction to the non-contextual case. We consider very general settings for both contextual bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties."
228,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing,"Yasin Abbasi-Yadkori, Peter Bartlett, Xi Chen, Alan Malek",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/abbasi-yadkori15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/abbasi-yadkori15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_abbasi-yadkori15,http://jmlr.csail.mit.edu/proceedings/papers/v37/abbasi-yadkori15.html,"We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical costs of finding the optimal policy scale with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can reduce policy optimization to a convex optimization and solve it approximately using a stochastic subgradient algorithm. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by controlling the important crowdsourcing application of budget allocation in crowd labeling."
229,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations,"H. Brendan McMahan, Francesco Orabona","JMLR W&CP 35 :1020-1039, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/mcmahan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_mcmahan14,http://jmlr.csail.mit.edu/proceedings/papers/v35/mcmahan14.html,"We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of \(O(U \sqrt{T \log( U \sqrt{T} \log^2 T +1)})\) , where \(U\) is the \(L_2\) norm of an arbitrary comparator and both \(T\) and \(U\) are unknown to the player. This bound is optimal up to \(\sqrt{\log \log T}\) terms. When \(T\) is known, we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown \(T\) case, a Normal approximation to the conditional value of the game proves to be the key analysis tool."
230,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Generic Exploration and K-armed Voting Bandits,"Tanguy Urvoy, Fabrice Clerot, Raphael F_raud, Sami Naamane",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/urvoy13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/urvoy13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_urvoy13,http://jmlr.csail.mit.edu/proceedings/papers/v28/urvoy13.html,"We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd."
231,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Tensor principal component analysis via sum-of-square proofs,"Samuel B. Hopkins, Jonathan Shi, David Steurer",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Hopkins15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Hopkins15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Hopkins15.html,"We study a statistical model for the tensor principal component analysis problem introduced by Montanari and Richard: Given a order- \(3\) tensor \(\mathbf T\) of the form \(\mathbf T = \tau \cdot v_0^{\otimes 3} + \mathbf A\) , where \(\tau \geq 0\) is a signal-to-noise ratio, \(v_0\) is a unit vector, and \(\mathbf A\) is a random noise tensor, the goal is to recover the planted vector \(v_0\) . For the case that \(\mathbf A\) has iid standard Gaussian entries, we give an efficient algorithm to recover \(v_0\) whenever \(\tau \geq \omega(n^{3/4} \log(n)^{1/4})\) , and certify that the recovered vector is close to a maximum likelihood estimator, all with high probability over the random choice of \(\mathbf A\) . The previous best algorithms with provable guarantees required \(\tau \geq \Omega(n)\) . In the regime \(\tau \leq o(n)\) , natural tensor-unfolding-based spectral relaxations for the underlying optimization problem break down. To go beyond this barrier, we use convex relaxations based on the sum-of-squares method. Our recovery algorithm proceeds by rounding a degree- \(4\) sum-of-squares relaxations of the maximum-likelihood-estimation problem for the statistical model. To complement our algorithmic results, we show that degree- \(4\) sum-of-squares relaxations break down for \(\tau \leq O(n^{3/4}/\log(n)^{1/4})\) , which demonstrates that improving our current guarantees (by more than logarithmic factors) would require new techniques or might even be intractable. Finally, we show how to exploit additional problem structure in order to solve our sum-of-squares relaxations, up to some approximation, very efficiently. Our fastest algorithm runs in nearly-linear time using shifted (matrix) power iteration and has similar guarantees as above. The analysis of this algorithm also confirms a variant of a conjecture of Montanari and Richard about singular vectors of tensor unfoldings."
232,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics,"Philipp Hennig, SËren Hauberg","JMLR W&CP 33 :347-355, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hennig14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/henning14-supp.zip,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hennig14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hennig14.html,"We study a probabilistic numerical method for the solution of both boundary and initial value problems that returns a joint Gaussian process posterior over the solution. Such methods have concrete value in the statistics on Riemannian manifolds, where non-analytic ordinary differential equations are involved in virtually all computations. The probabilistic formulation permits marginalising the uncertainty of the numerical solution such that statistics are less sensitive to inaccuracies. This leads to new Riemannian algorithms for mean value computations and principal geodesic analysis. Marginalisation also means results can be less precise than point estimates, enabling a noticeable speed-up over the state of the art. Our approach is an argument for a wider point that uncertainty caused by numerical calculations should be tracked throughout the pipeline of machine learning algorithms."
233,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,The Falling Factorial Basis and Its Statistical Applications,"Yu-Xiang Wang, Alex Smola, Ryan Tibshirani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wange14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wange14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wange14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wange14.html,"We study a novel spline-like basis, which we name the falling factorial basis , bearing many similarities to the classic truncated power basis. The advantage of the falling factorial basis is that it enables rapid, linear-time computations in basis matrix multiplication and basis matrix inversion. The falling factorial functions are not actually splines, but are close enough to splines that they provably retain some of the favorable properties of the latter functions. We examine their application in two problems: trend filtering over arbitrary input points, and a higher-order variant of the two-sample Kolmogorov-Smirnov test."
234,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Online Learning with Composite Loss Functions,"Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres","JMLR W&CP 35 :1214-1231, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/dekel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_dekel14,http://jmlr.csail.mit.edu/proceedings/papers/v35/dekel14.html,"We study a new class of online learning problems where each of the online algorithmês actions is assigned an adversarial value, and the loss of the algorithm at each step is a known and deterministic function of the values assigned to its recent actions. This class includes problems where the algorithmês loss is the minimum over the recent adversarial values, the maximum over the recent values, or a linear combination of the recent values. We analyze the minimax regret of this class of problems when the algorithm receives bandit feedback, and prove that when the minimum or maximum functions are used, the minimax regret is \(\widetilde \Omega(T^{2/3})\) (so called hard online learning problems), and when a linear function is used, the minimax regret is \(\widetilde O(\sqrt{T})\) (so called easy learning problems). Previously, the only online learning problem that was known to be provably hard was the multi-armed bandit with switching costs."
235,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian Inference for Change Points in Dynamical Systems with Reusable States - a Chinese Restaurant Process Approach,"Florian Stimberg, Andreas Ruttor, Manfred Opper",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/stimberg12/stimberg12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_stimberg12,http://jmlr.csail.mit.edu/proceedings/papers/v22/stimberg12.html,We study a model of a stochastic process with unobserved parameters which suddenly change at random times. The possible parameter values are assumed to be from a finite but unknown set. Using a Chinese restaurant process prior over parameters we develop an efficient MCMC procedure for Bayesian inference. We demonstrate the significance of our approach with an application to systems biology data.
236,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A General Framework for Structured Sparsity via Proximal Optimization,"Luca Baldassarre, Jean Morales, Andreas Argyriou, Massimiliano Pontil",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/baldassarre12/baldassarre12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_baldassarre12,http://jmlr.csail.mit.edu/proceedings/papers/v22/baldassarre12.html,We study a generalized framework for structured sparsity. It extends the well-known methods of Lasso and Group Lasso by incorporating additional constraints on the variables as part of a convex optimization problem. This framework provides a straightforward way of favoring prescribed sparsity patterns such as orderings contiguous regions and overlapping groups among others. Available optimization methods are limited to specific constraint sets and tend to not scale well with sample size and dimensionality. We propose a first order proximal method which builds upon results on fixed points and successive approximations. The algorithm can be applied to a general class of conic and norm constraints sets and relies on a proximity operator subproblem which can be computed numerically. Experiments on different regression problems demonstrate state-of-the art statistical performance which improves over Lasso Group Lasso and StructOMP. They also demonstrate the efficiency of the optimization algorithm and its scalability with the size of the problem.
237,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Online Learning with Feedback Graphs: Beyond Bandits,"Noga Alon, NicolÖ Cesa-Bianchi, Ofer Dekel, Tomer Koren",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Alon15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Alon15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Alon15.html,"We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced \(T\) -round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with \(\widetilde\Theta(\alpha^{1/2} T^{1/2})\) minimax regret, where \(\alpha\) is the independence number of the underlying graph; the second class induces problems with \(\widetilde\Theta(\delta^{1/3}T^{2/3})\) minimax regret, where \(\delta\) is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time."
238,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Divide and Conquer Kernel Ridge Regression,"Yuchen Zhang, John Duchi, Martin Wainwright",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Zhang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Zhang13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Zhang13.html,"We study a decomposition-based scalable approach to performing kernel ridge regression. The method is simply described: it randomly partitions a dataset of size \(N\) into \(m\) subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all \(N\) samples. Our main theorem establishes that despite the computational speed-up, statistical optimality is retained: that so long as \(m\) is not too large, the partition-based estimate achieves optimal rates of convergence for the full sample size \(N\) . As concrete examples, our theory guarantees that \(m\) may grow polynomially in \(N\) for Sobolev spaces, and nearly linearly for finite-rank kernels and Gaussian kernels. We conclude with simulations complementing our theoretical results and exhibiting the computational and statistical benefits of our approach."
239,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Localized Complexities for Transductive Learning,"Ilya Tolstikhin, Gilles Blanchard, Marius Kloft","JMLR W&CP 35 :857-884, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/tolstikhin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_tolstikhin14,http://jmlr.csail.mit.edu/proceedings/papers/v35/tolstikhin14.html,"We show two novel concentration inequalities for suprema of empirical processes when sampling without replacement, which both take the variance of the functions into account. While these inequalities may potentially have broad applications in learning theory in general, we exemplify their significance by studying the transductive setting of learning theory. For which we provide the first excess risk bounds based on the localized complexity of the hypothesis class, which can yield fast rates of convergence also in the transductive learning setting. We give a preliminary analysis of the localized complexities for the prominent case of kernel classes."
240,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Kendall and Mallows Kernels for Permutations,"Yunlong Jiao, Jean-Philippe Vert",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiao15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_jiao15,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiao15.html,"We show that the widely used Kendall tau correlation coefficient is a positive definite kernel for permutations. It offers a computationally attractive alternative to more complex kernels on the symmetric group to learn from rankings, or to learn to rank. We show how to extend it to partial rankings or rankings with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications."
241,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Stick-Breaking Beta Processes and the Poisson Process,"John Paisley, David Blei, Michael Jordan",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/paisley12/paisley12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_paisley12,http://jmlr.csail.mit.edu/proceedings/papers/v22/paisley12.html,We show that the stick-breaking construction of the beta process due to \cite{Paisley:2010} can be obtained from the characterization of the beta process as a Poisson process. Specifically we show that the mean measure of the underlying Poisson process is equal to that of the beta process. We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature. We also develop a new MCMC inference algorithm for beta processes based in part on our new Poisson process construction.
242,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Exploiting Within-Clique Factorizations in Junction-Tree Algorithms,"Julian McAuley, Tiberio Caetano","9:525-532, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/mcauley10a/mcauley10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_mcauley10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/mcauley10a.html,We show that the expected computational complexity of the Junction-Tree Algorithm for maximum a posteriori inference in graphical models can be improved. Our results apply whenever the potentials over maximal cliques of the triangulated graph are factored over subcliques. This is common in many real applications as we illustrate with several examples. The new algorithms are easily implemented and experiments show substantial speed-ups over the classical Junction-Tree Algorithm. This enlarges the class of models for which exact inference is efficient.
243,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,(weak) Calibration is Computationally Hard,Elad Hazan and Sham M. Kakade,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/hazan12a/hazan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_hazan12a,http://jmlr.csail.mit.edu/proceedings/papers/v23/hazan12a.html,"We show that the existence of a computationally efficient calibration algorithm, with a low weak calibration rate, would imply the existence of an efficient algorithm for computing approximate Nash equilibria -- thus implying the unlikely conclusion that every problem in PPAD is solvable in polynomial time."
244,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Hierarchical Beta Processes and the Indian Buffet Process,"Romain Thibaux, Michael I. Jordan","2:564-571, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/thibaux07a/thibaux07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_thibaux07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/thibaux07a.html,We show that the beta process is the de Finetti mixing distribution underlying the Indian buffet process of [2]. This result shows that the beta process plays the role for the Indian buffet process that the Dirichlet process plays for the Chinese restaurant process a parallel that guides us in deriving analogs for the beta process of the many known extensions of the Dirichlet process. In particular we define Bayesian hierarchies of beta processes and use the connection to the beta process to develop posterior inference algorithms for the Indian buffet process. We also present an application to document classification exploring a relationship between the hierarchical beta process and smoothed naive Bayes models.
245,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Bias in Natural Actor-Critic Algorithms,Philip Thomas,none,http://jmlr.org/proceedings/papers/v32/thomas14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_thomas14,http://jmlr.csail.mit.edu/proceedings/papers/v32/thomas14.html,"We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well."
246,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Large-Scale Bandit Problems and KWIK Learning,"Jacob Abernethy, Kareem Amin, Michael Kearns, Moez Draief",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/abernethy13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/abernethy13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_abernethy13,http://jmlr.csail.mit.edu/proceedings/papers/v28/abernethy13.html,"We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model known as Knows What It Knows or KWIK learning. We give matching impossibility results showing that the KWIK learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time."
247,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,"From Averaging to Acceleration, There is Only a Step-size","Nicolas Flammarion, Francis Bach",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Flammarion15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Flammarion15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Flammarion15.html,"We show that accelerated gradient descent, averaged gradient descent and the heavy-ball method for quadratic non-strongly-convex problems may be reformulated as constant parameter second-order difference equation algorithms, where stability of the system is equivalent to convergence at rate \(O(1/n^2)\) , where \(n\) is the number of iterations. We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system, showing various oscillatory and non-oscillatory behaviors, together with a sharp stability result with explicit constants. We also consider the situation where noisy gradients are available, where we extend our general convergence result, which suggests an alternative algorithm (i.e., with different step sizes) that exhibits the good aspects of both averaging and acceleration."
248,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Bayes consistent 1-NN classifier,"Aryeh Kontorovich, Roi Weiss",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kontorovich15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kontorovich15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kontorovich15.html,"We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported."
249,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Visualizing Similarity Data with a Mixture of Maps,"James Cook, Ilya Sutskever, Andriy Mnih, Geoffrey Hinton","2:67-74, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/cook07a/cook07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_cook07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/cook07a.html,"We show how to visualize a set of pairwise similarities between objects by using several different two-dimensional maps each of which captures different aspects of the similarity structure. When the objects are ambiguous words for example different senses of a word occur in different maps so ""river"" and ""loan"" can both be close to ""bank"" without being at all close to each other. Aspect maps resemble clustering because they model pair-wise similarities as a mixture of different types of similarity but they also resemble local multi-dimensional scaling because they model each type of similarity by a twodimensional map. We demonstrate our method on a toy example a database of human wordassociation data a large set of images of handwritten digits and a set of feature vectors that represent words."
250,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Optimally Sparse Support Vector Machines,"Andrew Cotter, Shai Shalev-Shwartz, Nati Srebro",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cotter13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/cotter13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cotter13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cotter13.html,"We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice."
251,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure,"Ruslan Salakhutdinov, Geoff Hinton","2:412-419, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/salakhutdinov07a/salakhutdinov07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_salakhutdinov07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/salakhutdinov07a.html,We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classification performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classification our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity.
252,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Exact Bayesian structure learning from uncertain interventions,"Daniel Eaton, Kevin Murphy","2:107-114, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/eaton07a/eaton07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_eaton07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/eaton07a.html,We show how to apply the dynamic programming algorithm of Koivisto and Sood [KS04 Koi06] which computes the exact posterior marginal edge probabilities $p(G_{ij} = 1|D)$ of a DAG G given data D to the case where the data is obtained by interventions (experiments). In particular we consider the case where the targets of the interventions are a priori unknown. We show that it is possible to learn the targets of intervention at the same time as learning the causal structure. We apply our exact technique to a biological data set that had previously been analyzed using MCMC [SPP+ 05 EW06 WGH06].
253,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Infinite-Dimensional Kalman Filtering Approach to Spatio-Temporal Gaussian Process Regression,"Simo Sarkka, Jouni Hartikainen",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sarkka12/sarkka12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sarkka12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sarkka12.html,We show how spatio-temporal Gaussian process (GP) regression problems (or the equivalent Kriging problems) can be formulated as infinite-dimensional Kalman filtering and Rauch-Tung-Striebel (RTS) smoothing problems and present a procedure for converting spatio-temporal covariance functions into infinite-dimensional stochastic differential equations (SDEs). The resulting infinite-dimensional SDEs belong to the class of stochastic pseudo-differential equations and can be numerically treated using the methods developed for deterministic counterparts of the equations. The scaling of the computational cost in the proposed approach is linear in the number of time steps as opposed to the cubic scaling of the direct GP regression solution. We also show how separable covariance functions lead to a finite-dimensional Kalman filtering and RTS smoothing problem present analytical and numerical examples and discuss numerical methods for computing the solutions.
254,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations,"Timothy Mann, Shie Mannor",none,http://jmlr.org/proceedings/papers/v32/mann14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mann14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mann14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mann14.html,"We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations."
255,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Interactive Fingerprinting Codes and the Hardness of Preventing False Discovery,"Thomas Steinke, Jonathan Ullman",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Steinke15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Steinke15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Steinke15.html,"We show an essentially tight bound on the number of adaptively chosen statistical queries that a computationally efficient algorithm can answer accurately given \(n\) samples from an unknown distribution. A statistical query asks for the expectation of a predicate over the underlying distribution, and an answer to a statistical query is accurate if it is –close” to the correct expectation over the distribution. This question was recently studied by Dwork et al. (2015), who showed how to answer \(\tilde{\Omega}(n^2)\) queries efficiently, and also by Hardt and Ulman (2014), who showed that answering \(\tilde{O}(n^3)\) queries is hard. We close the gap between the two bounds and show that, under a standard hardness assumption, there is no computationally efficient algorithm that, given \(n\) samples from an unknown distribution, can give valid answers to \(O(n^2)\) adaptively chosen statistical queries. An implication of our results is that computationally efficient algorithms for answering arbitrary, adaptively chosen statistical queries may as well be differentially private . We obtain our results using a new connection between the problem of answering adaptively chosen statistical queries and a combinatorial object called an interactive fingerprinting code Fiat and Tassa (2001). In order to optimize our hardness result, we give a new Fourier-analytic approach to analyzing fingerprinting codes that is simpler, more flexible, and yields better parameters than previous constructions."
256,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Efficient Learning of Simplices,"Joseph Anderson, Navin Goyal, Luis Rademacher",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anderson13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Anderson13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anderson13.html,"We show an efficient algorithm for the following problem: Given uniformly random points from an arbitrary \(n\) -dimensional simplex, estimate the simplex. The size of the sample and the number of arithmetic operations of our algorithm are polynomial in \(n\) . This answers a question of Frieze, Jerrum and Kannan Frieze et al. (1996). Our result can also be interpreted as efficiently learning the intersection of \(n + 1\) half-spaces in \(R^n\) in the model where the intersection is bounded and we are given polynomially many uniform samples from it. Our proof uses the local search technique from Independent Component Analysis (ICA), also used by Frieze et al. (1996). Unlike these previous algorithms, which were based on analyzing the fourth moment, ours is based on the third moment. We also show a direct connection between the problem of learning a simplex and ICA: a simple randomized reduction to ICA from the problem of learning a simplex. The connection is based on a known representation of the uniform measure on a simplex. Similar representations lead to a reduction from the problem of learning an affine transformation of an \(n\) -dimensional \(l_p\) ball to ICA."
257,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Robust partially observable Markov decision process,Takayuki Osogami,none,http://jmlr.csail.mit.edu/proceedings/papers/v37/osogami15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/osogami15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_osogami15,http://jmlr.csail.mit.edu/proceedings/papers/v37/osogami15.html,"We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies."
258,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Feature-Budgeted Random Forest,"Feng Nan, Joseph Wang, Venkatesh Saligrama",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/nan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/nan15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_nan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/nan15.html,"We seek decision rules for prediction-time cost reduction , where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms."
259,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Label optimal regret bounds for online local learning,"Pranjal Awasthi, Moses Charikar, Kevin A Lai, Andrej Risteski",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Awasthi15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Awasthi15a,http://jmlr.csail.mit.edu/proceedings/papers/v40/Awasthi15a.html,"We resolve an open question from Christiano (2014b) posed in COLTê14 regarding the optimal dependency of the regret achievable for online local learning on the size of the label set. In this framework, the algorithm is shown a pair of items at each step, chosen from a set of \(n\) items. The learner then predicts a label for each item, from a label set of size \(L\) and receives a real valued payoff. This is a natural framework which captures many interesting scenarios such as online gambling and online max cut. Christiano (2014a) designed an efficient online learning algorithm for this problem achieving a regret of \(O(\sqrt{nL^3 T})\) , where \(T\) is the number of rounds. Information theoretically, one can achieve a regret of \(O(\sqrt{n \log L T})\) . One of the main open questions left in this framework concerns closing the above gap. In this work, we provide a complete answer to the question above via two main results. We show, via a tighter analysis, that the semi-definite programming based algorithm of Christiano (2014a) in fact achieves a regret of \(O(\sqrt{nLT})\) . Second, we show a matching computational lower bound. Namely, we show that a polynomial time algorithm for online local learning with lower regret would imply a polynomial time algorithm for the planted clique problem which is widely believed to be hard. We prove a similar hardness result under a related conjecture concerning planted dense subgraphs that we put forth. Unlike planted clique, the planted dense subgraph problem does not have any known quasi-polynomial time algorithms. Computational lower bounds for online learning are relatively rare, and we hope that the ideas developed in this work will lead to lower bounds for other online learning scenarios as well."
260,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,A Subgroup Discovery Approach for Relating Chemical Structure and Phenotype Data in Chemical Genomics,"Lan Umek, Petra Kaferle, Mojca Mattiazzi, Ale_ Erjavec, _rtomir Gorup, Toma_ Curk, Uro_ Petrovi_, Bla_ Zupan","8:136-144, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/umek10a/umek10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_umek10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/umek10a.html,"We report on development of an algorithm that can infer relations between the chemical structure and biochemical pathways from mutant-based growth fitness characterizations of small molecules. Identification of such relations is very important in drug discovery and development from the perspective of argument-based selection of candidate molecules in target-specific screenings, and early exclusion of substances with highly probable undesired side-effects. The algorithm uses a combination of unsupervised and supervised machine learning techniques, and besides experimental fitness data uses knowledge on gene subgroups (pathways), structural descriptions of chemicals, and MeSH term-based chemical and pharmacological annotations. We demonstrate the utility of the proposed approach in the analysis of a genome-wide S. cerevisiae chemogenomics assay by Hillenmeyer et al. (Science, 2008)."
261,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Relating Function Class Complexity and Cluster Structure in the Function Domain with Applications to Transduction,Guy Lever,"9:437-444, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/lever10a/lever10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_lever10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/lever10a.html,We relate function class complexity to structure in the function domain. This facilitates risk analysis relative to cluster structure in the input space which is particularly effective in semi-supervised learning. In particular we quantify the complexity of function classes defined over a graph in terms of the graph structure.
262,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,MDL Histogram Density Estimation,"Petri Kontkanen, Petri Myllym_ki","2:219-226, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/kontkanen07a/kontkanen07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_kontkanen07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/kontkanen07a.html,We regard histogram density estimation as a model selection problem. Our approach is based on the information-theoretic minimum description length (MDL) principle which can be applied for tasks such as data clustering density estimation image denoising and model selection in general. MDL-based model selection is formalized via the normalized maximum likelihood (NML) distribution which has several desirable optimality properties. We show how this framework can be applied for learning generic irregular (variable-width bin) histograms and how to compute the NML model selection criterion efficiently. We also derive a dynamic programming algorithm for finding both the MDL-optimal bin count and the cut point locations in polynomial time. Finally we demonstrate our approach via simulation tests.
263,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Large-Scale Learning with Less RAM via Randomization,"Daniel Golovin, D. Sculley, Brendan McMahan, Michael Young",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/golovin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_golovin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/golovin13.html,"We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs."
264,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Revisiting the Nystrom method for improved large-scale machine learning,"Alex Gittens, Michael Mahoney",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gittens13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gittens13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gittens13.html,"We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing boundsã e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error."
265,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Does an Efficient Calibrated Forecasting Strategy Exist?,"Jacob Abernethy, Shie Mannor","19:811-814, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/abernethy11a/abernethy11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_abernethy11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/abernethy11a.html,We recall two previously-proposed notions of \emph{asymptotic calibration} for a forecaster making a sequence of probability predictions. We note that the existence of efficient algorithms for calibrated forecasting holds only in the case of binary outcomes. We pose the question: do there exist such efficient algorithms for the general (non-binary) case?
266,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization,Martin Jaggi,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jaggi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13.html,"We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices. We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach."
267,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction,"S _ bastien Gigu è re, Fran ç ois Laviolette, Mario Marchand, Khadidja Sylla",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/giguere13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/giguere13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_giguere13,http://jmlr.csail.mit.edu/proceedings/papers/v28/giguere13.html,We provide rigorous guarantees for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the prediction loss when the output kernel satisfies some condition with respect to the prediction loss. We provide two upper bounds of the prediction risk that depend on the empirical quadratic risk of the predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.
268,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence,"Nathaniel Korda, Prashanth La",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/korda15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_korda15,http://jmlr.csail.mit.edu/proceedings/papers/v37/korda15.html,"We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings."
269,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Entropy-Based Concentration Inequalities for Dependent Variables,"Liva Ralaivola, Massih-Reza Amini",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ralaivola15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ralaivola15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ralaivola15.html,"We provide new concentration inequalities for functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of the Laplace transform and the idea of fractional graph coloring, as well as many works that derive concentration inequalities using the entropy method (see, e.g., (Boucheron et al., 2003)). We give inequalities for fractionally sub-additive and fractionally self-bounding functions. In the way, we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for various applications where dependent variables naturally appear, such as in bipartite ranking."
270,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Learning Overcomplete Latent Variable Models through Tensor Methods,"Animashree Anandkumar, Rong Ge, Majid Janzamin",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Anandkumar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Anandkumar15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Anandkumar15.html,"We provide guarantees for learning latent variable models emphasizing on the overcomplete regime, where the dimensionality of the latent space exceeds the observed dimensionality. In particular, we consider multiview mixtures, ICA, and sparse coding models. Our main tool is a new algorithm for tensor decomposition that works in the overcomplete regime. In the semi-supervised setting, we exploit label information to get a rough estimate of the model parameters, and then refine it using the tensor method on unlabeled samples. We establish learning guarantees when the number of components scales as \(k=o(d^{p/2})\) , where \(d\) is the observed dimension, and \(p\) is the order of the observed moment employed in the tensor method (usually \(p=3,4\) ). In the unsupervised setting, a simple initialization algorithm based on SVD of the tensor slices is proposed, and the guarantees are provided under the stricter condition that \(k \leq \beta d\) (where constant \(\beta\) can be larger than \(1\) ). For the learning applications, we provide tight sample complexity bounds through novel covering arguments."
271,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Semi-supervised and Active Learning of Disjunctions,"Nina Balcan, Christopher Berlind, Steven Ehrlich, Yingyu Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_balcan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13.html,"We provide efficient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan & Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classification noise setting."
272,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians,"Constantinos Daskalakis, Gautam Kamath","JMLR W&CP 35 :1183-1213, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/daskalakis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_daskalakis14,http://jmlr.csail.mit.edu/proceedings/papers/v35/daskalakis14.html,"We provide an algorithm for properly learning mixtures of two single-dimensional Gaussians without any separability assumptions. Given \(\tilde{O}(1/\varepsilon^2)\) samples from an unknown mixture, our algorithm outputs a mixture that is \(\varepsilon\) -close in total variation distance, in time \(\tilde{O}(1/\varepsilon^5)\) . Our sample complexity is optimal up to logarithmic factors, and significantly improves upon both Kalai et al., whose algorithm has a prohibitive dependence on \(1/\varepsilon\) , and Feldman et al., whose algorithm requires bounds on the mixture parameters and depends pseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm for selecting a good candidate distribution from among competing hypotheses. Namely, given a collection of \(N\) hypotheses containing at least one candidate that is \(\varepsilon\) -close to an unknown distribution, our algorithm outputs a candidate which is \(O(\varepsilon)\) -close to the distribution. The algorithm requires \({O}(\log{N}/\varepsilon^2)\) samples from the unknown distribution and \({O}(N \log N/\varepsilon^2)\) time, which improves previous such results (such as the Scheff_ estimator) from a quadratic dependence of the running time on \(N\) to quasilinear. Given the wide use of such results for the purpose of hypothesis selection, our improved algorithm implies immediate improvements to any such use."
273,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Variable Metric Stochastic Approximation Theory,"Peter Sunehag, Jochen Trumpf, S.V.N. Vishwanathan, Nicol Schraudolph","5:560-566, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/sunehag09a/sunehag09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_sunehag09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/sunehag09a.html,We provide a variable metric stochastic approximation theory. In doing so we provide a convergence theory for a large class of online variable metric methods including the recently introduced online versions of the BFGS algorithm and its limited-memory LBFGS variant. We also discuss the implications of our results in the areas of elicitation of properties of distributions using prediction markets and in learning from expert advice.
274,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Non-asymptotic Analysis of Compressive Fisher Discriminants in terms of the Effective Dimension,Ata Kaban,none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kaban15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Kaban15a,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kaban15a.html,"We provide a non-asymptotic analysis of the generalisation error of compressive Fisher linear discriminant (FLD) classification that is dimension free under mild assumptions. Our analysis includes the effects that random projection has on classification performance under covariance model misspecification, as well as various good and bad effects of random projections that contribute to the overall performance of compressive FLD. We also give an asymptotic bound as a corollary of our finite sample result. An important ingredient of our analysis is to develop new dimension-free bounds on the largest and smallest eigenvalue of the compressive covariance, which may be of independent interest."
275,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A General Analysis of the Convergence of ADMM,"Robert Nishihara, Laurent Lessard, Ben Recht, Andrew Packard, Michael Jordan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/nishihara15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_nishihara15,http://jmlr.csail.mit.edu/proceedings/papers/v37/nishihara15.html,"We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence."
276,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy,"Jean Honorio, Jaakkola Tommi",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/honorio13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_honorio13,http://jmlr.csail.mit.edu/proceedings/papers/v28/honorio13.html,"We provide a method that approximates the Bayes error rate and the Shannon entropy with high probability. The Bayes error rate approximation makes possible to build a classifier that polynomially approaches Bayes error rate. The Shannon entropy approximation provides provable performance guarantees for learning trees and Bayesian networks from continuous variables. Our results rely on some reasonable regularity conditions of the unknown probability distributions, and apply to bounded as well as unbounded variables."
277,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers,"Pascal Germain, Amaury Habrard, Franois Laviolette, Emilie Morvant",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/germain13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/germain13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_germain13,http://jmlr.csail.mit.edu/proceedings/papers/v28/germain13.html,"We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools."
278,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Kernel Partial Least Squares is Universally Consistent,"Gilles Blanchard, Nicole Kr_mer","9:57-64, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/blanchard10a/blanchard10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_blanchard10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/blanchard10a.html,We prove the statistical consistency of kernel Partial Least Squares Regression applied to a bounded regression learning problem on a reproducing kernel Hilbert space. Partial Least Squares stands out of well-known classical approaches as e.g. Ridge Regression or Principal Components Regression as it is not defined as the solution of a global cost minimization procedure over a fixed model nor is it a linear estimator. Instead approximate solutions are constructed by projections onto a nested set of data-dependent subspaces. To prove consistency we exploit the known fact that Partial Least Squares is equivalent to the conjugate gradient algorithm in combination with early stopping. The choice of the stopping rule (number of iterations) is a crucial point. We study two empirical stopping rules. The first one monitors the estimation error in each iteration step of Partial Least Squares and the second one estimates the empirical complexity in terms of a condition number. Both stopping rules lead to universally consistent estimators provided the kernel is universal.
279,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,A Canonical Semi-Deterministic Transducer,"Achilles Beros, Colin de la Higuera","JMLR W&CP 34 :33-48, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/beros14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_beros14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/beros14a.html,"We prove the existence of a canonical form for semi-deterministic transducers with sets of pairwise incomparable output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge."
280,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Unifying Local Consistency and MAX SAT Relaxations for Scalable Inference with Rounding Guarantees,"Stephen Bach, Bert Huang, Lise Getoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/bach15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/bach15-supp.gz,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_bach15,http://jmlr.csail.mit.edu/proceedings/papers/v38/bach15.html,"We prove the equivalence of first-order local consistency relaxations and the MAX SAT relaxation of Goemans and Williamson (1994) for a class of MRFs we refer to as logical MRFs. This allows us to combine the advantages of each into a single MAP inference technique: solving the local consistency relaxation with any of a number of highly scalable message-passing algorithms, and then obtaining a high-quality discrete solution via a guaranteed rounding procedure when the relaxation is not tight. Logical MRFs are a general class of models that can incorporate many common dependencies, such as logical implications and mixtures of supermodular and submodular potentials. They can be used for many structured prediction tasks, including natural language processing, computer vision, and computational social science. We show that our new inference technique can improve solution quality by as much as 20% without sacrificing speed on problems with over one million dependencies."
281,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,The entropic barrier: a simple and optimal universal self-concordant barrier,"S_bastien Bubeck, Ronen Eldan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bubeck15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Bubeck15b,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bubeck15b.html,"We prove that the Fenchel dual of the log-Laplace transform of the uniform measure on a convex body in \(\mathbb{R}^n\) is a \((1+o(1)) n\) -self-concordant barrier, improving a seminal result of Nesterov and Nemirovski. This gives the first explicit construction of a universal barrier for convex bodies with optimal self-concordance parameter. The proof is based on basic geometry of log-concave distributions, and elementary duality in exponential families. The result also gives a new perspective on the minimax regret for the linear bandit problem."
282,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization,Brendan McMahan,"15:525-533, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mcmahan11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/mcmahan11b.html,We prove that many mirror descent algorithms for online convex optimization (such as online gradient descent) have an equivalent interpretation as follow-the-regularized-leader (FTRL) algorithms. This observation makes the relationships between many commonly used algorithms explicit and provides theoretical insight on previous experimental observations. In particular even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly it has been observed that the FTRL-style Regularized Dual Averaging (RDA) algorithm is even more effective at producing sparsity. Our results demonstrate that the key difference between these algorithms is how they handle the cumulative L1 penalty. While FOBOS handles the L1 term exactly on any given update we show that it is effectively using subgradient approximations to the L1 penalty from previous rounds leading to less sparsity than RDA which handles the cumulative penalty in closed form. The FTRL-Proximal algorithm which we introduce can be seen as a hybrid of these two algorithms and significantly outperforms both on a large real-world dataset.
283,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Activized Learning with Uniform Classification Noise,"Liu Yang, Steve Hanneke",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13c,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13c.html,"We prove that for any VC class, it is possible to transform any passive learning algorithm into an active learning algorithm with strong asymptotic improvements in label complexity for every nontrivial distribution satisfying a uniform classification noise condition. This generalizes a similar result proven by (Hanneke, 2009;2012) for the realizable case, and is the first result establishing that such general improvement guarantees are possible in the presence of restricted types of classification noise."
284,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Active and passive learning of linear separators under log-concave distributions,"Maria-Florina Balcan, Phil Long",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Balcan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Balcan13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Balcan13.html,"We prove that active learning provides an exponential improvement over PAC (passive) learning of homogeneous linear separators under nearly log-concave distributions. Building on this, we provide a computationally efficient PAC algorithm with optimal (up to a constant factor) sample complexity for such problems. This resolves an open question of (Long, 1995, 2003; Bshouty et al., 2009) concerning the sample complexity of efficient PAC algorithms under the uniform distribution in the unit ball. Moreover, it provides the first bound for a polynomial-time PAC algorithm that is tight for an interesting infinite class of hypothesis functions under a general class of data-distributions, providing significant progress towards a long standing open question of (Ehrenfeucht et al., 1989; Blumer et al., 1989). We also provide new bounds for active and passive learning in the case that the data might not be linearly separable, both in the agnostic case and and under the Tsybakov low-noise condition. To derive our results, we provide new structural results for (nearly) log-concave distributions, which might be of independent interest as well."
285,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,The Sample Complexity of Self-Verifying Bayesian Active Learning,"Liu Yang, Steve Hanneke, Jaime Carbonell","15:816-822, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/yang11a/yang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_yang11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/yang11a.html,We prove that access to a prior distribution over target functions can dramatically improve the sample complexity of self-terminating active learning algorithms so that it is always better than the known results for prior-dependent passive learning. In particular this is in stark contrast to the analysis of prior-independent algorithms where there are simple known learning problems for which no self-terminating algorithm can provide this guarantee for all priors.
286,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Beyond Logarithmic Bounds in Online Learning,"Francesco Orabona, Nicolo Cesa-Bianchi, Claudio Gentile",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/orabona12/orabona12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_orabona12,http://jmlr.csail.mit.edu/proceedings/papers/v22/orabona12.html,We prove logarithmic regret bounds that depend on the loss L_T^* of the competitor rather than on the number T of time steps. In the general online convex optimization setting our bounds hold for any smooth and exp-concave loss (such as the square loss or the logistic loss). This bridges the gap between the O(ln T) regret exhibited by exp-concave losses and the O(sqrt(L_T^*)) regret exhibited by smooth losses. We also show that these bounds are tight for specific losses thus they cannot be improved in general. For online regression with square loss our analysis can be used to derive a sparse randomized variant of the online Newton step whose expected number of updates scales with the algorithm's loss. For online classification we prove the first logarithmic mistake bounds that do not rely on prior knowledge of a bound on the competitor's norm.
287,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Concentration in unbounded metric spaces and algorithmic stability,Aryeh Kontorovich,none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovicha14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovicha14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kontorovicha14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovicha14.html,"We prove an extension of McDiarmidês inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the subgaussian diameter , which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogiês method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. This yields a novel risk bound for some regularized metric regression algorithms. We give two extensions of the basic concentration result. The first enables one to replace the independence assumption by appropriate strong mixing. The second generalizes the subgaussian technique to other Orlicz norms."
288,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Tight Bounds on Proper Equivalence Query Learning of DNF,"Lisa Hellerstein, Devorah Kletenik, Linda Sellie and Rocco Servedio",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/hellerstein12/hellerstein12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_hellerstein12,http://jmlr.csail.mit.edu/proceedings/papers/v23/hellerstein12.html,"We prove a new structural lemma for partial Boolean functions f , which we call the seed lemma for DNF . Using the lemma, we give the first subexponential algorithm for proper learning of poly( n )-term DNF in Angluin's Equivalence Query (EQ) model. The algorithm has time and query complexity 2 (êí n ) , which is optimal. We also give a new result on certificates for DNF-size, a simple algorithm for properly PAC-learning DNF, and new results on EQ-learning log n -term DNF and decision trees."
289,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Efficient and Exact MAP-MRF Inference using Branch and Bound,"Min Sun, Murali Telaprolu, Honglak Lee, Silvio Savarese",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sun12/sun12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sun12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sun12.html,We propose two novel Branch-and-Bound (BB) methods to efficiently solve exact MAP-MRF inference on problems with a large number of states (per variable) H. By organizing the data in a suitable structure the time complexity of our best method for evaluating the bound at each branch is reduced from O(H^2) to O(H). This permits searching for the MAP solution in O(BH+Q) instead of O(BH^2) (without using the data structure) where B is the number of branches and Q is the one-time cost to build the data structure which is proportional to H^2. Our analysis on synthetic data shows that given a limited time budget our method solves problems that are characterized by a much larger number of states when compared to state-of-the-art exact inference algorithms (e.g. Marinescu and Dechter (2007); Sontag et al. (2008)) and other baseline BB methods. We further show that our method is well suited for computer vision and computational biology problems where the state space (per variable) is large. In particular our approach is an order of magnitude faster on average than Sontag et al.'s Cluster Pursuit~(CP) on human pose estimation problems. Moreover given a time budget of up to 20 minutes our method consistently solves more protein design problems than CP does. Finally we successfully explore different branching strategies and ways to utilize domain knowledge of the problem to significantly reduce the empirical inference time.
290,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Improved Loss Bounds For Multiple Kernel Learning,"Zakria Hussain, John Shawe_Taylor","15:370-377, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/hussain11a/hussain11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_hussain11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/hussain11a.html,We propose two new generalization error bounds for multiple kernel learning (MKL). First using the bound of Srebro and Ben-David (2006) as a starting point we derive a new version which uses a simple counting argument for the choice of kernels in order to generate a tighter bound when 1-norm regularization (sparsity) is imposed in the kernel learning problem. The second bound is a Rademacher complexity bound which is additive in the (logarithmic) kernel complexity and margin term. This dependency is superior to all previously published Rademacher bounds for learning a convex combination of kernels including the recent bound of Cortes et al. (2010) which exhibits a multiplicative interaction. We illustrate the tightness of our bounds with simulations.
291,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Sparse Reinforcement Learning via Convex Optimization,"Zhiwei Qin, Weichang Li, Firdaus Janoos",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/qin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/qin14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_qin14,http://jmlr.csail.mit.edu/proceedings/papers/v32/qin14.html,"We propose two new algorithms for the sparse reinforcement learning problem based on different formulations. The first algorithm is an off-line method based on the alternating direction method of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual. The second algorithm is an online stochastic approximation algorithm that employs the regularized dual averaging technique, using the Lagrangian formulation. The convergence of both algorithms are established. We demonstrate the performance of these algorithms through two classical examples."
292,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning Bayesian Network Structure using LP Relaxations,"Tommi Jaakkola, David Sontag, Amir Globerson, Marina Meila","9:358-365, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/jaakkola10a/jaakkola10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_jaakkola10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/jaakkola10a.html,We propose to solve the combinatorial problem of finding the highest scoring Bayesian network structure from data. This structure learning problem can be viewed as an inference problem where the variables specify the choice of parents for each node in the graph. The key combinatorial difficulty arises from the global constraint that the graph structure has to be acyclic. We cast the structure learning problem as a linear program over the polytope defined by valid acyclic structures. In relaxing this problem we maintain an outer bound approximation to the polytope and iteratively tighten it by searching over a new class of valid constraints. If an integral solution is found it is guaranteed to be the optimal Bayesian network. When the relaxation is not tight the fast dual algorithms we develop remain useful in combination with a branch and bound method. Empirical results suggest that the method is competitive or faster than alternative exact methods based on dynamic programming.
293,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Information Theoretical Clustering via Semidefinite Programming,"Meihong Wang, Fei Sha","15:761-769, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11b/wang11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_wang11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11b.html,We propose techniques of convex optimization for information theoretical clustering. The clustering objective is to maximize the mutual information between data points and cluster assignments. We formulate this problem first as an instance of MAX K CUT on weighted graphs. We then apply the technique of semidefinite programming (SDP) relaxation to obtain a convex SDP problem. We show how the solution of the SDP problem can be further improved with a low-rank refinement heuristic. The low-rank solution reveals more clearly the cluster structure of the data. Empirical studies on several datasets demonstrate the effectiveness of our approach. In particular the approach outperforms several other clustering algorithms when compared on standard evaluation metrics.
294,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning,"Gang Niu, Wittawat Jitkrittum, Bo Dai, Hirotaka Hachiya, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/niu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/niu13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_niu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/niu13.html,"We propose squared-loss mutual information regularization (SMIR) for multi-class probabilistic classification, following the information maximization principle. SMIR is convex under mild conditions and thus improves the nonconvexity of mutual information regularization. It offers all of the following four abilities to semi-supervised algorithms: Analytical solution, out-of-sample/multi-class classification, and probabilistic output. Furthermore, novel generalization error bounds are derived. Experiments show SMIR compares favorably with state-of-the-art methods."
295,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Jointly Informative Feature Selection,"Leonidas Lefakis, Francois Fleuret","JMLR W&CP 33 :567-575, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/lefakis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_lefakis14,http://jmlr.csail.mit.edu/proceedings/papers/v33/lefakis14.html,"We propose several novel criteria for the selection of groups of jointly informative continuous features in the context of classification. Our approach is based on combining a Gaussian modeling of the feature responses, with derived upper bounds on their mutual information with the class label and their joint entropy. We further propose specific algorithmic implementations of these criteria which reduce the computational complexity of the algorithms by up to two-orders of magnitude, making these strategies tractable in practice. Experiments on multiple computer-vision data-bases, and using several types of classifiers, show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy."
296,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,On the Estimation of alpha-Divergences,"Barnabas Poczos, Jeff Schneider","15:609-617, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/poczos11a/poczos11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_poczos11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/poczos11a.html,We propose new nonparametric consistent Renyi-alpha and Tsallis-alpha divergence estimators for continuous distributions. Given two independent and identically distributed samples a `brute force' approach would be simply to estimate the underlying densities and plug these densities into the corresponding formulas. However it is not our goal to consistently estimate these possibly high dimensional densities and our algorithm avoids estimating them. We will use simple k-nearest-neighbor distance (k-NN) based statistics and interestingly enough we will still be able to prove that the proposed divergence estimators are consistent under certain conditions. We will also show how to use them for mutual information estimation and demonstrate their efficiency by some numerical experiments.
297,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Nested Sequential Monte Carlo Methods,"Christian Naesseth, Fredrik Lindsten, Thomas Schon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/naesseth15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_naesseth15,http://jmlr.csail.mit.edu/proceedings/papers/v37/naesseth15.html,"We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1000."
298,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Dimensionality Reduction for High-Dimensional Network Estimation,"Safiye Celik, Benjamin Logsdon, Su-In Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/celik14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_celik14,http://jmlr.csail.mit.edu/proceedings/papers/v32/celik14.html,"We propose module graphical lasso (MGL), an aggressive dimensionality reduction and network estimation technique for a high-dimensional Gaussian graphical model (GGM). MGL achieves scalability, interpretability and robustness by exploiting the modularity property of many real-world networks. Variables are organized into tightly coupled modules and a graph structure is estimated to determine the conditional independencies among modules. MGL iteratively learns the module assignment of variables, the latent variables, each corresponding to a module, and the parameters of the GGM of the latent variables. In synthetic data experiments, MGL outperforms the standard graphical lasso and three other methods that incorporate latent variables into GGMs. When applied to gene expression data from ovarian cancer, MGL outperforms standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients. The learned modules and their dependencies provide novel insights into cancer biology as well as identifying possible novel drug targets."
299,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Block-sparse Solutions using Kernel Block RIP and its Application to Group Lasso,"Rahul Garg, Rohit Khandekar","15:296-304, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/garg11a/garg11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_garg11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/garg11a.html,We propose Kernel Block Restricted Isometry Property (KB-RIP) as a generalization of the well-studied RIP and prove a variety of results. First we present a ``sum-of-norms''-minimization based formulation of the sparse recovery problem and prove that under certain conditions on KB-RIP it recovers the optimal sparse solution exactly. The Group Lasso formulation widely used as a good heuristic arises naturally from the Lagrangian relaxation of our formulation. Second we present an efficient combinatorial algorithm for provable sparse recovery under similar assumptions on KB-RIP. As a side product this result improves the previous best assumptions on RIP under which a combinatorial algorithm was known. Finally we provide numerical evidence to illustrate that not only are our sum-of-norms-minimization formulation and combinatorial algorithm significantly faster than Lasso they also outperforms Lasso in terms of recovery.
300,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Deeply-Supervised Nets,"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lee15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15a.html,"We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures: (1) transparency in the effect intermediate layers have on overall classification; (2) discriminativeness and robustness of learned features, especially in early layers; (3) training effectiveness in the face of –vanishing” gradients. To combat these issues, we introduce –companion” objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN."
301,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Low-Rank Matrix Recovery from Row-and-Column Affine Measurements,"Or Zuk, Avishai Wagner",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zuk15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/zuk15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zuk15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zuk15.html,"We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix \(X\) . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition ( \(SVD\) ) and least-squares ( \(LS\) ), which we term alg. We prove that (a simplified version of) our algorithm can recover \(X\) exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction."
302,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations,"Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/balasubramanian13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/balasubramanian13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_balasubramanian13,http://jmlr.csail.mit.edu/proceedings/papers/v28/balasubramanian13.html,"We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semisupervised sparse coding."
303,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Optimum Statistical Estimation with Strategic Data Sources,"Yang Cai, Constantinos Daskalakis, Christos Papadimitriou",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cai15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Cai15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cai15.html,"We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high quality data is provided at low cost, in the sense that the weighted sum of payments and estimation error is minimized. The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generalizes to several objectives, including minimizing estimation error subject to budget constraints. Besides our concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples."
304,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Learning Attribute-weighted Voter Model over Social Networks,"Y. Yamagishi, K. Saito, K. Ohara, M. Kimura & H. Motoda","20:263_280, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/yamagishi11/yamagishi11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_yamagishi11,http://jmlr.csail.mit.edu/proceedings/papers/v20/yamagishi11.html,We propose an opinion formation model an extension of the voter model that incorporates the strength of each node which is modeled as a function of the node attributes. Then we address the problem of estimating parameter values for these attributes that appear in the function from the observed opinion formation data and solve this by maximizing the likelihood using an iterative parameter value updating algorithm which is eçcient and is guaranteed to converge. We show that the proposed algorithm can correctly learn the dependency in our experiments on four real world networks for which we used the assumed attribute dependency. We further show that the in'uence degree of each node based on the extended voter model is substantially di_erent from that obtained assuming a uniform strength (a naive model for which the in'uence degree is known to be proportional to the node degree) and is more sensitive to the node strength than the node degree even for a moderate value of the node strength.   Page last modified on Sun Nov 6 15:43:53 2011.
305,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network,"Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hong15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hong15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hong15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hong15.html,"We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by back-projecting CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map reveals spatial configuration of target effectively, it improves target localization accuracy and enables us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms."
306,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Online Learning of Multiple Tasks and Their Relationships,"Avishek Saha, Piyush Rai, Hal Daum_ III, Suresh Venkatasubramanian","15:643-651, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11b/saha11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_saha11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11b.html,We propose an Online MultiTask Learning (OMTL) framework which simultaneously learns the task weight vectors as well as the task relatedness adaptively from the data. Our work is in contrast with prior work on online multitask learning which assumes fixed task relatedness a priori. Furthermore whereas prior work in such settings assume only positively correlated tasks our framework can capture negative correlations as well. Our proposed framework learns the task relationship matrix by framing the objective function as a Bregman divergence minimization problem for positive definite matrices. Subsequently we exploit this adaptively learned task-relationship matrix to select the most informative samples in an online multitask active learning setting. Experimental results on a number of real-world datasets and comparisons with numerous baselines establish the efficacy of our proposed approach.
307,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,Online Clustering with Experts,"Anna Choromanska, Claire Monteleoni","26:1-18, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/choromanska12a/choromanska12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,40726,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_choromanska12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/choromanska12a.html,"We propose an online clustering algorithm that manages the exploration/exploitation tradeoff using an adaptive weighting over batch clustering algorithms. We extend algorithms for online supervised learning, with access to expert predictors, to the unsupervised learning setting. Instead of computing prediction errors in order to re-weight the experts, the algorithm computes an approximation to the current value of the k -means objective obtained by each expert. When the experts are batch clustering algorithms with b -approximation guarantees with respect to the k -means objective (for example, the k -means++ or k -means # algorithms), applied to a sliding window of the data stream, our algorithm achieves an approximation guarantee with respect to the k -means objective. The form of this online clustering approximation guarantee is novel, and extends an evaluation framework proposed by Dasgupta as an analog to regret. Our algorithm tracks the best clustering algorithm on real and simulated data sets."
308,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Exact Rule Learning via Boolean Compressed Sensing,"Dmitry Malioutov, Kush Varshney",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/malioutov13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_malioutov13,http://jmlr.csail.mit.edu/proceedings/papers/v28/malioutov13.html,"We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean •sensingê matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting. We show competitive classification accuracy using the proposed approach."
309,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Locally Minimax Optimal Predictive Modeling with Bayesian Networks,"Tomi Silander, Teemu Roos, Petri Myllymaki","5:504-511, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/silander09a/silander09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_silander09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/silander09a.html,We propose an information-theoretic approach for predictive modeling with Bayesian networks. Our approach is based on the minimax optimal Normalized Maximum Likelihood (NML) distribution motivated by the MDL principle. In particular we present a parameter learning method which together with a previously introduced NML-based model selection criterion provides a way to construct highly predictive Bayesian network models from data. The method is parameter-free and robust unlike the currently popular Bayesian marginal likelihood approach which has been shown to be sensitive to the choice of prior hyperparameters. Empirical tests show that the proposed method compares favorably with the Bayesian approach in predictive tasks.
310,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Hybrid Pareto Model for Conditional Density Estimation of Asymmetric Fat-Tail Data,"Julie Carreau, Yoshua Bengio","2:51-58, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/carreau07a/carreau07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_carreau07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/carreau07a.html,We propose an estimator for the conditional density p(Y |X) that can adapt for asymmetric heavy tails which might depend on X. Such estimators have important applications in nance and insurance. We draw from Extreme Value Theory the tools to build a hybrid unimodal density having a parameter controlling the heaviness of the upper tail. This hybrid is a Gaussian whose upper tail has been replaced by a generalized Pareto tail. We use this hybrid in a multi-modal mixture in order to obtain a nonparametric density estimator that can easily adapt for heavy tailed data. To obtain a conditional density estimator the parameters of the mixture estimator can be seen as functions of X and these functions learned. We show experimentally that this approach better models the conditional density in terms of likelihood than compared competing algorithms : conditional mixture models with other types of components and multivariate nonparametric models.
311,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Optimizing Neural Networks with Kronecker-factored Approximate Curvature,"James Martens, Roger Grosse",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/martens15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/martens15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_martens15,http://jmlr.csail.mit.edu/proceedings/papers/v37/martens15.html,"We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural networkês Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FACês approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix."
312,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Tighter Relaxations for MAP-MRF Inference: A Local Primal-Dual Gap based Separation Algorithm,"Dhruv Batra, Sebastian Nowozin, Pushmeet Kohli","15:146-154, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/batra11a/batra11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_batra11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/batra11a.html,We propose an efficient and adaptive method for MAP-MRF inference that provides increasingly tighter upper and lower bounds on the optimal objective. Similar to Sontag et al. (2008) our method starts by solving the first-order LOCAL(G) linear programming relaxation. This is followed by an adaptive tightening of the relaxation where we incrementally add higher-order interactions to enforce proper marginalization over groups of variables. Computing the best interaction to add is an NP-hard problem. We show good solutions to this problem can be readily obtained from ñlocal primal-dual gapsî given the current primal solution and a dual reparameterization vector. This is not only extremely efficient but in contrast to previous approaches also allows us to search over prohibitively large sets of candidate interactions to add. We demonstrate the superiority of our approach on MAP-MRF inference problems encountered in computer vision.
313,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions,"Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/trandinh13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/trandinh13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_trandinh13,http://jmlr.csail.mit.edu/proceedings/papers/v28/trandinh13.html,"We propose an algorithmic framework for convex minimization problems of composite functions with two terms: a self-concordant part and a possibly nonsmooth regularization part. Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic step-size selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations."
314,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Model-Free Monte Carlo-like Policy Evaluation,"Raphael Fonteneau, Susan Murphy, Louis Wehenkel, Damien Ernst","9:217-224, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/fonteneau10a/fonteneau10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_fonteneau10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/fonteneau10a.html,We propose an algorithm for estimating the finite-horizon expected return of a closed loop control policy from an a priori given (off-policy) sample of one-step transitions. It averages cumulated rewards along a set of ``broken trajectories'' made of one-step transitions selected from the sample on the basis of the control policy. Under some Lipschitz continuity assumptions on the system dynamics reward function and control policy we provide bounds on the bias and variance of the estimator that depend only on the Lipschitz constants on the number of broken trajectories used in the estimator and on the sparsity of the sample of one-step transitions.
315,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Acceleration technique for boosting classi_cation and its application to face detection,"M. Kawakita, R. Izumi, J. Takeuchi, Y. Hu, T. Takamori & H. Kameyama","20:335_349, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/kawakita11/kawakita11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_kawakita11,http://jmlr.csail.mit.edu/proceedings/papers/v20/kawakita11.html,We propose an acceleration technique for boosting classi_cation without any loss of classi_cation accuracy and apply it to a face detection task. In classi_cation task much e_ort has been spent on improving the classi_cation accuracy and the computational cost of training. In addition to them the computational cost of classi_cation itself can be critical in several applications including face detection. In face detection a celebrating work by Viola and Jones (2001) developed a signi_cantly fast face detector achieving a competitive accuracy with all preceding face detectors. In their algorithm the cascade structure of boosting classi_er plays an important role. In this paper we propose an acceleration technique for boosting classi_er. The key idea of our proposal is the fact that one can determine the sign of discriminant function before all weak learners are evaluated in general. An advantage is that our algorithm has no loss in classi_cation accuracy. Another advantage is that our proposal is a unsupervised learning so that it can treat a covariate shift situation. We also apply our proposal to each cascaded boosting classi_er in Viola and Jones type face detector. As a result our proposal succeeds in reducing the classi_cation cost by 20%.   Page last modified on Sun Nov 6 15:44:19 2011.
316,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Prediction by random-walk perturbation,"Luc Devroye, Gˆbor Lugosi, Gergely Neu",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Devroye13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Devroye13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Devroye13.html,"We propose a version of the follow-the-perturbed-leader online prediction algorithm in which the cumulative losses are perturbed by independent symmetric random walks. The forecaster is shown to achieve an expected regret of the optimal order \(O(\sqrt{n \log N})\) where \(n\) is the time horizon and \(N\) is the number of experts. More importantly, it is shown that the forecaster changes its prediction at most \(O(\sqrt{n \log N})\) times, in expectation. We also extend the analysis to online combinatorial optimization and show that even in this more general setting, the forecaster rarely switches between experts while having a regret of near-optimal order."
317,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Unified Framework for Outlier-Robust PCA-like Algorithms,"Wenzhuo Yang, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yangc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangc15.html,"We propose a unified framework for making a wide range of PCA-like algorithms _ including the standard PCA, sparse PCA and non-negative sparse PCA, etc. _ robust when facing a constant fraction of arbitrarily corrupted outliers. Our theoretic analysis establishes solid performance guarantees of the proposed framework: its estimation error is upper bounded by a term depending on the intrinsic parameters of the data model, the selected PCA-like algorithm and the fraction of outliers. Comprehensive experiments on synthetic and real-world datasets demonstrate that the outlier-robust PCA-like algorithms derived from our framework have outstanding performance."
318,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Topic Modeling Approach to Ranking,"Weicong Ding, Prakash Ishwar, Venkatesh Saligrama",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/ding15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/ding15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_ding15,http://jmlr.csail.mit.edu/proceedings/papers/v38/ding15.html,We propose a topic modeling approach to the prediction of preferences in pairwise comparisons. We develop a new generative model for pairwise comparisons that accounts for multiple shared latent rankings that are prevalent in a population of users. This new model also captures inconsistent user behavior in a natural way. We show how the estimation of latent rankings in the new generative model can be formally reduced to the estimation of topics in a statistically equivalent topic modeling problem. We leverage recent advances in the topic modeling literature to develop an algorithm that can learn shared latent rankings with provable consistency as well as sample and computational complexity guarantees. We demonstrate that the new approach is empirically competitive with the current state-of-the-art approaches in predicting preferences on some semi-synthetic and real world datasets.
319,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A Competitive Test for Uniformity of Monotone Distributions,"Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Suresh",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/acharya13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_acharya13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/acharya13a.html,"We propose a test that takes random samples drawn from a monotone distribution and decides whether or not the distribution is uniform. The test is nearly optimal in that it uses at most \(O(n\sqrt{\log n})\) samples, where \(n\) is the number of samples that a genie who knew all but one bit about the underlying distribution would need for the same task. Furthermore, we show that any such test would require \(\Omega(n\sqrt{\log n})\) samples for some distributions."
320,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A Weighted Multi-Sequence Markov Model For Brain Lesion Segmentation,"Florence Forbes, Senan Doyle, Daniel Garcia_Lorenzo, Christian Barillot, Michel Dojat","9:225-232, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/forbes10a/forbes10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_forbes10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/forbes10a.html,We propose a technique for fusing the output of multiple Magnetic Resonance (MR) sequences to robustly and accurately segment brain lesions. It is based on an augmented multi-sequence Hidden Markov model that includes additional weight variables to account for the relative importance and control the impact of each sequence. The augmented framework has the advantage of allowing 1) the incorporation of expert knowledge on the a priori relevant information content of each sequence and 2) a weighting scheme which is modified adaptively according to the data and the segmentation task under consideration. The model applied to the detection of multiple sclerosis and stroke lesions shows promising results.
321,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Pursuit-Evasion Without Regret, with an Application to Trading","Lili Dworkin, Michael Kearns, Yuriy Nevmyvaka",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/dworkin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_dworkin14,http://jmlr.csail.mit.edu/proceedings/papers/v32/dworkin14.html,"We propose a state-based variant of the classical online learning problem of tracking the best expert. In our setting, the actions of the algorithm and experts correspond to local moves through a continuous and bounded state space. At each step, Nature chooses payoffs as a function of each playerês current position and action. Our model therefore integrates the problem of prediction with expert advice with the stateful formalisms of reinforcement learning. Traditional no-regret learning approaches no longer apply, but we propose a simple algorithm that provably achieves no-regret when the state space is any convex Euclidean region. Our algorithm combines techniques from online learning with results from the literature on pursuit-evasion games. We describe a quantitative trading application in which the convex region captures inventory risk constraints, and local moves limit market impact. Using historical market data, we show experimentally that our algorithm has a strong advantage over classic no-regret approaches."
322,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Consistency and Rates for Clustering with DBSCAN,"Bharath Sriperumbudur, Ingo Steinwart",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sriperumbudur12/sriperumbudur12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sriperumbudur12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sriperumbudur12.html,We propose a simple and efficient modification of the popular DBSCAN clustering algorithm. This modification is able to detect the most interesting vertical threshold level in an automated data-driven way. We establish both consistency and optimal learning rates for this modification.
323,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Doubly Stochastic Variational Bayes for non-Conjugate Inference,"Michalis Titsias, Miguel Lˆzaro-Gredilla",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/titsias14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/titsias14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_titsias14,http://jmlr.csail.mit.edu/proceedings/papers/v32/titsias14.html,We propose a simple and effective variational inference algorithm based on stochastic optimisation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference problems such as variable selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.
324,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Fast Allocation of Gaussian Process Experts,"Trung Nguyen, Edwin Bonilla",none,http://jmlr.org/proceedings/papers/v32/nguyena14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_nguyena14,http://jmlr.csail.mit.edu/proceedings/papers/v32/nguyena14.html,"We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using \(K\) experts, our method can run \(K^2\) times faster and use \(K^2\) times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity straightforwardly. Our experiments show that on medium-sized datasets (of around \(10^4\) training points) it trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset of \(10^5\) training points, our method significantly outperforms six competitive baselines while requiring only a few hours of training."
325,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Loss-Proportional Subsampling for Subsequent ERM,"Paul Mineiro, Nikos Karampatziakis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/mineiro13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_mineiro13,http://jmlr.csail.mit.edu/proceedings/papers/v28/mineiro13.html,"We propose a sampling scheme suitable for reducing a data set prior to selecting a hypothesis with minimum empirical risk. The sampling only considers a subset of the ultimate (unknown) hypothesis set, but can nonetheless guarantee that the final excess risk will compare favorably with utilizing the entire original data set. We demonstrate the practical benefits of our approach on a large dataset which we subsample and subsequently fit with boosted trees."
326,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Block-Coordinate Frank-Wolfe Optimization for Structural SVMs,"Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lacoste-julien13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/lacoste-julien13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lacoste-julien13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lacoste-julien13.html,"We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers."
327,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Probabilistic Matrix Factorization with Non-random Missing Data,"Jose Miguel Hernandez-Lobato, Neil Houlsby, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatob14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatob14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hernandez-lobatob14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatob14.html,"We propose a probabilistic matrix factorization model for collaborative filtering that learns from data that is missing not at random(MNAR). Matrix factorization models exhibit state-of-the-art predictive performance in collaborative filtering. However, these models usually assume that the data is missing at random (MAR), and this is rarely the case. For example, the data is not MAR if users rate items they like more than ones they dislike. When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over state-of-the-art methods when predicting the ratings and when modeling the data observation process. We present the first viable MF model for MNAR data. Our results are promising and we expect that further research on NMAR models will yield large gains in collaborative filtering."
328,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Predictable Dual-View Hashing,"Mohammad Rastegari, Jonghyun Choi, Shobeir Fakhraei, Daume Hal, Larry Davis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/rastegari13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_rastegari13,http://jmlr.csail.mit.edu/proceedings/papers/v28/rastegari13.html,"We propose a Predictable Dual-View Hashing (PDH) algorithm which embeds proximity of data samples in the original spaces. We create a cross-view hamming space with the ability to compare information from previously incomparable domains with a notion of •predictabilityê. By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms."
329,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,PAC-Bayesian Theory for Transductive Learning,"Luc B_gin, Pascal Germain, François Laviolette, Jean-Francis Roy","JMLR W&CP 33 :105-113, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/begin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/begin14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_begin14,http://jmlr.csail.mit.edu/proceedings/papers/v33/begin14.html,"We propose a PAC-Bayesian analysis of the transductive learning setting, introduced by Vapnik [2008], by proposing a family of new bounds on the generalization error. Some of them are derived from their counterpart in the inductive setting, and others are new. We also compare their behavior."
330,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,SMERED: A Bayesian Approach to Graphical Record Linkage and De-duplication,"Rebecca Steorts, Rob Hall, Stephen Fienberg","JMLR W&CP 33 :922-930, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/steorts14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/steorts14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_steorts14,http://jmlr.csail.mit.edu/proceedings/papers/v33/steorts14.html,"We propose a novel unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation is to represent the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible new representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate k-way posterior probabilities of matches across records, and propagate the uncertainty of record linkage into later analyses. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously proposed methods of record linkage, despite the high dimensional parameter space. We assess our results on real and simulated data."
331,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Markov Logic Mixtures of Gaussian Processes: Towards Machines Reading Regression Data,"Martin Schiegg, Marion Neumann, Kristian Kersting",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/schiegg12/schiegg12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_schiegg12,http://jmlr.csail.mit.edu/proceedings/papers/v22/schiegg12.html,"We propose a novel mixtures of Gaussian processes model in which the gating function is interconnected with a probabilistic logical model in our case Markov logic networks. In this way the resulting mixed graphical model called Markov logic mixtures of Gaussian processes (MLxGP) solves joint Bayesian non-parametric regression and probabilistic relational inference tasks. In turn MLxGP facilitates novel interesting tasks such as regression based on logical constraints or drawing probabilistic logical conclusions about regression data thus putting ""machines reading regression data"" in reach."
332,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Demystifying Information-Theoretic Clustering,"Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo",none,http://jmlr.org/proceedings/papers/v32/steeg14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/steeg14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_steeg14,http://jmlr.csail.mit.edu/proceedings/papers/v32/steeg14.html,"We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions. Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data."
333,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Exclusive Lasso for Multi-task Feature Selection,"Yang Zhou, Rong Jin, Steven Chu_Hong Hoi","9:988-995, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhou10a/zhou10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhou10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhou10a.html,We propose a novel group regularization which we call exclusive lasso. Unlike the group lasso regularizer that assumes co-varying variables in groups the proposed exclusive lasso regularizer models the scenario when variables in the same group compete with each other. Analysis is presented to illustrate the properties of the proposed regularizer. We present a framework of kernel-based multi-task feature selection algorithm based on the proposed exclusive lasso regularizer. An efficient algorithm is derived to solve the related optimization problem. Experiments with document categorization show that our approach outperforms state-of-the-art algorithms for multi-task feature selection.
334,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,An Improved 1-norm SVM for Simultaneous Classification and Variable Selection,Hui Zou,"2:675-681, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/zou07a/zou07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_zou07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/zou07a.html,We propose a novel extension of the 1-norm support vector machine (SVM) for simultaneous feature selection and classification. The new algorithm penalizes the empirical hinge loss by the adaptively weighted 1-norm penalty in which the weights are computed by the 2-norm SVM. Hence the new algorithm is called the hybrid SVM. Simulation and real data examples show that the hybrid SVM not only often improves upon the 1-norm SVM in terms of classification accuracy but also enjoys better feature selection performance.
335,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Sample Distillation for Object Detection and Image Classification,"Olivier Canevet, Leonidas Lefakis, Francois Fleuret",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/canevet14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_canevet14b,http://jmlr.csail.mit.edu/proceedings/papers/v39/canevet14b.html,"We propose a novel approach to efficiently select informative samples for large-scale learning. Instead of directly feeding a learning algorithm with a very large amount of samples, as it is usually done to reach state-of-the-art performance, we have developed a –distillation” procedure to recursively reduce the size of an initial training set using a criterion that ensures the maximization of the information content of the selected sub-set. We demonstrate the performance of this procedure for two different computer vision problems. First, we show that distillation can be used to improve the traditional bootstrapping approach to object detection. Second, we apply distillation to a classification problem with artificial distortions. We show that in both cases, using the result of a distillation process instead of a random sub-set taken uniformly in the original sample set improves performance significantly."
336,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sparse Submodular Probabilistic PCA,"Rajiv Khanna, Joydeep Ghosh, Russell Poldrack, Oluwasanmi Koyejo",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/khanna15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/khanna15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_khanna15,http://jmlr.csail.mit.edu/proceedings/papers/v38/khanna15.html,"We propose a novel approach for sparse probabilistic principal component analysis, that combines a low rank representation for the latent factors and loadings with a novel sparse variational inference approach for estimating distributions of latent variables subject to sparse support constraints. Inference and parameter estimation for the resulting model is achieved via expectation maximization with a novel variational inference method for the E-step that induces sparsity. We show that this inference problem can be reduced to discrete optimal support selection. The discrete optimization is submodular, hence, greedy selection is guaranteed to achieve 1-1/e fraction of the optimal. Empirical studies indicate effectiveness of the proposed approach for the recovery of a parsimonious decomposition as compared to established baseline methods. We also evaluate our method against state-of-the-art methods on high dimensional fMRI data, and show that the method performs as good as or better than other methods."
337,1,http://jmlr.csail.mit.edu/proceedings/papers/v1/,Salient Point and Scale Detection by Minimum Likelihood,"Kim S. Pedersen, Marco Loog, Pieter van Dorst","1:59-72, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v1/pedersen07a/pedersen07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v1/,,11th March 2007,"June 12-13, 2006",Gaussian Processes in Practice,Gaussian Processes in Practice,"Bletchley Park, Bletchley, U.K.","Neil Lawrence, Anton Schwaighofer and Joaquin QuiÕ±onero Candela",v1_pedersen07a,http://jmlr.csail.mit.edu/proceedings/papers/v1/pedersen07a.html,We propose a novel approach for detection of salient image points and estimation of their intrinsic scales based on the fractional Brownian image model. Under this model images are realisations of a Gaussian random process on the plane. We define salient points as points that have a locally unique image structure. Such points are usually sparsely distributed in images and carry important information about the image content. Locality is defined in terms of the measurement scale of the filters used to describe the image structure. Here we use partial derivatives of the image function defined using linear scale space theory. We propose to detect salient points and their intrinsic scale by detecting points in scale-space that locally minimise the likelihood under the model.
338,1,http://jmlr.csail.mit.edu/proceedings/papers/v1/,Learning RoboCup-Keepaway with Kernels,"Tobias Jung, Daniel Polani","1:33-57, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v1/jung07a/jung07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v1/,,11th March 2007,"June 12-13, 2006",Gaussian Processes in Practice,Gaussian Processes in Practice,"Bletchley Park, Bletchley, U.K.","Neil Lawrence, Anton Schwaighofer and Joaquin QuiÕ±onero Candela",v1_jung07a,http://jmlr.csail.mit.edu/proceedings/papers/v1/jung07a.html,We propose a novel approach for detection of salient image points and estimation of their intrinsic scales based on the fractional Brownian image model. Under this model images are realisations of a Gaussian random process on the plane. We define salient points as points that have a locally unique image structure. Such points are usually sparsely distributed in images and carry important information about the image content. Locality is defined in terms of the measurement scale of the filters used to describe the image structure. Here we use partial derivatives of the image function defined using linear scale space theory. We propose to detect salient points and their intrinsic scale by detecting points in scale-space that locally minimise the likelihood under the model.
339,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach,"Mladen Kolar, Eric Xing","9:413-420, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kolar10a/kolar10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kolar10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kolar10a.html,We propose a novel application of the Simultaneous Orthogonal Matching Pursuit (S-OMP) procedure to perform variable selection in ultra-high dimensional multiple output regression problems which is the first attempt to utilize multiple outputs to perform fast removal of the irrelevant variables. As our main theoretical contribution we show that the S-OMP can be used to reduce an ultra-high number of variables to below the sample size without losing relevant variables. We also provide formal evidence that the modified Bayesian information criterion (BIC) can be used to efficiently select the number of iterations in the S-OMP. Once the number of variables has been reduced to a manageable size we show that a more computationally demanding procedure can be used to identify the relevant variables for each of the regression outputs. We further provide evidence on the benefit of variable selection using the regression outputs jointly as opposed to performing variable selection for each output separately. The finite sample performance of the S-OMP has been demonstrated on extensive simulation studies.
340,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Fast Convergent Algorithms for Expectation Propagation Approximate Bayesian Inference,"Matthias Seeger, Hannes Nickisch","15:652-660, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/seeger11a/seeger11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_seeger11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/seeger11a.html,We propose a novel algorithm to solve the expectation propagation relaxation of Bayesian inference for continuous-variable graphical models. In contrast to most previous algorithms our method is provably convergent. By marrying convergent EP ideas from (Opper&Winther 2005) with covariance decoupling techniques (Wipf&Nagarajan 2008; Nickisch&Seeger 2009) it runs at least an order of magnitude faster than the most common EP solver.
341,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Adaptive Metropolis with Online Relabeling,"Remi Bardenet, Olivier Cappe, Gersende Fort, Balazs Kegl",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bardenet12/bardenet12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bardenet12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bardenet12.html,We propose a novel adaptive MCMC algorithm named AMOR (Adaptive Metropolis with Online Relabeling) for efficiently simulating from permutation-invariant targets occurring in for example Bayesian analysis of mixture models. An important feature of the algorithm is to tie the adaptation of the proposal distribution to the choice of a particular restriction of the target to a domain where label switching cannot occur. The algorithm relies on a stochastic approximation procedure for which we exhibit a Lyapunov function that formally defines the criterion used for selecting the relabeling rule. This criterion reveals an interesting connection with the problem of optimal quantifier design in vector quantization which was only implicit in previous works on the label switching problem. In benchmark examples the algorithm turns out to be fast converging and efficient at selecting meaningful non-trivial relabeling rules to allow accurate parameter inference.
342,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Kernel Belief Propagation,"Le Song, Arthur Gretton, Danny Bickson, Yucheng Low, Carlos Guestrin","15:707-715, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/song11a/song11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_song11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/song11a.html,We propose a nonparametric generalization of belief propagation Kernel Belief Propagation (KBP) for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS) and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution nor must their relations take any particular parametric form. Rather the relations between variables are represented implicitly and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (R^d strings groups) even where explicit parametric models are not known or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments we apply KBP to image denoising depth prediction from still images and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude in some cases) while providing significantly more accurate results.
343,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Learning Efficient Anomaly Detectors from K-NN Graphs,"Jonathan Root, Jing Qian, Venkatesh Saligrama",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/root15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/root15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_root15,http://jmlr.csail.mit.edu/proceedings/papers/v38/root15.html,"We propose a non-parametric anomaly detection algorithm for high dimensional data. We score each datapoint by its average \(K\) -NN distance, and rank them accordingly. We then train limited complexity models to imitate these scores based on the max-margin learning-to-rank framework. A test-point is declared as an anomaly at \(\alpha\) -false alarm level if the predicted score is in the \(\alpha\) -percentile. The resulting anomaly detector is shown to be asymptotically optimal in that for any false alarm rate \(\alpha\) , its decision region converges to the \(\alpha\) -percentile minimum volume level set of the unknown underlying density. In addition, we test both the statistical performance and computational efficiency of our algorithm on a number of synthetic and real-data experiments. Our results demonstrate the superiority of our algorithm over existing \(K\) -NN based anomaly detection algorithms, with significant computational savings."
344,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Neural conditional random fields,"Trinh_Minh_Tri Do, Thierry Artieres","9:177-184, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/do10a/do10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_do10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/do10a.html,We propose a non-linear graphical model for structured prediction. It combines the power of deep neural networks to extract high level features with the graphical framework of Markov networks yielding a powerful and scalable probabilistic model that we apply to signal labeling tasks.
345,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Active Learning as Non-Convex Optimization,"Andrew Guillory, Erick Chastain, Jeff Bilmes","5:201-208, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/guillory09a/guillory09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_guillory09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/guillory09a.html,We propose a new view of active learning algorithms as optimization. We show that many online active learning algorithms can be viewed as stochastic gradient descent on non-convex objective functions. Variations of some of these algorithms and objective functions have been previously proposed without noting this connection. We also point out a connection between the standard min-margin offline active learning algorithm and non-convex losses. Finally we discuss and show empirically how viewing active learning as non-convex loss minimization helps explain two previously observed phenomena: certain active learning algorithms achieve better generalization error than passive learning algorithms on certain data sets and on other data sets many active learning algorithms are prone to local minima.
346,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Dual Coordinate Ascent with Alternating Direction Method of Multipliers,Taiji Suzuki,none,http://jmlr.org/proceedings/papers/v32/suzuki14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/suzuki14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_suzuki14,http://jmlr.csail.mit.edu/proceedings/papers/v32/suzuki14.html,"We propose a new stochastic dual coordinate ascent technique that can be applied to a wide range of regularized learning problems. Our method is based on alternating direction method of multipliers (ADMM) to deal with complex regularization functions such as structured regularizations. Although the original ADMM is a batch method, the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations. Moreover, our method can naturally afford mini-batch update and it gives speed up of convergence. We show that, under mild assumptions, our method converges exponentially. The numerical experiments show that our method actually performs efficiently."
347,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Fast Stochastic Alternating Direction Method of Multipliers,"Wenliang Zhong, James Kwok",none,http://jmlr.org/proceedings/papers/v32/zhong14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhong14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhong14.html,"We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, it improves the convergence rate on convex problems from \(\mO(1/\sqrt{T})\) to \(\mO(1/T)\) , where \(T\) is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms."
348,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Subset Infinite Relational Models,"Katsuhiko Ishiguro, Naonori Ueda, Hiroshi Sawada",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/ishiguro12/ishiguro12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_ishiguro12,http://jmlr.csail.mit.edu/proceedings/papers/v22/ishiguro12.html,We propose a new probabilistic generative model for analyzing sparse and noisy pairwise relational data such as friend-links on SNSs and customer records in online shops. Real-world relational data often include a large portion of non-informative pairwise data entries. Many existing stochastic blockmodels suffer from these irrelevant data entries because of their rather simpler forms of priors. The proposed model newly incorporates a latent variable that explicitly indicates whether each data entry is relevant or not to diminish the bad effects associated with such irrelevant data. Through experimental results using synthetic and real data sets we show that the proposed model can extract clusters with stronger relations among data within the cluster than clusters obtained by the conventional model.
349,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Word Representations with Hierarchical Sparse Coding,"Dani Yogatama, Manaal Faruqui, Chris Dyer, Noah Smith",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yogatama15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yogatama15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yogatama15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yogatama15.html,"We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasksãword similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysisãdemonstrate that the method outperforms or is competitive with state-of-the-art methods."
350,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Estimation of Extreme Values and Associated Level Sets of a Regression Function via Selective Sampling,Stanislav Minsker,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Minsker13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Minsker13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Minsker13.html,"We propose a new method for estimating the locations and the value of an absolute maximum (minimum) of a function from the observations contaminated by random noise. Our goal is to solve the problem under minimal regularity and shape constraints. In particular, we do not assume differentiability of a function nor that its maximum is attained at a single point. We provide tight upper and lower bounds for the performance of proposed estimators. Our method is adaptive with respect to the unknown parameters of the problem over a large class of underlying distributions."
351,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Threshold Influence Model for Allocating Advertising Budgets,"Atsushi Miyauchi, Yuni Iwamasa, Takuro Fukunaga, Naonori Kakimura",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/miyauchi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/miyauchi15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_miyauchi15,http://jmlr.csail.mit.edu/proceedings/papers/v37/miyauchi15.html,"We propose a new influence model for allocating budgets to advertising channels. Our model captures customerês sensitivity to advertisements as a threshold behavior; a customer is expected to be influenced if the influence he receives exceeds his threshold. Over the threshold model, we discuss two optimization problems. The first one is the budget-constrained influence maximization. We propose two greedy algorithms based on different strategies, and analyze the performance when the influence is submodular. We then introduce a new characteristic to measure the cost-effectiveness of a marketing campaign, that is, the proportion of the resulting influence to the cost spent. We design an almost linear-time approximation algorithm to maximize the cost-effectiveness. Furthermore, we design a better-approximation algorithm based on linear programming for a special case. We conduct thorough experiments to confirm that our algorithms outperform baseline algorithms."
352,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Online Feature Selection for Model-based Reinforcement Learning,"Trung Nguyen, Zhuoru Li, Tomi Silander, Tze Yun Leong",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_nguyen13,http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13.html,"We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains."
353,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Deep Learning for Efficient Discriminative Parsing,Ronan Collobert,"15:224-232, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/collobert11a/collobert11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_collobert11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/collobert11a.html,We propose a new fast purely discriminative algorithm for natural language parsing based on a ñdeepî recurrent convolutional graph transformer network (GTN). Assuming a decomposition of a parse tree into a stack of ñlevelsî the network predicts a level of the tree taking into account predictions of previous levels. Using only few basic text features we show similar performance (in F1 score) to existing pure discriminative parsers and existing ñbenchmarkî parsers (like Collins parser probabilistic context-free grammars based) with a huge speed advantage.
354,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Max-Margin Min-Entropy Models,"Kevin Miller, M. Pawan Kumar, Ben Packer, Danny Goodman, Daphne Koller",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/miller12/miller12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_miller12,http://jmlr.csail.mit.edu/proceedings/papers/v22/miller12.html,We propose a new family of latent variable models called max-margin min-entropy (M3E) models which define a distribution over the output and the hidden variables conditioned on the input. Given an input an M3E model predicts the output with the smallest corresponding Renyi entropy of generalized distribution. This is equivalent to minimizing a score that consists of two terms: (i) the negative log-likelihood of the output ensuring that the output has a high probability; and (ii) a measure of uncertainty over the distribution of the hidden variables conditioned on the input and the output ensuring that there is little confusion in the values of the hidden variables. Given a training dataset the parameters of an M3E model are learned by maximizing the margin between the Renyi entropies of the ground-truth output and all other incorrect outputs. Training an M3E can be viewed as minimizing an upper bound on a user-defined loss and includes as a special case the latent support vector machine framework. We demonstrate the efficacy of M3E models on two standard machine learning applications discriminative motif finding and image classification using publicly available datasets.
355,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,DiSCO: Distributed Optimization for Self-Concordant Empirical Loss,"Yuchen Zhang, Xiao Lin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhangb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangb15.html,"We propose a new distributed algorithm for empirical risk minimization in machine learning. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, where the \(n\) data points are i.i.d. sampled and when the regularization parameter scales as \(1/\sqrt{n}\) , we show that the proposed algorithm is communication efficient: the required round of communication does not increase with the sample size \(n\) , and only grows slowly with the number of machines."
356,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning Markov Structure by Maximum Entropy Relaxation,"Jason K. Johnson, Venkat Chandrasekaran, Alan S. Willsky","2:203-210, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/johnson07a/johnson07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_johnson07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/johnson07a.html,We propose a new approach for learning a sparse graphical model approximation to a specified multivariate probability distribution (such as the empirical distribution of sample data). The selection of sparse graph structure arises naturally in our approach through solution of a convex optimization problem which differentiates our method from standard combinatorial approaches. We seek the maximum entropy relaxation (MER) within an exponential family which maximizes entropy subject to constraints that marginal distributions on small subsets of variables are close to the prescribed marginals in relative entropy. To solve MER we present a modified primal-dual interior point method that exploits sparsity of the Fisher information matrix in models defined on chordal graphs. This leads to a tractable scalable approach provided the level of relaxation in MER is sufficient to obtain a thin graph. The merits of our approach are investigated by recovering the structure of some simple graphical models from sample data.
357,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Monochromatic Bi-Clustering,"Sharon Wulff, Ruth Urner, Shai Ben-David",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulff13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulff13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wulff13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulff13.html,"We propose a natural cost function for the bi-clustering task, the monochromatic cost. This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees and formal NP-hardness results for any bi-clustering optimization problem. In addition, we show that our optimization problem can be efficiently solved by deterministic annealing, yielding a promising heuristic for large problem instances."
358,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Data dependent kernels in nearly-linear time,"Guy Lever, Tom Diethe, John Shawe-Taylor",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/lever12/lever12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_lever12,http://jmlr.csail.mit.edu/proceedings/papers/v22/lever12.html,We propose a method to efficiently construct data dependent kernels which can make use of large quantities of (unlabeled) data. Our construction makes an approximation in the standard construction of semi-supervised kernels in Sindhwani et al. (2005). In typical cases these kernels can be computed in nearly-linear time (in the amount of data) improving on the cubic time of the standard construction enabling large scale semi-supervised learning in a variety of contexts. The methods are validated on semi-supervised and unsupervised problems on data sets containing upto 64000 sample points.
359,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Aggregating Ordinal Labels from Crowds by Minimax Conditional Entropy,"Dengyong Zhou, Qiang Liu, John Platt, Christopher Meek",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhouc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhouc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhouc14.html,We propose a method to aggregate noisy ordinal labels collected from a crowd of workers or annotators. Eliciting ordinal labels is important in tasks such as judging web search quality and consumer satisfaction. Our method is motivated by the observation that workers usually have difficulty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We develop the method through minimax conditional entropy subject to constraints which encode this observation. Empirical evaluations on real datasets demonstrate significant improvements over existing methods.
360,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Agnostic Bayesian Learning of Ensembles,"Alexandre Lacoste, Mario Marchand, François Laviolette, Hugo Larochelle",none,http://jmlr.org/proceedings/papers/v32/lacoste14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lacoste14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lacoste14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lacoste14.html,"We propose a method for producing ensembles of predictors based on holdout estimations of their generalization performances. This approach uses a prior directly on the performance of predictors taken from a finite set of candidates and attempts to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was actually generated from any of the predictors in the ensemble. Since the problem of finding the best (as opposed to the true) predictor among a class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning. We also propose a method to address the case where the performance estimate is obtained from k-fold cross validation. While being efficient and easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach is state of the art compared to common baselines such as model selection based on k-fold cross-validation or a linear combination of predictor outputs."
361,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Loop Corrected Belief Propagation,"Joris Mooij, Bastian Wemmenhove, Bert Kappen, Tommaso Rizzo","2:331-338, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/mooij07a/mooij07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_mooij07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/mooij07a.html,We propose a method for improving Belief Propagation (BP) that takes into account the influence of loops in the graphical model. The method is a variation on and generalization of the method recently introduced by Montanari and Rizzo [2005]. It consists of two steps: (i) standard BP is used to calculate cavity distributions for each variable (i.e. probability distributions on the Markov blanket of a variable for a modified graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are combined by a messagepassing algorithm to obtain consistent single node marginals. The method is exact if the graphical model contains a single loop. The complexity of the method is exponential in the size of the Markov blankets. The results are very accurate in general: the error is often several orders of magnitude smaller than that of standard BP as illustrated by numerical experiments.
362,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Regression for sets of polynomial equations,"Franz Kiraly, Paul Von Buenau, Jan Muller, Duncan Blythe, Frank Meinecke, Klaus-Robert Muller",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kiraly12/kiraly12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kiraly12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kiraly12.html,We propose a method called ideal regression for approximating an arbitrary system of polynomial equations by a system of a particular type. Using techniques from approximate computational algebraic geometry we show how we can solve ideal regression directly without resorting to numerical optimization. Ideal regression is useful whenever the solution to a learning problem can be described by a system of polynomial equations. As an example we demonstrate how to formulate Stationary Subspace Analysis (SSA) a source separation problem in terms of ideal regression which also yields a consistent estimator for SSA. We then compare this estimator in simulations with previous optimization-based approaches for SSA.
363,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Low-Rank Stochastic Gradient Descent Methods for Solving Semidefinite Programs,"Jianhui Chen, Tianbao Yang, Shenghuo Zhu","JMLR W&CP 33 :122-130, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/chen14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_chen14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/chen14b.html,"We propose a low-rank stochastic gradient descent (LR-SGD) method for solving a class of semidefinite programming (SDP) problems. LR-SGD has clear computational advantages over the standard SGD peers as its iterative projection step (a SDP problem) can be solved in an efficient manner. Specifically, LR-SGD constructs a low-rank stochastic gradient and computes an optimal solution to the projection step via analyzing the low-rank structure of its stochastic gradient. Moreover, our theoretical analysis shows the universal existence of arbitrary low-rank stochastic gradients which in turn validates the rationale of the LR-SGD method. Since LR-SGD is a SGD based method, it achieves the optimal convergence rates of the standard SGD methods. The presented experimental results demonstrate the efficiency and effectiveness of the LR-SGD method."
364,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Fair Representations,"Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zemel13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zemel13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zemel13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zemel13.html,"We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification."
365,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Ellipsoidal Multiple Instance Learning,"Gabriel Krummenacher, Cheng Soon Ong, Joachim Buhmann",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/krummenacher13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/krummenacher13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_krummenacher13,http://jmlr.csail.mit.edu/proceedings/papers/v28/krummenacher13.html,"We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive. We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches."
366,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Bayesian Generalized Kernel Models,"Zhihua Zhang, Guang Dai, Donghui Wang, Michael Jordan","9:972-979, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10d/zhang10d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhang10d,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10d.html,We propose a fully Bayesian approach for generalized kernel models (GKMs) which are extensions of generalized linear models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman's g-prior on the regression vector of GKMs. This mixture prior allows a fraction of the regression vector to be zero. Thus it serves for sparse modeling and Bayesian computation. For inference we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction.
367,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Large-Margin Classification in Banach Spaces,"Ricky Der, Daniel Lee","2:91-98, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/der07a/der07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_der07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/der07a.html,"We propose a framework for dealing with binary hard-margin classification in Banach spaces centering on the use of a supporting semi-inner-product (s.i.p.) taking the place of an inner-product in Hilbert spaces. The theory of semi-inner-product spaces allows for a geometric Hilbert-like formulation of the problems and we show that a surprising number of results from the Euclidean case can be appropriately generalised. These include the Representer theorem convexity of the associated optimization programs and even for a particular class of Banach spaces a ""kernel trick"" for non-linear classification."
368,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines,"Kostadin Georgiev, Preslav Nakov",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/georgiev13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_georgiev13,http://jmlr.csail.mit.edu/proceedings/papers/v28/georgiev13.html,"We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBM-based approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previously-proposed approaches."
369,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Transfer Learning Method for Automatic Hyperparameter Tuning,"Dani Yogatama, Gideon Mann","JMLR W&CP 33 :1077-1085, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/yogatama14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_yogatama14,http://jmlr.csail.mit.edu/proceedings/papers/v33/yogatama14.html,"We propose a fast and effective algorithm for automatic hyperparameter tuning that can generalize across datasets. Our method is an instance of sequential model-based optimization (SMBO) that transfers information by constructing a common response surface for all datasets, similar to Bardenet et al. (2013). The time complexity of reconstructing the response surface at every SMBO iteration in our method is linear in the number of trials (significantly less than previous work with comparable performance), allowing the method to realistically scale to many more datasets. Specifically, we use deviations from the per-dataset mean as the response values. We empirically show the superiority of our method on a large number of synthetic and real-world datasets for tuning hyperparameters of logistic regression and ensembles of classifiers."
370,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization,"Necdet Aybat, Zi Wang, Garud Iyengar",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/aybat15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/aybat15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_aybat15,http://jmlr.csail.mit.edu/proceedings/papers/v37/aybat15.html,"We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any eps _ 0, an eps-optimal and eps-feasible solution can be computed within O(log(1/eps)) DFAL iterations, which require \(O(\psi_\text{max}^{1.5}/d_\text{min} \cdot 1/\epsilon)\) proximal gradient computations and communications per node in total, where \(\psi_\text{max}\) denotes the largest eigenvalue of the graph Laplacian, and \(d_\text{min}\) is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems."
371,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Robust Generation of Dynamical Patterns in Human Motion by a Deep Belief Nets,"S. Sukhbaatar, T. Makino, K. Aihara & T. Chikayama","20:231_246, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/sukhbaatar11/sukhbaatar11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_sukhbaatar11,http://jmlr.csail.mit.edu/proceedings/papers/v20/sukhbaatar11.html,We propose a Deep Belief Net model for robust motion generation which consists of two layers of Restricted Boltzmann Machines (RBMs). The lower layer has multiple RBMs for encoding real-valued spatial patterns of motion frames into compact representations. The upper layer has one conditional RBM for learning temporal constraints on transitions between those compact representations. This separation of spatial and temporal learning makes it possible to reproduce many attractive dynamical behaviors such as walking by a stable limit cycle a gait transition by bifurcation synchronization of limbs by phase-locking and easy top-down control. We trained the model with human motion capture data and the results of motion generation are reported here.   Page last modified on Sun Nov 6 15:43:39 2011.
372,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Cross-domain recommendation without shared users or items by sharing latent vector distributions,"Tomoharu Iwata, Takeuchi Koh",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/iwata15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_iwata15,http://jmlr.csail.mit.edu/proceedings/papers/v38/iwata15.html,"We propose a cross-domain recommendation method for predicting the ratings of items in different domains, where neither users nor items are shared across domains. The proposed method is based on matrix factorization, which learns a latent vector for each user and each item. Matrix factorization techniques for a single-domain fail in the cross-domain recommendation task because the learned latent vectors are not aligned over different domains. The proposed method assumes that latent vectors in different domains are generated from a common Gaussian distribution with a full covariance matrix. By inferring the mean and covariance of the common Gaussian from given cross-domain rating matrices, the latent factors are aligned, which enables us to predict ratings in different domains. Experiments conducted on rating datasets from a wide variety of domains, e.g., movie, books and electronics, demonstrate that the proposed method achieves higher performance for predicting cross-domain ratings than existing methods."
373,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,An Autoregressive Approach to Nonparametric Hierarchical Dependent Modeling,"Zhihua Zhang, Dakan Wang, Edward Chang",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12c/zhang12c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhang12c,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12c.html,We propose a conditional autoregression framework for a collection of random probability measures. Under this framework we devise a conditional autoregressive Dirichlet process (DP) that we call one-parameter dependent DP (wDDP). The appealing properties of this specification are that it has two equivalent representations and its inference can be implemented in a conditional Polya urn scheme. Moreover these two representations bear a resemblance to the Polya urn scheme and the stick-breaking representation in the conventional DP. We apply this wDDP to Bayesian multivariate-response regression problems. An efficient Markov chain Monte Carlo algorithm is developed for Bayesian computation and prediction.
374,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Bayesian Framework for Online Classifier Ensemble,"Qinxun Bai, Henry Lam, Stan Sclaroff",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bai14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bai14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bai14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bai14.html,"We propose a Bayesian framework for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our framework estimates the weights in terms of evolving posterior distributions. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our framework admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than online boosting algorithms."
375,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Vector-Space Markov Random Fields via Exponential Families,"Wesley Tansey, Oscar Hernan Madrid Padilla, Arun Sai Suggala, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tansey15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/tansey15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tansey15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tansey15.html,"We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent \(M\) -estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available."
376,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Manifold Preserving Hierarchical Topic Models for Quantization and Approximation,"Minje Kim, Paris Smaragdis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kim13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13a.html,"We present two complementary topic models to address the analysis of mixture data lying on manifolds. First, we propose a quantization method with an additional mid-layer latent variable, which selects only data points that best preserve the manifold structure of the input data. In order to address the case of modeling all the in-between parts of that manifold using this reduced representation of the input, we introduce a new model that provides a manifold-aware interpolation method. We demonstrate the advantages of these models with experiments on the hand-written digit recognition and the speech source separation tasks."
377,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,From Word Embeddings To Document Distances,"Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kusnerb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kusnerb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kusnerb15.html,"We present the Word Moverês Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to –travel” to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Moverês Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates."
378,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Online Inference for the Infinite Topic-Cluster Model: Storylines from Streaming Text,"Amr Ahmed, Qirong Ho, Choon Hui Teo, Jacob Eisenstein, Alex Smola, Eric Xing","15:101-109, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ahmed11a/ahmed11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ahmed11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/ahmed11a.html,We present the time-dependent topic-cluster model a hierarchical approach for combining Latent Dirichlet Allocation and clustering via the Recurrent Chinese Restaurant Process. It inherits the advantages of both of its constituents namely interpretability and concise representation. We show how it can be applied to streaming collections of objects such as real world feeds in a news portal. We provide details of a parallel Sequential Monte Carlo algorithm to perform inference in the resulting graphical model which scales to hundred of thousands of documents.
379,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,A Combination of Boosting and Bagging for KDD Cup 2009 - Fast Scoring on a Large Database,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/xie09/xie09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_xie09,http://jmlr.csail.mit.edu/proceedings/papers/v7/xie09.html,We present the ideas and methodologies that we used to address the KDD Cup 2009 challenge on rank-ordering the probability of churn appetency and up-selling of wireless customers. We choose stochastic gradient boosting tree (TreeNet ¬) as our main classifier to handle this large unbalanced dataset. In order to further improve the robustness and accuracy of our results we bag a series of boosted tree models together as our final submission. Through our exploration we conclude that the most critical factors to achieve our results are effective variable preprocessing and selection proper imbalanced data handling as well as the combination of bagging and boosting techniques.
380,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Hierarchical Dirichlet Scaling Process,"Dongwoo Kim, Alice Oh",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kim14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kim14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kim14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kim14.html,"We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model for multi-labeled data. We construct the HDSP based on the gamma representation of the hierarchical Dirichlet process (HDP) which allows scaling the mixture components. With such construction, HDSP allocates a latent location to each label and mixture component in a space, and uses the distance between them to guide membership probabilities. We develop a variational Bayes algorithm for the approximate posterior inference of the HDSP. Through experiments on synthetic datasets as well as datasets of newswire, medical journal articles, and Wikipedia, we show that the HDSP results in better predictive performance than HDP, labeled LDA and partially labeled LDA."
381,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,FuSSO: Functional Shrinkage and Selection Operator,"Junier Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff Schneider, Fang-Cheng Yeh, Wen-Yih Tseng","JMLR W&CP 33 :715-723, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14b-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_oliva14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/oliva14b.html,"We present the FuSSO, a functional analogue to the LASSO, that efficiently finds a sparse set of functional input covariates to regress a real-valued response against. The FuSSO does so in a semi-parametric fashion, making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response. We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions. Furthermore, we observe good results on both synthetic and real-world data."
382,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Sample Complexity of Composite Likelihood,"Joseph Bradley, Carlos Guestrin",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bradley12/bradley12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bradley12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bradley12.html,We present the first PAC bounds for learning parameters of Conditional Random Fields (CRFs) with general structures over discrete and real-valued variables. Our bounds apply to composite likelihood which generalizes maximum likelihood and pseudolikelihood. Moreover we show that the only existing algorithm with a PAC bound for learning high-treewidth discrete models can be viewed as a computationally inefficient method for computing pseudolikelihood. We present an extensive empirical study of the statistical efficiency of these estimators as predicted by our bounds. Finally we use our bounds to show how to construct computationally and statistically efficient composite likelihood estimators.
383,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Variational Inference for Gaussian Process Modulated Poisson Processes,"Chris Lloyd, Tom Gunter, Michael Osborne, Stephen Roberts",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lloyd15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lloyd15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lloyd15.html,"We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya."
384,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling,"John Paisley, Chong Wang, David Blei","15:74-82, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/paisley11a/paisley11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_paisley11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/paisley11a.html,"We present the discrete infinite logistic normal distribution (DILN """"Dylan"""") a Bayesian nonparametric prior for mixed membership models. DILN is a generalization of the hierarchical Dirichlet process (HDP) that models correlation structure between the weights of the atoms at the group level. We derive a representation of DILN as a normalized collection of gamma-distributed random variables and study its statistical properties. We consider applications to topic modeling and derive a variational Bayes algorithm for approximate posterior inference. We study the empirical performance of the DILN topic model on four corpora comparing performance with the HDP and the correlated topic model."
385,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation,"Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/guo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/guo15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_guo15,http://jmlr.csail.mit.edu/proceedings/papers/v38/guo15.html,"We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of peopleês language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our modelês ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie –12 Angry Men.” We showcase our modelês capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions."
386,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Neighbor Compression,"Matt Kusner, Stephen Tyree, Kilian Weinberger, Kunal Agrawal",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kusner14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kusner14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kusner14.html,"We present Stochastic Neighborhood Compression (SNC), an algorithm to compress a dataset for the purpose of k-nearest neighbor (kNN) classification. Given training data, SNC learns a much smaller synthetic data set, that minimizes the stochastic 1-nearest neighbor classification error on the training data. This approach has several appealing properties: due to its small size, the compressed set speeds up kNN testing drastically (up to several orders of magnitude, in our experiments); it makes the kNN classifier substantially more robust to label noise; on 4 of 7 data sets it yields lower test error than kNN on the entire training set, even at compression ratios as low as 2%; finally, the SNC compression leads to impressive speed ups over kNN even when kNN and SNC are both used with ball-tree data structures, hashing, and LMNN dimensionality reduction, demonstrating that it is complementary to existing state-of-the-art algorithms to speed up kNN classification and leads to substantial further improvements."
387,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,SpeedBoost: Anytime Prediction with Uniform Near-Optimality,"Alex Grubb, Drew Bagnell",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/grubb12/grubb12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_grubb12,http://jmlr.csail.mit.edu/proceedings/papers/v22/grubb12.html,We present SpeedBoost a natural extension of functional gradient descent for learning anytime predictors which automatically trade computation time for predictive accuracy by selecting from a set of simpler candidate predictors. These anytime predictors not only generate approximate predictions rapidly but are capable of using extra resources at prediction time when available to improve performance. We also demonstrate how our framework can be used to select weak predictors which target certain subsets of the data allowing for efficient use of computational resources on difficult examples. We also show that variants of the SpeedBoost algorithm produce predictors which are provably competitive with any possible sequence of weak predictors with the same total complexity.
388,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Visualizing User Model in Exploratory Search Tasks,"Kalle Ilves, Alan Medlar, Dorota Glowacka",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/ilves15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_ilves15,http://jmlr.csail.mit.edu/proceedings/papers/v43/ilves15.html,We present our ongoing work on an interactive exploratory information retrieval system designed to explain to the user the reasons for its predictions and help the user to build a mental model to predict how the system will behave. Users indicate which documents interest them and reinforcement learning is used to model the user by allowing the system to trade off between exploration and exploitation.
389,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Multilabel Classification through Random Graph Ensembles,"Hongyu Su, Juho Rousu","JMLR W&CP 29 :404-418, 2013",http://jmlr.org/proceedings/papers/v29/Su13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Su13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Su13.html,"We present new methods for multilabel classification, relying on ensemble learning on a collection of random output graphs imposed on the multilabel and a kernel-based structured output learner as the base classifier. For ensemble learning, differences among the output graphs provide the required base classifier diversity and lead to improved performance in the increasing size of the ensemble. We study different methods of forming the ensemble prediction, including majority voting and two methods that perform inferences over the graph structures before or after combining the base models into the ensemble. We compare the methods against the state-of-the-art machine learning approaches on a set of heterogeneous multilabel benchmark problems, including multilabel AdaBoost, convex multitask feature learning, as well as single target learning approaches represented by Bagging and SVM. In our experiments, the random graph ensembles are very competitive and robust, ranking first or second on most of the datasets. Overall, our results show that random graph ensembles are viable alternatives to flat multilabel and multitask learners."
390,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Pairwise Measures of Causal Direction in Linear Non-Gaussian Acyclic Models,Aapo Hyvarinen,"13:1-16, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/hyvarinen10a/hyvarinen10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_hyvarinen10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/hyvarinen10a.html,We present new measures of the causal direction between two nongaussian random variables. They are based on the likelihood ratio under the linear non-gaussian acyclic model (LiNGAM). We also develop simple first-order approximations and analyze them based on related cumulant-based measures. The cumulant-based measures can be shown to give the right causal directions and they are statistically consistent even in the presence of measurement noise. We further show how to apply these measures to estimate LiNGAM for more than two variables and even in the case of more variables than observations. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data and it is computationally and conceptually very simple.
391,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Fast Computation of Wasserstein Barycenters,"Marco Cuturi, Arnaud Doucet",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cuturi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cuturi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cuturi14.html,"We present new algorithms to compute the mean of a set of \(N\) empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter , is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of , we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem."
392,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Online Learning with Predictable Sequences,"Alexander Rakhlin, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Rakhlin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Rakhlin13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Rakhlin13.html,"We present methods for online linear optimization that take advantage of benign (as opposed to worst-case) sequences. Specifically if the sequence encountered by the learner is described well by a known –predictable process”, the algorithms presented enjoy tighter bounds as compared to the typical worst case bounds. Additionally, the methods achieve the usual worst-case regret bounds if the sequence is not benign. Our approach can be seen as a way of adding prior knowledge about the sequence within the paradigm of online learning. The setting is shown to encompass partial and side information. Variance and path-length bounds can be seen as particular examples of online learning with simple predictable sequences.We further extend our methods to include competing with a set of possible predictable processes (models), that is –learning” the predictable process itself concurrently with using it to obtain better regret guarantees. We show that such model selection is possible under various assumptions on the available feedback."
393,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Locally-Linear Learning Machines (L3M),"Joseph Wang, Venkatesh Saligrama","JMLR W&CP 29 :451-466, 2013",http://jmlr.org/proceedings/papers/v29/Wang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Wang13a,http://jmlr.csail.mit.edu/proceedings/papers/v29/Wang13a.html,"We present locally-linear learning machines (L3M) for multi-class classification. We formulate a global convex risk function to jointly learn linear feature space partitions and region-specific linear classifiers. L3Mês features such as: (1) discriminative power similar to Kernel SVMs and Adaboost; (2) tight control on generalization error; (3) low training time cost due to on-line training; (4) low test-time costs due to local linearity; are all potentially well-suited for –big-data” applications. We derive tight convex surrogates for the empirical risk function associated with space partitioning classifiers. These empirical risk functions are non-convex since they involve products of indicator functions. We obtain a global convex surrogate by first embedding empirical risk loss as an extremal point of an optimization problem and then convexifying this resulting problem. Using the proposed convex formulation, we demonstrate improvement in classification performance, test and training time relative to common discriminative learning methods on challenging multiclass data sets."
394,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Linear-Time Estimators for Propensity Scores,"Deepak Agarwal, Lihong Li, Alexander Smola","15:93-100, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/agarwal11c/agarwal11c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_agarwal11c,http://jmlr.csail.mit.edu/proceedings/papers/v15/agarwal11c.html,We present linear-time estimators for three popular covariate shift correction and propensity scoring algorithms: logistic regression(LR) kernel mean matching(KMM) and maximum entropy mean matching(MEMM). This allows applications in situations where \emph{both} treatment and control groups are large. We also show that the last two algorithms differ only in their choice of regularizer ($\ell_2$ of the Radon Nikodym derivative vs. maximum entropy). Experiments show that all methods scale well.
395,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Exploiting Ontology Structures and Unlabeled Data for Learning,"Nina Balcan, Avrim Blum, Yishay Mansour",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_balcan13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/balcan13a.html,"We present and analyze a theoretical model designed to understand and explain the effectiveness of ontologies for learning multiple related tasks from primarily unlabeled data. We present both information-theoretic results as well as efficient algorithms. We show in this model that an ontology, which specifies the relationships between multiple outputs, in some cases is sufficient to completely learn a classification using a large unlabeled data source."
396,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Structured Sparse Principal Component Analysis,"Rodolphe Jenatton, Guillaume Obozinski, Francis Bach","9:366-373, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/jenatton10a/jenatton10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_jenatton10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/jenatton10a.html,We present an extension of sparse PCA or sparse dictionary learning where the sparsity patterns of all dictionary elements are structured and constrained to belong to a prespecified set of shapes. This structured sparse PCA is based on a structured regularization recently introduced by Jenatton et al.(2009). While classical sparse priors only deal with cardinality the regularization we use encodes higher-order information about the data. We propose an efficient and simple optimization procedure to solve this problem. Experiments with two practical tasks the denoising of sparse structured signals and face recognition demonstrate the benefits of the proposed structured approach over unstructured approaches.
397,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,"On a Connection between Maximum Variance Unfolding, Shortest Path Problems and IsoMap","Alexander Paprotny, Jochen Garcke",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/paprotny12/paprotny12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_paprotny12,http://jmlr.csail.mit.edu/proceedings/papers/v22/paprotny12.html,We present an equivalent formulation of the Maximum Variance Unfolding (MVU) problem in terms of distance matrices. This yields a novel interpretation of the MVU problem as a regularized version of the shortest path problem on a graph. This interpretation enables us to establish an asymptotic convergence result for the case that the underlying data are drawn from a Riemannian manifold which is isometric to a convex subset of Euclidean space.
398,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Multi-object tracking with representations of the symmetric group,"Risi Kondor, Andrew Howard, Tony Jebara","2:211-218, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/kondor07a/kondor07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_kondor07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/kondor07a.html,"We present an efficient algorithm for approximately maintaining and updating a distribution over permutations matching tracks to real world objects. The algorithm hinges on two insights from the theory of harmonic analysis on noncommutative groups. The first is that most of the information in the distribution over permutations is captured by certain ""low frequency"" Fourier components. The second is that Bayesian updates of these components can be efficiently realized by extensions of Clausen's FFT for the symmetric group."
399,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Approximate parameter inference in a stochastic reaction-diffusion model,"Andreas Ruttor, Manfred Opper","9:669-676, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ruttor10a/ruttor10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ruttor10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ruttor10a.html,We present an approximate inference approach to parameter estimation in a spatio-temporal stochastic process of the reaction-diffusion type. The continuous space limit of an inference method for Markov jump processes leads to an approximation which is related to a spatial Gaussian process. An efficient solution in feature space using a Fourier basis is applied to inference on simulational data.
400,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Predictive Discretization during Model Selection,"Harald Steck, Tommi S. Jaakkola","2:532-539, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/steck07a/steck07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_steck07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/steck07a.html,We present an approach to discretizing multivariate continuous data while learning the structure of a graphical model. We derive the joint scoring function from the principle of predictive accuracy which inherently ensures the optimal trade-off between goodness of fit and model complexity (including the number of discretization levels). Using the so-called finest grid implied by the data our scoring function depends only on the number of data points in the various discretization levels. Not only can it be computed efficiently but it is also invariant under monotonic transformations of the continuous space. Our experiments show that the discretization method can substantially impact the resulting graph structure.
401,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Applying Grammar Inference To Identify Generalized Patterns of Facial Expressions of Pain,"Ute Schmid, Michael Siebers, Dominik Seu_, Miriam Kunz and Stefan Lautenbacher","21:183-188, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/schmid12a/schmid12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_schmid12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/schmid12a.html,"We present an application of grammar inference in the domain of facial expression analysis. Typically, only sets of AUs which occur in a given time frame are used for expression analysis, the sequence in which these AUs occur is ignored. We wanted to explore whether the strucural patterns of AU appearances contain diagnostically relevant information. We applied alignment-based learning (ABL) to data of facial expressions of pain collected in a psychological study. To evaluate the quality of the induced grammars we applied cross-validation to estimate the generalization error. We can show that the learned grammars have reasonably high coverages for unseen pain sequences. However, the number of rules of the learned grammars cannot be reduced substantially without loss of generalization."
402,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Combining Experiments to Discover Linear Cyclic Models with Latent Variables,"Frederick Eberhardt, Patrik Hoyer, Richard Scheines","9:185-192, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/eberhardt10a/eberhardt10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_eberhardt10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/eberhardt10a.html,We present an algorithm to infer causal relations between a set of measured variables on the basis of experiments on these variables. The algorithm assumes that the causal relations are linear but is otherwise completely general: It provides consistent estimates when the true causal structure contains feedback loops and latent variables while the experiments can involve surgical or 'soft' interventions on one or multiple variables at a time. The algorithm is 'online' in the sense that it combines the results from any set of available experiments can incorporate background knowledge and resolves conflicts that arise from combining results from different experiments. In addition we provide a necessary and sufficient condition that (i) determines when the algorithm can uniquely return the true graph and (ii) can be used to select the next best experiment until this condition is satisfied. We demonstrate the method by applying it to simulated data and the flow cytometry data of Sachs et al (2005).
403,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,One Practical Algorithm for Both Stochastic and Adversarial Bandits,"Yevgeny Seldin, Aleksandrs Slivkins",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/seldinb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/seldinb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_seldinb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/seldinb14.html,"We present an algorithm for multiarmed bandits that achieves almost optimal performance in both stochastic and adversarial regimes without prior knowledge about the nature of the environment. Our algorithm is based on augmentation of the EXP3 algorithm with a new control lever in the form of exploration parameters that are tailored individually for each arm. The algorithm simultaneously applies the –old” control lever, the learning rate, to control the regret in the adversarial regime and the new control lever to detect and exploit gaps between the arm losses. This secures problem-dependent –logarithmic” regret when gaps are present without compromising on the worst-case performance guarantee in the adversarial regime. We show that the algorithm can exploit both the usual expected gaps between the arm losses in the stochastic regime and deterministic gaps between the arm losses in the adversarial regime. The algorithm retains –logarithmic” regret guarantee in the stochastic regime even when some observations are contaminated by an adversary, as long as on average the contamination does not reduce the gap by more than a half. Our results for the stochastic regime are supported by experimental validation."
404,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods,"Jascha Sohl-Dickstein, Ben Poole, Surya Ganguli",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/sohl-dicksteinb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sohl-dicksteinb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sohl-dicksteinb14.html,"We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages."
405,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Landmarking Manifolds with Gaussian Processes,"Dawen Liang, John Paisley",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/liang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_liang15,http://jmlr.csail.mit.edu/proceedings/papers/v37/liang15.html,"We present an algorithm for finding landmarks along a manifold. These landmarks provide a small set of locations spaced out along the manifold such that they capture the low-dimensional non-linear structure of the data embedded in the high-dimensional space. The approach does not select points directly from the dataset, but instead we optimize each landmark by moving along the continuous manifold space (as approximated by the data) according to the gradient of an objective function. We borrow ideas from active learning with Gaussian processes to define the objective, which has the property that a new landmark is –repelled” by those currently selected, allowing for exploration of the manifold. We derive a stochastic algorithm for learning with large datasets and show results on several datasets, including the Million Song Dataset and articles from the New York Times."
406,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,"Fast interior-point inference in high-dimensional sparse, penalized state-space models","Eftychios Pnevmatikakis, Liam Paninski",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/pnevmatikakis12/pnevmatikakis12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_pnevmatikakis12,http://jmlr.csail.mit.edu/proceedings/papers/v22/pnevmatikakis12.html,We present an algorithm for fast posterior inference in penalized high-dimensional state-space models suitable in the case where a few measurements are taken in each time step. We assume that the state prior and observation likelihoods are log-concave and have a special structure that allows fast matrix-vector operations. We derive a second-order algorithm for computing the maximum a posteriori state path estimate where the cost per iteration scales linearly both in time and memory. This is done by computing an approximate Newton direction using an efficient forward-backward scheme based on a sequence of low rank updates. We formalize the conditions under which our algorithm is applicable and prove its stability and convergence. We show that the state vector can be drawn from a large class of prior distributions without affecting the linear complexity of our algorithm. This class includes both Gaussian and nonsmooth sparse and group sparse priors for which we employ an interior point modification of our algorithm. We discuss applications in text modeling and neuroscience.
407,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm,"Jacob Steinhardt, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/steinhardtb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/steinhardtb14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_steinhardtb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/steinhardtb14.html,"We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem posed by Kale (2012). Our techniques naturally extend to matrix-valued loss functions, where we present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound in the matrix case, we generalize the Follow-the-Regularized-Leader algorithm to vector-valued payoffs, which may be of independent interest."
408,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Topic Discovery through Data Dependent and Random Projections,"Weicong Ding, Mohammad Hossein Rohban, Prakash Ishwar, Venkatesh Saligrama",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ding13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ding13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ding13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ding13.html,"We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and realworld datasets to demonstrate qualitative and quantitative merits of our scheme."
409,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Reducing Dueling Bandits to Cardinal Bandits,"Nir Ailon, Zohar Karnin, Thorsten Joachims",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ailon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ailon14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ailon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ailon14.html,"We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form –A is preferred to B” (as opposed to cardinal feedback like –A has value 2.5”), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions _ named \(\Doubler\) , \(\MultiSbm\) and \(\DoubleSbm\) _ provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting. For \(\Doubler\) and \(\MultiSbm\) we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of \(\DoubleSbm\) which empirically outperforms the other two as well as previous algorithms in our experiments. In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms."
410,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,"AClass: A simple, online, parallelizable algorithm for probabilistic classification","Vikash K. Mansinghka, Daniel M. Roy, Ryan Rifkin, Josh Tenenbaum","2:315-322, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/mansinghka07a/mansinghka07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_mansinghka07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/mansinghka07a.html,We present AClass a simple online parallelizable algorithm for supervised multiclass classification. AClass models each classconditional density as a Chinese restaurant process mixture and performs approximate inference in this model using a sequential Monte Carlo scheme. AClass combines several strengths of previous approaches to classification that are not typically found in a single algorithm; it supports learning from missing data and yields sensibly regularized nonlinear decision boundaries while remaining computationally efficient. We compare AClass to several standard classification algorithms and show competitive performance.
411,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Automating Quantitative Narrative Analysis of News Data,"Saatviga Sudhahar, Roberto Franzosi, Nello Cristianini","17:63-71, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/sudhahar11a/sudhahar11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_sudhahar11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/sudhahar11a.html,"We present a working system for large scale quantitative narrative analysis (QNA) of news corpora, which includes various recent ideas from text mining and pattern analysis in order to solve a problem arising in computational social sciences. The task is that of identifying the key actors in a body of news, and the actions they perform, so that further analysis can be carried out. This step is normally performed by hand and is very labour intensive. We then characterise the actors by: studying their position in the overall network of actors and actions; studying the time series associated with some of their properties; generating scatter plots describing the subject/object bias of each actor; and investigating the types of actions each actor is most associated with. The system is demonstrated on a set of 100,000 articles about crime appeared on the New York Times between 1987 and 2007. As an example, we find that Men were most commonly responsible for crimes against the person, while Women and Children were most often victims of those crimes."
412,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Variable Selection for Gaussian Graphical Models,"Jean Honorio, Dimitris Samaras, Irina Rish, Guillermo Cecchi",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/honorio12/honorio12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_honorio12,http://jmlr.csail.mit.edu/proceedings/papers/v22/honorio12.html,We present a variable-selection structure learning approach for Gaussian graphical models. Unlike standard sparseness promoting techniques our method aims at selecting the most-important variables besides simply sparsifying the set of edges. Through simulations we show that our method outperforms the state-of-the-art in recovering the ground truth model. Our method also exhibits better generalization performance in a wide range of complex real-world datasets: brain fMRI gene expression NASDAQ stock prices and world weather. We also show that our resulting networks are more interpretable in the context of brain fMRI analysis while retaining discriminability. From an optimization perspective we show that a block coordinate descent method generates a sequence of positive definite solutions. Thus we reduce the original problem into a sequence of strictly convex ($\ell_1$$\ell_p$) regularized quadratic minimization subproblems for $p\in \{2\infty\}$. Our algorithm is well founded since the optimal solution of the maximization problem is unique and bounded.
413,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Adaptive Recovery of Signals by Convex Optimization,"Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski, Dmitry Ostrovsky",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Harchaoui15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Harchaoui15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Harchaoui15.html,"We present a theoretical framework for adaptive estimation and prediction of signals of unknown structure in the presence of noise. The framework allows to address two intertwined challenges: (i) designing optimal statistical estimators; (ii) designing efficient numerical algorithms. In particular, we establish oracle inequalities for the performance of adaptive procedures, which rely upon convex optimization and thus can be efficiently implemented. As an application of the proposed approach, we consider denoising of harmonic oscillations."
414,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning A* underestimates : Using inference to guide inference,"Gregory Druck, Mukund Narasimhan, Paul Viola","2:99-106, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/druck07a/druck07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_druck07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/druck07a.html,We present a technique for speeding up inference of structured variables using a prioritydriven search algorithm rather than the more conventional dynamic programing. A priority-driven search algorithm is guaranteed to return the optimal answer if the priority function is an underestimate of the true cost function. We introduce the notion of a probable approximate underestimate and show that it can be used to compute a probable approximate solution to the inference problem when used as a priority function. We show that we can learn probable approximate underestimate functions which have the functional form of simpler easy to decode models. These models can be learned from unlabeled data by solving a linear/quadratic optimization problem. As a result we get a priority function that can be computed quickly and results in solutions that are (provably) almost optimal most of the time. Using these ideas discriminative classifiers such as semi-Markov CRFs and discriminative parsers can be sped up using a generalization of the A* algorithm. Further this technique resolves one of the biggest obstacles to the use of A* as a general decoding procedure namely that of coming up with a admissible priority function. Applying this technique results in a algorithm that is more than 3 times as fast as the Viterbi algorithm for decoding semi-Markov Conditional Markov Models.
415,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity,"Santosh S. Vempala, Ying. Xiao",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Vempala15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Vempala15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Vempala15.html,"We present a simple, general technique for reducing the sample complexity of matrix and tensor decomposition algorithms applied to distributions. We use the technique to give a polynomial-time algorithm for standard ICA with sample complexity nearly linear in the dimension, thereby improving substantially on previous bounds. The analysis is based on properties of random polynomials, namely the spacings of an ensemble of polynomials. Our technique also applies to other applications of tensor decompositions, including spherical Gaussian mixture models."
416,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Learning Functions of Halfspaces using Prefix Covers,"Parikshit Gopalan, Adam R. Klivans and Raghu Meka",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/gopalan12/gopalan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_gopalan12,http://jmlr.csail.mit.edu/proceedings/papers/v23/gopalan12.html,"We present a simple query-algorithm for learning arbitrary functions of k halfspaces under any product distribution on the Boolean hypercube. Our algorithms learn any function of k halfspaces to within accuracy _ in time O((nk/_) k+1 ) under any product distribution on {0, 1} n using read-once branching programs as a hypothesis. This gives the first poly(n, 1/_) algorithm for learning even the intersection of 2 halfspaces under the uniform distribution on {0, 1} n previously known algorithms had an exponential dependence either on the accuracy parameter _ or the dimension n . To prove this result, we identify a new structural property of Boolean functions that yields learnability with queries: that of having a small prefix cover."
417,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Bayesian variable order Markov models,Christos Dimitrakakis,"9:161-168, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/dimitrakakis10a/dimitrakakis10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_dimitrakakis10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/dimitrakakis10a.html,We present a simple effective generalisation of variable order Markov models to full online Bayesian estimation. The mechanism used is close to that employed in context tree weighting. The main contribution is the addition of a prior conditioned on context on the Markov order. The resulting construction uses a simple recursion and can be updated efficiently. This allows the model to make predictions using more complex contexts as more data is acquired if necessary. In addition our model can be alternatively seen as a mixture of tree experts. Experimental results show that the predictive model exhibits consistently good performance in a variety of domains.
418,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Ensemble Methods for Structured Prediction,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesa14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cortesa14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesa14.html,"We present a series of learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks. This includes several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conversions and learning guarantees. We also report the results of extensive experiments with these algorithms in several structured prediction tasks."
419,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Fugue: Slow-Worker-Agnostic Distributed Learning for Big Models on Big Data,"Abhimanu Kumar, Alex Beutel, Qirong Ho, Eric Xing","JMLR W&CP 33 :531-539, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kumar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kumar14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kumar14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kumar14.html,"We present a scheme for fast, distributed learning on big (i.e. high-dimensional) models applied to big datasets. Unlike algorithms that focus on distributed learning in either the big data or big model setting (but not both), our scheme partitions both the data and model variables simultaneously. This not only leads to faster learning on distributed clusters, but also enables machine learning applications where both data and model are too large to fit within the memory of a single machine. Furthermore, our scheme allows worker machines to perform additional updates while waiting for slow workers to finish, which provides users with a tunable synchronization strategy that can be set based on learning needs and cluster conditions. We prove the correctness of such strategies, as well as provide bounds on the variance of the model variables under our scheme. Finally, we present empirical results for latent space models such as topic models, which demonstrate that our method scales well with large data and model sizes, while beating learning strategies that fail to take both data and model partitioning into account."
420,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Scalable Inference on Kingman's Coalescent using Pair Similarity,"Dilan Gorur, Levi Boyles, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/gorur12/gorur12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_gorur12,http://jmlr.csail.mit.edu/proceedings/papers/v22/gorur12.html,We present a scalable sequential Monte Carlo algorithm and its greedy counterpart for models based on Kingman's coalescent. We utilize fast nearest neighbor algorithms to limit expensive computations to only a subset of data point pairs. For a dataset size of n the resulting algorithm has O(n log n) computational complexity. We empirically verify that we achieve a large speedup in computation. When the gain in speed is used for increasing the number of particles we can often obtain significantly better samples than those of previous algorithms. We apply our algorithm for learning visual taxonomies of birds on 6033 examples a dataset size for which previous algorithms fail to be feasible.
421,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors,"Piyush Rai, Yingjian Wang, Shengbo Guo, Gary Chen, David Dunson, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/rai14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rai14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rai14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rai14.html,"We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the P„lya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets."
422,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,A PTAS for Agnostically Learning Halfspaces,Amit Daniely,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Daniely15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Daniely15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Daniely15.html,"We present a PTAS for agnostically learning halfspaces w.r.t. the uniform distribution on the \(d\) dimensional sphere. Namely, we show that for every \(\mu_0\) there is an algorithm that runs in time \(\mathrm{poly}\left(d,\frac{1}{\epsilon}\right)\) , and is guaranteed to return a classifier with error at most \((1+\mu)\mathrm{opt}+\epsilon\) , where \(\mathrm{opt}\) is the error of the best halfspace classifier. This improves on Awasthi, Balcan and Long (STOC 2014) who showed an algorithm with an (unspecified) constant approximation ratio. Our algorithm combines the classical technique of polynomial regression, together with the new localization technique of Awasthi et. al."
423,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Supervised Spectral Latent Variable Models,"Liefeng Bo, Cristian Sminchisescu","5:33-40, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/bo09a/bo09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_bo09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/bo09a.html,We present a probabilistic structured prediction method for learning input-output dependencies where correlations between outputs are modeled as low-dimensional manifolds constrained by both geometric distance preserving output relationsand predictive power of inputs. Technically this reduces to learning a probabilistic input conditional model over latent (manifold) and output variables using an alternation scheme. In one round we optimize the parameters of an input-driven manifold predictor using latent targets given by preimages (conditional expectations) of the current manifold-to-output model. In the next round we use the distribution given by the manifold predictor in order to maximize the probability of the outputs with an additional implicit distance preserving constraint on the manifold. The resulting Supervised Spectral Latent Variable Model (SSLVM) combines the properties of probabilistic geometric manifold learning (accommodates geometric constraints corresponding to any spectral embedding method including PCA ISOMAP or Laplacian Eigenmaps) with the additional supervisory information to further constrain it for predictive tasks. We demonstrate the superiority of the method over baseline PPCA + regression frameworks and show its potential in difficult realworld computer vision benchmarks designed for the reconstruction of three-dimensional human poses from monocular image sequences.
424,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Dual Query: Practical Private Query Release for High Dimensional Data,"Marco Gaboardi, Emilio Jesus Gallego Arias, Justin Hsu, Aaron Roth, Zhiwei Steven Wu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gaboardi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gaboardi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gaboardi14.html,"We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude."
425,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Semidifferential-based Submodular Function Optimization,"Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/iyer13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/iyer13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_iyer13,http://jmlr.csail.mit.edu/proceedings/papers/v28/iyer13.html,"We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments."
426,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Spectral Learning Approach to Range-Only SLAM,"Byron Boots, Geoff Gordon",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/boots13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/boots13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_boots13,http://jmlr.csail.mit.edu/proceedings/papers/v28/boots13.html,"We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our approach does not need to linearize a transition or measurement model. We provide a theoretical analysis of our method, including finite-sample error bounds. Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost."
427,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Communication-Efficient Distributed Optimization using an Approximate Newton-type Method,"Ohad Shamir, Nati Srebro, Tong Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/shamir14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/shamir14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_shamir14,http://jmlr.csail.mit.edu/proceedings/papers/v32/shamir14.html,"We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably improves with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM."
428,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Estimating Tree-Structured Covariance Matrices via Mixed-Integer Programming,"Hector Corrada Bravo, Stephen Wright, Kevin Eng, Sunduz Keles, Grace Wahba","5:41-48, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/bravo09a/bravo09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_bravo09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/bravo09a.html,We present a novel method for estimating tree-structured covariance matrices directly from observed continuous data. A representation of these classes of matrices as linear combinations of rank-one matrices indicating object partitions is used to formulate estimation as instances of well-studied numerical optimization problems. In particular our estimates are based on projection where the covariance estimate is the nearest tree-structured covariance matrix to an observed sample covariance matrix. The problem is posed as a linear or quadratic mixed-integer program (MIP) where a setting of the integer variables in the MIP specifies a set of tree topologies of the structured covariance matrix. We solve these problems to optimality using efficient and robust existing MIP solvers. We present a case study in phylogenetic analysis of expression in yeast gene families and a comparison using simulated data to distance-based tree estimating procedures.
429,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Tilted Variational Bayes,"James Hensman, Max Zwiessele, Neil Lawrence","JMLR W&CP 33 :356-364, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hensman14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/hensman14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hensman14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hensman14.html,"We present a novel method for approximate inference. Using some of the constructs from expectation propagation (EP), we derive a lower bound of the marginal likelihood in a similar fashion to variational Bayes (VB). The method combines some of the benefits of VB and EP: it can be used with light-tailed likelihoods (where traditional VB fails), and it provides a lower bound on the marginal likelihood. We apply the method to Gaussian process classification, a situation where the Kullback-Leibler divergence minimized in traditional VB can be infinite, and to robust Gaussian process regression, where the inference process is dramatically simplified in comparison to EP. Code to reproduce all the experiments can be found at github.com/SheffieldML/TVB."
430,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Informative Priors for Markov Blanket Discovery,"Adam Pocock, Mikel Lujan, Gavin Brown",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/pocock12/pocock12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_pocock12,http://jmlr.csail.mit.edu/proceedings/papers/v22/pocock12.html,We present a novel interpretation of information theoretic feature selection as optimization of a discriminative model. We show that this formulation coincides with a group of mutual information based filter heuristics in the literature and show how our probabilistic framework gives a well-founded extension for informative priors. We then derive a particular sparsity prior that recovers the well-known IAMB algorithm (Tsamardinos & Aliferis 2003) and extend it to create a novel algorithm IAMB-IP that includes domain knowledge priors. In empirical evaluations we find the new algorithm to improve Markov Blanket recovery even when a misspecified prior was used in which half the prior knowledge was incorrect.
431,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Ordinal Mixed Membership Models,"Seppo Virtanen, Mark Girolami",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/virtanen15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_virtanen15,http://jmlr.csail.mit.edu/proceedings/papers/v37/virtanen15.html,"We present a novel class of mixed membership models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range of practical contemporary applications. In this work, by way of illustration, we apply the models to a collection of consumer-generated reviews of mobile software applications, where each review contains unstructured text data accompanied with an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns of consumer feedback. We also compare the developed models to relevant existing works, which rely on improper statistical assumptions for ordinal variables, showing significant improvements both in predictive ability and knowledge extraction."
432,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On learning parametric-output HMMs,"Aryeh Kontorovich, Boaz Nadler, Roi Weiss",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kontorovich13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/kontorovich13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kontorovich13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kontorovich13.html,"We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results."
433,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Bagged Structure Learning of Bayesian Network,Gal Elidan,"15:251-259, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/elidan11a/elidan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_elidan11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/elidan11a.html,We present a novel approach for density estimation using Bayesian networks when faced with scarce and partially observed data. Our approach relies on Efron's bootstrap framework and replaces the standard model selection score by a bootstrap aggregation objective aimed at sifting out bad decisions during the learning procedure. Unlike previous bootstrap or MCMC based approaches that are only aimed at recovering specific structural features we learn a concrete density model that can be used for probabilistic generalization. To make use of our objective when some of the data is missing we propose a bagged structural EM procedure that does not incur the heavy computational cost typically associated with a bootstrap-based approach. We compare our bagged objective to the Bayesian score and the Bayesian information criterion (BIC) as well as other bootstrap-based model selection objectives and demonstrate its effectiveness in improving generalization performance for varied real-life datasets.
434,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Monotone multi-armed bandit allocations,Aleksandrs Slivkins,"19:831-836, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/slivkins11b/slivkins11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_slivkins11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/slivkins11b.html,We present a novel angle for multi-armed bandits (henceforth abbreviated MAB) which follows from the recent work on \emph{MAB mechanisms}~\citep{MechMAB-ec09DevanurK08Transform-ec10}. The new problem is essentially about designing MAB algorithms under an additional constraint motivated by their application to MAB mechanisms.This note is self-contained although some familiarity with MAB is assumed; we refer the reader to~\cite{CesaBL-book} for more background.
435,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A reversible infinite HMM using normalised random measures,"David Knowles, Zoubin Ghahramani, Konstantina Palla",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/knowles14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/knowles14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_knowles14,http://jmlr.csail.mit.edu/proceedings/papers/v32/knowles14.html,"We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges. By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording."
436,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Celeste: Variational inference for a generative model of astronomical images,"Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, Mr Prabhat",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/regier15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_regier15,http://jmlr.csail.mit.edu/proceedings/papers/v37/regier15.html,"We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors."
437,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Sparse meta-Gaussian information bottleneck,"Melani Rey, Volker Roth, Thomas Fuchs",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/rey14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rey14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rey14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rey14.html,"We present a new sparse compression technique based on the information bottleneck (IB) principle, which takes into account side information. This is achieved by introducing a sparse variant of IB which preserves the information in only a few selected dimensions of the original data through compression. By assuming a Gaussian copula we can capture arbitrary non-Gaussian margins, continuous or discrete. We apply our model to select a sparse number of biomarkers relevant to the evolution of malignant melanoma and show that our sparse selection provides reliable predictors."
438,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning the Irreducible Representations of Commutative Lie Groups,"Taco Cohen, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cohen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/cohen14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cohen14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cohen14.html,"We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups _ a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification."
439,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Online Linear Optimization via Smoothing,"Jacob Abernethy, Chansoo Lee, Abhinav Sinha, Ambuj Tewari","JMLR W&CP 35 :807-823, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/abernethy14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_abernethy14,http://jmlr.csail.mit.edu/proceedings/papers/v35/abernethy14.html,"We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between –Follow the Regularized Leader” and –Follow the Perturbed Leader” up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader."
440,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multi-period Trading Prediction Markets with Connections to Machine Learning,"Jinli Hu, Amos Storkey",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hu14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hu14.html,"We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice of modelling approach enables us to show that the whole market approaches a global objective, despite the fact that the market is designed such that each agent only cares about its own goal. In addition, the market dynamic provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective; and 2) solve machine learning problems by setting up and running certain markets."
441,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Cold-start Active Learning with Robust Ordinal Matrix Factorization,"Neil Houlsby, Jose Miguel Hernandez-Lobato, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/houlsby14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/houlsby14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_houlsby14,http://jmlr.csail.mit.edu/proceedings/papers/v32/houlsby14.html,"We present a new matrix factorization model for rating data and a corresponding active learning strategy to address the cold-start problem. Cold-start is one of the most challenging tasks for recommender systems: what to recommend with new users or items for which one has little or no data. An approach is to use active learning to collect the most useful initial ratings. However, the performance of active learning depends strongly upon having accurate estimates of i) the uncertainty in model parameters and ii) the intrinsic noisiness of the data. To achieve these estimates we propose a heteroskedastic Bayesian model for ordinal matrix factorization. We also present a computationally efficient framework for Bayesian active learning with this type of complex probabilistic model. This algorithm successfully distinguishes between informative and noisy data points. Our model yields state-of-the-art predictive performance and, coupled with our active learning strategy, enables us to gain useful information in the cold-start setting from the very first active sample."
442,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Deep Boltzmann Machines,"Ruslan Salakhutdinov, Geoffrey Hinton","5:448-455, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_salakhutdinov09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/salakhutdinov09a.html,We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer ``pre-training'' phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.
443,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"Michael Gutmann, Aapo Hyv_rinen","9:297-304, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/gutmann10a/gutmann10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_gutmann10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/gutmann10a.html,We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters and analyze the asymptotic variance. In particular the method is shown to directly work for unnormalized models i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model we compare the method with other estimation methods that can be used to learn unnormalized models including score matching contrastive divergence and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.
444,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deep Boosting,"Corinna Cortes, Mehryar Mohri, Umar Syed",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesb14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cortesb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cortesb14.html,"We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a •capacity-consciousê criterion for the selection of the hypotheses. We give new data-dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their \(L_1\) -regularized variants."
445,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Structural Maxent Models,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Umar Syed",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/cortes15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/cortes15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_cortes15,http://jmlr.csail.mit.edu/proceedings/papers/v37/cortes15.html,"We present a new class of density estimation models, Structural Maxent models, with feature functions selected from possibly very complex families. The design of our models is motivated by data-dependent convergence bounds and benefits from new data-dependent learning bounds expressed in terms of the Rademacher complexities of the sub-families composing the family of features considered. We prove a duality theorem, which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm, including the details of its derivation and report the results of several experiments demonstrating that its performance compares favorably to that of existing regularized Maxent. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models making use of possibly complex feature families. We also prove a duality theorem for these models which shows the connection between these models and existing binary and multi-class deep boosting algorithms."
446,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,The Best of Both Worlds: Stochastic and Adversarial Bandits,S_bastien Bubeck and Aleksandrs Slivkins,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/bubeck12b/bubeck12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_bubeck12b,http://jmlr.csail.mit.edu/proceedings/papers/v23/bubeck12b.html,"We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal) whose regret is (essentially) optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the O (í n ) worst-case regret of Exp3 (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats them separately, and does not attempt to jointly optimize for both. This result falls into the general agenda to design algorithms that combine the optimal worst-case performance with improved guarantees for ""nice"" problem instances."
447,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Efficient Learning of Deep Boltzmann Machines,"Ruslan Salakhutdinov, Hugo Larochelle","9:693-700, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/salakhutdinov10a/salakhutdinov10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_salakhutdinov10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/salakhutdinov10a.html,We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM's) a generative model with many layers of hidden variables. The algorithm learns a separate ``recognition'' model that is used to quickly initialize in a single bottom-up pass the values of the latent variables in all hidden layers. We show that using such a recognition model followed by a combined top-down and bottom-up pass it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM both as a generative and discriminative model. Moreover inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN) making large-scale learning of DBM's practical. Finally we demonstrate that the DBM's trained using the proposed approximate inference algorithm perform well compared to DBN's and SVM's on the MNIST handwritten digit OCR English letters and NORB visual object recognition tasks.
448,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Buffer k-d Trees: Processing Massive Nearest Neighbor Queries on GPUs,"Fabian Gieseke, Justin Heinermann, Cosmin Oancea, Christian Igel",none,http://jmlr.org/proceedings/papers/v32/gieseke14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gieseke14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gieseke14.html,"We present a new approach for combining k-d trees and graphics processing units for nearest neighbor search. It is well known that a direct combination of these tools leads to a non-satisfying performance due to conditional computations and suboptimal memory accesses. To alleviate these problems, we propose a variant of the classical k-d tree data structure, called buffer k-d tree, which can be used to reorganize the search. Our experiments show that we can take advantage of both the hierarchical subdivision induced by k-d trees and the huge computational resources provided by todayês many-core devices. We demonstrate the potential of our approach in astronomy, where hundreds of million nearest neighbor queries have to be processed."
449,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, Robert Schapire",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwalb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_agarwalb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwalb14.html,"We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of \(K\) actions in response to the observed context , and observes the reward only for that action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only \(\otil(\sqrt{KT})\) oracle calls across all \(T\) rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We conduct a proof-of-concept experiment which demonstrates the excellent computational and statistical performance of (an online variant of) our algorithm relative to several strong baselines."
450,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multi-Class Classification with Maximum Margin Multiple Kernel,"Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cortes13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/cortes13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cortes13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cortes13.html,"We present a new algorithm for multi-class classification with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel. We show that larger values of this quantity guarantee the existence of an accurate multi-class predictor and also define a family of multiple kernel algorithms based on the maximization of the multi-class margin of a kernel (M 3 K). We present an extensive theoretical analysis in support of our algorithm, including novel multi-class Rademacher complexity margin bounds. Finally, we also report the results of a series of experiments with several data sets, including comparisons where we improve upon the performance of state-of-the-art algorithms both in binary and multi-class classification with multiple kernels."
451,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Variance Reduction via Antithetic Markov Chains,"James Neufeld, Dale Schuurmans, Michael Bowling",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/neufeld15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/neufeld15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_neufeld15,http://jmlr.csail.mit.edu/proceedings/papers/v38/neufeld15.html,"We present a Monte Carlo integration method, antithetic Markov chain sampling (AMCS), that incorporates local Markov transitions in an underlying importance sampler. Like sequential Monte Carlo sampling, the proposed method uses a sequence of Markov transitions to adapt the sampling to favour more influential regions of the integrand (modes). However, AMCS differs in the type of transitions that may be used, the number of Markov chains, and the method of chain termination. In particular, from each point sampled from an initial proposal, AMCS collects a sequence of points by simulating two independent, but antithetic, Markov chains, each terminated by a sample-dependent stopping rule. This approach provides greater flexibility for targeting influential areas while eliminating the need to fix the length of the Markov chain a priori. We show that the resulting estimator is unbiased and can reduce variance on peaked multi-modal integrands that challenge existing methods."
452,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Simple Geometric Interpretation of SVM using Stochastic Adversaries,"Roi Livni, Koby Crammer, Amir Globerson",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/livni12/livni12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_livni12,http://jmlr.csail.mit.edu/proceedings/papers/v22/livni12.html,We present a minimax framework for classification that considers stochastic adversarial perturbations to the training data. We show that for binary classification it is equivalent to SVM but with a very natural interpretation of regularization parameter. In the multiclass case we obtain that our formulation is equivalent to regularizing the hinge loss with the maximum norm of the weight vector (i.e. the two-infinity norm). We test this new regularization scheme and show that it is competitive with the Frobenius regularization commonly used for multiclass SVM. We proceed to analyze various forms of stochastic perturbations and obtain compact optimization problems for the optimal classifiers. Taken together our results illustrate the advantage of using stochastic perturbations rather than deterministic ones as well as offer a simple geometric interpretation for SVM optimization in the non-separable case.
453,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Hamiltonian Monte Carlo Without Detailed Balance,"Jascha Sohl-Dickstein, Mayur Mudigonda, Michael DeWeese",none,http://jmlr.org/proceedings/papers/v32/sohl-dickstein14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sohl-dickstein14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sohl-dickstein14.html,"We present a method for performing Hamiltonian Monte Carlo that largely eliminates sample rejection. In situations that would normally lead to rejection, instead a longer trajectory is computed until a new state is reached that can be accepted. This is achieved using Markov chain transitions that satisfy the fixed point equation, but do not satisfy detailed balance. The resulting algorithm significantly suppresses the random walk behavior and wasted function evaluations that are typically the consequence of update rejection. We demonstrate a greater than factor of two improvement in mixing time on three test problems. We release the source code as Python and MATLAB packages."
454,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Markov Mixed Membership Models,"Aonan Zhang, John Paisley",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangd15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhangd15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhangd15.html,"We present a Markov mixed membership model (Markov M3) for grouped data that learns a fully connected graph structure among mixing components. A key feature of Markov M3 is that it interprets the mixed membership assignment as a Markov random walk over this graph of nodes. This is in contrast to tree-structured models in which the assignment is done according to a tree structure on the mixing components. The Markov structure results in a simple parametric model that can learn a complex dependency structure between nodes, while still maintaining full conjugacy for closed-form stochastic variational inference. Empirical results demonstrate that Markov M3 performs well compared with tree structured topic models, and can learn meaningful dependency structure between topics."
455,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models,"Robert McGibbon, Bharath Ramsundar, Mohammad Sultan, Gert Kiss, Vijay Pande",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mcgibbon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mcgibbon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mcgibbon14.html,"We present a machine learning framework for modeling protein dynamics. Our approach uses L1-regularized, reversible hidden Markov models to understand large protein datasets generated via molecular dynamics simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for rational protein engineering and drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein responsible for signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein."
456,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Geometric Algorithm for Scalable Multiple Kernel Learning,"John Moeller, Parasaran Raman, Suresh Venkatasubramanian, Avishek Saha","JMLR W&CP 33 :633-642, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/moeller14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/moeller14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_moeller14,http://jmlr.csail.mit.edu/proceedings/papers/v33/moeller14.html,"We present a geometric formulation of the Multiple Kernel Learning (MKL) problem. To do so, we reinterpret the problem of learning kernel weights as searching for a kernel that maximizes the minimum (kernel) distance between two convex polytopes. This interpretation combined with additional structural insights from our geometric formulation allows us to reduce the MKL problem to a simple optimization routine that yields provable convergence as well as quality guarantees. As a result our method scales efficiently to much larger data sets than most prior methods can handle. Empirical evaluation on eleven datasets shows that we are significantly faster and even compare favorably with an uniform unweighted combination of kernels."
457,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Towards Understanding Situated Natural Language,"Antoine Bordes, Nicolas Usunier, Ronan Collobert, Jason Weston","9:65-72, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/bordes10a/bordes10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_bordes10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/bordes10a.html,We present a general framework and learning algorithm for the task of concept labeling: each word in a given sentence has to be tagged with the unique physical entity (e.g. person object or location) or abstract concept it refers to. Our method allows both world knowledge and linguistic information to be used during learning and prediction. We show experimentally that we can learn to use world knowledge to resolve ambiguities in language such as word senses or reference resolution without the use of handcrafted rules or features.
458,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A unifying representation for a class of dependent random measures,"Nicholas Foti, Joseph Futoma, Daniel Rockmore, Sinead Williamson",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/foti13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/foti13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_foti13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/foti13a.html,"We present a general construction for dependent random measures based on thinning Poisson processes on an augmented space. The framework is not restricted to dependent versions of a specific nonparametric model, but can be applied to all models that can be represented using completely random measures. Several existing dependent random measures can be seen as specific cases of this framework. Interesting properties of the resulting measures are derived and the efficacy of the framework is demonstrated by constructing a covariate-dependent latent feature model and topic model that obtain superior predictive performance."
459,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Reversible Jump MCMC for Non-Negative Matrix Factorization,"Mingjun Zhong, Mark Girolami","5:663-670, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/zhong09a/zhong09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_zhong09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/zhong09a.html,We present a fully Bayesian approach to Non-Negative Matrix Factorisation (NMF) by developing a Reversible Jump Markov Chain Monte Carlo (RJMCMC) method which provides full posteriors over the matrix components. In addition the NMF model selection issue is addressed for the first time as our RJMCMC procedure provides the posterior distribution over the matrix dimensions and therefore the number of components in the NMF model. A comparative analysis is provided with the Bayesian Information Criterion (BIC) and model selection employing estimates of the marginal likelihood. An illustrative synthetic example is provided using blind mixtures of images. This is then followed by a large scale study of the recovery of component spectra from multiplexed Raman readouts. The power and flexibility of the Bayesian methodology and the proposed RJMCMC procedure to objectively assess differing model structures and infer the corresponding plausible component spectra for this complex data is demonstrated convincingly.
460,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Spatiotemporal Feature Extraction with Data-Driven Koopman Operators,"Dimitrios Giannakis, Joanna Slawinska, Zhizhen Zhao",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/giannakis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_giannakis15,http://jmlr.csail.mit.edu/proceedings/papers/v44/giannakis15.html,"We present a framework for feature extraction and mode decomposition of spatiotemporal data generated by ergodic dynamical systems. Unlike feature extraction techniques based on kernel operators, our approach is to construct feature maps using eigenfunctions of the Koopman group of unitary operators governing the dynamical evolution of observables and probability measures. We compute the eigenvalues and eigenfunctions of the Koopman group through a Galerkin scheme applied to time-ordered data without requiring a priori knowledge of the dynamical evolution equations. This scheme employs a data-driven set of basis functions on the state space manifold, computed through the diffusion maps algorithm and a variable-bandwidth kernel designed to enforce orthogonality with respect to the invariant measure of the dynamics. The features extracted via this approach have strong timescale separation, favorable predictability properties, and high smoothness on the state space manifold. The extracted features are also invariant under weakly restrictive changes of observation modality. We apply this scheme to a synthetic dataset featuring superimposed traveling waves in a one-dimensional periodic domain and satellite observations of organized convection in the tropical atmosphere."
461,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Convex Optimization Framework for Bi-Clustering,"Shiau Hong Lim, Yudong Chen, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/limb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/limb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_limb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/limb15.html,"We present a framework for biclustering and clustering where the observations are general labels. Our approach is based on the maximum likelihood estimator and its convex relaxation, and generalizes recent works in graph clustering to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two domain sets, we show that the same algorithm can be as effective when clustering structure only occurs in one domain. This allows for an alternative approach to clustering that is more natural in some scenarios. We present theoretical results that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further validated by our empirical results on both synthetic and real data."
462,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,MOA Concept Drift Active Learning Strategies for Streaming Data,"Indre Zliobaite, Albert Bifet, Geoff Holmes, Bernhard Pfahringer","17:48-55, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/zliobaite11a/zliobaite11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_zliobaite11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/zliobaite11a.html,"We present a framework for active learning on evolving data streams, as an extension to the MOA system. In learning to classify streaming data, obtaining the true labels may require major effort and may incur excessive cost. Active learning focuses on learning an accurate model with as few labels as possible. Streaming data poses additional challenges for active learning, since the data distribution may change over time (concept drift) and classifiers need to adapt. Conventional active learning strategies concentrate on querying the most uncertain instances, which are typically concentrated around the decision boundary. If changes do not occur close to the boundary, they will be missed and classifiers will fail to adapt. We propose a software system that implements active learning strategies, extending the MOA framework. This software is released under the GNU GPL license."
463,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Dimensionality Reduction for Canonical Correlation Analysis,"Haim Avron, Christos Boutsidis, Sivan Toledo, Anastasios Zouzias",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/avron13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_avron13,http://jmlr.csail.mit.edu/proceedings/papers/v28/avron13.html,"We present a fast algorithm for approximate Canonical Correlation Analysis (CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies any standard CCA algorithm to the new pair of matrices. The algorithm computes an approximate CCA to the original pair of matrices with provable guarantees, while requiring asymptotically less operations than the state-of-the-art exact algorithms."
464,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Computation-Risk Tradeoffs for Covariance-Thresholded Regression,"Dinah Shender, John Lafferty",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/shender13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_shender13,http://jmlr.csail.mit.edu/proceedings/papers/v28/shender13.html,"We present a family of linear regression estimators that provides a fine-grained tradeoff between statistical accuracy and computational efficiency. The estimators are based on hard thresholding of the sample covariance matrix entries together with l2-regularizion(ridge regression). We analyze the predictive risk of this family of estimators as a function of the threshold and regularization parameter. With appropriate parameter choices, the estimate is the solution to a sparse, diagonally dominant linear system, solvable in near-linear time. Our analysis shows how the risk varies with the sparsity and regularization level, thus establishing a statistical estimation setting for which there is an explicit, smooth tradeoff between risk and computation. Simulations are provided to support the theoretical analyses."
465,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,That was fast! Speeding up NN search of high dimensional distributions.,"Emanuele Coviello, Adeel Mumtaz, Antoni Chan, Gert Lanckriet",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/coviello13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/coviello13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_coviello13,http://jmlr.csail.mit.edu/proceedings/papers/v28/coviello13.html,"We present a data structure for fast nearest neighbor retrieval of generative models of documents based on KL divergence. Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database, and uses a novel branch and bound methodology for search. The main technical contribution of the paper is a novel and efficient algorithm for deciding whether to explore nodes during backtracking, based on a variational approximation. This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data. In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions. Experiments demonstrate substantial speed-ups over both Bregman Ball Trees and over brute force search, on both moderate and high dimensional histogram data. In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models."
466,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,An LP for Sequential Learning Under Budgets,"Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama","JMLR W&CP 33 :987-995, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14b-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wang14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14b.html,"We present a convex framework to learn sequential decisions and apply this to the problem of learning under a budget. We consider the structure proposed [1], where sensor measurements are acquired in a sequence. The goal after acquiring each new measurement is to make a decision whether to stop and classify or to pay the cost of using the next sensor in the sequence. We introduce a novel formulation of an empirical risk objective for the multi stage sequential decision problem. This objective naturally lends itself to a non-convex multilinear formulation. Nevertheless, we derive a novel perspective that leads to a tight convex objective. This is accomplished by expressing the empirical risk in terms of linear superposition of indicator functions. We then derive an LP formulation by utilizing hinge loss surrogates. Our LP achieves or exceeds the empirical performance as the non-convex alternating algorithm that requires a large number of random initializations. Consequently, the LP has the advantage of guaranteed convergence, global optimality, repeatability and computation efficiency."
467,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts,"Tien Vu Nguyen, Dinh Phung, Xuanlong Nguyen, Swetha Venkatesh, Hung Bui",none,http://jmlr.org/proceedings/papers/v32/nguyenb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/nguyenb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_nguyenb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/nguyenb14.html,"We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-speci_c contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an ef_cient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains."
468,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,High Confidence Policy Improvement,"Philip Thomas, Georgios Theocharous, Mohammad Ghavamzadeh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/thomas15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_thomas15,http://jmlr.csail.mit.edu/proceedings/papers/v37/thomas15.html,"We present a batch reinforcement learning (RL) algorithm that provides probabilistic guarantees about the quality of each policy that it proposes, and which has no hyper-parameter that requires expert tuning. Specifically, the user may select any performance lower-bound and confidence level and our algorithm will ensure that the probability that it returns a policy with performance below the lower bound is at most the specified confidence level. We then propose an incremental algorithm that executes our policy improvement algorithm repeatedly to generate multiple policy improvements. We show the viability of our approach with a simple 4 x 4 gridworld and the standard mountain car problem, as well as with a digital marketing application that uses real world data."
469,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A conditional game for comparing approximations,Frederik Eaton,"15:63-71, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/eaton11a/eaton11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_eaton11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/eaton11a.html,We present a ``conditional game'' to be played between two approximate inference algorithms. We prove that exact inference is an optimal strategy and demonstrate how the game can be used to estimate the relative accuracy of two different approximations in the absence of exact marginals.
470,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Open Problem: Does AdaBoost Always Cycle?,"Cynthia Rudin, Robert E. Schapire, and Ingrid Daubechies",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/rudin12/rudin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_rudin12,http://jmlr.csail.mit.edu/proceedings/papers/v23/rudin12.html,We pose the question of whether the distributions computed by AdaBoost always converge to a cycle.
471,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,A simple multi-armed bandit algorithm with optimal variation-bounded regret,"Elad Hazan, Satyen Kale","19:819-822, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/hazan11b/hazan11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_hazan11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/hazan11b.html,We pose the question of whether it is possible to design a simple linear-time algorithm for the basic multi-armed bandit problem in the adversarial setting which has a regret bound of $O(\sqrt{Q \log T})$ where $Q$ is the total quadratic variation of all the arms.
472,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Towards a Learning Theory of Cause-Effect Inference,"David Lopez-Paz, Krikamol Muandet, Bernhard Sch_lkopf, Iliya Tolstikhin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lopez-paz15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/lopez-paz15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lopez-paz15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lopez-paz15.html,"We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection \(\{(S_i,l_i)\}_{i=1}^n\) , where each \(S_i\) is a sample drawn from the probability distribution of \(X_i \times Y_i\) , and \(l_i\) is a binary label indicating whether – \(X_i \to Y_i\) ” or – \(X_i \leftarrow Y_i\) ”. Given these data, we build a causal inference rule in two steps. First, we featurize each \(S_i\) using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables."
473,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: Learning Quantum Circuits with Queries,"Jeremy Kun, Lev Reyzin",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kun15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kun15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kun15.html,"We pose an open problem on the complexity of learning the behavior of a quantum circuit with value injection queries. We define the learning model for quantum circuits and give preliminary results. Using the test-path lemma of Angluin et al. (2009a), we show that new ideas are likely needed to tackle value injection queries for the quantum setting."
474,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Facial Expression Detection using Filtered Local Binary Pattern Features with ECOC Classifiers and Platt Scaling,Raymond S. Smith and Terry Windeatt,"11:111-118, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/smith10a/smith10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_smith10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/smith10a.html,We outline a design for a FACS-based facial expression recognition system and describe in more detail the implementation of two of its main components. Firstly we look at how features that are useful from a pattern analysis point of view can be extracted from a raw input image. We show that good results can be obtained by using the method of local binary patterns (LPB) to generate a large number of candidate features and then selecting from them using fast correlation-based filtering (FCBF). Secondly we show how Platt scaling can be used to improve the performance of an error-correcting output code (ECOC) classifier.
475,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Analysis of the KDD Cup 2009: Fast Scoring on a Large Orange Customer Database,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/guyon09/guyon09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_guyon09,http://jmlr.csail.mit.edu/proceedings/papers/v7/guyon09.html,"We organized the KDD cup 2009 around a marketing problem with the goal of identifying data mining techniques capable of rapidly building predictive models and scoring new entries on a large database. Customer Relationship Management (CRM) is a key element of modern marketing strategies. The KDD Cup 2009 offered the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling). The challenge started on March 10, 2009 and ended on May 11, 2009. This challenge attracted over 450 participants from 46 countries. We attribute the popularity of the challenge to several factors: (1) A generic problem relevant to the Industry (a classification problem), but presenting a number of scientific and technical challenges of real practical interest (many missing values, large number of features - 15000 - and large number of training examples - 50000, unbalanced class proportions - fewer than 10% of the examples of the positive class), noisy data, many missing values, presence of categorical variables with many different values. (2) Prizes - Orange offers 10000 Euros in prizes. (3) A well designed protocol and web site - we benefitted from past experience. (4) An effective advertising campaign using mailings and a teleconference to answer potential participants questions. The results of the challenge were discussed at the KDD conference (June 28, 2009). The principal conclusions are that ensemble methods are very effective and that ensemble of decision trees offer off-the-shelf solutions to problems with large numbers of samples and attributes, mixed types of variables, and lots of missing values. The data and the platform of the challenge remain available for research and educational purposes at http://www.kddcup-orange.com/ ."
476,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Results of the Active Learning Challenge,"I. Guyon, G. Cawley, G. Dror & V.Lemaire ; 16:19_45, 2011. [ abs ] [ pdf ]","16:19_45, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/guyon11a/guyon11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_guyon11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/guyon11a.html,We organized a machine learning challenge on ñactive learningî addressing problems where labeling data is expensive but large amounts of unlabeled data are available at low cost. Examples include handwriting and speech recognition document classi_cation vision tasks drug design using recombinant molecules and protein engineering. The algorithms may place a limited number of queries to get new sample labels. The design of the challenge and its results are summarized in this paper and the best contributions made by the participants are included in these proceedings. The website of the challenge remains open as a resource for students and researchers (  http://clopinet.com/al  ).   Page last modified on Wed Mar 30 11:09:05 2011.
477,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,First Connectomics Challenge: From Imaging to Connectivity,"Javier G. Orlandi, Bisakha Ray, Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Mehreen Saeed, Alexander Statnikov, Olav Stetter, Jordi Soriano",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/orlandi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_orlandi15,http://jmlr.csail.mit.edu/proceedings/papers/v46/orlandi15.html,"We organized a Challenge to unravel the connectivity of simulated neuronal networks. The provided data was solely based on fluorescence time series of spontaneous activity in a net- work constituted by 1000 neurons. The task of the participants was to compute the effective connectivity between neurons, with the goal to reconstruct as accurately as possible the ground truth topology of the network. The procured dataset is similar to the one measured in in vivo and in vitro recordings of calcium fluorescence imaging, and therefore the algorithms developed by the participants may largely contribute in the future to unravel major topological features of living neuronal networks from just the analysis of recorded data, and without the need of slow, painstaking experimental connectivity labeling methods. Among 143 entrants, 16 teams participated in the final round of the challenge to compete for prizes. The winners significantly outperformed the baseline method provided by the organizers. To measure influences between neurons the participants used an array of diverse methods, including transfer entropy, regression algorithms, correlation, deep learning, and network deconvolution. The development of connectivity reconstruction techniques is a major step in brain science, with many ramifications in the comprehension of neuronal computation, as well as the understanding of network dysfunctions in neuropathologies."
478,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Learning without concentration,Shahar Mendelson,"JMLR W&CP 35 :25-39, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/mendelson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_mendelson14,http://jmlr.csail.mit.edu/proceedings/papers/v35/mendelson14.html,"We obtain sharp bounds on the convergence rate of Empirical Risk Minimization performed in a convex class and with respect to the squared loss, without any boundedness assumptions on class members or on the target. Rather than resorting to a concentration-based argument, the method relies on a •small-ballê assumption and thus holds for heavy-tailed sampling and heavy-tailed targets. Moreover, the resulting estimates scale correctly with the •noise levelê of the problem. When applied to the classical, bounded scenario, the method always improves the known estimates."
479,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sensor Selection for Crowdsensing Dynamical Systems,"Francois Schnitzler, Jia Yuan Yu, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/schnitzler15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/schnitzler15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_schnitzler15,http://jmlr.csail.mit.edu/proceedings/papers/v38/schnitzler15.html,"We model crowdsensing as the selection of sensors with unknown variance to monitor a large linear dynamical system. To achieve low estimation error, we propose a Thompson sampling approach combining submodular optimization and a scalable online variational inference algorithm to maintain the posterior distribution over the variance. We also consider three alternative parameter estimation algorithms. We illustrate the behavior of our sensor selection algorithms on real traffic data from the city of Dublin. Our online algorithm achieves significantly lower estimation error than sensor selection using a fixed variance value for all sensors."
480,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Backpropagation and Approximate Inference in Deep Generative Models,"Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/rezende14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rezende14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rezende14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rezende14.html,"We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation _ rules for gradient backpropagation through stochastic variables _ and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation."
481,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sparse coding for multitask and transfer learning,"Andreas Maurer, Massi Pontil, Bernardino Romera-Paredes",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/maurer13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/maurer13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_maurer13,http://jmlr.csail.mit.edu/proceedings/papers/v28/maurer13.html,"We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping."
482,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Fast Mixing for Discrete Point Processes,"Patrick Rebeschini, Amin Karbasi",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Rebeschini15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Rebeschini15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Rebeschini15.html,"We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions \(\mu(S)\propto \exp(\beta f(S))\) over all subsets \(S\in 2^V\) of a finite set \(V\) through a bounded set function \(f:2^V\rightarrow \mathbb{R}\) and a parameter \(\beta_0\) . A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage. We show that if the set function (not necessarily submodular) displays a natural notion of decay of correlation, then, for \(\beta\) small enough, it is possible to design fast mixing Markov chain Monte Carlo methods that yield error bounds on marginal approximations that do not depend on the size of the set \(V\) . The sufficient conditions that we derive involve a control on the (discrete) Hessian of set functions, a quantity that has not been previously considered in the literature. We specialize our results for submodular functions, and we discuss canonical examples where the Hessian can be easily controlled."
483,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Student-t Processes as Alternatives to Gaussian Processes,"Amar Shah, Andrew Wilson, Zoubin Ghahramani","JMLR W&CP 33 :877-885, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/shah14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/shah14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_shah14,http://jmlr.csail.mit.edu/proceedings/papers/v33/shah14.html,"We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process _ a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels _ but has enhanced flexibility, and a predictive covariance that, unlike a Gaussian process, explicitly depends on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes."
484,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Convergence rate of Bayesian tensor estimator and its minimax optimality,Taiji Suzuki,none,http://jmlr.csail.mit.edu/proceedings/papers/v37/suzuki15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/suzuki15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_suzuki15,http://jmlr.csail.mit.edu/proceedings/papers/v37/suzuki15.html,"We investigate the statistical convergence rate of a Bayesian low-rank tensor estimator, and derive the minimax optimal rate for learning a low-rank tensor. Our problem setting is the regression problem where the regression coefficient forms a tensor structure. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate of the Bayes tensor estimator is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a fast learning rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. Finally, we show the minimax optimal learning rate for the tensor estimation problem, and thus show that the derived bound of the Bayes estimator is tight and actually near minimax optimal."
485,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,On Average Reward Policy Evaluation in Infinite-State Partially Observable Systems,"Yuri Grinberg, Doina Precup",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/grinberg12/grinberg12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_grinberg12,http://jmlr.csail.mit.edu/proceedings/papers/v22/grinberg12.html,We investigate the problem of estimating the average reward of given decision policies in discrete-time controllable dynamical systems with finite action and observation sets but possibly infinite state space. Unlike in systems with finite state spaces in infinite--state systems the expected reward for some policies might not exist so policy evaluation which is a key step in optimal control methods might fail. Our main analysis tool is Ergodic theory which allows learning potentially useful quantities from the system without building a model. Our main contribution is three-fold. First we present several dynamical systems that demonstrate the difficulty of learning in the general case without making additional assumptions. We state the necessary condition that the underlying system must satisfy to be amenable for learning. Second we discuss the relationship between this condition and state-of-the-art predictive representations and we show that there are systems that satisfy the above condition but cannot be modeled by such representations. Third we establish sufficient conditions for average-reward policy evaluation in this setting.
486,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Learning Tree Adjoining Grammars from Structures and Strings,Christophe Costa Florêncio,"21:129-132, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/florencio12a/florencio12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_florencio12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/florencio12a.html,"We investigate the learnability of certain subclasses of tree adjoining grammars (TAGs). TAGs are based on two tree-tree operations, and generate structures known as derived trees . The corresponding strings form a mildly context-sensitive language. We prove that even very constrained subclasses of TAGs are not learnable from structures (derived trees) or strings, demonstrating that this type of problem is far from trivial. We also demonstrate that a large (parameterized) family of classes of TAGs is learnable from strings."
487,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,PLAL: Cluster-based active learning,"Ruth Urner, Sharon Wulff, Shai Ben-David",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Urner13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Urner13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Urner13.html,"We investigate the label complexity of active learning under some smoothness assumptions on the data-generating process.We propose a procedure, PLAL, for –activising” passive, sample-based learners. The procedure takes an unlabeledsample, queries the labels of some of its members, and outputs a full labeling of that sample. Assuming the data satisfies –Probabilistic Lipschitzness”, a notion of clusterability, we show that for several common learning paradigms, applying our procedure as a preprocessing leads to provable label complexity reductions (over any –passive”learning algorithm, under the same data assumptions). Our labeling procedure is simple and easy to implement. We complement our theoretical findings with experimental validations."
488,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Clustering Oligarchies,"Margareta Ackerman, Shai Ben-David, David Loker, Sivan Sabato",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/ackerman13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_ackerman13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/ackerman13a.html,"We investigate the extent to which clustering algorithms are robust to the addition of a small, potentially adversarial, set of points. Our analysis reveals radical differences in the robustness of popular clustering methods. k-means and several related techniques are robust when data is clusterable, and we provide a quantitative analysis capturing the precise relationship between clusterability and robustness. In contrast, common linkage-based algorithms and several standard objective-function-based clustering methods can be highly sensitive to the addition of a small set of points even when the data is highly clusterable. We call such sets of points oligarchies. Lastly, we show that the behavior with respect to oligarchies of the popular Lloydês method changes radically with the initialization technique."
489,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Norm-Based Capacity Control in Neural Networks,"Behnam Neyshabur, Ryota Tomioka, Nathan Srebro",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Neyshabur15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Neyshabur15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Neyshabur15.html,"We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks."
490,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Online Learning Algorithm for Bilinear Models,"Yuanbin Wu, Shiliang Sun",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wua15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wua15.html,"We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint. A new online learning algorithm is proposed to train the model parameters. Our algorithm runs in the manner of online mirror descent, and gradients are computed by the power iteration. To analyze it, we give a new second order approximation of the squared spectral norm, which helps us to get a regret bound. Experiments on two sequential labelling tasks give positive results."
491,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Online Passive Aggressive Active Learning and Its Applications,"Jing Lu, Peilin Zhao, Steven Hoi",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/lu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_lu14,http://jmlr.csail.mit.edu/proceedings/papers/v39/lu14.html,"We investigate online active learning techniques for classification tasks in data stream mining applications. Unlike traditional learning approaches (either batch or online learning) that often require to request class label of each incoming instance, online active learning queries only a subset of informative incoming instances to update the classification model, which aims to maximize the classification performance using the minimal human labeling effort during the entire online stream data mining tasks. In this paper, we present a new family of algorithms for online active learning called Passive-Aggressive Active (PAA) learning algorithms by adapting the popular Passive-Aggressive algorithms in an online active learning setting. Unlike the conventional Perceptron-based approach that employs only the misclassified instances for updating the model, the proposed PAA learning algorithms not only use the misclassified instances to update the classifier, but also exploit those correctly classified examples yet with low prediction confidence. We theoretically analyze the mistakes bounds of the proposed algorithms and conduct extensive experiments to examine their empirical performance, in which the encouraging results show clear advantages of our algorithms over the baselines."
492,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Exponential Integration for Hamiltonian Monte Carlo,"Wei-Lun Chao, Justin Solomon, Dominik Michels, Fei Sha",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/chao15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chao15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chao15.html,"We investigate numerical integration of ordinary differential equations (ODEs) for Hamiltonian Monte Carlo (HMC). High-quality integration is crucial for designing efficient and effective proposals for HMC. While the standard method is leapfrog (Stormer-Verlet) integration, we propose the use of an exponential integrator, which is robust to stiff ODEs with highly-oscillatory components. This oscillation is difficult to reproduce using leapfrog integration, even with carefully selected integration parameters and preconditioning. Concretely, we use a Gaussian distribution approximation to segregate stiff components of the ODE. We integrate this term analytically for stability and account for deviation from the approximation using variation of constants. We consider various ways to derive Gaussian approximations and conduct extensive empirical studies applying the proposed –exponential HMC” to several benchmarked learning problems. We compare to state-of-the-art methods for improving leapfrog HMC and demonstrate the advantages of our method in generating many effective samples with high acceptance rates in short running times."
493,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A Finite Newton Algorithm for Non-degenerate Piecewise Linear Systems,"Xiao_Tong Yuan, Shuicheng Yan","15:841-854, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/yuan11a/yuan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_yuan11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/yuan11a.html,We investigate Newton-type optimization methods for solving piecewise linear systems (PLS) with non-degenerate coefficient matrix. Such systems arise for example from the numerical solution of linear complementarity problem which is useful to model several learning and optimization problems. In this paper we propose an effective damped Newton method namely PLS-DN to find the exact solution of non-degenerate PLS. PLS-DN exhibits provable semi-iterative property i.e. the algorithm converges globally to the exact solution in a finite number of iterations. The rate of convergence is shown to be at least linear before termination. We emphasize the applications of our method to modeling from a novel perspective of PLS several statistical learning problems such as elitist Lasso non-negative least squares and support vector machines. Numerical results on synthetic and benchmark data sets are presented to demonstrate the effectiveness and efficiency of PLS-DN on these problems.
494,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Convergent Decomposition Solvers for Tree-reweighted Free Energies,"Jeremy Jancsary, Gerald Matz","15:388-398, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/jancsary11a/jancsary11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_jancsary11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/jancsary11a.html,We investigate minimization of tree-reweighted free energies for the purpose of obtaining approximate marginal probabilities and upper bounds on the partition function of cyclic graphical models. The solvers we present for this problem work by directly tightening tree-reweighted upper bounds. As a result they are particularly efficient for tree-reweighted energies arising from a small number of spanning trees. While this assumption may seem restrictive at first we show how small sets of trees can be constructed in a principled manner. An appealing property of our algorithms which results from the problem decomposition is that they are embarassingly parallel. In contrast to the original message passing algorithm introduced for this problem we obtain global convergence guarantees.
495,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Clusterability: A Theoretical Study,"Margareta Ackerman, Shai Ben-David","5:1-8, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/ackerman09a/ackerman09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_ackerman09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/ackerman09a.html,We investigate measures of the clusterability of data sets. Namely ways to define how `strong' or `conclusive' is the clustering structure of a given data set. We address this issue with generality aiming for conclusions that apply regardless of any particular clustering algorithm or any specific data generation model. We survey several notions of clusterability that have been discussed in the literature as well as propose a new notion of data clusterability. Our comparison of these notions reveals that although they all attempt to evaluate the same intuitive property they are pairwise inconsistent. Our analysis discovers an interesting phenomenon; Although most of the common clustering tasks are NP-hard finding a close-to-optimal clustering for well-clusterable data sets is easy (computationally). We prove instances of this general claim with respect to the various clusterability notions that we discuss. Finally we investigate how hard it is to determine the clusterability value of a given data set. In most cases it turns out that this is an NP-hard problem.
496,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nonlinear Information-Theoretic Compressive Measurement Design,"Liming Wang, Abolfazl Razi, Miguel Rodrigues, Robert Calderbank, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangh14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangh14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangh14.html,"We investigate design of general nonlinear functions for mapping high-dimensional data into a lower-dimensional (compressive) space. The nonlinear measurements are assumed contaminated by additive Gaussian noise. Depending on the application, we are either interested in recovering the high-dimensional data from the nonlinear compressive measurements, or performing classification directly based on these measurements. The latter case corresponds to classification based on nonlinearly constituted and noisy features. The nonlinear measurement functions are designed based on constrained mutual-information optimization. New analytic results are developed for the gradient of mutual information in this setting, for arbitrary input-signal statistics. We make connections to kernel-based methods, such as the support vector machine. Encouraging results are presented on multiple datasets, for both signal recovery and classification. The nonlinear approach is shown to be particularly valuable in high-noise scenarios."
497,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Multi-armed Bandit Problem with Lock-up Periods,"Junpei Komiyama, Issei Sato, Hiroshi Nakagawa","JMLR W&CP 29 :100-115, 2013",http://jmlr.org/proceedings/papers/v29/Komiyama13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Komiyama13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Komiyama13.html,"We investigate a stochastic multi-armed bandit problem in which the forecasterês choice is restricted. In this problem, rounds are divided into lock-up periods and the forecaster must select the same arm throughout a period. While there has been much work on finding optimal algorithms for the stochastic multi-armed bandit problem, their use under restricted conditions is not obvious. We extend the application ranges of these algorithms by proposing their natural conversion from ones for the stochastic bandit problem (index-based algorithms and greedy algorithms) to ones for the multi-armed bandit problem with lock-up periods. We prove that the regret of the converted algorithms is \(O(\log{T} + L_{max} )\) , where \(T\) is the total number of rounds and \(L_{max}\) is the maximum size of the lock-up periods. The regret is preferable, except for the case when the maximum size of the lock-up periods is large. For these cases, we propose a meta-algorithm that results in a smaller regret by using a empirical best arm for large periods. We empirically compare and discuss these algorithms."
498,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Deep Learning using Robust Interdependent Codes,"Hugo Larochelle, Dumitru Erhan, Pascal Vincent","5:312-319, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/larochelle09a/larochelle09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_larochelle09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/larochelle09a.html,We investigate a simple yet effective method to introduce inhibitory and excitatory interactions between units in the layers of a deep neural network classifier. The method is based on the greedy layer-wise procedure of deep learning algorithms and extends the denoising autoencoder of Vincent~et~al.~\cite{VincentPLarochelleH2008-small} by adding asymmetric lateral connections between its hidden coding units in a manner that is much simpler and computationally more efficient than previously proposed approaches.We present experiments on two character recognition problems which show for the first time that lateral connections can significantly improve the classification performance of deep networks.
499,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Following the Perturbed Leader for Online Structured Learning,"Alon Cohen, Tamir Hazan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohena15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohena15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_cohena15,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohena15.html,"We investigate a new Follow the Perturbed Leader (FTPL) algorithm for online structured prediction problems. We show a regret bound which is comparable to the state of the art of FTPL algorithms and is comparable with the best possible regret in some cases. To better understand FTPL algorithms for online structured learning, we present a lower bound on the regret for a large and natural class of FTPL algorithms that use logconcave perturbations. We complete our investigation with an online shortest path experiment and empirically show that our algorithm is both statistically and computationally efficient."
500,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data,"Do-kyum Kim, Matthew Der, Lawrence Saul","JMLR W&CP 33 :484-492, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kim14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kim14a-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kim14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/kim14a.html,"We investigate a Gaussian latent variable model for semi-supervised learning of linear large margin classifiers. The modelês latent variables encode the signed distance of examples to the separating hyperplane, and we constrain these variables, for both labeled and unlabeled examples, to ensure that the classes are separated by a large margin. Our approach is based on similar intuitions as semi-supervised support vector machines (S3VMs), but these intuitions are formalized in a probabilistic framework. Within this framework we are able to derive an especially simple Expectation-Maximization (EM) algorithm for learning. The algorithm alternates between applying Bayes rule to –fill in” the latent variables (the E-step) and performing an unconstrained least-squares regression to update the weight vector (the M-step). For the best results it is necessary to constrain the unlabeled data to have a similar ratio of positive to negative examples as the labeled data. Within our model this constraint renders exact inference intractable, but we show that a Lyapunov central limit theorem (for sums of independent, but non-identical random variables) provides an excellent approximation to the true posterior distribution. We perform experiments on large-scale text classification and find that our model significantly outperforms existing implementations of S3VMs."
501,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multimodal Neural Language Models,"Ryan Kiros, Ruslan Salakhutdinov, Rich Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kiros14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kiros14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kiros14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kiros14.html,"We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio."
502,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Thurstonian Boltzmann Machines: Learning from Multiple Inequalities,"Truyen Tran, Dinh Phung, Svetha Venkatesh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tran13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/tran13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tran13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tran13.html,"We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis."
503,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A Spike and Slab Restricted Boltzmann Machine,"Aaron Courville, James Bergstra, Yoshua Bengio","15:233-241, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/courville11a/courville11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_courville11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/courville11a.html,We introduce the spike and slab Restricted Boltzmann Machine characterized by having both a real-valued vector the slab and a binary variable the spike associated with each unit in the hidden layer. The model possesses some practical properties such as being amenable to Block Gibbs sampling as well as being capable of generating similar latent representations of the data to the recently introduced mean and covariance Restricted Boltzmann Machine. We illustrate how the spike and slab Restricted Boltzmann Machine achieves competitive performance on the CIFAR-10 object recognition task.
504,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Active Pointillistic Pattern Search,"Yifei Ma, Dougal Sutherland, Roman Garnett, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/ma15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/ma15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_ma15,http://jmlr.csail.mit.edu/proceedings/papers/v38/ma15.html,"We introduce the problem of active pointillistic pattern search (APPS), which seeks to discover regions of a domain exhibiting desired behavior with limited observations. Unusually, the patterns we consider are defined by large-scale properties of an underlying function that we can only observe at a limited number of points. Given a description of the desired patterns (in the form of a classifier taking functional inputs), we sequentially decide where to query function values to identify as many regions matching the pattern as possible, with high confience. For one broad class of models the expected reward of each unobserved point can be computed analytically. We demonstrate the proposed algorithm on three difficult search problems: locating polluted regions in a lake via mobile sensors, forecasting winning electoral districts with minimal polling, and identifying vortices in a fluid flow simulation."
505,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Computing the M Most Probable Modes of a Graphical Model,"Chao Chen, Vladimir Kolmogorov, Yan Zhu, Dimitris Metaxas, Christoph Lampert",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_chen13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13a.html,"We introduce the M-modes problem for graphical models: predicting the M label configurations of highest probability that are at the same time local maxima of the probability landscape. M-modes have multiple possible applications: because they are intrinsically diverse, they provide a principled alternative to non-maximum suppression techniques for structured prediction, they can act as codebook vectors for quantizing the configuration space, or they can form component centers for mixture model approximation. We present two algorithms for solving the M-modes problem. The first algorithm solves the problem in polynomial time when the underlying graphical model is a simple chain. The second algorithm solves the problem for junction chains. In synthetic and real dataset, we demonstrate how M-modes can improve the performance of prediction. We also use the generated modes as a tool to understand the topography of the probability distribution of configurations, for example with relation to the training set size and amount of noise in the data."
506,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,The Laplacian Eigenmaps Latent Variable Model,"Miguel A. Carreira-Perpi_an, Zhengdong Lu","2:59-66, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/carreira-perpinan07a/carreira-perpinan07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_carreira-perpinan07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/carreira-perpinan07a.html,We introduce the Laplacian Eigenmaps Latent Variable Model (LELVM) a probabilistic method for nonlinear dimensionality reduction that combines the advantages of spectral methods--global optimisation and ability to learn convoluted manifolds of high intrinsic dimensionality--with those of latent variable models--dimensionality reduction and reconstruction mappings and a density model. We derive LELVM by defining a natural out-of-sample mapping for Laplacian eigenmaps using a semi-supervised learning argument. LELVM is simple nonparametric and computationally not very costly and is shown to perform well with motion-capture data.
507,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Structured Prediction of Network Response,"Hongyu Su, Aristides Gionis, Juho Rousu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/su14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/su14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_su14,http://jmlr.csail.mit.edu/proceedings/papers/v32/su14.html,"We introduce the following network response problem: given a complex network and an action, predict the subnetwork that responds to action, that is, which nodes perform the action and which directed edges relay the action to the adjacent nodes. We approach the problem through max-margin structured learning, in which a compatibility score is learned between the actions and their activated subnetworks. Thus, unlike the most popular influence network approaches, our method, called SPIN, is context-sensitive, namely, the presence, the direction and the dynamics of influences depend on the properties of the actions. The inference problems of finding the highest scoring as well as the worst margin violating networks, are proven to be NP-hard. To solve the problems, we present an approximate inference method through a semi-definite programming relaxation (SDP), as well as a more scalable greedy heuristic algorithm. In our experiments, we demonstrate that taking advantage of the context given by the actions and the network structure leads SPIN to a markedly better predictive performance over competing methods."
508,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Modular Autoencoders for Ensemble Feature Extraction,"Henry Reeve, Gavin Brown",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/reeve15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_reeve15a,http://jmlr.csail.mit.edu/proceedings/papers/v44/reeve15a.html,"We introduce the concept of a Modular Autoencoder (MAE), capable of learning a set of diverse but complementary representations from unlabelled data, that can later be used for supervised tasks. The learning of the representations is controlled by a trade-off parameter, and we show on six benchmark datasets the optimum lies between two extremes: a set of smaller, independent autoencoders each with low capacity, versus a single monolithic encoding, outperforming an appropriate baseline. In the present paper we explore the special case of linear MAE, and derive an SVD-based algorithm which converges several orders of magnitude faster than gradient descent."
509,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Memory-efficient inference in dynamic graphical models using multiple cores,"Galen Andrew, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/andrew12/andrew12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_andrew12,http://jmlr.csail.mit.edu/proceedings/papers/v22/andrew12.html,We introduce the archipelagos algorithm for memory-efficient multi-core inference in dynamic graphical models. By making use of several processors running in parallel the archipelagos algorithm uses exponentially less memory compared to basic forward-backward message passing algorithms (O(log T) compared to O(T) on sequences of length T) and under often-satisfied assumptions on the relative speed of passing forward and backward messages runs no slower. We also describe a simple variant of the algorithm that achieves a factor of two speedup over forward-backward on a single core. Experiments with our implementation of archipelagos for the computation of posterior marginal probabilities in an HMM validate the space/time complexity analysis: using four cores the required memory on our test problem was reduced from 8 GB to 319 KB (a factor of 25000) relative to forward-backward but completed in essentially the same time. The archipelagos algorithm applies to any dynamic graphical model including dynamic Bayesian networks conditional random fields and hidden conditional random fields.
510,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration,Volodymyr Kuleshov,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kuleshov13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kuleshov13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kuleshov13.html,"We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with \(k\) non-zero elements at a cost of \(O(nk + k^3)\) flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be interpreted as a type of second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data."
511,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Mixed LICORS: A Nonparametric Algorithm for Predictive State Reconstruction,"Georg Goerg, Cosma Shalizi",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/goerg13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/goerg13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_goerg13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/goerg13a.html,"We introduce mixed LICORS, an algorithm for learning nonlinear, high-dimensional dynamics from spatio-temporal data, suitable for both prediction and simulation. Mixed LICORS extends the recent LICORS algorithm (Goerg and Shalizi, 2012) from hard clustering of predictive distributions to a non-parametric, EM-like soft clustering. This retains the asymptotic predictive optimality of LICORS, but, as we show in simulations, greatly improves out-of-sample forecasts with limited data. The new method is implemented in the publicly-available R package LICORS."
512,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Efficient Training of LDA on a GPU by Mean-for-Mode Estimation,"Jean-Baptiste Tristan, Joseph Tassarotti, Guy Steele",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tristan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tristan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tristan15.html,"We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler ã and unlike an uncollapsed Gibbs sampler ã it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler ã and unlike a collapsed Gibbs sampler ã it is embarrassingly parallel, and can use approximate counters."
513,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Gaussian Margin Machines,"Koby Crammer, Mehryar Mohri, Fernando Pereira","5:105-112, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/crammer09a/crammer09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_crammer09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/crammer09a.html,We introduce Gaussian Margin Machines (GMMs) which maintain a Gaussian distribu- tion over weight vectors for binary classi_cation. The learning algorithm for these machines seeks the least informative distribution that will classify the training data correctly with high probability. One formulation can be expressed as a convex constrained optimization problem whose solution can be represented linearly in terms of training instances and their inner and outer products supporting kernelization. The algorithm has a natural PAC-Bayesian generalization bound. A preliminary evaluation on handwriting recognition data shows that our algorithm improves over SVMs for the same task. methods we maintain a distribution over alternative weight vectors rather than committing to a single speci_c one. However these distributions are not derived by Bayes? rule. Instead they represent our knowledge of the weights given constraints imposed by the training examples. Speci_cally we use a Gaussian distribution over weight vectors with mean and covariance parameters that are learned from the training data. The learning algorithm seeks for a distribu- tion with a small Kullback-Leibler (KL) divergence from a _xed isotropic distribution such that each training exam- ple is correctly classi_ed by a strict majority of the weight vectors. Conceptually this is a large-margin probabilistic principle instead of the geometric large margin principle in SVMs. The learning problem for GMMs can be expressed as a convex constrained optimization and its optimal solution
514,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Regularization of Neural Networks using DropConnect,"Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wan13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wan13.html,"We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models."
515,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Deep Canonical Correlation Analysis,"Galen Andrew, Raman Arora, Jeff Bilmes, Karen Livescu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/andrew13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_andrew13,http://jmlr.csail.mit.edu/proceedings/papers/v28/andrew13.html,"We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation. It can be viewed as a nonlinear extension of the linear method canonical correlation analysis (CCA). It is an alternative to the nonparametric method kernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances. In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks."
516,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,BilBOWA: Fast Bilingual Distributed Representations without Word Alignments,"Stephan Gouws, Yoshua Bengio, Greg Corrado",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gouws15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gouws15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gouws15.html,"We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data."
517,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Generalization Bounds for Supervised Dimensionality Reduction,"Mehryar Mohri, Afshin Rostamizadeh, Dmitry Storcheus",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/mohri2015generalization.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_mohri2015generalization,http://jmlr.csail.mit.edu/proceedings/papers/v44/mohri2015generalization.html,"We introduce and study the learning scenario of supervised dimensionality reduction , which couples dimensionality reduction and a subsequent supervised learning step. We present new generalization bounds for this scenario based on a careful analysis of the empirical Rademacher complexity of the relevant hypothesis set. In particular, we show an upper bound on the Rademacher complexity that is in \(\widetilde O(\sqrt{{\Lambda_{(r)}}/m})\) , where \(m\) is the sample size and \({\Lambda_{(r)}}\) the upper bound on the Ky-Fan \(r\) -norm of the operator that defines the dimensionality reduction projection. We give both upper and lower bound guarantees in terms of that Ky-Fan \(r\) -norm, which strongly justifies the definition of our hypothesis set. To the best of our knowledge, these are the first learning guarantees for the problem of supervised dimensionality reduction with a learned kernel-based mapping. Our analysis and learning guarantees further apply to several special cases, such as that of using a fixed kernel with supervised dimensionality reduction or that of unsupervised learning of a kernel for dimensionality reduction followed by a supervised learning algorithm."
518,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning from Contagion (Without Timestamps),"Kareem Amin, Hoda Heidari, Michael Kearns",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/amin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/amin14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_amin14,http://jmlr.csail.mit.edu/proceedings/papers/v32/amin14.html,We introduce and study new models for learning from contagion processes in a network. A learning algorithm is allowed to either choose or passively observe an initial set of seed infections. This seed set then induces a final set of infections resulting from the underlying stochastic contagion dynamics. Our models differ from prior work in that detailed vertex-by-vertex timestamps for the spread of the contagion are not observed. The goal of learning is to infer the unknown network structure. Our main theoretical results are efficient and provably correct algorithms for exactly learning trees. We provide empirical evidence that our algorithm performs well more generally on realistic sparse graphs.
519,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A New Approach to Probabilistic Programming Inference,"Frank Wood, Jan Willem van de Meent, Vikash Mansinghka","JMLR W&CP 33 :1024-1032, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wood14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wood14,http://jmlr.csail.mit.edu/proceedings/papers/v33/wood14.html,"We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is easy to implement and to parallelize, applies to Turing-complete probabilistic programming languages, and supports accurate inference in models that make use of complex control flow, including stochastic recursion, as well as primitives from nonparametric Bayesian statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings samplers."
520,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Unified Energy-Based Framework for Unsupervised Learning,"Marc'Aurelio Ranzato, Y-Lan Boureau, Sumit Chopra, Yann LeCun","2:371-379, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/ranzato07a/ranzato07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_ranzato07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/ranzato07a.html,"We introduce a view of unsupervised learning that integrates probabilistic and nonprobabilistic methods for clustering dimensionality reduction and feature extraction in a unified framework. In this framework an energy function associates low energies to input points that are similar to training samples and high energies to unobserved points. Learning consists in minimizing the energies of training samples while ensuring that the energies of unobserved ones are higher. Some traditional methods construct the architecture so that only a small number of points can have low energy while other methods explicitly ""pull up"" on the energies of unobserved points. In probabilistic methods the energy of unobserved points is pulled by minimizing the log partition function an expensive and sometimes intractable process. We explore different and more efficient methods using an energy-based approach. In particular we show that a simple solution is to restrict the amount of information contained in codes that represent the data. We demonstrate such a method by training it on natural image patches and by applying to image denoising."
521,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Bayesian Gaussian Process Latent Variable Model,"Michalis Titsias, Neil Lawrence","9:844-851, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/titsias10a/titsias10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_titsias10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/titsias10a.html,We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems but the methodology is more general. For example our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.
522,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Stay on path: PCA along graph paths,"Megasthenis Asteris, Anastasios Kyrillidis, Alex Dimakis, Han-Gyol Yi, Bharath Chandrasekaran",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/asteris15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/asteris15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_asteris15,http://jmlr.csail.mit.edu/proceedings/papers/v37/asteris15.html,"We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model for sparse PCA. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets."
523,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,ABC Reinforcement Learning,"Christos Dimitrakakis, Nikolaos Tziortziotis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/dimitrakakis13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_dimitrakakis13,http://jmlr.csail.mit.edu/proceedings/papers/v28/dimitrakakis13.html,"We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound."
524,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization,"Shai Shalev-Shwartz, Tong Zhang",none,http://jmlr.org/proceedings/papers/v32/shalev-shwartz14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_shalev-shwartz14,http://jmlr.csail.mit.edu/proceedings/papers/v32/shalev-shwartz14.html,"We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings."
525,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deep Generative Stochastic Networks Trainable by Backprop,"Yoshua Bengio, Eric Laufer, Guillaume Alain, Jason Yosinski",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bengio14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bengio14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bengio14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bengio14.html,"We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining."
526,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Online-to-Confidence-Set Conversions and Application to Sparse Stochastic Bandits,"Yasin Abbasi-Yadkori, David Pal, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/abbasi-yadkori12/abbasi-yadkori12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_abbasi-yadkori12,http://jmlr.csail.mit.edu/proceedings/papers/v22/abbasi-yadkori12.html,We introduce a novel technique which we call online-to-confidence-set conversion. The technique allows us to construct high-probability confidence sets for linear prediction with correlated inputs given the predictions of any algorithm (e.g. online LASSO exponentiated gradient algorithm online least-squares p-norm algorithm) targeting online learning with linear predictors and the quadratic loss. By construction the size of the confidence set is directly governed by the regret of the online learning algorithm. Constructing tight confidence sets is interesting on its own but the new technique is given extra weight by the fact having access tight confidence sets underlies a number of important problems. The advantage of our construction here is that progress in constructing better algorithms for online prediction problems directly translates into tighter confidence sets. In this paper this is demonstrated in the case of linear stochastic bandits. In particular we introduce the sparse variant of linear stochastic bandits and show that a recent online algorithm together with our online-to-confidence-set conversion allows one to derive algorithms that can exploit if the reward is a function of a sparse linear combination of the components of the chosen action.
527,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Approximate inference using conditional entropy decompositions,"Amir Globerson, Tommi Jaakkola","2:130-138, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/globerson07a/globerson07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_globerson07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/globerson07a.html,We introduce a novel method for estimating the partition function and marginals of distributions defined using graphical models. The method uses the entropy chain rule to obtain an upper bound on the entropy of a distribution given marginal distributions of variable subsets. The structure of the bound is determined by a permutation or elimination order of the model variables. Optimizing this bound results in an upper bound on the log partition function and also yields an approximation to the model marginals. The optimization problem is convex and is in fact a dual of a geometric program. We evaluate the method on a 2D Ising model with a wide range of parameters and show that it compares favorably with previous methods in terms of both partition function bound and accuracy of marginals.
528,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Visualizing pairwise similarity via semidefinite programming,"Amir Globerson, Sam Roweis","2:139-146, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/globerson07b/globerson07b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_globerson07b,http://jmlr.csail.mit.edu/proceedings/papers/v2/globerson07b.html,We introduce a novel learning algorithm for binary pairwise similarity measurements on a set of objects. The algorithm delivers an embedding of the objects into a vector representation space that strictly respects the known similarities in the sense that objects known to be similar are always closer in the embedding than those known to be dissimilar. Subject to this constraint our method selects the mapping in which the variance of the embedded points is maximized. This has the effect of favoring embeddings with low effective dimensionality. The related optimization problem can be cast as a convex Semidefinite Program (SDP). We also present a parametric version of the problem which can be used for embedding out of sample points. The parametric version uses kernels to obtain nonlinear maps and can also be solved using an SDP. We apply the two algorithms to an image embedding problem where it effectively captures the low dimensional structure corresponding to camera viewing parameters.
529,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Asymmetric Transfer Learning with Deep Gaussian Processes,Melih Kandemir,none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kandemir15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kandemir15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kandemir15.html,"We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images."
530,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Online Clustering of Bandits,"Claudio Gentile, Shuai Li, Giovanni Zappella",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gentile14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gentile14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gentile14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gentile14.html,"We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (``bandit"") strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems."
531,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nonnegative Sparse PCA with Provable Guarantees,"Megasthenis Asteris, Dimitris Papailiopoulos, Alexandros Dimakis",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/asteris14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/asteris14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_asteris14,http://jmlr.csail.mit.edu/proceedings/papers/v32/asteris14.html,"We introduce a novel algorithm to compute nonnegative sparse principal components of positive semidefinite (PSD) matrices. Our algorithm comes with approximation guarantees contingent on the spectral profile of the input matrix A: the sharper the eigenvalue decay, the better the approximation quality. If the eigenvalues decay like any asymptotically vanishing function, we can approximate nonnegative sparse PCA within any accuracy \(\epsilon\) in time polynomial in the matrix size \(n\) and desired sparsity k, but not in \(1/\epsilon\) . Further, we obtain a data-dependent bound that is computed by executing an algorithm on a given data set. This bound is significantly tighter than a-priori bounds and can be used to show that for all tested datasets our algorithm is provably within 40%-90% from the unknown optimum. Our algorithm is combinatorial and explores a subspace defined by the leading eigenvectors of A. We test our scheme on several data sets, showing that it matches or outperforms the previous state of the art."
532,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sparse PCA through Low-rank Approximations,"Dimitris Papailiopoulos, Alexandros Dimakis, Stavros Korokythakis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/papailiopoulos13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_papailiopoulos13,http://jmlr.csail.mit.edu/proceedings/papers/v28/papailiopoulos13.html,"We introduce a novel algorithm that computes the \(k\) -sparse principal component of a positive semidefinite matrix \(A\) . Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of \(A\) . We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of \(A\) follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. We implement our algorithm and test it on multiple artificial and real data sets. Due to a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets."
533,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Weight Uncertainty in Neural Network,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/blundell15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_blundell15,http://jmlr.csail.mit.edu/proceedings/papers/v37/blundell15.html,"We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning."
534,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Reliable and Scalable Variational Inference for the Hierarchical Dirichlet Process,"Michael Hughes, Dae Il Kim, Erik Sudderth",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hughes15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/hughes15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hughes15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hughes15.html,"We introduce a new variational inference objective for hierarchical Dirichlet process admixture models. Our approach provides novel and scalable algorithms for learning nonparametric topic models of text documents and Gaussian admixture models of image patches. Improving on the point estimates of topic probabilities used in previous work, we define full variational posteriors for all latent variables and optimize parameters via a novel surrogate likelihood bound. We show that this approach has crucial advantages for data-driven learning of the number of topics. Via merge and delete moves that remove redundant or irrelevant topics, we learn compact and interpretable models with less computation. Scaling to millions of documents is possible using stochastic or memoized variational updates."
535,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP),"Andrew Wilson, Hannes Nickisch",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wilson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wilson15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wilson15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wilson15.html,"We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling."
536,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Spectral Dimensionality Reduction via Maximum Entropy,Neil Lawrence,"15:51-59, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/lawrence11a/lawrence11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_lawrence11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/lawrence11a.html,We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting probabilistic models are based on GRFs. The resulting model is a nonlinear generalization of principal component analysis. We show that parameter fitting in the locally linear embedding is approximate maximum likelihood in these models. We develop new algorithms that directly maximize the likelihood and show that these new algorithms are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set.
537,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Online Skill Discovery using Graph-based Clustering,Jan Hendrik Metzen,"24:77-88, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/metzen12a/metzen12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_metzen12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/metzen12a.html,"We introduce a new online skill discovery method for reinforcement learning in discrete domains. The method is based on the bottleneck principle and identifies skills using a bottom-up hierarchical clustering of the estimated transition graph. In contrast to prior clustering approaches it can be used incrementally and thus several times during the learning process. Our empirical evaluation shows that ""assuming dense local connectivity in the face of uncertainty"" can prevent premature identification of skills. Furthermore we show that the choice of the linkage criterion is crucial for dealing with non-random sampling policies and stochastic environments."
538,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Spectral Chinese Restaurant Processes: Nonparametric Clustering Based on Similarities,"Richard Socher, Andrew Maas, Christopher Manning","15:698-706, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/socher11a/socher11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_socher11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/socher11a.html,We introduce a new nonparametric clustering model which combines the recently proposed distance-dependent Chinese restaurant process (dd-CRP) and non-linear spectral methods for dimensionality reduction. Our model retains the ability of nonparametric methods to learn the number of clusters from data. At the same time it addresses two key limitations of nonparametric Bayesian methods: modeling data that are not exchangeable and have many correlated features. Spectral methods use the similarity between documents to map them into a low-dimensional spectral space where we then compare several clustering methods. Our experiments on handwritten digits and text documents show that nonparametric methods such as the CRP or dd-CRP can perform as well as or better than k-means and also recover the true number of clusters. We improve the performance of the dd-CRP in spectral space by incorporating the original similarity matrix in its prior. This simple modification results in better performance than all other methods we compared to. We offer a new formulation and first experimental evaluation of a general Gibbs sampler for mixture modeling with distance-dependent CRPs.
539,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Fast Multi-stage Submodular Maximization,"Kai Wei, Rishabh Iyer, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wei14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wei14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wei14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wei14.html,"We introduce a new multi-stage algorithmic framework for submodular maximization. We are motivated by extremely large scale machine learning problems, where both storing the whole data for function evaluation and running the standard accelerated greedy algorithm are prohibitive. We propose a multi-stage framework (called MultGreed), where at each stage we apply an approximate greedy procedure to maximize surrogate submodular functions. The surrogates serve as proxies for a target submodular function but require less memory and are easy to evaluate. We theoretically analyze the performance guarantee of the multi-stage framework, and give examples on how to design instances of MultGreed for a broad range of natural submodular functions. We show that MultGreed performs very close to the standard greedy algorithm, given appropriate surrogate functions, and argue how our framework can easily be integrated with distributive algorithms for optimization. We complement our theory by empirically evaluating on several real world problems, including data subset selection on millions of speech samples, where MultGreed yields at least a thousand times speedup and superior results over the state-of-the-art selection methods."
540,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Learning Using Local Membership Queries,"Pranjal Awasthi, Vitaly Feldman, Varun Kanade",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Awasthi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Awasthi13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Awasthi13.html,"We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are close to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant,1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points). Membership query algorithms are not popular among machine learning practitioners. Apart from the obvious difficulty of adaptively querying labellers, it has also been observed that querying unnatural points leads to increased noise from human labellers (Lang and Baum, 1992). This motivates our study of learning algorithms that make queries that are close to examples generated from the data distribution. We restrict our attention to functions defined on the n-dimensional Boolean hypercube and say that a membership query is local if its Hamming distance from some example in the (random) training data is at most \(O(log(n))\) . We show the following results in this model: The class of sparse polynomials (with coefficients in \(R\) ) over \(\{0, 1\}^n\) is polynomial time learnable under a large class of locally smooth distributions using \(O(log(n))\) -local queries. This class also includes the class of \(O(log(n))\) -depth decision trees. The class of polynomial-sized decision trees is polynomial time learnable under product distributions using \(O(log(n))\) -local queries. The class of polynomial size DNF formulas is learnable under the uniform distribution using \(O(log(n))\) -local queries in time \(n^{O(log(log(n)))}\) . In addition we prove a number of results relating the proposed model to the traditional PAC model and the PAC+MQ model."
541,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,The Group Dantzig Selector,"Han Liu, Jian Zhang, Xiaoye Jiang, Jun Liu","9:461-468, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/liu10a/liu10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_liu10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/liu10a.html,"We introduce a new method -- the group Dantzig selector -- for high dimensional sparse regression with group structure which has a convincing theory about why utilizing the group structure can be beneficial. Under a group restricted isometry condition we obtain a significantly improved nonasymptotic L2-norm bound over the basis pursuit or the Dantzig selector which ignores the group structure. To gain more insight we also introduce a surprisingly simple and intuitive ""sparsity oracle condition"" to obtain a block L1-norm bound which is easily accessible to a broad audience in machine learning community. Encouraging numerical results are also provided to support our theory."
542,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Hybrid Approach for Probabilistic Inference using Random Projections,"Michael Zhu, Stefano Ermon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhuc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhuc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhuc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhuc15.html,"We introduce a new meta-algorithm for probabilistic inference in graphical models based on random projections. The key idea is to use approximate inference algorithms for an (exponentially) large number of samples, obtained by randomly projecting the original statistical model using universal hash functions. In the case where the approximate inference algorithm is a variational approximation, this approach can be viewed as interpolating between sampling-based and variational techniques. The number of samples used controls the trade-off between the accuracy of the approximate inference algorithm and the variance of the estimator. We show empirically that by using random projections, we can improve the accuracy of common approximate inference algorithms."
543,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Convex QP Relaxations for Structured Prediction,"Jeremy Jancsary, Sebastian Nowozin, Carsten Rother",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jancsary13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jancsary13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jancsary13.html,"We introduce a new large margin approach to discriminative training of intractable discrete graphical models. Our approach builds on a convex quadratic programming relaxation of the MAP inference problem. The model parameters are trained directly within this restricted class of energy functions so as to optimize the predictions on the training data. We address the issue of how to parameterize the resulting model and point out its relation to existing approaches. The primary motivation behind our use of the QP relaxation is its computational efficiency; yet, empirically, its predictive accuracy compares favorably to more expensive approaches. This makes it an appealing choice for many practical tasks."
544,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Linear and Parallel Learning of Markov Random Fields,"Yariv Mizrahi, Misha Denil, Nando De Freitas",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mizrahi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mizrahi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mizrahi14.html,"We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields which is efficient for a large class of practical models. Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters."
545,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension,"Hamed Firouzi, Bala Rajaratnam, Alfred Hero III",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/firouzi13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_firouzi13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/firouzi13a.html,"We introduce a new approach to variable selection, called Predictive Correlation Screening, for predictor design. Predictive Correlation Screening (PCS) implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for Familywise Error Rate (FWER), and resultant mean square error of a linear predictor on the selected variables. We apply Predictive Correlation Screening to the following two-stage predictor design problem. An experimenter wants to learn a multivariate predictor of gene expressions based on successive biological samples assayed on mRNA arrays. She assays the whole genome on a few samples and from these assays she selects a small number of variables using Predictive Correlation Screening. To reduce assay cost, she subsequently assays only the selected variables on the remaining samples, to learn the predictor coefficients. We show superiority of Predictive Correlation Screening relative to LASSO and correlation learning (sometimes popularly referred to in the literature as marginal regression or simple thresholding) in terms of performance and computational complexity."
546,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Hidden Markov Anomaly Detection,"Nico Goernitz, Mikio Braun, Marius Kloft",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/goernitz15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/goernitz15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_goernitz15,http://jmlr.csail.mit.edu/proceedings/papers/v37/goernitz15.html,"We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM."
547,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Weighted Classification Cascades for Optimizing Discovery Significance in the HiggsML Challenge,"Lester Mackey, Jordan Bryan, Man Yue Mo",none,http://jmlr.csail.mit.edu/proceedings/papers/v42/mack14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_mack14,http://jmlr.csail.mit.edu/proceedings/papers/v42/mack14.html,"We introduce a minorization-maximization approach to optimizing common measures of discovery significance in high energy physics. The approach alternates between solving a weighted binary classification problem and updating class weights in a simple, closed-form manner. Moreover, an argument based on convex duality shows that an improvement in weighted classification error on any round yields a commensurate improvement in discovery significance. We complement our derivation with experimental results from the 2014 Higgs boson machine learning challenge."
548,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Sample-based approximate regularization,"Philip Bachman, Amir-Massoud Farahmand, Doina Precup",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bachman14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bachman14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bachman14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bachman14.html,"We introduce a method for regularizing linearly parameterized functions using general derivative-based penalties, which relies on sampling as well as finite-difference approximations of the relevant derivatives. We call this approach sample-based approximate regularization (SAR). We provide theoretical guarantees on the fidelity of such regularizers, compared to those they approximate, and prove that the approximations converge efficiently. We also examine the empirical performance of SAR on several datasets."
549,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Active Learning of Parameterized Skills,"Bruno Da Silva, George Konidaris, Andrew Barto",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/silva14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/silva14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_silva14,http://jmlr.csail.mit.edu/proceedings/papers/v32/silva14.html,"We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains."
550,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Nearly-Linear Time Framework for Graph-Structured Sparsity,"Chinmay Hegde, Piotr Indyk, Ludwig Schmidt",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hegde15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hegde15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hegde15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hegde15.html,"We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms improve on prior work also in practice."
551,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Localization and Adaptation in Online Learning,"Alexander Rakhlin, Ohad Shamir, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/rakhlin13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_rakhlin13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/rakhlin13a.html,"We introduce a formalism of localization for online learning problems, which, similarly to statistical learning theory, can be used to obtain fast rates. In particular, we introduce local sequential Rademacher complexities and other local measures. Based on the idea of relaxations for deriving algorithms, we provide a template method that takes advantage of localization. Furthermore, we build a general adaptive method that can take advantage of the suboptimality of the observed sequence. We illustrate the utility of the introduced concepts on several problems. Among them is a novel upper bound on regret in terms of classical Rademacher complexity when the data are i.i.d."
552,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Trend Filtering on Graphs,"Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan Tibshirani",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15d-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wang15d,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15d.html,"We introduce a family of adaptive estimators on graphs, based on penalizing the \(\ell_1\) norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009, Tibshirani, 2014) used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual \(\ell_2\) -based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory."
553,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian Group Factor Analysis,"Seppo Virtanen, Arto Klami, Suleiman Khan, Samuel Kaski",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/virtanen12/virtanen12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_virtanen12,http://jmlr.csail.mit.edu/proceedings/papers/v22/virtanen12.html,We introduce a factor analysis model that summarizes the dependencies between observed variable groups instead of dependencies between individual variables as standard factor analysis does. A group may correspond to one view of the same set of objects one of many data sets tied by co-occurrence or a set of alternative variables collected from statistics tables to measure one property of interest. We show that by assuming group-wise sparse factors active in a subset of the sets the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation. We formulate the assumptions in a Bayesian model providing the factors and apply the model to two data analysis tasks in neuroimaging and chemical systems biology.
554,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deep AutoRegressive Networks,"Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gregor14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gregor14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gregor14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gregor14.html,"We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games."
555,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Submodular Point Processes with Applications to Machine learning,"Rishabh Iyer, Jeffrey Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/iyer15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_iyer15,http://jmlr.csail.mit.edu/proceedings/papers/v38/iyer15.html,"We introduce a class of discrete point processes that we call the Submodular Point Processes (SPPs) . These processes are characterized via a submodular (or supermodular) function, and naturally model notions of information, coverage and diversity , as well as cooperation . Unlike Log-submodular and Log-supermodular distributions (Log-SPPs) such as determinantal point processes (DPPs), SPPs are themselves submodular (or supermodular). In this paper, we analyze the computational complexity of probabilistic inference in SPPs. We show that computing the partition function for SPPs (and Log-SPPs), requires exponential complexity in the worst case, and also provide algorithms which approximate SPPs up to polynomial factors. Moreover, for several subclasses of interesting submodular functions that occur in applications, we show how we can provide efficient closed form expressions for the partition functions, and thereby marginals and conditional distributions. We also show how SPPs are closed under mixtures, thus enabling maximum likelihood based strategies for learning mixtures of submodular functions. Finally, we argue how SPPs complement existing Log-SPP distributions, and are a natural model for several applications."
556,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bayesian Switching Interaction Analysis Under Uncertainty,"Zoran Dzunic, John Fisher III","JMLR W&CP 33 :220-228, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/dzunic14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_dzunic14,http://jmlr.csail.mit.edu/proceedings/papers/v33/dzunic14.html,"We introduce a Bayesian discrete-time framework for switching-interaction analysis under uncertainty, in which latent interactions, switching pattern and signal states and dynamics are inferred from noisy (and possibly missing) observations of these signals. We propose reasoning over full posterior distribution of these latent variables as a means of combating and characterizing uncertainty. This approach also allows for answering a variety of questions probabilistically, which is suitable for exploratory pattern discovery and post-analysis by human experts. This framework is based on a fully-Bayesian learning of the structure of a switching dynamic Bayesian network (DBN) and utilizes a state-space approach to allow for noisy observations and missing data. It generalizes the autoregressive switching interaction model of Siracusa et al., which does not allow observation noise, and the switching linear dynamic system model of Fox et al., which does not infer interactions among signals. Posterior samples are obtained via a Gibbs sampling procedure, which is particularly efficient in the case of linear Gaussian dynamics and observation models. We demonstrate the utility of our framework on a controlled human-generated data, and a real-world climate data."
557,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Exact and Approximate Sampling by Systematic Stochastic Search,"Vikash Mansinghka, Daniel Roy, Eric Jonas, Joshua Tenenbaum","5:400-407, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/mansinghka09a/mansinghka09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_mansinghka09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/mansinghka09a.html,We introduce _adaptive sequential rejection sampling_ an algorithm for generating exact samples from high-dimensional discrete distributions building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation and a randomized version of the directed arc consistency algorithm with backtracking when applied to deterministic problems. In this paper we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on ferromagnetic Isings and other probabilistic graphical models obtaining exact and approximate samples in a range of situations.
558,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Tightness of maximum likelihood semidefinite relaxations,"Afonso S. Bandeira, Yuehaw Khoo, Amit Singer","JMLR W&CP 35 :1265-1267, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/bandeira14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_bandeira14,http://jmlr.csail.mit.edu/proceedings/papers/v35/bandeira14.html,"We have observed an interesting, yet unexplained, phenomenon: Semidefinite programming (SDP) based relaxations of maximum likelihood estimators (MLE) tend to be tight in recovery problems with noisy data, even when MLE cannot exactly recover the ground truth. Several results establish tightness of SDP based relaxations in the regime where exact recovery from MLE is possible. However, to the best of our knowledge, their tightness is not understood beyond this regime. As an illustrative example, we focus on the generalized Procrustes problem."
559,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Feature Selection for Linear SVM with Provable Guarantees,"Saurabh Paul, Malik Magdon-Ismail, Petros Drineas",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/paul15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/paul15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_paul15,http://jmlr.csail.mit.edu/proceedings/papers/v38/paul15.html,"We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within \(\epsilon\) -relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. We present extensive experiments on real-world datasets to support our theory and to demonstrate that our methods are competitive and often better than prior state-of-the-art, for which there are no known provable guarantees."
560,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A New Perspective on Learning Linear Separators with Large \(L_qL_p\) Margins,"Maria-Florina Balcan, Christopher Berlind","JMLR W&CP 33 :68-76, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/balcan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/balcan14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_balcan14,http://jmlr.csail.mit.edu/proceedings/papers/v33/balcan14.html,We give theoretical and empirical results that provide new insights into large margin learning. We prove a bound on the generalization error of learning linear separators with large \(L_qL_p\) margins (where \(L_q\) and \(L_p\) are dual norms) for any finite \(p \ge 1\) . The bound leads to a simple data-dependent sufficient condition for fast learning in addition to extending and improving upon previous results. We also provide the first study that shows the benefits of taking advantage of margins with \(p _ 2\) over margins with \(p \ge 2\) . Our experiments confirm that our theoretical results are relevant in practice.
561,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Learning Halfspaces Under Log-Concave Densities: Polynomial Approximations and Moment Matching,"Daniel Kane, Adam Klivans, Raghu Meka",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Kane13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Kane13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Kane13.html,"We give the first polynomial-time algorithm for agnostically learning any function of a constant number of halfspaces with respect to any log-concave distribution (for any constant accuracy parameter). This result was not known even for the case of PAC learning the intersection of two halfspaces. We give two very different proofs of this result. The first develops a theory of polynomial approximation for log-concave measures and constructs a low-degree \(L_1\) polynomial approximator for sufficiently smooth functions. The second uses techniques related to the classical moment problem to obtain sandwiching polynomials. Both approaches deviate significantly from known Fourier-based methods, where essentially all previous work required the underlying distribution to have some product structure. Additionally, we show that in the smoothed-analysis setting, the above results hold with respect to distributions that have sub-exponential tails, a property satisfied by many natural and well-studied distributions in machine learning."
562,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Fast matrix completion without the condition number,"Moritz Hardt, Mary Wootters","JMLR W&CP 35 :638-678, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/hardt14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_hardt14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/hardt14a.html,"We give the first algorithm for Matrix Completion that achieves running time and sample complexity that is polynomial in the rank of the unknown target matrix, linear in the dimension of the matrix, and logarithmic in the condition number of the matrix. To the best of our knowledge, all previous algorithms either incurred a quadratic dependence on the condition number of the unknown matrix or a quadratic dependence on the dimension of the matrix. Our algorithm is based on a novel extension of Alternating Minimization which we show has theoretical guarantees under standard assumptions even in the presence of noise."
563,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sparse Solutions to Nonnegative Linear Systems and Applications,"Aditya Bhaskara, Ananda Suresh, Morteza Zadimoghaddam",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/bhaskara15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/bhaskara15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_bhaskara15,http://jmlr.csail.mit.edu/proceedings/papers/v38/bhaskara15.html,"We give an efficient algorithm for finding sparse approximate solutions to linear systems of equations with nonnegative coefficients. Unlike most known results for sparse recovery, we do not require any assumption on the matrix other than non-negativity. Our algorithm is combinatorial in nature, inspired by techniques for the –set cover” problem, as well as the multiplicative weight update method. We then present a natural application to learning mixture models in the PAC framework. For learning a mixture of \(k\) axis-aligned Gaussians in \(d\) dimensions, we give an algorithm that outputs a mixture of \(O(k/\epsilon^3)\) Gaussians that is \(\epsilon\) -close in statistical distance to the true distribution, without any separation assumptions. The time and sample complexity is roughly \(O(kd/\epsilon^3)^d\) . This is polynomial when \(d\) is constant _ precisely the regime in which known methods fail to identify the components efficiently. Given that non-negativity is a natural assumption, we believe that our result may find use in other settings in which we wish to approximately –explain” data using a small number of a (large) candidate set of components."
564,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Provable Bounds for Learning Some Deep Representations,"Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma",none,http://jmlr.org/proceedings/papers/v32/arora14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_arora14,http://jmlr.csail.mit.edu/proceedings/papers/v32/arora14.html,"We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer neural net that has degree at most \(n^{\gamma}\) for some \(\gamma _ 1\) and each edge has a random edge weight in [-1,1]. Our algorithm learns almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural nets with random edge weights."
565,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability,"Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan","JMLR W&CP 35 :742-778, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/bhaskara14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_bhaskara14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/bhaskara14a.html,"We give a robust version of the celebrated result of Kruskal on the uniqueness of tensor decompositions: given a tensor whose decomposition satisfies a robust form of Kruskalês rank condition, we prove that it is possible to approximately recover the decomposition if the tensor is known up to a sufficiently small (inverse polynomial) error. Kruskalês theorem has found many applications in proving the identifiability of parameters for various latent variable models and mixture models such as Hidden Markov models, topic models etc. Our robust version immediately implies identifiability using only polynomially many samples in many of these settings _ an essential first step towards efficient learning algorithms. Our methods also apply to the –overcomplete” case, which has proved challenging in many applications. Given the importance of Kruskalês theorem in the tensor literature, we expect that our robust version will have several applications beyond the settings we explore in this work."
566,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization,"Elad Hazan, Satyen Kale","19:421-436, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/hazan11a/hazan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_hazan11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/hazan11a.html,We give a novel algorithm for stochastic strongly-convex optimization in thegradient oracle model which returns an $O(\frac{1}{T})$-approximate solutionafter $T$ gradient updates. This rate of convergence is optimal in the gradientoracle model. This improves upon the previously known best rate of$O(\frac{\log(T)}{T})$ which was obtained by applying an onlinestrongly-convex optimization algorithm with regret $O(\log(T))$ to the batchsetting.We complement this result by proving that any algorithm has expected regret of$\Omega(\log(T))$ in the online stochastic strongly-convex optimizationsetting. This lower bound holds even in the full-information setting whichreveals more information to the algorithm than just gradients. This shows thatany online-to-batch conversion is inherently suboptimal for stochasticstrongly-convex optimization. This is the first formal evidence that onlineconvex optimization is strictly more difficult than batch stochastic convexoptimization.
567,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Error bounds for Kernel Fisher Linear Discriminant in Gaussian Hilbert space,"Robert Durrant, Ata Kaban",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/durrant12/durrant12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_durrant12,http://jmlr.csail.mit.edu/proceedings/papers/v22/durrant12.html,We give a non-trivial non-asymptotic upper bound on the classification error of the popular Kernel Fisher Linear Discriminant classifier under the assumption that the kernel-induced space is a Gaussian Hilbert space.
568,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,On Estimating L2 2 Divergence,"Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/krishnamurthy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/krishnamurthy15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_krishnamurthy15,http://jmlr.csail.mit.edu/proceedings/papers/v38/krishnamurthy15.html,"We give a comprehensive theoretical characterization of a nonparametric estimator for the \(L_2^2\) divergence between two continuous distributions. We first bound the rate of convergence of our estimator, showing that it is \(\sqrt{n}\) -consistent provided the densities are sufficiently smooth. In this smooth regime, we then show that our estimator is asymptotically normal, construct asymptotic confidence intervals, and establish a Berry-Ess_en style inequality characterizing the rate of convergence to normality. We also show that this estimator is minimax optimal."
569,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Spectral Regularization for Max-Margin Sequence Tagging,"Ariadna Quattoni, Borja Balle, Xavier Carreras, Amir Globerson",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/quattoni14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_quattoni14,http://jmlr.csail.mit.edu/proceedings/papers/v32/quattoni14.html,"We frame max-margin learning of latent variable structured prediction models as a convex optimization problem, making use of scoring functions computed by input-output observable operator models. This learning problem can be expressed as an optimization involving a low-rank Hankel matrix that represents the input-output operator model. The direct outcome of our work is a new spectral regularization method for max-margin structured prediction. Our experiments confirm that our proposed regularization framework leads to an effective way of controlling the capacity of structured prediction models."
570,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Loopy Belief Propagation for Bipartite Maximum Weight b-Matching,"Bert Huang, Tony Jebara","2:195-202, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/huang07a/huang07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_huang07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/huang07a.html,We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically the resulting algorithm is on average faster than popular combinatorial implementations while still scaling at the same asymptotic rate of $O(bn^3)$. Furthermore the algorithm shows promising performance in machine learning applications.
571,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Information-Theoretic Characterization of Sparse Recovery,"Cem Aksoylar, Venkatesh Saligrama","JMLR W&CP 33 :38-46, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/aksoylar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/aksoylar14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_aksoylar14,http://jmlr.csail.mit.edu/proceedings/papers/v33/aksoylar14.html,"We formulate sparse support recovery as a salient set identification problem and use information-theoretic analyses to characterize the recovery performance and sample complexity. We consider a very general framework where we are not restricted to linear models or specific distributions. We state non-asymptotic bounds on recovery probability and a tight mutual information formula for sample complexity. We evaluate our bounds for applications such as sparse linear regression and explicitly characterize effects of correlation or noisy features on recovery performance. We show improvements upon previous work and identify gaps between the performance of recovery algorithms and fundamental information. This illustrates a trade-off between computational complexity and sample complexity, contrasting the recovery of the support as a discrete object with signal estimation approaches."
572,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Multi-label Classi_cation with Error-correcting Codes,C.-S. Ferng & H.-T. Lin,"20:281_295, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/ferng11/ferng11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_ferng11,http://jmlr.csail.mit.edu/proceedings/papers/v20/ferng11.html,We formulate a framework for applying error-correcting codes (ECC) on multi-label classi_cation problems. The framework treats some base learners as noisy channels and uses ECC to correct the prediction errors made by the learners. An immediate use of the framework is a novel ECC-based explanation of the popular random k -label-sets (RAKEL) algorithm using a simple repetition ECC. Using the framework we empirically compare a broad spectrum of ECC designs for multi-label classi_cation. The results not only demonstrate that RAKEL can be improved by applying some stronger ECC but also show that the traditional Binary Relevance approach can be enhanced by learning more parity-checking labels. In addition our study on di_erent ECC helps understand the trade-o_ between the strength of ECC and the hardness of the base learning tasks.   Page last modified on Sun Nov 6 15:43:59 2011.
573,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Estimation of Generating Processes of Strings Represented with Patterns and Refinements,Keisuke Otaki and Akihiro Yamamoto,"21:177-182, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/otaki12a/otaki12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_otaki12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/otaki12a.html,We formalize generating processes of strings based on patterns and substitutions and give an algorithm to estimate a probability mass function on substitutions which is an element of processes. Patterns are non-empty sequences of characters and variables. Variables indicate unknown substrings and are replaced with other patterns by substitutions. By introducing variables and substitutions we can deal with the difficulty of preparing production rules in generative grammar and of representing context-sensitivity with them. Our key idea is to regard sequences of substitutions as generations of strings and to give probabilities of substitutions like PCFG. In this study after giving a problem to estimate a probability mass function from strings based on our formalization we solve it by the Passing algorithm that runs in an iterative manner. Our experimental results with synthetic strings show that our method estimates probability mass functions with sufficient small errors.
574,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Qualitative Multi-Armed Bandits: A Quantile-Based Approach,"Balazs Szorenyi, Robert Busa-Fekete, Paul Weng, Eyke Hôllermeier",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/szorenyi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/szorenyi15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_szorenyi15,http://jmlr.csail.mit.edu/proceedings/papers/v37/szorenyi15.html,"We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies."
575,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Anti-differentiating approximation algorithms:A case study with min-cuts, spectral, and flow","David Gleich, Michael Mahoney",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gleich14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gleich14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gleich14.html,"We formalize and illustrate the general concept of algorithmic anti-differentiation: given an algorithmic procedure, e.g., an approximation algorithm for which worst-case approximation guarantees are available or a heuristic that has been engineered to be practically-useful but for which a precise theoretical understanding is lacking, an algorithmic anti-derivative is a precise statement of an optimization problem that is exactly solved by that procedure. We explore this concept with a case study of approximation algorithms for finding locally-biased partitions in data graphs, demonstrating connections between min-cut objectives, a personalized version of the popular PageRank vector, and the highly effective –push” procedure for computing an approximation to personalized PageRank. We show, for example, that this latter algorithm solves (exactly, but implicitly) an l1-regularized l2-regression problem, a fact that helps to explain its excellent performance in practice. We expect that, when available, these implicit optimization problems will be critical for rationalizing and predicting the performance of many approximation algorithms on realistic data."
576,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Optimal rates for stochastic convex optimization under Tsybakov noise condition,"Aaditya Ramdas, Aarti Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ramdas13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ramdas13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ramdas13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ramdas13.html,"We focus on the problem of minimizing a convex function \(f\) over a convex set \(S\) given \(T\) queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum \(x^*_{f,S}\) , as quantified by a Tsybakov-like noise condition. Specifically, we prove that if \(f\) grows at least as fast as \(\|x-x^*_{f,S}\|^\kappa\) around its minimum, for some \(\kappa _ 1\) , then the optimal rate of learning \(f(x^*_{f,S})\) is \(\Theta(T^{-\frac{\kappa}{2\kappa-2}})\) . The classic rate \(\Theta(1/\sqrt T)\) for convex functions and \(\Theta(1/T)\) for strongly convex functions are special cases of our result for \(\kappa \rightarrow \infty\) and \(\kappa=2\) , and even faster rates are attained for \(1 _ \kappa _ 2\) . We also derive tight bounds for the complexity of learning \(x_{f,S}^*\) , where the optimal rate is \(\Theta(T^{-\frac{1}{2\kappa-2}})\) . Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries."
577,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Margins, Kernels and Non-linear Smoothed Perceptrons","Aaditya Ramdas, Javier Pe_a",none,http://jmlr.org/proceedings/papers/v32/ramdas14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ramdas14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ramdas14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ramdas14.html,"We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms. We cast our problem as one of maximizing the regularized normalized hard-margin ( \(\rho\) ) in an RKHS and use the Representer Theorem to rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernelês (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \(\tfrac{\sqrt {\log n}}{\rho}\) given \(n\) separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is \(\tfrac1{\rho^2}\) . When no such classifier exists, we prove a version of Gordanês separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in \(\min\{\tfrac{\sqrt n}{|\rho|}, \tfrac{\sqrt n}{\epsilon}\}\) iterations with a perfect separator in the RKHS if the primal is feasible or a dual \(\epsilon\) -certificate of near-infeasibility."
578,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Two-stage sampled learning theory on distributions,"Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/szabo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/szabo15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_szabo15,http://jmlr.csail.mit.edu/proceedings/papers/v38/szabo15.html,"We focus on the distribution regression problem: regressing to a real-valued response from a probability distribution. Although there exist a large number of similarity measures between distributions, very little is known about their generalization performance in specific learning tasks. Learning problems formulated on distributions have an inherent two-stage sampled difficulty: in practice only samples from sampled distributions are observable, and one has to build an estimate on similarities computed between sets of points. To the best of our knowledge, the only existing method with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which suffers from slow convergence issues in high dimensions), and the domain of the distributions to be compact Euclidean. In this paper, we provide theoretical guarantees for a remarkably simple algorithmic alternative to solve the distribution regression problem: embed the distributions to a reproducing kernel Hilbert space, and learn a ridge regressor from the embeddings to the outputs. Our main contribution is to prove the consistency of this technique in the two-stage sampled setting under mild conditions (on separable, topological domains endowed with kernels). As a special case, we answer a 15-year-old open question: we establish the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression, and cover more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010]."
579,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Closure-Based Confidence Boost in Association Rules,Jos_ L. Balcˆzar,"11:74-80, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/balcazar10a/balcazar10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_balcazar10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/balcazar10a.html,"We focus on association rule mining. It is well-known that naive miners end up often providing far too large amounts of mined associations to result actually useful in practice. Many proposals exist for selecting appropriate association rules, trying to measure their interest in various ways; most of these approaches are statistical in nature, or share their main traits with statistical notions. Alternatively, some existing notions of redundancy among association rules allow for a logical-style characterization and lead to irredundant bases (axiomatizations) of absolutely minimum size. Here we follow up on a study of closure-based redundancy, which, in practice, leads to smaller bases than simpler alternative forms of redundancy, with the proviso that, in principle, they need to be complemented with an implicational basis. One can push the intuition of redundancy further and gain a perspective of the interest of association rules in terms of their ""novelty"" with respect to other rules. An irredundant rule is so because its confidence is higher than what the rest of the rules would suggest; then, one can ask: how much higher? Among several variants, a recently proposed parameter, the confidence boost, succeeds in measuring a notion of novelty along these lines so that it fits better the needs of practical applications. However, that notion is based on plain redundancy, of relatively limited practical usefulness. Here we extend the confidence boost to closure-based redundancy, paying a small theoretical price to obtain several advantages in practical applications. We describe a rule-mining system implementing this contribution."
580,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,An Adaptive Accelerated Proximal Gradient Method and its Homotopy Continuation for Sparse Optimization,"Qihang Lin, Lin Xiao",none,http://jmlr.org/proceedings/papers/v32/lin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lin14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lin14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lin14.html,"We first propose an adaptive accelerated proximal gradient(APG) method for minimizing strongly convex composite functions with unknown convexity parameters. This method incorporates a restarting scheme to automatically estimate the strong convexity parameter and achieves a nearly optimal iteration complexity. Then we consider the _1-regularized least-squares (_1-LS) problem in the high-dimensional setting. Although such an objective function is not strongly convex, it has restricted strong convexity over sparse vectors. We exploit this property by combining the adaptive APG method with a homotopy continuation scheme, which generates a sparse solution path towards optimality. This method obtains a global linear rate of convergence and its overall iteration complexity has a weaker dependency on the restricted condition number than previous work."
581,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Q-learning for history-based reinforcement learning,"Mayank Daswani, Peter Sunehag, Marcus Hutter","JMLR W&CP 29 :213-228, 2013",http://jmlr.org/proceedings/papers/v29/Daswani13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Daswani13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Daswani13.html,"We extend the Q-learning algorithm from the Markov Decision Process setting to problems where observations are non-Markov and do not reveal the full state of the world i.e. to POMDPs. We do this in a natural manner by adding \(\ell_0\) regularisation to the pathwise squared Q-learning objective function and then optimise this over both a choice of map from history to states and the resulting MDP parameters. The optimisation procedure involves a stochastic search over the map class nested with classical Q-learning of the parameters. This algorithm fits perfectly into the feature reinforcement learning framework, which chooses maps based on a cost criteria. The cost criterion used so far for feature reinforcement learning has been model-based and aimed at predicting future states and rewards. Instead we directly predict the return, which is what is needed for choosing optimal actions. Our Q-learning criteria also lends itself immediately to a function approximation setting where features are chosen based on the history. This algorithm is somewhat similar to the recent line of work on lasso temporal difference learning which aims at finding a small feature set with which one can perform policy evaluation. The distinction is that we aim directly for learning the Q-function of the optimal policy and we use \(\ell_0\) instead of \(\ell_1\) regularisation. We perform an experimental evaluation on classical benchmark domains and find improvement in convergence speed as well as in economy of the state representation. We also compare against MC-AIXI on the large Pocman domain and achieve competitive performance in average reward. We use less than half the CPU time and 36 times less memory. Overall, our algorithm hQL provides a better combination of computational, memory and data efficiency than existing algorithms in this setting."
582,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Estimation Consistency of the Group Lasso and its Applications,"Han Liu, Jian Zhang","5:376-383, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/liu09a/liu09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_liu09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/liu09a.html,We extend the $\ell_2$-consistency result of (Meinshausen and Yu 2008) from the Lasso to the group Lasso. Our main theorem shows that the group Lasso achieves estimation consistency under a mild condition and an asymptotic upper bound on the number of selected variables can be obtained. As a result we can apply the nonnegative garrote procedure to the group Lasso result to obtain an estimator which is simultaneously estimation and variable selection consistent. In particular our setting allows both the number of groups and the number of variables per group increase and thus is applicable to high-dimensional problems. We also provide estimation consistency analysis for a version of the sparse additive models with increasing dimensions. Some finite-sample results are also reported.
583,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Kernelized Bayesian Matrix Factorization,"Mehmet G_nen, Suleiman Khan, Samuel Kaski",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gonen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/gonen13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gonen13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/gonen13a.html,"We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms."
584,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,"Safe Learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity",Peter Grônwald,"19:397-420, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/grunwald11a/grunwald11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_grunwald11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/grunwald11a.html,We extend Bayesian MAP and Minimum Description Length (MDL) learning by testing whether the data can be substantially more compressed by a mixture of the MDL/MAP distribution with another element of themodel and adjusting the learning rate if this is the case. While standard Bayes and MDL can fail to converge ifthe model is wrong the resulting ``safe'' estimator continues toachieve good rates with wrong models. Moreover when applied toclassification and regression models as considered in statisticallearning theory the approach achieves optimal rates under e.g.Tsybakov's conditions and reveals new situations in which we canpenalize by $(- \log \text{\sc prior})/n$ rather than $\sqrt{(- \log \text{\sc prior})/n}$.
585,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,The Deep Feed-Forward Gaussian Process: An Effective Generalization to Covariance Priors,"Melih Kandemir, Fred A. Hamprecht",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/kandemir15jmlr.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_kandemir15jmlr,http://jmlr.csail.mit.edu/proceedings/papers/v44/kandemir15jmlr.html,"We explore ways of applying a prior on the covariance matrix of a Gaussian Process (GP) in order to increase its expressive power. We show that two well-known covariance priors, Wishart Process and Inverse Wishart Process, boil down to a two-layer feed-forward net- work of GPs with a particular kernel function on the neuron at the output layer. Both of these models perform supervised manifold learning and target prediction jointly. Also, the resultant kernel functions of both of these priors lead to feature maps of finite dimen- sionality. Motivated by this fact, we promote replacing these kernels with the Radial Basis Function (RBF), which gives an infinite dimensional feature map, enhancing the model flex- ibility. We demonstrate on one benchmark task and two challenging medical image analysis tasks that our GP network with RBF kernel largely outperforms the earlier two covariance priors. We show also that it straightforwardly allows non-linear combination of different data views, leading to state-of-the-art multiple kernel learning only as a by-product."
586,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Parallel Gibbs Sampling: From Colored Fields to Thin Junction Trees,"Joseph Gonzalez, Yucheng Low, Arthur Gretton, Carlos Guestrin","15:324-332, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/gonzalez11a/gonzalez11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_gonzalez11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/gonzalez11a.html,We explore the task of constructing a parallel Gibbs sampler to both improve mixing and the exploration of high likelihood states. Recent work in parallel Gibbs sampling has focused on update schedules which do not guarantee convergence to the intended stationary distribution. In this work we propose two methods to construct parallel Gibbs samplers guaranteed to draw from the targeted distribution. The first method called the Chromatic sampler uses graph coloring to construct a direct parallelization of the classic sequential scan Gibbs sampler. In the case of 2-colorable models we relate the Chromatic sampler to the Synchronous Gibbs sampler (which draws all variables simultaneously in parallel) and reveal new ergodic properties of Synchronous Gibbs chains. Our second method the Splash sampler is a complementary strategy which can be used when the variables are tightly coupled. This constructs and samples multiple blocks in parallel using a novel locking protocol and an iterative junction tree generation algorithm. We further improve the Splash sampler through adaptive tree construction. We demonstrate the benefits of our two sampling algorithms on large synthetic and real-world models using a 32 processor multi-core system.
587,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Benefits of Learning with Strongly Convex Approximate Inference,"Ben London, Bert Huang, Lise Getoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/london15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/london15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_london15,http://jmlr.csail.mit.edu/proceedings/papers/v37/london15.html,"We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for \(\Omega(1)\) -strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework."
588,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Discriminative Topic Segmentation of Text and Speech,"Mehryar Mohri, Pedro Moreno, Eugene Weinstein","9:533-540, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/mohri10a/mohri10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_mohri10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/mohri10a.html,We explore automated discovery of topically-coherent segments in speech or text sequences. We give two new discriminative topic segmentation algorithms which employ a new measure of text similarity based on word co-occurrence. Both algorithms function by finding extrema in the similarity signal over the text with the latter algorithm using a compact support-vector based description of a window of text or speech observations in word similarity space to overcome noise introduced by speech recognition errors and off-topic content. In experiments over speech and text news streams we show that these algorithms outperform previous methods. We observe that topic segmentation of speech recognizer output is a more difficult problem than that of text streams; however we demonstrate that by using a lattice of competing hypotheses rather than just the one-best hypothesis as input to the segmentation algorithm the performance of the algorithm can be improved.
589,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Identifiability of Priors from Bounded Sample Sizes with Applications to Transfer Learning,"Liu Yang, Steve Hanneke, Jaime Carbonell","19:791-808, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/yang11a/yang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_yang11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/yang11a.html,We explore a transfer learning setting in which a finite sequence of target concepts are sampled independently with an unknown distribution from a known family.We study the total number of labeled examples required to learn all targets to an arbitrary specified expected accuracy focusing on the asymptotics in the number of tasks and the desired accuracy. Our primary interest is formally understanding the fundamental benefits of transfer learning compared to learning each target independently from the others.Our approach to the transfer problem is general in the sense that it can be used with a variety of learning protocols. The key insight driving our approach is that the distribution of the target concepts is identifiable from the joint distribution over a number of random labeled data points equal the Vapnik-Chervonenkis dimension of the concept space. This is not necessarily the case for the joint distribution over any smaller number of points.This work has particularly interesting implications when applied to active learning methods.
590,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,K-hyperplane Hinge-Minimax Classifier,"Margarita Osadchy, Tamir Hazan, Daniel Keren",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/osadchy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_osadchy15,http://jmlr.csail.mit.edu/proceedings/papers/v37/osadchy15.html,"We explore a novel approach to upper bound the misclassification error for problems with data comprising a small number of positive samples and a large number of negative samples. We assign the hinge-loss to upper bound the misclassification error of the positive examples and use the minimax risk to upper bound the misclassification error with respect to the worst case distribution that generates the negative examples. This approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, in contrast to kernel SVM which produces a very large number of support vectors in such settings. We derive empirical risk bounds for linear and non-linear classification and show that they are dimensionally independent and decay as \(1/\sqrt{m}\) for \(m\) samples. We propose an efficient algorithm for training an intersection of finite number of hyperplane and demonstrate its effectiveness on real data, including letter and scene recognition."
591,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Random Projections as Regularizers: Learning a Linear Discriminant Ensemble from Fewer Observations than Dimensions,"Robert Durrant, Ata Kaban","JMLR W&CP 29 :17-32, 2013",http://jmlr.org/proceedings/papers/v29/Durrant13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Durrant13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Durrant13.html,"We examine the performance of an ensemble of randomly-projected Fisher Linear Discriminant classifiers, focusing on the case when there are fewer training observations than data dimensions. Our ensemble is learned from a sequence of randomly-projected representations of the original high dimensional data and therefore for this approach data can be collected, stored and processed in such a compressed form. The specific form and simplicity of this ensemble permits a direct and much more detailed analysis than existing generic tools in previous works. In particular, we are able to derive the exact form of the generalization error of our ensemble, conditional on the training set, and based on this we give theoretical guarantees which directly link the performance of the ensemble to that of the corresponding linear discriminant learned in the full data space. To the best of our knowledge these are the first theoretical results to prove such an explicit link for any classifier and classifier ensemble pair. Furthermore we show that the randomly-projected ensemble is equivalent to implementing a sophisticated regularization scheme to the linear discriminant learned in the original data space and this prevents overfitting in conditions of small sample size where pseudo-inverse FLD learned in the data space is provably poor."
592,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,"Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell",none,http://jmlr.org/proceedings/papers/v32/donahue14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_donahue14,http://jmlr.csail.mit.edu/proceedings/papers/v32/donahue14.html,"We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
593,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Online Non-Parametric Regression,"Alexander Rakhlin, Karthik Sridharan","JMLR W&CP 35 :1232-1264, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/rakhlin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_rakhlin14,http://jmlr.csail.mit.edu/proceedings/papers/v35/rakhlin14.html,"We establish optimal rates for online regression for arbitrary classes of regression functions in terms of the sequential entropy introduced in (Rakhlin et al., 2010). The optimal rates are shown to exhibit a phase transition analogous to the i.i.d./statistical learning case, studied in (Rakhlin et al., 2014b). In the frequently encountered situation when sequential entropy and i.i.d. empirical entropy match, our results point to the interesting phenomenon that the rates for statistical learning with squared loss and online nonparametric regression are the same. In addition to a non-algorithmic study of minimax regret, we exhibit a generic forecaster that enjoys the established optimal rates. We also provide a recipe for designing online regression algorithms that can be computationally efficient. We illustrate the techniques by deriving existing and new forecasters for the case of finite experts and for online linear regression."
594,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Minimax rates for memory-bounded sparse linear regression,"Jacob Steinhardt, John Duchi",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Steinhardt15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Steinhardt15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Steinhardt15.html,"We establish a minimax lower bound of \(\Omega(\frac{kd}{B\epsilon})\) on the sample size needed to estimate parameters in a \(k\) -sparse linear regression of dimension \(d\) under memory restrictions to \(B\) bits, where \(\epsilon\) is the \(\ell_2\) parameter error. When the covariance of the regressors is the identity matrix, we also provide an algorithm that uses \(\tilde{O}(B+k)\) bits and requires \(\tilde{O}(\frac{kd}{B\epsilon^2})\) observations to achieve error \(\epsilon\) . Our lower bound also holds in the more general communication-bounded setting, where instead of a memory bound, at most \(B\) bits of information are allowed to be (adaptively) communicated about each sample."
595,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Predicting customer behaviour: The University of Melbourne's KDD Cup report,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/miller09/miller09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_miller09,http://jmlr.csail.mit.edu/proceedings/papers/v7/miller09.html,We discuss the challenges of the 2009 KDD Cup along with our ideas and methodologies for modelling the problem. The main stages included aggressive nonparametric feature selection careful treatment of categorical variables and tuning a gradient boosting machine under Bernoulli loss with trees.
596,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Multiclass-Multilabel Classification with More Classes than Examples,"Ofer Dekel, Ohad Shamir","9:137-144, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/dekel10a/dekel10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_dekel10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/dekel10a.html,We discuss multiclass-multilabel classification problems in which the set of possible labels is extremely large. Most existing multiclass-multilabel learning algorithms expect to observe a reasonably large sample from each class and fail if they receive only a handful of examples with a given label. We propose and analyze the following two-stage approach: first use an arbitrary (perhaps heuristic) classification algorithm to construct an initial classifier then apply a simple but principled method to augment this classifier by removing harmful labels from its output. A careful theoretical analysis allows us to justify our approach under some reasonable conditions (such as label sparsity and power-law distribution of label frequencies) even when the training set does not provide a statistically accurate representation of most classes. Surprisingly our theoretical analysis continues to hold even when the number of classes exceeds the sample size. We demonstrate the merits of our approach on the ambitious task of categorizing the entire web using the 1.5 million categories defined on Wikipedia.
597,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Convex Formulation for Learning from Positive and Unlabeled Data,"Marthinus Du Plessis, Gang Niu, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/plessis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/plessis15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_plessis15,http://jmlr.csail.mit.edu/proceedings/papers/v37/plessis15.html,"We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost."
598,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Autoencoder Trees,"Ozan Irsoy, Ethem Alpaydn",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Irsoy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Irsoy15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Irsoy15.html,"We discuss an autoencoder model in which the encoding and decoding functions are implemented by decision trees. We use the soft decision tree where internal nodes realize soft multivariate splits given by a gating function and the overall output is the average of all leaves weighted by the gating values on their path. The encoder tree takes the input and generates a lower dimensional representation in the leaves and the decoder tree takes this and reconstructs the original input. Exploiting the continuity of the trees, autoencoder trees are trained with stochastic gradient-descent. On handwritten digit and news data, we see that the autoencoder trees yield good reconstruction error compared to traditional autoencoder perceptrons. We also see that the autoencoder tree captures hierarchical representations at different granularities of the data on its different levels and the leaves capture the localities in the input space."
599,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays,"Junpei Komiyama, Junya Honda, Hiroshi Nakagawa",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/komiyama15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/komiyama15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_komiyama15,http://jmlr.csail.mit.edu/proceedings/papers/v37/komiyama15.html,"We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance."
600,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Regularization Paths with Guarantees for Convex Semidefinite Optimization,"Joachim Giesen, Martin Jaggi, Soeren Laue",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/giesen12/giesen12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_giesen12,http://jmlr.csail.mit.edu/proceedings/papers/v22/giesen12.html,We devise a simple algorithm for computing an approximate solution path for parameterized semidefinite convex optimization problems that is guaranteed to be epsilon-close to the exact solution path. As a consequence we can compute the entire regularization path for many regularized matrix completion and factorization approaches as well as nuclear norm or weighted nuclear norm regularized convex optimization problems. This also includes robust PCA and variants of sparse PCA. On the theoretical side we show that the approximate solution path has low complexity. This implies that the whole solution path can be computed efficiently. Our experiments demonstrate the practicality of the approach for large matrix completion problems.
601,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Latent Confusion Analysis by Normalized Gamma Construction,"Issei Sato, Hisashi Kashima, Hiroshi Nakagawa",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/satob14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/satob14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_satob14,http://jmlr.csail.mit.edu/proceedings/papers/v32/satob14.html,"We developed a flexible framework for modeling the annotation and judgment processes of humans, which we called –normalized gamma construction of a confusion matrix.” This framework enabled us to model three properties: (1) the abilities of humans, (2) a confusion matrix with labeling, and (3) the difficulty with which items are correctly annotated. We also provided the concept of –latent confusion analysis (LCA),” whose main purpose was to analyze the principal confusions behind human annotations and judgments. It is assumed in LCA that confusion matrices are shared between persons, which we called –latent confusions”, in tribute to the –latent topics” of topic modeling. We aim at summarizing the workersê confusion matrices with the small number of latent principal confusion matrices because many personal confusion matrices is difficult to analyze. We used LCA to analyze latent confusions regarding the effects of radioactivity on fish and shellfish following the Fukushima Daiichi nuclear disaster in 2011."
602,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Relational Topic Models for Document Networks,"Jonathan Chang, David Blei","5:81-88, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/chang09a/chang09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_chang09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/chang09a.html,We develop the relational topic model (RTM) a model of documents and the links between them. For each pair of documents the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents predict links between them and predict words within them. We derive ef_cient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scienti_c abstracts and web documents.
603,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Stochastic Quasi-Newton Method for Online Convex Optimization,"Nicol N. Schraudolph, Jin Yu, Simon Gônter","2:436-443, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/schraudolph07a/schraudolph07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_schraudolph07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/schraudolph07a.html,We develop stochastic variants of the well-known BFGS quasi-Newton optimization method in both full and memory-limited (LBFGS) forms for online optimization of convex functions. The resulting algorithm performs comparably to a well-tuned natural gradient descent but is scalable to very high-dimensional problems. On standard benchmarks in natural language processing it asymptotically outperforms previous stochastic gradient methods for parameter estimation in conditional random fields. We are working on analyzing the convergence of online (L)BFGS and extending it to nonconvex optimization problems.
604,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Spectral Learning of Hidden Markov Models from Dynamic and Static Data,"Tzu-Kuo Huang, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/huang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_huang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/huang13.html,"We develop spectral learning algorithms for Hidden Markov Models that learn not only from time series, or dynamic data but also static data drawn independently from the HMMês stationary distribution. This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms."
605,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Primal-Dual methods for sparse constrained matrix completion,"Yu Xin, Tommi Jaakkola",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/xin12/xin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_xin12,http://jmlr.csail.mit.edu/proceedings/papers/v22/xin12.html,We develop scalable algorithms for regular and non-negative matrix completion. In particular we base the methods on trace-norm regularization that induces a low rank predicted matrix. The regularization problem is solved via a constraint generation method that explicitly maintains a sparse dual and the corresponding low rank primal solution. We provide a new dual block coordinate descent algorithm for solving the dual problem with a few spectral constraints. Empirical results illustrate the effectiveness of our method in comparison to recently proposed alternatives.
606,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method,Taiji Suzuki,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/suzuki13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/suzuki13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_suzuki13,http://jmlr.csail.mit.edu/proceedings/papers/v28/suzuki13.html,"We develop new stochastic optimization methods that are applicable to a wide range of structured regularizations. Basically our methods are combinations of basic stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM). ADMM is a general framework for optimizing a composite function, and has a wide range of applications. We propose two types of online variants of ADMM, which correspond to online proximal gradient descent and regularized dual averaging respectively. The proposed algorithms are computationally efficient and easy to implement. Our methods yield \(O(1/\sqrt{T})\) convergence of the expected risk. Moreover, the online proximal gradient descent type method yields \(O(\log(T)/T)\) convergence for a strongly convex loss. Numerical experiments show effectiveness of our methods in learning tasks with structured sparsity such as overlapped group lasso."
607,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Algorithms for Lipschitz Learning on Graphs,"Rasmus Kyng, Anup Rao, Sushant Sachdeva, Daniel A. Spielman",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kyng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kyng15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kyng15.html,"We develop fast algorithms for solving regression problems on graphs where one is given the value of a function at some vertices, and must find its smoothest possible extension to all vertices. The extension we compute is the absolutely minimal Lipschitz extension, and is the limit for large \(p\) of \(p\) -Laplacian regularization. We present an algorithm that computes a minimal Lipschitz extension in expected linear time, and an algorithm that computes an absolutely minimal Lipschitz extension in expected time \(\widetilde{O} (m n)\) . The latter algorithm has variants that seem to run much faster in practice. These extensions are particularly amenable to regularization: we can perform \(l_{0}\) -regularization on the given values in polynomial time and \(l_{1}\) -regularization on the initial function values and on graph edge weights in time \(\widetilde{O} (m^{3/2})\) . Our definitions and algorithms naturally extend to directed graphs."
608,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Learning of Mahalanobis Metrics for Ranking,"Daryl Lim, Gert Lanckriet",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lim14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lim14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lim14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lim14.html,"We develop an efficient algorithm to learn a Mahalanobis distance metric by directly optimizing a ranking loss. Our approach focuses on optimizing the top of the induced rankings, which is desirable in tasks such as visualization and nearest-neighbor retrieval. We further develop and justify a simple technique to reduce training time significantly with minimal impact on performance. Our proposed method significantly outperforms alternative methods on several real-world tasks, and can scale to large and high-dimensional data."
609,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Finding Galaxies in the Shadows of Quasars with Gaussian Processes,"Roman Garnett, Shirley Ho, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/garnett15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_garnett15,http://jmlr.csail.mit.edu/proceedings/papers/v37/garnett15.html,"We develop an automated technique for detecting damped Lyman- \(\alpha\) absorbers (DLAs) along spectroscopic sightlines to quasi-stellar objects (QSOs or quasars). The detection of DLAs in large-scale spectroscopic surveys such as SDSS_III is critical to address outstanding cosmological questions, such as the nature of galaxy formation. We use nearly 50000 QSO spectra to learn a tailored Gaussian process model for quasar emission spectra, which we apply to the DLA detection problem via Bayesian model selection. We demonstrate our methodês effectiveness with a large-scale validation experiment on over 100000 spectra, with excellent performance."
610,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Variational Generative Stochastic Networks with Collaborative Shaping,"Philip Bachman, Doina Precup",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachman15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bachman15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachman15.html,"We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chainês trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view."
611,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Optimally Combining Classifiers Using Unlabeled Data,"Akshay Balsubramani, Yoav Freund",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Balsubramani15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Balsubramani15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Balsubramani15.html,"We develop a worst-case analysis of aggregation of classifier ensembles for binary classification. The task of predicting to minimize error is formulated as a game played over a given set of unlabeled data (a transductive setting), where prior label information is encoded as constraints on the game. The minimax solution of this game identifies cases where a weighted combination of the classifiers can perform significantly better than any single classifier."
612,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Unified Robust Regression Model for Lasso-like Algorithms,"Wenzhuo Yang, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13e-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13e,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13e.html,"We develop a unified robust linear regression model and show that it is equivalent to a general regularization framework to encourage sparse-like structure that contains group Lasso and fused Lasso as specific examples. This provides a robustness interpretation of these widely applied Lasso-like algorithms, and allows us to construct novel generalizations of Lasso-like algorithms by considering different uncertainty sets. Using this robustness interpretation, we present new sparsity results, and establish the statistical consistency of the proposed regularized linear regression. This work extends a classical result from Xu et al. (2010) that relates standard Lasso with robust linear regression to learning problems with more general sparse-like structures, and provides new robustness-based tools to to understand learning problems with sparse-like structures."
613,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Guaranteed Tensor Decomposition: A Moment Approach,"Gongguo Tang, Parikshit Shah",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tanga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/tanga15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tanga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tanga15.html,"We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach."
614,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,PAC-Bayesian Bound for Gaussian Process Regression and Multiple Kernel Additive Model,Taiji Suzuki,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/suzuki12/suzuki12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_suzuki12,http://jmlr.csail.mit.edu/proceedings/papers/v23/suzuki12.html,"We develop a PAC-Bayesian bound for the convergence rate of a Bayesian variant of Multiple Kernel Learning (MKL) that is an estimation method for the sparse additive model. Standard analyses for MKL require a strong condition on the design analogous to the restricted eigenvalue condition for the analysis of Lasso and Dantzig selector. In this paper, we apply PAC-Bayesian technique to show that the Bayesian variant of MKL achieves the optimal convergence rate without such strong conditions on the design. Basically our approach is a combination of PAC-Bayes and recently developed theories of non-parametric Gaussian process regressions. Our bound is developed in a fixed design situation. Our analysis includes the existing result of Gaussian process as a special case and the proof is much simpler by virtue of PAC-Bayesian technique. We also give the convergence rate of the Bayesian variant of Group Lasso as a finite dimensional special case."
615,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Level-set Hit-and-run Sampler for Quasi-Concave Distributions,"Shane Jensen, Dean Foster","JMLR W&CP 33 :439-447, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/jensen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/jensen14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_jensen14,http://jmlr.csail.mit.edu/proceedings/papers/v33/jensen14.html,"We develop a new sampling strategy that uses the hit-and-run algorithm within level sets of a target density. Our method can be applied to any quasi-concave density, which covers a broad class of models. Standard sampling methods often perform poorly on densities that are high-dimensional or multi-modal. Our level set sampler performs well in high-dimensional settings, which we illustrate on a spike-and-slab mixture model. We also extend our method to exponentially-tilted quasi-concave densities, which arise in Bayesian models consisting of a log-concave likelihood and quasi-concave prior density. We illustrate our exponentially-tilted level-set sampler on a Cauchy-normal model where our sampler is better able to handle a high-dimensional and multi-modal posterior distribution compared to Gibbs sampling and Hamiltonian Monte Carlo."
616,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Polynomial-Time Exact Inference in NP-Hard Binary MRFs via Reweighted Perfect Matching,Nic Schraudolph,"9:717-724, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/schraudolph10a/schraudolph10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_schraudolph10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/schraudolph10a.html,We develop a new form of reweighting (Wainwright et al. 2005) to leverage the relationship between Ising spin glasses and perfect matchings into a novel technique for the exact computation of MAP states in hitherto intractable binary Markov random fields. Our method solves an n by n lattice with external field and random couplings much faster and for larger n than the best competing algorithms. It empirically scales as O(n_) even though this problem is NP-hard and non-approximable in polynomial time. We discuss limitations of our current implementation and propose ways to overcome them.
617,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball,"Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry",none,http://jmlr.org/proceedings/papers/v32/miller14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/miller14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_miller14,http://jmlr.csail.mit.edu/proceedings/papers/v32/miller14.html,"We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA. Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior. This makes it difficult to draw comparisons between players and make accurate player specific predictions. Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA. Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players. The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy."
618,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Counterfactual Risk Minimization: Learning from Logged Bandit Feedback,"Adith Swaminathan, Thorsten Joachims",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/swaminathan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_swaminathan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/swaminathan15.html,"We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method _ called Policy Optimizer for Exponential Models (POEM) _ for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art."
619,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Sparse Additive Machine,"Tuo Zhao, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhao12/zhao12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhao12,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhao12.html,We develop a high dimensional nonparametric classification method named sparse additive machine (SAM) which can be viewed as a functional version of support vector machines (SVM) combined with sparse additive modeling. SAM is related to multiple kernel learning (MKL) but is computationally more efficient and amenable to theoretical analysis. In terms of computation we develop an efficient accelerated proximal gradient descent algorithm which is also scalable to large data sets with a provable O(1/k^2) convergence rate and k is the number of iterations. In terms of theory we provide the oracle properties of SAM under asymptotic frameworks. Empirical results on3 both synthetic and real data are reported to back up our theory.
620,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Smooth Operators,"Steffen Grunewalder, Gretton Arthur, John Shawe-Taylor",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/grunewalder13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/grunewalder13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_grunewalder13,http://jmlr.csail.mit.edu/proceedings/papers/v28/grunewalder13.html,"We develop a generic approach to form smooth versions of basic mathematical operations like multiplication, composition, change of measure, and conditional expectation, among others. Operations which result in functions outside the reproducing kernel Hilbert space (such as the product of two RKHS functions) are approximated via a natural cost function, such that the solution is guaranteed to be in the targeted RKHS. This approximation problem is reduced to a regression problem using an adjoint trick, and solved in a vector-valued RKHS, consisting of continuous, linear, smooth operators which map from an input, real-valued RKHS to the desired target RKHS. Important constraints, such as an almost everywhere positive density, can be enforced or approximated naturally in this framework, using convex constraints on the operators. Finally, smooth operators can be composed to accomplish more complex machine learning tasks, such as the sum rule and kernelized approximate Bayesian inference, where state-of-the-art convergence rates are obtained."
621,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Maximum Margin Multiclass Nearest Neighbors,"Aryeh Kontorovich, Roi Weiss",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovichb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovichb14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kontorovichb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kontorovichb14.html,"We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size \(n\) and significantly improve the dependence on the number of classes \(k\) . Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of \(k\) . Although \(k\) -free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on \(k\) . As the best previous risk estimates in this setting were of order \(\sqrt k\) , our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on \(n\) examples in \(O(n^2\log n)\) time and evaluated on new points in \(O(\log n)\) time."
622,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization,"Roy Frostig, Rong Ge, Sham Kakade, Aaron Sidford",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/frostig15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/frostig15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_frostig15,http://jmlr.csail.mit.edu/proceedings/papers/v37/frostig15.html,"We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem."
623,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bayesian Nonparametric Poisson Factorization for Recommendation Systems,"Prem Gopalan, Francisco J. Ruiz, Rajesh Ranganath, David Blei","JMLR W&CP 33 :275-283, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/gopalan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/gopalan14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_gopalan14,http://jmlr.csail.mit.edu/proceedings/papers/v33/gopalan14.html,"We develop a Bayesian nonparametric Poisson factorization model for recommendation systems. Poisson factorization implicitly models each userês limited budget of attention (or money) that allows consumption of only a small subset of the available items. In our Bayesian nonparametric variant, the number of latent components is theoretically unbounded and effectively estimated when computing a posterior with observed user behavior data. To approximate the posterior, we develop an efficient variational inference algorithm. It adapts the dimensionality of the latent components to the data, only requires iteration over the user/item pairs that have been rated, and has computational complexity on the same order as for a parametric model with fixed dimensionality. We studied our model and algorithm with large real-world data sets of user-movie preferences. Our model eases the computational burden of searching for the number of latent components and gives better predictive performance than its parametric counterpart."
624,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Supervised Dimension Reduction Using Bayesian Mixture Modeling,"Kai Mao, Feng Liang, Sayan Mukherjee","9:501-508, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/mao10a/mao10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_mao10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/mao10a.html,We develop a Bayesian framework for supervised dimension reduction using a flexible nonparametric Bayesian mixture modeling approach. Our method retrieves the dimension reduction or d.r. subspace by utilizing a dependent Dirichlet process that allows for natural clustering for the data in terms of both the response and predictor variables. Formal probabilistic models with likelihoods and priors are given and efficient posterior sampling of the d.r. subspace can be obtained by a Gibbs sampler. As the posterior draws are linear subspaces which are points on a Grassmann manifold we output the posterior mean d.r. subspace with respect to geodesics on the Grassmannian. The utility of our approach is illustrated on a set of simulated and real examples. Some Key Words: supervised dimension reduction inverse regression Dirichlet process factor models Grassman manifold.
625,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,"Differentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso","Abhradeep Guha Thakurta, Adam Smith",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Guha13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Guha13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Guha13.html,"We design differentially private algorithms for statistical model selection. Given a data set and a large, discrete collection of –models”, each of which is a family of probability distributions, the goal is to determine the model that best –fits” the data. This is a basic problem in many areas of statistics and machine learning. We consider settings in which there is a well-defined answer, in the following sense: Suppose that there is a nonprivate model selection procedure \(f\) , which is the reference to which we compare our performance. Our differentially private algorithms output the correct value \(f(D)\) whenever \(f\) is stable on the input data set \(D\) . We work with two notions, perturbation stability and sub-sampling stability. We give two classes of results: generic ones, that apply to any function with discrete output set; and specific algorithms for the problem of sparse linear regression. The algorithms we describe are efficient and in some cases match the optimal non-private asymptotic sample complexity. Our algorithms for sparse linear regression require analyzing the stability properties of the popular LASSO estimator. We give sufficient conditions for the LASSO estimator to be robust to small changes in the data set, and show that these conditions hold with high probability under essentially the same stochastic assumptions that are used in the literature to analyze convergence of the LASSO."
626,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Learning to Rank Using an Ensemble of Lambda-Gradient Models,"C. Burges, K. Svore, P. Bennett, A. Pastusiak & Q. Wu","14:25_35, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/burges11a/burges11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_burges11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/burges11a.html,We describe the system that won Track 1 of the Yahoo! Learning to Rank Challenge.   Page last modified on Wed Jan 26 10:36:45 2011.
627,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative Attribute Graphs,"Hyokun Yun, S V N Vishwanathan",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/yun12/yun12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_yun12,http://jmlr.csail.mit.edu/proceedings/papers/v22/yun12.html,We describe the first sub-quadratic sampling algorithm for Multiplicative Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close connection between MAGM and the Kronecker Product Graph Model (KPGM) of Leskovec et al. (2010) and show that to sample a graph from a MAGM it suffices to sample small number of KPGM graphs and quilt them together. Under a restricted set of technical conditions our algorithm runs in O((\log_2(n))^3 |E|) time where n is the number of nodes and |E| is the number of edges in the sampled graph. We demonstrate the scalability of our algorithm via extensive empirical evaluation; we can sample a MAGM graph with 8 million nodes and 20 billion edges in under 6 hours.
628,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Winning the KDD Cup Orange Challenge with Ensemble Selection,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/niculescu09/niculescu09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_niculescu09,http://jmlr.csail.mit.edu/proceedings/papers/v7/niculescu09.html,We describe our wining solution for the KDD Cup Orange Challenge.
629,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Combining Predictors for Recommending Music: the False Positivesê approach to KDD Cup track 2,S. Balakrishnan,"18:199_213, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/balakrishnan12a/balakrishnan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_balakrishnan12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/balakrishnan12a.html,We describe our solution for the KDD Cup 2011 track 2 challenge. Our solution relies heavily on ensembling together diverse individual models for the prediction task and achieved a _nal leaderboard/Test 1 misclassi_cation rate of 3.8863%. This paper provides details on both the modeling and ensemble creation steps.   Page last modified on Tue May 29 10:23:32 2012.
630,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Expectation Propagation for Likelihoods Depending on an Inner Product of Two Multivariate Random Variables,"Tomi Peltola, Pasi Jyl_nki, Aki Vehtari","JMLR W&CP 33 :769-777, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/peltola14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/peltola14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_peltola14,http://jmlr.csail.mit.edu/proceedings/papers/v33/peltola14.html,"We describe how a deterministic Gaussian posterior approximation can be constructed using expectation propagation (EP) for models, where the likelihood function depends on an inner product of two multivariate random variables. The family of applicable models includes a wide variety of important linear latent variable models used in statistical machine learning, such as principal component and factor analysis, their linear extensions, and errors-in-variables regression. The EP computations are facilitated by an integral transformation of the Dirac delta function, which allows transforming the multidimensional integrals over the two multivariate random variables into an analytically tractable form up to one-dimensional analytically intractable integrals that can be efficiently computed numerically. We study the resulting posterior approximations in sparse principal component analysis with Gaussian and probit likelihoods. Comparisons to Gibbs sampling and variational inference are presented."
631,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Deep Exponential Families,"Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David Blei",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/ranganath15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/ranganath15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_ranganath15,http://jmlr.csail.mit.edu/proceedings/papers/v38/ranganath15.html,"We describe deep exponential families (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent ``black box"" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models."
632,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate,Ohad Shamir,none,http://jmlr.csail.mit.edu/proceedings/papers/v37/shamir15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/shamir15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_shamir15,http://jmlr.csail.mit.edu/proceedings/papers/v37/shamir15.html,"We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis."
633,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Boosting Algorithm for Label Covering in Multilabel Problems,"Yonatan Amit, Ofer Dekel, Yoram Singer","2:27-34, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/amit07a/amit07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_amit07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/amit07a.html,We describe analyze and experiment with a boosting algorithm for multilabel categorization problems. Our algorithm includes as special cases previously studied boosting algorithms such as Adaboost.MH. We cast the multilabel problem as multiple binary decision problems based on a user-defined covering of the set of labels. We prove a lower bound on the progress made by our algorithm on each boosting iteration and demonstrate the merits of our algorithm in experiments with text categorization problems.
634,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,An Asynchronous Parallel Stochastic Coordinate Descent Algorithm,"Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, Srikrishna Sridhar",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/liud14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liud14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liud14.html,"We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate ( \(1/K\) ) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is \(O(n^{1/2})\) in unconstrained optimization and \(O(n^{1/4})\) in the separable-constrained case, where \(n\) is the number of variables. We describe results from implementation on 40-core processors."
635,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Scaling Nonparametric Bayesian Inference via Subsample-Annealing,"Fritz Obermeyer, Jonathan Glidden, Eric Jonas","JMLR W&CP 33 :696-705, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/obermeyer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/obermeyer14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_obermeyer14,http://jmlr.csail.mit.edu/proceedings/papers/v33/obermeyer14.html,"We describe an adaptation of the simulated annealing algorithm to nonparametric clustering and related probabilistic models. This new algorithm learns nonparametric latent structure over a growing and constantly churning subsample of training data, where the portion of data subsampled can be interpreted as the inverse temperature \(\beta(t)\) in an annealing schedule. Gibbs sampling at high temperature (i.e., with a very small subsample) can more quickly explore sketches of the final latent state by (a) making longer jumps around latent space (as in block Gibbs) and (b) lowering energy barriers (as in simulated annealing). We prove subsample annealing speeds up mixing time \(N^2 \rightarrow N\) in a simple clustering model and \(\exp(N) \rightarrow N\) in another class of models, where \(N\) is data size. Empirically subsample-annealing outperforms naive Gibbs sampling in accuracy-per-wallclock time, and can scale to larger datasets and deeper hierarchical models. We demonstrate improved inference on million-row subsamples of US Census data and network log data and a 307-row hospital rating dataset, using a Pitman-Yor generalization of the Cross Categorization model."
636,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Towards building a Crowd-Sourced Sky Map,"Dustin Lang, David Hogg, Bernhard Sch_lkopf","JMLR W&CP 33 :549-557, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/lang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_lang14,http://jmlr.csail.mit.edu/proceedings/papers/v33/lang14.html,"We describe a system that builds a high dynamic-range and wide-angle image of the night sky by combining a large set of input images. The method makes use of pixel-rank information in the individual input images to improve a –consensus” pixel rank in the combined image. Because it only makes use of ranks and the complexity of the algorithm is linear in the number of images, the method is useful for large sets of uncalibrated images that might have undergone unknown non-linear tone mapping transformations for visualization or aesthetic reasons. We apply the method to images of the night sky (of unknown provenance) discovered on the Web. The method permits discovery of astronomical objects or features that are not visible in any of the input images taken individually. More importantly, however, it permits scientific exploitation of a huge source of astronomical images that would not be available to astronomical research without our automatic system."
637,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Discriminative Mixtures of Sparse Latent Fields for Risk Management,"Felix Agakov, Peter Orchard, Amos Storkey",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/agakov12/agakov12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_agakov12,http://jmlr.csail.mit.edu/proceedings/papers/v22/agakov12.html,We describe a simple and efficient approach to learning structures of sparse high-dimensional latent variable models. Standard algorithms either learn structures of specific predefined forms or estimate sparse graphs in the data space ignoring the possibility of the latent variables. In contrast our method learns rich dependencies and allows for latent variables that may confound the relations between the observations. We extend the model to conditional mixtures with side information and non-Gaussian marginal distributions of the observations. We then show that our model may be used for learning sparse latent variable structures corresponding to multiple unknown states and for uncovering features useful for explaining and predicting structural changes. We apply the model to real-world financial data with heavy-tailed marginals covering the low- and high- market volatility periods of 2005-2011. We show that our method tends to give rise to significantly higher likelihoods of test data than standard network learning methods exploiting the sparsity assumption. We also demonstrate that our approach may be practical for financial stress testing and visualization of dependencies between financial instruments.
638,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A low variance consistent test of relative dependency,"Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus, Matthew Blaschko",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bounliphone15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bounliphone15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bounliphone15.html,"We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep/."
639,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Dense Message Passing for Sparse Principal Component Analysis,"Kevin Sharp, Magnus Rattray","9:725-732, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sharp10a/sharp10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sharp10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sharp10a.html,We describe a novel inference algorithm for sparse Bayesian PCA with a zero-norm prior on the model parameters. Bayesian inference is very challenging in probabilistic models of this type. MCMC procedures are too slow to be practical in a very high-dimensional setting and standard mean-field variational Bayes algorithms are ineffective. We adopt a dense message passing algorithm similar to algorithms developed in the statistical physics community and previously applied to inference problems in coding and sparse classification. The algorithm achieves near-optimal performance on synthetic data for which a statistical mechanics theory of optimal learning can be derived. We also study two gene expression datasets used in previous studies of sparse PCA. We find our method performs better than one published algorithm and comparably to a second.
640,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bayesian Quadrature for Ratios,"Michael Osborne, Roman Garnett, Stephen Roberts, Christopher Hart, Suzanne Aigrain, Neale Gibson",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/osborne12/osborne12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_osborne12,http://jmlr.csail.mit.edu/proceedings/papers/v22/osborne12.html,We describe a novel approach to quadrature for ratios of probabilistic integrals such as are used to compute posterior probabilities. It offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the non-negativity of our integrands and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate as is commonplace in exoplanets research; we demonstrate the efficacy of our method on data from the Kepler spacecraft.
641,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nuclear Norm Minimization via Active Subspace Selection,"Cho-Jui Hsieh, Peder Olsen",none,http://jmlr.org/proceedings/papers/v32/hsiehb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsiehb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hsiehb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsiehb14.html,"We describe a novel approach to optimizing matrix problems involving nuclear norm regularization and apply it to the matrix completion problem. We combine methods from non-smooth and smooth optimization. At each step we use the proximal gradient to select an active subspace. We then find a smooth, convex relaxation of the smaller subspace problems and solve these using second order methods. We apply our methods to matrix completion problems including Netflix dataset, and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers. Also, this is the first paper to scale nuclear norm solvers to the Yahoo-Music dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like Alternating Least Squares (ALS)."
642,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning Multilevel Distributed Representations for High-Dimensional Sequences,"Ilya Sutskever, Geoffrey Hinton","2:548-555, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/sutskever07a/sutskever07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_sutskever07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/sutskever07a.html,We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box.
643,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,The Neural Autoregressive Distribution Estimator,"Hugo Larochelle, Iain Murray","15:29-37, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/larochelle11a/larochelle11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_larochelle11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/larochelle11a.html,We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM) which has been shown to be a powerful model of such distributions. However an RBM typically does not provide a tractable distribution estimator since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.
644,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Removing systematic errors for exoplanet search via latent causes,"Bernhard Sch_lkopf, David Hogg, Dun Wang, Dan Foreman-Mackey, Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/scholkopf15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_scholkopf15,http://jmlr.csail.mit.edu/proceedings/papers/v37/scholkopf15.html,"We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as half-sibling regression , is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification and illustrate the potential of the method in a challenging astronomy application."
645,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Taxonomy-Informed Latent Factor Models for Implicit Feedback,A. Mnih,"18:169_181, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/mnih12a/mnih12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_mnih12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/mnih12a.html,We describe a latent-factor-model-based approach to the Track 2 task of KDD Cup 2011 which required learning to discriminate between highly rated and unrated items from a large dataset of music ratings. We take the pairwise ranking route training our models to rank the highly rated items above the unrated items that are sampled from the same distribution. Using the item relationship information from the provided taxonomy to constrain item representations results in improved predictive performance. Providing the model with features summarizing the userÍs rating history as it relates to the item being ranked leads to further gains producing the best single model result on Track 2.   Page last modified on Tue May 29 10:23:26 2012.
646,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation,"David Wingate, Andreas Stuhlmueller, Noah Goodman","15:770-778, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/wingate11a/wingate11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_wingate11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/wingate11a.html,"We describe a general method of transforming arbitrary programming languages into probabilistic programming languages with straightforward MCMC inference engines. Random choices in the program are """"named"""" with information about their position in an execution trace; these names are used in conjunction with a database of randomness to implement MCMC inference in the space of execution traces. We encode naming information using lightweight source-to-source compilers. Our method enables us to reuse existing infrastructure (compilers interpreters etc.) with minimal additional code implying fast models with low development overhead. We illustrate the technique on two languages one functional and one imperative: Bher a compiled version of the Church language which eliminates interpretive overhead of the original MIT-Church implementation and Stochastic Matlab a new open-source language."
647,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deterministic Anytime Inference for Stochastic Continuous-Time Markov Processes,"E. Busra Celikkaya, Christian Shelton",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/celikkaya14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_celikkaya14,http://jmlr.csail.mit.edu/proceedings/papers/v32/celikkaya14.html,"We describe a deterministic anytime method for calculating filtered and smoothed distributions in large variable-based continuous time Markov processes. Prior non-random algorithms do not converge to the true distribution in the limit of infinite computation time. Sampling algorithms give different results each time run, which can lead to instability when used inside expectation-maximization or other algorithms. Our method combines the anytime convergent properties of sampling with the non-random nature of variational approaches. It is built upon a sum of time-ordered products, an expansion of the matrix exponential. We demonstrate that our method performs as well as or better than the current best sampling approaches on benchmark problems."
648,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sharp Generalization Error Bounds for Randomly-projected Classifiers,"Robert Durrant, Ata Kaban",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/durrant13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_durrant13,http://jmlr.csail.mit.edu/proceedings/papers/v28/durrant13.html,"We derive sharp bounds on the generalization error of a generic linear classifier trained by empirical risk minimization on randomly-projected data. We make no restrictive assumptions (such as sparsity or separability) on the data: Instead we use the fact that, in a classification setting, the question of interest is really •what is the effect of random projection on the predicted class labels?ê and we therefore derive the exact probability of •label flippingê under Gaussian random projection in order to quantify this effect precisely in our bounds."
649,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Bayesian and Empirical Bayesian Forests,"Taddy Matthew, Chun-Sheng Chen, Jun Yu, Mitch Wyle",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/matthew15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_matthew15,http://jmlr.csail.mit.edu/proceedings/papers/v37/matthew15.html,"We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view such ensembles as samples from a posterior distribution. This insight motivates a class of Bayesian Forest (BF) algorithms that provide small gains in performance and large gains in interpretability. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This motivates an empirical Bayesian Forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin."
650,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,PAC-Bayesian Generalization Bound for Density Estimation with Application to Co-clustering,"Yevgeny Seldin, Naftali Tishby","5:472-479, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/seldin09a/seldin09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_seldin09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/seldin09a.html,We derive a PAC-Bayesian generalization bound for density estimation. Similar to the PAC-Bayesian generalization bound for classification the result has the appealingly simple form of a tradeoff between empirical performance and the KL-divergence of the posterior from the prior. Moreover the PAC-Bayesian generalization bound for classification can be derived as a special case of the bound for density estimation. To illustrate a possible application of our bound we derive a generalization bound for co-clustering. The bound provides a criterion to evaluate the ability of co-clustering to predict new co-occurrences thus introducing a supervised flavor to this traditionally unsupervised task.
651,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Matching Pursuit Kernel Fisher Discriminant Analysis,"Tom Diethe, Zakria Hussain, David Hardoon, John Shawe-Taylor","5:121-128, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/diethe09a/diethe09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_diethe09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/diethe09a.html,We derive a novel sparse version of Kernel Fisher Discriminant Analysis (KFDA) using an approach based on Matching Pursuit (MP). We call this algorithm Matching Pursuit Kernel Fisher Discriminant Analysis (MPKFDA). We provide generalisation error bounds analogous to those constructed for the Robust Minimax algorithm together with a sample compression bounding technique. We present experimental results on real world datasets which show that MPKFDA is competitive with the KFDA and the SVM on UCI datasets and additional experiments that show that the MPKFDA on average outperforms KFDA and SVM in extremely high dimensional settings.
652,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Reward,"Matthew Hoffman, Nando de Freitas, Arnaud Doucet, Jan Peters","5:232-239, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/hoffman09a/hoffman09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_hoffman09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/hoffman09a.html,We derive a new expectation maximization algorithm for policy optimization in linear Gaussian Markov decision processes where the reward function is parameterised in terms of a flexible mixture of Gaussians. This approach exploits both analytical tractability and numerical optimization. Consequently on the one hand it is more flexible and general than closed-form solutions such as the widely used linear quadratic Gaussian (LQG) controllers. On the other hand it is more accurate and faster than optimization methods that rely on approximation and simulation. Partial analytical solutions (though costly) eliminate the need for simulation and hence avoid approximation error. The experiments will show that for the same cost of computation policy optimization methods that rely on analytical tractability have higher value than the ones that rely on simulation.
653,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Efficient Estimation of Mutual Information for Strongly Dependent Variables,"Shuyang Gao, Greg Ver Steeg, Aram Galstyan",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/gao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/gao15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_gao15,http://jmlr.csail.mit.edu/proceedings/papers/v38/gao15.html,"We demonstrate that a popular class of non-parametric mutual information (MI) estimators based on \(k\) -nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on local uniformity of the underlying joint distribution. We introduce a new estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data."
654,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Maximum Entropy Density Estimation with Incomplete Presence-Only Data,"Bert Huang, Ansaf Salleb-Aouissi","5:240-247, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/huang09a/huang09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_huang09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/huang09a.html,We demonstrate a generalization of Maximum Entropy Density Estimation that elegantly handles incomplete presence-only data. We provide a formulation that is able to learn from known values of incomplete data without having to learn imputed values which may be inaccurate. This saves the effort needed to perform accurate imputation while observing the principle of maximum entropy throughout the learning process. We provide analysis and examples of our algorithm under different settings of missing data.
655,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Beta Diffusion Trees,"Creighton Heaukulani, David Knowles, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/heaukulani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_heaukulani14,http://jmlr.csail.mit.edu/proceedings/papers/v32/heaukulani14.html,"We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles & Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements."
656,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning Low Density Separators,"Shai Ben-David, Tyler Lu, David Pal, Miroslava Sotakova","5:25-32, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/ben-david09a/ben-david09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_ben-david09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/ben-david09a.html,We define a novel basic unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution. This task is relevant to several problems in machine learning such as semi-supervised learning and clustering stability. We investigate the question of existence of a universally consistent algorithm for this problem. We propose two natural learning paradigms and prove that on input unlabeled random samples generated by any member of a rich family of distributions they are guaranteed to converge to the optimal separator for that distribution. We complement this result by showing that no learning algorithm for our task can achieve uniform learning rates (that are independent of the data generating distribution).
657,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Combinatorial Multi-Armed Bandit: General Framework and Applications,"Wei Chen, Yajun Wang, Yang Yuan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13a.html,"We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions form super arms . In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an \((\alpha,\beta)\) -approximation oracle that takes the means of the distributions of arms and outputs a super arm that with probability \(\beta\) generates an \(\alpha\) fraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize \((\alpha,\beta)\) -approximation regret , which is the difference in total expected reward between the \(\alpha\beta\) fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves \(O(\log n)\) regret, where \(n\) is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures."
658,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Large-Margin Metric Learning for Constrained Partitioning Problems,"R_mi Lajugie, Francis Bach, Sylvain Arlot",none,http://jmlr.org/proceedings/papers/v32/lajugie14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lajugie14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lajugie14.html,"We consider unsupervised partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, such as clustering, image or video segmentation, and other change-point detection problems. We emphasize on cases with specific structure, which include many practical situations ranging from mean-based change-point detection to image segmentation problems. We aim at learning a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several (partially) labeled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently. Our experiments show how learning the metric can significantly improve performance on bioinformatics, video or image segmentation problems."
659,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,The price of bandit information in multiclass online classification,"Amit Daniely, Tom Helbertal",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Daniely13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Daniely13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Daniely13.html,"We consider two scenarios of multiclass online learning of a hypothesis class \(H\subseteq Y^X\) . In the full information scenario, the learner is exposed to instances together with their labels. In the bandit scenario, the true label is not exposed, but rather an indication whether the learnerês prediction is correct or not. We show that the ratio between the error rates in the two scenarios is at most \(8\cdot|Y|\cdot \log(|Y|)\) in the realizable case, and \(\tilde{O}(\sqrt{|Y|})\) in the agnostic case. The results are tight up to a logarithmic factor and essentially answer an open question from (Daniely et. al. - Multiclass learnability and the erm principle).We apply these results to the class of \(\gamma\) -margin multiclass linear classifiers in \(\mathbb{R}^d\) . We show that the bandit error rate of this class is \(\tilde{\Theta}\left(\frac{|Y|}{\gamma^2}\right)\) in the realizable case and \(\tilde{\Theta}\left(\frac{1}{\gamma}\sqrt{|Y|T}\right)\) in the agnostic case. This resolves an open question from (Kakade et. al. - Efficient bandit algorithms for onlinemulticlass prediction)."
660,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On-Line Learning Algorithms for Path Experts with Non-Additive Losses,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Manfred Warmuth",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cortes15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Cortes15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cortes15.html,We consider two broad families of non-additive loss functions covering a large number of applications: rational losses and tropical losses. We give new algorithms extending the Follow-the-Perturbed-Leader (FPL) algorithm to both of these families of loss functions and similarly give new algorithms extending the Randomized Weighted Majority (RWM) algorithm to both of these families. We prove that the time complexity of our extensions to rational losses of both FPL and RWM is polynomial and present regret bounds for both. We further show that these algorithms can play a critical role in improving performance in applications such as structured prediction.
661,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stability and Hypothesis Transfer Learning,"Ilja Kuzborskij, Francesco Orabona",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kuzborskij13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kuzborskij13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kuzborskij13.html,"We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it _ the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain."
662,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Covariance Operator Based Dimensionality Reduction with Extension to Semi-Supervised Settings,"Minyoung Kim, Vladimir Pavlovic","5:280-287, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/kim09a/kim09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_kim09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/kim09a.html,We consider the task of dimensionality reduction for regression (DRR) informed by real-valued multivariate labels. The problem is often treated as a regression task where the goal is to find a low dimensional representation of the input data that preserves the statistical correlation with the targets. Recently Covariance Operator Inverse Regression (COIR) was proposed as an effective solution that exploits the covariance structures of both input and output. COIR addresses known limitations of recent DRR techniques and allows a closed-form solution without resorting to explicit output space slicing often required by existing IR-based methods. In this work we provide a unifying view of COIR and other DRR techniques and relate them to the popular supervised dimensionality reduction methods including the canonical correlation analysis (CCA) and the linear discriminant analysis (LDA). We then show that COIR can be effectively extended to a semi-supervised learning setting where many of the input points lack their corresponding multivariate targets. A study of benefits of proposed approaches is presented on several important regression problems in both fully-supervised and semi-supervised settings.
663,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Implementable confidence sets in high dimensional regression,Alexandra Carpentier,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/carpentier15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_carpentier15,http://jmlr.csail.mit.edu/proceedings/papers/v38/carpentier15.html,"We consider the setting of linear regression in high dimension. We focus on the problem of constructing adaptive and honest confidence sets for the sparse parameter , i.e. we want to construct a confidence set for theta that contains theta with high probability, and that is as small as possible. The l 2 diameter of a such confidence set should depend on the sparsity S of - the larger S, the wider the confidence set. However, in practice, S is unknown. This paper focuses on constructing a confidence set for which contains with high probability, whose diameter is adaptive to the unknown sparsity S, and which is implementable in practice."
664,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Semi-supervised Clustering with Pairwise Constraints: A Discriminative Approach,Zhengdong Lu,"2:299-306, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/lu07a/lu07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_lu07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/lu07a.html,We consider the semi-supervised clustering problem where we know (with varying degree of certainty) that some sample pairs are (or are not) in the same class. Unlike previous efforts in adapting clustering algorithms to incorporate those pairwise relations our work is based on a discriminative model. We generalize the standard Gaussian process classifier (GPC) to express our classification preference. To use the samples not involved in pairwise relations we employ the graph kernels (covariance matrix) based on the entire data set. Experiments on a variety of data sets show that our algorithm significantly outperforms several state-of-the-art methods.
665,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Density-preserving quantization with application to graph downsampling,"Morteza Alamgir, Gˆbor Lugosi, Ulrike von Luxburg","JMLR W&CP 35 :543-559, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/alamgir14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_alamgir14,http://jmlr.csail.mit.edu/proceedings/papers/v35/alamgir14.html,"We consider the problem of vector quantization of i.i.d. samples drawn from a density \(p\) on \(\mathbb{R}^d\) . It is desirable that the representatives selected by the quantization algorithm have the same distribution \(p\) as the original sample points. However, quantization algorithms based on Euclidean distance, such as \(k\) -means, do not have this property. We provide a solution to this problem that takes the unweighted \(k\) -nearest neighbor graph on the sample as input. In particular, it does not need to have access to the data points themselves. Our solution generates quantization centers that are ``evenly spaced"". We exploit this property to downsample geometric graphs and show that our method produces sparse downsampled graphs. Our algorithm is easy to implement, and we provide theoretical guarantees on the performance of the proposed algorithm."
666,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning,"K. Lakshmanan, Ronald Ortner, Daniil Ryabko",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lakshmanan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lakshmanan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lakshmanan15.html,"We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of \(O(T^{3/4})\) after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be \(O(T^{2/3})\) asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space."
667,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Theoretical Analysis of Metric Hypothesis Transfer Learning,"Michaïl Perrot, Amaury Habrard",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/perrot15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/perrot15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_perrot15,http://jmlr.csail.mit.edu/proceedings/papers/v37/perrot15.html,"We consider the problem of transferring some a priori knowledge in the context of supervised metric learning approaches. While this setting has been successfully applied in some empirical contexts, no theoretical evidence exists to justify this approach. In this paper, we provide a theoretical justification based on the notion of algorithmic stability adapted to the regularized metric learning setting. We propose an on-average-replace-two-stability model allowing us to prove fast generalization rates when an auxiliary source metric is used to bias the regularizer. Moreover, we prove a consistency result from which we show the interest of considering biased weighted regularized formulations and we provide a solution to estimate the associated weight. We also present some experiments illustrating the interest of the approach in standard metric learning tasks and in a transfer learning problem where few labelled data are available."
668,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Online Time Series Prediction with Missing Data,"Oren Anava, Elad Hazan, Assaf Zeevi",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/anava15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/anava15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_anava15,http://jmlr.csail.mit.edu/proceedings/papers/v37/anava15.html,"We consider the problem of time series prediction in the presence of missing data. We cast the problem as an online learning problem in which the goal of the learner is to minimize prediction error. We then devise an efficient algorithm for the problem, which is based on autoregressive model, and does not assume any structure on the missing data nor on the mechanism that generates the time series. We show that our algorithmês performance asymptotically approaches the performance of the best AR predictor in hindsight, and corroborate the theoretic results with an empirical study on synthetic and real-world data."
669,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Active Sequential Learning with Tactile Feedback,"Hannes Saal, Jo_Anne Ting, Sethu Vijayakumar","9:677-684, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/saal10a/saal10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_saal10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/saal10a.html,We consider the problem of tactile discrimination with the goal of estimating an underlying state parameter in a sequential setting. If the data is continuous and high-dimensional collecting enough representative data samples becomes difficult. We present a framework that uses active learning to help with the sequential gathering of data samples using information-theoretic criteria to find optimal actions at each time step. We consider two approaches to recursively update the state parameter belief: an analytical Gaussian approximation and a Monte Carlo sampling method. We show how both active frameworks improve convergence demonstrating results on a real robotic hand-arm system that estimates the viscosity of liquids from tactile feedback data.
670,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Elementary Estimators for High-Dimensional Linear Regression,"Eunho Yang, Aurelie Lozano, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yangc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangc14.html,"We consider the problem of structurally constrained high-dimensional linear regression. This has attracted considerable attention over the last decade, with state of the art statistical estimators based on solving regularized convex programs. While these typically non-smooth convex programs can be solved in polynomial time, scaling the state of the art optimization methods to very large-scale problems is an ongoing and rich area of research. In this paper, we attempt to address this scaling issue at the source, by asking whether one can build simpler possibly closed-form estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized likelihood estimators! We answer this question in the affirmative, with variants of the classical ridge and OLS (ordinary least squares estimators) for linear regression. We analyze our estimators in the high-dimensional setting, and moreover provide empirical corroboration of its performance on simulated as well as real world microarray data."
671,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning,"Arash Afkanpour, Andrˆs Gy_rgy, Csaba Szepesvari, Michael Bowling",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/afkanpour13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_afkanpour13,http://jmlr.csail.mit.edu/proceedings/papers/v28/afkanpour13.html,"We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows sampling from this distribution in \(O(\log(d))\) time, making the total computational cost of the method to achieve an epsilon-optimal solution to be \(O(\log(d)/epsilon^2)\) , thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives."
672,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Adaptive Monte Carlo via Bandit Allocation,"James Neufeld, Andras Gyorgy, Csaba Szepesvari, Dale Schuurmans",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/neufeld14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/neufeld14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_neufeld14,http://jmlr.csail.mit.edu/proceedings/papers/v32/neufeld14.html,"We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic, costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages."
673,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Top-k Selection based on Adaptive Sampling of Noisy Preferences,"Robert Busa-Fekete, Balazs Szorenyi, Weiwei Cheng, Paul Weng, Eyke Huellermeier",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/busa-fekete13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/busa-fekete13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_busa-fekete13,http://jmlr.csail.mit.edu/proceedings/papers/v28/busa-fekete13.html,"We consider the problem of reliably selecting an optimal subset of fixed size from a given set of choice alternatives, based on noisy information about the quality of these alternatives. Problems of similar kind have been tackled by means of adaptive sampling schemes called racing algorithms. However, in contrast to existing approaches, we do not assume that each alternative is characterized by a real-valued random variable, and that samples are taken from the corresponding distributions. Instead, we only assume that alternatives can be compared in terms of pairwise preferences. We propose and formally analyze a general preference-based racing algorithm that we instantiate with three specific ranking procedures and corresponding sampling schemes. Experiments with real and synthetic data are presented to show the efficiency of our approach."
674,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Latent feature regression for multivariate count data,"Arto Klami, Abhishek Tripathi, Johannes Sirola, Lauri V_re, Frederic Roulland",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/klami15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_klami15,http://jmlr.csail.mit.edu/proceedings/papers/v38/klami15.html,"We consider the problem of regression on multivariate count data and present a Gibbs sampler for a latent feature regression model suitable for both under- and overdispersed response variables. The model learns count-valued latent features conditional on arbitrary covariates, modeling them as negative binomial variables, and maps them into the dependent count-valued observations using a Dirichlet-multinomial distribution. From another viewpoint, the model can be seen as a generalization of a specific topic model for scenarios where we are interested in generating the actual counts of observations and not just their relative frequencies and co-occurrences. The model is demonstrated on a smart traffic application where the task is to predict public transportation volume for unknown locations based on a characterization of the close-by services and venues."
675,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Fluid Dynamics Models for Low Rank Discriminant Analysis,"Yung_Kyun Noh, Byoung_Tak Zhang, Daniel Lee","9:565-572, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/noh10a/noh10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_noh10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/noh10a.html,We consider the problem of reducing the dimensionality of labeled data for classification. Unfortunately the optimal approach of finding the low-dimensional projection with minimal Bayes classification error is intractable so most standard algorithms optimize a tractable heuristic function in the projected subspace. Here we investigate a physics-based model where we consider the labeled data as interacting fluid distributions. We derive the forces arising in the fluids from information theoretic potential functions and consider appropriate low rank constraints on the resulting acceleration and velocity flow fields. We show how to apply the Gauss principle of least constraint in fluids to obtain tractable solutions for low rank projections. Our fluid dynamic approach is demonstrated to better approximate the Bayes optimal solution on Gaussian systems including infinite dimensional Gaussian processes.
676,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Graphs with a Few Hubs,"Rashish Tandon, Pradeep Ravikumar",none,http://jmlr.org/proceedings/papers/v32/tandon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/tandon14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tandon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tandon14.html,"We consider the problem of recovering the graph structure of a –hub-networked” Ising model given iid samples, under high-dimensional settings, where number of nodes \(p\) could be potentially larger than the number of samples \(n\) . By a –hub-networked” graph, we mean a graph with a few –hub nodes” with very large degrees. State of the art estimators for Ising models have a sample complexity that scales polynomially with the maximum node-degree, and are thus ill-suited to recovering such graphs with a few hub nodes. Some recent proposals for specifically recovering hub graphical models do not come with theoretical guarantees, and even empirically provide limited improvements over vanilla Ising model estimators. Here, we show that under such low sample settings, instead of estimating –difficult” components such as hub-neighborhoods, we can use quantitative indicators of our inability to do so, and thereby identify hub-nodes. This simple procedure allows us to recover hub-networked graphs with very strong statistical guarantees even under very low sample settings."
677,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Complete Dictionary Recovery Using Nonconvex Optimization,"Ju Sun, Qing Qu, John Wright",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sund15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sund15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sund15.html,"We consider the problem of recovering a complete (i.e., square and invertible) dictionary \(mb A_0\) , from \(mb Y = mb A_0 mb X_0\) with \(mb Y \in \mathbb R^{n \times p}\) . This recovery setting is central to the theoretical understanding of dictionary learning. We give the first efficient algorithm that provably recovers \(mb A_0\) when \(mb X_0\) has \(O(n)\) nonzeros per column, under suitable probability model for \(mb X_0\) . Prior results provide recovery guarantees when \(mb X_0\) has only \(O(\sqrt{n})\) nonzeros per column. Our algorithm is based on nonconvex optimization with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. Our proofs give a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no spurious local minima. Experiments with synthetic data corroborate our theory. Full version of this paper is available online: http://arxiv.org/abs/1504.06785 ."
678,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,"Belief propagation, robust reconstruction and optimal recovery of block models","Elchanan Mossel, Joe Neeman, Allan Sly","JMLR W&CP 35 :356-370, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/mossel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_mossel14,http://jmlr.csail.mit.edu/proceedings/papers/v35/mossel14.html,"We consider the problem of reconstructing sparse symmetric block models with two blocks and connection probabilities \(a/n\) and \(b/n\) for inter- and intra-block edge probabilities respectively. It was recently shown that one can do better than a random guess if and only if \((a-b)^2 _ 2(a+b)\) . Using a variant of Belief Propagation, we give a reconstruction algorithm that is optimal in the sense that if \((a-b)^2 _ C (a+b)\) for some constant \(C\) then our algorithm maximizes the fraction of the nodes labelled correctly. Along the way we prove some results of independent interest regarding robust reconstruction for the Ising model on regular and Poisson trees."
679,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Why Steiner-tree type algorithms work for community detection,"Mung Chiang, Henry Lam, Zhenming Liu, Vincent Poor",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/chiang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/chiang13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_chiang13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/chiang13a.html,"We consider the problem of reconstructing a specific connected community \(S \subset V\) in a graph \(G = (V, E)\) , where each node \(v\) is associated with a signal whose strength grows with the likelihood that \(v\) belongs to \(S\) . This problem appears in social or protein interaction network, the latter also referred to as the signaling pathway reconstruction problem. We study this community reconstruction problem under several natural generative models, and make the following two contributions. First, in the context of social networks, where the signals are modeled as bounded-supported random variables, we design an efficient algorithm for recovering most members in \(S\) with well-controlled false positive overhead, by utilizing the network structure for a large family of –homogeneous” generative models. This positive result is complemented by an information theoretic lower bound for the case where the network structure is unknown or the network is heterogeneous. Second, we consider the case in which the graph represents the protein interaction network, in which it is customary to consider signals that have unbounded support, we generalize our first contribution to give the first theoretical justification of why existing Steiner-tree type heuristics work well in practice."
680,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top,"Arun Rajkumar, Suprovat Ghoshal, Lek-Heng Lim, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajkumar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajkumar15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_rajkumar15,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajkumar15.html,"We consider the problem of ranking \(n\) items from stochastically sampled pairwise preferences. It was shown recently that when the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm, the Matrix Borda algorithm, and the SVM-RankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal, 2014). In this paper, we consider settings where pairwise preferences can contain cycles. In such settings, one may still like to be able to recover •goodê items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be ranked at the top. More generally, several tournament solution concepts such as the top cycle, Copeland set, Markov set and others have been proposed in the social choice literature for choosing a set of winners in the presence of cycles. We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners and various natural tournament solution sets at the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm to recover an optimal ranking under a Bradley-Terry-Luce (BTL) condition, which answers an open question of Rajkumar and Agarwal (2014)."
681,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Bayesian Multiple Target Localization,"Purnima Rajan, Weidong Han, Raphael Sznitman, Peter Frazier, Bruno Jedynak",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajan15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_rajan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/rajan15.html,"We consider the problem of quickly localizing multiple targets by asking questions of the form ``How many targets are within this set"" while obtaining noisy answers. This setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried. We assume that the targets are points on the real line, or in a two dimensional plane for the experiments, drawn independently from a known distribution. We evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers. We derive a lower bound for the value of this problem and study a specific policy, named the dyadic policy. We show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free, and show a more general constant factor approximation guarantee for the noisy setting. We present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image. Finally, we present experiments on localizing multiple faces simultaneously on real images."
682,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Distributed Learning, Communication Complexity and Privacy","Maria Florina Balcan, Avrim Blum, Shai Fine and Yishay Mansour",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12a/balcan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_balcan12a,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12a.html,"We consider the problem of PAC-learning from distributed data and analyze fundamental communication complexity questions involved. We provide general upper and lower bounds on the amount of communication needed to learn well, showing that in addition to VC-dimension and covering number, quantities such as the teaching-dimension and mistake-bound of a class play an important role. We also present tight results for a number of common concept classes including conjunctions, parity functions, and decision lists. For linear separators, we show that for non-concentrated distributions, we can use a version of the Perceptron algorithm to learn with much less communication than the number of updates given by the usual margin bound. We also show how boosting can be performed in a generic manner in the distributed setting to achieve communication with only logarithmic dependence on 1/_ for any concept class, and demonstrate how recent work on agnostic learning from class-conditional queries can be used to achieve low communication in agnostic settings as well. We additionally present an analysis of privacy, considering both differential privacy and a notion of distributional privacy that is especially appealing in this context."
683,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions,"Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander Rakhlin",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Belloni15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Belloni15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Belloni15.html,"We consider the problem of optimizing an approximately convex function over a bounded convex set in \(\mathbb{R}^n\) using only function evaluations. The problem is reduced to sampling from an approximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \(\mathcal{O}^*\) complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an \(\epsilon\) -minimizer after \(\mathcal{O}^*(n^{7.5}\epsilon^{-2})\) noisy function evaluations by inducing a \(\mathcal{O}(\epsilon/n)\) -approximately log concave distribution. We also consider in detail the case when the –amount of non-convexity” decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning."
684,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Structure Identification by Optimized Interventions,"Alberto Giovanni Busetto, Joachim Buhmann","5:57-64, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/busetto09a/busetto09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_busetto09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/busetto09a.html,We consider the problem of optimal experimental design in structure identification. Whereas standard approaches simply minimize Shannon's entropy of the estimated parameter posterior we show how to select between alternative model configurations too. Our method specifies the intervention that makes an experiment capable of determining whether or not a particular configuration hypothesis is correct. This is performed by a novel clustering technique in approximated Bayesian parameter estimation for non-linear dynamical systems. The computation of the perturbation that minimizes the effective number of clusters in the belief state is constrained by the increase of the expected Kullback-Leibler divergence between the parameter prior and the posterior. This enables the disambiguation of persisting alternative explanations in cases where standard design systematically fails. Its applicability is illustrated with a biochemical Goodwin model showing correct identification between multiple kinetic structures. We expect that our approach will prove useful especially for complex structures with reduced observability and multimodal posteriors.
685,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,A Chaining Algorithm for Online Nonparametric Regression,"Pierre Gaillard, S_bastien Gerchinovitz",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Gaillard15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Gaillard15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Gaillard15.html,"We consider the problem of online nonparametric regression with arbitrary deterministic sequences. Using ideas from the chaining technique, we design an algorithm that achieves a Dudley-type regret bound similar to the one obtained in a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound is expressed in terms of the metric entropy in the sup norm, which yields optimal guarantees when the metric and sequential entropies are of the same order of magnitude. In particular our algorithm is the first one that achieves optimal rates for online regression over H_lder balls. In addition we show for this example how to adapt our chaining algorithm to get a reasonable computational efficiency with similar regret guarantees (up to a log factor)."
686,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Boosting with Online Binary Learners for the Multiclass Bandit Problem,"Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu",none,http://jmlr.org/proceedings/papers/v32/chenb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chenb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenb14.html,"We consider the problem of online multiclass prediction in the bandit setting. Compared with the full-information setting, in which the learner can receive the true label as feedback after making each prediction, the bandit setting assumes that the learner can only know the correctness of the predicted label. Because the bandit setting is more restricted, it is difficult to design good bandit learners and currently there are not many bandit learners. In this paper, we propose an approach that systematically converts existing online binary classifiers to promising bandit learners with strong theoretical guarantee. The approach matches the idea of boosting, which has been shown to be powerful for batch learning as well as online learning. In particular, we establish the weak-learning condition on the online binary classifiers, and show that the condition allows automatically constructing a bandit learner with arbitrary strength by combining several of those classifiers. Experimental results on several real-world data sets demonstrate the effectiveness of the proposed approach."
687,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,First-order regret bounds for combinatorial semi-bandits,Gergely Neu,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Neu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Neu15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Neu15.html,"We consider the problem of online combinatorial optimization under semi-bandit feedback, where a learner has to repeatedly pick actions from a combinatorial decision set in order to minimize the total losses associated with its decisions. After making each decision, the learner observes the losses associated with its action, but not other losses. For this problem, there are several learning algorithms that guarantee that the learnerês expected regret grows as \(\widetilde{O}(\sqrt{T})\) with the number of rounds \(T\) . In this paper, we propose an algorithm that improves this scaling to \(\widetilde{O}(\sqrt{{L_T^*}})\) , where \(L_T^*\) is the total loss of the best action. Our algorithm is among the first to achieve such guarantees in a partial-feedback scheme, and the first one to do so in a combinatorial setting."
688,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Robust Multivariate Regression with Grossly Corrupted Observations and Its Application to Personality Prediction,"Xiaowei Zhang, Li Cheng, Tingshao Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhang15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhang15a,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhang15a.html,"We consider the problem of multivariate linear regression with a small fraction of the responses being missing and grossly corrupted, where the magnitudes and locations of such occurrences are not known in priori. This is addressed in our approach by explicitly taking into account the error source and its sparseness nature. Moreover, our approach allows each regression task to possess its distinct noise level. We also propose a new algorithm that is theoretically shown to always converge to the optimal solution of its induced non-smooth optimization problem. Experiments on controlled simulations suggest the competitiveness of our algorithm comparing to existing multivariate regression models. In particular, we apply our model to predict the Big-Five personality from user behaviors at Social Network Sites (SNSs) and microblogs, an important yet difficult problem in psychology, where empirical results demonstrate its superior performance with respect to related learning methods."
689,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Convex Multi-Task Learning by Clustering,"Aviad Barzilai, Koby Crammer",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/barzilai15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_barzilai15,http://jmlr.csail.mit.edu/proceedings/papers/v38/barzilai15.html,We consider the problem of multi-task learning in which tasks belong to hidden clusters. We formulate the learning problem as a novel convex optimization problem in which linear classifiers are combinations of (a small number of) some basis. Our formulation jointly learns both the basis and the linear combination. We propose a scalable optimization algorithm for finding the optimal solution. Our new methods outperform existing state-of-the-art methods on multi-task sentiment classification tasks.
690,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Graphlet decomposition of a weighted network,"Hossein Azari Soufiani, Edo Airoldi",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/azari12/azari12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_azari12,http://jmlr.csail.mit.edu/proceedings/papers/v22/azari12.html,We consider the problem of modeling networks with nonnegative edge weights. We develop a \emph{bit-string decomposition (BSD)} for weighted networks a new representation of social information based on social structure with an underlying semi-parametric statistical model. We develop a scalable inference algorithm which combines Expectation-Maximization with Bron-Kerbosch in a novel fashion for estimating the model's parameters from a network sample. We present theoretical descriptions to the computational complexity of the method. Finally we demonstrate the performance of the proposed methodology for synthetic data academic networks from Facebook and finding communities in a historical data from 19th century.
691,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Multiarmed Bandits With Limited Expert Advice,Satyen Kale,"JMLR W&CP 35 :107-122, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kale14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kale14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/kale14a.html,"We consider the problem of minimizing regret in the setting of advice-efficient multiarmed bandits with expert advice. We give an algorithm for the setting of \(K\) arms and \(N\) experts out of which we are allowed to query and use only \(M\) expertsê advice in each round, which has a regret bound of \(\tilde{O}\left(\sqrt{\frac{\min\{K, M\} N}{M} T}\right)\) after \(T\) rounds. We also prove that any algorithm for this problem must have expected regret at least \(\tilde{\Omega}\left(\sqrt{\frac{\min\{K, M\} N}{M}T}\right)\) , thus showing that our upper bound is nearly tight. This solves the COLT 2013 open problem of Seldin et al. (2013)."
692,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Contextual Dueling Bandits,"Miroslav DudÍk, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, Masrour Zoghi",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Dudik15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Dudik15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Dudik15.html,"We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (COLTê09), which we extend to incorporate context. Roughly, the learnerês goal is to find the best policy, or way of behaving, in some space of policies, although –best” is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner , a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space."
693,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs,"K. S. Sesh Kumar, Francis Bach",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13c-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kumar13c,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13c.html,"We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures. A supergradient method is used to solve the dual problem, with a run-time complexity of \(O(k^3 n^{k+2} \log n)\) for each iteration, where \(n\) is the number of variables and \(k\) is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach."
694,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Structure Learning of Mixed Graphical Models,"Jason Lee, Trevor Hastie",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/lee13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/lee13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_lee13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/lee13a.html,"We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme is new and follows naturally from a particular parametrization of the model."
695,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A fast algorithm for learning large scale preference relations,"Vikas C. Raykar, Ramani Duraiswami, Balaji Krishnapuram","2:388-395, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/raykar07a/raykar07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_raykar07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/raykar07a.html,We consider the problem of learning the ranking function that maximizes a generalization of the Wilcoxon-Mann-Whitney statistic on training data. Relying on an -exact approximation for the error-function we reduce the computational complexity of each iteration of a conjugate gradient algorithm for learning ranking functions from $O(m^2)$ to $O(m)$ where m is the size of the training data. Experiments on public benchmarks for ordinal regression and collaborative filtering show that the proposed algorithm is as accurate as the best available methods in terms of ranking accuracy when trained on the same data and is several orders of magnitude faster.
696,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Tractable Search for Learning Exponential Models of Rankings,"Bhushan Mandhani, Marina Meila","5:392-399, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/mandhani09a/mandhani09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_mandhani09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/mandhani09a.html,We consider the problem of learning the Generalized Mallows (GM) model of \cite{fv86} which represents a probability distribution over all possible permutations (aka rankings) of a given set of objects. The training data consists of a set of permutations. This problem generalizes the well known rank aggregation problem. Maximum Likelihood estimation of the GM model is NP-hard. An exact but inefficient search-based method was recently proposed for this problem. Here we introduce the first non-trivial heuristic function for this search. We justify it theoretically and show why it is admissible in practice. We experimentally demonstrate its effectiveness and show that it is superior to existing techniques for learning the GM model. We also show good performance of a family of faster approximate methods of search.
697,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Learning Sparsely Used Overcomplete Dictionaries,"Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, Rashish Tandon","JMLR W&CP 35 :123-137, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/agarwal14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_agarwal14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/agarwal14a.html,"We consider the problem of learning sparsely used overcomplete dictionaries, where each observation is a sparse combination of elements from an unknown overcomplete dictionary. We establish exact recovery when the dictionary elements are mutually incoherent. Our method consists of a clustering-based initialization step, which provides an approximate estimate of the true dictionary with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following alternating steps: 1) estimation of the dictionary coefficients for each observation through \(\ell_1\) minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions, our method converges at a linear rate to the true dictionary as well as the true coefficients for each observation."
698,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Exact Recovery of Sparsely-Used Dictionaries,"Daniel A. Spielman, Huan Wang and John Wright",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/spielman12/spielman12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_spielman12,http://jmlr.csail.mit.edu/proceedings/papers/v23/spielman12.html,"We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that O(n log n) samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms."
699,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Learning Exploration/Exploitation Strategies for Single Trajectory Reinforcement Learning,"Michael Castronovo, Francis Maes, Raphael Fonteneau, Damien Ernst","24:1-10, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/castronovo12a/castronovo12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_castronovo12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/castronovo12a.html,We consider the problem of learning high-performance Exploration/Exploitation (E/E) strategies for finite Markov Decision Processes (MDPs) when the MDP to be controlled is supposed to be drawn from a known probability distribution pM (‡). The performance criterion is the sum of discounted rewards collected by the E/E strategy over an infinite length trajectory. We propose an approach for solving this problem that works by considering a rich set of candidate E/E strategies and by looking for the one that gives the best average performances on MDPs drawn according to pM (‡). As candidate E/E strategies we consider index-based strategies parametrized by small formulas combining variables that include the estimated reward function the number of times each transition has occurred and the optimal value functions and the optimal value functions V_ and Q_ of the estimated MDP (obtained through value iteration). The search for the best formula is formalized as a multi-armed bandit problem each arm being associated with a formula. We experimentally compare the performances of the approach with R-max as well as with -Greedy strategies and the results are promising.
700,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Graph Approximation and Clustering on a Budget,"Ethan Fetaya, Ohad Shamir, Shimon Ullman",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/fetaya15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/fetaya15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_fetaya15,http://jmlr.csail.mit.edu/proceedings/papers/v38/fetaya15.html,"We consider the problem of learning from a similarity matrix (such as spectral clustering and low-dimensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results, which focused on spectral clustering with two clusters. We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper."
701,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Generative Moment Matching Networks,"Yujia Li, Kevin Swersky, Rich Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/li15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_li15,http://jmlr.csail.mit.edu/proceedings/papers/v37/li15.html,"We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database."
702,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Protocols for Learning Classifiers on Distributed Data,"Hal Daume III, Jeff Phillips, Avishek Saha, Suresh Venkatasubramanian",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/daume12/daume12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_daume12,http://jmlr.csail.mit.edu/proceedings/papers/v22/daume12.html,We consider the problem of learning classifiers for labeled data that has been distributed across several nodes. Our goal is to find a single classifier with small approximation error across all datasets while minimizing the communication between nodes. This setting models real-world communication bottlenecks in the processing of massive distributed datasets. We present several very general sampling-based solutions as well as some two-way protocols which have a provable exponential speed-up over any one-way protocol. We focus on core problems for noiseless data distributed across two or more nodes. The techniques we introduce are reminiscent of active learning but rather than actively probing labels nodes actively communicate with each other each node simultaneously learning the important data from another node.
703,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Joint Estimation of Structured Sparsity and Output Structure in Multiple-Output Regression via Inverse-Covariance Regularization,"Kyung-Ah Sohn, Seyoung Kim",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sohn12/sohn12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sohn12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sohn12.html,We consider the problem of learning a sparse regression model for predicting multiple related outputs given high-dimensional inputs where related outputs are likely to share common relevant inputs. Most of the previous methods for learning structured sparsity assumed that the structure over the outputs is known a priori and focused on designing regularization functions that encourage structured sparsity reflecting the given output structure. In this paper we propose a new approach for sparse multiple-output regression that can jointly learn both the output structure and regression coefficients with structured sparsity. Our approach reformulates the standard regression model into an alternative parameterization that leads to a conditional Gaussian graphical model and employes an inverse-covariance regularization. We show that the orthant-wise quasi-Newton algorithm developed for L1-regularized log-linear model can be adopted for a fast optimization for our method. We demonstrate our method on simulated datasets and real datasets from genetics and finances applications.
704,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Inference in a Partially Observed Queuing Model with Applications in Ecology,"Kevin Winner, Garrett Bernstein, Dan Sheldon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/winner15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/winner15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_winner15,http://jmlr.csail.mit.edu/proceedings/papers/v37/winner15.html,"We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an M/G/inf queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach."
705,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels,"Jiyan Yang, Vikas Sindhwani, Haim Avron, Michael Mahoney",none,http://jmlr.org/proceedings/papers/v32/yangb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yangb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangb14.html,"We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem."
706,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Relative Entropy Inverse Reinforcement Learning,"Abdeslam Boularias, Jens Kober, Jan Peters","15:182-189, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/boularias11a/boularias11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_boularias11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/boularias11a.html,We consider the problem of imitation learning where the examples demonstrated by an expert cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)-optimal policy can be computed for different reward functions. However this requirement can hardly be satisfied in systems with a large or continuous state space. In this paper we propose a model-free IRL algorithm where the relative entropy between the empirical distribution of the state-action trajectories under a baseline policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using learned MDP models. Empirical results on simulated car racing gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.
707,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On Identifying Good Options under Combinatorially Structured Feedback in Finite Noisy Environments,"Yifan Wu, Andras Gyorgy, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wub15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wub15.html,"We consider the problem of identifying a good option out of finite set of options under combinatorially structured, noisy feedback about the quality of the options in a sequential process: In each round, a subset of the options, from an available set of subsets, can be selected to receive noisy information about the quality of the options in the chosen subset. The goal is to identify the highest quality option, or a group of options of the highest quality, with a small error probability, while using the smallest number of measurements. The problem generalizes best-arm identification problems. By extending previous work, we design new algorithms that are shown to be able to exploit the combinatorial structure of the problem in a nontrivial fashion, while being unimprovable in special cases. The algorithms call a set multi-covering oracle, hence their performance and efficiency is strongly tied to whether the associated set multi-covering problem can be efficiently solved."
708,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Complexity-Based Approach to Calibration with Checking Rules,"Dean P. Foster, Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari","19:293-314, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/foster11a/foster11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_foster11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/foster11a.html,We consider the problem of forecasting a sequence of outcomes from an unknown source. The quality of the forecaster is measured by a family of checking rules. We prove upper bounds on the value of the associated game thus certifying the existence of a calibrated strategy for the forecaster. We show that complexity of the family of checking rules can be captured by the notion of a sequential cover introduced in \citep{RakSriTew10a}. Various natural assumptions on the class of checking rules are considered including finiteness of Vapnik-Chervonenkis and Littlestone's dimensions.
709,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Truthful Linear Regression,"Rachel Cummings, Stratis Ioannidis, Katrina Ligett",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cummings15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Cummings15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cummings15.html,"We consider the problem of fitting a linear model to data held by individuals who are concerned about their privacy. Incentivizing most players to truthfully report their data to the analyst constrains our design to mechanisms that provide a privacy guarantee to the participants; we use differential privacy to model individualsê privacy losses. This immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation, and existing approaches to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators. We overcome this challenge through an appropriate design of the computation and payment scheme."
710,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Polynomial Time Optimal Query Algorithms for Finding Graphs with Arbitrary Real Weights,Sung-Soon Choi,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Choi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Choi13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Choi13.html,"We consider the problem of finding the edges of a hidden weighted graph and their weights by using a certain type of queries as few times as possible, with focusing on two types of queries with additive property. For a set of vertices, the additive query asks the sum of weights of the edges with both ends in the set. For a pair of disjoint sets of vertices, the cross-additive query asks the sum of weights of the edges crossing between the two sets. These queries were motivated by DNA sequencing, population genetics, and artificial intelligence, and have been paid considerable attention to in computational learning. In this paper, we achieve an ultimate goal of recent years for graph finding with the two types of queries, by constructing the first polynomial time algorithms with optimal query complexity for the general class of graphs with n vertices and at most m edges in which the weights of edges are arbitrary real numbers. The algorithms are randomized and their query complexities are \(O(\frac{m \log n}{\log m})\) . These bounds improve the best known by a factor of \(\log m\) , and settle the open problem posed in some papers including [Choi and Kim, Optimal query complexity bounds for finding graphs, STOC 2008]. To build key components for graph finding, we consider the coin weighing problem with a spring scale. The problem itself has been paid much attention to in a long history of combinatorial search. We construct the first polynomial time algorithm with optimal query complexity for the general case in which the weight differences between counterfeit and authentic coins are arbitrary real numbers. We also construct the first polynomial time optimal query algorithm for finding the Fourier coefficients of a certain class of pseudo-Boolean functions."
711,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Robust Stochastic Principal Component Analysis,"John Goes, Teng Zhang, Raman Arora, Gilad Lerman","JMLR W&CP 33 :266-274, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/goes14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_goes14,http://jmlr.csail.mit.edu/proceedings/papers/v33/goes14.html,"We consider the problem of finding lower dimensional subspaces in the presence of outliers and noise in the online setting. In particular, we extend previous batch formulations of robust PCA to the stochastic setting with minimal storage requirements and runtime complexity. We introduce three novel stochastic approximation algorithms for robust PCA that are extensions of standard algorithms for PCA - the stochastic power method, incremental PCA and online PCA using matrix-exponentiated-gradient (MEG) updates. For robust online PCA we also give a sub-linear convergence guarantee. Our numerical results demonstrate the superiority of the the robust online method over the other robust stochastic methods and the advantage of robust methods over their non-robust counterparts in the presence of outliers in artificial and real scenarios."
712,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A recursive estimate for the predictive likelihood in a topic model,"James Scott, Jason Baldridge",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/scott13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_scott13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/scott13a.html,"We consider the problem of evaluating the predictive log likelihood of a previously un- seen document under a topic model. This task arises when cross-validating for a model hyperparameter, when testing a model on a hold-out set, and when comparing the performance of different fitting strategies. Yet it is known to be very challenging, as it is equivalent to estimating a marginal likelihood in Bayesian model selection. We propose a fast algorithm for approximating this likelihood, one whose computational cost is linear both in document length and in the number of topics. The method is a first-order approximation to the algorithm of Carvalho et al. (2010), and can also be interpreted as a one-particle, Rao-Blackwellized version of the ""left-to-right"" method of Wallach et al. (2009). On our test examples, the proposed method gives similar answers to these other methods, but at lower computational cost."
713,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Learning mixtures of Gaussians with maximum-a-posteriori oracle,Satyaki Mahalanabis,"15:489-497, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/mahalanabis11a/mahalanabis11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mahalanabis11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/mahalanabis11a.html,We consider the problem of estimating the parameters of a mixture of distributions where each component distribution is from a given parametric family e.g. exponential Gaussian etc. We define a learning model in which the learner has access to a ñmaximum-a-posterioriî oracle which given any sample from a mixture of distributions tells the learner which component distribution was the most likely to have generated it. We describe a learning algorithm in this setting which accurately estimates the parameters of a mixture of k spherical Gaussians in $R^d$ assuming the component Gaussians satisfy a mild separation condition. Our algorithm uses only polynomially many (in d k) samples and oracle calls and our separation condition is much weaker than those required by unsupervised learning algorithms like [Arora 01 Vempala 02].
714,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Sparse Nonparametric Density Estimation in High Dimensions Using the Rodeo,"Han Liu, John Lafferty, Larry Wasserman","2:283-290, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/liu07a/liu07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_liu07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/liu07a.html,We consider the problem of estimating the joint density of a d-dimensional random vector $X = (X_1  X_2 ... X_d )$ when d is large. We assume that the density is a product of a parametric component and a nonparametric component which depends on an unknown subset of the variables. Using a modification of a recently developed nonparametric regression framework called rodeo (regularization of derivative expectation operator) we propose a method to greedily select bandwidths in a kernel density estimate. It is shown empirically that the density rodeo works well even for very high dimensional problems. When the unknown density function satisfies a suitably defined sparsity condition and the parametric baseline density is smooth the approach is shown to achieve near optimal minimax rates of convergence and thus avoids the curse of dimensionality.
715,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Class-prior Estimation for Learning from Positive and Unlabeled Data,"Marthinus Christoffel, Gang Niu, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Christoffel15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Christoffel15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Christoffel15.html,"We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized \(L_1\) -distance gives a computationally efficient algorithm with an analytic solution, and establish its uniform deviation bound and estimation error bound. Finally, we experimentally demonstrate the usefulness of the proposed method."
716,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Gaussian Copula Precision Estimation with Missing Values,"Huahua Wang, Farideh Fazayeli, Soumyadeep Chatterjee, Arindam Banerjee","JMLR W&CP 33 :978-986, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wang14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14a.html,"We consider the problem of estimating sparse precision matrix of Gaussian copula distributions using samples with missing values in high dimensions. Existing approaches, primarily designed for Gaussian distributions, suggest using plugin estimators by disregarding the missing values. In this paper, we propose double plugin Gaussian (DoPinG) copula estimators to estimate the sparse precision matrix corresponding to non-paranormal distributions. DoPinG uses two plugin procedures and consists of three steps: (1) estimate nonparametric correlations based on observed values, including Kendallês tau and Spearmanês rho; (2) estimate the non-paranormal correlation matrix; (3) plug into existing sparse precision estimators. We prove that DoPinG copula estimators consistently estimate the non-paranormal correlation matrix at a rate of \(O(\frac{1}{(1-\delta)}\sqrt{\frac{\log p}{n}})\) , where \(\delta\) is the probability of missing values. We provide experimental results to illustrate the effect of sample size and percentage of missing data on the model performance. Experimental results show that DoPinG is significantly better than estimators like mGlasso, which are primarily designed for Gaussian data."
717,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Scalable Multidimensional Hierarchical Bayesian Modeling on Spark,"Robert Ormandi, Hongxia Yang, Quan Lu",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/ormandi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_ormandi15,http://jmlr.csail.mit.edu/proceedings/papers/v41/ormandi15.html,"We consider the problem of estimating occurrence rates of rare events for extremely sparse data using pre-existing hierarchies and selected features to perform inference along multiple dimensions. In particular, we focus on the problem of estimating click rates for {Advertiser, Publisher, User} tuples where both the Advertisers and the Publishers are organized as hierarchies that capture broad contextual information at different levels of granularities. Typically, the click rates are low and the coverage of the hierarchies and dimensions is sparse. To overcome these difficulties, we decompose the joint prior of the three-dimensional Click-Through-Rate (CTR) using tensor decomposition and propose a Multidimensional Hierarchical Bayesian framework (abbreviated as MadHab). We set up a specific framework of each dimension to model dimension-specific characteristics. More specifically, we consider the hierarchical beta process prior for the Advertiser dimension and for the Publisher dimension respectively and a feature-dependent mixture model for the User dimension. Besides the centralized implementation, we propose a distributed algorithm through Spark for inference which make the model highly scalable and suited for large scale data mining applications. We demonstrate that on a real world ads campaign platform our framework can effectively discriminate extremely rare events in terms of their click propensity."
718,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Space-Efficient Sampling,"Sudipto Guha, Andrew McGregor","2:171-178, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/guha07a/guha07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_guha07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/guha07a.html,We consider the problem of estimating nonparametric probability density functions from a sequence of independent samples. The central issue that we address is to what extent this can be achieved with only limited memory. Our main result is a space-efficient learning algorithm for determining the probability density function of a piecewise-linear distribution. However the primary goal of this paper is to demonstrate the utility of various techniques from the burgeoning field of data stream processing in the context of learning algorithms.
719,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Elementary Estimators for Sparse Covariance Matrices and other Structured Moments,"Eunho Yang, Aurelie Lozano, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangd14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangd14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yangd14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yangd14.html,"We consider the problem of estimating distributional parameters that are expected values of given feature functions. We are interested in recovery under high-dimensional regimes, where the number of variables \(p\) is potentially larger than the number of samples \(n\) , and where we need to impose structural constraints upon the parameters. In a natural distributional setting for this problem, the feature functions comprise the sufficient statistics of an exponential family, so that the problem would entail estimating structured moments of exponential family distributions. A special case of the above involves estimating the covariance matrix of a random vector, and where the natural distributional setting would correspond to the multivariate Gaussian distribution. Unlike the inverse covariance estimation case, we show that the regularized MLEs for covariance estimation, as well as natural Dantzig variants, are non-convex , even when the regularization functions themselves are convex; with the same holding for the general structured moment case. We propose a class of elementary convex estimators, that in many cases are available in closed-form , for estimating general structured moments. We then provide a unified statistical analysis of our class of estimators. Finally, we demonstrate the applicability of our class of estimators on real-world climatology and biology datasets."
720,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Information Complexity in Bandit Subset Selection,"Emilie Kaufmann, Shivaram Kalyanakrishnan",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Kaufmann13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Kaufmann13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Kaufmann13.html,"We consider the problem of efficiently exploring the arms of a stochastic bandit to identify the best subset. Under the PAC and the fixed-budget formulations, we derive improved bounds by using KL-divergence-based confidence intervals. Whereas the application of a similar idea in the regret setting has yielded bounds in terms of the KL-divergence between the arms, our bounds in the pure-exploration setting involve the Chernoff information between the arms. In addition to introducing this novel quantity to the bandits literature, we contribute a comparison between the –racing” and –smart sampling” strategies for pure-exploration problems, finding strong evidence in favor of the latter."
721,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood,"Xin Yuan, Ricardo Henao, Ephraim Tsalik, Raymond Langley, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yuan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yuan15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yuan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yuan15.html,"We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a max-margin version of the rank-likelihood. A discriminative factor model is then developed, integrating the new max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology."
722,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Differentially Private Stochastic Gradient Descent Algorithm for Multiparty Classification,"Arun Rajkumar, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/rajkumar12/rajkumar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_rajkumar12,http://jmlr.csail.mit.edu/proceedings/papers/v22/rajkumar12.html,We consider the problem of developing privacy preserving machine learning algorithms in a distributed multiparty setting. Here different parties own different parts of a data set and the goal is to learn a classifier from the entire data set without any party revealing any information about the individual data points it owns. Pathak et al (2010) recently proposed a solution to this problem in which each party learns a local classifier from its own data and a third party then aggregates these classifiers in a privacy-preserving manner using a cryptographic scheme. The generalization performance of their algorithm is sensitive to the number of parties and the relative fractions of data owned by the different parties. In this paper we describe a new differentially private algorithm for the multiparty setting that uses a stochastic gradient descent based procedure to directly optimize the overall multiparty objective rather than combining classifiers learned from optimizing local objectives. The algorithm achieves a slightly weaker form of differential privacy than that of Pathak et al (2010) but provides improved generalization guarantees that do not depend on the number of parties or the relative sizes of the individual data sets. Experimental results corroborate our theoretical findings.
723,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation,"Hema Koppula, Ashutosh Saxena",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/koppula13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_koppula13,http://jmlr.csail.mit.edu/proceedings/papers/v28/koppula13.html,"We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects."
724,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Maxout Networks,"Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/goodfellow13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_goodfellow13,http://jmlr.csail.mit.edu/proceedings/papers/v28/goodfellow13.html,"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We dene a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropouts fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classication performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN."
725,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On Symmetric and Asymmetric LSHs for Inner Product Search,"Behnam Neyshabur, Nathan Srebro",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/neyshabur15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/neyshabur15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_neyshabur15,http://jmlr.csail.mit.edu/proceedings/papers/v37/neyshabur15.html,"We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required."
726,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Improving posterior marginal approximations in latent Gaussian models,"Botond Cseke, Tom Heskes","9:121-128, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/cseke10a/cseke10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_cseke10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/cseke10a.html,We consider the problem of correcting the posterior marginal approximations computed by expectation propagation and Laplace approximation in latent Gaussian models and propose correction methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show that in the case of sparse Gaussian models the computational complexity of expectation propagation can be made comparable to that of the Laplace approximation by using a parallel updating scheme. In some cases expectation propagation gives excellent estimates where the Laplace approximation fails. Inspired by bounds on the marginal corrections we arrive at factorized approximations which can be applied on top of both expectation propagation and Laplace. These give nearly indistinguishable results from the non-factorized approximations in a fraction of the time.
727,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Linear Programming for Large-Scale Markov Decision Problems,"Alan Malek, Yasin Abbasi-Yadkori, Peter Bartlett",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/malek14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/malek14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_malek14,http://jmlr.csail.mit.edu/proceedings/papers/v32/malek14.html,"We consider the problem of controlling a Markov decision process (MDP) with a large state space, so as to minimize average cost. Since it is intractable to compete with the optimal policy for large scale problems, we pursue the more modest goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a neighborhood of a low-dimensional subset of the set of stationary distributions (defined in terms of state-action features) as the comparison class. We propose two techniques, one based on stochastic convex optimization, and one based on constraint sampling. In both cases, we give bounds that show that the performance of our algorithms approaches the best achievable by any policy in the comparison class. Most importantly, these results depend on the size of the comparison class, but not on the size of the state space. Preliminary experiments show the effectiveness of the proposed algorithms in a queuing application."
728,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Nonnegative Garrote Component Selection in Functional ANOVA models,Ming Yuan,"2:660-666, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/yuan07b/yuan07b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_yuan07b,http://jmlr.csail.mit.edu/proceedings/papers/v2/yuan07b.html,We consider the problem of component selection in a functional ANOVA model. A nonparametric extension of the nonnegative garrote (Breiman 1996) is proposed. We show that the whole solution path of the proposed method can be efficiently computed which in turn  facilitates the selection of the tuning parameter. We also show that the final estimate enjoys nice theoretical properties given that the tuning parameter is appropriately chosen. Simulation and a real data example demonstrate promising performance of the new approach.
729,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Optimal aggregation of affine estimators,"Joseph Salmon, Arnak Dalalyan","19:635-660, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/salmon11a/salmon11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_salmon11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/salmon11a.html,We consider the problem of combining a (possibly uncountably infinite) set of affine estimators innon-parametric regression model with heteroscedastic Gaussian noise. Focusing on the exponentially weighted aggregate we prove a PAC-Bayesian type inequality that leadsto sharp oracle inequalities in discrete but also in continuous settings. The framework is general enough to cover the combinations of various procedures such asleast square regression kernel ridge regression shrinking estimators and many other estimators used in the literature on statistical inverse problems. As aconsequence we show that the proposed aggregateprovides an adaptive estimator in the exact minimax sense without neither discretizing the range of tuningparameters nor splitting the set of observations. We also illustrate numerically the good performance achievedby the exponentially weighted aggregate.
730,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Sparse Subspace Clustering with Missing Entries,"Congyuan Yang, Daniel Robinson, Rene Vidal",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangf15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yangf15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangf15.html,"We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large."
731,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian Mixtures,"Martin Azizyan, Aarti Singh, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/azizyan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/azizyan15-supp.zip,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_azizyan15,http://jmlr.csail.mit.edu/proceedings/papers/v38/azizyan15.html,"We consider the problem of clustering data points in high dimensions, i.e., when the number of data points may be much smaller than the number of dimensions. Specifically, we consider a Gaussian mixture model (GMM) with two non-spherical Gaussian components, where the clusters are distinguished by only a few relevant dimensions. The method we propose is a combination of a recent approach for learning parameters of a Gaussian mixture model and sparse linear discriminant analysis (LDA). In addition to cluster assignments, the method returns an estimate of the set of features relevant for clustering. Our results indicate that the sample complexity of clustering depends on the sparsity of the relevant feature set, while only scaling logarithmically with the ambient dimension. Further, we require much milder assumptions than existing work on clustering in high dimensions. In particular, we do not require spherical clusters nor necessitate mean separation along relevant dimensions."
732,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Bimodal Modelling of Source Code and Natural Language,"Miltos Allamanis, Daniel Tarlow, Andrew Gordon, Yi Wei",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/allamanis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/allamanis15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_allamanis15,http://jmlr.csail.mit.edu/proceedings/papers/v37/allamanis15.html,"We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks."
733,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,GEV-Canonical Regression for Accurate Binary Class Probability Estimation when One Class is Rare,"Arpit Agarwal, Harikrishna Narasimhan, Shivaram Kalyanakrishnan, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwalc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwalc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_agarwalc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwalc14.html,"We consider the problem of binary class probability estimation (CPE) when one class is rare compared to the other. It is well known that standard algorithms such as logistic regression do not perform well on this task as they tend to under-estimate the probability of the rare class. Common fixes include under-sampling and weighting, together with various correction schemes. Recently, Wang & Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the generalized extreme value (GEV) distribution, which has been used for modeling rare events in statistics. The approach showed promising initial results, but combined with the logarithmic CPE loss implicitly used in their work, it results in a non-convex composite loss that is difficult to optimize. In this paper, we use tools from the theory of proper composite losses (Buja et al, 2005; Reid & Williamson, 2010) to construct a canonical underlying CPE loss corresponding to the GEV link, which yields a convex proper composite loss that we call the GEV-canonical loss; this loss is tailored for the task of CPE when one class is rare, and is easy to minimize using an IRLS-type algorithm similar to that used for logistic regression. Our experiments on both synthetic and real data demonstrate that the resulting algorithm _ which we term GEV-canonical regression _ outperforms common approaches such as under-sampling and weights correction for this problem."
734,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo,"Yu-Xiang Wang, Stephen Fienberg, Alex Smola",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangg15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangg15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wangg15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangg15.html,"We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to –differential privacy”, a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private –for free”; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an –anytime” algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets."
735,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Tractable Bayesian Inference of Time-Series Dependence Structure,"Michael Siracusa, John Fisher III","5:528-535, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/siracusa09a/siracusa09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_siracusa09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/siracusa09a.html,We consider the problem of Bayesian inference over graphical structures describing the interactions among multiple vector time-series. A directed temporal interaction model is presented which assumes a fixed dependence structure among time-series. Using a conjugate prior over this model's structure and parameters we focus our attention on characterizing the exact posterior uncertainty in the structure given data. The model is extended via the introduction of a dynamically evolving latent variable which indexes dependence structures over time. Performing inference using this model yields promising results when analyzing the interaction of multiple tracked moving objects.
736,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Average Reward Optimization Objective In Partially Observable Domains,"Yuri Grinberg, Doina Precup",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/grinberg13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/grinberg13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_grinberg13,http://jmlr.csail.mit.edu/proceedings/papers/v28/grinberg13.html,"We consider the problem of average reward optimization in domains with partial observability, within the modeling framework of linear predictive state representations (PSRs). The key to average-reward computation is to have a well-defined stationary behavior of a system, so the required averages can be computed. If, additionally, the stationary behavior varies smoothly with changes in policy parameters, average-reward control through policy search also becomes a possibility. In this paper, we show that PSRs have a well-behaved stationary distribution, which is a rational function of policy parameters. Based on this result, we define a related reward process particularly suitable for average reward optimization, and analyze its properties. We show that in such a predictive state reward process, the average reward is a rational function of the policy parameters, whose complexity depends on the dimension of the underlying linear PSR. This result suggests that average reward-based policy search methods can be effective when the dimension of the system is small, even when the system representation in the POMDP framework requires many hidden states. We provide illustrative examples of this type."
737,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Dynamic Programming Boosting for Discriminative Macro-Action Discovery,"Leonidas Lefakis, Francois Fleuret",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lefakis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lefakis14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lefakis14.html,"We consider the problem of automatic macro-action discovery in imitation learning, which we cast as one of change-point detection. Unlike prior work in change-point detection, the present work leverages discriminative learning algorithms. Our main contribution is a novel supervised learning algorithm which extends the classical Boosting framework by combining it with dynamic programming. The resulting process alternatively improves the performance of individual strong predictors and the estimated change-points in the training sequence. Empirical evaluation is presented for the proposed method on tasks where change-points arise naturally as part of a classification problem. Finally we show the applicability of the algorithm to macro-action discovery in imitation learning and demonstrate it allows us to solve complex image-based goal-planning problems with thousands of features."
738,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A simple sketching algorithm for entropy estimation over streaming data,"Peter Clifford, Ioana Cosma",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/clifford13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_clifford13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/clifford13a.html,"We consider the problem of approximating the empirical Shannon entropy of a high-frequency data stream under the relaxed strict-turnstile model, when space limitations make exact computation infeasible. An equivalent measure of entropy is the Renyi entropy that depends on a constant \(\alpha\) . This quantity can be estimated efficiently and unbiasedly from a low-dimensional synopsis called an \(\alpha\) -stable data sketch via the method of compressed counting. An approximation to the Shannon entropy can be obtained from the Renyi entropy by taking alpha sufficiently close to 1. However, practical guidelines for parameter calibration with respect to \(\alpha\) are lacking. We avoid this problem by showing that the random variables used in estimating the Renyi entropy can be transformed to have a proper distributional limit as \(\alpha\) approaches 1: the maximally skewed, strictly stable distribution with \(\alpha = 1\) defined on the entire real line. We propose a family of asymptotically unbiased log-mean estimators of the Shannon entropy, indexed by a constant \(\zeta _ 0\) , that can be computed in a single-pass algorithm to provide an additive approximation. We recommend the log-mean estimator with \(\zeta = 1\) that has exponentially decreasing tail bounds on the error probability, asymptotic relative efficiency of 0.932, and near-optimal computational complexity."
739,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Concentration-Based Guarantees for Low-Rank Matrix Reconstruction,"Rina Foygel, Nathan Srebro","19:315-340, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/foygel11a/foygel11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_foygel11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/foygel11a.html,We consider the problem of approximately reconstructing a partially-observed approximately low-rank matrix. This problem has received much attention lately mostly using the trace-norm as a surrogate to the rank. Here we study low-rank matrix reconstruction using both the trace-norm as well as the less-studied max-norm and present reconstruction guarantees based on existing analysis on the Rademacher complexity of the unit balls of these norms. We show how these are superior in several ways to recently published guarantees based on specialized analysis.
740,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Approximate inference for the loss-calibrated Bayesian,"Simon Lacoste_Julien, Ferenc Huszar, Zoubin Ghahramani","15:416-424, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/lacoste_julien11a/lacoste_julien11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_lacoste_julien11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/lacoste_julien11a.html,We consider the problem of approximate inference in the context of Bayesian decision theory. Traditional approaches focus on approximating general properties of the posterior ignoring the decision task -- and associated losses -- for which the posterior could be used. We argue that this can be suboptimal and propose instead to loss-calibrate the approximate inference methods with respect to the decision task at hand. We present a general framework rooted in Bayesian decision theory to analyze approximate inference from the perspective of losses opening up several research directions. As a first loss-calibrated approximate inference attempt we propose an EM-like algorithm on the Bayesian posterior risk and show how it can improve a standard approach to Gaussian process classification when losses are asymmetric.
741,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scalable Variational Inference in Log-supermodular Models,"Josip Djolonga, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/djolonga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/djolonga15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_djolonga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/djolonga15.html,"We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field and variants. We show that a recently proposed variational approach to inference in log-supermodular models _ L-Field _ reduces to the widely studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and allows solving the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-Field, demonstrating that it exactly minimizes a specific type of Renyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-Field. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals and the benefit of incorporating higher order potentials."
742,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Toward Optimal Stratification for Stratified Monte-Carlo Integration,"Alexandra Carpentier, R_mi Munos",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/carpentier13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_carpentier13,http://jmlr.csail.mit.edu/proceedings/papers/v28/carpentier13.html,"We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm Monte-Carlo Upper-Lower Confidence Bound that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition."
743,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Variance Minimization Criterion to Active Learning on Graphs,"Ming Ji, Jiawei Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/ji12/ji12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_ji12,http://jmlr.csail.mit.edu/proceedings/papers/v22/ji12.html,We consider the problem of active learning over the vertices in a graph without feature representation. Our study is based on the common graph smoothness assumption which is formulated in a Gaussian random field model. We analyze the probability distribution over the unlabeled vertices conditioned on the label information which is a multivariate normal with the mean being the harmonic solution over the field. Then we select the nodes to label such that the total variance of the distribution on the unlabeled data as well as the expected prediction error is minimized. In this way the classifier we obtain is theoretically more robust. Compared with existing methods our algorithm has the advantage of selecting data in a batch offline mode with solid theoretical support. We show improved performance over existing label selection criteria on several real world data sets.
744,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Generalized Non-metric Multidimensional Scaling,"Sameer Agarwal, Josh Wills, Lawrence Cayton, Gert Lanckriet, David Kriegman, Serge Belongie","2:11-18, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/agarwal07a/agarwal07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_agarwal07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/agarwal07a.html,We consider the non-metric multidimensional scaling problem: given a set of dissimilarities $\Delta$ find an embedding whose inter-point Euclidean distances have the same ordering as $\Delta$. In this paper we look at a generalization of this problem in which only a set of order relations of the form $d_{ij} _ d_{kl}$ are provided. Unlike the original problem these order relations can be contradictory and need not be specified for all pairs of dissimilarities. We argue that this setting is more natural in some experimental settings and propose an algorithm based on convex optimization techniques to solve this problem. We apply this algorithm to human subject data from a psychophysics experiment concerning how reflectance properties are perceived. We also look at the standard NMDS problem where a dissimilarity matrix $\Delta$ is provided as input and show that we can always find an orderrespecting embedding of $\Delta$.
745,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sparsistency of 1-Regularized M-Estimators,"Yen-Huan Li, Jonathan Scarlett, Pradeep Ravikumar, Volkan Cevher",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15f.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15f-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15f,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15f.html,"We consider the model selection consistency or sparsistency of a broad set of 1 -regularized M-estimators for linear and non-linear statistical models in a unified fashion. For this purpose, we propose the local structured smoothness condition (LSSC) on the loss function. We provide a general result giving deterministic sufficient conditions for sparsistency in terms of the regularization parameter, ambient dimension, sparsity level, and number of measurements. We show that several important statistical models have M-estimators that indeed satisfy the LSSC, and as a result, the sparsistency guarantees for the corresponding 1 -regularized M-estimators can be derived as simple applications of our main theorem."
746,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,A Convex Formulation for Mixed Regression with Two Components: Minimax Optimal Rates,"Yudong Chen, Xinyang Yi, Constantine Caramanis","JMLR W&CP 35 :560-604, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/chen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_chen14,http://jmlr.csail.mit.edu/proceedings/papers/v35/chen14.html,"We consider the mixed regression problem with two components, under adversarial and stochastic noise. We give a convex optimization formulation that provably recovers the true solution, and provide upper bounds on the recovery errors for both arbitrary noise and stochastic noise settings. We also give matching minimax lower bounds (up to log factors), showing that under certain assumptions, our algorithm is information-theoretically optimal. Our results represent the first (and currently only known) tractable algorithm guaranteeing successful recovery with tight bounds on recovery errors and sample complexity."
747,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Exponential Family Matrix Completion under Structural Constraints,"Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gunasekar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gunasekar14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gunasekar14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gunasekar14.html,"We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low_rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin_tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data_types, such as skewed_continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low_rank, such as block_sparsity, or a superposition structure of low_rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions ; and impose general structural constraints on the underlying matrix, as captured by a general regularizer \(\mathcal{R}(.)\) . We propose a simple convex regularized \(M\) _estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets."
748,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: The Statistical Query Complexity of Learning Sparse Halfspaces,Vitaly Feldman,"JMLR W&CP 35 :1283-1289, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_feldman14c,http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14c.html,"We consider the long-open problem of attribute-efficient learning of halfspaces. In this problem the learner is given random examples labeled by an unknown halfspace function \(f\) on \(\mathbb{R}^n\) . Further \(f\) is \(r\) -sparse, that is it depends on at most \(r\) out of \(n\) variables. An attribute-efficient learning algorithm is an algorithm that can output a hypothesis close to \(f\) using a polynomial in \(r\) and \(\log n\) number of examples (Blum, 1992). Despite a number of attempts and some partial progress, there are no efficient algorithms or hardness results for the problem. We propose a potentially easier question: what is the query complexity of this learning problem in the statistical query (SQ) model of Kearns (1998). We show that, as in the case of general PAC learning, the query complexity of attribute-efficient SQ learning of any concept class can be characterized by a combinatorial parameter of the concept class. The proposed question is then equivalent to estimating the value of this parameter for the concept class of halfspaces. A potentially simpler problem is to estimate this parameter for the concept class of decision lists, a subclass of halfspaces."
749,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Averaged Least-Mean-Squares: Bias-Variance Trade-offs and Optimal Sampling Distributions,"Alexandre Defossez, Francis Bach",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/defossez15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/defossez15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_defossez15,http://jmlr.csail.mit.edu/proceedings/papers/v38/defossez15.html,"We consider the least-squares regression problem and provide a detailed asymptotic analysis of the performance of averaged constant-step-size stochastic gradient descent. In the strongly-convex case, we provide an asymptotic expansion up to explicit exponentially decaying terms. Our analysis leads to new insights into stochastic approximation algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the generalization error may be divided into a variance term which is decaying as O(1/n), independently of the step-size g, and a bias term that decays as O(1/g 2 n 2 ); (c) when allowing non-uniform sampling of examples over a dataset, the choice of a good sampling density depends on the trade-off between bias and variance: when the variance term dominates, optimal sampling densities do not lead to much gain, while when the bias term dominates, we can choose larger step-sizes that lead to significant improvements."
750,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Approximate Policy Iteration Schemes: A Comparison,Bruno Scherrer,none,http://jmlr.csail.mit.edu/proceedings/papers/v32/scherrer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/scherrer14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_scherrer14,http://jmlr.csail.mit.edu/proceedings/papers/v32/scherrer14.html,"We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP \(_\infty\) ), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API( \(\alpha\) ), but this comes at the cost of a relativeãexponential in \(\frac{1}{\epsilon}\) ãincrease of the number of iterations. 2) PSDP \(_\infty\) enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP \(_\infty\) is proportional to their number of iterations, which may be problematic when the discount factor \(\gamma\) is close to 1 or the approximation error \(\epsilon\) is close to \(0\) ; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis."
751,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Non-Stationary Approximate Modified Policy Iteration,"Boris Lesner, Bruno Scherrer",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lesner15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/lesner15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lesner15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lesner15.html,"We consider the infinite-horizon \(\gamma\) -discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iterationãa family of algorithms that can interpolate between Value and Policy Iterationãwith an error \(\epsilon\) at each iteration is known to lead to stationary policies that are at least \(\frac{2\gamma\epsilon}{(1-\gamma)^2}\) -optimal. Variations of Value and Policy Iteration, that build \(\ell\) -periodic non-stationary policies, have recently been shown to display a better \(\frac{2\gamma\epsilon}{(1-\gamma)(1-\gamma^\ell)}\) -optimality guarantee. Our first contribution is to describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers \(m \ge 0\) and \(\ell \ge 1\) that generalizes all the above mentionned algorithms. While \(m\) allows to interpolate between Value-Iteration-style and Policy-Iteration-style updates, \(\ell\) specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved \(\frac{2\gamma\epsilon}{(1-\gamma)(1-\gamma^\ell)}\) -optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all \(m\) and \(\ell\) ; this tightness was to our knowledge only proved two specific cases, Value Iteration \((m=0,\ell=1)\) and Policy Iteration \((m=\infty,\ell=1)\) ."
752,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Generalization error bounds for learning to rank: Does the length of document lists matter?,"Ambuj Tewari, Sougata Chaudhuri",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tewari15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/tewari15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tewari15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tewari15.html,"We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under \(\ell_1\) regularization and faster convergence rates if the loss function is smooth."
753,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Hard-Margin Active Linear Regression,"Elad Hazan, Zohar Karnin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hazan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hazan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hazan14.html,"We consider the fundamental problem of linear regression in which the designer can actively choose observations. This model naturally captures various experiment design settings in medical experiments, ad placement problems and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give efficient algorithms that attain near-optimal bounds."
754,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Guaranteed Sparse Recovery under Linear Transformation,"Ji Liu, Lei Yuan, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/liu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_liu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/liu13.html,"We consider the following signal recovery problem: given a measurement matrix \(\Phi\in \mathbb{R}^{n\times p}\) and a noisy observation vector \(c\in \mathbb{R}^{n}\) constructed from \(c = \Phi\theta^* + \epsilon\) where \(\epsilon\in \mathbb{R}^{n}\) is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal \(\theta^*\) if \(D\theta^*\) is sparse under a linear transformation \(D\in\mathbb{R}^{m\times p}\) ? One natural method using convex optimization is to solve the following problem: \[\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 + \lambda\|D\theta\|_1.\] This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix \(\Phi\) is a Gaussian random matrix. Specifically, we show 1) in the noiseless case, if the condition number of \(D\) is bounded and the measurement number \(n\geq \Omega(s\log(p))\) where \(s\) is the sparsity number, then the true solution can be recovered with high probability; and 2) in the noisy case, if the condition number of \(D\) is bounded and the measurement increases faster than \(s\log(p)\) , that is, \(s\log(p)=o(n)\) , the estimate error converges to zero with probability 1 when \(p\) and \(s\) go to infinity. Our results are consistent with those for the special case \(D=\bold{I}_{p\times p}\) (equivalently LASSO) and improve the existing analysis. The condition number of \(D\) plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if \(m\over p\) (i.e., \(\#\text{edge}\over \#\text{vertex}\) ) is larger than a certain constant. Numerical simulations are consistent with our theoretical results."
755,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Detecting Activations over Graphs using Spanning Tree Wavelet Bases,"James Sharpnack, Aarti Singh, Akshay Krishnamurthy",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_sharpnack13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13a.html,"We consider the detection of clusters of activation over graphs under Gaussian noise. This problem appears in many real world scenarios, such as the detecting contamination or seismic activity by sensor networks, viruses in human and computer networks, and groups with anomalous behavior in social and biological networks. Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with provable theoretical guarantees. To this end, we introduce the spanning tree wavelet basis over a graph, a localized basis that reflects the topology of the graph. We first provide a necessary condition for asymptotic distinguishability of the null and alternative hypotheses. Then we prove that for any spanning tree, we can hope to correctly detect signals in a low signal-to-noise regime using spanning tree wavelets. We propose a randomized test, in which we use a uniform spanning tree in the basis construction. Using electrical network theory, we show that the uniform spanning tree provides strong guarantees that in many cases match our necessary condition. We prove that for edge transitive graphs, \(k\) -nearest neighbor graphs, and \(\epsilon\) -graphs we obtain nearly optimal performance with the uniform spanning tree wavelet detector."
756,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,A Characterization of Scoring Rules for Linear Properties,Jacob D. Abernethy and Rafael M. Frongillo,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/abernethy12/abernethy12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_abernethy12,http://jmlr.csail.mit.edu/proceedings/papers/v23/abernethy12.html,"We consider the design of proper scoring rules, equivalently proper losses, when the goal is to elicit some function, known as a property, of the underlying distribution. We provide a full characterization of the class of proper scoring rules when the property is linear as a function of the input distribution. A key conclusion is that any such scoring rule can be written in the form of a Bregman divergence for some convex function. We also apply our results to the design of prediction market mechanisms, showing a strong equivalence between scoring rules for linear properties and automated prediction market makers."
757,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Changepoint Detection over Graphs with the Spectral Scan Statistic,"James Sharpnack, Aarti Singh, Alessandro Rinaldo",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13b-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_sharpnack13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/sharpnack13b.html,"We consider the change-point detection problem of deciding, based on noisy measurements, whether an unknown signal over a given graph is constant or is instead piecewise constant over two induced subgraphs of relatively low cut size. We analyze the corresponding generalized likelihood ratio (GLR) statistic and relate it to the problem of finding a sparsest cut in a graph. We develop a tractable relaxation of the GLR statistic based on the combinatorial Laplacian of the graph, which we call the spectral scan statistic, and analyze its properties. We show how its performance as a testing procedure depends directly on the spectrum of the graph, and use this result to explicitly derive its asymptotic properties on few graph topologies. Finally, we demonstrate both theoretically and by simulations that the spectral scan statistic can outperform naive testing procedures based on edge thresholding and \(\chi^2\) testing."
758,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Label Partitioning For Sublinear Ranking,"Jason Weston, Ameesh Makadia, Hector Yee",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/weston13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_weston13,http://jmlr.csail.mit.edu/proceedings/papers/v28/weston13.html,"We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance."
759,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Optimal Budget Allocation: Theoretical Guarantee and Efficient Algorithm,"Tasuku Soma, Naonori Kakimura, Kazuhiro Inaba, Ken-ichi Kawarabayashi",none,http://jmlr.org/proceedings/papers/v32/soma14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/soma14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_soma14,http://jmlr.csail.mit.edu/proceedings/papers/v32/soma14.html,"We consider the budget allocation problem over bipartite influence model proposed by Alon et al. This problem can be viewed as the well-known influence maximization problem with budget constraints. We first show that this problem and its much more general form fall into a general setting; namely the monotone submodular function maximization over integer lattice subject to a knapsack constraint. Our framework includes Alon et al.ês model, even with a competitor and with cost. We then give a (1-1/e)-approximation algorithm for this more general problem. Furthermore, when influence probabilities are nonincreasing, we obtain a faster (1-1/e)-approximation algorithm, which runs essentially in linear time in the number of nodes. This allows us to implement our algorithm up to almost 10M edges (indeed, our experiments tell us that we can implement our algorithm up to 1 billion edges. It would approximately take us only 500 seconds.)."
760,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Finite-sample Analysis of Bellman Residual Minimization,"Odalric-Ambrym Maillard, Remi Munos, Alessandro Lazaric, and Mohammad Ghavamzadeh","13:299-314, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/maillard10a/maillard10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_maillard10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/maillard10a.html,We consider the Bellman residual minimization approach for solving discounted Markov decision problems where we assume that a generative model of the dynamics and rewards is available. At each policy iteration step an approximation of the value function for the current policy is obtained by minimizing an empirical Bellman residual defined on a set of n states drawn i.i.d. from a distribution \mu the immediate rewards and the next states sampled from the model. Our main result is a generalization bound for the Bellman residual in linear approximation spaces. In particular we prove that the empirical Bellman residual approaches the true (quadratic) Bellman residual in \mu-norm with a rate of order O(1=pn). This result implies that minimizing the empirical residual is indeed a sound approach for the minimization of the true Bellman residual which guarantees a good approximation of the value function for each policy. Finally we derive performance bounds for the resulting approximate policy iteration algorithm in terms of the number of samples n and a measure of how well the function space is able to approximate the sequence of value functions.
761,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Convex formulations of radius-margin based Support Vector Machines,"Huyen Do, Alexandros Kalousis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/do13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_do13,http://jmlr.csail.mit.edu/proceedings/papers/v28/do13.html,"We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer fixed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound. In this paper we present two novel algorithms: \(R-SVM_{\mu}^+\) ãa SVM radius-margin based feature selection algorithm, and \(R-SVM^+\) ã a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets. \(R-SVM_{\mu}^+\) exhibits excellent feature selection performance compared to the state-of-the-art feature selection methods, such as \(L_1\) -norm and elastic-net based methods. \(R-SVM^+\) achieves a significantly better classification performance compared to SVM and its other state-of-the-art variants. From the results it is clear that the incorporation of the radius, as a means to control the data spread, in the cost function has strong beneficial effects."
762,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Sharp analysis of low-rank kernel matrix approximations,Francis Bach,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bach13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Bach13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bach13.html,"We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations \(n\) , i.e., \(O(n^2)\) . Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to \(O(p^2 n)\) , where \(p\) is the rank of the approximation. The practicality of such methods thus depends on the required rank \(p\) . In this paper, we show that for approximations based on a random subset of columns of the original kernel matrix, the rank \(p\) may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms, for any given problem instance, and not only for worst-case situations."
763,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Cheap Bandits,"Manjesh Hanawal, Venkatesh Saligrama, Michal Valko, Remi Munos",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hanawal15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hanawal15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hanawal15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hanawal15.html,"We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a \(\Omega(\sqrt(dT))\) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension \(d\) ."
764,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms,"Richard Combes, Alexandre Proutiere",none,http://jmlr.org/proceedings/papers/v32/combes14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_combes14,http://jmlr.csail.mit.edu/proceedings/papers/v32/combes14.html,"We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure."
765,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Thompson Sampling for Complex Online Problems,"Aditya Gopalan, Shie Mannor, Yishay Mansour",none,http://jmlr.org/proceedings/papers/v32/gopalan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gopalan14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gopalan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gopalan14.html,"We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic armsê rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems."
766,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Lipschitz Bandits: Regret Lower Bound and Optimal Algorithms,"Stefan Magureanu, Richard Combes, Alexandre Proutiere","JMLR W&CP 35 :975-999, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/magureanu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_magureanu14,http://jmlr.csail.mit.edu/proceedings/papers/v35/magureanu14.html,"We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities."
767,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares,"Garvesh Raskutti, Michael Mahoney",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/raskutti15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_raskutti15,http://jmlr.csail.mit.edu/proceedings/papers/v37/raskutti15.html,"We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an algorithmic perspective , when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples \(r\) much smaller than the original sample size \(n\) , then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a statistical perspective , one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when \(r\) is much smaller than \(n\) , while the PE typically requires the number of samples \(r\) to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved."
768,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Safe Exploration for Optimization with Gaussian Processes,"Yanan Sui, Alkis Gotovos, Joel Burdick, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sui15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/sui15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sui15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sui15.html,"We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified –safety” threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation."
769,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Thompson Sampling for Learning Parameterized Markov Decision Processes,"Aditya Gopalan, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Gopalan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Gopalan15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Gopalan15.html,"We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space."
770,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Actor-Critic Reinforcement Learning with Energy-Based Policies,"Nicolas Heess, David Silver, Yee Whye Teh","24:43-58, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/heess12a/heess12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_heess12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/heess12a.html,We consider reinforcement learning in Markov decision processes with high dimensional state and action spaces. We parametrize policies using energy-based models (particularly restricted Boltzmann machines) and train them using policy gradient learning. Our approach builds upon Sallans and Hinton (2004) who parameterized value functions using energy-based models trained using a non-linear variant of temporal-difference (TD) learning. Unfortunately non-linear TD is known to diverge in theory and practice. We introduce the first sound and efficient algorithm for training energy-based policies based on an actor-critic architecture. Our algorithm is computationally efficient converges close to a local optimum and outperforms Sallans and Hinton (2004) in several high dimensional domains.
771,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Variational methods for Reinforcement Learning,"Thomas Furmston, David Barber","9:241-248, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/furmston10a/furmston10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_furmston10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/furmston10a.html,We consider reinforcement learning as solving a Markov decision process with unknown transition distribution. Based on interaction with the environment an estimate of the transition matrix is obtained from which the optimal decision policy is formed. The classical maximum likelihood point estimate of the transition model does not reect the uncertainty in the estimate of the transition model and the resulting policies may consequently lack a sufficient degree of exploration. We consider a Bayesian alternative that maintains a distribution over the transition so that the resulting policy takes into account the limited experience of the environment. The resulting algorithm is formally intractable and we discuss two approximate solution methods Variational Bayes and Expectation Propagation.
772,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Better Rates for Any Adversarial Deterministic MDP,"Ofer Dekel, Elad Hazan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/dekel13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_dekel13,http://jmlr.csail.mit.edu/proceedings/papers/v28/dekel13.html,"We consider regret minimization in adversarial deterministic Markov Decision Processes (ADMDPs) with bandit feedback. We devise a new algorithm that pushes the state-of-the-art forward in two ways: First, it attains a regret of \(O(T^{2/3})\) with respect to the best fixed policy in hindsight, whereas the previous best regret bound was \(O(T^{3/4})\) . Second, the algorithm and its analysis are compatible with any feasible ADMDP graph topology, while all previous approaches required additional restrictions on the graph topology."
773,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Learning with Square Loss: Localization through Offset Rademacher Complexity,"Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Liang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Liang15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Liang15.html,"We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of for the bounded case while also providing guarantees without the boundedness assumption."
774,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Inference for Complex Queries on Complex Distributions,"Lili Dworkin, Michael Kearns, Lirong Xia","JMLR W&CP 33 :211-219, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/dworkin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/dworkin14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_dworkin14,http://jmlr.csail.mit.edu/proceedings/papers/v33/dworkin14.html,"We consider problems of approximate inference in which the query of interest is given by a complex formula (such as a formula in disjunctive formal form (DNF)) over a joint distribution given by a graphical model. We give a general reduction showing that (approximate) marginal inference for a class of distributions yields approximate inference for DNF queries, and extend our techniques to accommodate even more complex queries, and dense graphical models with variational inference, under certain conditions. Our results unify and generalize classical inference techniques (which are generally restricted to simple marginal queries) and approximate counting methods such as those introduced by Karp, Luby and Madras (which are generally restricted to product distributions)."
775,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Surrogate regret bounds for generalized classification performance metrics,"Wojciech Kotlowski, Krzysztof Dembczynski",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kotlowski15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Kotlowski15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kotlowski15.html,"We consider optimization of generalized performance metrics for binary classification by means of surrogate loss. We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include \(F_{\beta}\) -measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function \(f\) is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given \(f\) , a threshold \(\hat{\theta}\) is tuned on a separate validation sample, by direct optimization of the target performance measure. We show that the regret of the resulting classifier (obtained from thresholding \(f\) on \(\hat{\theta}\) ) measured with respect to the target metric is upperbounded by the regret of \(f\) measured with respect to the surrogate loss. Our finding is further analyzed in a computational study on both synthetic and real data sets."
776,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Online Similarity Prediction of Networked Data from Known and Unknown Graphs,"Claudio Gentile, Mark Herbster, Stephen Pasteris",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Gentile13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Gentile13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Gentile13.html,"We consider online similarity prediction problems over networked data. We begin by relating this task to the more standard class prediction problem, showing that, given an arbitrary algorithm for class prediction, we can construct an algorithm for similarity prediction with –nearly” the same mistake bound, and vice versa. After noticing that this general construction is computationally infeasible, we target our study to feasible similarity prediction algorithms on networked data. We initially assume that the network structure is known to the learner. Here we observe that Matrix Winnow (Warmuth, 2007) has a near-optimal mistake guarantee, at the price of cubic prediction time per round. This motivates our effort for an efficient implementation of a Perceptron-like algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time. Our focus then turns to the challenging case of networks whose structure is initially unknown to the learner. In this novel setting, where the network structure is only incrementally revealed, we obtain a mistake-bounded algorithm with a quadratic prediction time per round."
777,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Follow the Leader with Dropout Perturbations,"Tim Van Erven, Wojciech Kot _ owski, Manfred K. Warmuth","JMLR W&CP 35 :949-974, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/vanerven14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_vanerven14,http://jmlr.csail.mit.edu/proceedings/papers/v35/vanerven14.html,"We consider online prediction with expert advice. Over the course of many trials, the goal of the learning algorithm is to achieve small additional loss (i.e. regret) compared to the loss of the best from a set of \(K\) experts. The two most popular algorithms are Hedge/Weighted Majority and Follow the Perturbed Leader (FPL). The latter algorithm first perturbs the loss of each expert by independent additive noise drawn from a fixed distribution, and then predicts with the expert of minimum perturbed loss (–the leader”) where ties are broken uniformly at random. To achieve the optimal worst-case regret as a function of the loss \(L^*\) of the best expert in hindsight, the two types of algorithms need to tune their learning rate or noise magnitude, respectively, as a function of \(L^*\) . Instead of perturbing the losses of the experts with additive noise, we randomly set them to \(0\) or \(1\) before selecting the leader. We show that our perturbations are an instance of dropout ã because experts may be interpreted as features ã although for non-binary losses the dropout probability needs to be made dependent on the losses to get good regret bounds. We show that this simple, tuning-free version of the FPL algorithm achieves two feats: optimal worst-case \(O(\sqrt{L^* \ln K} + \ln K)\) regret as a function of \(L^*\) , and optimal \(O(\ln K)\) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a gap between the expected loss of the best expert and all others. A number of recent algorithms from the Hedge family (AdaHedge and FlipFlop) also achieve this, but they employ sophisticated tuning regimes. The dropout perturbation of the losses of the experts result in different noise distributions for each expert (because they depend on the expertês total loss) and curiously enough no additional tuning is needed: the choice of dropout probability only affects the constants."
778,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Towards Minimax Online Learning with Unknown Time Horizon,"Haipeng Luo, Robert Schapire",none,http://jmlr.org/proceedings/papers/v32/luo14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/luo14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_luo14,http://jmlr.csail.mit.edu/proceedings/papers/v32/luo14.html,"We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which –pretends” that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen, the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting."
779,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nonparametric Estimation of Renyi Divergence and Friends,"Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/krishnamurthy14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/krishnamurthy14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_krishnamurthy14,http://jmlr.csail.mit.edu/proceedings/papers/v32/krishnamurthy14.html,"We consider nonparametric estimation of \(L_2\) , Renyi- \(\alpha\) and Tsallis- \(\alpha\) divergences between continuous distributions. Our approach is to construct estimators for particular integral functionals of two densities and translate them into divergence estimators. For the integral functionals, our estimators are based on corrections of a preliminary plug-in estimator. We show that these estimators achieve the parametric convergence rate of \(n^{-1/2}\) when the densitiesê smoothness, \(s\) , are both at least \(d/4\) where \(d\) is the dimension. We also derive minimax lower bounds for this problem which confirm that \(s _ d/4\) is necessary to achieve the \(n^{-1/2}\) rate of convergence. We validate our theoretical guarantees with a number of simulations."
780,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Adaptive Bandits: Towards the best history-dependent strategy,"Maillard Odalric, Remi Munos","15:570-578, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/odalric11a/odalric11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_odalric11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/odalric11a.html,We consider multi-armed bandit games with possibly adaptive opponents. We introduce models Theta of constraints based on equivalence classes on the common history (information shared by the player and the opponent) which define two learning scenarios: (1) The opponent is constrained i.e.~he provides rewards that are stochastic functions of equivalence classes defined by some model theta* \in Theta. The regret is measured with respect to (w.r.t.) the best history-dependent strategy. (2) The opponent is arbitrary and we measure the regret w.r.t.~the best strategy among all mappings from classes to actions (i.e.~the best history-class-based strategy) for the best model in Theta. This allows to model opponents (case 1) or strategies (case 2) which handles finite memory periodicity standard stochastic bandits and other situations. When Theta={theta} i.e.~only one model is considered we derive tractable algorithms achieving a tight regret (at time T) bounded by \tilde O(\sqrt{TAC}) where C is the number of classes of \theta. Now when many models are available all known algorithms achieving a nice regret O(\sqrt{T}) are unfortunately not tractable and scale poorly with the number of models $|\Theta|$. Our contribution here is to provide tractable algorithms with regret bounded by T^{2/3}C^{1/3}\log(|\Theta|)^{1/2}.
781,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Modelling Policies in MDPs in Reproducing Kernel Hilbert Space,"Guy Lever, Ronnie Stafford",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lever15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lever15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lever15,http://jmlr.csail.mit.edu/proceedings/papers/v38/lever15.html,"We consider modelling policies for MDPs in (vector-valued) reproducing kernel Hilbert function spaces (RKHS). This enables us to work –non-parametrically” in a rich function class, and provides the ability to learn complex policies. We present a framework for performing gradient-based policy optimization in the RKHS, deriving the functional gradient of the return for our policy, which has a simple form and can be estimated efficiently. The policy representation naturally focuses on the relevant region of state space defined by the policy trajectories, and does not rely on a-priori defined basis points; this can be an advantage in high dimensions where suitable basis points may be difficult to define a-priori. The method is adaptive in the sense that the policy representation will naturally adapt to the complexity of the policy being modelled, which is achieved with standard efficient sparsification tools in an RKHS. We argue that finding a good kernel on states can be easier then remetrizing a high dimensional feature space. We demonstrate the approach on benchmark domains and a simulated quadrocopter navigation task."
782,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On the Rate of Convergence and Error Bounds for LSTD(\(\lambda\)),"Manel Tagorti, Bruno Scherrer",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tagorti15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/tagorti15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tagorti15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tagorti15.html,"We consider LSTD( \(\lambda\) ), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a \(\beta\) -mixing assumption, we derive, for any value of \(\lambda \in (0,1)\) , a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where \(\lambda=0\) . In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter \(\lambda\) with respect to the approximation quality of the space and the number of samples."
783,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Polytope samplers for inference in ill-posed inverse problems,"Edoardo Airoldi, Bertrand Haas","15:110-118, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/airoldi11a/airoldi11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_airoldi11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/airoldi11a.html,We consider linear ill-posed inverse problems y=Ax in which we want to infer many count parameters x from few count observations y where the matrix A is binary and has some unimodularity property. Such problems are typical in applications such as contingency table analysis and network tomography (on which we present testing results). These properties of A have a geometrical implication for the solution space: It is a convex integer polytope. We develop a novel approach to characterize this polytope in terms of its vertices; by taking advantage of the geometrical intuitions behind the Hermite normal form decomposition of the matrix A and of a newly defined pivoting operation to travel across vertices. Next we use this characterization to develop three (exact) polytope samplers for x with emphasis on uniform distributions. We showcase one of these samplers on simulated and real data.
784,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Moderated and Drifting Linear Dynamical Systems,"Jinyan Guan, Kyle Simek, Ernesto Brau, Clayton Morrison, Emily Butler, Kobus Barnard",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/guan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_guan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/guan15.html,"We consider linear dynamical systems, particularly coupled linear oscillators, where the parameters represent meaningful values in a domain theory and thus learning what affects them contributes to explanation. Rather than allow perturbations of latent states, we assume that temporal variation beyond noise is explained by parameter drift, and variation across coupled systems is a function of moderating variables. This change of focus reduces opportunities for efficient inference, and we propose sampling procedures to learn and fit the models. We test our approach on a real dataset of physiological measures of heterosexual couples engaged in a conversation about a potentially emotional topic, with body mass index (BMI) being considered as a moderator. We evaluate several models on their ability to predict future conversation dynamics (the last 20% of the data for each test couple), with shared parameters being learned using held out data. As proof of concept, we validate the hypothesis that BMI affects the conversation dynamic in the experimentally chosen topic."
785,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On Deep Multi-View Representation Learning,"Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wangb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangb15.html,"We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE)."
786,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Learning from Data with Heterogeneous Noise using SGD,"Shuang Song, Kamalika Chaudhuri, Anand Sarwate",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/song15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/song15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_song15,http://jmlr.csail.mit.edu/proceedings/papers/v38/song15.html,"We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogeneous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Finally, we evaluate the performance of our algorithm on real data."
787,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Model-Based Relational RL When Object Existence is Partially Observable,"Vien Ngo, marc Toussaint",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ngo14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ngo14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ngo14.html,"We consider learning and planning in relational MDPs when object existence is uncertain and new objects may appear or disappear depending on previous actions or properties of other objects. Optimal policies actively need to discover objects to achieve a goal; planning in such domains in general amounts to a POMDP problem, where the belief is about the existence and properties of potential not-yet-discovered objects. We propose a computationally efficient extension of model-based relational RL methods that approximates these beliefs using discrete uncertainty predicates. In this formulation the belief update is learned using probabilistic rules and planning in the approximated belief space can be achieved using an extension of existing planners. We prove that the learned belief update rules encode an approximation of the exact belief updates of a POMDP formulation and demonstrate experimentally that the proposed approach successfully learns a set of relational rules appropriate to solve such problems."
788,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scaling Up Robust MDPs using Function Approximation,"Aviv Tamar, Shie Mannor, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/tamar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/tamar14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tamar14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tamar14.html,"We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the –curse of dimensionality”, MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm."
789,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Robust Sparse Regression under Adversarial Corruption,"Yudong Chen, Constantine Caramanis, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13h.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13h-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13h,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13h.html,"We consider high dimensional sparse regression with arbitrary _ possibly, severe or coordinated _ errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error. We explore the power of a simple idea: replace the essential linear algebraic calculation _ the inner product _ with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust."
790,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint,"Ji Liu, Jieping Ye, Ryohei Fujimaki",none,http://jmlr.org/proceedings/papers/v32/liub14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liub14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liub14.html,"We consider forward-backward greedy algorithms for solving sparse feature selection problems with general convex smooth functions. A state-of-the-art greedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to solve a large number of optimization problems, thus it is not scalable for large-size problems. The FoBa-gdt algorithm, which uses the gradient information for feature selection at each forward iteration, significantly improves the efficiency of FoBa-obj. In this paper, we systematically analyze the theoretical properties of both algorithms. Our main contributions are: 1) We derive better theoretical bounds than existing analyses regarding FoBa-obj for general smooth convex functions; 2) We show that FoBa-gdt achieves the same theoretical performance as FoBa-obj under the same condition: restricted strong convexity condition. Our new bounds are consistent with the bounds of a special case (least squares) and fills a previously existing theoretical gap for general convex smooth functions; 3) We show that the restricted strong convexity condition is satisfied if the number of independent samples is more than \(\bar{k}\log d\) where \(\bar{k}\) is the sparsity number and \(d\) is the dimension of the variable; 4) We apply FoBa-gdt (with the conditional random field objective) to the sensor selection problem for human indoor activity recognition and our results show that FoBa-gdt outperforms other methods based on forward greedy selection and L1-regularization."
791,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On Convergence of Emphatic Temporal-Difference Learning,H. Yu,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Yu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Yu15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Yu15.html,"We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD( \(\lambda\) ) and ELSTD( \(\lambda\) ). We prove, under general off-policy conditions, the convergence in \(L^1\) for ELSTD( \(\lambda\) ) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD( \(\lambda\) ) also converges under off-policy training for \(\lambda\) sufficiently large."
792,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Latent Space Approach to Dynamic Embedding of Co-occurrence Data,"Purnamrita Sarkar, Sajid M. Siddiqi, Geogrey J. Gordon","2:420-427, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/sarkar07a/sarkar07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_sarkar07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/sarkar07a.html,We consider dynamic co-occurrence data such as author-word links in papers published in successive years of the same conference. For static co-occurrence data researchers often seek an embedding of the entities (authors and words) into a lowdimensional Euclidean space. We generalize a recent static co-occurrence model the CODE model of Globerson et al. (2004) to the dynamic setting: we seek coordinates for each entity at each time step. The coordinates can change with time to explain new observations but since large changes are improbable we can exploit data at previous and subsequent steps to find a better explanation for current observations. To make inference tractable we show how to approximate our observation model with a Gaussian distribution allowing the use of a Kalman filter for tractable inference. The result is the first algorithm for dynamic embedding of co-occurrence data which provides distributional information for its coordinate estimates. We demonstrate our model both on synthetic data and on author-word data from the NIPS corpus showing that it produces intuitively reasonable embeddings. We also provide evidence for the usefulness of our model by its performance on an author-prediction task.
793,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Distributed Learning of Gaussian Graphical Models via Marginal Likelihoods,"Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred Hero III",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/meng13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_meng13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/meng13a.html,"We consider distributed estimation of the inverse covariance matrix, also called the concentration matrix, in Gaussian graphical models. Traditional centralized estimation often requires iterative and expensive global inference and is therefore difficult in large distributed networks. In this paper, we propose a general framework for distributed estimation based on a maximum marginal likelihood (MML) approach. Each node independently computes a local estimate by maximizing a marginal likelihood defined with respect to data collected from its local neighborhood. Due to the non-convexity of the MML problem, we derive and consider solving a convex relaxation. The local estimates are then combined into a global estimate without the need for iterative message-passing between neighborhoods. We prove that this relaxed MML estimator is asymptotically consistent. Through numerical experiments on several synthetic and real-world data sets, we demonstrate that the two-hop version of the proposed estimator is significantly better than the one-hop version, and nearly closes the gap to the centralized maximum likelihood estimator in many situations."
794,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression,"Daniel Kifer, Adam Smith and Abhradeep Thakurta",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/kifer12/kifer12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_kifer12,http://jmlr.csail.mit.edu/proceedings/papers/v23/kifer12.html,"We consider differentially private algorithms for convex empirical risk minimization (ERM). Differential privacy (Dwork et al., 2006b) is a recently introduced notion of privacy which guarantees that an algorithm's output does not depend on the data of any individual in the dataset. This is crucial in fields that handle sensitive data, such as genomics, collaborative filtering, and economics. Our motivation is the design of private algorithms for sparse learning problems, in which one aims to find solutions (e.g., regression parameters) with few non-zero coefficients. To this end: (a)We significantly extend the analysis of the ""objective perturbation"" algorithm of Chaudhuri et al. (2011) for convex ERM problems. We show that their method can be modified to use less noise (be more accurate), and to apply to problems with hard constraints and non-differentiable regularizers. We also give a tighter, data-dependent analysis of the additional error introduced by their method. A key tool in our analysis is a new nontrivial limit theorem for differential privacy which is of independent interest: if a sequence of differentially private algorithms converges, in a weak sense, then the limit algorithm is also differentially private. In particular, our methods give the best known algorithms for differentially private linear regression. These methods work in settings where the number of parameters p is less than the number of samples n . (b)We give the first two private algorithms for sparse regression problems in high-dimensional settings, where p is much larger than n. We analyze their performance for linear regression: under standard assumptions on the data, our algorithms have vanishing empirical risk for n = poly(s , log p) when there exists a good regression vector with s nonzero coefficients. Our algorithms demonstrate that randomized algorithms for sparse regression problems can be both stable and accurate - a combination which is impossible for deterministic algorithms."
795,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Inference algorithms for pattern-based CRFs on sequence data,"Rustem Takhanov, Vladimir Kolmogorov",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/takhanov13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_takhanov13,http://jmlr.csail.mit.edu/proceedings/papers/v28/takhanov13.html,"We consider Conditional Random Fields (CRFs) with pattern-based potentials defined on a chain. In this model the energy of a string (labeling) \(x_1\ldots x_n\) is the sum of terms over intervals \([i,j]\) where each term is non-zero only if the substring \(x_i\ldots x_j\) equals a prespecified pattern \(\alpha\) . Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely computing (i) the partition function, (ii) marginals, and (iii) computing the MAP. Their complexities are respectively \(O(n L)\) , \(O(n L \ell_{\max})\) and \(O(n L \min\{|D|,\log (\ell_{\max} + 1)\})\) where \(L\) is the combined length of input patterns, \(\ell_{\max}\) is the maximum length of a pattern, and \(D\) is the input alphabet. This improves on the previous algorithms of whose complexities are respectively \(O(n L |D|)\) , \(O\left(n |\Gamma| L^2 \ell_{\max}^2\right)\) and \(O(n L |D|)\) , where \(|\Gamma|\) is the number of input patterns. In addition, we give an efficient algorithm for sampling, and revisit the case of MAP with non-positive weights. Finally, we apply pattern-based CRFs to the problem of the protein dihedral angles prediction."
796,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Classification with Low Rank and Missing Data,"Elad Hazan, Roi Livni, Yishay Mansour",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hazan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hazan15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hazan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hazan15.html,"We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data."
797,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Hedge Algorithm on a Continuum,"Walid Krichene, Maximilian Balandat, Claire Tomlin, Alexandre Bayen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/krichene15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/krichene15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_krichene15,http://jmlr.csail.mit.edu/proceedings/papers/v37/krichene15.html,"We consider an online optimization problem on a subset S of \(R^n\) (not necessarily convex), in which a decision maker chooses, at each iteration t, a probability distribution \(x^{(t)}\) over S, and seeks to minimize a cumulative expected loss, where each loss is a Lipschitz function revealed at the end of iteration t. Building on previous work, we propose a generalized Hedge algorithm and show a \(O(\sqrt{t \log t})\) bound on the regret when the losses are uniformly Lipschitz and S is uniformly fat (a weaker condition than convexity). Finally, we propose a generalization to the dual averaging method on the set of Lebesgue-continuous distributions over S."
798,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Online matrix prediction for sparse loss matrices,"Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto, Koji Tsuda",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/moridomi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_moridomi14,http://jmlr.csail.mit.edu/proceedings/papers/v39/moridomi14.html,"We consider an online matrix prediction problem. The FTRL is a famous method to deal with online prediction task, which makes prediction by minimizing cumulative loss function and regularizer function. There are three popular regularizer functions for matrices, Frobenius norm, quantum relative entropy and log-determinant. We propose a FTRL based algorithm with log-determinant as regularizer and show regret bound of algorithm. Our main contribution is to show that log-determinant regularization is efficient when sparse loss function setting. We also show the optimal performance algorithm for online collaborative filtering problem with log-determinant regularization."
799,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Online Density Estimation of Bradley-Terry Models,"Issei Matsumoto, Kohei Hatano, Eiji Takimoto",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Matsumoto15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Matsumoto15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Matsumoto15.html,"We consider an online density estimation problem for the Bradley-Terry model, where each model parameter defines the probability of a match result between any pair in a set of \(n\) teams. The problem is hard because the loss function (i.e., the negative log-likelihood function in our problem setting) is not convex. To avoid the non-convexity, we can change parameters so that the loss function becomes convex with respect to the new parameter. But then the radius \(K\) of the reparameterized domain may be infinite, where \(K\) depends on the outcome sequence. So we put a mild assumption that guarantees that \(K\) is finite. We can thus employ standard online convex optimization algorithms, namely OGD and ONS, over the reparameterized domain, and get regret bounds \(O(n^{\frac{1}{2}}(\ln K)\sqrt{T})\) and \(O(n^{\frac{3}{2}}K\ln T)\) , respectively, where \(T\) is the horizon of the game. The bounds roughly means that OGD is better when \(K\) is large while ONS is better when \(K\) is small. But how large can \(K\) be? We show that \(K\) can be as large as \(\Theta(T^{n-1})\) , which implies that the worst case regret bounds of OGD and ONS are \(O(n^{\frac{3}{2}}\sqrt{T}\ln T)\) and \(\tilde{O}(n^{\frac{3}{2}}(T)^{n-1})\) , respectively. We then propose a version of Follow the Regularized Leader, whose regret bound is close to the minimum of those of OGD and ONS. In other words, our algorithm is competitive with both for a wide range of values of \(K\) . In particular, our algorithm achieves the worst case regret bound \(O(n^{\frac{5}{2}}T^{\frac{1}{3}} \ln T)\) , which is slightly better than OGD with respect to \(T\) . In addition, our algorithm works without the knowledge \(K\) , which is a practical advantage."
800,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning,"Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, Daniil Ryabko",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/maillard13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_maillard13,http://jmlr.csail.mit.edu/proceedings/papers/v28/maillard13.html,"We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order \(O(T^{2/3})\) with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is \(O(\sqrt{T})\) , with all constants reasonably small. This is optimal in T since \(O(\sqrt{T})\) is the optimal regret in the setting of learning in a (single discrete) MDP."
801,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards,"Varun Kanade, H. Brendan McMahan, Brent Bryan","5:272-279, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/kanade09a/kanade09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_kanade09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/kanade09a.html,We consider algorithms for selecting actions in order to maximize rewards chosen by an adversary where the set of actions available on any given round is selected stochastically. We present the first polynomial-time no-regret algorithms for this setting. In the full-observation (experts) version of the problem we present an exponential-weights algorithm that achieves regret O(\sqrt{T log n}) which is the best possible. For the bandit setting (where the algorithm only observes the reward of the action selected) we present a no-regret algorithm based on follow-the-perturbed-leader. This algorithm runs in polynomial time unlike the EXP4 algorithm which can also be applied to this setting. Our algorithm has the interesting interpretation of solving a geometric experts problem where the embedding in which rewards are linear is never explicitly constructed. We argue that this adversarial-reward stochastic availability formulation is important in practice as assuming stationary stochastic rewards is unrealistic in many domains.
802,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret,"Lachlan Andrew, Siddharth Barman, Katrina Ligett, Minghong Lin, Adam Meyerson, Alan Roytman, Adam Wierman",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Andrew13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Andrew13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Andrew13.html,"We consider algorithms for –smoothed online convex optimization” problems, a variant of the class of online convex optimization problems that is strongly related to metrical task systems. Prior literature on these problems has focused on two performance metrics: regret and the competitive ratio. There exist known algorithms with sublinear regret and known algorithms with constant competitive ratios; however, no known algorithm achieves both simultaneously. We show that this is due to a fundamental incompatibility between these two metrics - no algorithm (deterministic or randomized) can achieve sublinear regret and a constant competitive ratio, even in the case when the objective functions are linear. However, we also exhibit an algorithm that, for the important special case of one dimensional decision spaces, provides sublinear regret while maintaining a competitive ratio that grows arbitrarily slowly."
803,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Principal Component Analysis and Higher Correlations for Distributed Data,"Ravi Kannan, Santosh Vempala, David Woodruff","JMLR W&CP 35 :1040-1057, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kannan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kannan14,http://jmlr.csail.mit.edu/proceedings/papers/v35/kannan14.html,"We consider algorithmic problems in the setting in which the input data has been partitioned arbitrarily on many servers. The goal is to compute a function of all the data, and the bottleneck is the communication used by the algorithm. We present algorithms for two illustrative problems on massive data sets: (1) computing a low-rank approximation of a matrix \(A=A^1 + A^2 + \ldots + A^s\) , with matrix \(A^t\) stored on server \(t\) and (2) computing a function of a vector \(a_1 + a_2 + \ldots + a_s\) , where server \(t\) has the vector \(a_t\) ; this includes the well-studied special case of computing frequency moments and separable functions, as well as higher-order correlations such as the number of subgraphs of a specified type occurring in a graph. For both problems we give algorithms with nearly optimal communication, and in particular the only dependence on \(n\) , the size of the data, is in the number of bits needed to represent indices and words ( \(O(\log n)\) )."
804,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Sequential Event Prediction with Association Rules,"Cynthia Rudin, Benjamin Letham, Ansaf Salleb-Aouissi, Eugene Kogan, David Madigan","19:615-634, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/rudin11a/rudin11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_rudin11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/rudin11a.html,"We consider a supervised learning problem in which data are revealed sequentially and the goal is to determine what will next be revealed. In the context of this problem algorithms based on association rules have a distinct advantage over classical statistical and machine learning methods; however there has not previously been a theoretical foundation established for using association rules in supervised learning. We present two simple algorithms that incorporate association rules and provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining and introduce an ``adjusted confidence"" measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory association rule mining and Bayesian analysis."
805,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Simple regret for infinitely many armed bandits,"Alexandra Carpentier, Michal Valko",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/carpentier15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_carpentier15,http://jmlr.csail.mit.edu/proceedings/papers/v37/carpentier15.html,"We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \(\beta\) characterizing the distribution of the near-optimal arms. We prove that depending on \(\beta\) , our algorithm is minimax optimal either up to a multiplicative constant or up to a \(\log(n)\) factor. We also provide extensions to several important cases: when \(\beta\) is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon."
806,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Riemannian Similarity Learning,Li Cheng,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cheng13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cheng13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cheng13.html,"We consider a similarity-score based paradigm to address scenarios where either the class labels are only partially revealed during learning, or the training and testing data are drawn from heterogeneous sources. The learning problem is subsequently formulated as optimization over a bilinear form of fixed rank. Our paradigm bears similarity to metric learning, where the major difference lies in its aim of learning a rectangular similarity matrix, instead of a proper metric. We tackle this problem in a Riemannian optimization framework. In particular, we consider its applications in pairwise-based action recognition, and cross-domain image-based object recognition. In both applications, the proposed algorithm produces competitive performance on respective benchmark datasets."
807,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Online Ranking with Top-1 Feedback,"Sougata Chaudhuri, Ambuj Tewari",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chaudhuri15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/chaudhuri15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chaudhuri15,http://jmlr.csail.mit.edu/proceedings/papers/v38/chaudhuri15.html,"We consider a setting where a system learns to rank a fixed set of \(m\) items. The goal is produce good item rankings for users with diverse interests who interact online with the system for \(T\) rounds. We consider a novel top- \(1\) feedback model: at the end of each round, the relevance score for only the top ranked object is revealed. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For PairwiseLoss and DCG, two popular ranking measures, we prove that the minimax regret is \(\Theta(T^{2/3})\) . Moreover, the minimax regret is achievable using an efficient strategy that only spends \(O(m \log m)\) time per round. The same efficient strategy achieves \(O(T^{2/3})\) regret for Precision@ \(k\) . Surprisingly, we show that for normalized versions of these ranking measures, i.e., AUC, NDCG & MAP, no online ranking algorithm can have sublinear regret."
808,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Maximally Informative Hierarchical Representations of High-Dimensional Data,"Greg Ver Steeg, Aram Galstyan",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/versteeg15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/versteeg15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_versteeg15,http://jmlr.csail.mit.edu/proceedings/papers/v38/versteeg15.html,"We consider a set of probabilistic functions of some input variables as a representation of the inputs. We present bounds on how informative a representation is about input data. We extend these bounds to hierarchical representations so that we can quantify the contribution of each layer towards capturing the information in the original data. The special form of these bounds leads to a simple, bottom-up optimization procedure to construct hierarchical representations that are also maximally informative about the data. This optimization has linear computational complexity and constant sample complexity in the number of variables. These results establish a new approach to unsupervised learning of deep representations that is both principled and practical. We demonstrate the usefulness of the approach on both synthetic and real-world data."
809,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Competing with an Infinite Set of Models in Reinforcement Learning,"Phuong Nguyen, Odalric-Ambrym Maillard, Daniil Ryabko, Ronald Ortner",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/nguyen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_nguyen13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/nguyen13a.html,"We consider a reinforcement learning setting where the learner also has to deal with the problem of finding a suitable state-representation function from a given set of models. This has to be done while interacting with the environment in an online fashion (no resets), and the goal is to have small regret with respect to any Markov model in the set. For this setting, recently the BLB algorithm has been proposed, which achieves regret of order \(T^{2/3}\) , provided that the given set of models is finite. Our first contribution is to extend this result to a countably infinite set of models. Moreover, the BLB regret bound suffers from an additive term that can be exponential in the diameter of the MDP involved, since the diameter has to be guessed. The algorithm we propose avoids guessing the diameter, thus improving the regret bound."
810,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Active Diagnosis under Persistent Noise with Unknown Noise Distribution: A Rank-Based Approach,"Gowtham Bellala, Suresh Bhavnani, Clayton Scott","15:155-163, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/bellala11a/bellala11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_bellala11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/bellala11a.html,We consider a problem of active diagnosis where the goal is to efficiently identify an unknown object by sequentially selecting and observing the responses to binary valued queries. We assume that query observations are noisy and further that the noise is persistent meaning that repeating a query does not change the response. Previous work in this area either assumed the knowledge of the query noise distribution or that the noise level is sufficiently low so that the unknown object can be identified with high accuracy. We make no such assumptions and introduce an algorithm that returns a ranked list of objects such that the expected rank of the true object is optimized. Furthermore our algorithm does not require knowledge of the query noise distribution.
811,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Latent Bandits.,"Odalric-Ambrym Maillard, Shie Mannor",none,http://jmlr.org/proceedings/papers/v32/maillard14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_maillard14,http://jmlr.csail.mit.edu/proceedings/papers/v32/maillard14.html,"We consider a multi-armed bandit problem where the reward distributions are indexed by two sets _one for arms, one for type_ and can be partitioned into a small number of clusters according to the type. First, we consider the setting where all reward distributions are known and all types have the same underlying cluster, the typeês identity is, however, unknown. Second, we study the case where types may come from different classes, which is significantly more challenging. Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive non-trivial regret performance. Numerical experiments show that, in the most challenging agnostic case, the proposed algorithm achieves excellent performance in several difficult scenarios."
812,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Learning and inference in the presence of corrupted inputs,"Uriel Feige, Yishay Mansour, Robert Schapire",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Feige15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Feige15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Feige15.html,"We consider a model where given an uncorrupted input an adversary can corrupt it to one out of \(m\) corrupted inputs. We model the classification and inference problems as a zero-sum game between a learner, minimizing the expected error, and an adversary, maximizing the expected error. The value of this game is the optimal error rate achievable. For learning using a limited hypothesis class \(\mathcal{H}\) over corrupted inputs, we give an efficient algorithm that given an uncorrupted sample returns a hypothesis \(h\in \mathcal{H}\) whose error on adversarially corrupted inputs is near optimal. Our algorithm uses as a blackbox an oracle that solves the ERM problem for the hypothesis class \(\mathcal{H}\) . We provide a generalization bound for our setting, showing that for a sufficiently large sample, the performance on the sample and future unseen corrupted inputs will be similar. This gives an efficient learning algorithm for our adversarial setting, based on an ERM oracle. We also consider an inference related setting of the problem, where given a corrupted input, the learner queries the target function on various uncorrupted inputs and generates a prediction regarding the given corrupted input. There is no limitation on the prediction function the learner may generate, so implicitly the hypothesis class includes all possible hypotheses. In this setting we characterize the optimal learner policy as a minimum vertex cover in a given bipartite graph, and the optimal adversary policy as a maximum matching in the same bipartite graph. We design efficient local algorithms for approximating minimum vertex cover in bipartite graphs, which implies an efficient near optimal algorithm for the learner."
813,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Group Orthogonal Matching Pursuit for Logistic Regression,"Aurelie Lozano, Grzegorz Swirszcz, Naoki Abe","15:452-460, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/lozano11a/lozano11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_lozano11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/lozano11a.html,We consider a matching pursuit approach for variable selection and estimation in logistic regression models. Specifically we propose Logistic Group Orthogonal Matching Pursuit (Logit-GOMP) which extends the Group-OMP procedure originally proposed for linear regression models to select groups of variables in logistic regression models given a predefined grouping structure within the explanatory variables. We theoretically characterize the performance of Logit-GOMP in terms of predictive accuracy and also provide conditions under which Logit-GOMP is able to identify the correct (groups of) variables. Our results are non-asymptotic in contrast to classical consistency results for logistic regression which only apply in the asymptotic limit where the dimensionality is fixed or is restricted to grow slowly with the sample size. We conduct empirical evaluation on simulated data sets and the real world problem of splice site detection in DNA sequences. The results indicate that Logit-GOMP compares favorably to Logistic Group Lasso both in terms of variable selection and prediction accuracy. We also provide a generic version of our algorithm that applies to the wider class of generalized linear models.
814,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Bandit Theory meets Compressed Sensing for high dimensional Stochastic Linear Bandit,"Alexandra Carpentier, Remi Munos",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/carpentier12/carpentier12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_carpentier12,http://jmlr.csail.mit.edu/proceedings/papers/v22/carpentier12.html,We consider a linear stochastic bandit problem where the dimension K of the unknown parameter heta is larger than the sampling budget n. Since usual linear bandit algorithms have a regret in O(K\sqrt{n}) it is in general impossible to obtain a sub-linear regret without further assumption. In this paper we make the assumption that heta is S-sparse i.e. has at most S-non-zero components and that the space of arms is the unit ball for the ||.||_2 norm. We combine ideas from Compressed Sensing and Bandit Theory to derive an algorithm with a regret bound in O(S\sqrt{n}). We detail an application to the problem of optimizing a function that depends on many variables but among which only a small number of them (initially unknown) are relevant.
815,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Minimax Fixed-Design Linear Regression,"Peter L. Bartlett, Wouter M. Koolen, Alan Malek, Eiji Takimoto, Manfred K. Warmuth",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bartlett15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Bartlett15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bartlett15.html,"We consider a linear regression game in which the covariates are known in advance: at each round, the learner predicts a real-value, the adversary reveals a label, and the learner incurs a squared error loss. The aim is to minimize the regret with respect to linear predictions. For a variety of constraints on the adversaryês labels, we show that the minimax optimal strategy is linear, with a parameter choice that is reminiscent of ordinary least squares (and as easy to compute). The predictions depend on all covariates, past and future, with a particular weighting assigned to future covariates corresponding to the role that they play in the minimax regret. We study two families of label sequences: box constraints (under a covariate compatibility condition), and a weighted 2-norm constraint that emerges naturally from the analysis. The strategy is adaptive in the sense that it requires no knowledge of the constraint set. We obtain an explicit expression for the minimax regret for these games. For the case of uniform box constraints, we show that, with worst case covariate sequences, the regret is \(O(d\log T)\) , with no dependence on the scaling of the covariates."
816,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,A Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences,"Odalric-Ambrym Maillard, R_mi Munos, Gilles Stoltz","19:497-514, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/maillard11a/maillard11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_maillard11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/maillard11a.html,We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed bandit problem in the case of distributions with finite supports(not necessarily known beforehand) whose asymptotic regret matches the lower bound of \citet{Burnetas96}. Our contribution is to provide a finite-time analysis of this algorithm;we get bounds whose main terms are smaller than the ones of previously known algorithms with finite-time analyses (like UCB-type algorithms).
817,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization,"Yuchen Zhang, Xiao Lin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhanga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhanga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhanga15.html,"We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods."
818,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Reactive bandits with attitude,"Pedro Ortega, Kee-Eung Kim, Daniel Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/ortega15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_ortega15,http://jmlr.csail.mit.edu/proceedings/papers/v38/ortega15.html,"We consider a general class of K-armed bandits that adapt to the actions of the player. A single continuous parameter characterizes the –attitude” of the bandit, ranging from stochastic to cooperative or to fully adversarial in nature. The player seeks to maximize the expected return from the adaptive bandit, and the associated optimization problem is related to the free energy of a statistical mechanical system under an external field. When the underlying stochastic distribution is Gaussian, we derive an analytic solution for the long run optimal player strategy for different regimes of the bandit. In the fully adversarial limit, this solution is equivalent to the Nash equilibrium of a two-player, zero-sum semi-infinite game. We show how optimal strategies can be learned from sequential draws and reward observations in these adaptive bandits using Bayesian filtering and Thompson sampling. Results show the qualitative difference in policy pseudo-regret between our proposed strategy and other well-known bandit algorithms."
819,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Algorithms and Hardness for Robust Subspace Recovery,"Moritz Hardt, Ankur Moitra",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Hardt13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Hardt13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Hardt13.html,"We consider a fundamental problem in unsupervised learning called subspace recovery: given a collection of m points in \(R^n\) , if many but not necessarily all of these points are contained in a \(d\) -dimensional subspace \(T\) can we find it? The points contained in T are called inliers and the remaining points are outliers. This problem has received considerable attention in computer science and in statistics. Yet efficient algorithms from computer science are not robust to adversarial outliers, and the estimators from robust statistics are hard to compute in high dimensions. This is a serious and persistent issue not just in this application, but for many other problems in unsupervised learning. Are there algorithms for subspace recovery that are both robust to outliers and efficient? We give an algorithm that finds \(T\) when it contains more than a \(d/n\) fraction of the points. Hence, for say \(d = n/2\) this estimator is both easy to compute and well-behaved when there are a constant fraction of outliers. We prove that it is small set expansion hard to find \(T\) when the fraction of errors is any larger and so our estimator is an optimal compromise between efficiency and robustness. In fact, this basic problem has a surprising number of connections to other areas including small set expansion, matroid theory and functional analysis that we make use of here."
820,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Mixtures of Linear Classifiers,"Yuekai Sun, Stratis Ioannidis, Andrea Montanari",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sunb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunb14.html,"We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a •mirroringê trick, that discovers the subspace spanned by the classifiersê parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency."
821,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,A UCB-Like Strategy of Collaborative Filtering,Atsuyoshi Nakamura,none,http://jmlr.csail.mit.edu/proceedings/papers/v39/nakamura14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_nakamura14,http://jmlr.csail.mit.edu/proceedings/papers/v39/nakamura14.html,"We consider a direct mail problem in which a system repeats the following process every day during some period: select a set of user-item pairs (u,i), send a recommendation mail of item i to user u for each selected pair (u,i), and receive a response from each user. We assume that each response can be obtained before the next process and through the response, the system can know the userês evaluation of the recommended item directly or indirectly. Each pair (u,i) can be selected at most once during the period. If the total number of selections is very small compared to the number of entries in the whole user-item matrix, what selection strategy should be used to maximize the total sum of usersê evaluations during the period? We consider a UCB-like strategy for this problem, and show two methods using the strategy. The effectiveness of our methods are demonstrated by experiments using synthetic and real datasets."
822,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Attribute Efficient Linear Regression with Distribution-Dependent Sampling,"Doron Kukliansky, Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kukliansky15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kukliansky15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kukliansky15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kukliansky15.html,"We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(d/k) over the state-of-the-art, where d is the dimension and k+1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use *less* attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims."
823,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization,"Krzysztof Dembczynski, Arkadiusz Jachnik, Wojciech Kotlowski, Willem Waegeman, Eyke Huellermeier",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/dembczynski13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_dembczynski13,http://jmlr.csail.mit.edu/proceedings/papers/v28/dembczynski13.html,"We compare the plug-in rule approach for optimizing the F-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F-measure directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm."
824,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Connected Sub-graph Detection,"Jing Qian, Venkatesh Saligrama, Yuting Chen","JMLR W&CP 33 :796-804, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/qian14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/qian14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_qian14,http://jmlr.csail.mit.edu/proceedings/papers/v33/qian14.html,"We characterize the family of connected subgraphs in terms of linear matrix inequalities (LMI) with additional integrality constraints. We then show that convex relaxations of the integral LMI lead to parameterization of all weighted connected subgraphs. These developments allow for optimizing arbitrary graph functionals under connectivity constraints. For concreteness we consider the connected sub-graph detection problem that arises in a number of applications including network intrusion, disease outbreaks, and video surveillance. In these applications feature vectors are associated with nodes and edges of a graph. The problem is to decide whether or not the null hypothesis is true based on the measured features. For simplicity we consider the elevated mean problem wherein feature values at various nodes are distributed IID under the null hypothesis. The non-null (positive) hypothesis is distinguished from the null hypothesis by the fact that feature values on some unknown connected sub-graph has elevated mean."
825,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Unified Framework for Consistency of Regularized Loss Minimizers,"Jean Honorio, Tommi Jaakkola",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/honorio14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/honorio14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_honorio14,http://jmlr.csail.mit.edu/proceedings/papers/v32/honorio14.html,"We characterize a family of regularized loss minimization problems that satisfy three properties: scaled uniform convergence, super-norm regularization, and norm-loss monotonicity. We show several theoretical guarantees within this framework, including loss consistency, norm consistency, sparsistency (i.e. support recovery) as well as sign consistency. A number of regularization problems can be shown to fall within our framework and we provide several examples. Our results can be seen as a concise summary of existing guarantees but we also extend them to new settings. Our formulation enables us to assume very little about the hypothesis class, data distribution, the loss, or the regularization. In particular, many of our results do not require a bounded hypothesis class, or identically distributed samples. Similarly, we do not assume boundedness, convexity or smoothness of the loss nor the regularizer. We only assume approximate optimality of the empirical minimizer. In terms of recovery, in contrast to existing results, our sparsistency and sign consistency results do not require knowledge of the sub-differential of the objective function."
826,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Hierarchical Conditional Random Fields for Outlier Detection: An Application to Detecting Epileptogenic Cortical Malformations,"Bilal Ahmed, Thomas Thesen, Karen Blackmon, Yijun Zhao, Orrin Devinsky, Ruben Kuzniecky, Carla Brodley",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ahmed14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ahmed14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ahmed14.html,"We cast the problem of detecting and isolating regions of abnormal cortical tissue in the MRIs of epilepsy patients in an image segmentation framework. Employing a multiscale approach we divide the surface images into segments of different sizes and then classify each segment as being an outlier, by comparing it to the same region across controls. The final classification is obtained by fusing the outlier probabilities obtained at multiple scales using a tree-structured hierarchical conditional random field (HCRF). The proposed method correctly detects abnormal regions in 90% of patients whose abnormality was detected via routine visual inspection of their clinical MRI. More importantly, it detects abnormalities in 80% of patients whose abnormality escaped visual inspection by expert radiologists."
827,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Multiple Texture Boltzmann Machines,"Jyri Kivinen, Christopher Williams",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kivinen12/kivinen12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kivinen12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kivinen12.html,We assess the generative power of the mPoT-model of [10] with tiled-convolutional weight sharing as a model for visual textures by specifically training on this task evaluating model performance on texture synthesis and inpainting tasks using quantitative metrics. We also analyze the relative importance of the mean and covariance parts of the mPoT model by comparing its performance to those of its subcomponents tiled-convolutional versions of the PoT/FoE and Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM). Our results suggest that while state-of-the-art or better performance can be achieved using the mPoT similar performance can be achieved with the mean-only model. We then develop a model for multiple textures based on the GB-RBM using a shared set of weights but texture-specific hidden unit biases. We show comparable performance of the multiple texture model to individually trained texture models.
828,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Matrix-Variate Dirichlet Process Mixture Models,"Zhihua Zhang, Guang Dai, Michael Jordan","9:980-987, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10e/zhang10e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhang10e,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10e.html,We are concerned with a multivariate response regression problem where the interest is in considering correlations both across response variates and across response samples. In this paper we develop a new Bayesian nonparametric model for such a setting based on Dirichlet process priors. Building on an additive kernel model we allow each sample to have its own regression matrix. Although this overcomplete representation could in principle suffer from severe overfitting problems we are able to provide effective control over the model via a matrix-variate Dirichlet process prior on the regression matrices. Our model is able to share statistical strength among regression matrices due to the clustering property of the Dirichlet process. We make use of a Markov chain Monte Carlo algorithm for inference and prediction. Compared with other Bayesian kernel models our model has advantages in both computational and statistical efficiency.
829,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Hierarchical Tensor Decomposition of Latent Tree Graphical Models,"Le Song, Mariya Ishteva, Ankur Parikh, Eric Xing, Haesun Park",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/song13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_song13,http://jmlr.csail.mit.edu/proceedings/papers/v28/song13.html,"We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. In this new view, the marginal probability table of the observed variables in a latent tree is treated as a tensor, and we show that: (i) the latent variables induce low rank structures in various matricizations of the tensor; (ii) this collection of low rank matricizations induce a hierarchical low rank decomposition of the tensor. Exploiting these properties, we derive an optimization problem for estimating the parameters of a latent tree graphical model, i.e., hierarchical decomposion of a tensor which minimizes the Frobenius norm of the difference between the original tensor and its decomposition. When the latent tree graphical models are correctly specified, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm. This algorithm recovers previous spectral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) as special cases, elucidating the global objective these algorithms are optimizing for. When the latent tree graphical models are misspecified, we derive a better decomposition based on our framework, and provide approximation guarantee for this new estimator. In both synthetic and real world data, this new estimator significantly improves over the-state-of-the-art."
830,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions,"Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/luo13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_luo13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/luo13a.html,We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or surpasses the state-of-the-art on texture synthesis and inpainting by parametric models. We also develop a novel RBM model with a spike-and-slab visible layer and binary variables in the hidden layer. This model is designed to be stacked on top of the ssRBM. We show the resulting deep belief network (DBN) is a powerful generative model that improves on single-layer models and is capable of modeling not only single high-resolution and challenging textures but also multiple textures with fixed-size filters in the bottom layer.
831,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields,"Mark Schmidt, Reza Babanezhad, Mohamed Ahmed, Aaron Defazio, Ann Clifton, Anoop Sarkar",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/schmidt15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/schmidt15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_schmidt15,http://jmlr.csail.mit.edu/proceedings/papers/v38/schmidt15.html,"We apply stochastic average gradient (SAG) algorithms for training conditional random fields (CRFs). We describe a practical implementation that uses structure in the CRF gradient to reduce the memory requirement of this linearly-convergent stochastic gradient method, propose a non-uniform sampling scheme that substantially improves practical performance, and analyze the rate of convergence of the SAGA variant under non-uniform sampling. Our experimental results reveal that our method significantly outperforms existing methods in terms of the training objective, and performs as well or better than optimally-tuned stochastic gradient methods in terms of test error."
832,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Fast Function to Function Regression,"Junier Oliva, William Neiswanger, Barnabas Poczos, Eric Xing, Hy Trac, Shirley Ho, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/oliva15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/oliva15-supp.zip,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_oliva15,http://jmlr.csail.mit.edu/proceedings/papers/v38/oliva15.html,"We analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class. Function to function regression (FFR) covers a large range of interesting applications including time-series prediction problems, and also more general tasks like studying a mapping between two separate types of distributions. However, previous nonparametric estimators for FFR type problems scale badly computationally with the number of input/output pairs in a data-set. Given the complexity of a mapping between general functions it may be necessary to consider large data-sets in order to achieve a low estimation risk. To address this issue, we develop a novel scalable nonparametric estimator, the Triple-Basis Estimator (3BE), which is capable of operating over datasets with many instances. To the best of our knowledge, the 3BE is the first nonparametric FFR estimator that can scale to massive data-sets. We analyze the 3BEês risk and derive an upperbound rate. Furthermore, we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators in various real-world data-sets."
833,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Bandit Convex Optimization: \(\sqrt{T}\) Regret in One Dimension,"S_bastien Bubeck, Ofer Dekel, Tomer Koren, Yuval Peres",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bubeck15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Bubeck15a,http://jmlr.csail.mit.edu/proceedings/papers/v40/Bubeck15a.html,"We analyze the minimax regret of the adversarial bandit convex optimization problem. Focusing on the one-dimensional case, we prove that the minimax regret is \(\widetilde\Theta(\sqrt{T})\) and partially resolve a decade-old open problem. Our analysis is non-constructive, as we do not present a concrete algorithm that attains this regret rate. Instead, we use minimax duality to reduce the problem to a Bayesian setting, where the convex loss functions are drawn from a worst-case distribution, and then we solve the Bayesian version of the problem with a variant of Thompson Sampling. Our analysis features a novel use of convexity, formalized as a –local-to-global” property of convex functions, that may be of independent interest."
834,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Theory and Algorithms for the Localized Setting of Learning Kernels,"Yunwen Lei, Alexander Binder, Êrôn Dogan, Marius Kloft",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/LeiBinDogKlo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_LeiBinDogKlo15,http://jmlr.csail.mit.edu/proceedings/papers/v44/LeiBinDogKlo15.html,"We analyze the localized setting of learning kernels also known as localized multiple kernel learning. This problem has been addressed in the past using rather heuristic approaches based on approximately optimizing non-convex problem formulations, of which up to now no theoretical learning bounds are known. In this paper, we show generalization error bounds for learning localized kernel classes where the localities are coupled using graph-based regularization. We propose a novel learning localized kernels algorithm based on this hypothesis class that is formulated as a convex optimization problem using a pre-obtained cluster structure of the data. We derive dual representations using Fenchel conjugation theory, based on which we give a simple yet efficient wrapper-based optimization algorithm. We apply the method to problems involving multiple heterogeneous data sources, taken from domains of computational biology and computer vision. The results show that the proposed convex approach to learning localized kernels can achieve higher prediction accuracies than its global and non-convex local counterparts."
835,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite-Sample Guarantees,"Jean Honorio, Tommi Jaakkola","JMLR W&CP 33 :384-392, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/honorio14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/honorio14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_honorio14,http://jmlr.csail.mit.edu/proceedings/papers/v33/honorio14.html,"We analyze the expected risk of linear classifiers for a fixed weight vector in the –minimax” setting. That is, we analyze the worst-case risk among all data distributions with a given mean and covariance. We provide a simpler proof of the tight polynomial-tail bound for general random variables. For sub-Gaussian random variables, we derive a novel tight exponential-tail bound. We also provide new PAC-Bayes finite-sample guarantees when training data is available. Our –minimax” generalization bounds are dimensionality-independent and \(\mathcal{O}(\sqrt{1/m})\) for \(m\) samples."
836,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Escaping From Saddle Points ã Online Stochastic Gradient for Tensor Decomposition,"Rong Ge, Furong Huang, Chi Jin, Yang Yuan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Ge15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Ge15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Ge15.html,"We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points . In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that from an arbitrary starting point, stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee."
837,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Distribution to Distribution Regression,"Junier Oliva, Barnabas Poczos, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/oliva13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/oliva13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_oliva13,http://jmlr.csail.mit.edu/proceedings/papers/v28/oliva13.html,"We analyze êDistribution to Distribution regressionê where one is regressing a mapping where both the covariate (inputs) and response (outputs) are distributions. No parameters on the input or output distributions are assumed, nor are any strong assumptions made on the measure from which input distributions are drawn from. We develop an estimator and derive an upper bound for the \(L2\) risk; also, we show that when the effective dimension is small enough (as measured by the doubling dimension), then the risk converges to zero with a polynomial rate."
838,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Oracle inequalities for computationally budgeted model selection,"Alekh Agarwal, John C. Duchi, Peter L. Bartlett, Clement Levrard","19:69-86, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/agarwal11a/agarwal11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_agarwal11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/agarwal11a.html,We analyze general model selection procedures using penalized empirical loss minimization under computational constraints. While classical model selection approaches do not consider computational aspects of performing model selection we argue that any practical model selection procedure must not only trade off estimation and approximation error but also the effects of the computational effort required to compute empirical minimizers for different function classes. We provide a framework for analyzing such problems and we give algorithms for model selection under a computational budget. These algorithms satisfy oracle inequalities that show that the risk of the selected model is not much worse than if we had devoted all of our computational budget to the best function class.
839,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Metric recovery from directed unweighted graphs,"Tatsunori Hashimoto, Yi Sun, Tommi Jaakkola",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hashimoto15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/hashimoto15-supp.zip,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hashimoto15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hashimoto15.html,"We analyze directed, unweighted graphs obtained from \(x_i\in \RR^d\) by connecting vertex \(i\) to \(j\) iff \(|x_i - x_j| _ \epsilon(x_i)\) . Examples of such graphs include \(k\) -nearest neighbor graphs, where \(\epsilon(x_i)\) varies from point to point, and, arguably, many real world graphs such as co-purchasing graphs. We ask whether we can recover the underlying Euclidean metric \(\epsilon(x_i)\) and the associated density \(p(x_i)\) given only the directed graph and \(d\) . We show that consistent recovery is possible up to isometric scaling when the vertex degree is at least \(\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})\) . Our estimator is based on a careful characterization of a random walk over the directed graph and the associated continuum limit. As an algorithm, it resembles the PageRank centrality metric. We demonstrate empirically that the estimator performs well on simulated examples as well as on real-world co-purchasing graphs even with a small number of points and degree scaling as low as \(\log(n)\) ."
840,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Consistency of Causal Inference under the Additive Noise Model,"Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, Bernhard Schoelkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kpotufe14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kpotufe14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kpotufe14.html,"We analyze a family of methods for statistical causal inference from sample under the so-called Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting."
841,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Robust Bayesian Matrix Factorisation,"Balaji Lakshminarayanan, Guillaume Bouchard, Cedric Archambeau","15:425-433, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/lakshminarayanan11a/lakshminarayanan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_lakshminarayanan11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/lakshminarayanan11a.html,We analyse the noise arising in collaborative filtering when formalised as a probabilistic matrix factorisation problem. We show empirically that modelling row- and column-specific variances is important the noise being in general non-Gaussian and heteroscedastic. We also advocate for the use of a Student-t prior for the latent features as the standard Gaussian is included as a special case. We derive several variational inference algorithms and estimate the hyperparameters by type-II maximum likelihood. Experiments on real data show that the predictive performance is significantly improved.
842,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Dual Decomposition for Joint Discrete-Continuous Optimization,Christopher Zach,none,http://jmlr.csail.mit.edu/proceedings/papers/v31/zach13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/zach13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_zach13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/zach13a.html,"We analyse convex formulations for combined discrete-continuous MAP inference using the dual decomposition method. As a consquence we can provide a more intuitive derivation for the resulting convex relaxation than presented in the literature. Further, we show how to strengthen the relaxation by reparametrizing the potentials, hence convex relaxations for discrete-continuous inference does not share an important feature of LP relaxations for discrete labeling problems: incorporating unary potentials into higher order ones affects the quality of the relaxation. We argue that the convex model for discrete-continuous inference is very general and can be used as alternative for alternation-based methods often employed for such joint inference tasks."
843,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM,"Xiaojun Chang, Yi Yang, Eric Xing, Yaoliang Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/changa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/changa15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_changa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/changa15.html,"We aim to detect complex events in long Internet videos that may last for hours. A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest. We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector. Next, we propose a new isotonic regularizer that is able to exploit the semantic ordering information. The resulting nearly-isotonic SVM classifier exhibits higher discriminative power. Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new, closed-form proximal steps. We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach."
844,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Second-order Quantile Methods for Experts and Combinatorial Games,"Wouter M. Koolen, Tim Van Erven",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Koolen15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Koolen15a,http://jmlr.csail.mit.edu/proceedings/papers/v40/Koolen15a.html,"We aim to design strategies for sequential decision making that adjust to the difficulty of the learning problem. We study this question both in the setting of prediction with expert advice, and for more general combinatorial decision tasks. We are not satisfied with just guaranteeing minimax regret rates, but we want our algorithms to perform significantly better on easy data. Two popular ways to formalize such adaptivity are second-order regret bounds and quantile bounds. The underlying notions of •easy dataê, which may be paraphrased as –the learning problem has small variance” and –multiple decisions are useful”, are synergetic. But even though there are sophisticated algorithms that exploit one of the two, no existing algorithm is able to adapt to both. The difficulty in combining the two notions lies in tuning a parameter called the learning rate, whose optimal value behaves non-monotonically. We introduce a potential function for which (very surprisingly!) it is sufficient to simply put a prior on learning rates; an approach that does not work for any previous method. By choosing the right prior we construct efficient algorithms and show that they reap both benefits by proving the first bounds that are both second-order and incorporate quantiles."
845,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Variational Relevance Vector Machine for Tabular Data,"Dmitry Kropotov, Dmitry Vetrov, Lior Wolf, and Tal Hassner","13:79-94, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/kropotov10a/kropotov10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_kropotov10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/kropotov10a.html,We adopt the Relevance Vector Machine (RVM) framework to handle cases of table-structured data such as image blocks and image descriptors. This is achieved by coupling the regularization coefficients of rows and columns of features. We present two variants of this new gridRVM framework based on the way in which the regularization coefficients of the rows and columns are combined. Appropriate variational optimization algorithms are derived for inference within this framework. The consequent reduction in the number of parameters from the product of the table's dimensions to the sum of its dimensions allows for better performance in the face of small training sets resulting in improved resistance to overfitting as well as providing better interpretation of results. These properties are demonstrated on synthetic data-sets as well as on a modern and challenging visual identification benchmark.
846,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Preference-Based Rank Elicitation using Statistical Models: The Case of Mallows,"Robert Busa-Fekete, Eyke Huellermeier, Balˆzs Sz_r_nyi",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/busa-fekete14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/busa-fekete14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_busa-fekete14,http://jmlr.csail.mit.edu/proceedings/papers/v32/busa-fekete14.html,"We address the problem of rank elicitation assuming that the underlying data generating process is characterized by a probability distribution on the set of all rankings (total orders) of a given set of items. Instead of asking for complete rankings, however, our learner is only allowed to query pairwise preferences. Using information of that kind, the goal of the learner is to reliably predict properties of the distribution, such as the most probable top-item, the most probable ranking, or the distribution itself. More specifically, learning is done in an online manner, and the goal is to minimize sample complexity while guaranteeing a certain level of confidence."
847,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Clustering in the Presence of Background Noise,"Shai Ben-David, Nika Haghtalab",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ben-david14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ben-david14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ben-david14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ben-david14.html,"We address the problem of noise management in clustering algorithms. Namely, issues that arise when on top of some cluster structure the data also contains an unstructured set of points. We consider how clustering algorithms can be ``robustified"" so that they recover the cluster structure in spite of the unstructured part of the input. We introduce some quantitative measures of such robustness that take into account the strength of the embedded cluster structure as well was the mildness of the noise subset. We propose a simple and efficient method to turn any centroid-based clustering algorithm into a noise-robust one, and prove robustness guarantees for our method with respect to these measures. We also prove that more straightforward ways of –robustifying” clustering algorithms fail to achieve similar guarantees."
848,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,On Approximate Non-submodular Minimization via Tree-Structured Supermodularity,"Yoshinobu Kawahara, Rishabh Iyer, Jeffrey Bilmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kawahara15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/kawahara15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kawahara15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kawahara15.html,"We address the problem of minimizing non-submodular functions where the supermodularity is restricted to tree-structured pairwise terms. We are motivated by several real world applications, which require submodularity along with structured supermodularity, and this forms a rich class of expressive models, where the non-submodularity is restricted to a tree. While this problem is NP hard (as we show), we develop several practical algorithms to find approximate and near-optimal solutions for this problem, some of which provide lower and others of which provide upper bounds thereby allowing us to compute a tightness gap. We also show that some of our algorithms can be extended to handle more general forms of supermodularity restricted to arbitrary pairwise terms. We compare our algorithms on synthetic data, and also demonstrate the advantage of the formulation on the real world application of image segmentation, where we incorporate structured supermodularity into higher-order submodular energy minimization."
849,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Message-Passing Algorithms for MAP Estimation Using DC Programming,"Akshat Kumar, Shlomo Zilberstein, Marc Toussaint",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kumar12/kumar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kumar12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kumar12.html,We address the problem of finding the most likely assignment or MAP estimation in a Markov random field. We analyze the linear programming formulation of MAP through the lens of difference of convex functions (DC) programming and use the concave-convex procedure (CCCP) to develop efficient message-passing solvers. The resulting algorithms are guaranteed to converge to a global optimum of the well-studied local polytope an outer bound on the MAP marginal polytope. To tighten the outer bound we show how to combine it with the mean-field based inner bound and again solve it using CCCP. We also identify a useful relationship between the DC formulations and some recently proposed algorithms based on Bregman divergence. Experimentally this hybrid approach produces optimal solutions for a range of hard OR problems and near-optimal solutions for standard benchmarks.
850,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning,"Matthew Hoffman, Bobak Shahriari, Nando de Freitas","JMLR W&CP 33 :365-374, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hoffman14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/hoffman14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hoffman14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hoffman14.html,"We address the problem of finding the maximizer of a nonlinear function that can only be evaluated, subject to noise, at a finite number of query locations. Further, we will assume that there is a constraint on the total number of permitted function evaluations. We introduce a Bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other Bayesian optimization methods. The Bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. As a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. This feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. The paper presents comprehensive comparisons of the proposed approach with many Bayesian and bandit optimization techniques, the first comparison of many of these methods in the literature."
851,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Estimating Di_usion Probability Changes for AsIC-SIS Model,"A. Koide, K. Saito, K. Ohara, M. Kimura & H. Motoda","20:297_313, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/koide11/koide11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_koide11,http://jmlr.csail.mit.edu/proceedings/papers/v20/koide11.html,We address the problem of estimating changes in di_usion probability over a social network from the observed information di_usion results which is possibly caused by an unknown external situation change. For this problem we focused on the asynchronous independent cascade (AsIC) model in the SIS (Susceptible/Infected/Susceptible) setting in order to meet more realistic situations such as communication in a blogosphere. This model is referred to as the AsIC-SIS model. We assume that the di_usion parameter changes are approximated by a series of step functions and their changes are re§ected in the observed di_usion results. Thus the problem is reduced to detecting how many step functions are needed where in time each one starts and how long it lasts and what the hight of each one is. The method employs the derivative of the likelihood function of the observed data that are assumed to be generated from the AsIC-SIS model adopts a divide-and-conquer type greedy recursive partitioning and utilizes an MDL model selection measure to determine the adequate number of step functions. The results obtained using real world network structures con_rmed that the method works well as intended. The MDL criterion is useful to avoid over_tting and the found pattern is not necessarily the same in terms of the number of step functions as the one assumed to be true but the error is always reduced to a small value.   Page last modified on Sun Nov 6 15:44:06 2011.
852,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Active Boundary Annotation using Random MAP Perturbations,"Subhransu Maji, Tamir Hazan, Tommi Jaakkola","JMLR W&CP 33 :604-613, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/maji14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_maji14,http://jmlr.csail.mit.edu/proceedings/papers/v33/maji14.html,"We address the problem of efficiently annotating labels of objects when they are structured. Often the distribution over labels can be described using a joint potential function over the labels for which sampling is provably hard but efficient maximum a-posteriori (MAP) solvers exist. In this setting we develop novel entropy bounds that are based on the expected amount of perturbation to the potential function that is needed to change MAP decisions. By reasoning about the entropy reduction and cost tradeoff, our algorithm actively selects the next annotation task. As an example of our framework we propose a boundary refinement task which can used to obtain pixel-accurate image boundaries much faster than traditional tools by focussing on parts of the image for refinement in a multi-scale manner."
853,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Intersecting singularities for multi-structured estimation,"Emile Richard, Francis BACH, Jean-Philippe Vert",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/richard13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/richard13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_richard13,http://jmlr.csail.mit.edu/proceedings/papers/v28/richard13.html,We address the problem of designing a convex nonsmooth regularizer encouraging multiple structural effects simultaneously. Focusing on the inference of sparse and low-rank matrices we suggest a new complexity index and a convex penalty approximating it. The new penalty term can be written as the trace norm of a linear function of the matrix. By analyzing theoretical properties of this family of regularizers we come up with oracle inequalities and compressed sensing results ensuring the quality of our regularized estimator. We also provide algorithms and supporting numerical experiments.
854,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Contextual Bandit Algorithms with Supervised Learning Guarantees,"Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert Schapire","15:19-26, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_beygelzimer11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/beygelzimer11a.html,We address the problem of competing with any large set of $N$ policies in the non-stochastic bandit setting where the learner must repeatedly select among $K$ actions but observes only the reward of the chosen action. We present a modification of the Exp4 algorithm of [Auer et al. 2002] called Exp4.P which with high probability incurs regret at most $O(\sqrt{KT\ln N})$. Such a bound does not hold for Exp4 due to the large variance of the importance-weighted estimates used in the algorithm. The new algorithm is tested empirically in a large-scale real-world dataset. For the stochastic version of the problem we can use Exp4.P as a subroutine to compete with a possibly infinite set of policies of VC-dimension $d$ while incurring regret at most $O(\sqrt{Td\ln T})$ with high probability. These guarantees improve on those of all previous algorithms whether in a stochastic or adversarial environment and bring us closer to providing guarantees for this setting that are comparable to those in standard supervised learning.
855,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Inference of Cause and Effect with Unsupervised Inverse Regression,"Eleni Sgouritsa, Dominik Janzing, Philipp Hennig, Bernhard Sch_lkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/sgouritsa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_sgouritsa15,http://jmlr.csail.mit.edu/proceedings/papers/v38/sgouritsa15.html,"We address the problem of causal discovery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X -_ Y (X causes Y), the marginal distribution of the cause, P(X), contains no information about the conditional distribution P(Y|X). Consequently, estimating P(Y|X) from P(X) should not be possible. However, estimating P(X|Y) based on P(Y) may be possible. This paper employs this asymmetry to propose CURE, a causal discovery method which decides upon the causal direction by comparing the accuracy of the estimations of P(Y|X) and P(X|Y). To this end, we propose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP regression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference methods."
856,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data,"Jan-Willem van de Meent, Jonathan Bronson, Frank Wood, Ruben Gonzalez Jr., Chris Wiggins",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/willemvandemeent13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_willemvandemeent13,http://jmlr.csail.mit.edu/proceedings/papers/v28/willemvandemeent13.html,"We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process."
857,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Algorithms for the Hard Pre-Image Problem of String Kernels and the General Problem of String Prediction,"S_bastien Giguère, Am_lie Rolland, Francois Laviolette, Mario Marchand",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/giguere15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/giguere15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_giguere15,http://jmlr.csail.mit.edu/proceedings/papers/v37/giguere15.html,"We address the pre-image problem encountered in structured output prediction and the one of finding a string maximizing the prediction function of various kernel-based classifiers and regressors. We demonstrate that these problems reduce to a common combinatorial problem valid for many string kernels. For this problem, we propose an upper bound on the prediction function which has low computational complexity and which can be used in a branch and bound search algorithm to obtain optimal solutions. We also show that for many string kernels, the complexity of the problem increases significantly when the kernel is normalized. On the optical word recognition task, the exact solution of the pre-image problem is shown to significantly improve the prediction accuracy in comparison with an approximation found by the best known heuristic. On the task of finding a string maximizing the prediction function of kernel-based classifiers and regressors, we highlight that existing methods can be biased toward long strings that contain many repeated symbols. We demonstrate that this bias is removed when using normalized kernels. Finally, we present results for the discovery of lead compounds in drug discovery. The source code can be found at https://github.com/a-ro/preimage"
858,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Towards Minimax Policies for Online Linear Optimization with Bandit Feedback,"S_bastien Bubeck, Nicolo Cesa-Bianchi and Sham M. Kakade",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/bubeck12a/bubeck12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_bubeck12a,http://jmlr.csail.mit.edu/proceedings/papers/v23/bubeck12a.html,"We address the online linear optimization problem with bandit feedback. Our contribution is twofold. First, we provide an algorithm (based on exponential weights) with a regret of order í dn log N for any finite action set with N actions, under the assumption that the instantaneous loss is bounded by 1. This shaves off an extraneous í d factor compared to previous works, and gives a regret bound of order d í n log n for any compact set of actions. Without further assumptions on the action set, this last bound is minimax optimal up to a logarithmic factor. Interestingly, our result also shows that the minimax regret for bandit linear optimization with expert advice in d dimension is the same as for the basic d -armed bandit with expert advice. Our second contribution is to show how to use the Mirror Descent algorithm to obtain computationally efficient strategies with minimax optimal regret bounds in specific examples. More precisely we study two canonical action sets: the hypercube and the Euclidean ball. In the former case, we obtain the first computationally efficient algorithm with a d í n regret, thus improving by a factor í d log n over the best known result for a computationally efficient algorithm. In the latter case, our approach gives the first algorithm with a í dn log n , again shaving off an extraneous í d compared to previous works."
859,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Minimax Policies for Combinatorial Prediction Games,"Jean-Yves Audibert, S_bastien Bubeck, Gˆbor Lugosi","19:107-132, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/audibert11a/audibert11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_audibert11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/audibert11a.html,We address the online linear optimization problem when theactions of the forecaster are represented by binary vectors.Our goal is to understand the magnitude of the minimax regretfor the worst possible set of actions. We study the problemunder three different assumptions for the feedback: full information and the partial information models of theso-called ``semi-bandit'' and ``bandit'' problems. We consider both $L_\infty$- and $L_2$-type of restrictions forthe losses assigned by the adversary.We formulate a general strategy using Bregman projections on top of a potential-based gradient descent which generalizes the ones studied in the series of papers \cite{GLLO07 DHK08 AHR08 CL09 HW09 KWK10 UNK10 KRS10} and \cite{AB10}. We provide simpleproofs that recover most of the previous results. We propose new upper bounds for the semi-bandit game. Moreover we derive lower bounds for all three feedback assumptions. With the only exception of the bandit game the upper and lower boundsare tight up to a constant factor.Finally we answer a question asked by \cite{KWK10} by showing that the exponentially weighted average forecaster is suboptimal against $L_{\infty}$ adversaries.
860,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Tight conditions for consistent variable selection in high dimensional nonparametric regression,"Laïtitia Comminges, Arnak S. Dalalyan","19:187-206, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/comminges11a/comminges11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_comminges11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/comminges11a.html,We address the issue of variable selection in the regression model with very high ambient dimension \textit{i.e.} when the number of covariates is very large. The main focus is on the situation where the number of relevant covariates called intrinsic dimension is much smaller than the ambient dimension. Without assuming any parametric form of the underlying regression function we get tight conditions making it possible to consistently estimate the set of relevant variables. These conditions relate the intrinsic dimension to the ambient dimension and to the sample size. The procedure that is provably consistent under these tight conditions is simple and is based on comparing the empirical Fourier coefficients with an appropriately chosen threshold value.
861,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Mini-Batch Primal and Dual Methods for SVMs,"Martin Takac, Avleen Bijral, Peter Richtarik, Nati Srebro",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/takac13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/takac13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_takac13,http://jmlr.csail.mit.edu/proceedings/papers/v28/takac13.html,"We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent(SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss."
862,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Bayes-Optimal Scorers for Bipartite Ranking,"Aditya Krishna Menon, Robert C. Williamson","JMLR W&CP 35 :68-106, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/menon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_menon14,http://jmlr.csail.mit.edu/proceedings/papers/v35/menon14.html,"We address the following seemingly simple question: what is the Bayes-optimal scorer for a bipartite ranking risk? The answer to this question helps establish the consistency of the minimisation of surrogate bipartite risks, and elucidates the relationship between bipartite ranking and other established learning problems. We show that the answer is non-trivial in general, but may be easily determined for certain special cases using the theory of proper losses. Our analysis immediately establishes equivalences between several seemingly disparate risks for bipartite ranking, such as minimising a suitable class-probability estimation risk, and minimising the \(p\) -norm push risk proposed by Rudin (2009)."
863,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Consistent Collective Matrix Completion under Joint Low Rank Structure,"Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/gunasekar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/gunasekar15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_gunasekar15,http://jmlr.csail.mit.edu/proceedings/papers/v38/gunasekar15.html,"We address the collective matrix completion problem of jointly recovering a collection of matrices with shared structure from partial (and potentially noisy) observations. To ensure well_posedness of the problem, we impose a joint low rank structure, wherein each component matrix is low rank and the latent space of the low rank factors corresponding to each entity is shared across the entire collection. We first develop a rigorous algebra for representing and manipulating collective_matrix structure, and identify sufficient conditions for consistent estimation of collective matrices. We then propose a tractable convex estimator for solving the collective matrix completion problem, and provide the first non_trivial theoretical guarantees for consistency of collective matrix completion. We show that under reasonable assumptions stated in Sec. 3.1, with high probability, the proposed estimator exactly recovers the true matrices whenever sample complexity requirements dictated by Theorem 1 are met. The sample complexity requirement derived in the paper are optimum up to logarithmic factors, and significantly improve upon the requirements obtained by trivial extensions of standard matrix completion. Finally, we propose a scalable approximate algorithm to solve the proposed convex program, and corroborate our results through simulated and real life experiments."
864,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Hierarchical Gaussian Process Regression,Sunho Park and Seungjin Choi,"13:95-110, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/park10a/park10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_park10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/park10a.html,We address an approximation method for Gaussian process (GP) regression where we approximate covariance by a block matrix such that diagonal blocks are calculated exactly while off-diagonal blocks are approximated. Partitioning input data points we present a two-layer hierarchical model for GP regression where prototypes of clusters in the upper layer are involved for coarse modeling by a GP and data points in each cluster in the lower layer are involved for fine modeling by an individual GP whose prior mean is given by the corresponding prototype and covariance is parameterized by data points in the partition. In this hierarchical model integrating out latent variables in the upper layer leads to a block covariance matrix where diagonal blocks contain similarities between data points in the same partition and off-diagonal blocks consist of approximate similarities calculated using prototypes. This particular structure of the covariance matrix divides the full GP into a pieces of manageable sub-problems whose complexity scales with the number of data points in a partition. In addition our hierarchical GP regression (HGPR) is also useful for cases where partitions of data reveal different characteristics. Experiments on several benchmark datasets confirm the useful behavior of our method.
865,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Interval Insensitive Loss for Ordinal Classification,"Kostiantyn Antoniuk, Vojtech Franc, Vaclav Hlavac",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/antoniuk14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_antoniuk14,http://jmlr.csail.mit.edu/proceedings/papers/v39/antoniuk14.html,We address a problem of learning ordinal classifier from partially annotated examples. We introduce an interval-insensitive loss function to measure discrepancy between predictions of an ordinal classifier and a partial annotation provided in the form of intervals of admissible labels. The proposed interval-insensitive loss is an instance of loss functions previously used for learning of different classification models from partially annotated examples. We propose several convex surrogates of the interval-insensitive loss which can be efficiently optimized by existing solvers. Experiments on standard benchmarks and a real-life application show that learning ordinal classifiers from partially annotated examples is competitive to the so-far used methods learning from the complete annotation.
866,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Predictive Simulation Framework of Stochastic Diffusion Model for Identifying Top-K Influential Nodes,"Kouzou Ohara, Kazumi Saito, Masahiro Kimura, Hiroshi Motoda","JMLR W&CP 29 :149-164, 2013",http://jmlr.org/proceedings/papers/v29/Ohara13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Ohara13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Ohara13.html,"We address a problem of efficiently estimating the influence of a node in information diffusion over a social network. Since the information diffusion is a stochastic process, the influence degree of a node is quantified by the expectation, which is usually obtained by very time consuming many runs of simulation. Our contribution is that we proposed a framework for predictive simulation based on the leave- \(N\) -out cross validation technique that well approximates the error from the unknown ground truth for two target problems: one to estimate the influence degree of each node, and the other to identify top- \(K\) influential nodes. The method we proposed for the first problem estimates the approximation error of the influence degree of each node, and the method for the second problem estimates the precision of the derived top- \(K\) nodes, both without knowing the true influence degree. We experimentally evaluate the proposed methods using the three real world networks, and show that they can serve as a good measure to solve the target problems with far fewer runs of simulation ensuring the accuracy if \(N\) is appropriately chosen, and that estimating the top- \(K\) nodes is easier than estimating the influence degree, which means one can identify the influential nodes without knowing exactly their influence degree."
867,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Optimization Equivalence of Divergences Improves Neighbor Embedding,"Zhirong Yang, Jaakko Peltonen, Samuel Kaski",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yange14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yange14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yange14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yange14.html,"Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two previously separate approaches are tied by an optimization equivalence, making it possible to relate methods from the two approaches and to build new methods that take the best of both worlds. In detail, we prove a theorem of optimization equivalences between beta- and gamma-, as well as alpha- and Renyi-divergences through a connection scalar. Through the equivalences we represent several nonlinear dimensionality reduction and graph drawing methods in a generalized stochastic neighbor embedding setting, where information divergences are minimized between similarities in input and output spaces, and the optimal connection scalar provides a natural choice for the tradeoff between attractive and repulsive forces. We give two examples of developing new visualization methods through the equivalences: 1) We develop weighted symmetric stochastic neighbor embedding (ws-SNE) from Elastic Embedding and analyze its benefits, good performance for both vectorial and network data; in experiments ws-SNE has good performance across data sets of different types, whereas comparison methods fail for some of the data sets; 2) we develop a gamma-divergence version of a PolyLog layout method; the new method is scale invariant in the output space and makes it possible to efficiently use large-scale smoothed neighborhoods."
868,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Natural Image Bases to Represent Neuroimaging Data,"Ashish Gupta, Murat Ayhan, Anthony Maida",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gupta13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/gupta13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gupta13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/gupta13b.html,"Visual inspection of neuroimagery is susceptible to human eye limitations. Computerized methods have been shown to be equally or more effective than human clinicians in diagnosing dementia from neuroimages. Nevertheless, much of the work involves the use of domain expertise to extract hand-crafted features. The key technique in this paper is the use of cross-domain features to represent MRI data. We used a sparse autoencoder to learn a set of bases from natural images and then applied convolution to extract features from the Alzheimerês Disease Neuroimaging Initiative (ADNI) dataset.Using this new representation, we classify MRI instances into three categories: Alzheimerês Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC).Our approach, in spite of being very simple, achieved high classification performance, which is competitive with or better than other approaches."
869,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning Nonlinear Dynamic Models from Non-sequenced Data,"Tzu_Kuo Huang, Le Song, Jeff Schneider","9:350-357, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10c/huang10c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_huang10c,http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10c.html,Virtually all methods of learning dynamic systems from data start from the same basic assumption: the learning algorithm will be given a sequence or trajectory of data generated from the dynamic system. We consider the case where the data is not sequenced. The training data points come from the system's operation but with no temporal ordering. The data are simply drawn as individual disconnected points. While making this assumption may seem absurd at first glance many scientific modeling tasks have exactly this property. Previous work proposed methods for learning linear discrete time models under these assumptions by optimizing approximate likelihood functions. In this paper we extend those methods to nonlinear models using kernel methods. We go on to propose a new approach to solving the problem that focuses on achieving temporal smoothness in the learned dynamics. The result is a convex criterion that can be easily optimized and often outperforms the earlier methods. We test these methods on several synthetic data sets including one generated from the Lorenz attractor.
870,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Comparing classification methods for predicting distance students' performance,"Diego Garcia-Saiz, Marta Zorrilla","17:26-32, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/garcia-saiz11a/garcia-saiz11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_garcia-saiz11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/garcia-saiz11a.html,"Virtual teaching is constantly growing and, with it, the necessity of instructors to predict the performance of their students. In response to this necessity, different machine learning techniques can be used. Although there are so many benchmarks comparing their performance and accuracy, there are still very few experiments carried out on educational datasets which have very special features which make them different from other datasets. Therefore, in this work we compare the performance and interpretation level of the output of the different classification techniques applied on educational datasets and propose a meta-algorithm to preprocess the datasets and improve the accuracy of the model, which will be used by virtual instructors for their decision making through the ElWM tool."
871,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Direct Modeling of Complex Invariances for Visual Object Features,Ka Yu Hui,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yuhui13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yuhui13,http://jmlr.csail.mit.edu/proceedings/papers/v28/yuhui13.html,"View-invariant object representations created from feature pooling networks have been widely adopted in state-of-the-art visual recognition systems. Recently, the research community seeks to improve these view-invariant representations further by additional invariance and receptive field learning, or by taking on the challenge of processing massive amounts of learning data. In this paper we consider an alternate strategy of directly modeling complex invariances of object features. While this may sound like a naive and inferior approach, our experiments show that this approach can achieve competitive and state-of-the-art accuracy on visual recognition data sets such as CIFAR-10 and STL-10. We present an highly applicable dictionary learning algorithm on complex invariances that can be used in most feature pooling network settings. It also has the merits of simplicity and requires no additional tuning. We also discuss the implication of our experiment results concerning recent observations on the usefulness of pre-trained features, and the role of direct invariance modeling in invariance learning."
872,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Optimal Probability Estimation with Applications to Prediction and Classification,"Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Acharya13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Acharya13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Acharya13.html,"Via a unified viewpoint of probability estimation, classification,and prediction, we derive a uniformly-optimal combined-probability estimator, construct a classifier that uniformly approaches the error of the best possible label-invariant classifier, and improve existing results on pattern prediction and compression."
873,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem,"Ittai Abraham, Omar Alonso, Vasilis Kandylas, Aleksandrs Slivkins",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Abraham13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Abraham13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Abraham13.html,"Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have some limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the –bandit survey problem”. This model is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations.Our approach is based in our experience conducting relevance evaluation for a large commercial search engine."
874,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Black Box Variational Inference,"Rajesh Ranganath, Sean Gerrish, David Blei","JMLR W&CP 33 :814-822, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ranganath14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/ranganath14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ranganath14,http://jmlr.csail.mit.edu/proceedings/papers/v33/ranganath14.html,"Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a –black box” variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data."
875,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Analysis of Empirical MAP and Empirical Partially Bayes: Can They be Alternatives to Variational Bayes?,"Shinichi Nakajima, Masashi Sugiyama","JMLR W&CP 33 :20-28, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/nakajima14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_nakajima14,http://jmlr.csail.mit.edu/proceedings/papers/v33/nakajima14.html,"Variational Bayesian (VB) learning is known to be a promising approximation to Bayesian learning with computational efficiency. However, in some applications, e.g., large-scale collaborative filtering and tensor factorization, VB is still computationally too costly. In such cases, looser approximations such as MAP estimation and partially Bayesian (PB) learning, where a part of the parameters are point-estimated, seem attractive. In this paper, we theoretically investigate the behavior of the MAP and the PB solutions of matrix factorization. A notable finding is that the global solutions of MAP and PB in the empirical Bayesian scenario, where the hyperparameters are also estimated from observation, are trivial and useless, while their local solutions behave similarly to the global solution of VB. This suggests that empirical MAP and empirical PB with local search can be alternatives to empirical VB equipped with the useful automatic relevance determination property. Experiments support our theory."
876,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,"Proteins, Particles, and Pseudo-Max-Marginals: A Submodular Approach","Jason Pacheco, Erik Sudderth",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/pacheco15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_pacheco15,http://jmlr.csail.mit.edu/proceedings/papers/v37/pacheco15.html,"Variants of max-product (MP) belief propagation effectively find modes of many complex graphical models, but are limited to discrete distributions. Diverse particle max-product (D-PMP) robustly approximates max-product updates in continuous MRFs using stochastically sampled particles, but previous work was specialized to tree-structured models. Motivated by the challenging problem of protein side chain prediction, we extend D-PMP in several key ways to create a generic MAP inference algorithm for loopy models. We define a modified diverse particle selection objective that is provably submodular, leading to an efficient greedy algorithm with rigorous optimality guarantees, and corresponding max-marginal error bounds. We further incorporate tree-reweighted variants of the MP algorithm to allow provable verification of global MAP recovery in many models. Our general-purpose Matlab library is applicable to a wide range of pairwise graphical models, and we validate our approach using optical flow benchmarks. We further demonstrate superior side chain prediction accuracy compared to baseline algorithms from the state-of-the-art Rosetta package."
877,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Variable Selection is Hard,"Dean Foster, Howard Karloff, Justin Thaler",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Foster15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Foster15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Foster15.html,"Variable selection for sparse linear regression is the problem of finding, given an \(m\times p\) matrix \(B\) and a target vector \(\bfy\) , a sparse vector \(\bfx\) such that \(B\bfx\) approximately equals \(\bfy\) . Assuming a standard complexity hypothesis, we show that no polynomial-time algorithm can find a \(k'\) -sparse \(\bfx\) with \(\|B\bfx-\bfy\|^2\le h(m,p)\) , where \(k'=k\cdot 2^{\log ^{1-\delta} p}\) and \(h(m,p)= p^{C_1} m^{1-C_2}\) , where \(\delta_0,C_1_0,C_2_0\) are arbitrary. This is true even under the promise that there is an unknown \(k\) -sparse vector \(\bfx^*\) satisfying \(B\bfx^*=\bfy\) . We prove a similar result for a statistical version of the problem in which the data are corrupted by noise. To the authorsê knowledge, these are the first hardness results for sparse regression that apply when the algorithm simultaneously has \(k'_k\) and \(h(m,p)_0\) ."
878,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Universal Value Function Approximators,"Tom Schaul, Daniel Horgan, Karol Gregor, David Silver",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/schaul15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/schaul15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_schaul15,http://jmlr.csail.mit.edu/proceedings/papers/v37/schaul15.html,"Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \(\theta\) . In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals."
879,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Distribution-Independent Evolvability of Linear Threshold Functions,Vitaly Feldman,"19:253-272, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/feldman11b/feldman11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_feldman11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/feldman11b.html,Valiant's model of evolvability models the evolutionary process of acquiring useful functionality as a restricted form of learning from random examples \citep{Valiant:09}.Linear threshold functions and their various subclasses such as conjunctions and decision lists play a fundamental role in learning theory and hence their evolvabilityhas been the primary focus of research on Valiant's framework. One of the main open problems regarding the model is whether conjunctions are evolvable distribution-independently\citep{FeldmanValiant:08colt}. We show that the answer is negative. Our proof is based on a new combinatorial parameter of a concept class that lower-bounds the complexity of learning fromcorrelations.We contrast the lower bound with a proof that linear threshold functions having a non-negligible margin on the data points are evolvable distribution-independently via a simple mutationalgorithm. Our algorithm relies on a non-linear loss function being used to select the hypotheses instead of 0-1 loss in Valiant's original definition. The proof of evolvabilityrequires that the loss function satisfies several mild conditions that are for example satisfied by the quadratic loss function studied in several other works \citep{Michael:07Feldman:09sqdValiantp:11manu}. An important property of our evolution algorithm is monotonicity that is the algorithm guaranteesevolvability without any decreases in performance. Previously monotone evolvability was only shown for conjunctions with quadratic loss \citep{Feldman:09sqd} or when the distribution on the domain is severely restricted \citep{Michael:07Feldman:09sqdKanadeVV:10}.
880,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Sparse Bayesian Variable Selection for the Identification of Antigenic Variability in the Foot-and-Mouth Disease Virus,"Vinny Davies, Richard Reeve, William Harvey, Francois Maree, Dirk Husmeier","JMLR W&CP 33 :149-158, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/davies14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_davies14,http://jmlr.csail.mit.edu/proceedings/papers/v33/davies14.html,"Vaccines created from closely related viruses are vital for offering protection against newly emerging strains. For Foot-and-Mouth disease virus (FMDV), where multiple serotypes co-circulate, testing large numbers of vaccines can be infeasible. Therefore the development of an in silico predictor of cross-protection between strains is important to help optimise vaccine choice. Here we describe a novel sparse Bayesian variable selection model using spike and slab priors which is able to predict antigenic variability and identify sites which are important for the neutralisation of the virus. We are able to identify multiple residues which are known to be key indicators of antigenic variability. Many of these were not identified previously using Frequentist mixed-effects models and still cannot be found when an L1 penalty is used. We further explore how the Markov chain Monte Carlo (MCMC) proposal method for the inclusion of variables can offer significant reductions in computational requirements, both for spike and slab priors in general, and our hierarchical Bayesian model in particular."
881,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Stepwise uncertainty reduction approach to constrained global optimization,Victor Picheny,"JMLR W&CP 33 :787-795, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/picheny14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_picheny14,http://jmlr.csail.mit.edu/proceedings/papers/v33/picheny14.html,"Using statistical emulators to guide sequential evaluations of complex computer experiments is now a well-established practice. When a model provides multiple outputs, a typical objective is to optimize one of the outputs with constraints (for instance, a threshold not to exceed) on the values of the other outputs. We propose here a new optimization strategy based on the stepwise uncertainty reduction paradigm, which offers an efficient trade-off between exploration and local search near the boundaries. The strategy is illustrated on numerical examples."
882,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Filtering with Abstract Particles,"Jacob Steinhardt, Percy Liang",none,http://jmlr.org/proceedings/papers/v32/steinhardt14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/steinhardt14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_steinhardt14,http://jmlr.csail.mit.edu/proceedings/papers/v32/steinhardt14.html,"Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue by using –abstract particles” that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task."
883,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Learning and Testing the Bounded Retransmission Protocol,"Fides Aarts, Harco Kuppens, Jan Tretmans, Frits Vaandrager and Sicco Verwer","21:4-18, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/aarts12a/aarts12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_aarts12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/aarts12a.html,"Using a well-known industrial case study from the verification literature, the bounded retransmission protocol, we show how active learning can be used to establish the correctness of protocol implementation I relative to a given reference implementation R . Using active learning, we learn a model M R of reference implementation R , which serves as input for a model based testing tool that checks conformance of implementation I to M R . In addition, we also explore an alternative approach in which we learn a model M I of implementation I , which is compared to model M R using an equivalence checker. Our work uses a unique combination of software tools for model construction (Uppaal), active learning (LearnLib, Tomte), model-based testing (JTorX, TorXakis) and verification (CADP, MRMC). We show how these tools can be used for learning these models, analyzing the obtained results, and improving the learning performance."
884,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Inferning with High Girth Graphical Models,"Uri Heinemann, Amir Globerson",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/heinemann14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/heinemann14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_heinemann14,http://jmlr.csail.mit.edu/proceedings/papers/v32/heinemann14.html,"Unsupervised learning of graphical models is an important task in many domains. Although maximum likelihood learning is computationally hard, there do exist consistent learning algorithms (e.g., psuedo-likelihood and its variants). However, inference in the learned models is still hard, and thus they are not directly usable. In other words, given a probabilistic query they are not guaranteed to provide an answer that is close to the true one. In the current paper, we provide a learning algorithm that is guaranteed to provide approximately correct probabilistic inference. We focus on a particular class of models, namely high girth graphs in the correlation decay regime. It is well known that approximate inference (e.g, using loopy BP) in such models yields marginals that are close to the true ones. Motivated by this, we propose an algorithm that always returns models of this type, and hence in the models it returns inference is approximately correct. We derive finite sample results guaranteeing that beyond a certain sample size, the resulting models will answer probabilistic queries with a high level of accuracy. Results on synthetic data show that the models we learn indeed outperform those obtained by other algorithms, which do not return high girth graphs."
885,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,K-means recovers ICA filters when independent components are sparse,"Alon Vinnikov, Shai Shalev-Shwartz",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/vinnikov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/vinnikov14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_vinnikov14,http://jmlr.csail.mit.edu/proceedings/papers/v32/vinnikov14.html,"Unsupervised feature learning is the task of using unlabeled examples for building a representation of objects as vectors. This task has been extensively studied in recent years, mainly in the context of unsupervised pre-training of neural networks. Recently, (Coates et al., 2011) conducted extensive experiments, comparing the accuracy of a linear classifier that has been trained using features learnt by several unsupervised feature learning methods. Surprisingly, the best performing method was the simplest feature learning approach that was based on applying the K-means clustering algorithm after a whitening of the data. The goal of this work is to shed light on the success of K-means with whitening for the task of unsupervised feature learning. Our main result is a close connection between K-means and ICA (Independent Component Analysis). Specifically, we show that K-means and similar clustering algorithms can be used to recover the ICA mixing matrix or its inverse, the ICA filters. It is well known that the independent components found by ICA form useful features for classification (Le et al., 2012; 2011; 2010), hence the connection between K-mean and ICA explains the empirical success of K-means as a feature learner. Moreover, our analysis underscores the significance of the whitening operation, as was also observed in the experiments reported in (Coates et al., 2011). Finally, our analysis leads to a better initialization of K-means for the task of feature learning."
886,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning and Selecting Features Jointly with Point-wise Gated Boltzmann Machines,"Kihyuk Sohn, Guanyu Zhou, Chansoo Lee, Honglak Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sohn13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sohn13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sohn13.html,"Unsupervised feature learning has emerged as a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i.e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines feature learning and feature selection. Our model performs not only feature selection on learned high-level features (i.e., hidden units), but also dynamic feature selection on raw features (i.e., visible units) through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the visible units corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks."
887,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Reconstruction of Excitatory Neuronal Connectivity via Metric Score Pooling and Regularization,"Chenyang Tao, Wei Lin, Jianfeng Feng",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/tao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_tao15,http://jmlr.csail.mit.edu/proceedings/papers/v46/tao15.html,"Unravelling the causal link of neuronal pairs has considerable impacts in neuroscience, yet it still remains a major challenge. Recent investigations in the literature show that the Generalized Transfer Entropy (GTE), derived from information theory, has a great capability of reconstructing the underlying connectomics. In this work, we first generalize the GTE to a measure called Csiszarês Transfer Entropy (CTE). With a proper choice of the convex function, the CTE outperforms the GTE in connectomic reconstruction, especially in the synchronized bursting regime where the GTE was reported to have poor sensitivity. Akin to the ensemble learning approach, we then pool various measures to achieve cutting edge neuronal network connectomic reconstruction performance. As a final step emphasize the importance of introducing regularization schemes in the network reconstruction."
888,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Predictive Entropy Search for Bayesian Optimization with Unknown Constraints,"Jose Miguel Hernandez-Lobato, Michael Gelbart, Matthew Hoffman, Ryan Adams, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatob15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatob15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hernandez-lobatob15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatob15.html,"Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraintsãi.e., when one can independently evaluate the objective or the constraintsãEI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization."
889,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Generating Efficient MCMC Kernels from Probabilistic Programs,"Lingfeng Yang, Patrick Hanrahan, Noah Goodman","JMLR W&CP 33 :1068-1076, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_yang14d,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14d.html,"Universal probabilistic programming languages (such as Church) trade performance for abstraction: any model can be represented compactly as an arbitrary stochastic computation, but costly online analyses are required for inference. We present a technique that recovers hand-coded levels of performance from a universal probabilistic language, for the Metropolis-Hastings (MH) MCMC inference algorithm. It takes a Church program as input and traces its execution to remove computation overhead. It then analyzes the trace for each proposal, using slicing, to identify the minimal computation needed to evaluate the MH acceptance probability. Generated incremental code is much faster than a baseline implementation (up to 600x) and usually as fast as hand-coded MH kernels."
890,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,"On the relation between universality, characteristic kernels and RKHS embedding of measures","Bharath Sriperumbudur, Kenji Fukumizu, Gert Lanckriet","9:773-780, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sriperumbudur10a/sriperumbudur10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sriperumbudur10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sriperumbudur10a.html,Universal kernels have been shown to play an important role in the achievability of the Bayes risk by many kernel-based algorithms that include binary classification regression etc. In this paper we propose a notion of universality that generalizes the notions introduced by Steinwart and Micchelli et al. and study the necessary and sufficient conditions for a kernel to be universal. We show that all these notions of universality are closely linked to the injective embedding of a certain class of Borel measures into a reproducing kernel Hilbert space (RKHS). By exploiting this relation between universality and the embedding of Borel measures into an RKHS we establish the relation between universal and characteristic kernels. The latter have been proposed in the context of the RKHS embedding of probability measures used in statistical applications like homogeneity testing independence testing etc.
891,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Stochastic Optimization with Importance Sampling for Regularized Loss Minimization,"Peilin Zhao, Tong Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaoa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhaoa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaoa15.html,"Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness."
892,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades,"Xinran He, Theodoros Rekatsinas, James Foulds, Lise Getoor, Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/he15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/he15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_he15,http://jmlr.csail.mit.edu/proceedings/papers/v37/he15.html,"Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as –retweeting a post” or –publishing a follow-up blog post”. HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks."
893,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Lower bounds on the performance of polynomial-time algorithms for sparse linear regression,"Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan","JMLR W&CP 35 :921-948, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/zhang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_zhang14,http://jmlr.csail.mit.edu/proceedings/papers/v35/zhang14.html,"Under a standard assumption in complexity theory (NP not in P/poly), we demonstrate a gap between the minimax prediction risk for sparse linear regression that can be achieved by polynomial-time algorithms, and that achieved by optimal algorithms. In particular, when the design matrix is ill-conditioned, the minimax prediction loss achievable by polynomial-time algorithms can be substantially greater than that of an optimal algorithm. This result is the first known gap between polynomial and optimal algorithms for sparse linear regression, and does not depend on conjectures in average-case complexity."
894,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Gradient-based Hyperparameter Optimization through Reversible Learning,"Dougal Maclaurin, David Duvenaud, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/maclaurin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/maclaurin15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_maclaurin15,http://jmlr.csail.mit.edu/proceedings/papers/v37/maclaurin15.html,"Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum."
895,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A PAC-Bayesian bound for Lifelong Learning,"Anastasia Pentina, Christoph Lampert",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/pentina14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_pentina14,http://jmlr.csail.mit.edu/proceedings/papers/v32/pentina14.html,"Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods."
896,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Active Transfer Learning under Model Shift,"Xuezhi Wang, Tzu-Kuo Huang, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangi14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangi14.html,"Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source task) but only very limited training data for a second task (the target task) that is similar but not identical to the first. These algorithms use varying assumptions about the similarity between the tasks to carry information from the source to the target task. Common assumptions are that only certain specific marginal or conditional distributions have changed while all else remains the same. Alternatively, if one has only the target task, but also has the ability to choose a limited amount of additional training data to collect, then active learning algorithms are used to make choices which will most improve performance on the target task. These algorithms may be combined into active transfer learning, but previous efforts have had to apply the two methods in sequence or use restrictive transfer assumptions. We propose two transfer learning algorithms that allow changes in all marginal and conditional distributions but assume the changes are smooth in order to achieve transfer between the tasks. We then propose an active learning algorithm for the second method that yields a combined active transfer learning algorithm. We demonstrate the algorithms on synthetic functions and a real-world task on estimating the yield of vineyards from images of the grapes."
897,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Support vector machines with indefinite kernels,"Ibrahim Alabdulmohsin, Xin Gao, Xiangliang Zhang Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/alabdulmohsin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_alabdulmohsin14,http://jmlr.csail.mit.edu/proceedings/papers/v39/alabdulmohsin14.html,"Training support vector machines (SVM) with indefinite kernels has recently attracted attention in the machine learning community. This is partly due to the fact that many similarity functions that arise in practice are not symmetric positive semidefinite, i.e. the Mercer condition is not satisfied, or the Mercer condition is difficult to verify. Previous work on training SVM with indefinite kernels has generally fallen into three categories: (1) positive semidefinite kernel approximation, (2) non-convex optimization, and (3) learning in Krein spaces. All approaches are not fully satisfactory. They have either introduced sources of inconsistency in handling training and test examples using kernel approximation, settled for approximate local minimum solutions using non-convex optimization, or produced non-sparse solutions. In this paper, we establish both theoretically and experimentally that the 1-norm SVM, proposed more than 10 years ago for embedded feature selection, is a better solution for extending SVM to indefinite kernels. More specifically, 1-norm SVM can be interpreted as a structural risk minimization method that seeks a decision boundary with large similarity margin in the original space. It uses a linear programming formulation that remains convex even if the kernel matrix is indefinite, and hence can always be solved quite efficiently. Also, it uses the indefinite similarity function (or distance) directly without any transformation, and, hence, it always treats both training and test examples consistently. Finally, it achieves the highest accuracy among all methods that train SVM with indefinite kernels with a statistically significant evidence while also retaining sparsity of the support vector set."
898,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Treba: Efficient Numerically Stable EM for PFA,Mans Hulden,"21:249-253, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/hulden12a/hulden12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_hulden12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/hulden12a.html,"Training probabilistic finite automata with the EM/Baum-Welch algorithm is computationally very intensive, especially if random ergodic automata are used initially, and additional strategies such as deterministic annealing are used. In this paper we present some optimization and parallelization strategies to the Baum-Welch algorithm that often allow for training of much larger automata with a larger number of observations. The tool, treba , which implements the optimizations, is available open-source and its results were used to participate in the PAutomaC PFA/HMM competition."
899,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,DivMCuts: Faster Training of Structural SVMs with Diverse M-Best Cutting-Planes,"Abner Guzman-Rivera, Pushmeet Kohli, Dhruv Batra",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/guzman-rivera13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/guzman-rivera13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_guzman-rivera13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/guzman-rivera13a.html,"Training of Structural SVMs involves solving a large Quadratic Program (QP). One popular method for solving this QP is a cutting-plane approach, where the most violated constraint is iteratively added to a working-set of constraints. Unfortunately, training models with a large number of parameters remains a time consuming process. This paper shows that significant computational savings can be achieved by adding multiple diverse and highly violated constraints at every iteration of the cutting-plane algorithm. We show that generation of such diverse cutting-planes involves extracting diverse M-Best solutions from the loss-augmented score of the training instances. To find these diverse M-Best solutions, we employ a recently proposed algorithm [4]. Our experiments on image segmentation and protein side-chain prediction show that the proposed approach can lead to significant computational savings, e.g., \(\sim 28\%\) reduction in training time."
900,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Deep Learning with Limited Numerical Precision,"Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gupta15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gupta15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gupta15.html,"Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the networkês behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding"
901,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM,"Ching-Pei Lee, Dan Roth",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/leea15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/leea15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_leea15,http://jmlr.csail.mit.edu/proceedings/papers/v37/leea15.html,"Training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine, motivating recent works on developing algorithms that train in a distributed fashion. This paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines (SVMs) with large data. Our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration, using an efficient method that requires only O(1) communication cost to ensure fast convergence. With this optimal step size, our approach is superior to other methods by possessing global linear convergence, or, equivalently, \(O(\log(1/\epsilon))\) iteration complexity for an epsilon-accurate solution, for distributedly solving the non-strongly-convex linear SVM dual problem. Experiments also show that our method is significantly faster than state-of- the-art distributed linear SVM algorithms including DSVM-AVE, DisDCA and TRON."
902,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Sergey Ioffe, Christian Szegedy",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ioffe15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ioffe15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ioffe15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ioffe15.html,"Training Deep Neural Networks is complicated by the fact that the distribution of each layerês inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
903,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Sparse Higher-Order Principal Components Analysis,Genevera Allen,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/allen12/allen12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_allen12,http://jmlr.csail.mit.edu/proceedings/papers/v22/allen12.html,Traditional tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions yield higher-order principal components that have been used to understand tensor data in areas such as neuroimaging microscopy chemometrics and remote sensing. Sparsity in high-dimensional matrix factorizations and principal components has been well-studied exhibiting many benefits; less attention has been given to sparsity in tensor decompositions. We propose two novel tensor decompositions that incorporate sparsity: the Sparse Higher-Order SVD and the Sparse CP Decomposition. The latter solves a 1-norm penalized relaxation of the single-factor CP optimization problem thereby automatically selecting relevant features for each tensor factor. Through experiments and a scientific data analysis example we demonstrate the utility of our methods for dimension reduction feature selection signal recovery and exploratory data analysis of high-dimensional tensors.
904,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust Distance Metric Learning via Simultaneous L1-Norm Minimization and Maximization,"Hua Wang, Feiping Nie, Heng Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangj14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangj14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangj14.html,"Traditional distance metric learning with side information usually formulates the objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Because the covariance matrix computes the sum of the squared L2-norm distances, it is prone to both outlier samples and outlier features. To develop a robust distance metric learning method, in this paper we propose a new objective for distance metric learning using the L1-norm distances. However, the resulted objective is very challenging to solve, because it simultaneously minimizes and maximizes (minmax) a number of non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is rarely studied in literature. We have performed extensive empirical evaluations, where our new distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings to cluster both noiseless and noisy data."
905,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions,"Lijun Zhang, Tianbao Yang, Rong Jin, Xiaofei He",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13e-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13e,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13e.html,"Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result."
906,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Mining Recurring Concept Drifts with Limited Labeled Streaming Data,"Peipei Li, Xindong Wu, and Xuegang Hu","13:241-252, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/li10a/li10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_li10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/li10a.html,Tracking recurring concept drifts is a significant issue for machine learning and data mining that frequently appears in real world stream classification problems. It is a challenge for many streaming classification algorithms to learn recurring concepts in a data stream envi- ronment with unlabeled data and this challenge has received little attention from the research community. Motivated by this challenge this paper focuses on the problem of recurring contexts in streaming environments with limited labeled data. We propose a Semisupervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data called REDLLA in which a decision tree is adopted as the classification model. When growing a tree a clustering algorithm based on k-Means is installed to produce concept clusters and unlabeled data are labeled at leaves. In view of deviations between history and new concept clusters potential concept drifts are distinguished and recurring concepts are maintained. Extensive studies on both synthetic and real-world data confirm the advantages of our REDLLA algorithm over two state-of-the-art online classification algorithms of CVFDT and CDRDT and several known online semi-supervised algorithms even in the case with more than 90% unlabeled data.
907,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,A Linear Ensemble of Individual and Blended Models for Music Rating Prediction,P.-L. Chen,"18:21_60, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/chen12a/chen12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_chen12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/chen12a.html,Track 1 of KDDCup 2011 aims at predicting the rating behavior of users in the Yahoo! Music system. At National Taiwan University we organize a course that teams up students to work on both tracks of KDDCup 2011. For track 1 we _rst tackle the problem by building variants of existing individual models including Matrix Factorization Restricted Boltzmann Machine k -Nearest Neighbors Probabilistic Latent Semantic Analysis Probabilistic Principle Component Analysis and Supervised Regression. We then blend the individual models along with some carefully extracted features in a non-linear manner. A large linear ensemble that contains both the individual and the blended models is learned and taken through some post-processing steps to form the _nal solution. The four stages: individual model building non-linear blending linear ensemble and post-processing lead to a successful _nal solution within which techniques on feature engineering and aggregation (blending and ensemble learning) play crucial roles. Our team is the _rst prize winner of both tracks of KDD Cup 2011.   Page last modified on Tue May 29 10:22:57 2012.
908,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,"Collaborative Filtering with the Trace Norm: Learning, Bounding, and Transducing","Ohad Shamir, Shai Shalev-Shwartz","19:661-678, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/shamir11a/shamir11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_shamir11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/shamir11a.html,Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. However its theoretical understanding is surprisingly weak and despite previous attempts there are no distribution-free non-trivial learning guarantees currently known. In this paper we bridge this gap by providing such guarantees under mild assumptions which correspond to collaborative filtering as performed in practice. In fact we claim that previous difficulties partially stemmed from a mismatch betweenthe standard learning-theoretic modeling of collaborative filtering and its practical application. Our results also shed some light on the issue of collaborative filtering with bounded models which enforce predictions to lie within a certain range. In particular we provide experimental and theoretical evidence that such models lead to a modest yet significant improvement.
909,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Excess risk bounds for multitask learning with trace norm regularization,"Massimiliano Pontil, Andreas Maurer",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Pontil13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Pontil13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Pontil13.html,"Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be infinite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments."
910,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models,"Jie Wang, Qingyang Li, Sen Yang, Wei Fan, Peter Wonka, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangb14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangb14.html,"Total variation (TV) models are among the most popular and successful tools in signal processing. However, due to the complex nature of the TV term, it is challenging to efficiently compute a solution for large-scale problems. State-of-the-art algorithms that are based on the alternating direction method of multipliers (ADMM) often involve solving large-size linear systems. In this paper, we propose a highly scalable parallel algorithm for TV models that is based on a novel decomposition strategy of the problem domain. As a result, the TV models can be decoupled into a set of small and independent subproblems, which admit closed form solutions. This makes our approach particularly suitable for parallel implementation. Our algorithm is guaranteed to converge to its global minimum. With \(N\) variables and n p processes, the time complexity is \(O(N/(\epsilon n_p))\) to reach an epsilon-optimal solution. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art algorithms, especially in dealing with high-resolution, mega-size images."
911,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Unsupervised Domain Adaptation by Backpropagation,"Yaroslav Ganin, Victor Lempitsky",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ganin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ganin15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ganin15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ganin15.html,"Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of –deep” features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets."
912,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis,"Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu Mei, Ming Zhang",none,http://jmlr.org/proceedings/papers/v32/tang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tang14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tang14.html,"Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDAês behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDAês performance. We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters."
913,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Practical Algorithm for Topic Modeling with Provable Guarantees,"Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, Michael Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/arora13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/arora13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_arora13,http://jmlr.csail.mit.edu/proceedings/papers/v28/arora13.html,"Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster."
914,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Latent Topic Networks: A Versatile Probabilistic Programming Framework for Topic Models,"James Foulds, Shachi Kumar, Lise Getoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/foulds15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/foulds15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_foulds15,http://jmlr.csail.mit.edu/proceedings/papers/v37/foulds15.html,"Topic models have become increasingly prominent text-analytic machine learning tools for research in the social sciences and the humanities. In particular, custom topic models can be developed to answer specific research questions. The design of these models requires a non-trivial amount of effort and expertise, motivating general-purpose topic modeling frameworks. In this paper we introduce latent topic networks, a flexible class of richly structured topic models designed to facilitate applied research. Custom models can straightforwardly be developed in our framework with an intuitive first-order logical probabilistic programming language. Latent topic networks admit scalable training via a parallelizable EM algorithm which leverages ADMM in the M-step. We demonstrate the broad applicability of the models with case studies on modeling influence in citation networks, and U.S. Presidential State of the Union addresses."
915,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Model Selection for Topic Models via Spectral Decomposition,"Dehua Cheng, Xinran He, Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/cheng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/cheng15-supp.zip,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_cheng15,http://jmlr.csail.mit.edu/proceedings/papers/v38/cheng15.html,"Topic models have achieved significant successes in analyzing large-scale text corpus. In practical applications, we are always confronted with the challenge of model selection, i.e., how to appropriately set the number of topics. Following the recent advances in topic models via tensor decomposition, we make a first attempt to provide theoretical analysis on model selection in latent Dirichlet allocation. With mild conditions, we derive the upper bound and lower bound on the number of topics given a text collection of finite size. Experimental results demonstrate that our bounds are correct and tight. Furthermore, using Gaussian mixture model as an example, we show that our methodology can be easily generalized to model selection analysis in other latent models."
916,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Online Latent Dirichlet Allocation with Infinite Vocabulary,"Ke Zhai, Jordan Boyd-Graber",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhai13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhai13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhai13.html,"Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary a priori. This is reasonable in batch settings, but it is not reasonable when data are revealed over time, as is the case with streaming / online algorithms. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and because we only can consider a finite number of words for each truncated topic propose heuristics to dynamically organize, expand, and contract the set of words we consider in our vocabulary truncation. We show our model can successfully incorporate new words as it encounters new terms and that it performs better than online LDA in evaluations of topic quality and classification performance."
917,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data","Zhiyuan Chen, Bing Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenf14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenf14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chenf14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenf14.html,"Topic modeling has been commonly used to discover topics from document collections. However, unsupervised models can generate many incoherent topics. To address this problem, several knowledge-based topic models have been proposed to incorporate prior domain knowledge from the user. This work advances this research much further and shows that without any user input, we can mine the prior knowledge automatically and dynamically from topics already found from a large number of domains. This paper first proposes a novel method to mine such prior knowledge dynamically in the modeling process, and then a new topic model to use the knowledge to guide the model inference. What is also interesting is that this approach offers a novel lifelong learning algorithm for topic discovery, which exploits the big (past) data and knowledge gained from such data for subsequent modeling. Our experimental results using product reviews from 50 domains demonstrate the effectiveness of the proposed approach."
918,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Distributed Topic Modeling with Provable Guarantees,"Weicong Ding, Mohammad Rohban, Prakash Ishwar, Venkatesh Saligrama","JMLR W&CP 33 :167-175, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ding14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/ding14a-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ding14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/ding14a.html,Topic modeling for large-scale distributed web-collections requires distributed techniques that account for both computational and communication costs. We consider topic modeling under the separability assumption and develop novel computationally efficient methods that provably achieve the statistical performance of the state-of-the-art centralized approaches while requiring insignificant communication between the distributed document collections. We achieve tradeoffs between communication and computation without actually transmitting the documents. Our scheme is based on exploiting the geometry of normalized word-word co-occurrence matrix and viewing each row of this matrix as a vector in a high-dimensional space. We relate the solid angle subtended by extreme points of the convex hull of these vectors to topic identities and construct distributed schemes to identify topics.
919,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Missing Information Impediments to Learnability,Loizos Michael,"19:827-830, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/michael11a/michael11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_michael11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/michael11a.html,To what extent is learnability impeded when information is missing in learning instances? We present relevant known results and concrete open problems in the context of a natural extension of the PAC learning model that accounts for arbitrarily missing information.
920,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multi-label Classification via Feature-aware Implicit Label Space Encoding,"Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/linc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/linc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_linc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/linc14.html,"To tackle a multi-label classification problem with many classes, recently label space dimension reduction (LSDR) is proposed. It encodes the original label space to a low-dimensional latent space and uses a decoding process for recovery. In this paper, we propose a novel method termed FaIE to perform LSDR via Feature-aware Implicit label space Encoding. Unlike most previous work, the proposed FaIE makes no assumptions about the encoding process and directly learns a code matrix, i.e. the encoding result of some implicit encoding function, and a linear decoding matrix. To learn both matrices, FaIE jointly maximizes the recoverability of the original label space from the latent space, and the predictability of the latent space from the feature space, thus making itself feature-aware. FaIE can also be specified to learn an explicit encoding function, and extended with kernel tricks to handle non-linear correlations between the feature space and the latent space. Extensive experiments conducted on benchmark datasets well demonstrate its effectiveness."
921,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Distributed Gaussian Processes,"Marc Deisenroth, Jun Wei Ng",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/deisenroth15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_deisenroth15,http://jmlr.csail.mit.edu/proceedings/papers/v37/deisenroth15.html,"To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets."
922,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Single versus Multiple Sorting in All Pairs Similarity Search,"Yasuo Tabei, Takeaki Uno, Masashi Sugiyama, and Koji Tsuda","13:145-160, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/tabei10a/tabei10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_tabei10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/tabei10a.html,To save memory and improve speed vectorial data such as images and signals are often represented as strings of discrete symbols (i.e. sketches). Chariker (2002) proposed a fast approximate method for finding neighbor pairs of strings by sorting and scanning with a small window. This method which we shall call_gsingle sorting_h is applied to locality sensitive codes and prevalently used in speed-demanding web-related applications. To improve on single sorting we propose a novel method that employs blockwise masked sorting. Our method can dramatically reduce the number of candidate pairs which have to be verified by distance calculation in exchange with an increased amount of sorting operations. So it is especially attractive for high dimensional dense data where distance calculation is expensive. Empirical results show the efficiency of our method in comparison to single sorting and recent fast nearest neighbor methods.
923,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Connections in Financial Time Series,"Gartheeban Ganeshapillai, John Guttag, Andrew Lo",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ganeshapillai13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ganeshapillai13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ganeshapillai13.html,"To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only –beat the market,” but also outperform optimal (i.e., minimum variance) portfolios."
924,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,To go deep or wide in learning?,"Gaurav Pandey, Ambedkar Dukkipati","JMLR W&CP 33 :724-732, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/pandey14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_pandey14,http://jmlr.csail.mit.edu/proceedings/papers/v33/pandey14.html,"To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets."
925,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bayesian Logistic Gaussian Process Models for Dynamic Networks,"Daniele Durante, David Dunson","JMLR W&CP 33 :194-201, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/durante14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_durante14,http://jmlr.csail.mit.edu/proceedings/papers/v33/durante14.html,"Time-varying adjacency matrices encoding the presence or absence of a relation among entities are available in many research fields. Motivated by an application to studying dynamic networks among sports teams, we propose a Bayesian nonparametric model. The proposed approach uses a logistic mapping from the probability matrix, encoding link probabilities between each team, to an embedded latent relational space. Within this latent space, we incorporate a dictionary of Gaussian process (GP) latent trajectories characterizing changes over time in each team, while allowing learning of the number of latent dimensions through a specially tailored prior for the GP covariance. The model is provably flexible and borrows strength across the network and over time. We provide simulation experiments and an application to the Italian soccer Championship."
926,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Evolving Cluster Mixed-Membership Blockmodel for Time-Evolving Networks,"Qirong Ho, Le Song, Eric Xing","15:342-350, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ho11b/ho11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ho11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/ho11b.html,Time-evolving networks are a natural presentation for dynamic social and biological interactions. While latent space models are gaining popularity in network modeling and analysis previous works mostly ignore networks with temporal behavior and multi-modal actor roles. Furthermore prior knowledge such as division and grouping of social actors or biological specificity of molecular functions has not been systematically exploited in network modeling. In this paper we develop a network model featuring a state space mixture prior that tracks complex actor latent role changes through time. We provide a fast variational inference algorithm for learning our model and validate it with simulations and held-out likelihood comparisons on real-world time-evolving networks. Finally we demonstrate our model's utility as a network analysis tool by applying it to United States Congress voting data.
927,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Effective Bayesian Modeling of Groups of Related Count Time Series,Nicolas Chapados,none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chapados14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chapados14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chapados14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chapados14.html,"Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning."
928,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Further Optimal Regret Bounds for Thompson Sampling,"Shipra Agrawal, Navin Goyal",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_agrawal13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/agrawal13a.html,"Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have comparable or better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that proves the first near-optimal problem-independent bound of \(O(\sqrt{NT\ln T})\) on the expected regret of this algorithm. Our novel martingale-based analysis techniques are conceptually simple, and easily extend to distributions other than the Beta distribution. For the version of Thompson Sampling that uses Gaussian priors, we prove a problem-independent bound of \(O(\sqrt{NT\ln N})\) on the expected regret, and demonstrate the optimality of this bound by providing a matching lower bound. This lower bound of \(\Omega(\sqrt{NT\ln N})\) is the first lower bound on the performance of a natural version of Thompson Sampling that is away from the general lower bound of \(O(\sqrt{NT})\) for the multi-armed bandit problem. Our near-optimal problem-independent bounds for Thompson Sampling solve a COLT 2012 open problem of Chapelle and Li. Additionally, our techniques simultaneously provide the optimal problem-dependent bound of \((1+\epsilon)\sum_i \frac{\ln T}{d(\mu_i, \mu_1)}+O(\frac{N}{\epsilon^2})\) on the expected regret. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [2012]."
929,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Thompson Sampling for Contextual Bandits with Linear Payoffs,"Shipra Agrawal, Navin Goyal",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/agrawal13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/agrawal13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_agrawal13,http://jmlr.csail.mit.edu/proceedings/papers/v28/agrawal13.html,"Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \(\tilde{O}(\frac{d}{\sqrt{\epsilon}}\sqrt{T^{1+\epsilon}})\) in time \(T\) for any \(\epsilon \in (0,1)\) , where \(d\) is the dimension of each context vector and \(\epsilon\) is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \(\Omega(\sqrt{dT})\) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem with linear payoff functions. Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of other distributions, satisfying certain general conditions."
930,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Thompson Sampling in Switching Environments with Bayesian Online Change Detection,"Joseph Mellor, Jonathan Shapiro",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/mellor13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_mellor13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/mellor13a.html,"Thompson Sampling has recently been shown to achieve the lower bound on regret in the Bernoulli Multi-Armed Bandit setting. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results in 4 artificial environments, and 2 derived from real world data: news click-through and foreign exchange data, comparing them to some other bandit algorithms. In real world data CTS is the most effective."
931,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Sample Complexity Bounds for Differentially Private Learning,"Kamalika Chaudhuri, Daniel Hsu","19:155-186, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/chaudhuri11a/chaudhuri11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_chaudhuri11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/chaudhuri11a.html,This work studies the problem of privacy-preserving classification -- namely learning a classifier from sensitive data while preserving the privacy ofindividuals in the training set.In particular the learning algorithm is required in this problem toguarantee differential privacy a very strong notion of privacy that hasgained significant attention in recent years.A natural question to ask is: what is the sample requirement of a learningalgorithm that guarantees a certain level of privacy and accuracy?We address this question in the context of learning with infinite hypothesis classes whenthe data is drawn from a continuous distribution.We first show that even for very simple hypothesis classes any algorithmthat uses a finite number of examples and guarantees differential privacymust fail to return an accurate classifier for at least some unlabeled datadistributions.This result is unlike the case with either finite hypothesis classes ordiscrete data domains in which distribution-free private learning ispossible as previously shown by \citet{KLNRS08}.We then consider two approaches to differentially private learning that get around this lower bound.The first approach is to use prior knowledge about the unlabeled data distribution in the form of a reference distribution $\U$ chosen independently of the sensitive data. Given such a reference $\U$ we provide an upper bound on the sample requirement that depends (among other things) on a measure of closenessbetween $\U$ and the unlabeled data distribution. Our upper bound appliesto the non-realizable as well as the realizable case. The second approachis to relax the privacy requirement by requiring only label-privacy --namely that the only labels (and not the unlabeled parts of the examples)be considered sensitive information. An upper bound on the samplerequirement of learning with label privacy was shown by \citet{CDKMT06}; inthis work we show a lower bound.
932,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Least Squares Revisited: Scalable Approaches for Multi-class Prediction,"Alekh Agarwal, Sham Kakade, Nikos Karampatziakis, Le Song, Gregory Valiant",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwala14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_agarwala14,http://jmlr.csail.mit.edu/proceedings/papers/v32/agarwala14.html,"This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples \(n\) and the data dimension \(d\) are relatively large. These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we show how to scale our approach to high dimensional datasets, achieving dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies."
933,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Heavy-tailed regression with a generalized median-of-means,"Daniel Hsu, Sivan Sabato",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hsu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsu14.html,"This work proposes a simple and computationally efficient estimator for linear regression, and other smooth and strongly convex loss minimization problems. We prove loss approximation guarantees that hold for general distributions, including those with heavy tails. All prior results only hold for estimators which either assume bounded or subgaussian distributions, require prior knowledge of distributional properties, or are not known to be computationally tractable. In the special case of linear regression with possibly heavy-tailed responses and with bounded and well-conditioned covariates in \(d\) -dimensions, we show that a random sample of size \(\tilde{O}(d\log(1/\delta))\) suffices to obtain a constant factor approximation to the optimal loss with probability \(1-\delta\) , a minimax optimal sample complexity up to log factors. The core technique used in the proposed estimator is a new generalization of the median-of-means estimator to arbitrary metric spaces."
934,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Random Design Analysis of Ridge Regression,"Daniel Hsu, Sham M. Kakade and Tong Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/hsu12/hsu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_hsu12,http://jmlr.csail.mit.edu/proceedings/papers/v23/hsu12.html,"This work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. In particular, the analysis provides sharp results on the ""out-of-sample"" prediction error, as opposed to the ""in-sample"" (fixed design) error. The analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors; neither of which effects are present in the fixed design setting. The proof of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices."
935,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Higher-Order Regret Bounds with Switching Costs,Eyal Gofer,"JMLR W&CP 35 :210-243, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/gofer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_gofer14,http://jmlr.csail.mit.edu/proceedings/papers/v35/gofer14.html,"This work examines online linear optimization with full information and switching costs (SCs) and focuses on regret bounds that depend on properties of the loss sequences. The SCs considered are bounded functions of a pair of decisions, and regret is augmented with the total SC. We show under general conditions that for any normed SC, \(\sigma(\mathbf{x},\mathbf{x}')=\|\mathbf{x}-\mathbf{x}'\|\) , regret cannot be bounded given only a bound \(Q\) on the quadratic variation of losses. With an additional bound \(\Lambda\) on the total length of losses, we prove \(O(\sqrt{Q+\Lambda})\) regret for Regularized Follow the Leader (RFTL). Furthermore, an \(O(\sqrt{Q})\) bound holds for RFTL given a cost \(\|\mathbf{x}-\mathbf{x}'\|^2\) . By generalizing the Shrinking Dartboard algorithm, we also show an expected regret bound for the best expert setting with any SC, given bounds on the total loss of the best expert and the quadratic variation of any expert. As SCs vanish, all our bounds depend purely on quadratic variation. We apply our results to pricing options in an arbitrage-free market with proportional transaction costs. In particular, we upper bound the price of –at the money” call options, assuming bounds on the quadratic variation of a stock price and the minimum of summed gains and summed losses."
936,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Linear Bayesian Networks with Latent Variables,"Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham Kakade",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/anandkumar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_anandkumar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/anandkumar13.html,"This work considers the problem of learning linear Bayesian networks when some of the variables are unobserved. Identifiability and efficient recovery from low-order observable moments are established under a novel graphical constraint. The constraint concerns the expansion properties of the underlying directed acyclic graph (DAG) between observed and unobserved variables in the network, and it is satisfied by many natural families of DAGs that include multi-level DAGs, DAGs with effective depth one, as well as certain families of polytrees."
937,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Class Proportion Estimation with Application to Multiclass Anomaly Rejection,"Tyler Sanderson, Clayton Scott","JMLR W&CP 33 :850-858, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/sanderson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/sanderson14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_sanderson14,http://jmlr.csail.mit.edu/proceedings/papers/v33/sanderson14.html,"This work addresses two classification problems that fall under the heading of domain adaptation, wherein the distributions of training and testing examples differ. The first problem studied is that of class proportion estimation, which is the problem of estimating the class proportions in an unlabeled testing data set given labeled examples of each class. Compared to previous work on this problem, our approach has the novel feature that it does not require labeled training data from one of the classes. This property allows us to address the second domain adaptation problem, namely, multiclass anomaly rejection. Here, the goal is to design a classifier that has the option of assigning a –reject” label, indicating that the instance did not arise from a class present in the training data. We establish consistent learning strategies for both of these domain adaptation problems, which to our knowledge are the first of their kind. We also implement the class proportion estimation technique and demonstrate its performance on several benchmark data sets."
938,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Feature Extraction for Outlier Detection in High-Dimensional Spaces,Hoang Vu Nguyen and Vivekanand Gopalkrishnan,"10:66-75, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/nguyen10a/nguyen10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_nguyen10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/nguyen10a.html,This work addresses the problem of feature extraction for boosting the performance of outlier detectors in high-dimensional spaces. Recent years have observed the prominence of multidimensional data on which traditional detection techniques usually fail to work as expected due to the curse of dimensionality. This paper introduces an efficient feature extraction method which brings nontrivial improvements in detection accuracy when applied on two popular detection techniques. Experiments carried out on real datasets demonstrate the feasibility of feature extraction in outlier detection.
939,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Exploiting the Limits of Structure Learning via Inherent Symmetry,"Peng He, Changshui Zhang","JMLR W&CP 33 :328-337, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/he14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_he14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/he14b.html,"This theoretical paper is concerned with the structure learning limit for Gaussian Markov random fields from i.i.d. samples. The common strategy is applying the Fano method to a family of restricted ensembles. The efficiency of this method, however, depends crucially on selected restricted ensembles. To break through this limitation, we analyze the whole graph ensemble from high-dimensional geometric and group-theoretical perspectives. The key ingredients of our approach are the geometric property of concentration matrices and the invariance of orthogonal group actions on the symmetric Kullback-Leibler divergence. We then establish the connection of the learning limit and eigenvalues of concentration matrices, which leads to a sharper structure learning limit. To our best knowledge, this is the first paper to consider the structure learning problem via inherent symmetries of the whole ensemble. Finally, our approach can be applicable to other graphical structure learning problems."
940,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Non-Asymptotic Analysis of Relational Learning with One Network,"Peng He, Changshui Zhang","JMLR W&CP 33 :320-327, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/he14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_he14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/he14a.html,"This theoretical paper is concerned with a rigorous non-asymptotic analysis of relational learning applied to a single network. Under suitable and intuitive conditions on features and clique dependencies over the network, we present the first probably approximately correct (PAC) bound for maximum likelihood estimation (MLE). To our best knowledge, this is the first sample complexity result of this problem. We propose a novel combinational approach to analyze complex dependencies of relational data, which is crucial to our non-asymptotic analysis. The consistency of MLE under our conditions is also proved as the consequence of our sample complexity bound. Finally, our combinational method for analyzing dependent data can be easily generalized to treat other generalized maximum likelihood estimators for relational learning."
941,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Unsupervised Feature Selection for Pattern Discovery in Seismic Wavefields,"Andreas K_hler, Matthias Ohrnberger, Carsten Riggelsen, Frank Scherbaum","4:106-121, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/koehler08a/koehler08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_koehler08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/koehler08a.html,This study presents an unsupervised feature selection approach for the discovery of significant patterns in seismic wavefields. We iteratively reduce the number of features generated from seismic time series by first considering significance of individual features. Significance testing is done by assessing the randomness of the time series with the Wald-Wolfowitz runs test and by comparing observed and theoretical variability of features. In a second step the in-between feature dependencies are assessed based on correlation hunting in feature subsets using Self-Organizing Maps (SOMs). We show the improved discriminative power of our procedure compared to manually selected feature subsets by cross-validation applied to synthetic seismic wavefield data. Furthermore we apply the method to real-world data with the aim to define suitable features for earthquake detection and seismic phase classification in seismic recordings.
942,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Assessment of Cow's Body Condition Score Through Statistical Shape Analysis and Regression Machines,"Sebastiano Battiato, Giovanni Maria Farinella, Giuseppe Claudio Guarnera, Giovanni Puglisi, Giuseppe Azzaro and Margherita Caccamo","11:66-73, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/battiato10a/battiato10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_battiato10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/battiato10a.html,"This study explores the feasibility of estimating the Body Condition Score (BCS) of cows from digital images by employing statistical shape analysis and regression machines. The shapes of body cows are described through a number of variations from a unique average shape. Specifically, Kernel Principal Component Analysis is used to determine the components describing the many ways in which the body shape of different cows tend to deform from the average shape. This description is used for automatic estimation of BCS through regression approach. The proposed method has been tested on a new benchmark dataset available through the Internet. Experimental results confirm the effectiveness of the proposed technique that outperforms the state-of-the-art approaches proposed in the context of dairy cattle research."
943,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Margin based Transductive Graph Cuts using Linear Programming,"K. Pelckmans, J. Shawe-Taylor, J.A.K. Suykens, B. De Moor","2:363-370, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/pelckmans07a/pelckmans07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_pelckmans07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/pelckmans07a.html,This paper studies the problem of inferring a partition (or a graph cut) of an undirected deterministic graph where the labels of some nodes are observed - thereby bridging a gap between graph theory and probabilistic inference techniques. Given a weighted graph we focus on the rules of weighted neighbors to predict the label of a particular node. A maximum margin and maximal average margin based argument is used to prove a generalization bound and is subsequently related to the classical MINCUT approach. From a practical perspective a simple and intuitive but efficient convex formulation is constructed. This scheme can readily be implemented as a linear program which scales well till a few thousands of (labeled or unlabeled) data-points. The extremal case is studied where one observes only a single label and this setting is related to the task of unsupervised clustering.
944,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Rare Probability Estimation under Regularly Varying Heavy Tails,Mesrob I. Ohannessian and Munther A. Dahleh,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/ohannessian12/ohannessian12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_ohannessian12,http://jmlr.csail.mit.edu/proceedings/papers/v23/ohannessian12.html,"This paper studies the problem of estimating the probability of symbols that have occurred very rarely, in samples drawn independently from an unknown, possibly infinite, discrete distribution. In particular, we study the multiplicative consistency of estimators, defined as the ratio of the estimate to the true quantity converging to one. We first show that the classical Good-Turing estimator is not universally consistent in this sense, despite enjoying favorable additive properties. We then use Karamata's theory of regular variation to prove that regularly varying heavy tails are sufficient for consistency. At the core of this result is a multiplicative concentration that we establish both by extending the McAllester-Ortiz additive concentration for the missing mass to all rare probabilities and by exploiting regular variation. We also derive a family of estimators which, in addition to being consistent, address some of the shortcomings of the Good-Turing estimator. For example, they perform smoothing implicitly and have the absolute discounting structure of many heuristic algorithms. This also establishes a discrete parallel to extreme value theory, and many of the techniques therein can be adapted to the framework that we set forth."
945,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Computational Lower Bounds for Community Detection on Random Graphs,"Bruce Hajek, Yihong Wu, Jiaming Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Hajek15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Hajek15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Hajek15.html,"This paper studies the problem of detecting the presence of a small dense community planted in a large Erds-R_nyi random graph \(\calG(N,q)\) , where the edge probability within the community exceeds \(q\) by a constant factor. Assuming the hardness of the planted clique detection problem, we show that the computational complexity of detecting the community exhibits the following phase transition phenomenon: As the graph size \(N\) grows and the graph becomes sparser according to \(q=N^{-\alpha}\) , there exists a critical value of \(\alpha = \frac{2}{3}\) , below which there exists a computationally intensive procedure that can detect far smaller communities than any computationally efficient procedure, and above which a linear-time procedure is statistically optimal. The results also lead to the average-case hardness results for recovering the dense community and approximating the densest \(K\) -subgraph."
946,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Toward Minimax Off-policy Value Estimation,"Lihong Li, Remi Munos, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15b-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15b.html,"This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a finite-time minimax risk lower bound, and analyze the risk of three standard estimators. It is shown that in a large class of settings the so-called regression estimator is minimax optimal up to a constant that depends on the number of actions, while the other two can be arbitrarily worse even in the limit of infinitely many data points, despite their empirical success and popularity. The performance of these estimators are studied in synthetic and real problems; illustrating the nontriviality of this simple task. Finally the results are extended to the problem of off-policy evaluation in contextual bandits and fixed-horizon Markov decision processes."
947,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Optimal and Robust Price Experimentation: Learning by Lottery,"Christopher Dance, Onno Zoeter","15:242-250, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/dance11a/dance11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_dance11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/dance11a.html,"This paper studies optimal price learning for one or more items. We introduce the Schr\""""odinger price experiment (SPE) which superimposes classical price experiments using lotteries and thereby extracts more information from each customer interaction. If buyers are perfectly rational we show that there exist SPEs that in the limit of infinite superposition learn optimally \emph{and} exploit optimally. We refer to the new resulting mechanism as the hopeful mechanism (HM) since although it is incentive compatible buyers can deviate with extreme consequences for the seller at very little cost to themselves. For real-world settings we propose a robust version of the approach which takes the form of a Markov decision process where the actions are functions. We provide approximate policies motivated by the best of sampled set (BOSS) algorithm coupled with approximate Bayesian inference. Numerical studies show that the proposed method significantly increases seller revenue compared to classical price experimentation even for the single-item case."
948,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Consistency of Nearest Neighbor Classification under Selective Sampling,Sanjoy Dasgupta,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/dasgupta12/dasgupta12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_dasgupta12,http://jmlr.csail.mit.edu/proceedings/papers/v23/dasgupta12.html,"This paper studies nearest neighbor classification in a model where unlabeled data points arrive in a stream, and the learner decides, for each one, whether to ask for its label. Are there generic ways to augment or modify any selective sampling strategy so as to ensure the consistency of the resulting nearest neighbor classifier?"
949,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Sample Compression for Multi-label Concept Classes,"Rahim Samei, Pavel Semukhin, Boting Yang, Sandra Zilles","JMLR W&CP 35 :371-393, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/samei14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_samei14,http://jmlr.csail.mit.edu/proceedings/papers/v35/samei14.html,"This paper studies labeled sample compression for multi-label concept classes. For a specific extension of the notion of VC-dimension to multi-label classes, we prove that every maximum multi-label class of dimension \(d\) has a sample compression scheme in which every sample is compressed to a subset of size at most \(d\) . We further show that every multi-label class of dimension \(1\) has a sample compression scheme using only sets of size at most \(1\) . As opposed to the binary case, the latter result is not immediately implied by the former, since there are multi-label concept classes of dimension \(1\) that are not contained in maximum classes of dimension \(1\) ."
950,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,"On Sparse, Spectral and Other Parameterizations of Binary Probabilistic Models","David Buchman, Mark Schmidt, Shakir Mohamed, David Poole, Nando De Freitas",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/buchman12/buchman12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_buchman12,http://jmlr.csail.mit.edu/proceedings/papers/v22/buchman12.html,This paper studies issues relating to the parameterization of probability distributions over binary data sets. Several such parameterizations of models for binary data are known including the Ising generalized Ising canonical and full parameterizations. We also discuss a parameterization that we call the ``spectral parameterization'' which has received significantly less coverage in existing literature. We provide this parameterization with a spectral interpretation by casting log-linear models in terms of orthogonal Walsh-Hadamard harmonic expansions. Using various standard and group sparse regularizers for structural learning we provide a comprehensive theoretical and empirical comparison of these parameterizations. We show that the spectral parameterization along with the canonical has the best performance and sparsity levels while the spectral does not depend on any particular reference state. The spectral interpretation also provides a new starting point for analyzing the statistics of binary data sets; we measure the magnitude of higher order interactions in the underlying distributions for several data sets.
951,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Detecting Accounting Frauds in Publicly Traded U.S. Firms: A Machine Learning Approach,"Bin Li, Julia Yu, Jie Zhang, Bin Ke",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Li15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Li15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Li15.html,"This paper studies how machine learning techniques can facilitate the detection of accounting fraud in publicly traded US firms. Existing studies often mimic human experts and employ the financial or nonfinancial ratios as the features for their systems. We depart from these studies by adopting raw accounting variables, which are directly available from a firmês financial statement and thereby can be easily applied to new firms at low cost. Further, we collected the most complete fraud dataset of US publicly traded firms and labeled the fraud and non-fraud firm-years. One key issue of the dataset is that the data is extremely imbalanced, in which the fraud firm-years are often less than one percent. Without re-sampling the data, we further propose to tackle the imbalance issue by adopting the techniques of imbalanced learning. In particular, we employ the linear and nonlinear Biased Penalty Support Vector Machine and the Ensemble Methods, both of which have been proved to successfully handle the imbalance issue in the machine learning literatures. We finally evaluate our approach by conducting extensive empirical studies. Empirical results show that the proposed schema can achieve much better performance, in terms of balanced accuracy, than the state of the art. Besides the performance, our approaches can also compute very fast, which further supports their practical deployment."
952,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Active Learning for Undirected Graphical Model Selection,"Divyanshu Vats, Robert Nowak, Richard Baraniuk","JMLR W&CP 33 :958-967, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/vats14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/vats14b-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_vats14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/vats14b.html,"This paper studies graphical model selection, i.e., the problem of estimating a graph of statistical relationships among a collection of random variables. Conventional graphical model selection algorithms are passive, i.e., they require all the measurements to have been collected before processing begins. We propose an active learning algorithm that uses junction tree representations to adapt future measurements based on the information gathered from prior measurements. We prove that, under certain conditions, our active learning algorithm requires fewer scalar measurements than any passive algorithm to reliably estimate a graph. A range of numerical results validate our theory and demonstrates the benefits of active learning."
953,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Large-margin Weakly Supervised Dimensionality Reduction,"Chang Xu, Dacheng Tao, Chao Xu, Yong Rui",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/xu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/xu14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_xu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/xu14.html,"This paper studies dimensionality reduction in a weakly supervised setting, in which the preference relationship between examples is indicated by weak cues. A novel framework is proposed that integrates two aspects of the large margin principle (angle and distance), which simultaneously encourage angle consistency between preference pairs and maximize the distance between examples in preference pairs. Two specific algorithms are developed: an alternating direction method to learn a linear transformation matrix and a gradient boosting technique to optimize a non-linear transformation directly in the function space. Theoretical analysis demonstrates that the proposed large margin optimization criteria can strengthen and improve the robustness and generalization performance of preference learning algorithms on the obtained low-dimensional subspace. Experimental results on real-world datasets demonstrate the significance of studying dimensionality reduction in the weakly supervised setting and the effectiveness of the proposed framework."
954,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Average Case Analysis of High-Dimensional Block-Sparse Recovery and Regression for Arbitrary Designs,"Waheed Bajwa, Marco Duarte, Robert Calderbank","JMLR W&CP 33 :57-67, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/bajwa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_bajwa14,http://jmlr.csail.mit.edu/proceedings/papers/v33/bajwa14.html,"This paper studies conditions for high-dimensional inference when the set of observations is given by a linear combination of a small number of groups of columns of a design matrix, termed the –block-sparse” case. In this regard, it first specifies conditions on the design matrix under which most of its block submatrices are well conditioned. It then leverages this result for average-case analysis of high-dimensional block-sparse recovery and regression. In contrast to earlier works, the results of this paper are fundamentally different because (i) they provide conditions on arbitrary designs that can be explicitly computed in polynomial time, (ii) the provided conditions translate into near-optimal scaling of the number of observations with the number of active blocks of the design matrix, and (iii) they suggest that the spectral norm, rather than the column/block coherences, of the design matrix fundamentally limits the performance of computational methods in high-dimensional settings."
955,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Central Limit Theorems for Conditional Markov Chains,"Mathieu Sinn, Bei Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/sinn13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/sinn13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_sinn13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/sinn13a.html,"This paper studies Central Limit Theorems for real-valued functionals of Conditional Markov Chains. Using a classical result by Dobrushin (1956) for non-stationary Markov chains, a conditional Central Limit Theorem for fixed sequences of observations is established. The asymptotic variance can be estimated by resampling the latent states conditional on the observations. If the conditional means themselves are asymptotically normally distributed, an unconditional Central Limit Theorem can be obtained. The methodology is used to construct a statistical hypothesis test which is applied to synthetically generated environmental data."
956,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Kernel Extraction via Voted Risk Minimization,"Corinna Cortes, Prasoon Goyal, Vitaly Kuznetsov, Mehryar Mohri",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/cortes15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_cortes15a,http://jmlr.csail.mit.edu/proceedings/papers/v44/cortes15a.html,"This paper studies a new framework for learning a predictor in the presence of multiple kernel functions where the learner selects or extracts several kernel functions from potentially complex families and finds an accurate predictor defined in terms of these functions. We present an algorithm, Voted Kernel Regularization, that provides the flexibility of using very complex kernel functions such as predictors based on high-degree polynomial kernels or narrow Gaussian kernels, while benefitting from strong learning guarantees. We show that our algorithm benefits from strong learning guarantees suggesting a new regularization penalty depending on the Rademacher complexities of the families of kernel functions used. Our algorithm admits several other favorable properties: its optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements. We report the results of some preliminary experiments comparing the performance of our algorithm to several baselines."
957,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Recursive Karcher Expectation Estimators And Geometric Law of Large Numbers,"Jeffrey Ho, Guang Cheng, Hesamoddin Salehian, Baba Vemuri",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/ho13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_ho13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/ho13a.html,"This paper studies a form of law of large numbers on Pn, the space of nxn symmetric positive-definite matrices equipped with Fisher-Rao metric. Specifically, we propose a recursive algorithm for estimating the Karcher expectation of an arbitrary distribution defined on Pn, and we show that the estimates computed by the recursive algorithm asymptotically converge in probability to the correct Karcher expectation. The steps in the recursive algorithm mainly consist of making appropriate moves on geodesics in Pn, and the algorithm is simple to implement and it offers a tremendous gain in computation time of several orders in magnitude over existing non-recursive algorithms. We elucidate the connection between the more familiar law of large numbers for real-valued random variables and the asymptotic convergence of the proposed recursive algorithm, and our result provides an example of a new form of law of large numbers for random variables taking values in a Riemannian manifold. From the practical side, the computation of the mean of a collection of symmetric positive-definite (SPD) matrices is a fundamental ingredient in many algorithms in machine learning, computer vision and medical imaging applications. We report an experiment using the proposed recursive algorithm for K-means clustering, demonstrating the algorithmês efficiency, accuracy and stability."
958,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Explicit Link Between Periodic Covariance Functions and State Space Models,"Arno Solin, Simo S_rkk_","JMLR W&CP 33 :904-912, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/solin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_solin14,http://jmlr.csail.mit.edu/proceedings/papers/v33/solin14.html,"This paper shows how periodic covariance functions in Gaussian process regression can be reformulated as state space models, which can be solved with classical Kalman filtering theory. This reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The representation is based on expanding periodic covariance functions into a series of stochastic resonators. The explicit representation of the canonical periodic covariance function is written out and the expansion is shown to uniformly converge to the exact covariance function with a known convergence rate. The framework is generalized to quasi-periodic covariance functions by introducing damping terms in the system and applied to two sets of real data. The approach could be easily extended to non-stationary and spatio-temporal variants."
959,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Online PCA with Spectral Bounds,"Zohar Karnin, Edo Liberty",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Karnin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Karnin15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Karnin15.html,"This paper revisits the online PCA problem. Given a stream of \(n\) vectors \(x_t \in \mathbb{R}^d\) (columns of \(X\) ) the algorithm must output \(y_t \in \mathbb{R}^\ell\) (columns of \(Y\) ) before receiving \(x_{t+1}\) . The goal of online PCA is to simultaneously minimize the target dimension \(\ell\) and the error \(\|X - (XY^{{\scriptstyle{ \textrm +}}})Y\|^2\) . We describe two simple and deterministic algorithms. The first, receives a parameter \(\Delta\) and guarantees that \(\|X - (XY^{{\scriptstyle{ \textrm +}}})Y\|^2\) is not significantly larger than \(\Delta\) . It requires a target dimension of \(\ell = O(k/\epsilon)\) for any \(k,\epsilon\) such that \(\Delta \ge \epsilon\sigma_1^2 + \sigma_{k+1}^2\) , with \(\sigma_i\) being the \(i\) êth singular value of \(X\) . The second receives \(k\) and \(\epsilon\) and guarantees that \(\|X - (XY^{{\scriptstyle{ \textrm +}}})Y\|^2 \le \epsilon\sigma_1^2 + \sigma_{k+1}^2\) . It requires a target dimension of \(O( k\log n/\epsilon^2)\) . Different models and algorithms for Online PCA were considered in the past. This is the first that achieves a bound on the spectral norm of the residual matrix."
960,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Active Automata Learning: From DFAs to Interface Programs and Beyond,Bernhard Steffen and Falk Howar and Malte Isberner,"21:195-209, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/steffen12a/steffen12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_steffen12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/steffen12a.html,This paper reviews the development of active learning in the last decade under the perspective of treating of data a major source of undecidability and therefore a key problem to achieve practicality. Starting with the first case studies in which data was completely disregarded we revisit different steps towards dealing with data explicitly in active learning: We discuss Mealy Machines as a model for systems with (data) output automated alphabet abstraction refinement as a two-dimensional extension of the partition-refinement based approach of active learning for inferring not only states but also optimal alphabet abstractions and Register Mealy Machines which can be regarded as programs restricted to data-independent data processing as it is typical for protocols or interface programs. We are convinced that this development has the potential to transform active automata learning into a technology of high practical importance.
961,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Collaborative Filtering Ensemble for Ranking,M. Jahrer & A. T _ scher,"18:153_167, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/jahrer12b/jahrer12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_jahrer12b,http://jmlr.csail.mit.edu/proceedings/papers/v18/jahrer12b.html,This paper provides the solution of the team ñcommendoî on the Track2 dataset of the KDD Cup 2011 Dror et al.. Yahoo Labs provides a snapshot of their music-rating database as dataset for the competition consisting of approximately 62 million ratings from 250k users on 300k items. The dataset includes hierachical information about the items. The goal of the competition is to distinguish beteen ñHigh ratedî and ñNot ratedî items of a user. The rating scale is discrete and ranges from 0 to 100 while a ñHighî rating is a rating _ 80. The error measure is the percent of false rated tracks over all users known as the fractions of misclassi_cations. The task is to minimize this error rate hence the ranking should be optimized. Our _nal submission is a blend of di_erent collaborative _ltering algorithms enhanced with basic statistics. The algorithms are trained consecutively and they are blended together with a neural network. Each of the algorithms optimizes a rank error measure.   Page last modified on Tue May 29 10:23:22 2012.
962,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Collaborative Filtering Ensemble,M. Jahrer & A. T _ scher,"18:61_74, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/jahrer12a/jahrer12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_jahrer12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/jahrer12a.html,This paper provides the solution of the team ñcommendoî on the Track1 dataset of the KDD Cup 2011 Dror et al.. Yahoo Labs provides a snapshot of their music-rating database as dataset for the competition. We get approximately 260 million ratings from 1 million users on 600k items. Timestamp and taxonomy information are added to the ratings. The goal of the competition was to predict unknown ratings on a testset with RMSE as error measure. Our _nal submission is a blend of di_erent collaborative _ltering algorithms. The algorithms are trained consecutively and they are blended together with a neural network.   Page last modified on Tue May 29 10:23:01 2012.
963,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Testing for Membership to the IFRA and the NBU Classes of Distributions,"Radhendushka Srivastava, Ping Li, Debasis Sengupta",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/srivastava12/srivastava12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_srivastava12,http://jmlr.csail.mit.edu/proceedings/papers/v22/srivastava12.html,This paper provides test procedures to determine whether the probability distribution underlying a set of non-negative valued samples belongs to the Increasing Failure Rate Average (IFRA) class or the New Better than Used (NBU) class. Membership of a distribution to one of these classes is known to have implications which are important in reliability queuing theory game theory and other disciplines. Our proposed test is based on the Kolmogorov-Smirnov distance between an empirical cumulative hazard function and its best approximation from the class of distributions constituting the null hypothesis. It turns out that the least favorable distribution which produces the largest probability of Type I error of each of the tests is the exponential distribution. This fact is used to produce an appropriate cut-off or p-value. Monte Carlo simulations are conducted to check small sample size (i.e. significance) and power of the test. Usefulness of the test is illustrated through the analysis of a set of monthly family expenditure data collected by the National Sample Survey Organization of the Government of India.
964,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games,"Julien Perolat, Bruno Scherrer, Bilal Piot, Olivier Pietquin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/perolat15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/perolat15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_perolat15,http://jmlr.csail.mit.edu/proceedings/papers/v37/perolat15.html,"This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in \(L_p\) -norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is \(\frac{2\gamma}{(1 - \gamma)^2} \epsilon + \frac{1}{(1 - \gamma)^2}\epsilon'\) -optimal, where \(\epsilon\) is the value function approximation error and \(\epsilon'\) is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI- \(Q\) ) to solve infinite horizon \(\gamma\) -discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted- \(Q\) algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI- \(Q\) on a simultaneous two-player game, namely Alesia."
965,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Convex Risk Minimization and Conditional Probability Estimation,"Matus Telgarsky, Miroslav Dud Í k, Robert Schapire",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Telgarsky15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Telgarsky15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Telgarsky15.html,"This paper proves, in very general settings, that convex risk minimization is a procedure to select a unique conditional probability model determined by the classification problem. Unlike most previous work, we give results that are general enough to include cases in which no minimum exists, as occurs typically, for instance, with standard boosting algorithms. Concretely, we first show that any sequence of predictors minimizing convex risk over the source distribution will converge to this unique model when the class of predictors is linear (but potentially of infinite dimension). Secondly, we show the same result holds for empirical risk minimization whenever this class of predictors is finite dimensional, where the essential technical contribution is a norm-free generalization bound."
966,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Von Mises-Fisher Clustering Models,"Siddharth Gopal, Yiming Yang",none,http://jmlr.org/proceedings/papers/v32/gopal14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gopal14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gopal14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gopal14.html,"This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on Von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include a) A Bayesian formulation of vMF mixture that enables information sharing among clusters, b) a Hierarchical vMF mixture that provides multi-scale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data. For posterior inference, we develop fast variational methods as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation."
967,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust Inverse Covariance Estimation under Noisy Measurements,"Jun-Kun Wang, Shou-de Lin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangf14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangf14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangf14.html,"This paper proposes a robust method to estimate the inverse covariance under noisy measurements. The method is based on the estimation of each column in the inverse covariance matrix independently via robust regression, which enables parallelization. Different from previous linear programming based methods that cannot guarantee a positive semi-definite covariance matrix, our method adjusts the learned matrix to satisfy this condition, which further facilitates the tasks of forecasting future values. Experiments on time series prediction and classification under noisy condition demonstrate the effectiveness of the approach."
968,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Rectangular Tiling Process,"Masahiro Nakano, Katsuhiko Ishiguro, Akisato Kimura, Takeshi Yamada, Naonori Ueda",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/nakano14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/nakano14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_nakano14,http://jmlr.csail.mit.edu/proceedings/papers/v32/nakano14.html,"This paper proposes a novel stochastic process that represents the arbitrary rectangular partitioning of an infinite-dimensional matrix as the conditional projective limit. Rectangular partitioning is used in relational data analysis, and is classified into three types: regular grid, hierarchical, and arbitrary. Conventionally, a variety of probabilistic models have been advanced for the first two, including the product of Chinese restaurant processes and the Mondrian process. However, existing models for arbitrary partitioning are too complicated to permit the analysis of the statistical behaviors of models, which places very severe capability limits on relational data analysis. In this paper, we propose a new probabilistic model of arbitrary partitioning called the rectangular tiling process (RTP). Our model has a sound mathematical base in projective systems and infinite extension of conditional probabilities, and is capable of representing partitions of infinite elements as found in ordinary Bayesian nonparametric models."
969,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Exploiting tree-based variable importances to selectively identify relevant variables,"V_n Anh Huynh-Thu, Louis Wehenkel, Pierre Geurts","4:60-73, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/huynhthu08a/huynhthu08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_huynhthu08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/huynhthu08a.html,This paper proposes a novel statistical procedure based on permutation tests for extracting a subset of truly relevant variables from multivariate importance rankings derived from tree-based supervised learning methods. It shows also that the direct extension of the classical approach based on permutation tests for estimating false discovery rates of univariate variable scoring procedures does not extend very well to the case of multivariate tree-based importance measures.
970,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Confidence Weighted Mean Reversion Strategy for On-Line Portfolio Selection,"Bin Li, Steven C.H. Hoi, Peilin Zhao, Vivekanand Gopalkrishnan","15:434-442, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/li11b/li11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_li11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/li11b.html,This paper proposes a novel on-line portfolio selection strategy named ñConfidence Weighted Mean Reversionî (CWMR). Inspired by the mean reversion principle and the confidence weighted online learning technique CWMR models a portfolio vector as Gaussian distribution and sequentially updates the distribution by following the mean reversion trading principle. The CWMR strategy is able to effectively exploit the power of mean reversion for on-line portfolio selection. Extensive experiments on various real markets demonstrate the effectiveness of our strategy in comparison to the state of the art.
971,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficiently Enforcing Diversity in Multi-Output Structured Prediction,"Abner Guzman-Rivera, Pushmeet Kohli, Dhruv Batra, Rob Rutenbar","JMLR W&CP 33 :284-292, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/guzman-rivera14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_guzman-rivera14,http://jmlr.csail.mit.edu/proceedings/papers/v33/guzman-rivera14.html,"This paper proposes a novel method for efficiently generating multiple diverse predictions for structured prediction problems. Existing methods like SDPPs or DivMBest work by making a series of predictions where each prediction is made after considering the predictions that came before it. Such approaches are inherently sequential and computationally expensive. In contrast, our method, Diverse Multiple Choice Learning, learns a set of models to make multiple independent, yet diverse, predictions at testtime. We achieve this by including a diversity encouraging term in the loss function used for training the models. This approach encourages diversity in the predictions while preserving computational efficiency at test-time. Experimental results on a number of challenging problems show that our method learns models that not only predict more diverse results than competing methods, but are also able to generalize better and produce results with high test accuracy."
972,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Factorized Asymptotic Bayesian Inference for Mixture Modeling,"Ryohei Fujimaki, Satoshi Morinaga",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/fujimaki12/fujimaki12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_fujimaki12,http://jmlr.csail.mit.edu/proceedings/papers/v22/fujimaki12.html,This paper proposes a novel Bayesian approximation inference method for mixture modeling. Our key idea is to factorize marginal log-likelihood using a variational distribution over latent variables. An asymptotic approximation a factorized information criterion (FIC) is obtained by applying the Laplace method to each of the factorized components. In order to evaluate FIC we propose factorized asymptotic Bayesian inference (FAB) which maximizes an asymptotically-consistent lower bound of FIC. FIC and FAB have several desirable properties: 1) asymptotic consistency with the marginal log-likelihood 2) automatic component selection on the basis of an intrinsic shrinkage mechanism and 3) parameter identifiability in mixture modeling. Experimental results show that FAB outperforms state-of-the-art VB methods.
973,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Adaptive MCMC with Bayesian Optimization,"Nimalan Mahendran, Ziyu Wang, Firas Hamze, Nando De Freitas",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/mahendran12/mahendran12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_mahendran12,http://jmlr.csail.mit.edu/proceedings/papers/v22/mahendran12.html,This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained discrete and densely connected probabilistic graphical models where for each variation of the problem one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.
974,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem,"Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zoghi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zoghi14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zoghi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zoghi14.html,"This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a sharp finite-time regret bound of order O(K log t) on a very general class of dueling bandit problems that matches a lower bound proven in (Yue et al., 2012). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art."
975,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Structured Denoising Autoencoder for Fault Detection and Analysis,"Takaaki Tagawa, Yukihiro Tadokoro, Takehisa Yairi",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/tagawa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_tagawa14,http://jmlr.csail.mit.edu/proceedings/papers/v39/tagawa14.html,"This paper proposes a new fault detection and analysis approach which can leverage incomplete prior information. Conventional data-driven approaches suffer from the problem of overfitting and result in high rates of false positives, and model-driven approaches suffer from a lack of specific information about complex systems. We overcome these problems by modifying the denoising autoencoder (DA), a data-driven method, to form a new approach, called the structured denoising autoencoder (SDA), which can utilize incomplete prior information. The SDA does not require specific information and can perform well without overfitting. In particular, an empirical analysis with synthetic data revealed that the SDA performs better than the DA even when there is partially incorrect or abstract information. An evaluation using real data from moving cars also showed that the SDA with incomplete knowledge outperformed conventional methods. Surprisingly, the SDA results were better even though the parameters of the conventional methods were tuned using faulty data, which are normally unknown. In addition, the SDA fault analysis was able to extract the true causes of the changes within the faulty data; the other methods were unable to do this. Thus, only our proposed method can explain why the faults occurred."
976,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup,"Yufei Ding, Yue Zhao, Xipeng Shen, Madanlal Musuvathi, Todd Mytkowicz",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ding15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ding15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ding15.html,"This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performanceãplus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means doesãmakes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance."
977,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Supervised Neural Network Structure Recovery,"Ildefons Magrans de Abril, Ann Now_",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/abril15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_abril15,http://jmlr.csail.mit.edu/proceedings/papers/v46/abril15.html,This paper presents our solution to the European Conference of Machine Learning Neural Connectomics Discovery Challenge. The challenge goal was to improve the performance of existing methods for recovering the neural network structure given the time series of neural activities. We propose to approximate a function able to combine several connectivity indicators between neuron pairs where each indicator is the result of running a feature engineering pipeline optimized for a particular noise level and firing synchronization rate among neurons. We proved the suitability of our solution by improving the state of the art prediction performance more than 6% and by obtaining the third best score on the test dataset out of 144 teams.
978,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Consistent Multiclass Algorithms for Complex Performance Measures,"Harikrishna Narasimhan, Harish Ramaswamy, Aadirupa Saha, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhanb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhanb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_narasimhanb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhanb15.html,"This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro \(F_1\) measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratio-of-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf."
979,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Hybrid Recommendation Models for Binary User Preference Prediction Problem,"S. Lai, Y. Liu, H. Gu, L. Xu, K. Liu, S. Xiang, J. Zhao, R. Diao, L. Xiang, H. Li & D. Wang","18:137_151, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/lai12a/lai12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_lai12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/lai12a.html,This paper presents detailed information of our solutions to the task 2 of KDD Cup 2011. The task 2 is called binary user preference prediction problem in the paper because it aims at separating tracks rated highly by speci_c users from tracks not rated by them and the solutions of this task can be easily applied to binary user behavior data. In the contest we _rstly implemented many di_erent models including neighborhood-based models latent factor models content-based models etc. Then linear combination is used to combine di_erent models together. Finally we used robust post-processing to further re_ne the special user-item pairs. The _nal error rate is 2.4808% which placed number 2 in the Leaderboard.   Page last modified on Tue May 29 10:23:19 2012.
980,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,An inclusion optimal algorithm for chain graph structure learning,"Jose Pe_a, Dag Sonntag, Jens Nielsen","JMLR W&CP 33 :778-786, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/pena14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/pena14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_pena14,http://jmlr.csail.mit.edu/proceedings/papers/v33/pena14.html,"This paper presents and proves an extension of Meekês conjecture to chain graphs under the Lauritzen-Wermuth-Frydenberg interpretation. The proof of the conjecture leads to the development of a structure learning algorithm that finds an inclusion optimal chain graph for any given probability distribution satisfying the composition property. Finally, the new algorithm is experimentally evaluated."
981,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Efficient Gaussian Process Inference for Short-Scale Spatio-Temporal Modeling,"Jaakko Luttinen, Alexander Ilin",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/luttinen12/luttinen12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_luttinen12,http://jmlr.csail.mit.edu/proceedings/papers/v22/luttinen12.html,This paper presents an efficient Gaussian process inference scheme for modeling shortscale phenomena in spatio-temporal datasets. Our model uses a sum of separable compactly supported covariance functions which yields a full covariance matrix represented in terms of small sparse matrices operating either on the spatial or temporal domain. The proposed inference procedure is based on Gibbs sampling in which samples from the conditional distribution of the latent function values are obtained by applying a simple linear transformation to samples drawn from the joint distribution of the function values and the observations. We make use of the proposed model structure and the conjugate gradient method to compute the required transformation. In the experimental part the proposed algorithm is compared to the standard approach using the sparse Cholesky decomposition and it is shown to be much faster and computationally feasible for 100-1000 times larger datasets. We demonstrate the advantages of the proposed method in the problem of reconstructing sea surface temperature which requires processing of a real-world dataset with 10^6 observations.
982,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Safe Policy Iteration,"Matteo Pirotta, Marcello Restelli, Alessio Pecorino, Daniele Calandriello",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/pirotta13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/pirotta13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_pirotta13,http://jmlr.csail.mit.edu/proceedings/papers/v28/pirotta13.html,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms. When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur. To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops. We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy. Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game."
983,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Towards End-To-End Speech Recognition with Recurrent Neural Networks,"Alex Graves, Navdeep Jaitly",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/graves14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_graves14,http://jmlr.csail.mit.edu/proceedings/papers/v32/graves14.html,"This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%."
984,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Compositional Morphology for Word Representations and Language Modelling,"Jan Botha, Phil Blunsom",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/botha14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_botha14,http://jmlr.csail.mit.edu/proceedings/papers/v32/botha14.html,"This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models."
985,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Unifying Framework of Anytime Sparse Gaussian Process Regression Models with Stochastic Variational Inference for Big Data,"Trong Nghia Hoang, Quang Minh Hoang, Bryan Kian Hsiang Low",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hoang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hoang15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hoang15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hoang15.html,"This paper presents a novel unifying framework of anytime sparse Gaussian process regression (SGPR) models that can produce good predictive performance fast and improve their predictive performance over time. Our proposed unifying framework reverses the variational inference procedure to theoretically construct a non-trivial, concave functional that is maximized at the predictive distribution of any SGPR model of our choice. As a result, a stochastic natural gradient ascent method can be derived that involves iteratively following the stochastic natural gradient of the functional to improve its estimate of the predictive distribution of the chosen SGPR model and is guaranteed to achieve asymptotic convergence to it. Interestingly, we show that if the predictive distribution of the chosen SGPR model satisfies certain decomposability conditions, then the stochastic natural gradient is an unbiased estimator of the exact natural gradient and can be computed in constant time (i.e., independent of data size) at each iteration. We empirically evaluate the trade-off between the predictive performance vs. time efficiency of the anytime SGPR models on two real-world million-sized datasets."
986,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Effcient combination of pairwise feature networks,"Pau Bellot, Patrick E. Meyer",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/bellot15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_bellot15,http://jmlr.csail.mit.edu/proceedings/papers/v46/bellot15.html,"This paper presents a novel method for the reconstruction of a neural network connectivity using calcium fluorescence data. We introduce a fast unsupervised method to integrate different networks that reconstructs structural connectivity from neuron activity. Our method improves the state-of-the-art reconstruction method General Transfer Entropy (GTE). We are able to better eliminate indirect links, improving therefore the quality of the network via a normalization and ensemble process of GTE and three new informative features. The approach is based on a simple combination of networks, which is remarkably fast. The performance of our approach is benchmarked on simulated time series provided at the connectomics challenge and also submitted at the public competition."
987,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Large-Margin Structured Prediction via Linear Programming,"Zhuoran Wang, John Shawe-Taylor","5:599-606, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09d/wang09d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_wang09d,http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09d.html,This paper presents a novel learning algorithm for structured classification problems where the task is to predict multiple and interacting labels (multilabel) for an input object. The maximum margin separation between the correct multilabels and the incorrect ones is formulated as a linear program. Instead of explicitly writing out the entire problem with an exponentially large constraint set the linear program is solved iteratively via column generation. In this case the process of generating most violated constraints is equivalent to searching for highest-scored misclassified incorrect multilabels which can be easily achieved by decoding the structure based on current estimations. In addition we also explore the integration of the column generation and the extragradient method for linear programming to gain further efficiency. Compared to previous works on large-margin structured prediction this framework has advantages in handling arbitrary structures and larger-scale problems. Experimental results on part-of-speech tagging and statistical machine translation tasks are reported demonstrating the competitiveness of the proposed approach.
988,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Composite Quantization for Approximate Nearest Neighbor Search,"Ting Zhang, Chao Du, Jingdong Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangd14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangd14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhangd14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangd14.html,"This paper presents a novel compact coding approach, composite quantization, for approximate nearest neighbor search. The idea is to use the composition of several elements selected from the dictionaries to accurately approximate a vector and to represent the vector by a short code composed of the indices of the selected elements. To efficiently compute the approximate distance of a query to a database vector using the short code, we introduce an extra constraint, constant inter-dictionary-element-product, resulting in that approximating the distance only using the distance of the query to each selected element is enough for nearest neighbor search. Experimental comparison with state-of-the-art algorithms over several benchmark datasets demonstrates the efficacy of the proposed approach."
989,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Streaming Multi-label Classification,"Jesse Read, Albert Bifet, Geoff Holmes, Bernhard Pfahringer","17:19-25, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/read11a/read11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_read11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/read11a.html,"This paper presents a new experimental framework for studying multi-label evolving stream classification, with efficient methods that combine the best practices in streaming scenarios with the best practices in multi-label classification. Many real world problems involve data which can be considered as multi-label data streams. Efficient methods exist for multi-label classification in non streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as the learners must be able to adapt to change using limited time and memory. We present a new experimental software that extends the MOA framework. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. It is released under the GNU GPL license."
990,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals,"Kazuyoshi Yoshii, Ryota Tomioka, Daichi Mochihashi, Masataka Goto",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yoshii13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/yoshii13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yoshii13,http://jmlr.csail.mit.edu/proceedings/papers/v28/yoshii13.html,"This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority."
991,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Integrating Grammatical Inference into Robotic Planning,"Jane Chandlee, Jie Fu, Konstantinos Karydis, Cesar Koirala, Jeffrey Heinz and Herbert Tanner","21:69-83, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/chandlee12a/chandlee12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_chandlee12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/chandlee12a.html,This paper presents a method for the control synthesis of robotic systems in an unknown dynamic and adversarial environments. We (1) incorporate a grammatical inference module that identifies the governing dynamics of the adversarial environment and (2) utilize game theory to compute a motion plan for a system given a task specification. The framework is flexible and modular since different games can be formulated for different system objectives and different grammatical inference algorithms can be utilized depending on the abstract nature of the dynamic environment.
992,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Human-Guided Learning of Social Action Selection for Robot-Assisted Therapy,"Emmanuel Senft, Paul Baxter, Tony Belpaeme",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/senft15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_senft15,http://jmlr.csail.mit.edu/proceedings/papers/v43/senft15.html,"This paper presents a method for progressively increasing autonomous action selection capabilities in sensitive environments, where random exploration-based learning is not desirable, using guidance provided by a human supervisor. We describe the global framework and a simulation case study based on a scenario in Robot Assisted Therapy for children with Autism Spectrum Disorder. This simulation illustrates the functional features of our proposed approach, and demonstrates how a system following these principles adapts to different interaction contexts while maintaining an appropriate behaviour for the system at all times."
993,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Lower Bound for the Optimization of Finite Sums,"Alekh Agarwal, Leon Bottou",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/agarwal15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/agarwal15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_agarwal15,http://jmlr.csail.mit.edu/proceedings/papers/v37/agarwal15.html,"This paper presents a lower bound for optimizing a finite sum of \(n\) functions, where each function is \(L\) -smooth and the sum is \(\mu\) -strongly convex. We show that no algorithm can reach an error \(\epsilon\) in minimizing all functions from this class in fewer than \(\Omega(n + \sqrt{n(\kappa-1)}\log(1/\epsilon))\) iterations, where \(\kappa=L/\mu\) is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally."
994,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Discriminative Latent Variable Model for Online Clustering,"Rajhans Samdani, Kai-Wei Chang, Dan Roth",none,http://jmlr.org/proceedings/papers/v32/samdani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/samdani14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_samdani14,http://jmlr.csail.mit.edu/proceedings/papers/v32/samdani14.html,"This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L3 M outperforms several existing online as well as batch supervised clustering techniques."
995,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A unifying framework for vector-valued manifold regularization and multi-view learning,"Minh H_ Quang, Loris Bazzani, Vittorio Murino",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/haquang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/haquang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_haquang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/haquang13.html,"This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches. In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning."
996,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Handling Sparsity via the Horseshoe,"Carlos M. Carvalho, Nicholas G. Polson, James G. Scott","5:73-80, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/carvalho09a/carvalho09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_carvalho09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/carvalho09a.html,This paper presents a general fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals and is therefore closely related to widely used approaches for sparse Bayesian learning including among others Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.
997,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Minimax hypothesis testing for curve registration,Olivier Collier,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/collier12/collier12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_collier12,http://jmlr.csail.mit.edu/proceedings/papers/v22/collier12.html,This paper is concerned with the problem of goodness-of-fit for curve registration and more precisely for the shifted curve model whose application field reaches from computer vision and road trafic prediction to medicine. We give bounds for the asymptotic minimax separation rate when the functions in the alternative lie in Sobolev balls and the separation from the null hypothesis is measured by the l2-norm. We use the generalized likelihood ratio to build a nonadaptive procedure depending on a tuning parameter which we choose in an optimal way according to the smoothness of the ambient space. Then a Bonferroni procedure is applied to give an adaptive test over a range of Sobolev balls. Both achieve the asymptotic minimax separation rates up to possible logarithmic factors.
998,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Visual Boundary Prediction: A Deep Neural Prediction Network and Quality Dissection,"Jyri Kivinen, Chris Williams, Nicolas Heess","JMLR W&CP 33 :512-521, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kivinen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kivinen14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kivinen14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kivinen14.html,"This paper investigates visual boundary detection, i.e. prediction of the presence of a boundary at a given image location. We develop a novel neurally-inspired deep architecture for the task. Notable aspects of our work are (i) the use of –covariance features” [Ranzato and Hinton, 2010] which depend on the squared response of a filter to the input image, and (ii) the integration of image information from multiple scales and semantic levels via multiple streams of interlinked, layered, and non-linear –deep” processing. Our results on the Berkeley Segmentation Data Set 500 (BSDS500) show comparable or better performance to the top-performing methods [Arbelaez et al., 2011, Ren and Bo, 2012, Lim et al., 2013, Dollˆr and Zitnick, 2013] with effective inference times. We also propose novel quantitative assessment techniques for improved method understanding and comparison. We carefully dissect the performance of our architecture, feature-types used and training methods, providing clear signals for model understanding and development."
999,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Maximizing a Tree Series in the Representation Space,"Guillaume Rabusseau, François Denis","JMLR W&CP 34 :124-138, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/rabusseau14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_rabusseau14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/rabusseau14a.html,"This paper investigates the use of linear representations of trees (i.e. mappings from the set of trees into a finite dimensional vector space which are induced by rational series on trees) in the context of structured data learning. We argue that this representation space can be more appealing than the space of trees to handle machine learning problems involving trees. Focusing on a tree series maximization problem, we first analyze its complexity to motivate the use of approximation techniques. We then show how a tree series can be extended to the continuous representation space, we propose an adaptive Metropolis-Hastings algorithm to solve the maximization problem in this space, and we establish convergence guarantees. Finally, we provide some experiments comparing our algorithm with an implementation of the Metropolis-Hastings algorithm in the space of trees."
1000,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification,"Gautam Dasarathy, Robert Nowak, Xiaojin Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Dasarathy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Dasarathy15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Dasarathy15.html,"This paper investigates the problem of active learning for binary label prediction on a graph. We introduce a simple and label-efficient algorithm called \(S^2\) for this task. At each step, \(S^2\) selects the vertex to be labeled based on the structure of the graph and all previously gathered labels. Specifically, \(S^2\) queries for the label of the vertex that bisects the shortest shortest path between any pair of oppositely labeled vertices. We present a theoretical estimate of the number of queries \(S^2\) needs in terms of a novel parametrization of the complexity of binary functions on graphs. We also present experimental results demonstrating the performance of \(S^2\) on both real and synthetic data. While other graph-based active learning algorithms have shown promise in practice, our algorithm is the first with both good performance and theoretical guarantees. Finally, we demonstrate the implications of the \(S^2\) algorithm to the theory of nonparametric active learning. In particular, we show that \(S^2\) achieves near minimax optimal excess risk for an important class of nonparametric classification problems."
1001,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?,"Senjian An, Farid Boussaid, Mohammed Bennamoun",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/an15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_an15,http://jmlr.csail.mit.edu/proceedings/papers/v37/an15.html,"This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer."
1002,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Breaking the Small Cluster Barrier of Graph Clustering,"Nir Ailon, Yudong Chen, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ailon13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ailon13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ailon13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ailon13.html,"This paper investigates graph clustering in the planted cluster model in the presence of small clusters . Traditional results dictate that for an algorithm to provably correctly recover the clusters, all clusters must be sufficiently large (in particular, \(\tilde{\Omega}(\sqrt{n})\) where \(n\) is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in (Jalali et al. 2011) and (Chen et al. 2012), we prove that small clusters, under certain mild assuptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to recover almost all clusters via a –peeling strategy”, i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed). Our findings are supported by experiments. From a high level, this paper sheds novel insights on high-dimesional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations does the job."
1003,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Domain Generalization via Invariant Feature Representation,"Krikamol Muandet, David Balduzzi, Bernhard Sch_lkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/muandet13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/muandet13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_muandet13,http://jmlr.csail.mit.edu/proceedings/papers/v28/muandet13.html,"This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice."
1004,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Unsupervised SVMs: On the Complexity of the Furthest Hyperplane Problem,"Zohar Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz and Omri Weinstein",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/karnin12/karnin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_karnin12,http://jmlr.csail.mit.edu/proceedings/papers/v23/karnin12.html,"This paper introduces the Furthest Hyperplane Problem (FHP), which is an unsupervised counterpart of Support Vector Machines. Given a set of n points in R d , the objective is to produce the hyperplane (passing through the origin) which maximizes the separation margin, that is, the minimal distance between the hyperplane and any input point. To the best of our knowledge, this is the first paper achieving provable results regarding FHP. We provide both lower and upper bounds to this NP-hard problem. First, we give a simple randomized algorithm whose running time is n O(1/_ 2 ) where _ is the optimal separation margin. We show that its exponential dependency on 1/_ 2 is tight, up to sub-polynomial factors, assuming SAT cannot be solved in sub-exponential time. Next, we give an efficient approximation algorithm. For any _ _ [0, 1], the algorithm produces a hyperplane whose distance from at least 1 - 3_ fraction of the points is at least _ times the optimal separation margin. Finally, we show that FHP does not admit a PTAS by presenting a gap preserving reduction from a particular version of the PCP theorem."
1005,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,DRAW: A Recurrent Neural Network For Image Generation,"Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, Daan Wierstra",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gregor15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gregor15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gregor15.html,"This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye."
1006,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Ordered Stick-Breaking Prior for Sequential MCMC Inference of Bayesian Nonparametric Models,"Mrinal Das, Trapit Bansal, Chiranjib Bhattacharyya",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/das15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/das15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_das15,http://jmlr.csail.mit.edu/proceedings/papers/v37/das15.html,"This paper introduces ordered stick-breaking process (OSBP), where the atoms in a stick-breaking process (SBP) appear in order. The choice of weights on the atoms of OSBP ensure that; (1) probability of adding new atoms exponentially decrease, and (2) OSBP, though non-exchangeable, admit predictive probability functions (PPFs). In a Bayesian nonparametric (BNP) setting, OSBP serves as a natural prior over sequential mini-batches, facilitating exchange of relevant statistical information by sharing the atoms of OSBP. One of the major contributions of this paper is SUMO, an MCMC algorithm, for solving the inference problem arising from applying OSBP to BNP models. SUMO uses the PPFs of OSBP to obtain a Gibbs-sampling based truncation-free algorithm which applies generally to BNP models. For large scale inference problems existing algorithms such as particle filtering (PF) are not practical and variational procedures such as TSVI (Wang & Blei, 2012) are the only alternative. For Dirichlet process mixture model (DPMM), SUMO outperforms TSVI on perplexity by 33% on 3 datasets with million data points, which are beyond the scope of PF, using only 3GB RAM."
1007,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Hierarchical Quasi-Clustering Methods for Asymmetric Networks,"Gunnar Carlsson, Facundo M_moli, Alejandro Ribeiro, Santiago Segarra",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/carlsson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/carlsson14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_carlsson14,http://jmlr.csail.mit.edu/proceedings/papers/v32/carlsson14.html,"This paper introduces hierarchical quasi-clustering methods, a generalization of hierarchical clustering for asymmetric networks where the output structure preserves the asymmetry of the input data. We show that this output structure is equivalent to a finite quasi-ultrametric space and study admissibility with respect to two desirable properties. We prove that a modified version of single linkage is the only admissible quasi-clustering method. Moreover, we show stability of the proposed method and we establish invariance properties fulfilled by it. Algorithms are further developed and the value of quasi-clustering analysis is illustrated with a study of internal migration within United States."
1008,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Stochastic Dual Coordinate Ascent with Adaptive Probabilities,"Dominik Csiba, Zheng Qu, Peter Richtarik",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/csiba15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/csiba15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_csiba15,http://jmlr.csail.mit.edu/proceedings/papers/v37/csiba15.html,"This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods."
1009,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Admixture of Poisson MRFs: A Topic Model with Word Dependencies,"David Inouye, Pradeep Ravikumar, Inderjit Dhillon",none,http://jmlr.org/proceedings/papers/v32/inouye14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/inouye14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_inouye14,http://jmlr.csail.mit.edu/proceedings/papers/v32/inouye14.html,"This paper introduces a new topic model based on an admixture of Poisson Markov Random Fields (APM), which can model dependencies between words as opposed to previous independent topic models such as PLSA (Hofmann, 1999), LDA (Blei et al., 2003) or SAM (Reisinger et al., 2010). We propose a class of admixture models that generalizes previous topic models and show an equivalence between the conditional distribution of LDA and independent Poissonsãsuggesting that APM subsumes the modeling power of LDA. We present a tractable method for estimating the parameters of an APM based on the pseudo log-likelihood and demonstrate the benefits of APM over previous models by preliminary qualitative and quantitative experiments."
1010,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Hybrid Neural Network-Latent Topic Model,"Li Wan, Leo Zhu, Rob Fergus",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/wan12/wan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_wan12,http://jmlr.csail.mit.edu/proceedings/papers/v22/wan12.html,This paper introduces a hybrid model that combines a neural network with a latent topic model. The neural network provides a low dimensional embedding for the input data whose subsequent distribution is captured by the topic model. The neural network thus acts as a trainable feature extractor while the topic model captures the group structure of the data. Following an initial pretraining phase to separately initialize each part of the model a unified training scheme is introduced that allows for discriminative training of the entire model. The approach is evaluated on visual data in scene classification task where the hybrid model is shown to outperform models based solely on neural networks or topic models as well as other baseline methods.
1011,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Covariance Selection in the Linear Mixed Effect Mode,"Jonathan P. Williams, Ying Lu",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/Williams2015.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_Williams2015,http://jmlr.csail.mit.edu/proceedings/papers/v44/Williams2015.html,"This paper improves and extends the two-step penalized iterative estimation procedure for the linear mixed effect model (LMM) by explicitly penalizing the off-diagonal components of the covariance matrix of random effects. To explicitly penalize the off-diagonal terms in the covariance matrix of random effects, glasso is incorporated in the penalized LMM approach. The paper also provides theoretical justification and a computational algorithm for the provided approach. Empirical analysis using random simulated data shows that explicitly penalizing the off-diagonal covariance components can greatly improve the model selection procedure."
1012,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Mass Fatality Incident Identification based on nuclear DNA evidence,Fabio Corradi,"9:105-112, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/corradi10a/corradi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_corradi10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/corradi10a.html,This paper focuses on the use of nuclear DNA Short Tandem Repeat traits for the identification of the victims of a Mass Fatality Incident. The goal of the analysis is the assessment of the identification probabilities concerning the recovered victims. Identification hypotheses are evaluated conditionally to the DNA evidence observed both on the recovered victims and on the relatives of the missing persons disappeared in the tragical event. After specifying a set of conditional independence assertions suitable for the problem an inference strategy is provided treating some points to achieve computational efficiency. Finally the proposal is tested through the simulation of a Mass Fatality Incident and the results are examined in details.
1013,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,An Efficient Algorithm for Large Scale Compressive Feature Learning,"Hristo Paskov, John Mitchell, Trevor Hastie","JMLR W&CP 33 :760-768, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/paskov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/paskov14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_paskov14,http://jmlr.csail.mit.edu/proceedings/papers/v33/paskov14.html,"This paper focuses on large-scale unsupervised feature selection from text. We expand upon the recently proposed Compressive Feature Learning (CFL) framework, a method that uses dictionary-based compression to select a K-gram representation for a document corpus. We show that CFL is NP-Complete and provide a novel and efficient approximation algorithm based on a homotopy that transforms a convex relaxation of CFL into the original problem. Our algorithm allows CFL to scale to corpuses comprised of millions of documents because each step is linear in the corpus length and highly parallelizable. We use it to extract features from the BeerAdvocate dataset, a corpus of over 1.5 million beer reviews spanning 10 years. CFL uses two orders of magnitude fewer features than the full trigram space. It beats a standard unigram model in a number of prediction tasks and achieves nearly twice the accuracy on an author identification task."
1014,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Multi-task Learning for Recommender System,Xia Ning and George Karypis,"13:269-284, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/ning10a/ning10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_ning10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/ning10a.html,This paper focuses on exploring personalized multi-task learning approaches for collaborative filtering towards the goal of improving the prediction performance of rating prediction systems. These methods first specifically identify a set of users that are closely related to the user under consideration (i.e. active user) and then learn multiple rating prediction models simultaneously one for the active user and one for each of the related users. Such learning for multiple models (tasks) in parallel is implemented by representing all learning instances (users and items) using a coupled user-item representation and within errorinsensitive Support Vector Regression (e-SVR) framework applying multi-task kernel tricks. A comprehensive set of experiments shows that multi-task learning approaches lead to significant performance improvement over conventional alternatives.
1015,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Signal Correlation Prediction Using Convolutional Neural Networks,Lukasz Romaszko,none,http://jmlr.csail.mit.edu/proceedings/papers/v46/romaszko15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_romaszko15,http://jmlr.csail.mit.edu/proceedings/papers/v46/romaszko15.html,"This paper focuses on analysing multiple time series relationships such as correlations between them. We develop a solution for the Connectiomics contest dataset of fluorescence imaging of neural activity recordings ã the aim is reconstruction of the wiring between brain neurons. The model is implemented to achieve high evaluation score. Our model took the fourth place in this contest. The performance is similar to the other leading solutions, thus we showed that deep learning methods for time series processing are comparable to the other approaches and have wide opportunities for further improvement. We discuss a range of methods and code optimisations applied for the convolutional neural network for the time series domain."
1016,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons,"Yuxin Chen, Changho Suh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chena15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/chena15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chena15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chena15.html,"This paper explores the preference-based top-K rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-K ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-K ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the K-th and (K+1)-th ranked items. The minimum sample complexity required for reliable top-K ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called Spectral MLE, that returns the indices of the top-K items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-K item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments."
1017,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,The Dependent Dirichlet Process Mixture of Objects for Detection-free Tracking and Object Modeling,"Willie Neiswanger, Frank Wood, Eric Xing","JMLR W&CP 33 :660-668, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/neiswanger14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_neiswanger14,http://jmlr.csail.mit.edu/proceedings/papers/v33/neiswanger14.html,"This paper explores how to find, track, and learn models of arbitrary objects in a video without a predefined method for object detection. We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors. Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing. We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos. We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods."
1018,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Accelerating AdaBoost using UCB,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/busa09/busa09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_busa09,http://jmlr.csail.mit.edu/proceedings/papers/v7/busa09.html,"This paper explores how multi-armed bandits (MABs) can be applied to accelerate AdaBoost. AdaBoost constructs a strong classifier in a stepwise fashion by adding simple base classifiers to a pool and using their weighted ñvoteî to determine the final classification. We model this stepwise base classifier selection as a sequential decision problem and optimize it with MABs. Each arm represents a subset of the base classifier set. The MAB gradually learns the ""utility"" of the subsets and selects one of the subsets in each iteration. ADABOOST then searches only this subset instead of optimizing the base classifier over the whole space. The reward is defined as a function of the accuracy of the base classifier. We investigate how the well-known UCB algorithm can be applied in the case of boosted stumps trees and products of base classifiers. The KDD Cup 2009 was a large-scale learning task with a limited training time thus this challenge offered us a good opportunity to test the utility of our approach. During the challenge our best results came in the Up-selling task where our model was within 1% of the best AUC rate. After more thorough post-challenge validation the algorithm performed as well as the best challenge submission on the small data set in two of the three tasks."
1019,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,A Novel Hybrid Feature Selection Method Based on IFSFFS and SVM for the Diagnosis of Erythemato-Squamous Diseases,"Juanying Xie, Weixin Xie, Chunxia Wang and Xinbo Gao","11:142-151, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/xie10a/xie10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_xie10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/xie10a.html,"This paper developed a diagnosis model based on Support Vector Machines (SVM) with a novel hybrid feature selection method to diagnose erythemato-squamous diseases. Our hybrid feature selection method, named IFSFFS (Improved F-score and Sequential Forward Floating Search), combines the advantages of filters and wrappers to select the optimal feature subset from the original feature set. In our IFSFFS, we firstly generalized the original F-score to the improved F-score measuring the discrimination of more than two sets of real numbers. Then we proposed to combine Sequential Forward Floating Search (SFFS) and our improved F-score to accomplish the optimal feature subset selection. Where, our improved F-score is an evaluation criterion for filters, while SFFS and SVM compose an evaluation system of wrappers. The best parameters of kernel function of SVM are found out by grid search technique with ten-fold cross validation. Experiments have been conducted on five random training-test partitions of the erythemato-squamous diseases dataset from UCI machine learning database. The experimental results show that our SVM-based model with IFSFFS achieved the optimal classification accuracy with no more than 14 features as well."
1020,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Pinview: Implicit Feedback in Content-Based Image Retrieval,"Peter Auer, Zakria Hussain, Samuel Kaski, Arto Klami, Jussi Kujala, Jorma Laaksonen, Alex P. Leung, Kitsuchart Pasupa, John Shawe-Taylor","11:51-57, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/auer10a/auer10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_auer10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/auer10a.html,"This paper describes Pinview, a content-based image retrieval system that exploits implicit relevance feedback during a search session. Pinview contains several novel methods that infer the intent of the user. From relevance feedback, such as eye movements or clicks, and visual features of images Pinview learns a similarity metric between images which depends on the current interests of the user. It then retrieves images with a specialized reinforcement learning algorithm that balances the tradeoff between exploring new images and exploiting the already inferred interests of the user. In practise, we have integrated Pinview to the content-based image retrieval system PicSOM, in order to apply it to real-world image databases. Preliminary experiments show that eye movements provide a rich input modality from which it is possible to learn the interests of the user."
1021,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,"An Ensemble of Three Classifiers for KDD Cup 2009: Expanded Linear Model, Heterogeneous Boosting, and Selective Naive Bayes",none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/lo09/lo09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_lo09,http://jmlr.csail.mit.edu/proceedings/papers/v7/lo09.html,This paper describes our ensemble of three classifiers for the KDD Cup 2009 challenge. First we transform the three binary classification tasks into a joint multi-class classification problem and solve an l1-regularized maximum entropy model under the LIBLINEAR framework. Second we propose a heterogeneous base learner which is capable of handling different types of features and missing values and use AdaBoost to improve the base learner. Finally we adopt a selective na¥ve Bayes classifier that automatically groups categorical features and discretizes numerical ones. The parameters are tuned using crossvalidation results rather than the 10% test results on the competition website. Based on the observation that the three positive labels are exclusive we conduct a post-processing step using the linear SVM to jointly adjust the prediction scores of each classifier on the three tasks. Then we average these prediction scores with careful validation to get the final outputs. Our final average AUC on the whole test set is 0.8461 which ranks third place in the slow track of KDD Cup 2009.
1022,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Interactive Pattern Recognition and Human Language Technology for Digital Audiovisual Content Processing,"Antonio Lagarda, Jorge Civera, Alfons Juan and Francisco Casacuberta","11:103-110, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/lagarda10a/lagarda10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_lagarda10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/lagarda10a.html,"This paper describes ongoing research work by the Pattern Recognition and Human Language Technology (PRHLT) group (UPV PASCAL2 node) in two important technology transfer projects: i3media and erudito.com. On the one hand, i3media (2007-2010) is a 35M— ""tractor"" technology project within the Spanish Programa CENIT-Ingenio 2010, run through a consortium of 12 main enterprises of the media sector, which also involve 19 research groups, including PRHLT. i3media focuses on the creation and automated management of intelligent audiovisual content, so as to facilitate both, content personalisation and interaction with users (i3media.barcelonamedia.org). Our participation in i3media is centred on interactive machine translation, to transfer and adapt our experience on this technology to i3media-specific needs. On the other hand, erudito.com (2010-2012) is a 1.4M— experimental design project, supported by the Spanish Ministry of Industry, Tourism and Trade under the Avanza I+D program, aimed at developing a tool to encapsulate, distribute and intelligently use digital content such as that showed on thematic TV channels. In this project, PRHLT contributes to the development of interactive closed captioning (speech transcription) and machine translation tools."
1023,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Simple Variable Length N-grams for Probabilistic Automata Learning,"Fabio N. Kepler, Sergio L. S. Mergen, and Cleo Z. Billa","21:254-258, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/kepler12a/kepler12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_kepler12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/kepler12a.html,This paper describes an approach used in the 2012 Probabilistic Automata Learning Competition. The main goal of the competition was to obtain insights about which techniques and approaches work best for sequence learning based on different kinds of automata generating machines. This paper proposes the usage of n-gram models with variable length. Experiments show that using the test sets provided by the competition the variable-length approach works better than fixed 3-grams.
1024,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Modeling Knowledge Worker Activity,Tadej _tajner and Dunja Mladeni_,"11:127-133, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/stajner10a/stajner10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_stajner10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/stajner10a.html,"This paper describes an approach to constructing a probabilistic process model representing knowledge worker activity out of a log of primitive events, such as e-mails, web page visits and document accesses. Firstly, we present the process of enriching the primitive events into abstract actions, executed in different contexts. We explain the process of obtaining both context and action for each event by clustering the events via two different views. Secondly, we present an application of probabilistic deterministic finite automata to model the transitions between consecutive actions within the same context and demonstrate the approach on real-world knowledge worker data for the purpose of understanding knowledge processes and demonstrating the feasibility of the proposed approach, where a process model is constructed out of low-level events."
1025,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Modeling Knowledge Worker Activity,Tadej _tajner and Dunja Mladeni_,"11:127-133, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/stajner10a/stajner10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_stajner10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/stajner10a.html,"This paper describes an approach to constructing a probabilistic process model representing knowledge worker activity out of a log of primitive events, such as e-mails, web page visits and document accesses. Firstly, we present the process of enriching the primitive events into abstract actions, executed in different contexts. We explain the process of obtaining both context and action for each event by clustering the events via two different views. Secondly, we present an application of probabilistic deterministic finite automata to model the transitions between consecutive actions within the same context and demonstrate the approach on real-world knowledge worker data for the purpose of understanding knowledge processes and demonstrating the feasibility of the proposed approach, where a process model is constructed out of low-level events."
1026,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Active Learning Based on Parzen Window,"L. Lan, H. Shi, Z. Wang & S. Vucetic ; 16:99_112, 2011. [ abs ] [ pdf ]","16:99_112, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/lan11a/lan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_lan11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/lan11a.html,This paper describes active learning algorithm used in AISTATS 2010 Active Learning Challenge as well as several of its extensions evaluated in the post-competition experiments. The algorithm consists of a pair of Regularized Parzen Window Classi_ers one trained on full set of features and another on features _ltered using Pearson correlation. Predictions of the two classi_ers are averaged to obtain the ensemble classi_er. Parzen Window classi_er was chosen because is an easy to implement lazy algorithm and has a single parameter the kernel window size that is determined by the cross-validation. The labeling schedule started by selecting random 20 examples and then continued by doubling the number of labeled examples in each round of active learning. A combination of random sampling and uncertainty sampling was used for querying. For the random sampling examples were _rst clustered using either all features or the _ltered features (whichever resulted in higher cross-validated accuracy) and then the same number of random examples was selected from each cluster. Our algorithm ranked as the 5th overall and was consistently ranked in the upper half of the competing algorithms. The challenge results show that Parzen Window classi_ers are less accurate than several competing learning algorithms used in the competition but also indicate the success of the simple querying strategy that was employed. In the post-competition we were able to improve the accuracy by using an ensemble of 5 Parzen Window classi_ers each trained on features selected by di_erent _lters. We also explored how more involved querying during the initial stages of active learning and the pre-clustering querying strategy would in§uence the performance of the proposed algorithm.   Page last modified on Wed Mar 30 11:09:57 2011.
1027,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,"Committee Based Prediction System for Recommendation: KDD Cup 2011, Track2","H. Zhang, E. Riedl, V. Petrushin, S. Pal & J. Spoelstra","18:215_229, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/zhang12a/zhang12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_zhang12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/zhang12a.html,This paper describes a solution to the 2011 KDD Cup competition Track2: discriminating between highly rated tracks and unrated tracks in a Yahoo! Music dataset. Our approach was to use supervised learning based on 65 features generated using various techniques such as collaborative _ltering SVD and similarity scoring. During our modeling stage we created a number of predictors including logistic regression arti_cial neural networks and gradient-boosted decision trees. To further improve robustness and reduce the variance we used three of our top performing models and took a weighted average for the _nal submission which achieved 4.3768% error.   Page last modified on Tue May 29 10:23:35 2012.
1028,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A totally unimodular view of structured sparsity,"Marwa El Halabi, Volkan Cevher",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/elhalabi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/elhalabi15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_elhalabi15,http://jmlr.csail.mit.edu/proceedings/papers/v38/elhalabi15.html,"This paper describes a simple framework for structured sparse recovery based on convex optimization. We show that many structured sparsity models can be naturally represented by linear matrix inequalities on the support of the unknown parameters, where the constraint matrix has a totally unimodular (TU) structure. For such structured models, tight convex relaxations can be obtained in polynomial time via linear programming. Our modeling framework unifies the prevalent structured sparsity norms in the literature, introduces new interesting ones, and renders their tightness and tractability arguments transparent."
1029,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Visualization of Online Discussion Forums,Mitja Trampu_ and Marko Grobelnik,"11:134-141, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/trampus10a/trampus10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_trampus10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/trampus10a.html,"This paper describes a set of visualization tools which aid the understanding of discussion topics and trends in online discussion forums. The tools integrate into the forum's web page, allowing for easy exploration of its contents. Three visualizations are presented: a visual browsing suggestions mechanism, a semantic ""atlas"" providing a thematic overview of larger forum segments, and a timeline displaying temporal evolution of forum topics. The underlying algorithms have very few language-dependent components. The software is operational and can be tested live on Slovene, Slovak and Hungarian pilot sites, containing up to 5 million forum posts."
1030,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Ensembles for Time Series Forecasting,"Mariana Oliveira, Luis Torgo",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/oliveira14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_oliveira14,http://jmlr.csail.mit.edu/proceedings/papers/v39/oliveira14.html,This paper describes a new type of ensembles that aims at improving the predictive performance of these approaches in time series forecasting. Ensembles are recognised as one of the most successful approaches to prediction tasks. Previous theoretical studies of ensembles have shown that one of the key reasons for this performance is diversity among ensemble members. Several methods exist to generate diversity. The key idea of the work we are presenting here is to propose a new form of diversity generation that explores some specific properties of time series prediction tasks. Our hypothesis is that the resulting ensemble members will be better at addressing different dynamic regimes of time series data. Our large set of experiments confirms that the methods we have explored for generating diversity are able to improve the performance of the equivalent ensembles with standard diversity generation procedures.
1031,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Dynamical Models and tracking regret in online convex programming,"Eric Hall, Rebecca Willett",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/hall13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_hall13,http://jmlr.csail.mit.edu/proceedings/papers/v28/hall13.html,"This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with comparatorês deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network."
1032,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Application of Additive Groves Ensemble with Multiple Counts Feature Evaluation to KDD Cup'09 Small Data Set,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/sorokina09/sorokina09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_sorokina09,http://jmlr.csail.mit.edu/proceedings/papers/v7/sorokina09.html,This paper describes a field trial for a recently developed ensemble called Additive Groves on KDD CupÍ09 competition. Additive Groves were applied to three tasks provided at the competition using the îsmallî data set. On one of the three tasks appetency we achieved the best result among participants who similarly worked with the small dataset only. Postcompetition analysis showed that less successfull result on another task churn was partially due to insufficient preprocessing of nominal attributes. Code for Additive Groves is publicly available as a part of TreeExtra package. Another part of this package provides an important preprocessing technique also used for this competition entry feature evaluation through bagging with multiple counts.
1033,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Sample Efficient Reinforcement Learning with Gaussian Processes,"Robert Grande, Thomas Walsh, Jonathan How",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/grande14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/grande14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_grande14,http://jmlr.csail.mit.edu/proceedings/papers/v32/grande14.html,"This paper derives sample complexity results for using Gaussian Processes (GPs) in both model-based and model-free reinforcement learning (RL). We show that GPs are KWIK learnable, proving for the first time that a model-based RL approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that previous approaches to model-free RL using GPs take an exponential number of steps to find an optimal policy, and are therefore not sample efficient. The third and main contribution is the introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and, in contrast to model-based algorithms, capable of acting in real time, as demonstrated on a five-dimensional aircraft simulator."
1034,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Faithfulness in Chain Graphs: The Gaussian Case,Jose Pe_a,"15:588-599, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/pena11a/pena11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_pena11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/pena11a.html,This paper deals with chain graphs under the classic Lauritzen-Wermuth-Frydenberg interpretation. We prove that almost all the regular Gaussian distributions that factorize with respect to a chain graph are faithful to it. This result has three important consequences. First chain graphs are more powerful than undirected graphs and acyclic directed graphs for representing regular Gaussian distributions as some of these distributions can be represented exactly by the former but not by the latter. Second the moralization and c-separation criteria for reading independencies from a chain graph are complete in the sense that they identify all the independencies that can be identified from the chain graph alone. Third some definitions of equivalence in chain graphs coincide and thus they have the same graphical characterization.
1035,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,An Optimal Policy for Target Localization with Application to Electron Microscopy,"Raphael Sznitman, Aurelien Lucchi, Peter Frazier, Bruno Jedynak, Pascal Fua",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sznitman13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sznitman13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sznitman13.html,"This paper considers the task of finding a target location by making a limited number of sequential observations. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies."
1036,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,"Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting","Matt Wytock, Zico Kolter",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wytock13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wytock13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wytock13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wytock13.html,"This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs. The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems. In this paper, we make three contributions: 1) we develop a second-order active-set method which is several orders of magnitude faster that previously proposed optimization approaches for this problem 2) we analyze the model from a theoretical standpoint, improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and 3) we apply the method to large-scale energy forecasting problems, demonstrating state-of-the-art performance on two real-world tasks."
1037,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Meta-Transportability of Causal Effects: A Formal Approach,"Elias Bareinboim, Judea Pearl",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/bareinboim13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_bareinboim13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/bareinboim13a.html,"This paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a different environment, in which only passive observations can be collected. Pearl and Bareinboim (2011) established a complete characterization for such transfer between two domains, a source and a target, and this paper generalizes their results to multiple heterogeneous domains. It establishes a necessary and sufficient condition for deciding when effects in the target domain are estimable from both statistical and causal information transferred from the experiments in the source domains. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing observational and experimental information to synthesize an unbiased estimate of the desired effects."
1038,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Noisy Sparse Subspace Clustering,"Yu-Xiang Wang, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13.html,"This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications."
1039,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,On Estimating Causal Effects based on Supplemental Variables,"Takahiro Hayashi, Manabu Kuroki","JMLR W&CP 33 :312-319, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hayashi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hayashi14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hayashi14.html,"This paper considers the problem of estimating causal effects of a treatment on a response using supplementary variables. Under the assumption that a treatment is associated with a response through a univariate supplementary variable in the framework of linear regression models, Cox (1960) showed that the estimation accuracy of the regression coefficient of the treatment on the response in the single linear regression model can be improved by using the recursive linear regression model based on the supplementary variable from the viewpoint of the asymptotic variance. However, such assumptions may not hold in many practical situations. In this paper, we consider the situation where a treatment is associated with a response through a set of supplementary variables in both linear and discrete models. Then, we show that the estimation accuracy of the causal effect can be improved by using the supplementary variables. Different from Cox (1960), the results of this paper are derived without the assumption of Gaussian error terms in linear models or dichotomous variables in discrete models. The results of this paper help us to obtain the reliable evaluation of causal effects from observed data."
1040,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,High-Rank Matrix Completion,"Brian Eriksson, Laura Balzano, Robert Nowak",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/eriksson12/eriksson12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_eriksson12,http://jmlr.csail.mit.edu/proceedings/papers/v22/eriksson12.html,This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank. Since the columns belong to a union of subspaces this problem may also be viewed as a missing-data version of the subspace clustering problem. Let X be an nxN matrix whose (complete) columns lie in a union of at most k subspaces each of rank = r n and assume Nkn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least C r N \log^2(n) entries of X are observed uniformly at random with C1 a constant depending on the usual incoherence conditions the geometrical arrangement of subspaces and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.
1041,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Near-optimal max-affine estimators for convex regression,"Gabor Balazs, Andrˆs Gy_rgy, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/balazs15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/balazs15-supp.zip,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_balazs15,http://jmlr.csail.mit.edu/proceedings/papers/v38/balazs15.html,"This paper considers least squares estimators for regression problems over convex, uniformly bounded, uniformly Lipschitz function classes minimizing the empirical risk over max-affine functions (the maximum of finitely many affine functions). Based on new results on nonlinear nonparametric regression and on the approximation accuracy of max-affine functions, these estimators are proved to achieve the optimal rate of convergence up to logarithmic factors. Preliminary experiments indicate that a simple randomized approximation to the optimal estimator is competitive with state-of-the-art alternatives."
1042,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Streaming Sparse Principal Component Analysis,"Wenzhuo Yang, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangd15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangd15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yangd15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangd15.html,"This paper considers estimating the leading k principal components with at most s non-zero attributes from p-dimensional samples collected sequentially in memory limited environments. We develop and analyze two memory and computational efficient algorithms called streaming sparse PCA and streaming sparse ECA for analyzing data generated according to the spike model and the elliptical model respectively. In particular, the proposed algorithms have memory complexity O(pk), computational complexity O(pk min k,slogp ) and sample complexity \(\Theta(s \log p)\) . We provide their finite sample performance guarantees, which implies statistical consistency in the high dimensional regime. Numerical experiments on synthetic and real-world datasets demonstrate good empirical performance of the proposed algorithms."
1043,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Approximate Inference in Additive Factorial HMMs with Application to Energy Disaggregation,"J. Zico Kolter, Tommi Jaakkola",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zico12/zico12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zico12,http://jmlr.csail.mit.edu/proceedings/papers/v22/zico12.html,"This paper considers additive factorial hidden Markov models an extension to HMMs where the state factors into multiple independent chains and the output is an additive function of all the hidden states. Although such models are very powerful accurate inference is unfortunately difficult: exact inference is not computationally tractable and existing approximate inference techniques are highly susceptible to local optima. In this paper we propose an alternative inference method for such models which exploits their additive structure by 1) looking at the observed difference signal of the observation 2) incorporating a ""robust"" mixture component that can account for unmodeled observations and 3) constraining the posterior to allow at most one hidden state to change at a time. Combining these elements we develop a convex formulation of approximate inference that is computationally efficient has no issues of local optima and which performs much better than existing approaches in practice. The method is motivated by the problem of energy disaggregation the task of taking a whole home electricity signal and decomposing it into its component appliances; applied to this task our algorithm achieves state-of-the-art performance and is able to separate many appliances almost perfectly using just the total aggregate signal."
1044,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Learning to Rank for Personalized News Article Retrieval,"Lorand Dali, Bla_ Fortuna and Jan Rupnik","11:152-159, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/dali10a/dali10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_dali10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/dali10a.html,"This paper aims to tackle the very interesting and important problem of user personalized ranking of search results. The focus is on news retrieval and the data from which the ranking model is learned was provided by a large online newspaper. The personalized news search ranking model which we have developed takes into account not only document content and metadata, but also data specific to the user such as age, gender, job, income, city, country etc. All the user specific data is provided by the user himself when registering to the news site."
1045,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Multi-Manifold Modeling in Non-Euclidean spaces,"Xu Wang, Konstantinos Slavakis, Gilad Lerman",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wang15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15b.html,"This paper advocates a novel framework for segmenting a dataset on a Riemannian manifold \(M\) into clusters lying around low-dimensional submanifolds of \(M\) . Important examples of \(M\) , for which the proposed algorithm is computationally efficient, include the sphere, the set of positive definite matrices, and the Grassmannian. The proposed algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering. Local geometry is encoded by sparse coding and directional information of local tangent spaces and geodesics, which is important in resolving intersecting clusters and establishing the theoretical guarantees for a simplified variant of the algorithm. To avoid complication, these guarantees assume that the underlying submanifolds are geodesic. Extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical (geodesic) model as well as its superior performance over state-of-the-art techniques."
1046,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Programming by Feedback,"Marc Schoenauer, Riad Akrour, Michele Sebag, Jean-Christophe Souplet",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/schoenauer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_schoenauer14,http://jmlr.csail.mit.edu/proceedings/papers/v32/schoenauer14.html,"This paper advocates a new ML-based programming framework, called Programming by Feedback (PF), which involves a sequence of interactions between the active computer and the user. The latter only provides preference judgments on pairs of solutions supplied by the active computer. The active computer involves two components: the learning component estimates the userês utility function and accounts for the userês (possibly limited) competence; the optimization component explores the search space and returns the most appropriate candidate solution. A proof of principle of the approach is proposed, showing that PF requires a handful of interactions in order to solve some discrete and continuous benchmark problems."
1047,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Bipartite Edge Prediction via Transductive Learning over Product Graphs,"Hanxiao Liu, Yiming Yang",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/liuc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/liuc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_liuc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/liuc15.html,"This paper addresses the problem of predicting the missing edges of a bipartite graph where each side of the vertices has its own intrinsic structure. We propose a new optimization framework to map the two sides of the intrinsic structures onto the manifold structure of the edges via a graph product, and to reduce the original problem to vertex label propagation over the product graph. This framework enjoys flexible choices in the formulation of graph products, and supports a rich family of graph transduction schemes with scalable inference. Experiments on benchmark datasets for collaborative filtering, citation network analysis and prerequisite prediction of online courses show advantageous performance of the proposed approach over other state-of-the-art methods."
1048,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Learning from Weak Teachers,"Ruth Urner, Shai Ben David, Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/urner12/urner12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_urner12,http://jmlr.csail.mit.edu/proceedings/papers/v22/urner12.html,This paper addresses the problem of learning when high-quality labeled examples are an expensive resource while samples with error-prone labeling (for example generated by crowdsourcing) are readily available. We introduce a formal framework for such learning scenarios with label sources of varying quality and we propose a parametric model for such label sources (``weak teachers'') reflecting the intuition that their labeling is likely to be correct in label-homogeneous regions but may deteriorate near classification boundaries. We consider learning when the learner has access to weakly labeled random samples and on top of that can actively query the correct labels of sample points of its choice. We propose a learning algorithm for this scenario analyze its sample complexity and prove that under certain conditions on the underlying data distribution our learner can utilize the weak labels to reduce the number of expert labels it requires. We view this paper as a first step towards the development of a theory of learning from labels generated by teachers of varying accuracy a scenario that is relevant in various practical applications.
1049,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Descent Methods for Tuning Parameter Refinement,"Alexander Lorbert, Peter Ramadge","9:469-476, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/lorbert10a/lorbert10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_lorbert10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/lorbert10a.html,"This paper addresses multidimensional tuning parameter selection in the context of ""train-validate-test"" and K-fold cross validation. A coarse grid search over tuning parameter space is used to initialize a descent method which then jointly optimizes over variables and tuning parameters. We study four regularized regression methods and develop the update equations for the corresponding descent algorithms. Experiments on both simulated and real-world datasets show that the method results in significant tuning parameter refinement."
1050,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Dynamic Resource Allocation for Optimizing Population Diffusion,"Shan Xue, Alan Fern, Daniel Sheldon","JMLR W&CP 33 :1033-1041, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/xue14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_xue14,http://jmlr.csail.mit.edu/proceedings/papers/v33/xue14.html,"This paper addresses adaptive conservation planning, where the objective is to maximize the population spread of a species by allocating limited resources over time to conserve land parcels. This problem is characterized by having highly stochastic exogenous events (population spread), a large action branching factor (number of allocation options) and state space, and the need to reason about numeric resources. Together these characteristics render most existing AI planning techniques ineffective. The main contribution of this paper is to design and evaluate an online planner for this problem based on Hindsight Optimization (HOP), a technique that has shown promise in other stochastic planning problems. Unfortunately, standard implementations of HOP scale linearly with the number of actions in a domain, which is not feasible for conservation problems such as ours. Thus, we develop a new approach for computing HOP policies based on mixed-integer programming and dual decomposition. Our experiments on synthetic and real-world scenarios show that this approach is effective and scalable compared to existing alternatives."
1051,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,"Margins, Shrinkage, and Boosting",Matus Telgarsky,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/telgarsky13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/telgarsky13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_telgarsky13,http://jmlr.csail.mit.edu/proceedings/papers/v28/telgarsky13.html,"This manuscript shows that AdaBoost and its immediate variants can produce approximately maximum margin classifiers simply by scaling their step size choices by a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedmanês empirically successful –shrinkage” procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss."
1052,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Boosting with the Logistic Loss is Consistent,Matus Telgarsky,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Telgarsky13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Telgarsky13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Telgarsky13.html,"This manuscript provides optimization guarantees, generalization bounds, and statistical consistency results for AdaBoost variants which replace the exponential loss with the logistic and similar losses (specifically, twice differentiable convex losses which are Lipschitz and tend to zero on one side).The heart of the analysis is to show that, in lieu of explicit regularization and constraints, the structure of the problem is fairly rigidly controlled by the source distribution itself. The first control of this type is in the separable case, where a distribution-dependent relaxed weak learning rate induces speedy convergence with high probability over any sample. Otherwise, in the nonseparable case, the convex surrogate risk itself exhibits distribution-dependent levels of curvature, and consequently the algorithmês output has small norm with high probability."
1053,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Preface,The Editors,none,http://jmlr.csail.mit.edu/proceedings/papers/v42/edit14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_edit14b,http://jmlr.csail.mit.edu/proceedings/papers/v42/edit14b.html,This is the preface
1054,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,From Theories to Queries,"B. Settles ; 16:1_18, 2011. [ abs ] [ pdf ]","16:1_18, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/settles11a/settles11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_settles11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/settles11a.html,This article surveys recent work in active learning aimed at making it more practical for real-world use. In general active learning systems aim to make machine learning more economical since they can participate in the acquisition of their own training data. An active learner might iteratively select informative query instances to be labeled by an oracle  for example. Work over the last two decades has shown that such approaches are e_ective at maintaining accuracy while reducing training set size in many machine learning applications. However as we begin to deploy active learning in real ongoing learning systems and data annotation projects we are encountering unexpected problems„due in part to practical realities that violate the basic assumptions of earlier foundational work. I review some of these issues and discuss recent work being done to address the challenges.   Page last modified on Wed Mar 30 11:08:56 2011.
1055,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Fast b-matching via Sufficient Selection Belief Propagation,"Bert Huang, Tony Jebara","15:361-369, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/huang11a/huang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_huang11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/huang11a.html,This article describes scalability enhancements to a previously established belief propagation algorithm that solves bipartite maximum weight b-matching. The previous algorithm required O(|V|+|E|) space and O(|V||E|) time whereas we apply improvements to reduce the space to O(|V|) and the time to O(|V|^{2.5}) in the expected case (though worst case time is still O(|V||E|)). The space improvement is most significant in cases where edge weights are determined by a function of node descriptors such as a distance or kernel function. In practice we demonstrate maximum weight b-matchings to be solvable on graphs with hundreds of millions of edges in only a few hours of compute time on a modern personal computer without parallelization whereas neither the memory nor the time requirement of previously known algorithms would have allowed graphs of this scale.
1056,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,On Estimation and Selection for Topic Models,Matt Taddy,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/taddy12/taddy12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_taddy12,http://jmlr.csail.mit.edu/proceedings/papers/v22/taddy12.html,This article describes posterior maximization for topic models identifying computational and conceptual gains from inference under a non-standard parametrization. We then show that fitted parameters can be used as the basis for a novel approach to marginal likelihood estimation via block-diagonal approximation to the information matrix that facilitates choosing the number of latent topics. This likelihood-based model selection is complemented with a goodness-of-fit analysis built around estimated residual dispersion. Examples are provided to illustrate model selection as well as to compare our estimation against standard alternative techniques.
1057,47,http://jmlr.csail.mit.edu/proceedings/papers/v47/,Preface to the 1st ECML/PKDD workshop on Statistically Sound Data Mining,"Wilhelmiina H _ m _ l _ inen, François Petitjean, Geoffrey I. Webb",none,http://jmlr.csail.mit.edu/proceedings/papers/v47/preface.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v47/,,27th November 2015,41897,ECML/PKDD Workshop on Statistically Sound Data Mining 2014 Proceedings,Statistically Sound Data Mining,"Joensuu, Finland","Wilhelmiina HÕ_mÕ_lÕ_inen, FranÕ_ois Petitjean, Geoffrey, I. Webb",v47_preface,http://jmlr.csail.mit.edu/proceedings/papers/v47/preface.html,"These proceedings contain the papers accepted to the 1st ECML/PKDD Workshop on Statistically Sound Data Mining, which took place at the French Institute for Computer Science (INRIA) in Nancy, at the opening of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD) on the 15th of September 2014."
1058,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Bayesian learning of joint distributions of objects,"Anjishnu Banerjee, Jared Murray, David Dunson",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/banerjee13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_banerjee13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/banerjee13a.html,"There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure. The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types. The ITF prior is formulated as a tensor product of stick-breaking processes. Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior. Focusing on ITF mixtures of product kernels, we develop a new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications."
1059,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,HOP-MAP: Efficient Message Passing with High Order Potentials,"Daniel Tarlow, Inmar Givoni, Richard Zemel","9:812-819, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/tarlow10a/tarlow10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_tarlow10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/tarlow10a.html,There is a growing interest in building probabilistic models with high order potentials (HOPs) or interactions among discrete variables. Message passing inference in such models generally takes time exponential in the size of the interaction but in some cases maximum a posteriori (MAP) inference can be carried out efficiently. We build upon such results introducing two new classes including composite HOPs that allow us to flexibly combine tractable HOPs using simple logical switching rules. We present efficient message update algorithms for the new HOPs and we improve upon the efficiency of message updates for a general class of existing HOPs. Importantly we present both new and existing HOPs in a common representation; performing inference with any combination of these HOPs requires no change of representations or new derivations.
1060,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Feature Reinforcement Learning using Looping Suffix Trees,"Mayank Daswani, Peter Sunehag, Marcus Hutter","24:11-24, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/daswani12a/daswani12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_daswani12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/daswani12a.html,There has recently been much interest in history-based methods using suffix trees to solve POMDPs. However these suffix trees cannot efficiently represent environments that have long-term dependencies. We extend the recently introduced CT_MDP algorithm to the space of looping suffix trees which have previously only been used in solving deterministic POMDPs. The resulting algorithm replicates results from CT_MDP for environments with short term dependencies while it outperforms LSTM-based methods on TMaze a deep memory environment.
1061,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection,"Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, Hoyt Koepke",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/nutini15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/nutini15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_nutini15,http://jmlr.csail.mit.edu/proceedings/papers/v37/nutini15.html,"There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing thatãexcept in extreme casesãitês convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule."
1062,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: Online Sabotaged Shortest Path,"Wouter M. Koolen, Manfred K. Warmuth, Dmitri Adamskiy",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Koolen15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Koolen15b,http://jmlr.csail.mit.edu/proceedings/papers/v40/Koolen15b.html,There has been much work on extending the prediction with expert advice methodology to the case when experts are composed of components and there are combinatorially many such experts. One of the core examples is the Online Shortest Path problem where the components are edges and the experts are paths. In this note we revisit this online routing problem in the case where in each trial some of the edges or components are sabotaged / blocked. In the vanilla expert setting a known method can solve this extension where experts are now awake or asleep in each trial. We ask whether this technology can be upgraded efficiently to the case when at each trial every component can be awake or asleep. It is easy get to get an initial regret bound by using combinatorially many experts. However it is open whether there are efficient algorithms achieving the same regret.
1063,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data,"Arun Rajkumar, Shivani Agarwal",none,http://jmlr.org/proceedings/papers/v32/rajkumar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rajkumar14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rajkumar14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rajkumar14.html,"There has been much interest recently in the problem of rank aggregation from pairwise data. A natural question that arises is: under what sorts of statistical assumptions do various rank aggregation algorithms converge to an •optimalê ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently from some underlying probability distribution. We first show that, under a •time-reversibilityê or Bradley-Terry-Luce (BTL) condition on the distribution generating the outcomes of the pairwise comparisons, the rank centrality (PageRank) and least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a matrix version of the Borda count algorithm, and more surprisingly, an algorithm which performs maximal likelihood estimation under a BTL assumption, both converge to an optimal ranking under a •low-noiseê condition that is strictly more general than BTL. Finally, we propose a new SVM-based algorithm for rank aggregation from pairwise data, and show that this converges to an optimal ranking under an even more general condition that we term •generalized low-noiseê. In all cases, we provide explicit sample complexity bounds for exact recovery of an optimal ranking. Our experiments confirm our theoretical findings and help to shed light on the statistical behavior of various rank aggregation algorithms."
1064,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Stochastic Block Transition Models for Dynamic Networks,Kevin Xu,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/xu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/xu15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_xu15,http://jmlr.csail.mit.edu/proceedings/papers/v38/xu15.html,"There has been great interest in recent years on statistical models for dynamic networks. In this paper, I propose a stochastic block transition model (SBTM) for dynamic networks that is inspired by the well-known stochastic block model (SBM) for static networks and previous dynamic extensions of the SBM. Unlike most existing dynamic network models, it does not make a hidden Markov assumption on the edge-level dynamics, allowing the presence or absence of edges to directly influence future edge probabilities while retaining the interpretability of the SBM. I derive an approximate inference procedure for the SBTM and demonstrate that it is significantly better at reproducing durations of edges in real social network data."
1065,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Faster Algorithms for Testing under Conditional Sampling,"Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, Venkatadheeraj Pichapati, Ananda Theertha Suresh",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Falahatgar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Falahatgar15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Falahatgar15.html,"There has been considerable recent interest in distribution-tests whose run-time and sample requirements are sublinear in the domain-size \(k\) . We study two of the most important tests under the conditional-sampling model where each query specifies a subset \(S\) of the domain, and the response is a sample drawn from \(S\) according to the underlying distribution. For identity testing, which asks whether the underlying distribution equals a specific given distribution or \(\epsilon\) -differs from it, we reduce the known time and sample complexities from \(\widetilde{\mathcal{O}}(\epsilon^{-4})\) to \(\widetilde{\mathcal{O}}(\epsilon^{-2})\) , thereby matching the information theoretic lower bound. For closeness testing, which asks whether two distributions underlying observed data sets are equal or different, we reduce existing complexity from \(\widetilde{\mathcal{O}}(\epsilon^{-4} \log^5 k)\) to an even sub-logarithmic \(\widetilde{\mathcal{O}}(\epsilon^{-5} \log \log k)\) thus providing a better bound to an open problem in Bertinoro Workshop on Sublinear Algorithms (Fisher, \(2014\) )."
1066,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,MADE: Masked Autoencoder for Distribution Estimation,"Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/germain15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/germain15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_germain15,http://jmlr.csail.mit.edu/proceedings/papers/v37/germain15.html,"There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoderês parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators."
1067,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On the difficulty of training recurrent neural networks,"Razvan Pascanu, Tomas Mikolov, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/pascanu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/pascanu13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_pascanu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/pascanu13.html,"There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
1068,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Deep Edge-Aware Filters,"Li Xu, Jimmy Ren, Qiong Yan, Renjie Liao, Jiaya Jia",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/xub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_xub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/xub15.html,"There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters."
1069,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Relational Learning with One Network: An Asymptotic Analysis,"Rongjing Xiang, Jennifer Neville","15:779-788, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/xiang11a/xiang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_xiang11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/xiang11a.html,Theoretical analysis of structured learning methods has focused primarily on domains where the data consist of independent (albeit structured) examples. Although the statistical relational learning (SRL) community has recently developed many classification methods for graph and network domains much of this work has focused on modeling domains where there is a single network for learning. For example we could learn a model to predict the political views of users in an online social network based on the friendship relationships among users. In this example the data would be drawn from a single large network (e.g. Facebook) and increasing the data size would correspond to acquiring a larger graph. Although SRL methods can successfully improve classification in these types of domains there has been little theoretical analysis of addressing the issue of single network domains. In particular the asymptotic properties of estimation are not clear if the size of the model grows with the size of the network. In this work we focus on outlining the conditions under which learning from a single network will be asymptotically consistent and normal. Moreover we compare the properties of maximum likelihood estimation (MLE) with that of generalized maximum pseudolikelihood estimation (MPLE) and use the resulting understanding to propose novel MPLE estimators for single network domains. We include empirical analysis on both synthetic and real network data to illustrate the findings.
1070,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Rating Prediction with Informative Ensemble of Multi-Resolution Dynamic Models,"Z. Zheng, T. Chen, N. Liu, Q. Yang & Y. Yu","18:75_97, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/zheng12a/zheng12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_zheng12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/zheng12a.html,The Yahoo! music rating data set in KDD Cup 2011 raises several interesting challenges: (1) The data covers a lengthy time period of more than eight years. (2) Not only are training ratings associated date and time information so are the test ratings. (3) The items form a hierarchy consisting of four types of items: genres artists albums and tracks. To capture the rich temporal dynamics within the data set we design a class of time-aware matrix/tensor factorization models which adopts time series based parameterizations and models user/item drifting behaviors at multiple temporal resolutions. We also incorporate the taxonomical structure into the item parameters by introducing sharing parameters between ancestors and descendants in the taxonomy. Finally we have identi_ed some conditions that systematically a_ect the e_ectiveness of di_erent types of models and parameter settings. Based on these _ndings we designed an informative  ensemble framework which considers additional meta features when making predictions for a particular pair of user and item. Using these techniques we built the best single model reported o_cially and our _nal ensemble model got third place in KDD Cup 2011.   Page last modified on Tue May 29 10:23:04 2012.
1071,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Ensembles of Adaptive Model Rules from High-Speed Data Streams,"Jo†o Duarte, Jo†o Gama","JMLR W&CP 36 :198-213, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/duarte14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_duarte14,http://jmlr.csail.mit.edu/proceedings/papers/v36/duarte14.html,"The volume and velocity of data is increasing at astonishing rates. In order to extract knowledge from this huge amount of information there is a need for efficient on-line learning algorithms. Rule-based algorithms produce models that are easy to understand and can be used almost offhand. Ensemble methods combine several predicting models to improve the quality of prediction. In this paper, a new on-line ensemble method that combines a set of rule-based models is proposed to solve regression problems from data streams. Experimental results using synthetic and real time-evolving data streams show the proposed method significantly improves the performance of the single rule-based learner, and outperforms two state-of-the-art regression algorithms for data streams."
1072,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Vanishing Component Analysis,"Roi Livni, David Lehavi, Sagi Schein, Hila Nachliely, Shai Shalev-Shwartz, Amir Globerson",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/livni13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_livni13,http://jmlr.csail.mit.edu/proceedings/papers/v28/livni13.html,"The vanishing ideal of a set of n points S, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials. The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy."
1073,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment,"Jason Chuang, Sonal Gupta, Christopher Manning, Jeffrey Heer",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chuang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chuang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chuang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/chuang13.html,"The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure they are meaningful. We introduce a framework to support large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality."
1074,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multiresolution Matrix Factorization,"Risi Kondor, Nedelina Teneva, Vikas Garg",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kondor14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kondor14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kondor14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kondor14.html,"The types of large matrices that appear in modern Machine Learning problems often have complex hierarchical structures that go beyond what can be found by traditional linear algebra tools, such as eigendecompositions. Inspired by ideas from multiresolution analysis, this paper introduces a new notion of matrix factorization that can capture structure in matrices at multiple different scales. The resulting Multiresolution Matrix Factorizations (MMFs) not only provide a wavelet basis for sparse approximation, but can also be used for matrix compression (similar to Nystrom approximations) and as a prior for matrix completion."
1075,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,Novel Models and Ensemble Techniques to Discriminate Favorite Items from Unrated Ones for Personalized Music Recommendation,T.G. McKenzie,"18:101_135, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/mckenzie12a/mckenzie12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_mckenzie12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/mckenzie12a.html,The Track 2 problem in KDD-Cup 2011 (music recommendation) is to discriminate between music tracks highly rated by a given user from those which are overall highly rated but not rated by the given user. The training dataset consists of not only user rating history but also the taxonomic information of track artist album and genre. This paper describes the solution of the National Taiwan University team which ranked _rst place in the competition. We exploited a diverse of models (neighborhood models latent models Bayesian Personalized Ranking models and random-walk models) with local blending and global ensemble to achieve 97.45% in accuracy on the testing dataset.   Page last modified on Tue May 29 10:23:11 2012.
1076,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,On Time Varying Undirected Graphs,"Mladen Kolar, Eric Xing","15:407-415, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/kolar11a/kolar11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_kolar11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/kolar11a.html,The time-varying multivariate Gaussian distribution and the undirected graph associated with it as introduced in Zhou et al. (2008) provide a useful statistical framework for modeling complex dynamic networks. In many application domains it is of high importance to estimate the graph structure of the model consistently for the purpose of scientific discovery. In this short note we show that under suitable technical conditions the structure of the undirected graphical model can be consistently estimated in the high dimensional setting when the dimensionality of the model is allowed to diverge with the sample size. The model selection consistency is shown for the procedure proposed in Zhou et al. (2008) and for the modified neighborhood selection procedure of Meinshausen and BÙhlmann (2006).
1077,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Stochastic Regret Minimization via Thompson Sampling,"Sudipto Guha, Kamesh Munagala","JMLR W&CP 35 :317-338, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/guha14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_guha14,http://jmlr.csail.mit.edu/proceedings/papers/v35/guha14.html,"The Thompson Sampling (TS) policy is a widely implemented algorithm for the stochastic multi-armed bandit (MAB) problem. Given a prior distribution over possible parameter settings of the underlying reward distributions of the arms, at each time instant, the policy plays an arm with probability equal to the probability that this arm has largest mean reward conditioned on the current posterior distributions of the arms. This policy generalizes the celebrated –probability matching” heuristic which has been experimentally and widely observed in human decision making. However, despite its ubiquity, the Thompson Sampling policy is poorly understood. Our goal in this paper is to make progress towards understanding the empirical success of this policy. We proceed using the lens of approximation algorithms and problem definitions from stochastic optimization. We focus on an objective function termed stochastic regret that captures the expected number of times the policy plays an arm that is not the eventual best arm, where the expectation is over the prior distribution. Given such a definition, we show that TS is a \(2\) _approximation to the optimal decision policy in two extreme but canonical scenarios. One such scenario is the two-armed bandit problem which is used as a calibration point in all bandit literature. The second scenario is stochastic optimization where the outcome of a random variable is revealed in a single play to a high or low deterministic value. We show that the \(2\) approximation is tight in both these scenarios. We provide an uniform analysis framework that in theory is capable of proving our conjecture that the TS policy is a \(2\) _approximation to the optimal decision policy for minimizing stochastic regret, for any prior distribution and any time horizon."
1078,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,MCMC Learning,"Varun Kanade, Elchanan Mossel",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kanade15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kanade15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kanade15.html,"The theory of learning under the uniform distribution is rich and deep, with connections to cryptography, computational complexity, and the analysis of boolean functions to name a few areas. This theory however is very limited due to the fact that the uniform distribution and the corresponding Fourier basis are rarely encountered as a statistical model. A family of distributions that vastly generalizes the uniform distribution on the Boolean cube is that of distributions represented by Markov Random Fields (MRF). Markov Random Fields are one of the main tools for modeling high dimensional data in many areas of statistics and machine learning. In this paper we initiate the investigation of extending central ideas, methods and algorithms from the theory of learning under the uniform distribution to the setup of learning concepts given examples from MRF distributions. In particular, our results establish a novel connection between properties of MCMC sampling of MRFs and learning under the MRF distribution."
1079,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Copula Network Classifiers (CNCs),Gal Elidan,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/elidan12a/elidan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_elidan12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/elidan12a.html,The task of classification is of paramount importance and extensive research has been aimed at developing general purpose classifiers that can be used effectively in a variety of domains. Network-based classifiers such as the tree augmented naive Bayes model are appealing since they are easily interpretable can naturally handle missing data and are often quite effective. Yet for complex domains with continuous explanatory variables practical performance is often sub-optimal. To overcome this limitation we introduce Copula Network Classifiers (CNCs) a model that combines the flexibility of a graph based representation with the modeling power of copulas. As we demonstrate on ten varied continuous real-life datasets CNCs offer better overall performance than linear and non-linear standard generative models as well as discriminative RBF and polynomial kernel SVMs. In addition since no parameter tuning is required CNCs can be trained dramatically faster than SVMs.
1080,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Iterative Learning and Denoising in Convolutional Neural Associative Memories,"Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/karbasi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/karbasi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_karbasi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/karbasi13.html,"The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance can be made linear in the size of the network."
1081,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Efficient variable selection in support vector machines via the alternating direction method of multipliers,"Gui_Bo Ye, Yifei Chen, Xiaohui Xie","15:832-840, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ye11a/ye11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ye11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/ye11a.html,The support vector machine (SVM) is a widely used tool for classification. Although commonly understood as a method of finding the maximum-margin hyperplane it can also be formulated as a regularized function estimation problem corresponding to a hinge loss function plus an l2-norm regulation term. The doubly regularized support vector machine (DrSVM) is a variant of the standard SVM which introduces an additional l1-norm regularization term on the fitted coefficients. The combined l1 and l2 regularization termed elastic net penalty has the interesting property of achieving simultaneous variable selection and margin-maximization within a single framework. However because of the nonsmoothness of both the loss function and the regularization term there is no efficient method to solve DrSVM for large scale problems. Here we develop an efficient algorithm based on the alternating direction method of multipliers (ADMM) to solve the optimization problem in DrSVM. The utility of the method is further illustrated using both simulated and real-world datasets.
1082,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scaling SVM and Least Absolute Deviations via Exact Data Reduction,"Jie Wang, Peter Wonka, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangd14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangd14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangd14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangd14.html,"The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solver. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and it is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude."
1083,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fisher Consistency of Multicategory Support Vector Machines,Yufeng Liu,"2:291-298, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/liu07b/liu07b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_liu07b,http://jmlr.csail.mit.edu/proceedings/papers/v2/liu07b.html,The Support Vector Machine (SVM) has become one of the most popular machine learning techniques in recent years. The success of the SVM is mostly due to its elegant margin concept and theory in binary classification. Generalization to the multicategory setting however is not trivial. There are a number of different multicategory extensions of the SVM in the literature. In this paper we review several commonly used extensions and Fisher consistency of these extensions. For inconsistent extensions we propose two approaches to make them Fisher consistent one is to add bounded constraints and the other is to truncate unbounded hinge losses.
1084,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Accurate Ensembles for Data Streams: Combining Restricted Hoeffding Trees using Stacking,"Albert Bifet, Eibe Frank, Geoffrey Holmes, and Bernhard Pfahringer","13:225-240, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/bifet10a/bifet10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_bifet10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/bifet10a.html,The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this paper we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons with one perceptron per class. We propose a mechanism for setting the perceptrons' learning rate using the ADWIN change detection method for data streams and also use ADWIN to reset ensemble members (i.e. Hoeffding trees) when they no longer perform well. Our experiments show that the resulting ensemble classifier outperforms bagging for data streams in terms of accuracy when both are used in conjunction with adaptive naive Bayes Hoeffding trees at the expense of runtime and memory consumption.
1085,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine,"Tu Dinh Nguyen, Truyen Tran, Dinh Phung, Svetha Venkatesh","JMLR W&CP 29 :133-148, 2013",http://jmlr.org/proceedings/papers/v29/Nguyen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Nguyen13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Nguyen13.html,"The success of any machine learning system depends critically on effective representations of data. In many cases, especially those in vision, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data. We demonstrate the capacity of our model on well-known datasets of handwritten digits, faces and documents. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorisation (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). However, the learnt features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM."
1086,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit Feedback,"Ankan Saha, Ambuj Tewari","15:636-642, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11a/saha11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_saha11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/saha11a.html,The study of online convex optimization in the bandit setting was initiated by Kleinberg (2004) and Flaxman et al. (2005). Such a setting models a decision maker that has to make decisions in the face of adversarially chosen convex loss functions. Moreover the only information the decision maker receives are the losses. The identity of the loss functions themselves is not revealed. In this setting we reduce the gap between the best known lower and upper bounds for the class of smooth convex functions i.e. convex functions with a Lipschitz continuous gradient. Building upon existing work on self-concordant regularizers and one-point gradient estimation we give the first algorithm whose expected regret ignoring constant and logarithmic factors is O(T^{2/3}).
1087,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Approximation Analysis of Stochastic Gradient Langevin Dynamics by using Fokker-Planck Equation and Ito Process,"Issei Sato, Hiroshi Nakagawa",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/satoa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_satoa14,http://jmlr.csail.mit.edu/proceedings/papers/v32/satoa14.html,"The stochastic gradient Langevin dynamics (SGLD) algorithm is appealing for large scale Bayesian learning. The SGLD algorithm seamlessly transit stochastic optimization and Bayesian posterior sampling. However, solid theories, such as convergence proof, have not been developed. We theoretically analyze the SGLD algorithm with constant stepsize in two ways. First, we show by using the Fokker-Planck equation that the probability distribution of random variables generated by the SGLD algorithm converges to the Bayesian posterior. Second, we analyze the convergence of the SGLD algorithm by using the Ito process, which reveals that the SGLD algorithm does not strongly but weakly converges. This result indicates that the SGLD algorithm can be an approximation method for posterior averaging."
1088,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Estimating Dependency Structures for non-Gaussian Components with Linear and Energy Correlations,"Hiroaki Sasaki, Michael Gutmann, Hayaru Shouno, Aapo Hyvarinen","JMLR W&CP 33 :868-876, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/sasaki14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/sasaki14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_sasaki14,http://jmlr.csail.mit.edu/proceedings/papers/v33/sasaki14.html,"The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered."
1089,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A Fast Algorithm for Recovery of Jointly Sparse Vectors based on the Alternating Direction Methods,"Hongtao Lu, Xianzhong Long, Jingyuan Lv","15:461-469, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/lu11a/lu11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_lu11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/lu11a.html,The standard compressive sensing (CS) aims to recover sparse signal from single measurement vector (SMV) which is known as SMV model. In this paper we consider the recovery of jointly sparse signals in the multiple measurement vector (MMV) scenario where signal is represented as a matrix and the sparsity of signal occurs in a common location set. The sparse MMV model can be formulated as a matrix (21)-norm minimization problem. However the (21)-norm minimization problem is much more difficult to solve than l1-norm minimization. In this paper we propose a very fast algorithm called MMV-ADM for jointly sparse signal recovery in MMV settings based on the alternating direction method (ADM). The MMV-ADM alternately updates the signal matrix the Lagrangian multiplier and the residue and all update rules only involve matrix or vector multiplications and summations so it is simple easy to implement and much more fast than the state-of-the-art method MMV{\small prox}. Numerical simulations show that MMV-ADM is at least dozens of times faster than MMV{\small prox} with comparable recovery accuracy.
1090,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization,"Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kumar13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13b.html,"The separability assumption (Arora et al., 2012; Donoho & Stodden, 2003) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several favorable properties in relation to existing methods. A parallel implementation of our algorithm scales excellently on shared and distributed-memory machines."
1091,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines,"Kohei Ogawa, Motoki Imamura, Ichiro Takeuchi, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ogawa13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13a.html,"The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches."
1092,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Gated Autoencoders with Tied Input Weights,"Droniou Alain, Sigaud Olivier",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/alain13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_alain13,http://jmlr.csail.mit.edu/proceedings/papers/v28/alain13.html,"The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities."
1093,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Active Area Search via Bayesian Quadrature,"Yifei Ma, Roman Garnett, Jeff Schneider","JMLR W&CP 33 :595-603, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ma14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ma14,http://jmlr.csail.mit.edu/proceedings/papers/v33/ma14.html,"The selection of data collection locations is a problem that has received significant research attention from classical design of experiments to various recent active learning algorithms. Typical objectives are to map an unknown function, optimize it, or find level sets in it. Each of these objectives focuses on an assessment of individual points. The introduction of set kernels has led to algorithms that instead consider labels assigned to sets of data points. In this paper we combine these two concepts and consider the problem of choosing data collection locations when the goal is to identify regions whose set of collected data would be labeled positively by a set classifier. We present an algorithm for the case where the positive class is defined in terms of a regionês average function value being above some threshold with high probability, a problem we call active area search. To this end, we model the latent function using a Gaussian process and use Bayesian quadrature to estimate its integral on predefined regions. Our method is the first which directly solves the active area search problem. In experiments it outperforms previous algorithms that were developed for other active search goals."
1094,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Event based text mining for integrated network construction,"Yvan Saeys, Sofie Van Landeghem, Yves Van de Peer","8:112-121, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/saeys10a/saeys10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_saeys10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/saeys10a.html,The scientific literature is a rich and challenging data source for research in systems biology providing numerous interactions between biological entities. Text mining techniques have been increasingly useful to extract such information from the literature in an automatic way but up to now the main focus of text mining in the systems biology field has been restricted mostly to the discovery of protein-protein interactions. Here we take this approach one step further and use machine learning techniques combined with text mining to extract a much wider variety of interactions between biological entities. Each particular interaction type gives rise to a separate network represented as a graph all of which can be subsequently combined to yield a so-called integrated network representation. This provides a much broader view on the biological system as a whole which can then be used in further investigations to analyse specific properties of the network.
1095,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Memory Efficient Kernel Approximation,"Si Si, Cho-Jui Hsieh, Inderjit Dhillon",none,http://jmlr.org/proceedings/papers/v32/si14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/si14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_si14,http://jmlr.csail.mit.edu/proceedings/papers/v32/si14.html,"The scalability of kernel machines is a big challenge when facing millions of samples due to storage and computation issues for large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation algorithm _ Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the MNIST2M dataset with two-million samples, our method takes 550 seconds on a single machine using less than 500 MBytes memory to achieve 0.2313 test RMSE for kernel ridge regression, while standard Nystr_m approximation takes more than 2700 seconds and uses more than 2 GBytes memory on the same problem to achieve 0.2318 test RMSE."
1096,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Lanczos Approximations for the Speedup of Kernel Partial Least Squares Regression,"Nicole Kramer, Masashi Sugiyama, Mikio Braun","5:288-295, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/kramer09a/kramer09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_kramer09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/kramer09a.html,The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples. However the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime and thus constitutes a computational bottleneck in real-world data analysis. We propose a novel algorithm for KPLS which not only computes (a) the fit but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime. The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.
1097,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Modeling Musical Influence with Topic Models,"Uri Shalit, Daphna Weinshall, Gal Chechik",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/shalit13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/shalit13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_shalit13,http://jmlr.csail.mit.edu/proceedings/papers/v28/shalit13.html,"The role of musical influence has long been debated by scholars and critics in the humanities, but never in a data-driven way. In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970ês and the mid 1990ês."
1098,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Future directions in learning to rank,"O. Chapelle, Y. Chang & T.-Y. Liu","14:91_100, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/chapelle11b/chapelle11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_chapelle11b,http://jmlr.csail.mit.edu/proceedings/papers/v14/chapelle11b.html,The results of the learning to rank challenge showed that the quality of the predictions from the top competitors are very close from each other. This raises a question: is learning to rank a solved problem? On the on hand it is likely that only small incremental progress can be made in the ñcoreî and traditional problematics of learning to rank. The challenge was set in this standard learning to rank scenario: optimize a ranking measure on a test set. But on the other hand there are a lot of related questions and settings in learning to rank that have not been yet fully explored. We review some of them in this paper and hope that researchers interested in learning to rank will try to answer these challenging and exciting research questions.   Page last modified on Wed Jan 26 10:37:11 2011.
1099,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: Restricted Eigenvalue Condition for Heavy Tailed Designs,"Arindam Banerjee, Sheng Chen, Vidyashankar Sivakumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Banerjee15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Banerjee15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Banerjee15.html,"The restricted eigenvalue (RE) condition characterizes the sample complexity of accurate recovery in the context of high-dimensional estimators such as Lasso and Dantzig selector (Bickel et al., 2009). Recent work has shown that random design matrices drawn from any thin-tailed (sub-Gaussian) distributions satisfy the RE condition with high probability, when the number of samples scale as the square of the Gaussian width of the restricted set (Banerjee et al., 2014; Tropp, 2015). We pose the equivalent question for heavy-tailed distributions: Given a random design matrix drawn from a heavy-tailed distribution satisfying the smallball property (Mendelson, 2015), does the design matrix satisfy the RE condition with the same order of sample complexity as sub-Gaussian distributions? An answer to the question will guide the design of highdimensional estimators for heavy tailed problems."
1100,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust Principal Component Analysis with Complex Noise,"Qian Zhao, Deyu Meng, Zongben Xu, Wangmeng Zuo, Lei Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhao14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhao14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhao14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhao14.html,"The research on robust principal component analysis (RPCA) has been attracting much attention recently. The original RPCA model assumes sparse noise, and use the \(L_1\) -norm to characterize the error term. In practice, however, the noise is much more complex and it is not appropriate to simply use a certain \(L_p\) -norm for noise modeling. We propose a generative RPCA model under the Bayesian framework by modeling data noise as a mixture of Gaussians (MoG). The MoG is a universal approximator to continuous distributions and thus our model is able to fit a wide range of noises such as Laplacian, Gaussian, sparse noises and any combinations of them. A variational Bayes algorithm is presented to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed form. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and background subtraction."
1101,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Characterizing the Representer Theorem,"Yaoliang Yu, Hao Cheng, Dale Schuurmans, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/yu13.html,"The representer theorem assures that kernel methods retain optimality under penalized empirical risk minimization. While a sufficient condition on the form of the regularizer guaranteeing the representer theorem has been known since the initial development of kernel methods, necessary conditions have only been investigated recently. In this paper we completely characterize the necessary and sufficient conditions on the regularizer that ensure the representer theorem holds. The results are surprisingly simple yet broaden the conditions where the representer theorem is known to hold. Extension to the matrix domain is also addressed."
1102,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,History-alignment models for bias-aware prediction of virological response to HIV combination therapy,"Jasmina Bogojeska, Daniel Stockel, Maurizio Zazzi, Rolf Kaiser, Francesca Incardona, Michal Rosen-Zvi, Thomas Lengauer",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bogojeska12/bogojeska12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bogojeska12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bogojeska12.html,The relevant HIV data sets used for predicting outcomes of HIV combination therapies suffer from several problems: uneven therapy representation different treatment backgrounds of the samples and uneven representation with respect to the level of therapy experience. Also they comprise only viral strain(s) that can be detected in the patients' blood serum. The approach presented in this paper tackles these issues by considering not only the most recent therapies but also the different treatment backgrounds of the samples making up the clinical data sets when predicting the outcomes of HIV therapies. For this purpose we introduce a similarity measure for sequences of therapies and use it for training separate linear models for predicting therapy outcome for each target sample. Compared to the most commonly used approach that encodes all available treatment information only by specific input features our approach has the advantage of delivering significantly more accurate predictions for therapy-experienced patients and for rare therapies. Additionally the sample-specific models are more interpretable which is very important in medical applications.
1103,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Optimistic planning for Markov decision processes,"Lucian Busoniu, Remi Munos",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/busoniu12/busoniu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_busoniu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/busoniu12.html,The reinforcement learning community has recently intensified its interest in online planning methods due to their relative independence on the state space size. However tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop state-dependent planning policies. We therefore consider an algorithm related to AO* that optimistically explores a tree representation of the space of closed-loop policies and we analyze the near-optimality of the action it returns after n tree node expansions. While this optimistic planning requires a finite number of actions and possible next states for each transition its asymptotic performance does not depend directly on these numbers but only on the subset of nodes that significantly impact near-optimal policies. We characterize this set by introducing a novel measure of problem complexity called the near-optimality exponent. Specializing the exponent and performance bound for some interesting classes of MDPs illustrates the algorithm works better when there are fewer near-optimal policies and less uniform transition probabilities.
1104,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: Recursive Teaching Dimension Versus VC Dimension,"Hans U. Simon, Sandra Zilles",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Simon15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Simon15b,http://jmlr.csail.mit.edu/proceedings/papers/v40/Simon15b.html,"The Recursive Teaching Dimension (RTD) of a concept class \(\mathcal{C}\) is a complexity parameter referring to the worst-case number of labelled examples needed to learn any target concept in \(\mathcal{C}\) from a teacher following the recursive teaching model. It is the first teaching complexity notion for which interesting relationships to the VC dimension (VCD) have been established. In particular, for finite maximum classes of a given VCD \(d\) , the RTD equals \(d\) . To date, there is no concept class known for which the ratio of RTD over VCD exceeds \(3/2\) . However, the only known upper bound on RTD in terms of VCD is exponential in the VCD and depends on the size of the concept class. We pose the following question: is the RTD upper-bounded by a function that grows only linearly in the VCD? Answering this question would further our understanding of the relationships between the complexity of teaching and the complexity of learning from randomly chosen examples. In addition, the answer to this question, whether positive or negative, is known to have implications on the study of the long-standing open sample compression conjecture, which claims that every concept class of VCD \(d\) has a sample compression scheme in which samples for concepts in the class are compressed to subsets of size no larger than \(d\) ."
1105,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Structured Recurrent Temporal Restricted Boltzmann Machines,"Roni Mittelman, Benjamin Kuipers, Silvio Savarese, Honglak Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mittelman14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mittelman14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mittelman14.html,"The Recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic model for temporal data, that has been shown to effectively capture both short and long-term dependencies in time-series. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible and hidden units, therefore ignoring the dependency structure between the different observations. Learning this structure has the potential to not only improve the prediction performance, but it can also reveal important patterns in the data. For example, given an econometric dataset, we could identify interesting dependencies between different market sectors; given a meteorological dataset, we could identify regional weather patterns. In this work we propose a new class of RTRBM, which explicitly uses a dependency graph to model the structure in the problem and to define the energy function. We refer to the new model as the structured RTRBM (SRTRBM). Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with our method to learn structure in datasets with real valued observations. Our experimental results using synthetic and real datasets, demonstrate that the SRTRBM can improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the structure underlying our benchmark datasets."
1106,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Empirical Exploration of Recurrent Network Architectures,"Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/jozefowicz15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_jozefowicz15,http://jmlr.csail.mit.edu/proceedings/papers/v37/jozefowicz15.html,"The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTMês architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTMês forget gate closes the gap between the LSTM and the GRU."
1107,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Dissecting the Winning Solution of the HiggsML Challenge,Gˆbor Melis,none,http://jmlr.csail.mit.edu/proceedings/papers/v42/meli14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_meli14,http://jmlr.csail.mit.edu/proceedings/papers/v42/meli14.html,"The recent Higgs Machine Learning Challenge pitted one of the largest crowds seen in machine learning contests against one another. In this paper, we present the winning solution and investigate the effect of extra features, the choice of neural network activation function, regularization and data set size. We demonstrate improved classification accuracy using a very similar network architecture on the permutation invariant MNIST benchmark. Furthermore, we advocate the use of a simple method that lies on the boundary between bagging and cross-validation to both estimate the generalization error and improve accuracy."
1108,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,High-Dimensional Density Ratio Estimation with Extensions to Approximate Likelihood Computation,"Rafael Izbicki, Ann Lee, Chad Schafer","JMLR W&CP 33 :420-429, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/izbicki14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/izbicki14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_izbicki14,http://jmlr.csail.mit.edu/proceedings/papers/v33/izbicki14.html,"The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments."
1109,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Feature Selection: An Ever Evolving Frontier in Data Mining,"Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao","10:4-13, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/liu10b/liu10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_liu10b,http://jmlr.csail.mit.edu/proceedings/papers/v10/liu10b.html,The rapid advance of computer technologies in data processing collection and storage has provided unparalleled opportunities to expand capabilities in production services communications and research. However immense quantities of high-dimensional data renew the challenges to the state-of-the-art data mining techniques. Feature selection is an effective technique for dimension reduction and an essential step in successful data mining applications. It is a research area of great practical significance and has been developed and evolved to answer the challenges due to data of increasingly high dimensionality. Its direct benefits include: building simpler and more comprehensible models improving data mining performance and helping prepare clean and understand data. We first briefly introduce the key components of feature selection and review its developments with the growth of data mining. We then overview FSDM and the papers of FSDM10 which showcases of a vibrant research field of some contemporary interests new applications and ongoing research efforts. We then examine nascent demands in data-intensive applications and identify some potential lines of research that require multidisciplinary efforts.
1110,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Ranking from Pairwise Comparisons,"Fabian Wauthier, Michael Jordan, Nebojsa Jojic",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wauthier13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wauthier13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wauthier13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wauthier13.html,"The ranking of \(n\) objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all \(n(n-1)/2\) comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an \(\Omega(n)\) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same \(\Omega(n)\) lower bound in expectation. Furthermore, if an average of \(O(n\log(n))\) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives."
1111,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,"Distributional Rank Aggregation, and an Axiomatic Analysis","Adarsh Prasad, Harsh Pareek, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/prasad15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/prasad15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_prasad15,http://jmlr.csail.mit.edu/proceedings/papers/v37/prasad15.html,"The rank aggregation problem has been studied with varying desiderata in varied communities such as Theoretical Computer Science, Statistics, Information Retrieval and Social Welfare Theory. We introduce a variant of this problem we call distributional rank aggregation, where the ranking data is only available via the induced distribution over the set of all permutations. We provide a novel translation of the usual social welfare theory axioms to this setting. As we show this allows for a more quantitative characterization of these axioms: which then are not only less prone to misinterpretation, but also allow simpler proofs for some key impossibility theorems. Most importantly, these quantitative characterizations lead to natural and novel relaxations of these axioms, which as we show, allow us to get around celebrated impossibility results in social choice theory. We are able to completely characterize the class of positional scoring rules with respect to our axioms and show that Borda Count is optimal in a certain sense."
1112,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search,"Anshumali Shrivastava, Ping Li",none,http://jmlr.org/proceedings/papers/v32/shrivastava14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_shrivastava14,http://jmlr.csail.mit.edu/proceedings/papers/v32/shrivastava14.html,"The query complexity of locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size . In industrial applications such as search where the data are often high-dimensional and binary (e.g., text \(n\) -grams), minwise hashing is widely adopted, which requires applying a large number of permutations on the data. This is costly in computation and energy-consumption. In this paper, we propose a hashing technique which generates all the necessary hash evaluations needed for similarity search, using one single permutation. The heart of the proposed hash function is a –rotation” scheme which densifies the sparse sketches of one permutation hashing in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent interest for densifying other types of sparse sketches. Using our proposed hashing method, the query time of a \((K,L)\) -parameterized LSH is reduced from the typical \(O(dKL)\) complexity to merely \(O(KL+dL)\) , where \(d\) is the number of nonzeros of the data vector, \(K\) is the number of hashes in each hash table, and \(L\) is the number of hash tables. Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies."
1113,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,On Partitioning Rules for Bipartite Ranking,"Stephan Clemencon, Nicolas Vayatis","5:97-104, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/clemencon09a/clemencon09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_clemencon09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/clemencon09a.html,The purpose of this paper is to investigate the properties of partitioning scoring rules in the bipartite ranking setup. We focus on ranking rules based on scoring functions. General suf- cient conditions for the AUC consistency of scoring functions that are constant on cells of a partition of the feature space are provided. Rate bounds are obtained for cubic histogram scoring rules under mild smoothness assumptions on the regression function. In this setup it is shown how to penalize the empirical AUC criterion in order to select a scoring rule nearly as good as the one that can be built when the degree of smoothness of the regression function is known.
1114,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Su_cient Component Analysis,"M. Yamada, G. Niu, J. Takagi & M. Sugiyama","20:247_262, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/yamada11/yamada11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_yamada11,http://jmlr.csail.mit.edu/proceedings/papers/v20/yamada11.html,The purpose of su_cient dimension reduction (SDR) is to _nd a low-dimensional expression of input features that is su_cient for predicting output values. In this paper we propose a novel distribution-free SDR method called su_cient component analysis (SCA) which is computationally more e_cient than existing methods. In our method a solution is computed by iteratively performing dependence estimation and maximization: Dependence estimation is analytically carried out by recently-proposed least-squares mutual information (LSMI) and dependence maximization is also analytically carried out by utilizing the Epanechnikov kernel . Through large-scale experiments on real-world image classi_cation and audio tagging problems the proposed method is shown to compare favorably with existing dimension reduction approaches.   Page last modified on Sun Nov 6 15:43:46 2011.
1115,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,WASP: Scalable Bayes via barycenters of subset posteriors,"Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, David Dunson",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/srivastava15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/srivastava15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_srivastava15,http://jmlr.csail.mit.edu/proceedings/papers/v38/srivastava15.html,"The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency. Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models."
1116,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Reinforcement learning with value advice,"Mayank Daswani, Peter Sunehag, Marcus Hutter",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/daswani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_daswani14,http://jmlr.csail.mit.edu/proceedings/papers/v39/daswani14.html,"The problem we consider in this paper is reinforcement learning with value advice. In this setting, the agent is given limited access to an oracle that can tell it the expected return (value) of any state-action pair with respect to the optimal policy. The agent must use this value to learn an explicit policy that performs well in the environment. We provide an algorithm called RLAdvice, based on the imitation learning algorithm DAgger. We illustrate the effectiveness of this method in the Arcade Learning Environment on three different games, using value estimates from UCT as advice."
1117,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization,Ohad Shamir,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Shamir13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Shamir13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Shamir13.html,"The problem of stochastic convex optimization with bandit feedback (in the learning community) or without knowledge of gradients (in the optimization community) has received much attention in recent years, in the form of algorithms and performance upper bounds. However, much less is known about the inherent complexity of these problems, and there are few lower bounds in the literature, especially for nonlinear functions. In this paper, we investigate the attainable error/regret in the bandit and derivative-free settings, as a function of the dimension d and the available number of queries \(T\) . We provide a precise characterization of the attainable performance for strongly-convex and smooth functions, which also imply a non-trivial lower bound for more general problems. Moreover, we prove that in both the bandit and derivative-free setting, the required number of queries must scale at least quadratically with the dimension. Finally, we show that on the natural class of quadratic functions, it is possible to obtain a –fast” \(O(1/T)\) error rate in terms of \(T\) , under mild assumptions, even without having access to gradients. To the best of our knowledge, this is the first such rate in a derivative-free stochastic setting, and holds despite previous results which seem to imply the contrary."
1118,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Winning The Transfer Learning Track of Yahoo!ês Learning To Rank Challenge with YetiRank,"A. Gulin, I. Kuralenok & D. Pavlov","14:63_76, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/gulin11a/gulin11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_gulin11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/gulin11a.html,The problem of ranking the documents according to their relevance to a given query is a hot topic in information retrieval. Most learning-to-rank methods are supervised and use human editor judgements for learning. In this paper we introduce novel pairwise method called YetiRank that modi_es FriedmanÍs gradient boosting method in part of gradient computation for optimization and takes uncertainty in human judgements into account. Proposed enhancements allowed YetiRank to outperform many state-of-the-art learning to rank methods in o_ine experiments as well as take the _rst place in the second track of the Yahoo! learning-to-rank contest. Even more remarkably the _rst result in the learning to rank competition that consisted of a transfer learning task was achieved without ever relying on the bigger data from the ñtransfer-fromî domain.   Page last modified on Wed Jan 26 10:37:00 2011.
1119,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Online Clustering of Processes,"Azadeh Khaleghi, Daniil Ryabko, Jeremie Mary, Philippe Preux",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/khaleghi12/khaleghi12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_khaleghi12,http://jmlr.csail.mit.edu/proceedings/papers/v22/khaleghi12.html,The problem of online clustering is considered in the case where each data point is a sequence generated by a stationary ergodic process. Data arrive in an online fashion so that the sample received at every time-step is either a continuation of some previously received sequence or a new sequence. The dependence between the sequences can be arbitrary. No parametric or independence assumptions are made; the only assumption is that the marginal distribution of each sequence is stationary and ergodic. A novel computationally efficient algorithm is proposed and is shown to be asymptotically consistent (under a natural notion of consistency). The performance of the proposed algorithm is evaluated on simulated data as well as on real datasets (motion classification).
1120,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Multi-Label Classification with Unlabeled Data: An Inductive Approach,"Le Wu, Min-Ling Zhang","JMLR W&CP 29 :197-212, 2013",http://jmlr.org/proceedings/papers/v29/Wu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Wu13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Wu13.html,"The problem of multi-label classification has attracted great interests in the last decade. Multi-label classification refers to the problems where an example that is represented by a single instance can be assigned to more than one category. Until now, most of the researches on multi-label classification have focused on supervised settings whose assumption is that large amount of labeled training data is available. Unfortunately, labeling training example is expensive and time-consuming, especially when it has more than one label. However, in many cases abundant unlabeled data is easy to obtain. Current attempts toward exploiting unlabeled data for multi-label classification work under the transductive setting, which aim at making predictions on existing unlabeled data while can not generalize to new unseen data. In this paper, the problem of inductive semi-supervised multi-label classification is studied, where a new approach named iMLCU , i.e. inductive Multi-Label Classification with Unlabeled data , is proposed. We formulate the inductive semi-supervised multi-label learning as an optimization problem of learning linear models and ConCave Convex Procedure (CCCP) is applied to optimize the non-convex optimization problem. Empirical studies on twelve diversified real-word multi-label learning tasks clearly validate the superiority of iMLCU against the other well-established multi-label learning approaches."
1121,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Surrogate Functions for Maximizing Precision at the Top,"Purushottam Kar, Harikrishna Narasimhan, Prateek Jain",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kar15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kar15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kar15.html,"The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k."
1122,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Permutation estimation and minimax rates of identifiability,"Olivier Collier, Arnak Dalalyan",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/collier13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_collier13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/collier13a.html,"The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the notion of the minimax matching threshold is introduced and its expression is obtained as a function of the sample size, noise level and dimensionality. We consider the cases of homoscedastic and heteroscedastic noise and carry out, in each case, upper bounds on the matching threshold of several estimators. This upper bounds are shown to be unimprovable in the homoscedastic setting. We also discuss the computational aspects of the estimators and provide some empirical evidence of their consistency on synthetic data-sets."
1123,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Universal Matrix Completion,"Srinadh Bhojanapalli, Prateek Jain",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bhojanapalli14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bhojanapalli14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bhojanapalli14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bhojanapalli14.html,"The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled afresh . In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties.Moreover, we also show that under certain stricter incoherence conditions, \(O(nr^2)\) uniformly sampled entries are enough to recover any rank- \(r\) \(n\times n\) matrix, in contrast to the \(O(nr\log n)\) sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method."
1124,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Learnability of Solutions to Conjunctive Queries: The Full Dichotomy,"Hubie Chen, Matthew Valeriote",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chen15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Chen15a,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chen15a.html,"The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classification program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In order to obtain our negative result, we make use of universal-algebraic concepts, and our result is phrased in terms of the varietal property of non-congruence modularity."
1125,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,ELLA: An Efficient Lifelong Learning Algorithm,"Paul Ruvolo, Eric Eaton",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ruvolo13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ruvolo13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ruvolo13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ruvolo13.html,"The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines. In this paper, we develop a method for online multi-task learning in the lifelong learning setting. The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust theoretical performance guarantees. We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time."
1126,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Induction of Non-Deterministic Finite Automata on Supercomputers,Wojciech Wieczorek,"21:237-242, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/wieczorek12a/wieczorek12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_wieczorek12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/wieczorek12a.html,The problem of inducing automata of minimal size consistent with finite sets of examples and counter-examples is the combinatorial optimization task of grammatical inference and is known to be computationally hard. Both an exact and a heuristic method of finding a non-deterministic finite automaton (NFA) with a given number of states such that all examples are accepted and all counter-examples are rejected by the automaton will be evaluated. The methods are based on a translation of NFA identification into integer nonlinear programming.
1127,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Frequent Subgraph Discovery in Large Attributed Streaming Graphs,"Abhik Ray, Larry Holder, Sutanay Choudhury","JMLR W&CP 36 :166-181, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/ray14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_ray14,http://jmlr.csail.mit.edu/proceedings/papers/v36/ray14.html,"The problem of finding frequent subgraphs in large dynamic graphs has so far only considered a dynamic graph as being represented by a series of static snapshots taken at various points in time. This representation of a dynamic graph does not lend itself well to real time processing of real world graphs like social networks or internet traffic which consist of a stream of nodes and edges. In this paper we propose an algorithm that discovers the frequent subgraphs present in a graph represented by a stream of labeled nodes and edges. Our algorithm is efficient and is easily tuned by the user to produce interesting patterns from various kinds of graph data. In our model, updates to the graph arrive in the form of batches which contain new nodes and edges. Our algorithm continuously reports the frequent subgraphs that are estimated to be found in the entire graph as each batch arrives. We evaluate our system using five large dynamic graph datasets: the Hetrec 2011 challenge data, Twitter, DBLP and two synthetic. We evaluate our approach against two popular large graph miners, i.e., SUBDUE and GERM. Our experimental results show that we can find the same frequent subgraphs as a non-incremental approach applied to snapshot graphs, and in less time."
1128,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,A Survey of Modern Questions and Challenges in Feature Extraction,"Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/storcheus2015survey.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_storcheus2015survey,http://jmlr.csail.mit.edu/proceedings/papers/v44/storcheus2015survey.html,"The problem of extracting features from given data is of critical importance for successful application of machine learning. Feature extraction, as usually understood, seeks an optimal transformation from input data into a (typically real-valued) feature vector that can be used as an input for a learning algorithm. Over time, this problem has been attacked using a growing number of diverse techniques that originated in separate research communities, including feature selection, dimensionality reduction, manifold learning, distance metric learning and representation learning. The goal of this paper is to contrast and compare feature extraction techniques coming from different machine learning areas, discuss the modern challenges and open problems in feature extraction and suggest novel solutions to some of them."
1129,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Wilks' phenomenon and penalized likelihood-ratio test for nonparametric curve registration,"Arnak Dalalyan, Olivier Collier",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/dalalyan12/dalalyan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_dalalyan12,http://jmlr.csail.mit.edu/proceedings/papers/v22/dalalyan12.html,The problem of curve registration appears in many different areas of applications ranging from neuroscience to road traffic modeling. In the present work we propose a nonparametric testing framework in which we develop a generalized likelihood ratio test to perform curve registration. We first prove that under the null hypothesis the resulting test statistic is asymptotically distributed as a chi-squared random variable (Wilks' phenomenon). We also prove that the proposed test is consistent extit{i.e.} its power is asymptotically equal to $1$. Finite sample properties of the proposed methodology are demonstrated by numerical simulations.
1130,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Asymptotically consistent estimation of the number of change points in highly dependent time series,"Azadeh Khaleghi, Daniil Ryabko",none,http://jmlr.org/proceedings/papers/v32/khaleghi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_khaleghi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/khaleghi14.html,"The problem of change point estimation is considered in a general framework where the data are generated by arbitrary unknown stationary ergodic process distributions. This means that the data may have long-range dependencies of an arbitrary form. In this context the consistent estimation of the number of change points is provably impossible. A formulation is proposed which overcomes this obstacle: it is possible to find the correct number of change points at the expense of introducing the additional constraint that the correct number of process distributions that generate the data is provided. This additional parameter has a natural interpretation in many real-world applications. It turns out that in this formulation change point estimation can be reduced to time series clustering. Based on this reduction, an algorithm is proposed that finds the number of change points and locates the changes. This algorithm is shown to be asymptotically consistent. The theoretical results are complemented with empirical evaluations."
1131,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,The Feature Selection Path in Kernel Methods,"Fuxin Li, Cristian Sminchisescu","9:445-452, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/li10a/li10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_li10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/li10a.html,The problem of automatic feature selection/weighting in kernel methods is examined. We work on a formulation that optimizes both the weights of features and the parameters of the kernel model simultaneously using L_1 regularization for feature selection. Under quite general choices of kernels we prove that there exists a unique regularization path for this problem that runs from 0 to a stationary point of the non-regularized problem. We propose an ODE-based homotopy method to follow this trajectory. By following the path our algorithm is able to automatically discard irrelevant features and to automatically go back and forth to avoid local optima. Experiments on synthetic and real datasets show that the method achieves low prediction error and is efficient in separating relevant from irrelevant features.
1132,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Fast and Exact Energy Minimization Algorithm for Cycle MRFs,"Huayan Wang, Koller Daphne",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13f.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13f-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13f,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13f.html,"The presence of cycles gives rise to the difficulty in performing inference for MRFs. Handling cycles efficiently would greatly enhance our ability to tackle general MRFs. In particular, for dual decomposition of energy minimization (MAP inference), using cycle subproblems leads to a much tighter relaxation than using trees, but solving the cycle subproblems turns out to be the bottleneck. In this paper, we present a fast and exact algorithm for energy minimization in cycle MRFs, which can be used as a subroutine in tackling general MRFs. Our method builds on junction-tree message passing, with a large portion of the message entries pruned for efficiency. The pruning conditions fully exploit the structure of a cycle. Experimental results show that our algorithm is more than an order of magnitude faster than other state-of-the-art fast inference methods, and it performs consistently well in several different real problems."
1133,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Improving Predictive Specificity of Description Logic Learners by Fortification,"An Tran, Jens Dietrich, Hans Guesgen, Stephen Marsland","JMLR W&CP 29 :419-434, 2013",http://jmlr.org/proceedings/papers/v29/Tran13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Tran13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Tran13.html,"The predictive accuracy of a learning algorithm can be split into specificity and sensitivity, amongst other decompositions. Sensitivity, also known as completeness, is the ratio of true positives to the total number of positive examples, while specificity is the ratio of true negative to the total negative examples. In top-down learning methods of inductive logic programming, there is generally a bias towards sensitivity, since the learning starts from the most general rule (everything is positive) and specialises by excluding some of the negative examples. While this is often useful, it is not always the best choice: for example, in novelty detection, where the negative examples are rare and often varied, they may well be ignored by the learning. In this paper we introduce a method that attempts to remove the bias towards sensitivity by fortifying the model by computing and then including in the model some descriptions of the negative data even if they are considered redundant by the normal learning algorithm. We demonstrate the method on a set of standard datasets for description logic learning and show that the predictive accuracy increases."
1134,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Expectation Propagation for Rectified Linear Poisson Regression,"Young-Jun Ko, Matthias W. Seeger",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Ko15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Ko15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Ko15.html,"The Poisson likelihood with rectified linear function as non-linearity is a physically plausible model to discribe the stochastic arrival process of photons or other particles at a detector. At low emission rates the discrete nature of this process leads to measurement noise that behaves very differently from additive white Gaussian noise. To address the intractable inference problem for such models, we present a novel efficient and robust Expectation Propagation algorithm entirely based on analytically tractable computations operating reliably in regimes where quadrature based implementations can fail. Full posterior inference therefore becomes an attractive alternative in areas generally dominated by methods of point estimation. Moreover, we discuss the rectified linear function in the context of other common non-linearities and identify situations where it can serve as a robust alternative."
1135,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Statistical-Computational Phase Transitions in Planted Models: The High-Dimensional Setting,"Yudong Chen, Jiaming Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chene14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chene14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chene14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chene14.html,"The planted models assume that a graph is generated from some unknown clusters by randomly placing edges between nodes according to their cluster memberships; the task is to recover the clusters given the graph. Special cases include planted clique, planted partition, planted densest subgraph and planted coloring. Of particular interest is the High-Dimensional setting where the number of clusters is allowed to grow with the number of nodes. We show that the space of model parameters can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the exponential-time Maximum Likelihood Estimator (MLE) succeeds, and no polynomial-time method is known; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a simple counting/thresholding procedure succeeds. Moreover, each of these algorithms provably fails in the previous harder regimes. Our theorems establish the first minimax recovery results for the high-dimensional setting, and provide the best known guarantees for polynomial-time algorithms. Our results extend to the related problem of submatrix localization, a.k.a. bi-clustering. These results demonstrate the tradeoffs between statistical and computational considerations."
1136,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,No more pesky learning rates,"Tom Schaul, Sixin Zhang, Yann LeCun",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/schaul13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/schaul13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_schaul13,http://jmlr.csail.mit.edu/proceedings/papers/v28/schaul13.html,"The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning."
1137,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,An Efficient Approach for Assessing Hyperparameter Importance,"Frank Hutter, Holger Hoos, Kevin Leyton-Brown",none,http://jmlr.org/proceedings/papers/v32/hutter14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hutter14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hutter14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hutter14.html,"The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate thatãeven in very high-dimensional casesãmost performance variation is attributable to just a few hyperparameters."
1138,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Latent Semantic Representation Learning for Scene Classification,"Xin Li, Yuhong Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lid14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lid14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lid14.html,"The performance of machine learning methods is heavily dependent on the choice of data representation. In real world applications such as scene recognition problems, the widely used low-level input features can fail to explain the high-level semantic label concepts. In this work, we address this problem by proposing a novel patch-based latent variable model to integrate latent contextual representation learning and classification model training in one joint optimization framework. Within this framework, the latent layer of variables bridge the gap between inputs and outputs by providing discriminative explanations for the semantic output labels, while being predictable from the low-level input features. Experiments conducted on standard scene recognition tasks demonstrate the efficacy of the proposed approach, comparing to the state-of-the-art scene recognition methods."
1139,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,Stumping along a Summary for Exploration & Exploitation Challenge 2011,"Christophe Salperwyck, Tanguy Urvoy","26:86-97, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/salperwyck12a/salperwyck12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,40726,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_salperwyck12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/salperwyck12a.html,"The Pascal Exploration & Exploitation challenge 2011 seeks to evaluate algorithms for the online website content selection problem. This article presents the solution we used to achieve second place in this challenge and some side-experiments we performed. The methods we evaluated are all structured in three layers. The first layer provides an online summary of the data stream for continuous and nominal data. Continuous data are handled using an online quantile summary. Nominal data are summarized with a hash-based counting structure. With these techniques, we managed to build an accurate stream summary with a small memory footprint. The second layer uses the summary to build predictors. We exploited several kinds of trees from simple decision stumps to deep multivariate ones. For the last layer, we explored several combination strategies: online bagging, exponential weighting, linear ranker, and simple averaging."
1140,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Extended Parameter Filter,"Yusuf Bugra Erol, Lei Li, Bharath Ramsundar, Russell Stuart",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bugraerol13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/bugraerol13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bugraerol13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bugraerol13.html,"The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvikês filter and a Kalman filter in parameter space and establish more general conditions under which Storvikês filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvikês method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods."
1141,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Spectral Compressed Sensing via Structured Matrix Completion,"Yuxin Chen, Yuejie Chi",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13g.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13g-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13g,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13g.html,"The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension \(n\) is assumed to be a mixture of \(r\) complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the basis mismatch issue when imposing a discrete dictionary on the Fourier representation. To address this problem, we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by converting the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of \(\mathcal{O}(r\log^{2} n)\) . We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments."
1142,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,lilê UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits,"Kevin Jamieson, Matthew Malloy, Robert Nowak, S_bastien Bubeck","JMLR W&CP 35 :423-439, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/jamieson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_jamieson14,http://jmlr.csail.mit.edu/proceedings/papers/v35/jamieson14.html,"The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art."
1143,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Transferring Knowledge by Prior Feature Sampling,"Victor Eruhimov, Vladimir Martyanov, Aleksey Polovinkin","4:135-147, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/eruhimov08a/eruhimov08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_eruhimov08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/eruhimov08a.html,The paper presents a novel method for transfer learning through prior variable sampling. A set of problems defined in the same feature space with similar dependencies of target on features is considered. We suggest a method for learning a decision tree ensemble on each of the problems by prior estimation of variable importance on other problems in the set and using it for regularizing model learning for a small amount of training samples. The method is tested on several simulated and real datasets. In particular we apply our method for a set of time series classification (TSC) problems. Our analysis demonstrates an intriguing result: a model trained on several TSC problems can learn a new problem with high accuracy from a low number of samples.
1144,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning a Parametric Embedding by Preserving Local Structure,Laurens van der Maaten,"5:384-391, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/maaten09a/maaten09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_maaten09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/maaten09a.html,The paper presents a new unsupervised dimensionality reduction technique called parametric t-SNE that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space. We evaluate the performance of parametric t-SNE in experiments on two datasets in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE in particular in learning settings in which the dimensionality of the latent space is relatively low.
1145,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Framework for Probability Density Estimation,"John Shawe-Taylor, Alex Dolia","2:468-475, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/shawe-taylor07a/shawe-taylor07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_shawe-taylor07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/shawe-taylor07a.html,The paper introduces a new framework for learning probability density functions. A theoretical analysis suggests that we can tailor a distribution for a class of tasks by training it to fit a small subsample. Experimental evidence is given to support the theoretical analysis.
1146,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Hidden-Unit Conditional Random Fields,"Laurens van der Maaten, Max Welling, Lawrence Saul","15:479-488, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/maaten11b/maaten11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_maaten11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/maaten11b.html,The paper explores a generalization of conditional random fields (CRFs) in which binary stochastic hidden units appear between the data and the labels. Hidden-unit CRFs are potentially more powerful than standard CRFs because they can represent nonlinear dependencies at each frame. The hidden units in these models also learn to discover latent distributed structure in the data that improves classification. We derive efficient algorithms for inference and learning in these models by observing that the hidden units are conditionally independent given the data and the labels. Finally we show that hidden-unit CRFs perform well in experiments on a range of tasks including optical character recognition text classification protein structure prediction and part-of-speech tagging.
1147,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Maximum Likelihood vs. Sequential Normalized Maximum Likelihood in On-line Density Estimation,"Wojciech Kot_owski, Peter Grônwald","19:457-476, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/kotlowski11a/kotlowski11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_kotlowski11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/kotlowski11a.html,The paper considers sequential prediction of individual sequences with log loss (online density estimation) using an exponential family of distributions. We first analyze the regret of the maximum likelihood (``follow the leader'') strategy. We find that this strategy is (1) suboptimal and (2) requires an additional assumption about boundedness of the data sequence. We then show that both problems can be be addressed by adding the currently predicted outcome to the calculation of the maximum likelihood followed by normalization of the distribution. The strategy obtained in this way is known in the literature as the \emph{sequential normalized maximum likelihood} or \emph{last-step minimax} strategy. We show for the first time that for general exponential families the regret is bounded by the familiar $(k/2) \log n$ and thus optimal up to $O(1)$. We also show the relationship to the Bayes strategy with Jeffreys' prior.
1148,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Content-based Image Retrieval with Multinomial Relevance Feedback,Dorota Glowacka and John Shawe-Taylor,"13:111-125, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/glowacka10a/glowacka10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_glowacka10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/glowacka10a.html,The paper considers an interactive search paradigm in which at each round a user is presented with a set of k images and is required to select one that is closest to her target. Performance is measured by the number of rounds needed to identify a specific target image or to find an image among the t nearest neighbours to the target in the database. Building on earlier work we assume a multinomial user model with the probabilities of response proportional to a function of the distance to the target. The conjugate prior Dirichlet distribution is used to model the problem motivating an algorithm that trades exploration and exploitation in presenting the images in each round. Experimental results verify the fit of the model with the problem as well as show that the new approach compares favourably with previous work.
1149,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Feature Extraction for Machine Learning: Logic-Probabilistic Approach,Vladimir Gorodetsky and Vladimir Samoylov,"10:55-65, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/gorodetsky10a/gorodetsky10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_gorodetsky10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/gorodetsky10a.html,The paper analyzes peculiarities of preprocessing of learning data represented in object data bases constituted by multiple relational tables with ontology on top of it. Exactly such learning data structures are peculiar to many novel challenging applications. The paper proposes a new technology supported by a number of novel algorithms intended for ontology-centered transformation of heterogeneous possibly poor structured learning data into homogeneous informative binary feature space based on (1) aggregation of the ontology notion instances and their attribute domains and subsequent probabilistic cause-consequence analysis aimed at extraction more informative features. The proposed technology is fully implemented and validated on several case studies.
1150,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Deterministic Bayesian inference for the p* model,"Haakon Austad, Nial Friel","9:41-48, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/austad10a/austad10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_austad10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/austad10a.html,The p* model is widely used in social network analysis. The likelihood of a network under this model is impossible to calculate for all but trivially small networks. Various approximation have been presented in the literature and the pseudolikelihood approximation is the most popular. The aim of this paper is to introduce two likelihood approximations which have the pseudolikelihood estimator as a special case. We show for the examples that we have considered that both approximations result in improved estimation of model parameters with respect to the standard methodological approaches. We provide a deterministic approach and also illustrate how Bayesian model choice can be carried out in this setting.
1151,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis,"Pinghua Gong, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gonga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/gonga15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gonga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gonga15.html,"The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the \(\ell_1\) -regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to \(\ell_1\) -regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any \(\ell_1\) -regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving \(\ell_1\) -regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN."
1152,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Ladder: A Reliable Leaderboard for Machine Learning Competitions,"Avrim Blum, Moritz Hardt",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/blum15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_blum15,http://jmlr.csail.mit.edu/proceedings/papers/v37/blum15.html,"The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever."
1153,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Double Nystr_m Method: An Efficient and Accurate Nystr_m Scheme for Large-Scale Data Sets,"Woosang Lim, Minhwan Kim, Haesun Park, Kyomin Jung",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lima15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/lima15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lima15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lima15.html,"The Nystr_m method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nystr_m method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nystr_m method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nystr_m methods by evaluating them on a number of real data sets."
1154,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Missing at Random in Graphical Models,Jin Tian,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/tian15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_tian15,http://jmlr.csail.mit.edu/proceedings/papers/v38/tian15.html,"The notion of missing at random (MAR) plays a central role in the theory underlying current methods for handling missing data. However the standard definition of MAR is difficult to interpret in practice. In this paper, we assume the missing data model is represented as a directed acyclic graph that not only encodes the dependencies among the variables but also explicitly portrays the causal mechanisms responsible for the missingness process. We introduce an intuitively appealing notion of MAR in such graphical models, and establish its relation with the standard MAR and a few versions of MAR used in the literature. We address the question of whether MAR is testable, given that data are corrupted by missingness, by proposing a general method for identifying testable implications imposed by the graphical structure on the observed data."
1155,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Achievability of Asymptotic Minimax Regret in Online and Batch Prediction,"Kazuho Watanabe, Teemu Roos, Petri Myllym_ki","JMLR W&CP 29 :181-196, 2013",http://jmlr.org/proceedings/papers/v29/Watanabe13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Watanabe13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Watanabe13.html,"The normalized maximum likelihood model achieves the minimax coding (log-loss) regret for data of fixed sample size \(n\) . However, it is a batch strategy, i.e., it requires that \(n\) be known in advance. Furthermore, it is computationally infeasible for most statistical models, and several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by batch strategies (i.e., strategies that depend on \(n\) ) as well as online strategies (i.e., strategies independent of \(n\) ). On one hand, we conjecture that for a large class of models, no online strategy can be asymptotically minimax. We prove that this holds under a slightly stronger definition of asymptotic minimaxity. Our numerical experiments support the conjecture about non-achievability by so called last-step minimax algorithms, which are independent of \(n\) . On the other hand, we show that in the multinomial model, a Bayes mixture defined by the conjugate Dirichlet prior with a simple dependency on \(n\) achieves asymptotic minimaxity for all sequences, thus providing a simpler asymptotic minimax strategy compared to earlier work by Xie and Barron. The numerical results also demonstrate superior finite-sample behavior by a number of novel batch and online algorithms."
1156,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Causality: Objectives and Assessment,none,"6:1-42, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/guyon10a/guyon10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_guyon10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/guyon10a.html,"The NIPS 2008 workshop on causality provided a forum for researchers from different horizons to share their view on causal modeling and address the difficult question of assessing causal models. There has been a vivid debate on properly separating the notion of causality from particular models such as graphical models, which have been dominating the field in the past few years. Part of the workshop was dedicated to discussing the results of a challenge, which offered a wide variety of applications of causal modeling. We have regrouped in these proceedings the best papers presented. Most lectures were videotaped or recorded. All information regarding the challenge and the lectures are found at http://www.clopinet.com/isabelle/Projects/NIPS2008/ . This introduction provides a synthesis of the findings and a gentle introduction to causality topics, which are the object of active research."
1157,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Deep and Tractable Density Estimator,"Benigno Uria, Iain Murray, Hugo Larochelle",none,http://jmlr.org/proceedings/papers/v32/uria14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_uria14,http://jmlr.csail.mit.edu/proceedings/papers/v32/uria14.html,"The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance."
1158,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Block Regularized Lasso for Multivariate Multi-Response Linear Regression,"Weiguang Wang, Yingbin Liang, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_wang13c,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13c.html,"The multivariate multi-response (MVMR) linear regression problem is investigated, in which design matrices can be distributed differently across \(K\) linear regressions. The support union of \(K\) \(p\) -dimensional regression vectors are recovered via block regularized Lasso which uses the \(l_1/l_2\) norm for regression vectors across \(K\) tasks. Sufficient and necessary conditions to guarantee successful recovery of the support union are characterized. More specifically, it is shown that under certain conditions on the distributions of design matrices, if \(n _ c_{p1} \psi(B^*,\Sigma^{(1:K)})\log(p-s)\) where \(c_{p1}\) is a constant and \(s\) is the size of the support set, then the \(l_1/l_2\) regularized Lasso correctly recovers the support union; and if \(n _ c_{p2} \psi(B^*,\Sigma^{(1:K)})\log(p-s)\) where \(c_{p2}\) is a constant, then the \(l_1/l_2\) regularized Lasso fails to recover the support union. In particular, \(\psi(B^*,\Sigma^{(1:K)})\) captures the sparsity of \(K\) regression vectors and the statistical properties of the design matrices. Numerical results are provided to demonstrate the advantages of joint support union recovery using multi-task Lasso problem over studying each problem individually."
1159,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Large-scale Multi-label Learning with Missing Labels,"Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, Inderjit Dhillon",none,http://jmlr.org/proceedings/papers/v32/yu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yu14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yu14.html,"The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels."
1160,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Analysis of Thompson Sampling for the Multi-armed Bandit Problem,Shipra Agrawal and Navin Goyal,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/agrawal12/agrawal12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_agrawal12,http://jmlr.csail.mit.edu/proceedings/papers/v23/agrawal12.html,"The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret in time T is O ( (ln T ) ˜ _ + 1 ˜ _ 3 ). And, for the stochastic N -armed bandit problem, the expected regret in time T is O ([_ i=2..N 1 ˜ (_ i ) 2 ] ln T ). Our bounds are optimal but for the dependence on _ i and the constant factors in big-Oh."
1161,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Gossip-based distributed stochastic bandit algorithms,"Balazs Szorenyi, Robert Busa-Fekete, Istvan Hegedus, Robert Ormandi, Mark Jelasity, Balazs Kegl",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/szorenyi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/szorenyi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_szorenyi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/szorenyi13.html,"The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks. In our setup, the same set of arms is available in each peer. In every iteration each peer can pull one arm independently of the other peers, and then some limited communication is possible with a few random other peers. As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network. More precisely, we show that the probability of playing a suboptimal arm at a peer in iteration \(t = \Omega( \log N )\) is proportional to \(1/(Nt)\) where \(N\) denotes the number of peers. The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network."
1162,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Lower Bounds for the Gibbs Sampler over Mixtures of Gaussians,"Christopher Tosh, Sanjoy Dasgupta",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/tosh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tosh14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tosh14.html,"The mixing time of a Markov chain is the minimum time \(t\) necessary for the total variation distance between the distribution of the Markov chainês current state \(X_t\) and its stationary distribution to fall below some \(\epsilon _ 0\) . In this paper, we present lower bounds for the mixing time of the Gibbs sampler over Gaussian mixture models with Dirichlet priors."
1163,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,"Rademacher Observations, Private Data, and Boosting","Richard Nock, Giorgio Patrini, Arik Friedman",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/nock15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_nock15,http://jmlr.csail.mit.edu/proceedings/papers/v37/nock15.html,"The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear classifiers, the minimization of the logistic loss is equivalent to the minimization of an exponential rado -loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the same classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be directly used to classify observations . We provide a learning algorithm over rados with boosting-compliant convergence rates on the logistic loss (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the complete set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados brings non-trivial privacy vs accuracy tradeoffs."
1164,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Exploiting Symmetries to Construct Efficient MCMC Algorithms With an Application to SLAM,"Roshan Shariff, Andrˆs Gy_rgy, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/shariff15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/shariff15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_shariff15,http://jmlr.csail.mit.edu/proceedings/papers/v38/shariff15.html,"The Metropolis-Hastings (MH) algorithm is a flexible method to generate samples from a target distribution, a key problem in probabilistic inference. In this paper we propose a variation of the MH algorithm based on group moves, where the next state is obtained by first choosing a random transformation of the state space and then applying this transformation to the current state. This adds much-needed flexibility to the –textbook” MH algorithm where all measures involved must be given in terms of densities with respect to a common reference measure. Under mild conditions, our main result extends the acceptance probability formula of the textbook algorithm to MH algorithms with group moves. We work out how the new algorithms can be used to exploit a problemês natural symmetries and apply the technique to the simultaneous localization and mapping (SLAM) problem, obtaining the first fully rigorous justification of a previous MCMC-based SLAM method. New experimental results comparing our method to existing state-of-the-art specialized methods on a standard range-only SLAM benchmark problem validate the strength of the approach."
1165,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Coding for Random Projections,"Ping Li, Michael Mitzenmacher, Anshumali Shrivastava",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lie14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lie14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lie14.html,"The method of random projections has become popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard and influential method , which used a window-and-random offset scheme. Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a non-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM). Proofs and additional experiments are available at arXiv:1308.2218 . In the context of using coded random projections for approximate near neighbor search by building hash tables ( arXiv:1403.8144 ) , we show that the step of random offset in is again not needed and may hurt the performance. Furthermore, we show that, unless the target similarity level is high, it usually suffices to use only 1 or 2 bits to code each hashed value for this task. Section [sec L SH] presents some experimental results for LSH."
1166,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On Measure Concentration of Random Maximum A-Posteriori Perturbations,"Francesco Orabona, Tamir Hazan, Anand Sarwate, Tommi Jaakkola",none,http://jmlr.org/proceedings/papers/v32/orabona14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/orabona14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_orabona14,http://jmlr.csail.mit.edu/proceedings/papers/v32/orabona14.html,"The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving Monte Carlo estimation of expectations."
1167,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Low Rank Matrix Completion with Exponential Family Noise,Jean Lafond,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Lafond15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Lafond15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Lafond15.html,"The matrix completion problem consists in reconstructing a matrix from a sample of entries, possibly observed with noise. A popular class of estimator, known as nuclear norm penalized estimators, are based on minimizing the sum of a data fitting term and a nuclear norm penalization. Here, we investigate the case where the noise distribution belongs to the exponential family and is sub-exponential. Our framework allows for a general sampling scheme. We first consider an estimator defined as the minimizer of the sum of a log-likelihood term and a nuclear norm penalization and prove an upper bound on the Frobenius prediction risk. The rate obtained improves on previous works on matrix completion for exponential family. When the sampling distribution is known, we propose another estimator and prove an oracle inequality w.r.t. the Kullback-Leibler prediction risk, which translates immediately into an upper bound on the Frobenius prediction risk. Finally, we show that all the rates obtained are minimax optimal up to a logarithmic factor."
1168,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Anomaly Ranking as Supervised Bipartite Ranking,"Stephan Cl_mençon, Sylvain Robbiano",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/clemencon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_clemencon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/clemencon14.html,"The Mass Volume (MV) curve is a visual tool to evaluate the performance of a scoring function with regard to its capacity to rank data in the same order as the underlying density function. Anomaly ranking refers to the unsupervised learning task which consists in building a scoring function, based on unlabeled data, with a MV curve as low as possible at any point. In this paper, it is proved that, in the case where the data generating probability distribution has compact support, anomaly ranking is equivalent to (supervised) bipartite ranking, where the goal is to discriminate between the underlying probability distribution and the uniform distribution with same support. In this situation, the MV curve can be then seen as a simple transform of the corresponding ROC curve. Exploiting this view, we then show how to use bipartite ranking algorithms, possibly combined with random sampling, to solve the MV curve minimization problem. Numerical experiments based on a variety of bipartite ranking algorithms well-documented in the literature are displayed in order to illustrate the relevance of our approach."
1169,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Mapping kernels for in_nite mapping systems,K. Shin,"20:367_382, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/shin11/shin11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_shin11,http://jmlr.csail.mit.edu/proceedings/papers/v20/shin11.html,The mapping kernel is a generalization of HausslerÍs convolution kernel and has a wide range of application including kernels for higher degree structures such as trees. Like HausslerÍs convolution kernel a mapping kernel is a _nite sum of values of a primitive kernel. One of the major reasons to use the mapping kernel template in engineering novel kernels is because a strong theorem is known for positive de_niteness of the resulting mapping kernels. If the mapping kernel meets the transitivity condition and if the primitive kernel is positive de_nite the mapping kernel is also positive de_nite. In this paper we generalize this theorem by showing even when we extend the de_nition of mapping kernels so that a mapping kernel can be a converging sum of countably in_nite primitive kernel values the transitivity condition is still a criteria to determine positive de_niteness of mapping kernels according to the extended de_nition. Interestingly this result is also useful to investigate positive de_niteness of mapping kernels determined as _nite sums when they do not meet the transitivity condition. For this purpose we introduce a general method that we call covering  technique .   Page last modified on Sun Nov 6 15:44:32 2011.
1170,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification,"Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, Xilin Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/huanga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_huanga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/huanga15.html,"The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method."
1171,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Seeking The Truly Correlated Topic Posterior - on tight approximate inference of logistic-normal admixture model,"Amr Ahmed, Eric P. Xing","2:19-26, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/ahmed07a/ahmed07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_ahmed07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/ahmed07a.html,The Logistic-Normal Topic Admixture Model (LoNTAM) also known as correlated topic model (Blei and Lafferty 2005) is a promising and expressive admixture-based text model. It can capture topic correlations via the use of a logistic-normal distribution to model non-trivial variabilities in the topic mixing vectors underlying documents. However the non-conjugacy caused by the logistic-normal makes posterior inference and model learning significantly more challenging. In this paper we present a new tight approximate inference algorithm for LoNTAM based on a multivariate quadratic Taylor approximation scheme that facilitates elegant closed-form message passing. We present experimental results on simulated data as well as on the NIPS17 and PNAS document collections and show that our approach is not only simple and easy to implement but also it converges faster and leads to more accurate recovery of the semantic truth underlying documents and estimates of the parameters comparing to previous methods.
1172,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Logistic Regression: Tight Bounds for Stochastic and Online Optimization,"Elad Hazan, Tomer Koren, Kfir Y. Levy","JMLR W&CP 35 :197-209, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/hazan14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_hazan14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/hazan14a.html,"The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012)."
1173,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Estimating beta-mixing coefficients,"Daniel McDonald, Cosma Shalizi, Mark Schervish","15:516-524, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/mcdonald11a/mcdonald11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mcdonald11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/mcdonald11a.html,"The literature on statistical learning for time series assumes the asymptotic independence or """"mixing"""" of the data-generating process. These mixing assumptions are never tested nor are there methods for estimating mixing rates from data. We give an estimator for the beta-mixing rate based on a single stationary sample path and show it is L1-risk consistent."
1174,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,LDA Topic Model with Soft Assignment of Descriptors to Words,"Daphna Weinshall, Gal Levi, Dmitri Hanukaev",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/weinshall13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_weinshall13,http://jmlr.csail.mit.edu/proceedings/papers/v28/weinshall13.html,"The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented more naturally by bags of continuous descriptors. Given a finite dictionary of words which are generative models of descriptors, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudo-counts, reflecting the fact that with soft assignment words are not independent. We use this approach to address novelty detection, where we seek to identify video events with low posterior probability. Video events are described by a generative dynamic texture model, from which we naturally derive a dictionary of generative words. Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results."
1175,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Real-time data analysis at the LHC: present and future,Vladimir Gligorov,none,http://jmlr.csail.mit.edu/proceedings/papers/v42/glig14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_glig14,http://jmlr.csail.mit.edu/proceedings/papers/v42/glig14.html,"The Large Hadron Collider (LHC), which collides protons at an energy of 14 TeV, produces hundreds of exabytes of data per year, making it one of the largest sources of data in the world today. At present it is not possible to even transfer most of this data from the four main particle detectors at the LHC to –offline” data facilities, much less to permanently store it for future processing. For this reason the LHC detectors are equipped with real-time analysis systems, called triggers, which process this volume of data and select the most interesting proton-proton ( \(pp\) ) collisions. The LHC experiment triggers reduce the data produced by the LHC by between 1/1000 and 1/100000, to tens of petabytes per year, allowing its economical storage and further analysis. The bulk of the data-reduction is performed by custom electronics which ignores most of the data in its decision making, and is therefore unable to exploit the most powerful known data analysis strategies. I cover the present status of real-time data analysis at the LHC, before explaining why the future upgrades of the LHC experiments will increase the volume of data which can be sent off the detector and into off-the-shelf data processing facilities (such as CPU or GPU farms) to tens of exabytes per year. This development will simultaneously enable a vast expansion of the physics programme of the LHCês detectors, and make it mandatory to develop and implement a new generation of real-time multivariate analysis tools in order to fully exploit this new potential of the LHC. I explain what work is ongoing in this direction and motivate why more effort is needed in the coming years."
1176,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Bridging the Language Gap: Topic Adaptation for Documents with Different Technicality,"Shuang_Hong Yang, Steven Crain, Hongyuan Zha","15:823-831, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/yang11b/yang11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_yang11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/yang11b.html,The language-gap for example between low-literacy laypersons and highly-technical experts is a fundamental barrier for cross-domain knowledge transfer. This paper seeks to close the gap at the thematic level via topic adaptation i.e. adjusting topical structures for cross-domain documents according to a domain factor such as technicality. We present a probabilistic model for this purpose based on joint modeling of topic and technicality. The proposed tLDA model explicitly encodes the interplay between topic and technicality hierarchies providing an effective topic-bridge between lay and expert documents. We demonstrate the usefulness of tLDA with an application to consumer medical informatics.
1177,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Robust and Discriminative Self-Taught Learning,"Hua Wang, Feiping Nie, Heng Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13g.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13g-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13g,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13g.html,"The lack of training data is a common challenge in many machine learning problems, which is often tackled by semi-supervised learning methods or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data. We derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach."
1178,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Divide-and-Conquer Solver for Kernel Support Vector Machines,"Cho-Jui Hsieh, Si Si, Inderjit Dhillon",none,http://jmlr.org/proceedings/papers/v32/hsieha14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsieha14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hsieha14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hsieha14.html,"The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10 -6 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM."
1179,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Consistency of Robust Kernel Density Estimators,"Robert Vandermeulen, Clayton Scott",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Vandermeulen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Vandermeulen13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Vandermeulen13.html,The kernel density estimator (KDE) based on a radial positive-semidefinite kernel may be viewed as a sample mean in a reproducing kernel Hilbert space. This mean can be viewed as the solution of a least squares problem in that space. Replacing the squared loss with a robust loss yields a robust kernel density estimator (RKDE). Previous work has shown that RKDEs are weighted kernel density estimators which have desirable robustness properties. In this paper we establish asymptotic \(L^1\) consistency of the RKDE for a class of losses and show that the RKDE converges with the same rate on bandwidth required for the traditional KDE. We also present a novel proof of the consistency of the traditional KDE.
1180,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Randomized partition trees for exact nearest neighbor search,"Sanjoy Dasgupta, Kaushik Sinha",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Dasgupta13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Dasgupta13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Dasgupta13.html,"The k-d tree was one of the first spatial data structures proposed for nearest neighbor search. Its efficacy is diminished in high-dimensional spaces, but several variants, with randomization and overlapping cells, have proved to be successful in practice. We analyze three such schemes. We show that the probability that they fail to find the nearest neighbor, for any data set and any query point, is directly related to a simple potential function that captures the difficulty of the point configuration. We then bound this potential function in two situations of interest: the first, when data come from a doubling measure, and the second, when the data are documents from a topic model."
1181,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,The Block Diagonal Infinite Hidden Markov Model,"Thomas Stepleton, Zoubin Ghahramani, Geoffrey Gordon, Tai-Sing Lee","5:552-559, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/stepleton09a/stepleton09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_stepleton09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/stepleton09a.html,"The Infinite Hidden Markov Model (IHMM) extends hidden Markov models to have a countably infinite number of hidden states \cite{ihmmhdp}. We present a generalization of this framework that introduces block-diagonal structure in the transitions between the hidden states. These blocks correspond to ""sub-behaviors"" exhibited by data sequences. In identifying such structure the model classifies or partitions sequence data according to these sub-behaviors in an unsupervised way. We present an application of this model to artificial data a video gesture classification task and a musical theme labeling task and show that components of the model can also be applied to graph segmentation."
1182,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Variational Inference for the Indian Buffet Process,"Finale Doshi, Kurt Miller, Jurgen Van Gael, Yee Whye Teh","5:137-144, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/doshi09a/doshi09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_doshi09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/doshi09a.html,The Indian Buffet Process (IBP) is a nonparametric prior for latent feature models in which observations are influenced by a combination of several hidden features. For example images may be composed of several objects or sounds may consist of several notes. Latent feature models seek to infer what these latent features from a set of observations. Current inference methods for the IBP have all relied on sampling. While these methods are guaranteed to be accurate in the limit in practice samplers for the IBP tend to mix slow. We develop a deterministic variational method for the IBP. We provide theoretical guarantees on its truncation bounds and demonstrate its superior performance for high dimensional data sets.
1183,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Stick-breaking Construction for the Indian Buffet Process,"Yee Whye Teh, Dilan Grôr, Zoubin Ghahramani","2:556-563, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/teh07a/teh07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_teh07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/teh07a.html,The Indian buffet process (IBP) is a Bayesian nonparametric distribution whereby objects are modelled using an unbounded number of latent features. In this paper we derive a stick-breaking representation for the IBP. Based on this new representation we develop slice samplers for the IBP that are efficient easy to implement and are more generally applicable than the currently available Gibbs sampler. This representation along with the work of Thibaux and Jordan [17] also illuminates interesting theoretical connections between the IBP Chinese restaurant processes Beta processes and Dirichlet processes.
1184,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Statistical Model for Event Sequence Data,"Kevin Heins, Hal Stern","JMLR W&CP 33 :338-346, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/heins14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_heins14,http://jmlr.csail.mit.edu/proceedings/papers/v33/heins14.html,"The identification of recurring patterns within a sequence of events is an important task in behavior research. In this paper, we consider a general probabilistic framework for identifying such patterns, by distinguishing between events that belong to a pattern and events that occur as part of background processes. The event processes, both for background events and events that are part of recurring patterns, are modeled as competing renewal processes. Using this framework, we develop an inference procedure to detect the sequences present in observed data. Our method is compared to a current approach used within the ethology literature on both simulated data and data collected to study the impact of fragmented and unpredictable maternal behavior on cognitive development of children."
1185,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Transductive Classification via Local Learning Regularization,"Mingrui Wu, Bernhard Scholkopf","2:628-635, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/wu07a/wu07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_wu07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/wu07a.html,The idea of local learning classifying a particular point based on its neighbors has been successfully applied to supervised learning problems. In this paper we adapt it for Transductive Classification (TC) problems. Specifically we formulate a Local Learning Regularizer (LL-Reg) which leads to a solution with the property that the label of each data point can be well predicted based on its neighbors and their labels. For model selection an efficient way to compute the leave-one-out classification error is provided for the proposed and related algorithms. Experimental results using several benchmark datasets illustrate the effectiveness of the proposed approach.
1186,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Bigraphical Lasso,"Alfredo Kalaitzis, John Lafferty, Neil Lawrence, Shuheng Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kalaitzis13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/kalaitzis13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kalaitzis13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kalaitzis13.html,"The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions."
1187,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,The Higgs boson machine learning challenge,"Claire Adam-Bourdarios, Glen Cowan, C_cile Germain, Isabelle Guyon, Bal_zs K_gl, David Rousseau",none,http://jmlr.csail.mit.edu/proceedings/papers/v42/cowa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_cowa14,http://jmlr.csail.mit.edu/proceedings/papers/v42/cowa14.html,"The Higgs Boson Machine Learning Challenge (HiggsML or the Challenge for short) was organized to promote collaboration between high energy physicists and data scientists. The ATLAS experiment at CERN provided simulated data that has been used by physicists in a search for the Higgs boson. The Challenge was organized by a small group of ATLAS physicists and data scientists. It was hosted by Kaggle at https://www.kaggle.com/c/higgs-boson ; the challenge data is now available on \opendataLink . This paper provides the physics background and explains the challenge setting, the challenge design, and analyzes its results."
1188,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Online Variational Inference for the Hierarchical Dirichlet Process,"Chong Wang, John Paisley, David Blei","15:752-760, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_wang11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a.html,The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a potentially infinite number of components. It has been applied widely in probabilistic topic modeling where the data are documents and the components are distributions of terms that reflect recurring patterns (or ``topics'') in the collection. Given a document collection posterior inference is used to determine the number of topics needed and to characterize their distributions. One limitation of HDP analysis is that existing posterior inference algorithms require multiple passes through all the data---these algorithms are intractable for very large scale applications. We propose an online variational inference algorithm for the HDP an algorithm that is easily applicable to massive and streaming data. Our algorithm is significantly faster than traditional inference algorithms for the HDP and lets us analyze much larger data sets. We illustrate the approach on two large collections of text showing improved performance over online LDA the finite counterpart to the HDP topic model.
1189,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Approval Voting and Incentives in Crowdsourcing,"Nihar Shah, Dengyong Zhou, Yuval Peres",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/shaha15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/shaha15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_shaha15,http://jmlr.csail.mit.edu/proceedings/papers/v37/shaha15.html,"The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (–strictly proper”) incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach."
1190,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Greedy Homotopy Method for Regression with Nonconvex Constraints,"Fabian Wauthier, Peter Donnelly",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wauthier15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/wauthier15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wauthier15,http://jmlr.csail.mit.edu/proceedings/papers/v38/wauthier15.html,"The goal of this paper is to estimate sparse linear regression models, where for a given partition \(\mathcal{G}\) of input variables, the selected variables are chosen from a diverse set of groups in \(\mathcal{G}\) . We consider a novel class of nonconvex constraint functions, and develop RepLasso, a greedy homotopy method that exploits geometrical properties of the constraint functions to build a sequence of suitably adapted convex surrogate problems. We prove that in some situations RepLasso recovers the global minima path of the nonconvex problem. Moreover, even if it does not recover the global minima, we prove that it will often do no worse than the Lasso in terms of (signed) support recovery, while in practice outperforming it. We show empirically that the strategy can also be used to improve over various other Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitis highlights our methodês practical utility."
1191,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Recurrent Convolutional Neural Networks for Scene Labeling,"Pedro Pinheiro, Ronan Collobert",none,http://jmlr.org/proceedings/papers/v32/pinheiro14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_pinheiro14,http://jmlr.csail.mit.edu/proceedings/papers/v32/pinheiro14.html,"The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time."
1192,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sparsity-Based Generalization Bounds for Predictive Sparse Coding,"Nishant Mehta, Alexander Gray",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/mehta13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/mehta13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_mehta13,http://jmlr.csail.mit.edu/proceedings/papers/v28/mehta13.html,"The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations."
1193,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Exp-Concavity of Proper Composite Losses,"Parameswaran Kamalaruban, Robert Williamson, Xinhua Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kamalaruban15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kamalaruban15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kamalaruban15.html,"The goal of online prediction with expert advice is to find a decision strategy which will perform almost as well as the best expert in a given pool of experts, on any sequence of outcomes. This problem has been widely studied and \(O(\sqrt{T})\) and \(O(\log{T})\) regret bounds can be achieved for convex losses and strictly convex losses with bounded first and second derivatives respectively. In special cases like the Aggregating Algorithm with mixable losses and the Weighted Average Algorithm with exp-concave losses, it is possible to achieve \(O(1)\) regret bounds. But mixability and exp-concavity are roughly equivalent under certain conditions. Thus by understanding the underlying relationship between these two notions we can gain the best of both algorithms (strong theoretical performance guarantees of the Aggregating Algorithm and the computational efficiency of the Weighted Average Algorithm). In this paper we provide a complete characterization of the exp-concavity of any proper composite loss. Using this characterization and the mixability condition of proper losses, we show that it is possible to transform (re-parameterize) any \(\beta\) -mixable binary proper loss into a \(\beta\) -exp-concave composite loss with the same \(\beta\) . In the multi-class case, we propose an approximation approach for this transformation."
1194,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning with Marginalized Corrupted Features,"Laurens Van der Maaten, Minmin Chen, Stephen Tyree, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/vandermaaten13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_vandermaaten13,http://jmlr.csail.mit.edu/proceedings/papers/v28/vandermaaten13.html,"The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples _ which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution _ essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time."
1195,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A Last-Step Regression Algorithm for Non-Stationary Online Learning,"Edward Moroshko, Koby Crammer",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/moroshko13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_moroshko13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/moroshko13a.html,"The goal of a learner in standard online learning is to maintain an average loss close to the loss of the best-performing single function in some class. In many real-world problems, such as rating or ranking items, there is no single best target function during the runtime of the algorithm, instead the best (local) target function is drifting over time. We develop a novel last step minmax optimal algorithm in context of a drift. We analyze the algorithm in the worst-case regret framework and show that it maintains an average loss close to that of the best slowly changing sequence of linear functions, as long as the total of drift is sublinear. In some situations, our bound improves over existing bounds, and additionally the algorithm suffers logarithmic regret when there is no drift. We also build on the H1 filter and its bound, and develop and analyze a second algorithm for drifting setting. Synthetic simulations demonstrate the advantages of our algorithms in a worst-case constant drift setting."
1196,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Dynamic Factorization Tests: Applications to Multi-modal Data Association,"Michael R. Siracusa, John W. Fisher III","2:508-515, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/siracusa07a/siracusa07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_siracusa07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/siracusa07a.html,The goal of a dynamic dependency test is to correctly label the interaction of multiple observed data streams and to describe how this interaction evolves over time. To this end we propose the use of a hidden factorization Markov model (HFactMM) in which a hidden state indexes into a finite set of possible dependence structures on observations. We show that a dynamic dependency test using an HFactMM takes advantage of both structural and parametric changes associated with changes in interaction. This is contrasted both theoretically and empirically with standard sliding window based dependence analysis. Using this model we obtain state-ofthe-art performance on an audio-visual association task without the benefit of labeled training data.
1197,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Sparsistency of the Edge Lasso over Graphs,"James Sharpnack, Aarti Singh, Alessandro Rinaldo",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sharpnack12/sharpnack12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sharpnack12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sharpnack12.html,The fused lasso was proposed recently to enable recovery of high-dimensional patterns which are piece-wise constant on a graph by penalizing the $\ell_1$-norm of differences of measurements at vertices that share an edge. While there have been some attempts at coming up with efficient algorithms for solving the fused lasso optimization a theoretical analysis of its performance is mostly lacking except for the simple linear graph topology. In this paper we investigate {\em sparsistency} of fused lasso for general graph structures i.e. its ability to correctly recover the exact support of piece-wise constant graph-structured patterns asymptotically (for large-scale graphs). To emphasize this distinction over previous work we will refer to it as Edge Lasso. We focus on the (structured) normal means setting and our results provide necessary and sufficient conditions on the graph properties as well as the signal-to-noise ratio needed to ensure sparsistency. We examplify our results using simple graph-structured patterns and demonstrate that in some cases fused lasso is sparsistent at very weak signal-to-noise ratios (scaling as $\sqrt{(\log n)/|A|}$ where $n$ is the number of vertices in the graph and $A$ is the smallest set of vertices with constant activation). In other cases it performs no better than thresholding the difference of measurements at vertices which share an edge (which requires signal-to-noise ratio that scales as $\sqrt{\log n}$).
1198,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Optimal learners for multiclass problems,"Amit Daniely, Shai Shalev-Shwartz","JMLR W&CP 35 :287-316, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/daniely14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_daniely14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/daniely14b.html,"The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper , namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundamental question of –how to learn”? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et el (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005)"
1199,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Bayesian Analysis of the Radioactive Releases of Fukushima,"Ryota Tomioka, Morten Mrup",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/tomioka12/tomioka12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_tomioka12,http://jmlr.csail.mit.edu/proceedings/papers/v22/tomioka12.html,The Fukushima Daiichi disaster 11 March 2011 is considered the largest nuclear accident since the 1986 Chernobyl disaster and has been rated at level 7 on the International Nuclear Event Scale. As different radioactive materials have different effects to human body it is important to know the types of nuclides and their levels of concentration from the recorded mixture of radiations to well take necessary measures. We presently formulate a Bayesian generative model for the data available on radioactive releases from the Fukushima Daiichi disaster across Japan. The model can infer from the sparsely sampled measurements what nuclides are present as well as their concentration levels. An important property of the proposed model is that it admits unique recovery of the parameters. On synthetic data we demonstrate that our model is able to infer the underlying components and on data from the Fukushima Daiichi plant we establish that the model is able to well account for the data. We further demonstrate how the model extends to include all the available measurements recorded throughout Japan. The model can be considered a first attempt to apply Bayesian learning unsupervised in order to give a more detailed account also of the latent structure present in the data of the Fukushima Daiichi disaster.
1200,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets,"Dan Garber, Elad Hazan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/garbera15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/garbera15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_garbera15,http://jmlr.csail.mit.edu/proceedings/papers/v37/garbera15.html,"The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of \(\frac{1}{t^2}\) . This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order \(\frac{1}{t}\) , and known to be tight. We show that various balls induced by \(\ell_p\) norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis."
1201,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Making Fisher Discriminant Analysis Scalable,"Bojun Tu, Zhihua Zhang, Shusen Wang, Hui Qian",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/tu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/tu14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tu14.html,"The Fisher linear discriminant analysis (LDA) is a classical method for classification and dimension reduction jointly. A major limitation of the conventional LDA is a so-called singularity issue. Many LDA variants, especially two-stage methods such as PCA+LDA and LDA/QR, were proposed to solve this issue. In the two-stage methods, an intermediate stage for dimension reduction is developed before the actual LDA method works. These two-stage methods are scalable because they are an approximate alternative of the LDA method. However, there is no theoretical analysis on how well they approximate the conventional LDA problem. In this paper we present theoretical analysis on the approximation error of a two-stage algorithm. Accordingly, we develop a new two-stage algorithm. Furthermore, we resort to a random projection approach, making our algorithm scalable. We also provide an implemention on distributed system to handle large scale problems. Our algorithm takes LDA/QR as its special case, and outperforms PCA+LDA while having a similar scalability. We also generalize our algorithm to kernel discriminant analysis, a nonlinear version of the classical LDA. Extensive experiments show that our algorithms outperform PCA+LDA and have a similar scalability with it."
1202,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Global Optimization Methods for Extended Fisher Discriminant Analysis,"Satoru Iwata, Yuji Nakatsukasa, Akiko Takeda","JMLR W&CP 33 :411-419, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/iwata14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/iwata14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_iwata14,http://jmlr.csail.mit.edu/proceedings/papers/v33/iwata14.html,"The Fisher discriminant analysis (FDA) is a common technique for binary classification. A parametrized extension, which we call the extended FDA, has been introduced from the viewpoint of robust optimization. In this work, we first give a new probabilistic interpretation of the extended FDA. We then develop algorithms for solving an optimization problem that arises from the extended FDA: computing the distance between a point and the surface of an ellipsoid. We solve this problem via the KKT points, which we show are obtained by solving a generalized eigenvalue problem. We speed up the algorithm by taking advantage of the matrix structure and proving that a globally optimal solution is a KKT point with the smallest Lagrange multiplier, which can be computed efficiently as the leftmost eigenvalue. Numerical experiments illustrate the efficiency and effectiveness of the extended FDA model combined with our algorithm."
1203,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning the Switching Rate by Discretising Bernoulli Sources Online,"Steven de Rooij, Tim van Erven","5:432-439, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/rooij09a/rooij09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_rooij09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/rooij09a.html,The expert tracking algorithm Fixed-Share depends on a parameter alpha called the switching rate. If the final number of outcomes T is known in advance then the switching rate can be learned with regret 1/2 log T + O(1) bits. The current fastest method that achieves this Learn-alpha is based on optimal discretisation of the Bernoulli distributions into O(ÌT) bins and runs in O(TÌT) time; however the exact locations of these points have to be determined algorithmically. This paper introduces a new discretisation scheme with the same regret bound for known T that specifies the number and positions of the discretisation points explicitly. The scheme is especially useful when T is not known in advance: a new fully on-line algorithm Refine-Online is presented which runs in O(TÌT log T) time and achieves a regret of 1/2 log 3 log T + O(log log T) bits.
1204,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Age-Layered Expectation Maximization for Parameter Learning in Bayesian Networks,"Avneesh Saluja, Priya Krishnan Sundararajan, Ole J Mengshoel",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/saluja12/saluja12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_saluja12,http://jmlr.csail.mit.edu/proceedings/papers/v22/saluja12.html,The expectation maximization (EM) algorithm is a popular algorithm for parameter estimation in models with hidden variables. However the algorithm has several non-trivial limitations a significant one being variation in eventual solutions found due to convergence to local optima. Several techniques have been proposed to allay this problem for example initializing EM from multiple random starting points and selecting the highest likelihood out of all runs. In this work we a) show that this method can be very expensive computationally for difficult Bayesian networks and b) in response we propose an age-layered EM approach (ALEM) that efficiently discards less promising runs well before convergence. Our experiments show a significant reduction in the number of iterations typically two- to four-fold with minimal or no reduction in solution quality indicating the potential for ALEM to streamline parameter estimation in Bayesian networks.
1205,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Honest Compressions and Their Application to Compression Schemes,"Roi Livni, Pierre Simon",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Livni13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Livni13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Livni13.html,"The existence of a compression scheme for every concept class with bounded VC-dimension is one of the oldest open problems in statistical learning theory. Here we demonstrate the existence of such compression schemes under stronger assumptions than finite VC-dimension. Specifically, for each concept class we associate a family of concept classes that we call the alternating concept classes. Under the assumption that these concept classes have bounded VC dimension, we prove existence of a compression scheme. This result is motivated by recent progress in the field of model theory with respect to an analogues problem. In fact, our proof can be considered as a constructive proof of these advancements. This means that we describe the reconstruction function explicitly. Not less important, the theorems and proofs we present are in purely combinatorial terms and are available to the reader who is unfamiliar with model theory. Also, using tools from model theory, we apply our results and prove existence of compression schemes in interesting cases, such as concept classes defined by hyperplanes, polynomials, exponentials, restricted analytic functions and compositions, additions and multiplications of all of the above."
1206,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Data-driven covariate selection for nonparametric estimation of causal effects,"Doris Entner, Patrik Hoyer, Peter Spirtes",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/entner13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/entner13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_entner13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/entner13a.html,"The estimation of causal effects from non-experimental data is a fundamental problem in many fields of science. One of the main obstacles concerns confounding by observed or latent covariates, an issue which is typically tackled by adjusting for some set of observed covariates. In this contribution, we analyze the problem of inferring whether a given variable has a causal effect on another and, if it does, inferring an adjustment set of covariates that yields a consistent and unbiased estimator of this effect, based on the (conditional) independence and dependence relationships among the observed variables. We provide two elementary rules that we show to be both sound and complete for this task, and compare the performance of a straightforward application of these rules with standard alternative procedures for selecting adjustment sets."
1207,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Best Agglomerative Ranked Subset for Feature Selection,"Roberto Ruiz, Jos_ C. Riquelme, Jesìs S. Aguilar-Ruiz","4:148-162, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/ruiz08a/ruiz08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_ruiz08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/ruiz08a.html,The enormous increase of the size in databases makes finding an optimal subset of features extremely difficult. In this paper a new feature selection method is proposed that will allow any subset evaluator -including the wrapper evaluation method- to be used to find a group of features that will allow a distinction to be made between the different possible classes. The method BARS (Best Agglomerative Ranked Subset) is based on the idea of relevance and redundancy in the sense that a ranked feature (or set) is more relevant if it adds information when it is included in the final subset of selected features. This heuristic method reduces dimensionality drastically and leads to improvements in the accuracy in comparison to a complete set and as opposed to other feature selection algorithms.
1208,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning invariant features by harnessing the aperture problem,"Roland Memisevic, Georgios Exarchakis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/memisevic13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_memisevic13,http://jmlr.csail.mit.edu/proceedings/papers/v28/memisevic13.html,"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset."
1209,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Atomic Spatial Processes,"Sean Jewell, Neil Spencer, Alexandre Bouchard-C»t_",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/jewell15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/jewell15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_jewell15,http://jmlr.csail.mit.edu/proceedings/papers/v37/jewell15.html,"The emergence of compact GPS systems and the establishment of open data initiatives has resulted in widespread availability of spatial data for many urban centres. These data can be leveraged to develop data-driven intelligent resource allocation systems for urban issues such as policing, sanitation, and transportation. We employ techniques from Bayesian non-parametric statistics to develop a process which captures a common characteristic of urban spatial datasets. Specifically, our new spatial process framework models events which occur repeatedly at discrete spatial points, the number and locations of which are unknown a priori. We develop a representation of our spatial process which facilitates posterior simulation, resulting in an interpretable and computationally tractable model. The frameworkês superiority over both empirical grid-based models and Dirichlet process mixture models is demonstrated by fitting, interpreting, and comparing models of graffiti prevalence for both downtown Vancouver and Manhattan."
1210,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Vector-Valued Property Elicitation,"Rafael Frongillo, Ian A. Kash",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Frongillo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Frongillo15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Frongillo15.html,"The elicitation of a statistic, or property of a distribution, is the task of devising proper scoring rules, equivalently proper losses, which incentivize an agent or algorithm to truthfully estimate the desired property of the underlying probability distribution or data set. Leveraging connections between elicitation and convex analysis, we address the vector-valued property case, which has received little attention in the literature despite its applications to both machine learning and statistics. We first provide a very general characterization of linear and ratio-of-linear properties, the first of which resolves an open problem by unifying and strengthening several previous characterizations in machine learning and statistics. We then ask which vectors of properties admit nonseparable scores, which cannot be expressed as a sum of scores for each coordinate separately, a natural desideratum for machine learning. We show that linear and ratio-of-linear do admit nonseparable scores, and provide evidence for a conjecture that these are the only such properties (up to link functions). Finally, we give a general method for producing identification functions and address an open problem by showing that convex maximal level sets are insufficient for elicitability in general."
1211,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sparse Dueling Bandits,"Kevin Jamieson, Sumeet Katariya, Atul Deshpande, Robert Nowak",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/jamieson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/jamieson15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_jamieson15,http://jmlr.csail.mit.edu/proceedings/papers/v38/jamieson15.html,"The dueling bandit problem is a variation of the classical multi-armed bandit in which the allowable actions are noisy comparisons between pairs of arms. This paper focuses on a new approach for finding the best arm according to the Borda criterion using noisy comparisons. We prove that in the absence of structural assumptions, the sample complexity of this problem is proportional to the sum of the inverse gaps squared of the Borda scores of each arm. We explore this dependence further and consider structural constraints on the pairwise comparison matrix (a particular form of sparsity natural to this problem) that can significantly reduce the sample complexity. This motivates a new algorithm called Successive Elimination with Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner using fewer samples than standard algorithms. We also evaluate the new algorithm experimentally with synthetic and real data. The results show that the sparsity model and the new algorithm can provide significant improvements over standard approaches."
1212,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Impossibility Theorems for Domain Adaptation,"Shai Ben David, Tyler Lu, Teresa Luu, David Pal","9:129-136, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/david10a/david10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_david10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/david10a.html,The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed. We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) Similarity between the unlabeled distributions (ii) Existence of a classifier in the hypothesis class with low error on both training and testing distributions and (iii) The covariate shift assumption. I.e. the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions. We show that without either assumption (i) or (ii) the combination of the remaining assumptions is not sufficient toguarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm as long as it does not have access to target labeled examples. In particular we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions. We also discuss the intuitively appealing paradigm of reweighing the labeled training sample according to the target unlabeled distribution. We show that somewhat counter intuitively that paradigm cannot be trusted in the following sense. There are DA tasks that are indistinguishable as far as the input training data goes but in which reweighing leads to significant improvement in one task while causing dramatic deterioration of the learning success in the other.
1213,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Higgs Boson Discovery with Boosted Trees,"Tianqi Chen, Tong He",none,http://jmlr.csail.mit.edu/proceedings/papers/v42/chen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_chen14,http://jmlr.csail.mit.edu/proceedings/papers/v42/chen14.html,"The discovery of the Higgs boson is remarkable for its importance in modern Physics research. The next step for physicists is to discover more about the Higgs boson from the data of the Large Hadron Collider (LHC). A fundamental and challenging task is to extract the signal of Higgs boson from background noises. The machine learning technique is one important component in solving this problem. In this paper, we propose to solve the Higgs boson classification problem with a gradient boosting approach. Our model learns ensemble of boosted trees that makes careful tradeoff between classification error and model complexity. Physical meaningful features are further extracted to improve the classification accuracy. Our final solution obtained an AMS of 3.71885 on the private leaderboard, making us the top 2% in the Higgs boson challenge."
1214,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Active Learning Using Smooth Relative Regret Approximations with Applications,"Nir Ailon, Ron Begleiter and Esther Ezra",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/ailon12/ailon12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_ailon12,http://jmlr.csail.mit.edu/proceedings/papers/v23/ailon12.html,"The disagreement coefficient of Hanneke has become a central concept in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones. We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only fails to guarantee any useful bounds for these problems. The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypotheses and some fixed ""pivotal"" hypothesis to within an absolute error of at most _ times the l 1 distance (the disagreement measure) between the two hypotheses. We prove that such an estimator implies the existence of a learning algorithm which, at each iteration, reduces its excess risk to within a constant factor. Each iteration replaces the current pivotal hypothesis with the minimizer of the estimated loss difference function with respect to the previous pivotal hypothesis. The label complexity essentially becomes that of computing this estimator. The two applications of interest are: learning to rank from pairwise preferences, and clustering with side information (a.k.a. semi-supervised clustering). They are both fundamental, and have started receiving more attention from active learning theoreticians and practitioners. Keywords: active learning, learning to rank from pairwise preferences, semi-supervised clustering, clustering with side information, disagreement coefficient, smooth relative regret approximation."
1215,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Covering Number for Efficient Heuristic-based POMDP Planning,"Zongzhang Zhang, David Hsu, Wee Sun Lee",none,http://jmlr.org/proceedings/papers/v32/zhanga14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhanga14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhanga14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhanga14.html,"The difficulty of POMDP planning depends on the size of the search space involved. Heuristics are often used to reduce the search space size and improve computational efficiency; however, there are few theoretical bounds on their effectiveness. In this paper, we use the covering number to characterize the size of the search space reachable under heuristics and connect the complexity of POMDP planning to the effectiveness of heuristics. With insights from the theoretical analysis, we have developed a practical POMDP algorithm, Packing-Guided Value Iteration (PGVI). Empirically, PGVI is competitive with the state-of-the-art point-based POMDP algorithms on 65 small benchmark problems and outperforms them on 4 larger problems."
1216,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Stick-Breaking Likelihood for Categorical Data Analysis with Latent Gaussian Models,"Mohammad Khan, Shakir Mohamed, Benjamin Marlin, Kevin Murphy",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/khan12/khan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_khan12,http://jmlr.csail.mit.edu/proceedings/papers/v22/khan12.html,The development of accurate models and efficient algorithms for the analysis of multivariate categorical data are important and long-standing problems in machine learning and computational statistics. In this paper we focus on modeling categorical data using Latent Gaussian Models (LGMs). We propose a novel stick-breaking likelihood function for categorical LGMs that exploits accurate linear and quadratic bounds on the logistic log-partition function leading to an effective variational inference and learning framework. We thoroughly compare our approach to existing algorithms for multinomial logit/probit likelihoods on several problems including inference in multinomial Gaussian process classification and learning in latent factor models. Our extensive comparisons demonstrate that our stick-breaking model effectively captures correlation in discrete data and is well suited for the analysis of categorical data.
1217,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Improving Model Inference of Black Box Components having Large Input Test Set,"Muhammad Naeem Irfan, Roland Groz and Catherine Oriat","21:133-138, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/irfan12a/irfan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_irfan12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/irfan12a.html,The deterministic finite automata (DFA) learning algorithm L* has been extended to learn Mealy machine models which are more succinct for input/output (i/o) based systems. We propose an optimized learning algorithm L 1 to infer Mealy models of software black box components. The L 1 algorithm uses a modified observation table and avoids adding unnecessary elements to its columns and rows. The proposed improvements reduce the worst case time complexity. The L 1 algorithm is compared with the existing Mealy inference algorithms and the experiments conducted on a comprehensive set confirm the gain.
1218,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Deep Boltzmann Machines as Feed-Forward Hierarchies,"Gregoire Montavon, Mikio Braun, Klaus-Robert Muller",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/montavon12/montavon12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_montavon12,http://jmlr.csail.mit.edu/proceedings/papers/v22/montavon12.html,The deep Boltzmann machine is a powerful model that extracts the hierarchical structure of observed data. While inference is typically slow due to its undirected nature we argue that the emerging feature hierarchy is still explicit enough to be traversed in a feed-forward fashion. The claim is corroborated by training a set of deep neural networks on real data and measuring the evolution of the representation layer after layer. The analysis reveals that the deep Boltzmann machine produces a feed-forward hierarchy of increasingly invariant representations that clearly surpasses the layer-wise approach.
1219,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Spectral Gap Error Bounds for Improving CUR Matrix Decomposition and the Nystr_m Method,"David Anderson, Simon Du, Michael Mahoney, Christopher Melgaard, Kunming Wu, Ming Gu",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/anderson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/anderson15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_anderson15,http://jmlr.csail.mit.edu/proceedings/papers/v38/anderson15.html,"The CUR matrix decomposition and the related Nystr_m method build low-rank approximations of data matrices by selecting a small number of representative rows and columns of the data. Here, we introduce novel spectral gap error bounds that judiciously exploit the potentially rapid spectrum decay in the input matrix, a most common occurrence in machine learning and data analysis. Our error bounds are much tighter than existing ones for matrices with rapid spectrum decay, and they justify the use of a constant amount of oversampling relative to the rank parameter \(k\) , i.e, when the number of columns/rows is \(\ell=k+ O(1)\) . We demonstrate our analysis on a novel deterministic algorithm, StableCUR , which additionally eliminates a previously unrecognized source of potential instability in CUR decompositions. While our algorithm accepts any method of row and column selection, we implement it with a recent column selection scheme with strong singular value bounds. Empirical results on various classes of real world data matrices demonstrate that our algorithm is as efficient as and often outperforms competing algorithms."
1220,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer,"Yan-Fu Liu, Cheng-Yu Hsu, Shan-Hung Wu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/liua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/liua15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_liua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/liua15.html,"The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from multiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing technique that can only transfer linearly correlated knowledge between domains. In this paper, we propose the notion of Hyper-Structure Transfer (HST) that requires the rating matrices to be explained by the projections of some more complex structure, called the hyper-structure, shared by all domains, and thus allows the non-linearly correlated knowledge between domains to be identified and transferred. Extensive experiments are conducted and the results demonstrate the effectiveness of our HST models empirically."
1221,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Faster cover trees,"Mike Izbicki, Christian Shelton",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/izbicki15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_izbicki15,http://jmlr.csail.mit.edu/proceedings/papers/v37/izbicki15.html,"The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces. This paper makes cover trees even faster. In particular, we provide (1) a simpler definition of the cover tree that reduces the number of nodes from O(n) to exactly n, (2) an additional invariant that makes queries faster in practice, (3) algorithms for constructing and querying the tree in parallel on multiprocessor systems, and (4) a more cache efficient memory layout. On standard benchmark datasets, we reduce the number of distance computations by 10_50%. On a large-scale bioinformatics dataset, we reduce the number of distance computations by 71%. On a large-scale image dataset, our parallel algorithm with 16 cores reduces tree construction time from 3.5 hours to 12 minutes."
1222,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Non-Negative Semi-Supervised Learning,"Changhu Wang, Shuicheng Yan, Lei Zhang, Hongjiang Zhang","5:575-582, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09a/wang09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_wang09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09a.html,The contributions of this paper are three-fold. First we present a general formulation for reaping the benefits from both non-negative data factorization and semi-supervised learning and the solution naturally possesses the characteristics of sparsity robustness to partial occlusions and greater discriminating power via extra unlabeled data. Then an efficient multiplicative updating procedure is proposed along with its theoretic justification of the algorithmic convergency. Finally the tensorization of this general formulation for non-negative semi-supervised learning is also briefed for handling tensor data of arbitrary order. Extensive experiments compared with the state-of-the-art algorithms for non-negative data factorization and semi-supervised learning demonstrate the algorithmic properties in sparsity classification power and robustness to image occlusions.
1223,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Using Hyperbolic Cross Approximation to measure and compensate Covariate Shift,"Thomas Vanck, Jochen Garcke","JMLR W&CP 29 :435-450, 2013",http://jmlr.org/proceedings/papers/v29/Vanck13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Vanck13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Vanck13.html,"The concept of covariate shift in supervised data analysis describes a difference between the training and test distribution while the conditional distribution remains the same. To improve the prediction performance one can address such a change by using individual weights for each training datapoint, which emphasizes the training points close to the test data set so that these get a higher significance. We propose a new method for calculating such weights by minimizing a Fourier series approximation of distance measures, in particular we consider the total variation distance, the Euclidean distance and Kullback-Leibler divergence. To be able to use the Fourier approach for higher dimensional data, we employ the so-called hyperbolic cross approximation. Results show that the new approach can compete with the latest methods and that on real life data an improved performance can be obtained."
1224,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Active Learning for Unbalanced Data in the Challenge with Multiple Models and Biasing,"Y. Chen & S. Mani ; 16:113_126, 2011. [ abs ] [ pdf ]","16:113_126, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/chen11a/chen11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_chen11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/chen11a.html,The common uncertain sampling approach searches for the most uncertain samples closest to the decision boundary for a classi_cation task. However we might fail to _nd the uncertain samples when we have a poor probabilistic model. In this work we develop an active learning strategy called ñUncertainty Sampling with Biasing Consensusî (USBC) which predicts the unbalanced data by multi-model committee and ranks the informativeness of samples by uncertainty sampling with higher weight on the minority class. For prediction we use Random Forests based multiple models that generate the consensus posterior probability for each sample as part of USBC. To further improve the initial performance in active learning we also use a semi-supervised learning model that self labels predicted negative samples without querying. For more stable initial performance we use a _lter to avoid querying samples with high variance. We also introduce batch size validation to _nd the optimal initial batch size for querying samples in active learning.   Page last modified on Wed Mar 30 11:10:06 2011.
1225,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Gaussian Approximation of Collective Graphical Models,"Liping Liu, Daniel Sheldon, Thomas Dietterich",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuf14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuf14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liuf14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuf14.html,"The Collective Graphical Model (CGM) models a population of independent and identically distributed individuals when only collective statistics (i.e., counts of individuals) are observed. Exact inference in CGMs is intractable, and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP approximations for learning and inference. This paper studies Gaussian approximations to the CGM. As the population grows large, we show that the CGM distribution converges to a multivariate Gaussian distribution (GCGM) that maintains the conditional independence properties of the original CGM. If the observations are exact marginals of the CGM or marginals that are corrupted by Gaussian noise, inference in the GCGM approximation can be computed efficiently in closed form. If the observations follow a different noise model (e.g., Poisson), then expectation propagation provides efficient and accurate approximate inference. The accuracy and speed of GCGM inference is compared to the MCMC and MAP methods on a simulated bird migration problem. The GCGM matches or exceeds the accuracy of the MAP method while being significantly faster."
1226,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results,"Jiaming Xu, Laurent Massouli_, Marc Lelarge","JMLR W&CP 35 :903-920, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/xu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_xu14,http://jmlr.csail.mit.edu/proceedings/papers/v35/xu14.html,"The classical setting of community detection consists of networks exhibiting a clustered structure. To more accurately model real systems we consider a class of networks (i) whose edges may carry labels and (ii) which may lack a clustered structure. Specifically we assume that nodes possess latent attributes drawn from a general compact space and edges between two nodes are randomly generated and labeled according to some unknown distribution as a function of their latent attributes. Our goal is then to infer the edge label distributions from a partially observed network. We propose a computationally efficient spectral algorithm and show it allows for asymptotically correct inference when the average node degree could be as low as logarithmic in the total number of nodes. Conversely, if the average node degree is below a specific constant threshold, we show that no algorithm can achieve better inference than guessing without using the observations. As a byproduct of our analysis, we show that our model provides a general procedure to construct random graph models with a spectrum asymptotic to a pre-specified eigenvalue distribution such as a power-law distribution."
1227,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,MAD-Bayes: MAP-based Asymptotic Derivations from Bayes,"Tamara Broderick, Brian Kulis, Michael Jordan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/broderick13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/broderick13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_broderick13,http://jmlr.csail.mit.edu/proceedings/papers/v28/broderick13.html,"The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework."
1228,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Variational Inference with Normalizing Flows,"Danilo Rezende, Shakir Mohamed",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/rezende15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/rezende15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_rezende15,http://jmlr.csail.mit.edu/proceedings/papers/v37/rezende15.html,"The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference."
1229,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Long Short-Term Memory Over Recursive Structures,"Xiaodan Zhu, Parinaz Sobihani, Hongyu Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhub15.html,"The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures."
1230,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Estimation of Causal Peer Influence Effects,"Panos Toulis, Edward Kao",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/toulis13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/toulis13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_toulis13,http://jmlr.csail.mit.edu/proceedings/papers/v28/toulis13.html,"The broad adoption of social media has generated interest in leveraging peer influence for inducing desired user behavior. Quantifying the causal effect of peer influence presents technical challenges, however, including how to deal with social interference, complex response functions and network uncertainty. In this paper, we extend potential outcomes to allow for interference, we introduce well-defined causal estimands of peer-influence, and we develop two estimation procedures: a frequentist procedure relying on a sequential randomization design that requires knowledge of the network but operates under complicated response functions, and a Bayesian procedure which accounts for network uncertainty but relies on a linear response assumption to increase estimation precision. Our results show the advantages and disadvantages of the proposed methods in a number of situations."
1231,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,An Almost Optimal PAC Algorithm,Hans U. Simon,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Simon15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Simon15a,http://jmlr.csail.mit.edu/proceedings/papers/v40/Simon15a.html,"The best currently known general lower and upper bounds on the number of labeled examples needed for learning a concept class in the PAC framework (the realizable case) do not perfectly match: they leave a gap of order \(\log(1/\epsilon)\) (resp. a gap which is logarithmic in another one of the relevant parameters). It is an unresolved question whether there exists an –optimal PAC algorithm” which establishes a general upper bound with precisely the same order of magnitude as the general lower bound. According to a result of Auer and Ortner, there is no way for showing that arbitrary consistent algorithms are optimal because they can provably differ from optimality by factor \(\log(1/\epsilon)\) . In contrast to this result, we show that every consistent algorithm \(L\) (even a provably suboptimal one) induces a family \((L_K)_{K\ge1}\) of PAC algorithms (with \(2K-1\) calls of \(L\) as a subroutine) which come very close to optimality: the number of labeled examples needed by \(L_K\) exceeds the general lower bound only by factor \(\ell_K(1/\epsillon)\) where \(\ell_K\) denotes (a truncated version of) the \(K\) -times iterated logarithm. Moreover, \(L_K\) is applicable to any concept class \(C\) of finite VC-dimension and it can be implemented efficiently whenever the consistency problem for \(C\) is feasible. We show furthermore that, for every consistent algorithm \(L\) , \(L_2\) is an optimal PAC algorithm for precisely the same concept classes which were used by Auer and Ortner for showing the existence of suboptimal consistent algorithms. This can be seen as an indication that \(L_K\) may have an even better performance than it is suggested by our worstcase analysis."
1232,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property,"Maxime Gasse, Alexandre Aussem, Haytham Elghazel",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gasse15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gasse15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gasse15.html,"The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factorization of the conditional distribution of the label set given the input features play a pivotal role for multi-label classification in the context of subset Zero-One loss minimization, as they divide the learning task into simpler independent multi-class problems. We establish theoretical results to characterize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data."
1233,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Structural SVM Based Approach for Optimizing Partial AUC,"Harikrishna Narasimhan, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/narasimhan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/narasimhan13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_narasimhan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/narasimhan13.html,"The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachimsê algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks."
1234,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Surrogate Regret Bounds for the Area Under the ROC Curve via Strongly Proper Losses,Shivani Agarwal,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Agarwal13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Agarwal13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Agarwal13.html,"The area under the ROC curve (AUC) is a widely used performance measure in machine learning, and has been widely studied in recent years particularly in the context of bipartite ranking. A dominant theoretical and algorithmic framework for AUC optimization/bipartite ranking has been to reduce the problem to pairwise classification; in particular, it is well known that the AUC regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for binary classification. Recently, Kotlowski et al. (2011) showed AUC regret bounds in terms of the regret associated with •balancedê versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we obtain such (non-pairwise) surrogate regret bounds for the AUC in terms of a broad class of proper (composite) losses that we term strongly proper . Our proof technique is considerably simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2009, 2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact AUC-consistent; moreover, our results allow us to quantify the AUC regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate regret bounds under certain low-noise conditions via a recent result of Cl_menand Robbiano (2011)."
1235,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Consistent Method for Graph Based Anomaly Localization,"Satoshi Hara, Tetsuro Morimura, Toshihiro Takahashi, Hiroki Yanagisawa, Taiji Suzuki",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hara15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/hara15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hara15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hara15.html,"The anomaly localization task aims at detecting faulty sensors automatically by monitoring the sensor values. In this paper, we propose an anomaly localization algorithm with a consistency guarantee on its results. Although several algorithms were proposed in the last decade, the consistency of the localization results was not discussed in the literature. To the best of our knowledge, this is the first study that provides theoretical guarantees for the localization results. Our new approach is to formulate the task as solving the sparsest subgraph problem on a difference graph. Since this problem is NP-hard, we then use a convex quadratic programming approximation algorithm, which is guaranteed to be consistent under suitable conditions. Across the simulations on both synthetic and real world datasets, we verify that the proposed method achieves higher anomaly localization performance compared to existing methods."
1236,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Modeling Correlated Arrival Events with Latent Semi-Markov Processes,"Wenzhao Lian, Vinayak Rao, Brian Eriksson, Lawrence Carin",none,http://jmlr.org/proceedings/papers/v32/lian14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lian14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lian14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lian14.html,"The analysis and characterization of correlated point process data has wide applications, ranging from biomedical research to network analysis. In this work, we model such data as generated by a latent collection of continuous-time binary semi-Markov processes, corresponding to external events appearing and disappearing. A continuous-time modeling framework is more appropriate for multichannel point process data than a binning approach requiring time discretization, and we show connections between our model and recent ideas from the discrete-time literature. We describe an efficient MCMC algorithm for posterior inference, and apply our ideas to both synthetic data and a real-world biometrics application."
1237,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stochastic Alternating Direction Method of Multipliers,"Hua Ouyang, Niao He, Long Tran, Alexander Gray",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ouyang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ouyang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ouyang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ouyang13.html,"The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: \(O(1/\sqrt{t})\) for convex functions and \(O(\log t/t)\) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm."
1238,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Adaptive Stochastic Alternating Direction Method of Multipliers,"Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaob15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhaob15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaob15.html,"The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms."
1239,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Preface,"Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v36/fan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_fan14,http://jmlr.csail.mit.edu/proceedings/papers/v36/fan14.html,"The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years. Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)-based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models."
1240,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,A Software System for the Microbial Source Tracking Problem,"David Sanchez, Lluis A. Belanche, Anicet R. Blanch","17:56-62, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/sanchez11a/sanchez11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_sanchez11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/sanchez11a.html,"The aim of this paper is to report the achievement of Ichnaea, a fully computer-based prediction system that is able to make fairly accurate predictions for Microbial Source Tracking studies. The system accepts examples showing different concentration levels, uses indicators (variables) with different environmental persistence, and can be applied at different geographical or climatic areas. We describe the inner workings of the system and report on the specific problems and challenges arisen from the machine learning point of view and how they have been addressed."
1241,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,State Space Methods for Efficient Inference in Student-t Process Regression,"Arno Solin, Simo S_rkk_",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/solin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/solin15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_solin15,http://jmlr.csail.mit.edu/proceedings/papers/v38/solin15.html,"The added flexibility of Student-t processes (TPs) over Gaussian processes (GPs) robustifies inference in outlier-contaminated noisy data. The uncertainties are better accounted for than in GP regression, because the predictive covariances explicitly depend on the training observations. For an entangled noise model, the canonical-form TP regression problem can be solved analytically, but the naive TP and GP solutions share the same cubic computational cost in the number of training observations. We show how a large class of temporal TP regression models can be reformulated as state space models, and how a forward filtering and backward smoothing recursion can be derived for solving the inference analytically in linear time complexity. This is a novel finding that generalizes the previously known connection between Gaussian process regression and Kalman filtering to more general elliptical processes and non-Gaussian Bayesian filtering. We derive this connection, demonstrate the benefits of the approach with examples, and finally apply the method to empirical data."
1242,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,The Rate of Convergence of Adaboost,"Indraneel Mukherjee, Cynthia Rudin, Robert E. Schapire","19:537-558, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/mukherjee11a/mukherjee11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_mukherjee11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/mukherjee11a.html,The AdaBoost algorithm of \citet{FreundSc97} was designed to combine many ``weak'' hypotheses that perform slightly better than a random guess into a ``strong'' hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the ``exponential loss'' with a fast rate of convergence. Our proofs do not require a weak-learning assumption nor do they require that minimizers of the exponential loss are finite. Specifically our first result shows that at iteration $t$ the exponential loss of AdaBoost's computed parameter vector will be at most $\varepsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by $B$ in a number of rounds that is bounded by a polynomial in $B$ and $1/\varepsilon$. We also provide rate lower bound examples showing a polynomial dependence on these parameters is necessary. Our second result is that within $C/\varepsilon$ iterations AdaBoost achieves a value of the exponential loss that is at most $\varepsilon$ more than the best possible value where $C$ depends on the dataset. We show that this dependence of the rate on $\varepsilon$ is optimal up to constant factors i.e. at least $\Omega(1/\varepsilon)$ rounds are necessary to achieve within $\varepsilon$ of the optimal exponential loss.
1243,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Dynamic Covariance Models for Multivariate Financial Time Series,"Yue Wu, Jose Miguel Hernandez-Lobato, Ghahramani Zoubin",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wu13.html,"The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models."
1244,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,A Fast Distributed Stochastic Gradient Descent Algorithm for Matrix Factorization,"Fanglin Li, Bin Wu, Liutong Xu, Chuan Shi, Jing Shi","JMLR W&CP 36 :77-87, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/li14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_li14,http://jmlr.csail.mit.edu/proceedings/papers/v36/li14.html,"The accuracy and effectiveness of matrix factorization technique were well demonstrated in the Netflix movie recommendation contest. Among the numerous solutions for matrix factorization, Stochastic Gradient Descent (SGD) is one of the most widely used algorithms. However, as a sequential approach, SGD algorithm cannot directly be used in the Distributed Cluster Environment (DCE). In this paper, we propose a fast distributed SGD algorithm named FDSGD for matrix factorization, which can run efficiently in DCE. This algorithm solves data sharing problem based on independent storage system to avoid data synchronization which may cause a big influence to algorithm performance, and synchronous operation problem in DCE using a distributed synchronization tool so that distributed cooperation threads can perform in a harmonious environment."
1245,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Detecting Weak but Hierarchically-Structured Patterns in Networks,"Aarti Singh, Robert Nowak, Robert Calderbank","9:749-756, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/singh10a/singh10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_singh10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/singh10a.html,The ability to detect weak distributed activation patterns in networks is critical to several applications such as identifying the onset of anomalous activity or incipient congestion in the Internet or faint traces of a biochemical spread by a sensor network. This is a challenging problem since weak distributed patterns can be invisible in per node statistics as well as a global network-wide aggregate. Most prior work considers situations in which the activation/non-activation of each node is statistically independent but this is unrealistic in many problems. In this paper we consider structured patterns arising from statistical dependencies in the activation process. Our contributions are three-fold. First we propose a sparsifying transform that succinctly represents structured activation patterns that conform to a hierarchical dependency graph. Second we establish that the proposed transform facilitates detection of very weak activation patterns that cannot be detected with existing methods. Third we show that the structure of the hierarchical dependency graph governing the activation process and hence the network transform can be learnt from very few (logarithmic in network size) independent snapshots of network activity.
1246,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Constrained fractional set programs and their application in local clustering and community detection,"Thomas B ô hler, Shyam Sundar Rangapuram, Simon Setzer, Matthias Hein",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/buhler13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/buhler13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_buhler13,http://jmlr.csail.mit.edu/proceedings/papers/v28/buhler13.html,"The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems."
1247,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Convolutional Dictionary Learning through Tensor Factorization,"Furong Huang, Animashree Anandkumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/huang15convolutional.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_huang15convolutional,http://jmlr.csail.mit.edu/proceedings/papers/v44/huang15convolutional.html,"Tensor methods have emerged as a powerful paradigm for consistent learning of many latent variable models such as topic models, independent component analysis and dictionary learning. Model parameters are estimated via CP decomposition of the observed higher order input moments. In this paper, we extend tensor decomposition framework to models with invariances, such as convolutional dictionary models. Our tensor decomposition algorithm is based on the popular alternating least squares (ALS) method, but with additional shift invariance constraints on the factors. We demonstrate that each ALS update can be computed efficiently using simple operations such as fast Fourier transforms and matrix multiplications. Our algorithm converges to models with better reconstruction error and is much faster, compared to the popular alternating minimization heuristic, where the filters and activation maps are alternately updated."
1248,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Tensor Factorization via Matrix Factorization,"Volodymyr Kuleshov, Arun Chaganty, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kuleshov15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/kuleshov15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kuleshov15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kuleshov15.html,"Tensor factorization arises in many machine learning applications, such as knowledge base modeling and parameter estimation in latent variable models. However, numerical methods for tensor factorization have not reached the level of maturity of matrix factorization methods. In this paper, we propose a new algorithm for CP tensor factorization that uses random projections to reduce the problem to simultaneous matrix diagonalization. Our method is conceptually simple and also applies to non-orthogonal and asymmetric tensors of arbitrary order. We prove that a small number random projections essentially preserves the spectral information in the tensor, allowing us to remove the dependence on the eigengap that plagued earlier tensor-to-matrix reductions. Experimentally, our method outperforms existing tensor factorization methods on both simulated data and two real datasets."
1249,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Gradient Temporal Difference Networks,David Silver,"24:117-130, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/silver12a/silver12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_silver12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/silver12a.html,Temporal-difference (TD) networks (Sutton and Tanner 2004) are a predictive represen- tation of state in which each node is an answer to a question about future observations or questions. Unfortunately existing algorithms for learning TD networks are known to diverge even in very simple problems. In this paper we present the first sound learning rule for TD networks. Our approach is to develop a true gradient descent algorithm that takes account of all three roles performed by each node in the network: as state as an answer and as a target for other questions. Our algorithm combines gradient temporal-difference learning (Maei et al. 2009) with real-time recurrent learning (Williams and Zipser 1994). We provide a generalisation of the Bellman equation that corresponds to the semantics of the TD network and prove that our algorithm converges to a fixed point of this equation.
1250,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,FAQ: A Framework for Fast Approximate Query Processing on Temporal Data,"Udayan Khurana, Srinivasan Parthasarathy, Deepak Turaga","JMLR W&CP 36 :29-45, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/khurana14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_khurana14,http://jmlr.csail.mit.edu/proceedings/papers/v36/khurana14.html,"Temporal queries on time evolving data are at the heart of a broad range of business and network intelligence applications ranging from consumer behavior analysis, trend analysis, temporal pattern mining, sentiment analysis on social media, cyber security, and network monitoring. In this work, we present an innovative data structure called Fast Approximate Query-able(FAQ) which provides a unified framework for temporal query processing on Big Data. FAQ uses a novel composition of data sketching, wavelet-style differencing for temporal compression, and quantization, and handles diverse kinds of queries including distinct counts, set membership, frequency estimation, top-K, p-norms, empirical entropy, and distance queries such as Histogram \(\ell_p\) -norm distance (including Euclidean and Manhattan distance), cosine similarity, Jaccard coefficient, and rank correlation. Experiments on a real-life multi dimensional network monitoring data sets demonstrate speedups of 92x achieved by FAQ over a flat representation of data for a mixed temporal query workload."
1251,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Maximum Margin Temporal Clustering,"Minh Hoai, Fernando De La Torre",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/hoai12/hoai12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_hoai12,http://jmlr.csail.mit.edu/proceedings/papers/v22/hoai12.html,Temporal Clustering (TC) refers to the factorization of multiple time series into a set of non-overlapping segments that belong to k temporal clusters. Existing methods based on extensions of generative models such as k-means or Switching Linear Dynamical Systems often lead to intractable inference and lack a mechanism for feature selection critical when dealing with high dimensional data. To overcome these limitations this paper proposes Maximum Margin Temporal Clustering (MMTC). MMTC simultaneously determines the start and the end of each segment while learning a multi-class Support Vector Machine to discriminate among temporal clusters. MMTC extends Maximum Margin Clustering in two ways: first it incorporates the notion of TC and second it introduces additional constraints to achieve better balance between clusters. Experiments on clustering human actions and bee dancing motions illustrate the benefits of our approach compared to state-of-the-art methods.
1252,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Telling cause from effect in deterministic linear dynamical systems,"Naji Shajarisales, Dominik Janzing, Bernhard Schoelkopf, Michel Besserve",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/shajarisales15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/shajarisales15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_shajarisales15,http://jmlr.csail.mit.edu/proceedings/papers/v37/shajarisales15.html,"Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the –cause” and the –mechanism generating the effect from the cause” independently of each other. Specifically we postulate that the power spectrum of the –cause” time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series."
1253,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,True Online TD(lambda),"Harm van Seijen, Rich Sutton",none,http://jmlr.org/proceedings/papers/v32/seijen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_seijen14,http://jmlr.csail.mit.edu/proceedings/papers/v32/seijen14.html,"TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)ãmatching an intuitively clear forward view even in the online caseãthat we have found a new algorithm that simply improves on classical TD(lambda)."
1254,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices,"Inbal Horev, Florian Yger, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Horev15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Horev15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Horev15.html,"Symmetric positive definite (SPD) matrices, e.g. covariance matrices, are ubiquitous in machine learning applications. However, because their size grows as \(n^2\) (where \(n\) is the number of variables) their high-dimensionality is a crucial point when working with them. Thus, it is often useful to apply to them dimensionality reduction techniques. Principal component analysis (PCA) is a canonical tool for dimensionality reduction, which for vector data reduces the dimension of the input data while maximizing the preserved variance. Yet, the commonly used, naive extensions of PCA to matrices result in sub-optimal variance retention. Moreover, when applied to SPD matrices, they ignore the geometric structure of the space of SPD matrices, further degrading the performance. In this paper we develop a new Riemannian geometry based formulation of PCA for SPD matrices that i) preserves more data variance by appropriately extending PCA to matrix data, and ii) extends the standard definition from the Euclidean to the Riemannian geometries. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals."
1255,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On Consistent Surrogate Risk Minimization and Property Elicitation,"Arpit Agarwal, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Agarwal15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Agarwal15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Agarwal15.html,"Surrogate risk minimization is a popular framework for supervised learning; property elicitation is a widely studied area in probability forecasting, machine learning, statistics and economics. In this paper, we connect these two themes by showing that calibrated surrogate losses in supervised learning can essentially be viewed as eliciting or estimating certain properties of the underlying conditional label distribution that are sufficient to construct an optimal classifier under the target loss of interest. Our study helps to shed light on the design of convex calibrated surrogates. We also give a new framework for designing convex calibrated surrogates under low-noise conditions by eliciting properties that allow one to construct •coarseê estimates of the underlying distribution."
1256,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Microbagging Estimators: An Ensemble Approach to Distance-weighted Classi_ers,"B. Nelson, B. Biggio & P. Laskov","20:63_79, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/nelson11/nelson11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_nelson11,http://jmlr.csail.mit.edu/proceedings/papers/v20/nelson11.html,Support vector machines (SVMs) have been the predominate approach to kernel-based classi_cation. While SVMs have demonstrated excellent performance in many application domains they are known to be sensitive to noise in their training dataset. Motivated by the equalizing e_ect of bagging classi_ers we present a novel approach to kernel-based classi_cation that we call microbagging. This method bags all possible maximal-margin estimators between pairs of training points to create a novel linear kernel classi_er with weights de_ned directly as functions of the pairwise distance matrix induced by the kernel function. We derive relationships between linear and distance-based classi_ers and empirically compare microbagging to the SVMs and robust SVMs on several datasets.   Page last modified on Sun Nov 6 15:42:25 2011.
1257,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Linear Time Solver for Primal SVM,"Feiping Nie, Yizhen Huang, Heng Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/niea14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_niea14,http://jmlr.csail.mit.edu/proceedings/papers/v32/niea14.html,"Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, hence designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with linear-time computational cost for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easily parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state-of-the-art solvers such as SVMperf , Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms."
1258,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Affinity Weighted Embedding,"Jason Weston, Ron Weiss, Hector Yee",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/weston14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_weston14,http://jmlr.csail.mit.edu/proceedings/papers/v32/weston14.html,"Supervised linear embedding models like Wsabie (Weston et al., 2011) and supervised semantic indexing (Bai et al., 2010) have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family, and show its usefulness on several datasets."
1259,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Continuous Target Shift Adaptation in Supervised Learning,"Tuan Duong Nguyen, Marthinus Christoffel, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Nguyen15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Nguyen15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Nguyen15.html,"Supervised learning in machine learning concerns inferring an underlying relation between covariate \(\bx\) and target \(y\) based on training covariate-target data. It is traditionally assumed that training data and test data, on which the generalization performance of a learning algorithm is measured, follow the same probability distribution. However, this standard assumption is often violated in many real-world applications such as computer vision, natural language processing, robot control, or survey design, due to intrinsic non-stationarity of the environment or inevitable sample selection bias. This situation is called dataset shift and has attracted a great deal of attention recently. In the paper, we consider supervised learning problems under the target shift scenario, where the target marginal distribution \(p(y)\) changes between the training and testing phases, while the target-conditioned covariate distribution \(p(\bx|y)\) remains unchanged. Although various methods for mitigating target shift in classification (a.k.a. class prior change ) have been developed so far, few methods can be applied to continuous targets. In this paper, we propose methods for continuous target shift adaptation in regression and conditional density estimation. More specifically, our contribution is a novel importance weight estimator for continuous targets. Through experiments, the usefulness of the proposed method is demonstrated."
1260,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Modeling annotator expertise: Learning when everybody knows a bit of something,"Yan Yan, Romer Rosales, Glenn Fung, Mark Schmidt, Gerardo Hermosillo, Luca Bogoni, Linda Moy, Jennifer Dy","9:932-939, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/yan10a/yan10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_yan10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/yan10a.html,Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy) but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods which only consider general annotator characteristics.
1261,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,On Theoretical Properties of Sum-Product Networks,"Robert Peharz, Sebastian Tschiatschek, Franz Pernkopf, Pedro Domingos",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/peharz15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/peharz15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_peharz15,http://jmlr.csail.mit.edu/proceedings/papers/v38/peharz15.html,"Sum-product networks (SPNs) are a promising avenue for probabilistic modeling and have been successfully applied to various tasks. However, some theoretic properties about SPNs are not yet well understood. In this paper we fill some gaps in the theoretic foundation of SPNs. First, we show that the weights of any complete and consistent SPN can be transformed into locally normalized weights without changing the SPN distribution. Second, we show that consistent SPNs cannot model distributions significantly (exponentially) more compactly than decomposable SPNs. As a third contribution, we extend the inference mechanisms known for SPNs with finite states to generalized SPNs with arbitrary input distributions."
1262,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning the Structure of Sum-Product Networks,"Robert Gens, Domingos Pedro",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gens13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gens13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gens13.html,"Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy."
1263,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Sum-Product Networks with Direct and Indirect Variable Interactions,"Amirmohammad Rooshenas, Daniel Lowd",none,http://jmlr.org/proceedings/papers/v32/rooshenas14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rooshenas14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rooshenas14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rooshenas14.html,"Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference. SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures. Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables. In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables. In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models."
1264,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Strict Monotonicity of Sum of Squares Error and Normalized Cut in the Lattice of Clusterings,Nicola Rebagliati,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/rebagliati13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_rebagliati13,http://jmlr.csail.mit.edu/proceedings/papers/v28/rebagliati13.html,"Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings."
1265,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Sufficient Dimension Reduction via Direct Estimation of the Gradients of Logarithmic Conditional Densities,"Hiroaki Sasaki, Voot Tangkaratt, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Sasaki15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Sasaki15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Sasaki15.html,"Sufficient dimension reduction (SDR) is a framework of supervised linear dimension reduction, and is aimed at finding a low-dimensional orthogonal projection matrix for input data such that the projected input data retains maximal information on output data. A computationally efficient approach employs gradient estimates of the conditional density of the output given input data to find an appropriate projection matrix. However, since the gradients of the conditional densities are typically estimated by a local linear smoother, it does not perform well when the input dimensionality is high. In this paper, we propose a novel estimator of the gradients of logarithmic conditional densities called the least-squares logarithmic conditional density gradients (LSLCG), which fits a gradient model directly to the true gradient without conditional density estimation under the squared loss. Thanks to the simple least-squares formulation, LSLCG gives a closed-form solution that can be computed efficiently. In addition, all the parameters can be automatically determined by cross-validation. Through experiments on a large variety of artificial and benchmark datasets, we demonstrate that the SDR method based on LSLCG outperforms existing SDR methods both in estimation accuracy and computational efficiency."
1266,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Multi-Task Learning for Subspace Segmentation,"Yu Wang, David Wipf, Qing Ling, Wei Chen, Ian Wassell",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wangc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangc15.html,"Subspace segmentation is the process of clustering a set of data points that are assumed to lie on the union of multiple linear or affine subspaces, and is increasingly being recognized as a fundamental tool for data analysis in high dimensional settings. Arguably one of the most successful approaches is based on the observation that the sparsest representation of a given point with respect to a dictionary formed by the others involves nonzero coefficients associated with points originating in the same subspace. Such sparse representations are computed independently for each data point via \(\ell_1\) -norm minimization and then combined into an affinity matrix for use by a final spectral clustering step. The downside of this procedure is two-fold. First, unlike canonical compressive sensing scenarios with ideally-randomized dictionaries, the data-dependent dictionaries here are unavoidably highly structured, disrupting many of the favorable properties of the \(\ell_1\) norm. Secondly, by treating each data point independently, we ignore useful relationships between points that can be leveraged for jointly computing such sparse representations. Consequently, we motivate a multi-task learning-based framework for learning coupled sparse representations leading to a segmentation pipeline that is both robust against correlation structure and tailored to generate an optimal affinity matrix. Theoretical analysis and empirical tests are provided to support these claims."
1267,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,DP-space: Bayesian Nonparametric Subspace Clustering with Small-variance Asymptotics,"Yining Wang, Jun Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wanga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wanga15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wanga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wanga15.html,"Subspace clustering separates data points approximately lying on union of affine subspaces into several clusters. This paper presents a novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which retains the nonparametric ability under a small-variance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an explicit tradeoff between data fitness and model complexity. Experimental results demonstrate that DP-space outperforms various competitors in terms of clustering accuracy and at the same time it is highly efficient."
1268,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data,"Yining Wang, Yu-Xiang Wang, Aarti Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wange15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wange15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wange15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wange15.html,"Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering."
1269,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,SubSift: a novel application of the vector space model to support the academic research process,"Simon Price, Peter A. Flach and Sebastian Spiegler","11:20-27, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/price10a/price10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_price10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/price10a.html,"SubSift matches submitted conference or journal papers to potential peer reviewers based on the similarity between the paperês abstract and the reviewerês publications as found in online bibliographic databases such as Google Scholar. Using concepts from information retrieval including a bag-of-words representation and cosine similarity, the SubSift tools were originally created to streamline the peer review process for the ACM SIGKDDê09 data mining conference. This paper describes how these tools were subsequently developed and deployed in the form of web services designed to support not only peer review but also personalised data discovery and mashups. SubSift has already been used by several major data mining conferences and interesting applications in other fields are now emerging."
1270,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions,"Alina Ene, Huy Nguyen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ene15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ene15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ene15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ene15.html,"Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of –simple” functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations."
1271,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On Greedy Maximization of Entropy,"Dravyansh Sharma, Ashish Kapoor, Amit Deshpande",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sharma15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sharma15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sharma15.html,"Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels."
1272,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Exploiting the High Predictive Power of Multi-class Subgroups,Tarek Abudawood and Peter Flach,"13:177-192, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/abudawood10a/abudawood10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_abudawood10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/abudawood10a.html,Subgroup discovery aims at finding subsets of a population whose class distribution is significantly different from the overall distribution. A number of multi-class subgroup discovery methods has been previously investigated proposed and implemented in the CN2-MSD system. When a decision tree learner was applied using the induced subgroups as features it led to the construction of accurate and compact predictive models demonstrating the usefulness of the subgroups. In this paper we show that given a significant sufficient and diverse set of subgroups no further learning phase is required to build a good predictive model. Our systematic study bridges the gap between rule learning and decision tree modelling by proposing a method which uses the training information associated with the subgroups to form a simple tree-based probability estimator and ranker RankFree-MSD without the need for an additional learning phase. Furthermore we propose an efficient subgroup pruning algorithm RankFree-Pruning that prunes unimportant subgroups from the subgroup tree in order to reduce the number of subgroups and the size of the tree without decreasing predictive performance. Despite the simplicity of our approach we experimentally show that its predictive performance in general is comparable to other decision tree and rule learners over 10 multi-class UCI data sets.
1273,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Collective Stability in Structured Prediction: Generalization from One Example,"Ben London, Bert Huang, Ben Taskar, Lise Getoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/london13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/london13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_london13,http://jmlr.csail.mit.edu/proceedings/papers/v28/london13.html,"Structured predictors enable joint inference over multiple interdependent output variables. These models are often trained on a small number of examples with large internal structure. Existing distribution-free generalization bounds do not guarantee generalization in this setting, though this contradicts a large body of empirical evidence from computer vision, natural language processing, social networks and other fields. In this paper, we identify a set of natural conditions _ weak dependence, hypothesis complexity and a new measure, collective stability _ that are sufficient for generalization from even a single example, without imposing an explicit generative model of the data. We then demonstrate that the complexity and stability conditions are satisfied by a broad class of models, including marginal inference in templated graphical models. We thus obtain uniform convergence rates that can decrease significantly faster than previous bounds, particularly when each structured example is sufficiently large and the number of training examples is constant, even one."
1274,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Structured Prediction Cascades,"David Weiss, Benjamin Taskar","9:916-923, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/weiss10a/weiss10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_weiss10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/weiss10a.html,Structured prediction tasks pose a fundamental trade-off between the need for model complexity to increase predictive power and the limited computational resources for inference in the exponentially-sized output spaces such models require. We formulate and develop structured prediction cascades: a sequence of increasingly complex models that progressively filter the space of possible outputs. We represent an exponentially large set of filtered outputs using max marginals and propose a novel convex loss function that balances filtering error with filtering efficiency. We provide generalization bounds for these loss functions and evaluate our approach on handwriting recognition and part-of-speech tagging. We find that the learned cascades are capable of reducing the complexity of inference by up to five orders of magnitude enabling the use of models which incorporate higher order features and yield higher accuracy.
1275,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,How Hard is Inference for Structured Prediction?,"Amir Globerson, Tim Roughgarden, David Sontag, Cafer Yildirim",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/globerson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/globerson15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_globerson15,http://jmlr.csail.mit.edu/proceedings/papers/v37/globerson15.html,"Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. The goal of this paper is to develop a theoretical explanation of the empirical effectiveness of heuristic inference algorithms for solving such structured prediction problems. We study the minimum-achievable expected Hamming error in such problems, highlighting the case of 2D grid graphs, which are common in machine vision applications. Our main theorems provide tight upper and lower bounds on this error, as well as a polynomial-time algorithm that achieves the bound."
1276,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scalable Gaussian Process Structured Prediction for Grid Factor Graph Applications,"Sebastien Bratieres, Novi Quadrianto, Sebastian Nowozin, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bratieres14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bratieres14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bratieres14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bratieres14.html,"Structured prediction is an important and well studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Bratières et al. 2013). The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables. However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically. This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications. Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed. We show experiments with 4M latent variables on image segmentation. Our method outperforms widely-used conditional random field models trained with pseudo-likelihood. Moreover, in image segmentation problems it improves over recent state-of-the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes."
1277,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Efficient Training of Structured SVMs via Soft Constraints,"Ofer Meshi, Nathan Srebro, Tamir Hazan",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/meshi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/meshi15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_meshi15,http://jmlr.csail.mit.edu/proceedings/papers/v38/meshi15.html,"Structured output prediction is a powerful framework for jointly predicting interdependent output labels. Learning the parameters of structured predictors is a central task in machine learning applications. However, training the model from data often becomes computationally expensive. Several methods have been proposed to exploit the model structure, or decomposition, in order to obtain efficient training algorithms. In particular, methods based on linear programming relaxation, or dual decomposition, decompose the prediction task into multiple simpler prediction tasks and enforce agreement between overlapping predictions. In this work we observe that relaxing these agreement constraints and replacing them with soft constraints yields a much easier optimization problem. Based on this insight we propose an alternative training objective, analyze its theoretical properties, and derive an algorithm for its optimization. Our method, based on the Frank-Wolfe algorithm, achieves significant speedups over existing state-of-the-art methods without hurting prediction accuracy."
1278,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning Thin Junction Trees via Graph Cuts,"Shahaf Dafna, Carlos Guestrin","5:113-120, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/dafna09a/dafna09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_dafna09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/dafna09a.html,Structure learning algorithms usually focus on the compactness of the learned model. However compact representation does not imply the existence of tractable inference algorithms: both exact and approximate inference may still be NP-hard. This focus on compactness leads to learning good models that require approximate inference techniques thus reducing their prediction quality. In this paper we propose a method for learning an attractive class of models: bounded treewidth junction trees. Those models permit both compact representation of probability distributions and efficient exact inference. Our method uses a new global criterion to construct the tree. Our criterion based on a Bethe approximation of the likelihood transforms the structure learning problem into an intuitive graph theoretical task. We present an efficient randomized algorithm with theoretical guarantees for finding good separators. We recursively apply this procedure to obtain a thin junction tree. Our extensive empirical evaluation demonstrates the benefit of applying exact inference using our models to answer queries. We also extend our technique to learn low tree-width conditional random fields and demonstrate significant improvements over state of the art block-L1 regularization techniques.
1279,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Strongly Adaptive Online Learning,"Amit Daniely, Alon Gonen, Shai Shalev-Shwartz",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/daniely15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/daniely15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_daniely15,http://jmlr.csail.mit.edu/proceedings/papers/v37/daniely15.html,"Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems."
1280,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Stochastic Structured Variational Inference,"Matthew Hoffman, David Blei",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hoffman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/hoffman15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hoffman15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hoffman15.html,"Stochastic variational inference makes it possible to approximate posterior distributions induced by large datasets quickly using stochastic optimization. The algorithm relies on the use of fully factorized variational distributions. However, this –mean-field” independence approximation limits the fidelity of the posterior approximation, and introduces local optima. We show how to relax the mean-field approximation to allow arbitrary dependencies between global parameters and local hidden variables, producing better parameter estimates by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters."
1281,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,An Adaptive Learning Rate for Stochastic Variational Inference,"Rajesh Ranganath, Chong Wang, Blei David, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ranganath13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ranganath13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ranganath13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ranganath13.html,"Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets. It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient. Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic inference. Our method requires no tuning and is easily implemented with computations already made in the algorithm. We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora. Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates."
1282,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A trust-region method for stochastic variational inference with applications to streaming data,"Lucas Theis, Matt Hoffman",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/theis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/theis15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_theis15,http://jmlr.csail.mit.edu/proceedings/papers/v37/theis15.html,"Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance."
1283,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process,"Amar Shah, David Knowles, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/shahb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/shahb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_shahb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/shahb15.html,"Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we also show that different posterior dependencies are important in BPFA relative to LDA."
1284,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Metadata Dependent Mondrian Processes,"Yi Wang, Bin Li, Yang Wang, Fang Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangd15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wangd15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangd15.html,"Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure."
1285,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Probabilistic Optimization from Noisy Gradients,Philipp Hennig,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/hennig13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_hennig13,http://jmlr.csail.mit.edu/proceedings/papers/v28/hennig13.html,"Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions."
1286,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems,"Christopher De Sa, Christopher Re, Kunle Olukotun",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/sa15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sa15.html,"Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within \(O(\epsilon^{-1} n \log n)\) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show some experiments to illustrate the runtime and convergence of the algorithm."
1287,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes,"Ohad Shamir, Tong Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/shamir13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_shamir13,http://jmlr.csail.mit.edu/proceedings/papers/v28/shamir13.html,"Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after \(T\) rounds, the suboptimality of the last SGD iterate scales as \(O(\log(T)/\sqrt{T}\) ) for non-smooth convex objective functions, and \(O(\log(T)/T)\) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in is not as simple to implement). Finally, we provide some experimental illustrations."
1288,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Open Problem: Is Averaging Needed for Strongly Convex Stochastic Gradient Descent?,Ohad Shamir,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/shamir12/shamir12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_shamir12,http://jmlr.csail.mit.edu/proceedings/papers/v23/shamir12.html,"Stochastic gradient descent (SGD) is a simple and very popular iterative method to solve stochastic optimization problems which arise in machine learning. A common practice is to return the average of the SGD iterates. While the utility of this is well-understood for general convex problems, the situation is much less clear for strongly convex problems (such as solving SVM). Although the standard analysis in the strongly convex case requires averaging, it was recently shown that this actually degrades the convergence rate, and a better rate is obtainable by averaging just a suffix of the iterates. The question we pose is whether averaging is needed at all to get optimal rates."
1289,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Online Passive-Aggressive Algorithms for Non-Negative Matrix Factorization and Completion,"Mathieu Blondel, Yotaro Kubo, Ueda Naonori","JMLR W&CP 33 :96-104, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/blondel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/blondel14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_blondel14,http://jmlr.csail.mit.edu/proceedings/papers/v33/blondel14.html,"Stochastic Gradient Descent (SGD) is a popular online algorithm for large-scale matrix factorization. However, SGD can often be difficult to use for practitioners, because its performance is very sensitive to the choice of the learning rate parameter. In this paper, we present non-negative passive-aggressive (NN-PA), a family of online algorithms for non-negative matrix factorization (NMF). Our algorithms are scalable, easy to implement and do not require the tedious tuning of a learning rate parameter. We demonstrate the effectiveness of our algorithms on three large-scale matrix completion problems and analyze them in the regret bound model."
1290,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Open Problem: Fast Stochastic Exp-Concave Optimization,Tomer Koren,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Koren13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Koren13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Koren13.html,"Stochastic exp-concave optimization is an important primitive in machine learning that captures several fundamental problems, including linear regression, logistic regression and more. The exp-concavity property allows for fast convergence rates, as compared to general stochastic optimization. However, current algorithms that attain such rates scale poorly with the dimension \(n\) and run in time \(O(n^4)\) , even on very simple instances of the problem. The question we pose is whether it is possible to obtain fast rates for exp-concave functions using more computationally-efficient algorithms."
1291,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent,"Cho-Jui Hsieh, Hsiang-Fu Yu, Inderjit Dhillon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsieha15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsieha15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hsieha15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsieha15.html,"Stochastic Dual Coordinate Descent (DCD) is one of the most efficient ways to solve the family of L2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of parallel asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties of DCD when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present a novel error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with a perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers."
1292,1,http://jmlr.csail.mit.edu/proceedings/papers/v1/,Gaussian Process Approximations of Stochastic Differential Equations,"Cedric Archambeau, Dan Cornford, Manfred Opper and John Shawe-Taylor","1:1-16, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v1/archambeau07a/archambeau07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v1/,,11th March 2007,"June 12-13, 2006",Gaussian Processes in Practice,Gaussian Processes in Practice,"Bletchley Park, Bletchley, U.K.","Neil Lawrence, Anton Schwaighofer and Joaquin QuiÕ±onero Candela",v1_archambeau07a,http://jmlr.csail.mit.edu/proceedings/papers/v1/archambeau07a.html,"Stochastic differential equations arise naturally in a range of contexts, from financial to environmental modeling. Current solution methods are limited in their representation of the posterior process in the presence of data. In this work, we present a novel Gaussian process approximation to the posterior measure over paths for a general class of stochastic differential equations in the presence of observations. The method is applied to two simple problems: the Ornstein-Uhlenbeck process, of which the exact solution is known and can be compared to, and the double-well system, for which standard approaches such as the ensemble Kalman smoother fail to provide a satisfactory result. Experiments show that our variational approximation is viable and that the results are very promising as the variational approximate solution outperforms standard Gaussian process regression for non-Gaussian Markov processes."
1293,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,On Bayesian Upper Confidence Bounds for Bandit Problems,"Emilie Kaufmann, Olivier Cappe, Aurelien Garivier",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kaufmann12/kaufmann12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kaufmann12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kaufmann12.html,Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view where the parameter is a deterministic unknown quantity and a Bayesian approach where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits we prove that the corresponding algorithm termed Bayes-UCB satisfies finite-time regret bounds that imply its asymptotic optimality. More generally Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits Gaussian bandits with unknown mean and variance linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.
1294,47,http://jmlr.csail.mit.edu/proceedings/papers/v47/,A Critical View on Automatic Significance-Filtering in Pattern Mining,"Florian Lemmerich, Frank Puppe",none,http://jmlr.csail.mit.edu/proceedings/papers/v47/lemmerich14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v47/,,27th November 2015,41897,ECML/PKDD Workshop on Statistically Sound Data Mining 2014 Proceedings,Statistically Sound Data Mining,"Joensuu, Finland","Wilhelmiina HÕ_mÕ_lÕ_inen, FranÕ_ois Petitjean, Geoffrey, I. Webb",v47_lemmerich14a,http://jmlr.csail.mit.edu/proceedings/papers/v47/lemmerich14a.html,"Statistically sound validation of results plays an important role in modern data mining. In this context, it has been advocated to disregard patterns that cannot be automatically confirmed as statistically valid by the available data. In this short position paper, we argue against a mandatory automatic significance filtering of results."
1295,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Spherical Hamiltonian Monte Carlo for Constrained Target Distributions,"Shiwei Lan, Bo Zhou, Babak Shahbaba",none,http://jmlr.org/proceedings/papers/v32/lan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lan14.html,"Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit models, many copula models, and Latent Dirichlet Allocation (LDA) models. Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. For such problems, we propose a novel Markov Chain Monte Carlo (MCMC) method that provides a general and computationally efficient framework for handling boundary conditions. Our method first maps the \(D\) -dimensional constrained domain of parameters to the unit ball \({\bf B}_0^D(1)\) , then augments it to the \(D\) -dimensional sphere \({\bf S}^D\) such that the original boundary corresponds to the equator of \({\bf S}^D\) . This way, our method handles the constraints implicitly by moving freely on sphere generating proposals that remain within boundaries when mapped back to the original space. To improve the computational efficiency of our algorithm, we divide the dynamics into several parts such that the resulting split dynamics has a partial analytical solution as a geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions."
1296,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Manifold-valued Dirichlet Processes,"Hyunwoo Kim, Jia Xu, Baba Vemuri, Vikas Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kim15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kim15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kim15.html,"Statistical models for manifold-valued data permit capturing the intrinsic nature of the curved spaces in which the data lie and have been a topic of research for several decades. Typically, these formulations use geodesic curves and distances defined locally for most cases - this makes it hard to design parametric models globally on smooth manifolds. Thus, most (manifold specific) parametric models available today assume that the data lie in a small neighborhood on the manifold. To address this êlocalityê problem, we propose a novel nonparametric model which unifies multivariate general linear models (MGLMs) using multiple tangent spaces. Our framework generalizes existing work on (both Euclidean and non-Euclidean) general linear models providing a recipe to globally extend the locally-defined parametric models (using a mixture of local models). By grouping observations into sub-populations at multiple tangent spaces, our method provides insights into the hidden structure (geodesic relationships) in the data. This yields a framework to group observations and discover geodesic relationships between covariates X and manifold-valued responses Y, which we call Dirichlet process mixtures of multivariate general linear models (DP-MGLM) on Riemannian manifolds. Finally, we present proof of concept experiments to validate our model."
1297,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Hierarchical Probabilistic Models for Group Anomaly Detection,"Liang Xiong, Barnabas Poczos, Jeff Schneider, Andrew Connolly, Jake VanderPlas","15:789-797, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/xiong11a/xiong11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_xiong11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/xiong11a.html,Statistical anomaly detection typically focuses on finding individual data point anomalies. Often the most interesting or unusual things in a data set are not odd individual points but rather larger scale phenomena that only become apparent when groups of data points are considered. In this paper we propose two hierarchical probabilistic models for detecting such group anomalies. We evaluate our methods on synthetic data as well as astronomical data from the Sloan Digital Sky Survey. The experimental results show that the proposed models are effective in detecting group anomalies.
1298,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,MRA-based Statistical Learning from Incomplete Rankings,"Eric Sibony, St_phan Clemençon, J_r_mie Jakubowicz",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sibony15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/sibony15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sibony15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sibony15.html,"Statistical analysis of rank data describing preferences over small and variable subsets of a potentially large ensemble of items 1, ..., n is a very challenging problem. It is motivated by a wide variety of modern applications, such as recommender systems or search engines. However, very few inference methods have been documented in the literature to learn a ranking model from such incomplete rank data. The goal of this paper is twofold: it develops a rigorous mathematical framework for the problem of learning a ranking model from incomplete rankings and introduces a novel general statistical method to address it. Based on an original concept of multi-resolution analysis (MRA) of incomplete rankings, it finely adapts to any observation setting, leading to a statistical accuracy and an algorithmic complexity that depend directly on the complexity of the observed data. Beyond theoretical guarantees, we also provide experimental results that show its statistical performance."
1299,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,A General Linear Non-Gaussian State-Space Model,K. Zhang & A. Hyv _ rinen,"20:113_128, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/zhang11/zhang11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_zhang11,http://jmlr.csail.mit.edu/proceedings/papers/v20/zhang11.html,State-space modeling provides a powerful tool for system identi_cation and prediction. In linear state-space models the data are usually assumed to be Gaussian and the models have certain structural constraints such that they are identi_able. In this paper we propose a non-Gaussian state-space model which does not have such constraints. We prove that this model is fully identi_able. We then propose an e_cient two-step method for parameter estimation: one _rst extracts the subspace of the latent processes based on the temporal information of the data and then performs multichannel blind deconvolution making use of both the temporal information and non-Gaussianity. We conduct a series of simulations to illustrate the performance of the proposed method. Finally we apply the proposed model and parameter estimation method on real data including major world stock indices and magnetoencephalography (MEG) recordings. Experimental results are encouraging and show the practical usefulness of the proposed model and method.   Page last modified on Sun Nov 6 15:42:46 2011.
1300,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,State-Space Inference and Learning with Gaussian Processes,"Ryan Turner, Marc Deisenroth, Carl Rasmussen","9:868-875, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/turner10a/turner10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_turner10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/turner10a.html,State-space inference and learning with Gaussian processes (GPs) is an unsolved problem. We propose a new general methodology for inference and learning in nonlinear state-space models that are described probabilistically by non-parametric GP models. We apply the expectation maximization algorithm to iterate between inference in the latent state-space and learning the parameters of the underlying GP dynamics model.
1301,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Efficient graphlet kernels for large graph comparison,"Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, Karsten Borgwardt","5:488-495, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/shervashidze09a/shervashidze09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_shervashidze09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/shervashidze09a.html,State-of-the-art graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting {\it graphlets} \ie subgraphs with $k$ nodes where $k \in \{ 3 4 5 \}$. Exhaustive enumeration of all graphlets being prohibitively expensive we introduce two theoretically grounded speedup schemes one based on sampling and the second one specifically designed for bounded degree graphs. In our experimental evaluation our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.
1302,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Abstraction Selection in Model-based Reinforcement Learning,"Nan Jiang, Alex Kulesza, Satinder Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiang15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_jiang15,http://jmlr.csail.mit.edu/proceedings/papers/v37/jiang15.html,"State abstractions are often used to reduce the complexity of model-based reinforcement learning when only limited quantities of data are available. However, choosing the appropriate level of abstraction is an important problem in practice. Existing approaches have theoretical guarantees only under strong assumptions on the domain or asymptotically large amounts of data, but in this paper we propose a simple algorithm based on statistical hypothesis testing that comes with a finite-sample guarantee under assumptions on candidate abstractions. Our algorithm trades off the low approximation error of finer abstractions against the low estimation error of coarser abstractions, resulting in a loss bound that depends only on the quality of the best available abstraction and is polynomial in planning horizon."
1303,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Mean Reversion with a Variance Threshold,"Marco Cuturi, Alexandre DêAspremont",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cuturi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cuturi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cuturi13.html,"Starting from a multivariate data set, we study several techniques to isolate affine combinations of the variables with a maximum amount of mean reversion, while constraining the variance to be larger than a given threshold. We show that many of the optimization problems arising in this context can be solved exactly using semidefinite programming and some variant of the \(\mathcal{S}\) -lemma. In finance, these methods are used to isolate statistical arbitrage opportunities, i.e. mean reverting portfolios with enough variance to overcome market friction. In a more general setting, mean reversion and its generalizations are also used as a proxy for stationarity, while variance simply measures signal strength."
1304,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs,"Yarin Gal, Richard Turner",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/galb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/galb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_galb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/galb15.html,"Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting."
1305,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Conditional Restricted Boltzmann Machines for Multi-label Learning with Incomplete Labels,"Xin Li, Feipeng Zhao, Yuhong Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15e,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15e.html,"Standard multi-label learning methods assume fully labeled training data. This assumption however is impractical in many application domains where labels are difficult to collect and missing labels are prevalent. In this paper, we develop a novel conditional restricted Boltzmann machine model to address multi-label learning with incomplete labels. It uses a restricted Boltzmann machine to capture the high-order label dependence relationships in the output space, aiming to enhance the capacity of recovering missing labels and learning high quality multi-label prediction models. Moreover, it also incorporates label co-occurrence information retrieved from auxiliary resources as prior knowledge. We perform model training by maximizing the regularized marginal conditional likelihood of the label vectors given the input features, and develop a Viterbi style EM algorithm to solve the induced optimization problem. The proposed approach is evaluated on four real word multi-label data sets by comparing to a number of state-of-the-art methods. The experimental results show it outperforms all the other comparison methods across the applied data sets."
1306,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Global Multi-armed Bandits with H_lder Continuity,"Onur Atan, Cem Tekin, Mihaela van der Schaar",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/atan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/atan15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_atan15,http://jmlr.csail.mit.edu/proceedings/papers/v38/atan15.html,"Standard Multi-Armed Bandit (MAB) problems assume that the arms are independent. However, in many application scenarios, the information obtained by playing an arm provides information about the remainder of the arms. Hence, in such applications, this informativeness can and should be exploited to enable faster convergence to the optimal solution. In this paper, formalize a new class of multi-armed bandit methods, Global Multi-armed Bandit (GMAB), in which arms are globally informative through a global parameter, i.e., choosing an arm reveals information about all the arms. We propose a greedy policy for the GMAB which always selects the arm with the highest estimated expected reward, and prove that it achieves bounded parameter-dependent regret. Hence, this policy selects suboptimal arms only finitely many times, and after a finite number of initial time steps, the optimal arm is selected in all of the remaining time steps with probability one. In addition, we also study how the informativeness of the arms about each otherês rewards affects the speed of learning. Specifically, we prove that the parameter-free (worst-case) regret is sublinear in time, and decreases with the infor- mativeness of the arms. We also prove a sublinear in time Bayesian risk bound for the GMAB which reduces to the well-known Bayesian risk bound for linearly parameterized bandits when the arms are fully informa- tive. GMABs have applications ranging from drug dosage control to dynamic pricing."
1307,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Universal Measurement Bounds for Structured Sparse Signal Recovery,"Nikhil Rao, Ben Recht, Robert Nowak",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/rao12/rao12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_rao12,http://jmlr.csail.mit.edu/proceedings/papers/v22/rao12.html,Standard compressive sensing results state that to exactly recover an s sparse signal in Rp one requires O(s log p) measurements. While this bound is extremely useful in practice often real world signals are not only sparse but also exhibit structure in the sparsity pattern. We focus on group-structured patterns in this paper. Under this model groups of signal coefficients are active (or inactive) together. The groups are prede- fined but the particular set of groups that are active (i.e. in the signal support) must be learned from measurements. We show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of groups under consideration and not the particulars of the groups (e.g. compositions sizes ex- tents overlaps etc.). Experiments show that our result holds for a variety of overlapping group configurations.
1308,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Keyword Optimization in Sponsored Search via Feature Selection,"Svetlana Kiritchenko, Mikhail Jiline","4:122-134, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/kiritchenko08a/kiritchenko08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_kiritchenko08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/kiritchenko08a.html,Sponsored search is a new application domain for the feature selection area of research. When a user searches for products or services using the Internet most of the major search engines would return two sets of results: regular web pages and paid advertisements. An advertising company provides a set of keywords associated with an ad. If one of these keywords is present in a user's query the ad is displayed but the company is charged only if the user actually clicks on the ad. Ultimately a company would like to advertise on the most effective keywords to attract only prospective customers. A set of keywords can be optimized based on historic performance. We propose to optimize advertising keywords with feature selection techniques applied to the set of all possible word combinations comprising past users' queries. Unlike previous work in this area our approach not only recognizes the most profitable keywords but also discovers more specific combinations of keywords and other relevant words.
1309,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions,"Taehoon Lee, Sungroh Yoon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/leeb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_leeb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/leeb15.html,"Splicing refers to the elimination of non-coding regions in transcribed pre-messenger ribonucleic acid (RNA). Discovering splice sites is an important machine learning task that helps us not only to identify the basic units of genetic heredity but also to understand how different proteins are produced. Existing methods for splicing prediction have produced promising results, but often show limited robustness and accuracy. In this paper, we propose a deep belief network-based methodology for computational splice junction prediction. Our proposal includes a novel method for training restricted Boltzmann machines for class-imbalanced prediction. The proposed method addresses the limitations of conventional contrastive divergence and provides regularization for datasets that have categorical features. We tested our approach using public human genome datasets and obtained significantly improved accuracy and reduced runtime compared to state-of-the-art alternatives. The proposed approach was less sensitive to the length of input sequences and more robust for handling false splicing signals. Furthermore, we could discover non-canonical splicing patterns that were otherwise difficult to recognize using conventional methods. Given the efficiency and robustness of our methodology, we anticipate that it can be extended to the discovery of primary structural patterns of other subtle genomic elements."
1310,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Predicting Spiking Activities in DLS Neurons with Linear-Nonlinear-Poisson Model,"Sisi Ma, David J. Barker",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/ma15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_ma15,http://jmlr.csail.mit.edu/proceedings/papers/v46/ma15.html,"Spike train generation in primary motor cortex (M1) and somatosensory cortex (S1) has been studied extensively and is relatively well understood. On the contrary, the functionality and physiology of the dorsolateral striatum (DLS), the immediate downstream region of M1 and S1 and a critical link in the motor circuit, still requires intensive investigation. In the current study, spike trains of individual DLS neurons were reconstructed using a Linear-Nonlinear-Poisson model with features from two modalities: (1) the head position modality, which contains information regarding head movement and proprioception of the animalês head; (2) the spike history modality, which contains information regarding the intrinsic physiological properties of the neuron. For the majority of the neurons examined, viable reconstruction accuracy was achieved when the neural activity was modeled with either feature modality or the two feature modalities combined. Subpopulations of neurons were also identi_ied that had better reconstruction accuracy when modeled with features from single modalities. This study demonstrates the feasibility of spike train reconstruction in DLS neurons and provides insights into the physiology of DLS neurons."
1311,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Some improvements of the spectral learning approach for probabilistic grammatical inference,"Mattias Gybels, François Denis, Amaury Habrard","JMLR W&CP 34 :64-78, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/gybels14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_gybels14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/gybels14a.html,"Spectral methods propose new and elegant solutions in probabilistic grammatical inference. We propose two ways to improve them. We show how a linear representation, or equivalently a weighted automata, output by the spectral learning algorithm can be taken as an initial point for the Baum Welch algorithm, in order to increase the likelihood of the observation data. Secondly, we show how the inference problem can naturally be expressed in the framework of Structured Low-Rank Approximation. Both ideas are tested on a benchmark extracted from the PAutomaC challenge."
1312,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nonparametric Estimation of Multi-View Latent Variable Models,"Le Song, Animashree Anandkumar, Bo Dai, Bo Xie",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/songa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/songa14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_songa14,http://jmlr.csail.mit.edu/proceedings/papers/v32/songa14.html,"Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric and learned from data in an unsupervised fashion. The key idea of our method is to embed the joint distribution of a multi-view latent variable model into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our nonparametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. As a special case of our framework, we also obtain a first unsupervised conditional density estimator of the kind with provable guarantees. In both synthetic and real world datasets, the nonparametric tensor power method compares favorably to EM algorithm and other spectral algorithms."
1313,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Low-Rank Spectral Learning,"Alex Kulesza, N. Raj Rao, Satinder Singh","JMLR W&CP 33 :522-530, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kulesza14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kulesza14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kulesza14.html,"Spectral learning methods have recently been proposed as alternatives to slow, non-convex optimization algorithms like EM for a variety of probabilistic models in which hidden information must be inferred by the learner. These methods are typically controlled by a rank hyperparameter that sets the complexity of the model; when the model rank matches the true rank of the process generating the data, the resulting predictions are provably consistent and admit finite sample convergence bounds. However, in practice we usually do not know the true rank, and, in any event, from a computational and statistical standpoint it is likely to be prohibitively large. It is therefore of great practical interest to understand the behavior of low-rank spectral learning, where the model rank is less than the true rank. Counterintuitively, we show that even when the singular values omitted by lowering the rank are arbitrarily small, the resulting prediction errors can in fact be arbitrarily large. We identify two distinct possible causes for this bad behavior, and illustrate them with simple examples. We then show that these two causes are essentially complete: assuming that they do not occur, we can prove that the prediction error is bounded in terms of the magnitudes of the omitted singular values. We argue that the assumptions necessary for this result are relatively realistic, making low-rank spectral learning a viable option for many applications."
1314,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Spectral Clustering via the Power Method - Provably,"Christos Boutsidis, Prabhanjan Kambadur, Alex Gittens",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/boutsidis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_boutsidis15,http://jmlr.csail.mit.edu/proceedings/papers/v37/boutsidis15.html,"Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the –power method” from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the \(k\) -means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors."
1315,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Dimensionality Reduction for Spectral Clustering,"Donglin Niu, Jennifer Dy, Michael Jordan","15:552-560, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/niu11a/niu11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_niu11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/niu11a.html,Spectral clustering is a flexible clustering methodology that is applicable to a variety of data types and has the particular virtue that it makes few assumptions on cluster shapes. It has become popular in a variety of application areas particularly in computational vision and bioinformatics. The approach appears however to be particularly sensitive to irrelevant and noisy dimensions in the data. We thus introduce an approach that automatically learns the relevant dimensions and spectral clustering simultaneously. We pursue an augmented form of spectral clustering in which an explicit projection operator is incorporated in the relaxed optimization functional. We optimize this functional over both the projection and the spectral embedding. Experiments on simulated and real data show that this approach yields significant improvements in the performance of spectral clustering.
1316,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Sparsity and the truncated \(l^2\)-norm,Lee Dicker,"JMLR W&CP 33 :159-166, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/dicker14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/dicker14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_dicker14,http://jmlr.csail.mit.edu/proceedings/papers/v33/dicker14.html,"Sparsity is a fundamental topic in high-dimensional data analysis. Perhaps the most common measures of sparsity are the \(l^p\) -norms, for \(p _ 2\) . In this paper, we study an alternative measure of sparsity, the truncated \(l^2\) -norm, which is related to other \(l^p\) -norms, but appears to have some unique and useful properties. Focusing on the \(n\) -dimensional Gaussian location model, we derive exact asymptotic minimax results for estimation over truncated \(l^2\) -balls, which complement existing results for \(l^p\) -balls. We then propose simple new adaptive thresholding estimators that are inspired by the truncated \(l^2\) -norm and are adaptive asymptotic minimax over \(l^p\) -balls ( \(p _ 2\) ), as well as truncated \(l^2\) -balls. Finally, we derive lower bounds on the Bayes risk of an estimator, in terms of the parameterês truncated \(l^2\) -norm. These bounds provide necessary conditions for Bayes risk consistency in certain problems that are relevant for high-dimensional Bayesian modeling."
1317,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Sparsity on Statistical Simplexes and Diversity in Social Ranking,"Ke Sun, Hisham Mohamed, Stephane Marchand-Maillet",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/sun14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_sun14,http://jmlr.csail.mit.edu/proceedings/papers/v39/sun14.html,"Sparsity in \(\Re^m\) has been widely explored in machine learning. We study sparsity on a statistical simplex consisting of all categorical distributions. This is different from the case in \(\Re^m\) because such a simplex is a Riemannian manifold, a curved space. A learner with sparse constraints should be likely to fall to its low-dimensional boundaries. We present a novel analysis on the statistical simplex as a manifold with boundary. The main contribution is an explicit view of the learning dynamics in between high-dimensional models in the interior of the simplex and low-dimensional models on its boundaries. We prove the differentiability of the cost function, the natural gradient with respect to the Riemannian structure, and convexity around the singular regions. We uncover an interesting relationship with \(L_1\) regularization. We apply the proposed technique to social network analysis. Given a directed graph, the task is to rank a subset of influencer nodes. Here, sparsity means that the top-ranked nodes should present diversity in the sense of minimizing influence overlap. We present a ranking algorithm based on the natural gradient. It can scale up to graph datasets with millions of nodes. On real large networks, the top-ranked nodes are the most informative among several commonly-used techniques."
1318,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Safe Screening with Variational Inequalities and Its Application to Lasso,"Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuc14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liuc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liuc14.html,"Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries. Safe screening, which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called ``Sasvi"" (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening."
1319,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Variational Learning of Inducing Variables in Sparse Gaussian Processes,Michalis Titsias,"5:567-574, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/titsias09a/titsias09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_titsias09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/titsias09a.html,Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.
1320,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,The Log-Shift Penalty for Adaptive Estimation of Multiple Gaussian Graphical Models,"Yuancheng Zhu, Rina Foygel Barber",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhu15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_zhu15,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhu15.html,"Sparse Gaussian graphical models characterize sparse dependence relationships between random variables in a network. To estimate multiple related Gaussian graphical models on the same set of variables, we formulate a hierarchical model, which leads to an optimization problem with a nonconvex log-shift penalty function. We show that under mild conditions the optimization problem is convex despite the inclusion of a nonconvex penalty, and derive an efficient optimization algorithm. Experiments on both synthetic and real data show that the proposed method is able to achieve good selection and estimation performance simultaneously, because the nonconvexity of the log-shift penalty allows for weak signals to be thresholded to zero without excessive shrinkage on the strong signals."
1321,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Shuo Xiang, Xiaoshen Tong, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/xiang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/xiang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_xiang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/xiang13.html,"Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2) statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance."
1322,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Heteroscedastic Models by Convex Programming under Group Sparsity,"Arnak Dalalyan, Mohamed Hebiri, Katia Meziani, Joseph Salmon",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/dalalyan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/dalalyan13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_dalalyan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/dalalyan13.html,"Sparse estimation methods based on l1 relaxation, such as the Lasso and the Dantzig selector, require the knowledge of the variance of the noise in order to properly tune the regularization parameter. This constitutes a major obstacle in applying these methods in several frameworks, such as time series, random fields, inverse problems, for which noise is rarely homoscedastic and the noise level is hard to know in advance. In this paper, we propose a new approach to the joint estimation of the conditional mean and the conditional variance in a high-dimensional (auto-) regression setting. An attractive feature of the proposed estimator is that it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present theoretical analysis and numerical results assessing the performance of the proposed procedure."
1323,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,"Simple, Efficient, and Neural Algorithms for Sparse Coding","Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Arora15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Arora15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Arora15.html,"Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used"
1324,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Importance Sampling for General Hybrid Bayesian Networks,"Changhe Yuan, Marek J. Druzdzel","2:652-659, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/yuan07a/yuan07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_yuan07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/yuan07a.html,Some real problems are more naturally modeled by hybrid Bayesian networks that consist of mixtures of continuous and discrete variables with their interactions described by equations and continuous probability distributions. However inference in such general hybrid models is hard. Therefore existing approaches either only deal with special instances such as Conditional Linear Gaussians (CLGs) or approximate a general model with a restricted version and then perform inference on the simpler model. However results thus obtained highly depend on the quality of the approximations. This paper describes an importance sampling-based algorithm that directly deals with hybrid Bayesian networks constructed in the most general settings and guarantees to converge to the correct answers given enough time.
1325,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Exploiting Feature Covariance in High-Dimensional Online Learning,"Justin Ma, Alex Kulesza, Mark Dredze, Koby Crammer, Lawrence Saul, Fernando Pereira","9:493-500, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ma10a/ma10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ma10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ma10a.html,Some online algorithms for linear classification model the uncertainty in their weights over the course of learning. Modeling the full covariance structure of the weights can provide a significant advantage for classification. However for high-dimensional large-scale data even though there may be many second-order feature interactions it is computationally infeasible to maintain this covariance structure. To extend second-order methods to high-dimensional data we develop low-rank approximations of the covariance structure. We evaluate our approach on both synthetic and real-world data sets using the confidence-weighted online learning framework. We show improvements over diagonal covariance matrices for both low and high-dimensional data.
1326,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Sparse binary zero-sum games,"David Auger, Jianlin Liu, Sylkvie Ruette, David Saint-Pierre, Oliver Teytaud",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/auger14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_auger14,http://jmlr.csail.mit.edu/proceedings/papers/v39/auger14.html,"Solving zero-sum matrix games is polynomial, because it boils down to linear programming. The approximate solving is sublinear by randomized algorithms on machines with random access memory. Algorithms working separately and independently on columns and rows have been proposed, with the same performance; these versions are compliant with matrix games with stochastic reward. [1] has proposed a new version, empirically performing better on sparse problems, i.e. cases in which the Nash equilibrium has small support. In this paper, we propose a variant, similar to their work, also dedicated to sparse problems, with provably better bounds than existing methods. We then experiment the method on a card game."
1327,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,From Tweets to Stories: Using Stream-Dashboard to weave the twitter data stream into dynamic cluster models,"Basheer Hawwash, Olfa Nasraoui","JMLR W&CP 36 :182-197, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/hawwash14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_hawwash14,http://jmlr.csail.mit.edu/proceedings/papers/v36/hawwash14.html,"Social media has recently emerged as an invaluable source of information for decision making. Social media information reflects the interests of virtual communities in a spontaneous and timely manner. The need to understand the massive streams of data generated by social media platforms, such as Twitter and Facebook, has motivated researchers to use machine learning techniques to try to discover knowledge in real time. In this paper, we adapt our recently developed stream cluster mining, tracking and validation framework, Stream-Dashboard, to support detecting and tracking evolving discussion clusters in Twitter. The effectiveness of Stream-Dashboard in telling stories is illustrated by analyzing a couple of stories related to the Louisville Cardinalsê basketball championship. We further validate the detected story lines, that are automatically mined from user-generated tweets using as an alternative source, Google Trends, which are based on search queries."
1328,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Spectral Bandits for Smooth Graph Functions,"Michal Valko, Remi Munos, Branislav Kveton, Tomˆ_ Kocˆk",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/valko14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/valko14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_valko14,http://jmlr.csail.mit.edu/proceedings/papers/v32/valko14.html,"Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of nodes evaluations."
1329,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Learning DNF Expressions from Fourier Spectrum,Vitaly Feldman,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/feldman12b/feldman12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_feldman12b,http://jmlr.csail.mit.edu/proceedings/papers/v23/feldman12b.html,"Since its introduction by Valiant in 1984, PAC learning of DNF expressions remains one of the central problems in learning theory. We consider this problem in the setting where the underlying distribution is uniform, or more generally, a product distribution. Kalai, Samorodnitsky, and Teng (2009b) showed that in this setting a DNF expression can be efficiently approximated from its ""heavy"" low-degree Fourier coefficients alone. This is in contrast to previous approaches where boosting was used and thus Fourier coefficients of the target function modified by various distributions were needed. This property is crucial for learning of DNF expressions over smoothed product distributions, a learning model introduced by Kalai et al. (2009b) and inspired by the seminal smoothed analysis model of Spielman and Teng (2004). We introduce a new approach to learning (or approximating) a polynomial threshold functions which is based on creating a function with range [-1, 1] that approximately agrees with the unknown function on low-degree Fourier coefficients. We then describe conditions under which this is sufficient for learning polynomial threshold functions. Our approach yields a new, simple algorithm for approximating any polynomial-size DNF expression from its ""heavy"" low-degree Fourier coefficients alone. This algorithm greatly simplifies the proof of learnability of DNF expressions over smoothed product distributions and is simpler than all previous algorithm for PAC learning of DNF expression using membership queries. We also describe an application of our algorithm to learning monotone DNF expressions over product distributions. Building on the work of Servedio (2004), we give an algorithm that runs in time poly(( s _ log ( s /_)) log ( s /_) , n ), where s is the size of the DNF expression and _ is the accuracy. This improves on poly(( s _ log ( ns /_)) log ( s /_)_ log(1/_) , n ) bound of Servedio (2004)."
1330,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Multiple Kernel Learning on the Limit Order Book,"Tristan Fletcher, Zakria Hussain and John Shawe-Taylor","11:167_174, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/fletcher10a/fletcher10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_fletcher10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/fletcher10a.html,"Simple features constructed from order book data for the EURUSD currency pair were used to construct a set of kernels. These kernels were used both individually and simultaneously through the Multiple Kernel Learning (MKL) methods of SimpleMKL and the more novel LPBoostMKL to train multiclass Support Vector Machines to predict the direction of future price movements. The kernel methods outperformed a trend following benchmark both in their predictive ability and when used in a simple trading rule. Furthermore, the kernel weightings selected by the MKL techniques highlight which features of the EURUSD order book are the most informative for predictive tasks."
1331,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Consistent estimation of dynamic and multi-layer block models,"Qiuyi Han, Kevin Xu, Edoardo Airoldi",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hanb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hanb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hanb15.html,"Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multi-graph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations."
1332,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Consensus Ranking with Signed Permutations,"Raman Arora, Marina Meila",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/arora13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_arora13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/arora13a.html,Signed permutations (also known as the hyperoctahedral group) are used in modeling genome rearrangements. The algorithmic problems they raise are computationally demanding when not NP-hard. This paper presents a tractable algorithm for learning consensus ranking between signed permutations under the inversion distance. This can be extended to estimate a natural class of exponential models over the group of signed permutations. We investigate experimentally the efficiency of our algorithm for modeling data generated by random reversals.
1333,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Discriminatively Activated Sparselets,"Ross Girshick, Hyun Oh Song, Trevor Darrell",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/girshick13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/girshick13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_girshick13,http://jmlr.csail.mit.edu/proceedings/papers/v28/girshick13.html,"Shared representations are highly appealing due to their potential for gains in computational and statistical efficiency. Compressing a shared representation leads to greater computational savings, but at the same time can severely decrease performance on a target task. Recently, sparselets (Song et al., 2012) were introduced as a new shared intermediate representation for multiclass object detection with deformable part models (Felzenszwalb et al., 2010a), showing significant speedup factors, but with a large decrease in task performance. In this paper we describe a new training framework that learns which sparselets to activate in order to optimize a discriminative objective, leading to larger speedup factors with no decrease in task performance. We first reformulate sparselets in a general structured output prediction framework, then analyze when sparselets lead to computational efficiency gains, and lastly show experimental results on object detection and image classification tasks. Our experimental results demonstrate that discriminative activation substantially outperforms the previous reconstructive approach which, together with our structured output prediction formulation, make sparselets broadly applicable and significantly more effective."
1334,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Simple ensemble methods are competitive with state-of-the-art data integration methods for gene function prediction,"Matteo R_, Giorgio Valentini","8:98-111, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/re10a/re10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_re10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/re10a.html,Several works showed that biomolecular data integration is a key issue to improve the prediction of gene functions. Quite surprisingly only little attention has been devoted to data integration for gene function prediction through ensemble methods. In this work we show that relatively simple ensemble methods are competitive and in some cases are also able to outperform state-of-the-art data integration techniques for gene function prediction.
1335,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Joint Structure Learning of Multiple Non-Exchangeable Networks,"Chris Oates, Sach Mukherjee","JMLR W&CP 33 :687-695, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/oates14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/oates14-supp.zip,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_oates14,http://jmlr.csail.mit.edu/proceedings/papers/v33/oates14.html,"Several methods have recently been developed for joint structure learning of multiple (related) graphical models or networks. These methods treat individual networks as exchangeable, such that each pair of networks are equally encouraged to have similar structures. However, in many practical applications, exchangeability in this sense does not hold, as some pairs of networks may be more closely related than others, for example due to group and sub-group structures in the data. Here we present a novel Bayesian formulation that generalises joint structure learning beyond the exchangeable case. Moreover (i) a novel default prior over the joint structure space is proposed that requires no user input; (ii) latent networks are permitted; (iii) for time series data and dynamic Bayesian networks, an efficient, exact algorithm is provided. We present empirical results on non-exchangeable populations, including a real example from cancer biology, where cell-line specific networks are related according to known genomic features."
1336,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Emerge and spread models and word burstiness,Peter Sunehag,"2:540-547, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/sunehag07a/sunehag07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_sunehag07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/sunehag07a.html,Several authors have recently studied the problem of creating exchangeable models for natural languages that exhibit word burstiness. Word burstiness means that a word that has appeared once in a text should be more likely to appear again than it was to appear in the first place. In this article the different existing methods are compared theoretically through a unifying framework. New models that do not satisfy the exchangeability assumption but whose probability revisions only depend on the word counts of what has previously appeared are introduced within this framework. We will refer to these models as two-stage conditional presence/abundance models since they just like some recently introduced models for the abundance of rare species in ecology seperate the issue of presence from the issue of abundance when present. We will see that the widely used TF-IDF heuristic for information retrieval follows naturally from these models by calculating a crossentropy. We will also discuss a connection between TF-IDF and file formats that seperate presence from abundance given presence.
1337,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Stochastic blockmodeling of relational event dynamics,"Christopher DuBois, Carter Butts, Padhraic Smyth",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/dubois13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_dubois13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/dubois13a.html,"Several approaches have recently been proposed for modeling of continuous-time network data via dyadic event rates conditioned on the observed history of events and nodal or dyadic covariates. In many cases, however, interaction propensities _ and even the underlying mechanisms of interaction _ vary systematically across subgroups whose identities are unobserved. For static networks such heterogeneity has been treated via methods such as stochastic blockmodeling, which operate by assuming latent groups of individuals with similar tendencies in their group-wise interactions. Here we combine ideas from stochastic blockmodeling and continuous-time network models by positing a latent partition of the node set such that event dynamics within and between subsets evolve in potentially distinct ways. We illustrate the use of our model family by application to several forms of dyadic interaction data, including email communication and Twitter direct messages. Parameter estimates from the fitted models clearly reveal heterogeneity in the dynamics among groups of individuals. We also find that the fitted models have better predictive accuracy than both baseline models and relational event models that lack latent structure."
1338,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,"Stephane Ross, Geoffrey Gordon, Drew Bagnell","15:627-635, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/ross11a/ross11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_ross11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/ross11a.html,Sequential prediction problems such as imitation learning where future observations depend on previous predictions (actions) violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper we propose a new iterative algorithm which trains a stationary deterministic policy that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm combined with additional reduction assumptions must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.
1339,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Clockwork RNN,"Jan Koutnik, Klaus Greff, Faustino Gomez, Juergen Schmidhuber",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/koutnik14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_koutnik14,http://jmlr.csail.mit.edu/proceedings/papers/v32/koutnik14.html,"Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs."
1340,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Beyond Sentiment: The Manifold of Human Emotions,"Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/kim13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_kim13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/kim13a.html,"Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities. Besides obtaining significant improvements over a baseline without manifold, we are also able to visualize different notions of positive sentiment in different domains."
1341,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,High Order Regularization for Semi-Supervised Learning of Structured Output Problems,"Yujia Li, Rich Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lif14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lif14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lif14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lif14.html,"Semi-supervised learning, which uses unlabeled data to help learn a discriminative model, is especially important for structured output problems, as considerably more effort is needed to label its multidimensional outputs versus standard single output problems. We propose a new max-margin framework for semi-supervised structured output learning, that allows the use of powerful discrete optimization algorithms and high order regularizers defined directly on model predictions for the unlabeled examples. We show that our framework is closely related to Posterior Regularization, and the two frameworks optimize special cases of the same objective. The new framework is instantiated on two image segmentation tasks, using both a graph regularizer and a cardinality regularizer. Experiments also demonstrate that this framework can utilize unlabeled data from a different source than the labeled data to significantly improve performance while saving labeling effort."
1342,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Deep Semi-NMF Model for Learning Hidden Representations,"George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern Schuller",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/trigeorgis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/trigeorgis14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_trigeorgis14,http://jmlr.csail.mit.edu/proceedings/papers/v32/trigeorgis14.html,"Semi-NMF is a matrix factorization technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original features contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming Semi-NMF, but also other NMF variants."
1343,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Controlling Selection Bias in Causal Inference,"Elias Bareinboim, Judea Pearl",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bareinboim12/bareinboim12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bareinboim12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bareinboim12.html,Selection bias caused by preferential exclusion of samples from the data is a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can hardly be detected in either experimental or observational studies. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize previously reported results and identify the type of knowledge that is needed for reasoning in the presence of selection bias. Specifically we derive a general condition together with a procedure for deciding recoverability of the odds ratio (OR) from s-biased data. We show that recoverability is feasible if and only if our condition holds. We further offer a new method of controlling selection bias using instrumental variables that permits the recovery of other effect measures besides OR.
1344,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Approximation of hidden Markov models by mixtures of experts with application to particle filtering,"Jimmy Olsson, Jonas Str_jby","9:573-580, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/olsson10a/olsson10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_olsson10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/olsson10a.html,Selecting conveniently the proposal kernel and the adjustment multiplier weights of the auxiliary particle filter may increase significantly the accuracy and computational efficiency of the method. However in practice the optimal proposal kernel and multiplier weights are seldom known. In this paper we present a simulation-based method for constructing offline an approximation of these quantities that makes the filter close to fully adapted at a reasonable computational cost. The approximation is constructed as a mixture of experts optimised through an efficient stochastic approximation algorithm. The method is illustrated on two simulated examples.
1345,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Online Anomaly Detection under Adversarial Impact,"Marius Kloft, Pavel Laskov","9:405-412, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kloft10a/kloft10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kloft10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kloft10a.html,Security analysis of learning algorithms is gaining increasing importance especially since they have become target of deliberate obstruction in certain applications. Some security-hardened algorithms have been previously proposed for supervised learning; however very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution we analyze the performance of a particular method---online centroid anomaly detection---in the presence of adversarial noise. Our analysis addresses three key security-related issues: derivation of an optimal attack analysis of its efficiency and constraints. Experimental evaluation carried out on real HTTP and exploit traces confirms the tightness of our theoretical bounds.
1346,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Theory and Algorithms for revenue optimization in second price auctions with reserve,"Mehryar Mohri, Andres Munoz Medina",none,http://jmlr.org/proceedings/papers/v32/mohri14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mohri14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mohri14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mohri14.html,Second-price auctions with reserve play a critical role for modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function (it is non-convex and discontinuous). We further give novel algorithms for solving this problem and report the results of encouraging experiments demonstrating their effectiveness.
1347,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix,"Roger Grosse, Ruslan Salakhudinov",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/grosse15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_grosse15,http://jmlr.csail.mit.edu/proceedings/papers/v37/grosse15.html,"Second-order optimization methods, such as natural gradient, are difficult to apply to high-dimensional problems, because they require approximately solving large linear systems. We present FActorized Natural Gradient (FANG), an approximation to natural gradient descent where the Fisher matrix is approximated with a Gaussian graphical model whose precision matrix can be computed efficiently. We analyze the Fisher matrix for a small RBM and derive an extremely sparse graphical model which is a good match to the covariance of the sufficient statistics. Our experiments indicate that FANG allows RBMs to be trained more efficiently compared with stochastic gradient descent. Additionally, our analysis yields insight into the surprisingly good performance of the –centering trick” for training RBMs."
1348,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Mind the duality gap: safer rules for the Lasso,"Olivier Fercoq, Alexandre Gramfort, Joseph Salmon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/fercoq15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/fercoq15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_fercoq15,http://jmlr.csail.mit.edu/proceedings/papers/v37/fercoq15.html,"Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules."
1349,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,The Gamma Operator for Big Data Summarization on an Array DBMS,"Carlos Ordonez, Yiqun Zhang, Wellington Cabrera","JMLR W&CP 36 :88-103, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/ordonez14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_ordonez14,http://jmlr.csail.mit.edu/proceedings/papers/v36/ordonez14.html,"SciDB is a parallel array DBMS that provides multidimensional arrays, a query language and basic ACID properties. In this paper,we introduce a summarization matrix operator that computes sufficient statistics in one pass and in parallel on an array DBMS. Such sufficient statistics benefit a big family of statistical and machine learning models, including PCA, linear regression and variable selection. Experimental evaluation on a parallel cluster shows our matrix operator exhibits linear time complexity and linear speedup. Moreover, our operator is shown to be an order of magnitude faster than SciDB built-in operators, two orders of magnitude faster than SQL queries on a fast column DBMS and even faster than the R package when the data set fits in RAM. We show SciDB operators and the R package fail due to RAM limitations, whereas our operator does not. We also show PCA and linear regression computation is reduced to a few minutes for large data sets. On the other hand, a Gibbs sampler for variable selection can iterate much faster in the array DBMS than in R, exploiting the summarization matrix."
1350,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Deep learning with COTS HPC systems,"Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, Ng Andrew",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/coates13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_coates13,http://jmlr.csail.mit.edu/proceedings/papers/v28/coates13.html,"Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks."
1351,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Coresets for Nonparametric Estimation - the Case of DP-Means,"Olivier Bachem, Mario Lucic, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachem15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachem15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bachem15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bachem15.html,"Scalable training of Bayesian nonparametric models is a notoriously difficult challenge. We explore the use of coresets - a data summarization technique originating from computational geometry - for this task. Coresets are weighted subsets of the data such that models trained on these coresets are provably competitive with models trained on the full dataset. Coresets sublinear in the dataset size allow for fast approximate inference with provable guarantees. Existing constructions, however, are limited to parametric problems. Using novel techniques in coreset construction we show the existence of coresets for DP-Means - a prototypical nonparametric clustering problem - and provide a practical construction algorithm. We empirically demonstrate that our algorithm allows us to efficiently trade off computation time and approximation error and thus scale DP-Means to large datasets. For instance, with coresets we can obtain a computational speedup of 45x at an approximation error of only 2.4% compared to solving on the full data set. In contrast, for the same subsample size, the –naive” approach of uniformly subsampling the data incurs an approximation error of 22.5%."
1352,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,SampleSearch: A Scheme that Searches for Consistent Samples,"Vibhav Gogate, Rina Dechter","2:147-154, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/gogate07a/gogate07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_gogate07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/gogate07a.html,Sampling from belief networks which have a substantial number of zero probabilities is problematic. MCMC algorithms like Gibbs sampling do not converge and importance sampling schemes generate many zero weight samples that are rejected yielding an inefficient sampling process (the rejection problem). In this paper we propose to augment importance sampling with systematic constraint-satisfaction search in order to overcome the rejection problem. The resulting SampleSearch scheme can be made unbiased by using a computationally expensive weighting scheme. To overcome this an approximation is proposed such that the resulting estimator is asymptotically unbiased. Our empirical results demonstrate the potential of our new scheme.
1353,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Spoofing Large Probability Mass Functions to Improve Sampling Times and Reduce Memory Costs,"Jon Parker, Hans Engler","JMLR W&CP 33 :743-750, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/parker14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_parker14,http://jmlr.csail.mit.edu/proceedings/papers/v33/parker14.html,Sampling from a probability mass function (PMF) has many applications in modern computing. This paper presents a novel lossy compression method intended for large ( \(O(10^5)\) ) dense PMFs that speeds up the sampling process and guarantees high fidelity sampling. This compression method closely approximates an input PMF P with another PMF Q that is easy to store and sample from. All samples are drawn from Q as opposed to the original input distribution P. We say that Q –spoofs” P while this switch is difficult to detect with a statistical test. The lifetime of Q is the sample size required to detect the switch from P to Q. We show how to compute a single PMFês lifetime and present numeric examples demonstrating compression rates ranging from 62% to 75% when the input PMF is not sorted and 88% to 99% when the input is already sorted. These examples have speed ups ranging from 1.47 to 2.82 compared to binary search sampling.
1354,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Coherent Inference on Optimal Play in Game Trees,"Philipp Hennig, David Stern, Thore Graepel","9:326-333, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/hennig10a/hennig10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_hennig10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/hennig10a.html,Round-based games are an instance of discrete planning problems. Some of the best contemporary game tree search algorithms use random roll-outs as data. Relying on a good policy they learn on-policy values by propagating information upwards in the tree but not between sibling nodes. Here we present a generative model and a corresponding approximate message passing scheme for inference on the optimal off-policy value of nodes in smooth AND/OR trees given random roll-outs. The crucial insight is that the distribution of values in game trees is not completely arbitrary. We define a generative model of the on-policy values using a latent score for each state representing the value under the random roll-out policy. Inference on the values under the optimal policy separates into an inductive pre-data step and a deductive post-data part. Both can be solved approximately with Expectation Propagation allowing off-policy value inference for any node in the (exponentially big) tree in linear time.
1355,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Learning Class-relevant Features and Class-irrelevant Features via a Hybrid third-order RBM,"Heng Luo, Ruimin Shen, Changyong Niu, Carsten Ullrich","15:470-478, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/luo11a/luo11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_luo11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/luo11a.html,Restricted Boltzmann Machines are commonly used in unsupervised learning to extract features from training data. Since these features are learned for regenerating training data a classifier based on them has to be trained. If only a few of the learned features are discriminative other non-discriminative features will distract the classifier during the training process and thus waste computing resources for testing. In this paper we present a hybrid third-order Restricted Boltzmann Machine in which class-relevant features (for recognizing) and class-irrelevant features (for generating only) are learned simultaneously. As the classification task uses only the class-relevant features the test itself becomes very fast. We show that class-irrelevant features help class-relevant features to focus on the recognition task and introduce useful regularization effects to reduce the norms of class-relevant features. Thus there is no need to use weight-decay for the parameters of this model. Experiments on the MNIST NORB and Caltech101 Silhouettes datasets show very promising results.
1356,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Stochastic Spectral Descent for Restricted Boltzmann Machines,"David Carlson, Volkan Cevher, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/carlson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/carlson15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_carlson15,http://jmlr.csail.mit.edu/proceedings/papers/v38/carlson15.html,"Restricted Boltzmann Machines (RBMs) are widely used as building blocks for deep learning models. Learning typically proceeds by using stochastic gradient descent, and the gradients are estimated with sampling methods. However, the gradient estimation is a computational bottleneck, so better use of the gradients will speed up the descent algorithm. To this end, we first derive upper bounds on the RBM cost function, then show that descent methods can have natural ad- vantages by operating in the Land Shatten-norm. We introduce a new method called –Stochastic Spectral Descent” that updates parameters in the normed space. Empirical results show dramatic improvements over stochastic gradient descent, and have only have a fractional increase on the per-iteration cost."
1357,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Switch-Reset Models : Exact and Approximate Inference,"Chris Bracegirdle, David Barber","15:190-198, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/bracegirdle11a/bracegirdle11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_bracegirdle11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/bracegirdle11a.html,Reset models are constrained switching latent Markov models in which the dynamics either continues according to a standard model or the latent variable is resampled. We consider exact marginal inference in this class of models and their extension the switch-reset models. A further convenient class of conjugate-exponential reset models is also discussed. For a length T time-series exact filtering scales with T squared and smoothing T cubed. We discuss approximate filtering and smoothing routines that scale linearly with T. Applications are given to change-point models and reset linear dynamical systems.
1358,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Discriminative Features via Generalized Eigenvectors,"Nikos Karampatziakis, Paul Mineiro",none,http://jmlr.org/proceedings/papers/v32/karampatziakis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_karampatziakis14,http://jmlr.csail.mit.edu/proceedings/papers/v32/karampatziakis14.html,"Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results."
1359,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,A comparison of AUC estimators in small-sample studies,"Antti Airola, Tapio Pahikkala, Willem Waegeman, Bernard De Baets, Tapio Salakoski","8:3-13, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/airola10a/airola10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_airola10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/airola10a.html,Reliable estimation of the classification performance of learned predictive models is difficult when working in the small sample setting. When dealing with biological data it is often the case that separate test data cannot be afforded. Cross-validation is in this case a typical strategy for estimating the performance. Recent results further supported by experimental evidence presented in this article show that many standard approaches to cross-validation suffer from extensive bias or variance when the area under ROC curve (AUC) is used as performance measure. We advocate the use of leave-pair-out cross-validation (LPOCV) for performance estimation as it avoids many of these problems. A method previously proposed by us can be used to efficiently calculate this estimate for regularized least-squares (RLS) based learners.
1360,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,EPMC: Every Visit Preference Monte Carlo for Reinforcement Learning,"Christian Wirth, Johannes Fôrnkranz","JMLR W&CP 29 :483-497, 2013",http://jmlr.org/proceedings/papers/v29/Wirth13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Wirth13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Wirth13.html,"Reinforcement learning algorithms are usually hard to use for non expert users. It is required to consider several aspects like the definition of state-, action- and reward-space as well as the algorithms hyperparameters. Preference based approaches try to address these problems by omitting the requirement for exact rewards, replacing them with preferences over solutions. Some algorithms have been proposed within this framework, but they are usually requiring parameterized policies which is again a hinderance for their application. Monte Carlo based approaches do not have this restriction and are also model free. Hence, we present a new preference-based reinforcement learning algorithm, utilizing Monte Carlo estimates. The main idea is to estimate the relative Q-value of two actions for the same state within a every-visit framework. This means, preferences are used to estimate the Q-value of state-action pairs within a trajectory, based on the feedback concerning the complete trajectory. The algorithm is evaluated on three common benchmark problems, namely mountain car, inverted pendulum and acrobot, showing its advantage over a closely related algorithm which is also using estimates for intermediate states, but based on a probability theorem. In comparison to SARSA(_), EPMC converges somewhat slower, but computes policies that are almost as good or better."
1361,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Accelerated Stochastic Gradient Method for Composite Regularization,"Wenliang Zhong, James Kwok","JMLR W&CP 33 :1086-1094, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/zhong14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_zhong14,http://jmlr.csail.mit.edu/proceedings/papers/v33/zhong14.html,"Regularized risk minimization often involves nonsmooth optimization. This can be particularly challenging when the regularizer is a sum of simpler regularizers, as in the overlapping group lasso. Very recently, this is alleviated by using the proximal average, in which an implicitly nonsmooth function is employed to approximate the composite regularizer. In this paper, we propose a novel extension with accelerated gradient method for stochastic optimization. On both general convex and strongly convex problems, the resultant approximation errors reduce at a faster rate than methods based on stochastic smoothing and ADMM. This is also verified experimentally on a number of synthetic and real-world data sets."
1362,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Distributed training of Large-scale Logistic models,"Siddharth Gopal, Yiming Yang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gopal13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gopal13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gopal13.html,"Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time."
1363,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,"Fast, Exact Model Selection and Permutation Testing for l2-Regularized Logistic Regression","Bryan Conroy, Paul Sajda",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/conroy12/conroy12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_conroy12,http://jmlr.csail.mit.edu/proceedings/papers/v22/conroy12.html,Regularized logistic regression is a standard classification method used in statistics and machine learning. Unlike regularized least squares problems such as ridge regression the parameter estimates cannot be computed in closed-form and instead must be estimated using an iterative technique. This paper addresses the computational problem of regularized logistic regression that is commonly encountered in model selection and classifier statistical significance testing in which a large number of related logistic regression problems must be solved for. Our proposed approach solves the problems simultaneously through an iterative technique which also garners computational efficiencies by leveraging the redundancies across the related problems. We demonstrate analytically that our method provides a substantial complexity reduction which is further validated by our results on real-world datasets.
1364,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Minimizing Nonconvex Non-Separable Functions,"Yaoliang Yu, Xun Zheng, Micol Marchetti-Bowick, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/yu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/yu15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_yu15,http://jmlr.csail.mit.edu/proceedings/papers/v38/yu15.html,"Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference. A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances. In a different but analogous vein, nonconvex loss functions are promoted because of their robustness against –outliers”. However, these nonconvex formulations are computationally more challenging, especially in the presence of nonsmoothness and non-separability. To address this issue, we propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting. We formally prove its nice convergence properties, and illustrate its effectiveness on two applications: multi-task graph-guided fused lasso and robust support vector machines. Experiments demonstrate that our method compares favorably against other alternatives."
1365,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Transductive Learning of Structural SVMs via Prior Knowledge Constraints,Chun-Nam Yu,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/yu12/yu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_yu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/yu12.html,Reducing the number of labeled examples required to learn accurate prediction models is an important problem in structured output prediction. In this paper we propose a new transductive structural SVM algorithm that learns by incorporating prior knowledge constraints on unlabeled data. Our formulation supports different types of prior knowledge constraints and can be trained efficiently. Experiments on two citation and advertisement segmentation tasks show that our transductive structural SVM can learn effectively from unlabeled data achieving similar prediction accuracies when compared against other state-of-art algorithms.
1366,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Convex Learning of Multiple Tasks and their Structure,"Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ciliberto15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ciliberto15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ciliberto15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ciliberto15.html,"Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum."
1367,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery,"Cun Mu, Bo Huang, John Wright, Donald Goldfarb",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mu14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mu14.html,"Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms (SNN) of the unfolding matrices of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a \(K\) -way \(n\) \(\times\) \(n\) \(\times\) \(\cdots\) \(\times n\) tensor of Tucker rank \((r, r, \ldots, r)\) from Gaussian measurements requires \(\Omega( r n^{K-1} )\) observations. In contrast, a certain (intractable) nonconvex formulation needs only \(O(r^K + nrK)\) observations. We introduce a simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with \(O(r^{\lfloor K/2 \rfloor}n^{\lceil K/2 \rceil})\) observations. The lower bound for the SNN model follows from our new result on recovering signals with multiple structures (e.g. sparse, low rank), which indicates the significant suboptimality of the common approach of minimizing the sum of individual sparsity inducing norms (e.g. \(\ell_1\) , nuclear norm). Our new tractable formulation for low-rank tensor recovery shows how the sample complexity can be reduced by designing convex regularizers that exploit several structures jointly."
1368,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,"Greedy Bilateral Sketch, Completion & Smoothing","Tianyi Zhou, Dacheng Tao",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/zhou13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_zhou13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/zhou13b.html,"Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of ""greedy bilateral (GreB)"" paradigm in reducing both time and sample complexities for solving these problems. GreB models a low-rank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust PCA in GreBês paradigm. On their MATLAB implementations, approximating a noisy 10000x10000 matrix of rank 500 with SVD accuracy takes 6s; MovieLens10M matrix of size 69878x10677 can be completed in 10s from 30% of \(10^7\) ratings with RMSE 0.86 on the rest 70%; the low-rank background and sparse moving outliers in a 120x160 video of 500 frames are accurately separated in 1s. This brings 30 to 100 times acceleration in solving these popular statistical problems."
1369,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,The Love-Hate Square Counting Method for Recommender Systems,"J.S. Kong, K. Teague & J. Kessler","18:249_261, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/kong12a/kong12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_kong12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/kong12a.html,Recommender systems provide personalized suggestions to users and are critical to the success of many e-commerce sites such as Net§ix and Amazon. Outside of e-commerce recommender systems can be deployed in _elds such as intelligence analysis for recommending high-quality information source to analysts for further examination. In this work we present the square counting method for rating predictions in recommender systems. Our method is based on analyzing the bipartite rating network with score-labeled edges representing user nodesÍ ratings to item nodes. Edges are denoted as an I-love-it or I-hate-it edge based on whether the rating score on the edge is above or below a threshold. For a target user-item pair we count the number for each con_guration of love-hate squares that involve the target pair where the sequence of I-love-it or I-hate-it edges determine the particular con_guration. The counts are used as features in a supervised machine learning framework for training and rating prediction. The method is implemented and empirically evaluated on a large-scale Yahoo! music user-item rating dataset. Results show that the square counting method is fast simple to parallelize scalable to massive datasets and makes highly accurate predictions. Finally we report an interesting empirical _nding that con_gurations with consecutive I-hate-it edges seem to provide the most powerful signal in predicting a userÍs love for an item. (Approved for public release by Northrop Grumman Information Systems ISHQ-2011-0042. The work was entirely performed when Kyle Teague was at Northrop Grumman.)   Page last modified on Tue May 29 10:23:41 2012.
1370,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Ordinal Random Fields for Recommender Systems,"Shaowu Liu, Truyen Tran, Gang Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/liu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_liu14,http://jmlr.csail.mit.edu/proceedings/papers/v39/liu14.html,"Recommender Systems heavily rely on numerical preferences, whereas the importance of ordinal preferences has only been recognised in recent works of Ordinal Matrix Factorisation (OMF). Although the OMF can effectively exploit ordinal properties, it captures only the higher-order interactions among users and items, without considering the localised interactions properly. This paper employs Markov Random Fields (MRF) to investigate the localised interactions, and proposes a unified model called Ordinal Random Fields (ORF) to take advantages of both the representational power of the MRF and the ease of modelling ordinal preferences by the OMF. Experimental result on public datasets demonstrates that the proposed ORF model can capture both types of interactions, resulting in improved recommendation accuracy."
1371,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Novel Boosting Frameworks to Improve the Performance of Collaborative Filtering,"Xiaotian Jiang, Zhendong Niu, Jiamin Guo, Ghulam Mustafa, Zihan Lin, Baomi Chen, Qian Zhou","JMLR W&CP 29 :87-99, 2013",http://jmlr.org/proceedings/papers/v29/Jiang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Jiang13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Jiang13.html,"Recommender systems are often based on collaborative filtering. Previous researches on collaborative filtering mainly focus on one single recommender or formulating hybrid with different approaches. In consideration of the problems of sparsity, recommender error rate, sample weight update, and potential, we adapt AdaBoost and propose two novel boosting frameworks for collaborative filtering. Each of the frameworks combines multiple homogeneous recommenders, which are based on the same collaborative filtering algorithm with different sample weights. We use seven popular collaborative filtering algorithms to evaluate the two frameworks with two MovieLens datasets of different scale. Experimental result shows the proposed frameworks improve the performance of collaborative filtering."
1372,1,http://jmlr.csail.mit.edu/proceedings/papers/v1/,Multi-class Semi-supervised Learning with the _-truncated Multinomial Probit Gaussian Process,"Simon Rogers, Mark Girolami","1:17-32, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v1/rogers07a/rogers07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v1/,,11th March 2007,"June 12-13, 2006",Gaussian Processes in Practice,Gaussian Processes in Practice,"Bletchley Park, Bletchley, U.K.","Neil Lawrence, Anton Schwaighofer and Joaquin QuiÕ±onero Candela",v1_rogers07a,http://jmlr.csail.mit.edu/proceedings/papers/v1/rogers07a.html,"Recently, the null category noise model has been proposed as a simple and elegant solution to the problem of incorporating unlabeled data into a Gaussian process (GP) classification model. In this paper, we show how this binary likelihood model can be generalised to the multi-class setting through the use of the multinomial probit GP classifier. We present a Gibbs sampling scheme for sampling the GP parameters and also derive a more efficient variational updating scheme. We find that the performance improvement is roughly consistent with that observed in binary classification and that there is no significant difference in classification performance between the Gibbs sampling and variational schemes."
1373,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Sequential Kernel Herding: Frank-Wolfe Optimization for Particle Filtering,"Simon Lacoste-Julien, Fredrik Lindsten, Francis Bach",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lacoste-julien15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lacoste-julien15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lacoste-julien15,http://jmlr.csail.mit.edu/proceedings/papers/v38/lacoste-julien15.html,"Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure to obtain adaptive quadrature rules for integrals of functions in a reproducing kernel Hilbert space (RKHS) with a potentially faster rate of convergence than Monte Carlo integration (and –kernel herding” was shown to be a special case of this procedure). In this paper, we propose to replace the random sampling step in a particle filter by Frank-Wolfe optimization. By optimizing the position of the particles, we can obtain better accuracy than random or quasi-Monte Carlo sampling. In applications where the evaluation of the emission probabilities is expensive (such as in robot localization), the additional computational cost to generate the particles through optimization can be justified. Experiments on standard synthetic examples as well as on a robot localization task indicate indeed an improvement of accuracy over random and quasi-Monte Carlo sampling."
1374,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Cost-Sensitive Tree of Classifiers,"Zhixiang Xu, Matt Kusner, Kilian Weinberger, Minmin Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_xu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13.html,"Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across features. We incorporate this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost."
1375,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Hashing for Distributed Data,"Cong Leng, Jiaxiang Wu, Jian Cheng, Xi Zhang, Hanqing Lu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/leng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_leng15,http://jmlr.csail.mit.edu/proceedings/papers/v37/leng15.html,"Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method."
1376,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Grammatical Inference of some Probabilistic Context-Free Grammars from Positive Data using Minimum Satisfiability,"James Scicluna, Colin de la Higuera","JMLR W&CP 34 :139-152, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/scicluna14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_scicluna14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/scicluna14a.html,"Recently, different theoretical learning results have been found for a variety of context-free grammar subclasses through the use of distributional learning (Clark, 2010b). However, these results are still not extended to probabilistic grammars. In this work, we give a practical algorithm, with some proven properties, that learns a subclass of probabilistic grammars from positive data. A minimum satisfiability solver is used to direct the search towards small grammars. Experiments on typical context-free languages and artificial natural language grammars give positive results."
1377,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Structured Low-Rank Matrix Factorization: Optimality, Algorithm, and Applications to Image Processing","Benjamin Haeffele, Eric Young, Rene Vidal",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/haeffele14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/haeffele14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_haeffele14,http://jmlr.csail.mit.edu/proceedings/papers/v32/haeffele14.html,"Recently, convex solutions to low-rank matrix factorization problems have received increasing attention in machine learning. However, in many applications the data can display other structures beyond simply being low-rank. For example, images and videos present complex spatio-temporal structures, which are largely ignored by current low-rank methods. In this paper we explore a matrix factorization technique suitable for large datasets that captures additional structure in the factors by using a projective tensor norm, which includes classical image regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is not convex, we show that under certain conditions on the factors, any local minimizer for the factors yields a global minimizer for their product. Examples in biomedical video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets."
1378,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,"\(\ell_{1,p}\)-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods","Zirui Zhou, Qi Zhang, Anthony Man-Cho So",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoub15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhoub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoub15.html,"Recently, \(\ell_{1,p}\) -regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of \(\ell_{1,p}\) -regularized problems, an error bound condition is satisfied when \(p\in[1,2]\) or \(p=\infty\) but fails to hold for any \(p\in(2,\infty)\) . Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to \(\ell_{1,p}\) -regularized linear or logistic regression with \(p\in[1,2]\) or \(p=\infty\) . By contrast, numerical experiments suggest that for the same class of problems with \(p\in(2,\infty)\) , the aforementioned methods may not converge linearly."
1379,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Selective Sampling with Drift,"Edward Moroshko, Koby Crammer","JMLR W&CP 33 :651-659, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/moroshko14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/moroshko14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_moroshko14,http://jmlr.csail.mit.edu/proceedings/papers/v33/moroshko14.html,"Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting."
1380,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Rollout-based Game-tree Search Outprunes Traditional Alpha-beta,"Ari Weinstein, Michael L. Littman, Sergiu Goschin","24:155-167, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/weinstein12a/weinstein12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_weinstein12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/weinstein12a.html,"Recently rollout-based planning and search methods have emerged as an alternative to traditional tree-search methods. The fundamental operation in rollout-based tree search is the generation of trajectories in the search tree from root to leaf. Game-playing programs based on Monte-Carlo rollouts methods such as ""UCT"" have proven remarkably effective at using information from trajectories to make state-of-the-art decisions at the root. In this paper we show that trajectories can be used to prune more aggressively than classical alpha-beta search. We modify a rollout-based method FSSS to allow for use in game-tree search and show it outprunes alpha-beta both empirically and formally."
1381,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Dual Temporal Difference Learning,"Min Yang, Yuxi Li, Dale Schuurmans","5:631-638, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/yang09a/yang09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_yang09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/yang09a.html,Recently researchers have investigated novel dual representations as a basis for dynamic programming and reinforcement learning algorithms. Although the convergence properties of classical dynamic programming algorithms have been established for dual representations temporal difference learning algorithms have not yet been analyzed. In this paper we study the convergence properties of temporal difference learning using dual representations. We contribute significant progress by proving the convergence of dual temporal difference learning with eligibility traces. Experimental results suggest that the dual algorithms seem to demonstrate empirical benefits over standard primal algorithms.
1382,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Variational Inference for Sequential Distance Dependent Chinese Restaurant Process,"Sergey Bartunov, Dmitry Vetrov",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bartunov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bartunov14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bartunov14.html,"Recently proposed distance dependent Chinese Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP) by accounting for dependencies between data points. Its posterior is intractable and so far only MCMC methods were used for inference. Because of very different nature of ddCRP no prior developments in variational methods for Bayesian nonparametrics are appliable. In this paper we propose novel variational inference for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate its efficiency comparing to Gibbs sampler. We also apply our variational approximation to CRP-equivalent seqddCRP-mixture model, where it could be considered as alternative to one based on truncated stick-breaking representation. This allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process."
1383,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Forward Basis Selection for Sparse Approximation over Dictionary,"Xiaotong Yuan, Shuicheng Yan",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/yuan12/yuan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_yuan12,http://jmlr.csail.mit.edu/proceedings/papers/v22/yuan12.html,Recently forward greedy selection method has been successfully applied to approximately solve sparse learning problems characterized by a trade-off between sparsity and accuracy. In this paper we generalize this method to the setup of sparse approximation over a pre-fixed dictionary. A fully corrective forward selection algorithm is proposed along with convergence analysis. The per-iteration computational overhead of the proposed algorithm is dominated by a subproblem of linear optimization over the dictionary and a subproblem to optimally adjust the aggregation weights. The former is cheaper in several applications than the Euclidean projection while the latter is typically an unconstrained optimization problem which is relatively easy to solve. Furthermore we extend the proposed algorithm to the setting of non-negative/convex sparse approximation over a dictionary.Applications of our algorithms to several concrete learning problems are explored with efficiency validated on benchmark data sets.
1384,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images,Kyunghyun Cho,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cho13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cho13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cho13.html,"Recently Burger et al. (2012) and Xie et al. (2012) proposed to use a denoising autoencoder (DAE) for denoising noisy images. They showed that a plain, deep DAE can denoise noisy images as well as the conventional methods such as BM3D and KSVD. Both of them approached image denoising by denoising small, image patches of a larger image and combining them to form a clean image. In this setting, it is usual to use the encoder of the DAE to obtain the latent representation and subsequently apply the decoder to get the clean patch. We propose that a simple sparsification of the latent representation found by the encoder improves denoising performance, when the DAE was trained with sparsity regularization. The experiments confirm that the proposed sparsification indeed helps both denoising a small image patch and denoising a larger image consisting of those patches. Furthermore, it is found out that the proposed method improves even classification performance when test samples are corrupted with noise."
1385,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Semi-Supervised Affinity Propagation with Instance-Level Constraints,"Inmar Givoni, Brendan Frey","5:161-168, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/givoni09a/givoni09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_givoni09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/givoni09a.html,Recently affinity propagation (AP) was introduced as an unsupervised learning algorithm for exemplar based clustering. Here we extend the AP model to account for semi-supervised clustering. AP which is formulated as inference in a factor-graph can be naturally extended to account for ?instance-level? constraints: pairs of data points that cannot belong to the same cluster (cannot-link) or must belong to the same cluster (must-link). We present a semi-supervised AP algorithm (SSAP) that can use instance-level constraints to guide the clustering. We demonstrate the applicability of SSAP to interactive image segmentation by using SSAP to cluster superpixels while taking into account user instructions regarding which superpixels belong to the same object. We demonstrate SSAP can achieve better performance compared to other semi-supervised methods.
1386,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Preface,"Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/fan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_fan15,http://jmlr.csail.mit.edu/proceedings/papers/v41/fan15.html,"Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)-based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models. The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years."
1387,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Estimating Latent-Variable Graphical Models using Moments and Likelihoods,"Arun Tejasvi Chaganty, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chaganty14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chaganty14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chaganty14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chaganty14.html,"Recent work in method of moments provide consistent estimates for latent-variable models, avoiding local optima issues, but these methods can only be applied to certain types of graphical models. In this work, we show that the method of moments in conjunction with a composite marginal likelihood objective yields consistent parameter estimates for a much broader class of directed and undirected graphical models, including loopy graphs with high treewidth. Specifically, we use tensor factorization to reveal partial information about the hidden variables, rendering the otherwise non-convex negative log-likelihood convex. Our approach gracefully extends to models outside our class by incorporating the partial information via posterior regulraization."
1388,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Pitfalls in the use of Parallel Inference for the Dirichlet Process,"Yarin Gal, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gal14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gal14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gal14.html,"Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it _ work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process."
1389,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Deep Learners Benefit More from Out-of-Distribution Examples,"Yoshua Bengio, Fr_d_ric Bastien, Arnaud Bergeron, Nicolas Boulanger_Lewandowski, Thomas Breuel, Youssouf Chherawala, Moustapha Cisse, Myriam C»t_, Dumitru Erhan, Jeremy Eustache, Xavier Glorot, Xavier Muller, Sylvain Pannetier Lebeuf, Razvan Pascanu, Salah Rifai, François Savard, Guillaume Sicard","15:164-172, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/bengio11b/bengio11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_bengio11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/bengio11b.html,Recent theoretical and empirical work in statistical machine learning has demonstrated the potential of learning algorithms for deep architectures i.e. function classes obtained by composing multiple levels of representation. The hypothesis evaluated here is that intermediate levels of representation because they can be shared across tasks and examples from different but related distributions can yield even more benefits. Comparative experiments were performed on a large-scale handwritten character recognition setting with 62 classes (upper case lower case digits) using both a multi-task setting and perturbed examples in order to obtain out-of-distribution examples. The results agree with the hypothesis and show that a deep learner did {\em beat previously published results and reached human-level performance}.
1390,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Convergent Learning: Do different neural networks learn the same representations?,"Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/li15convergent.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_li15convergent,http://jmlr.csail.mit.edu/proceedings/papers/v44/li15convergent.html,"Recent successes in training large, deep neural networks (DNNs) have prompted active investigation into the underlying representations learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters. However, despite the difficulty, such research is valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. We argue for the value of investigating whether neural networks exhibit what we call convergent learning, which is when separately trained DNNs learn features that converge to span similar spaces. We further begin research into this question by introducing two techniques to approximately align neurons from two networks: a bipartite matching approach that makes one-to-one assignments between neurons and a spectral clustering approach that finds many-to-many mappings. Our initial approach to answering this question reveals many interesting, previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; and (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution."
1391,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Transferable Features with Deep Adaptation Networks,"Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/long15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_long15,http://jmlr.csail.mit.edu/proceedings/papers/v37/long15.html,"Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks."
1392,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,PAC-Bayesian Collective Stability,"Ben London, Bert Huang, Ben Taskar, Lise Getoor","JMLR W&CP 33 :585-594, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/london14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/london14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_london14,http://jmlr.csail.mit.edu/proceedings/papers/v33/london14.html,"Recent results have shown that the generalization error of structured predictors decreases with both the number of examples and the size of each example, provided the data distribution has weak dependence and the predictor exhibits a smoothness property called collective stability. These results use an especially strong definition of collective stability that must hold uniformly over all inputs and all hypotheses in the class. We investigate whether weaker definitions of collective stability suffice. Using the PAC-Bayes framework, which is particularly amenable to our new definitions, we prove that generalization is indeed possible when uniform collective stability happens with high probability over draws of predictors (and inputs). We then derive a generalization bound for a class of structured predictors with variably convex inference, which suggests a novel learning objective that optimizes collective stability."
1393,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Inductive Principles for Restricted Boltzmann Machine Learning,"Benjamin Marlin, Kevin Swersky, Bo Chen, Nando de Freitas","9:509-516, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/marlin10a/marlin10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_marlin10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/marlin10a.html,Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood contrastive divergence and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.
1394,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Online Optimization : Competing with Dynamic Comparators,"Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, Karthik Sridharan",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/jadbabaie15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/jadbabaie15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_jadbabaie15,http://jmlr.csail.mit.edu/proceedings/papers/v38/jadbabaie15.html,"Recent literature on online learning has focused on developing adaptive algorithms that take advantage of a regularity of the sequence of observations, yet retain worst-case performance guarantees. A complementary direction is to develop prediction methods that perform well against complex benchmarks. In this paper, we address these two directions together. We present a fully adaptive method that competes with dynamic benchmarks in which regret guarantee scales with regularity of the sequence of cost functions and comparators. Notably, the regret bound adapts to the smaller complexity measure in the problem environment. Finally, we apply our results to drifting zero-sum, two-player games where both players achieve no regret guarantees against best sequences of actions in hindsight."
1395,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy,"Gavin Taylor, Connor Geer, David Piekut",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/taylor14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_taylor14,http://jmlr.csail.mit.edu/proceedings/papers/v32/taylor14.html,"Recent interest in the use of \(L_1\) regularization in the use of value function approximation includes Petrik et al.ês introduction of \(L_1\) -Regularized Approximate Linear Programming (RALP). RALP is unique among \(L_1\) -regularized approaches in that it approximates the optimal value function using off-policy samples. Additionally, it produces policies which outperform those of previous methods, such as LSPI. RALPês value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature. In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations. The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP."
1396,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Context Aware Group Nearest Shrunken Centroids in Large-Scale Genomic Studies,"Juemin Yang, Fang Han, Rafael Irizarry, Han Liu","JMLR W&CP 33 :1051-1059, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14b-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_yang14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14b.html,"Recent genomic studies have identified genes related to specific phenotypes. In addition to marginal association analysis for individual genes, analyzing gene pathways (functionally related sets of genes) may yield additional valuable insights. We have devised an approach to phenotype classification from gene expression profiling. Our method named –group Nearest Shrunken Centroids (gNSC)” is an enhancement of the Nearest Shrunken Centroids (NSC) which is a popular and scalable method to analyze big data. While fully utilizing the variable structure of gene pathways, gNSC shares comparable computational speed as NSC if the group size is small. Comparing with NSC, gNSC improves the power of classification by utilizing the gene pathway information. In practice, we investigate the performance of gNSC on one of the largest microarray datasets aggregated from the internet. We show the effectiveness of our method by comparing the misclassification rate of gNSC with that of NSC. Additionally, we present a novel application of NSC/gNSC on context analysis of association between pathways and certain medical words. Some newest biological findings are rediscovered."
1397,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Towards a rationalist theory of language acquisition,Edward Stabler,"JMLR W&CP 34 :21-32, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/stabler14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_stabler14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/stabler14a.html,"Recent computational, mathematical work on learnability extends to classes of languages that plausibly include the human languages, but there is nevertheless a gulf between this work and linguistic theory. The languages of the two fields seem almost completely disjoint and incommensurable. This paper shows that this has happened, at least in part, because the recent advances in learnability have been misdescribed in two important respects. First, they have been described as resting on •empiricistê conceptions of language, when actually, in fundamental respects that are made precise here, they are equally compatible with the •rationalistê, •nativistê traditions in linguistic theory. Second, the recent mathematical proposals have sometimes been presented as if they not only advance but complete the account of human language acquisition, taking the rather dramatic difference between what current mathematical models can achieve and what current linguistic theories tell us as an indication that current linguistic theories are quite generally mistaken. This paper compares the two perspectives and takes some first steps toward a unified theory, aiming to identify some common ground where •rationalistê linguistic hypotheses could directly address weaknesses in the current mathematical proposals."
1398,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Recovering Distributions from Gaussian RKHS Embeddings,"Motonobu Kanagawa, Kenji Fukumizu","JMLR W&CP 33 :457-465, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kanagawa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kanagawa14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kanagawa14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kanagawa14.html,"Recent advances of kernel methods have yielded a framework for nonparametric statistical inference called RKHS embeddings, in which all probability distributions are represented as elements in a reproducing kernel Hilbert space, namely kernel means. In this paper, we consider the recovery of the information of a distribution from an estimate of the kernel mean, when a Gaussian kernel is used. To this end, we theoretically analyze the properties of a consistent estimator of a kernel mean, which is represented as a weighted sum of feature vectors. First, we prove that the weighted average of a function in a Besov space, whose weights and samples are given by the kernel mean estimator, converges to the expectation of the function. As corollaries, we show that the moments and the probability measures on intervals can be recovered from an estimate of the kernel mean. We also prove that a consistent estimator of the density of a distribution can be defined using a kernel mean estimator. This result confirms that we can in fact completely recover the information of distributions from RKHS embeddings."
1399,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Markov Chain Monte Carlo and Variational Inference: Bridging the Gap,"Tim Salimans, Diederik Kingma, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/salimans15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_salimans15,http://jmlr.csail.mit.edu/proceedings/papers/v37/salimans15.html,"Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results."
1400,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Finito: A faster, permutable incremental gradient method for big data problems","Aaron Defazio, Justin Domke, tiberio Caetano",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/defazio14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/defazio14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_defazio14,http://jmlr.csail.mit.edu/proceedings/papers/v32/defazio14.html,"Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box –batch” problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance."
1401,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,A Dynamic Relational Infinite Feature Model for Longitudinal Social Networks,"James Foulds, Christopher DuBois, Arthur Asuncion, Carter Butts, Padhraic Smyth","15:287-295, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/foulds11b/foulds11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_foulds11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/foulds11b.html,Real-world relational data sets such as social networks often involve measurements over time. We propose a Bayesian nonparametric latent feature model for such data where the latent features for each actor in the network evolve according to a Markov process extending recent work on similar models for static networks. We show how the number of features and their trajectories for each actor can be inferred simultaneously and demonstrate the utility of this model on prediction tasks using both synthetic and real-world data.
1402,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Unsupervised Link Selection in Networks,"Quanquan Gu, Charu Aggarwal, Jiawei Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/gu13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_gu13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/gu13a.html,"Real-world networks are often noisy, and the existing linkage structure may not be reliable. For example, a link which connects nodes from different communities may affect the group assignment of nodes in a negative way. In this paper, we study a new problem called link selection, which can be seen as the network equivalent of the traditional feature selection problem in machine learning. More specifically, we investigate unsupervised link selection as follows: given a network, it selects a subset of informative links from the original network which enhance the quality of community structures. To achieve this goal, we use Ratio Cut size of a network as the quality measure. The resulting link selection approach can be formulated as a semi-definite programming problem. In order to solve it efficiently, we propose a backward elimination algorithm using sequential optimization. Experiments on benchmark network datasets illustrate the effectiveness of our method."
1403,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Using GNUsmail to Compare Data Stream Mining Methods for On-line Email Classification,"Jose M. Carmona-Cejudo, Manuel Baena-Garcia, Jose del Campo-Avila, Rafael Morales-Bueno, Joao Gama, Albert Bifet","17:12-18, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/carmona11a/carmona11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_carmona11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/carmona11a.html,"Real-time classification of emails is a challenging task because of its online nature, and also because email streams are subject to concept drift. Identifying email spam, where only two different labels or classes are defined (spam or not spam), has received great attention in the literature. We are nevertheless interested in a more specific classification where multiple folders exist, which is an additional source of complexity: the class can have a very large number of different values. Moreover, neither cross-validation nor other sampling procedures are suitable for evaluation in data stream contexts, which is why other metrics, like the prequential error, have been proposed. However, the prequential error poses some problems, which can be alleviated by using recently proposed mechanisms such as fading factors. In this paper, we present GNUsmail, an open-source extensible framework for email classification, and we focus on its ability to perform online evaluation. GNUsmails architecture supports incremental and online learning, and it can be used to compare different data stream mining methods, using state-of-art online evaluation metrics. Besides describing the framework, characterized by two overlapping phases, we show how it can be used to compare different algorithms in order to find the most appropriate one. The GNUsmail source code includes a tool for launching replicable experiments."
1404,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Expensive Function Optimization with Stochastic Binary Outcomes,"Matthew Tesch, Jeff Schneider, Howie Choset",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tesch13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tesch13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tesch13.html,"Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome _ success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training. In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem. We propose an experiment selection metric for this setting based on expected improvement. We demonstrate the algorithmês performance on synthetic problems and on a real snake robot learning to move over an obstacle."
1405,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Why are DBNs sparse?,"Shaunak Chatterjee, Stuart Russell","9:81-88, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/chatterjee10a/chatterjee10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_chatterjee10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/chatterjee10a.html,Real stochastic processes operate in continuous time and can be modeled by sets of stochastic differential equations. On the other hand several popular model families including hidden Markov models and dynamic Bayesian networks (DBNs) use discrete time steps. This paper explores methods for converting DBNs with infinitesimal time steps into DBNs with finite time steps to enable efficient simulation and filtering over long periods. An exact conversion---summing out all intervening time slices between two steps---results in a completely connected DBN yet nearly all human-constructed DBNs are sparse. We show how this sparsity arises from well-founded approximations resulting from differences among the natural time scales of the variables in the DBN. We define an automated procedure for constructing a provably accurate approximate DBN model for any desired time step. We illustrate the method by generating a series of approximations to a simple pH model for the human body demonstrating speedups of several orders of magnitude compared to the original model.
1406,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,A Theoretical Analysis of NDCG Type Ranking Measures,"Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Wang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Wang13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Wang13.html,"Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is to design a ranking measure for evaluation of ranking functions. State of the art leaning to rank methods often train a ranking function by using a ranking measure as the objective to maximize. In this paper we study, from a theoretical perspective, the widely used NDCG type ranking measures. We analyze the behavior of these ranking measures as the number of objects to rank getting large. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to \(1\) as the number of items to rank goes to infinity. On the first sight, this result seems to imply that NDCG cannot distinguish good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. Our next main result is a theorem which shows that although NDCG converge to the same limit for all ranking functions, it has distinguishability for ranking functions in a strong sense. We then investigate NDCG with other possible discount. Specifically we characterize the class of feasible discount functions for NDCG. We also compare the limiting behavior and the power of distinguishability of these feasible NDCG type measures to the standard NDCG. We next turn to the cut-off version of NDCG, i.e., NDCG@k. The most popular NDCG@k uses a combination of a slow logarithmic decay and a hard cut-off as its discount. So a natural question is why not simply use a smooth discount with fast decay? We show that if the decay is too fast, then the NDCG measure does not have strong power of distinguishability and even not converge. Finally, feasible NDCG@k are also discussed."
1407,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Computing Parametric Ranking Models via Rank-Breaking,"Hossein Azari Soufiani, David Parkes, Lirong Xia",none,http://jmlr.org/proceedings/papers/v32/soufiani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/soufiani14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_soufiani14,http://jmlr.csail.mit.edu/proceedings/papers/v32/soufiani14.html,"Rank breaking is a methodology introduced by Azari Soufiani et al. (2013a) for applying a Generalized Method of Moments (GMM) algorithm to the estimation of parametric ranking models. Breaking takes full rankings and breaks, or splits them up, into counts for pairs of alternatives that occur in particular positions (e.g., first place and second place, second place and third place). GMMs are of interest because they can achieve significant speed-up relative to maximum likelihood approaches and comparable statistical efficiency. We characterize the breakings for which the estimator is consistent for random utility models (RUMs) including Plackett-Luce and Normal-RUM, develop a general sufficient condition for a full breaking to be the only consistent breaking, and provide a trichotomy theorem in regard to single-edge breakings. Experimental results are presented to show the computational efficiency along with statistical performance of the proposed method."
1408,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures, Case-studies, and Consequences",Benjamin Recht and Christopher Re,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/recht12/recht12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_recht12,http://jmlr.csail.mit.edu/proceedings/papers/v23/recht12.html,"Randomized algorithms that base iteration-level decisions on samples from some pool are ubiquitous in machine learning and optimization. Examples include stochastic gradient descent and randomized coordinate descent. This paper makes progress at theoretically evaluating the difference in performance between sampling with- and without-replacement in such algorithms. Focusing on least means squares optimization, we formulate a noncommutative arithmetic-geometric mean inequality that would prove that the expected convergence rate of without-replacement sampling is faster than that of with-replacement sampling. We demonstrate that this inequality holds for many classes of random matrices and for some pathological examples as well. We provide a deterministic worst-case bound on the gap between the discrepancy between the two sampling models, and explore some of the impediments to proving this inequality in full generality. We detail the consequences of this inequality for stochastic gradient descent and the randomized Kaczmarz algorithm for solving linear systems."
1409,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Recovering the Optimal Solution by Dual Random Projection,"Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, Shenghuo Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Zhang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Zhang13a,http://jmlr.csail.mit.edu/proceedings/papers/v30/Zhang13a.html,"Random projection has been widely used in data classification. It maps high-dimensional data into a low-dimensional subspace in order to reduce the computational cost in solving the related optimization problem. While previous studies are focused on analyzing the classification performance of using random projection, in this work, we consider the recovery problem, i.e., how to accurately recover the optimal solution to the original optimization problem in the high-dimensional space based on the solution learned from the subspace spanned by random projections. We present a simple algorithm, termed Dual Random Projection, that uses the dual solution of the low-dimensional optimization problem to recover the optimal solution to the original problem. Our theoretical analysis shows that with a high probability, the proposed algorithm is able to accurately recover the optimal solution to the original problem, provided that the data matrix is of low rank or can be well approximated by a low rank matrix."
1410,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Reconstruction from Anisotropic Random Measurements,Mark Rudelson and Shuheng Zhou,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/rudelson12/rudelson12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_rudelson12,http://jmlr.csail.mit.edu/proceedings/papers/v23/rudelson12.html,"Random matrices are widely used in sparse recovery problems, and the relevant properties of matrices with i.i.d. entries are well understood. The current paper discusses the recently introduced Restricted Eigenvalue (RE) condition, which is among the most general assumptions on the matrix, guaranteeing recovery. We prove a reduction principle showing that the RE condition can be guaranteed by checking the restricted isometry on a certain family of low-dimensional subspaces. This principle allows us to establish the RE condition for several broad classes of random matrices with dependent entries, including random matrices with subgaussian rows and non-trivial covariance structure, as well as matrices with independent rows, and uniformly bounded entries."
1411,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Quantile Regression for Large-scale Applications,"Jiyan Yang, Xiangrui Meng, Michael Mahoney",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13f.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13f,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13f.html,"Quantile regression is a method to estimate the quantiles of the conditional distribution of a response variable, and as such it permits a much more accurate portrayal of the relationship between the response variable and observed covariates than methods such as Least-squares or Least Absolute Deviations regression. It can be expressed as a linear program, and interior-point methods can be used to find a solution for moderately large problems. Dealing with very large problems, e.g. , involving data up to and beyond the terabyte regime, remains a challenge. Here, we present a randomized algorithm that runs in time that is nearly linear in the size of the input and that, with constant probability, computes a \((1+\epsilon)\) approximate solution to an arbitrary quantile regression problem. Our algorithm computes a low-distortion subspace-preserving embedding with respect to the loss function of quantile regression. Our empirical evaluation illustrates that our algorithm is competitive with the best previous work on small to medium-sized problems, and that it can be implemented in MapReduce-like environments and applied to terabyte-sized problems."
1412,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A new Q(lambda) with interim forward view and Monte Carlo equivalence,"Rich Sutton, Ashique Rupam Mahmood, Doina Precup, Hado van Hasselt",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/sutton14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/sutton14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sutton14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sutton14.html,"Q-learning, the most popular of reinforcement learning algorithms, has always included an extension to eligibility traces to enable more rapid learning and improved asymptotic performance on non-Markov problems. The lambda parameter smoothly shifts on-policy algorithms such as TD(lambda) and Sarsa(lambda) from a pure bootstrapping form (lambda=0) to a pure Monte Carlo form (lambda=1). In off-policy algorithms, including Q(lambda), GQ(lambda), and off-policy LSTD(lambda), the lambda parameter is intended to play the same role, but does not; on every exploratory action these algorithms bootstrap regardless of the value of lambda, and as a result they fail to approximate Monte Carlo learning when lambda=1. It may seem that this is inevitable for any online off-policy algorithm; if updates are made on each step on which the target policy is followed, then how could just the right updates be •un-madeê upon deviation from the target policy? In this paper, we introduce a new version of Q(lambda) that does exactly that, without significantly increased algorithmic complexity. En route to our new Q(lambda), we introduce a new derivation technique based on the forward-view/backward-view analysis familiar from TD(lambda) but extended to apply at every time step rather than only at the end of episodes. We apply this technique to derive first a new off-policy version of TD(lambda), called PTD(lambda), and then our new Q(lambda), called PQ(lambda)."
1413,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Latent Force Models,"Mauricio Alvarez, David Luengo, Neil Lawrence","5:9-16, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/alvarez09a/alvarez09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_alvarez09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/alvarez09a.html,Purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. In this paper we present a hybrid approach using Gaussian processes and differential equations to combine data driven modeling with a physical model of the system. We show how different physically-inspired kernel functions can be developed through sensible simple mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from computational biology motion capture and geostatistics.
1414,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Proximal Average Approximated Incremental Gradient Method for Composite Penalty Regularized Empirical Risk Minimization,"Yiu-ming Cheung, Jian Lou",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Cheung15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Cheung15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Cheung15.html,"Proximal average (PA) is an approximation technique proposed recently to handle nonsmooth composite regularizer in empirical risk minimization problem. For nonsmooth composite regularizer, it is often difficult to directly derive the corresponding proximal update when solving with popular proximal update. While traditional approaches resort to complex splitting methods like ADMM, proximal average provides an alternative, featuring the tractability of implementation and theoretical analysis. Nevertheless, compared to SDCA-ADMM and SAG-ADMM which are examples of ADMM-based methods achieving faster convergence rate and low per-iteration complexity, existing PA-based approaches either converge slowly (e.g. PA-ASGD) or suffer from high per-iteration cost (e.g. PA-APG). In this paper, we therefore propose a new PA-based algorithm called PA-SAGA, which is optimal in both convergence rate and per-iteration cost, by incorporating into incremental gradient-based framework."
1415,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Teaching iCub to recognize objects using deep Convolutional Neural Networks,"Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/pasquale15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_pasquale15,http://jmlr.csail.mit.edu/proceedings/papers/v43/pasquale15.html,"Providing robots with accurate and robust visual recognition capabilities in the real-world today is a challenge which prevents the use of autonomous agents for concrete applications. Indeed, the majority of tasks, as manipulation and interaction with other agents, critically depends on the ability to visually recognize the entities involved in a scene. At the same time, computer vision systems based on deep Convolutional Neural Networks (CNNs) are marking a breakthrough in fields as large-scale image classification and retrieval. In this work we investigate how latest results on deep learning can advance the visual recognition capabilities of a robotic platform (the iCub humanoid robot) in a real-world scenario. We benchmark the performance of the resulting system on a new dataset of images depicting 28 objects, named iCubWorld28, that we plan on releasing. As in the spirit of the iCubWorld dataset series, this has been collected in a framework reflecting the typical iCubês daily visual experience. Moreover, in this release we provide four different acquisition sessions, to test incremental learning capabilities over multiple days. Our study addresses the question: how many objects can the iCub recognize today?"
1416,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Program Embeddings to Propagate Feedback on Student Code,"Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/piech15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_piech15,http://jmlr.csail.mit.edu/proceedings/papers/v37/piech15.html,"Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford Universityês CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions."
1417,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,The Inverse Regression Topic Model,"Maxim Rabinovich, David Blei",none,http://jmlr.org/proceedings/papers/v32/rabinovich14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rabinovich14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rabinovich14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rabinovich14.html,"proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models."
1418,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Elicitation and Identification of Properties,"Ingo Steinwart, Chlo _ Pasin, Robert Williamson, Siyu Zhang","JMLR W&CP 35 :482-526, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/steinwart14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_steinwart14,http://jmlr.csail.mit.edu/proceedings/papers/v35/steinwart14.html,"Properties of distributions are real-valued functionals such as the mean, quantile or conditional value at risk. A property is elicitable if there exists a scoring function such that minimization of the associated risks recovers the property. We extend existing results to characterize the elicitability of properties in a general setting. We further relate elicitability to identifiability (a notion introduced by Osband) and provide a general formula describing all scoring functions for an elicitable property. Finally, we draw some connections to the theory of coherent risk measures."
1419,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,(Approximate) Subgradient Methods for Structured Prediction,"Nathan D. Ratliff, J. Andrew Bagnell, Martin A. Zinkevich","2:380-387, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/ratliff07a/ratliff07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_ratliff07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/ratliff07a.html,Promising approaches to structured learning problems have recently been developed in the maximum margin framework. Unfortunately algorithms that are computationally and memory efficient enough to solve large scale problems have lagged behind. We propose using simple subgradient-based techniques for optimizing a regularized risk formulation of these problems in both online and batch settings and analyze the theoretical convergence generalization and robustness properties of the resulting techniques. These algorithms are are simple memory efficient fast to converge and have small regret in the online setting. We also investigate a novel convex regression formulation of structured learning. Finally we demonstrate the benefits of the subgradient approach on three structured prediction problems.
1420,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,A highly efficient blocked Gibbs sampler reconstruction of multidimensional NMR spectra,"Ji Won Yoon, Simon Wilson, K. Hun Mok","9:940-947, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/yoon10a/yoon10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_yoon10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/yoon10a.html,Projection Reconstruction Nuclear Magnetic Resonance (PR-NMR) is a new technique to generate multi-dimensional NMR spectra which have discrete features that are relatively sparsely distributed in space. A small number of projections from lower dimensional NMR spectra are used to reconstruct the multi-dimensional NMR spectra. We propose an efficient algorithm which employs a blocked Gibbs sampler to accurately reconstruct NMR spectra. This statistical method generates samples in Bayesian scheme. Our proposed algorithm is tested on a set of six projections derived from the three-dimensional 700 MHz HNCO spectrum of HasA a 187-residue heme binding protein.
1421,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Wasserstein Propagation for Semi-Supervised Learning,"Justin Solomon, Raif Rustamov, Leonidas Guibas, Adrian Butscher",none,http://jmlr.org/proceedings/papers/v32/solomon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_solomon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/solomon14.html,"Probability distributions and histograms are natural representations for product ratings, traffic measurements, and other data considered in many machine learning applications. Thus, this paper introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be characterized by the moments and shapes of the histograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes."
1422,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On Modelling Non-linear Topical Dependencies,"Zhixing Li, Siqiang Wen, Juanzi Li, Peng Zhang, Jie Tang",none,http://jmlr.org/proceedings/papers/v32/lib14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lib14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lib14.html,"Probabilistic topic models such as Latent Dirichlet Allocation (LDA) discover latent topics from large corpora by exploiting wordsê co-occurring relation. By observing the topical similarity between words, we find that some other relations, such as semantic or syntax relation between words, lead to strong dependence between their topics. In this paper, sentences are represented as dependency trees and a Global Topic Random Field (GTRF) is presented to model the non-linear dependencies between words. To infer our model, a new global factor is defined over all edges and the normalization factor of GRF is proven to be a constant. As a result, no independent assumption is needed when inferring our model. Based on it, we develop an efficient expectation-maximization (EM) procedure for parameter estimation. Experimental results on four data sets show that GTRF achieves much lower perplexity than LDA and linear dependency topic models and produces better topic coherence."
1423,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Efficiently Sampling Probabilistic Programs via Program Analysis,"Arun Chaganty, Aditya Nori, Sriram Rajamani",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/chaganty13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_chaganty13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/chaganty13a.html,"Probabilistic programs are intuitive and succinct representations of complex probability distributions. A natural approach to performing inference over these programs is to execute them and compute statistics over the resulting samples. Indeed, this approach has been taken before in a number of probabilistic programming tools. In this paper, we address two key challenges of this paradigm: (i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently generating samples with minimal rejection. We present a new sampling algorithm Qi that addresses these challenges using concepts from the field of program analysis. To solve the first challenge (getting diverse samples), we use a technique called symbolic execution to systematically explore all the paths in a program. In the case of programs with loops, we systematically explore all paths up to a given depth, and present theorems on error bounds on the estimates as a function of the path bounds used. To solve the second challenge (efficient samples with minimal rejection), we propagate observations backward through the program using the notion of Dijkstraês weakest preconditions and hoist these propagated conditions to condition elementary distributions during sampling. We present theorems explaining the mathematical properties of Qi, as well as empirical results from an implementation of the algorithm."
1424,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Inference and Learning in Networks of Queues,"Charles Sutton, Michael Jordan","9:796-803, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sutton10a/sutton10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sutton10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sutton10a.html,Probabilistic models of the performance of computer systems are useful both for predicting system performance in new conditions and for diagnosing past performance problems. The most popular performance models are networks of queues. However no current methods exist for parameter estimation or inference in networks of queues with missing data. In this paper we present a novel viewpoint that combines queueing networks and graphical models allowing Markov chain Monte Carlo to be applied. We demonstrate the effectiveness of our sampler on real-world data from a benchmark Web application.
1425,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Dynamic Copula Networks for Modeling Real-valued Time Series,"Elad Eban, Gideon Rothschild, Adi Mizrahi, Israel Nelken, Gal Elidan",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/eban13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/eban13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_eban13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/eban13a.html,"Probabilistic modeling of temporal phenomena is of central importance in a variety of fields ranging from neuroscience to economics to speech recognition. While the task has received extensive attention in recent decades, learning temporal models for multivariate real-valued data that is non-Gaussian is still a formidable challenge. Recently, the power of copulas, a framework for representing complex multi-modal and heavy-tailed distributions, was fused with the formalism of Bayesian networks to allow for flexible modeling of high-dimensional distributions. In this work we introduce Dynamic Copula Bayesian Networks, a generalization aimed at capturing the distribution of rich temporal sequences. We apply our model to three markedly different real-life domains and demonstrate substantial quantitative and qualitative advantage."
1426,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison,"Borja Balle, William Hamilton, Joelle Pineau",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/balle14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/balle14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_balle14,http://jmlr.csail.mit.edu/proceedings/papers/v32/balle14.html,"Probabilistic latent-variable models are a powerful tool for modelling structured data. However, traditional expectation-maximization methods of learning such models are both computationally expensive and prone to local-minima. In contrast to these traditional methods, recently developed learning algorithms based upon the method of moments are both computationally efficient and provide strong statistical guarantees. In this work, we provide a unified presentation and empirical comparison of three general moment-based methods in the context of modelling stochastic languages. By rephrasing these methods upon a common theoretical ground, introducing novel theoretical results where necessary, we provide a clear comparison, making explicit the statistical assumptions upon which each method relies. With this theoretical grounding, we then provide an in-depth empirical analysis of the methods on both real and synthetic data with the goal of elucidating performance trends and highlighting important implementation details."
1427,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Distributed Stochastic Gradient MCMC,"Sungjin Ahn, Babak Shahbaba, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ahn14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ahn14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ahn14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ahn14.html,Probabilistic inference on a big data scale is becoming increasingly relevant to both the machine learning and statistics communities. Here we introduce the first fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic gradient MCMC algorithms are particularly suited for distributed inference because individual chains can draw minibatches from their local pool of data for a flexible amount of time before jumping to or syncing with other chains. This greatly reduces communication overhead and allows adaptive load balancing. Our experiments for LDA on Wikipedia and Pubmed show that relative to the state of the art in distributed MCMC we reduce compute time from 27 hours to half an hour in order to reach the same perplexity level.
1428,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,An Alternative Prior Process for Nonparametric Bayesian Clustering,"Hanna Wallach, Shane Jensen, Lee Dicker, Katherine Heller","9:892-899, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/wallach10a/wallach10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_wallach10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/wallach10a.html,"Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper we investigate the predictive probabilities that underlie these processes and the implicit ""rich-get-richer"" characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering the uniform process for applications where the ""rich-get-richer"" property is undesirable. We also explore the cost of this new process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. Finally we compare performance on a real document clustering task demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings."
1429,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Latent Variable Models for Dimensionality Reduction,"zhihua zhang, Michael Jordan","5:655-662, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/zhang09b/zhang09b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_zhang09b,http://jmlr.csail.mit.edu/proceedings/papers/v5/zhang09b.html,Principal coordinate analysis (PCO) as a duality of principal component analysis (PCA) is also a classical method for explanatory data analysis. In this paper we propose a probabilistic PCO by using a normal latent variable model in which maximum likelihood estimation and an expectation-maximization algorithm are respectively devised to calculate the configurations of objects in a low-dimensional Euclidean space. We also devise probabilistic formulations for kernel PCA which is a nonlinear extension of PCA.
1430,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Convex Structure Learning in Log-Linear Models: Beyond Pairwise Potentials,"Mark Schmidt, Kevin Murphy","9:709-716, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/schmidt10a/schmidt10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_schmidt10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/schmidt10a.html,Previous work has examined structure learning in log-linear models with L1-regularization largely focusing on the case of pairwise potentials. In this work we consider the case of models with potentials of arbitrary order but that satisfy a hierarchical constraint. We enforce the hierarchical constraint using group L1-regularization with overlapping groups and an active set method that enforces hierarchical inclusion allows us to tractably consider the exponential number of higher-order potentials. We use a spectral projected gradient method as a sub-routine for solving the overlapping group L1-regularization problem and make use of a sparse version of Dykstra's algorithm to compute the projection. Our experiments indicate that this model gives equal or better test set likelihood compared to previous models.
1431,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On Robustness and Regularization of Structural Support Vector Machines,"Mohamad Ali Torkamani, Daniel Lowd",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/torkamani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/torkamani14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_torkamani14,http://jmlr.csail.mit.edu/proceedings/papers/v32/torkamani14.html,"Previous analysis of binary SVMs has demonstrated a deep connection between robustness to perturbations over uncertainty sets and regularization of the weights. In this paper, we explore the problem of learning robust models for structured prediction problems. We first formulate the problem of learning robust structural SVMs when there are perturbations in the feature space. We consider two different classes of uncertainty sets for the perturbations: ellipsoidal uncertainty sets and polyhedral uncertainty sets. In both cases, we show that the robust optimization problem is equivalent to the non-robust formulation with an additional regularizer. For the ellipsoidal uncertainty set, the additional regularizer is based on the dual norm of the norm that constrains the ellipsoidal uncertainty. For the polyhedral uncertainty set, we show that the robust optimization problem is equivalent to adding a linear regularizer in a transformed weight space related to the linear constraints of the polyhedron. We also show that these constraint sets can be combined and demonstrate a number of interesting special cases. This represents the first theoretical analysis of robust optimization of structural support vector machines. Our experimental results show that our method outperforms the nonrobust structural SVMs on real world data when the test data distributions is drifted from the training data distribution."
1432,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast dropout training,"Sida Wang, Christopher Manning",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.html,"Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations."
1433,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Preserving Privacy of Continuous High-dimensional Data with Minimax Filters,Jihun Hamm,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hamm15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hamm15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hamm15.html,"Preserving privacy of high-dimensional and continuous data such as images or biometric data is a challenging problem. This paper formulates this problem as a learning game between three parties: 1) data contributors using a filter to sanitize data samples, 2) a cooperative data aggregator learning a target task using the filtered samples, and 3) an adversary learning to identify contributors using the same filtered samples. Minimax filters that achieve the optimal privacy-utility trade-off from broad families of filters and loss/classifiers are defined, and algorithms for learning the filers in batch or distributed settings are presented. Experiments with several real-world tasks including facial expression recognition, speech emotion recognition, and activity recognition from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower privacy risk, often significantly lower than previous methods."
1434,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Preferential Attachment in Graphs with Affinities,"Jay Lee, Manzil Zaheer, Stephan Gônnemann, Alex Smola",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15b-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lee15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15b.html,"Preferential attachment models for random graphs are successful in capturing many characteristics of real networks such as power law behavior. At the same time they lack flexibility to take vertex to vertex affinities into account, a feature that is commonly used in many link recommendation algorithms. We propose a random graph model based on both node attributes and preferential attachment. This approach overcomes the limitation of existing models on expressing vertex affinity and on reflecting properties of different subgraphs. We analytically prove that our model preserves the power law behavior in the degree distribution as expressed by natural graphs and we show that it satisfies the small world property. Experiments show that our model provides an excellent fit of many natural graph statistics and we provide an algorithm to infer the associated affinity function efficiently."
1435,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries,"Shengbo Guo, Scott Sanner","9:289-296, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/guo10b/guo10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_guo10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/guo10b.html,Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences. In this paper we outline five principles important for PE in real-world problems: (1) real-time (2) multiattribute (3) low cognitive load (4) robust to noise and (5) scalable. In light of these requirements we introduce an approximate PE framework based on TrueSkill for performing efficient closed-form Bayesian updates and query selection for a multiattribute utility belief state --- a novel PE approach that naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies. Our best VOI query strategy satisfies all five principles (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with synthetic and real-world datasets.
1436,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Conference on Learning Theory 2015: Preface,"Peter Grônwald, Elad Hazan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Grunwald15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Grunwald15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Grunwald15.html,Preface to COLT 2015
1437,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Preface,"Guy Lebanon, S.V.N. Vishwanathan",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lebanon15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lebanon15,http://jmlr.csail.mit.edu/proceedings/papers/v38/lebanon15.html,Preface to AISTATS 2015
1438,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Preface,"Samuel Kaski, Jukka Corander",none,http://jmlr.csail.mit.edu/proceedings/papers/v33/kaski14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kaski14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kaski14.html,Preface to AISTATS 2014
1439,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Preface,"Geoffrey Holmes, Tie-Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/preface.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_preface,http://jmlr.csail.mit.edu/proceedings/papers/v45/preface.html,Preface to ACML 2015.
1440,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Preface,"Dinh Phung, Hang Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/preface.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_preface,http://jmlr.csail.mit.edu/proceedings/papers/v39/preface.html,Preface to ACML 2014
1441,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Preface,"Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka","JMLR W&CP 34 :1-2, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/clark14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_clark14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/clark14a.html,Preface for the 12th International Conference on Grammatical Inference.
1442,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Bayesian Structure Learning for Functional Neuroimaging,"Mijung Park, Oluwasanmi Koyejo, Joydeep Ghosh, Russell Poldrack, Jonathan Pillow",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/park13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_park13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/park13a.html,"Predictive modeling of functional neuroimaging data has become an important tool for analyzing cognitive structures in the brain. Brain images are high-dimensional and exhibit large correlations, and imaging experiments provide a limited number of samples. Therefore, capturing the inherent statistical properties of the imaging data is critical for robust inference. Previous methods tackle this problem by exploiting either spatial sparsity or smoothness, which does not fully exploit the structure in the data. Here we develop a flexible, hierarchical model designed to simultaneously capture spatial block sparsity and smoothness in neuroimaging data. We exploit a function domain representation for the high-dimensional small-sample data and develop efficient inference, parameter estimation, and prediction procedures. Empirical results with simulated and real neuroimaging data suggest that simultaneously capturing the block sparsity and smoothness properties can significantly improve structure recovery and predictive modeling performance."
1443,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Predictive Inverse Optimal Control for Linear-Quadratic-Gaussian Systems,"Xiangli Chen, Brian Ziebart",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15d-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chen15d,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15d.html,"Predictive inverse optimal control is a powerful approach for estimating the control policy of an agent from observed control demonstrations. Its usefulness has been established in a number of large-scale sequential decision settings characterized by complete state observability. However, many real decisions are made in situations where the state is not fully known to the agent making decisions. Though extensions of predictive inverse optimal control to partially observable Markov decision processes have been developed, their applicability has been limited by the complexities of inference in those representations. In this work, we extend predictive inverse optimal control to the linear- quadratic-Gaussian control setting. We establish close connections between optimal control laws for this setting and the probabilistic predictions under our approach. We demonstrate the effectiveness and benefit in estimating control policies that are influenced by partial observability on both synthetic and real datasets."
1444,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Aggregating Predictions via Sequential Mini-Trading,"Mindika Premachandra, Mark Reid","JMLR W&CP 29 :373-387, 2013",http://jmlr.org/proceedings/papers/v29/Premachandra13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Premachandra13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Premachandra13.html,"Prediction markets which trade on contracts representing unknown future outcomes are designed specifically to aggregate expert predictions via the market price. While there are some existing machine learning interpretations for the market price and connections to Bayesian updating under the equilibrium analysis of such markets, there is less of an understanding of what the instantaneous price in sequentially traded markets means. In this paper we show that the prices generated in sequentially traded prediction markets are stochastic approximations to the price given by an equilibrium analysis. We do so by showing the equilibrium price is a solution to a stochastic optimisation problem which is solved by stochastic mirror descent (SMD) by a class of sequential pricing mechanisms. This connection leads us to propose a scheme called –mini-trading” which introduces a parameter related to the learning rate in SMD. We prove several properties of this scheme and show that it can improve the stability of prices in sequentially traded prediction markets."
1445,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction,"Jian Zhou, Olga Troyanskaya",none,http://jmlr.org/proceedings/papers/v32/zhou14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhou14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhou14.html,"Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino-acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem."
1446,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Large-scale Distributed Dependent Nonparametric Trees,"Zhiting Hu, Ho Qirong, Avinava Dubey, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hu15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hu15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hu15.html,"Practical applications of Bayesian nonparametric (BNP) models have been limited, due to their high computational complexity and poor scaling on large data. In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite model that captures time-evolving hierarchies, and develop a large-scale distributed training system. Our major contributions include: (1) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization; (3) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours, our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and long-tail topics. Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process, and the near-linear scalability indicates great potential for even bigger problem sizes."
1447,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,TopicFlow Model: Unsupervised Learning of Topic-specific Influences of Hyperlinked Documents,"Ramesh Nallapati, Daniel McFarland, Christopher Manning","15:543-551, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/nallapati11a/nallapati11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_nallapati11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/nallapati11a.html,Popular algorithms for modeling the influence of entities in networked data such as PageRank work by analyzing the hyperlink structure but ignore the contents of documents. However often times influence is topic dependent e.g. a web page of high influence in politics may be an unknown entity in sports. We design a new model called TopicFlow which combines ideas from network flow and topic modeling to learn this notion of topic specific influences of hyperlinked documents in a completely unsupervised fashion. On the task of citation recommendation which is an instance of capturing influence the TopicFlow model when combined with TF-IDF based cosine similarity outperforms several competitive baselines by as much as 11.8%. Our empirical study of the modelÍs output on ACL corpus demonstrates its ability to identify topically influential documents. The Topic- Flow model is also competitive with the state-of-theart Relational Topic Models in predicting the likelihood of unseen text on two different data sets. Due to its ability to learn topic-specific flows across each hyperlink the TopicFlow model can be a powerful visualization tool to track the diffusion of topics across a citation network.
1448,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Signal recovery from Pooling Representations,"Joan Bruna Estrach, Arthur Szlam, Yann LeCun",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/estrach14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/estrach14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_estrach14,http://jmlr.csail.mit.edu/proceedings/papers/v32/estrach14.html,"Pooling operators construct non-linear representations by cascading a redundant linear transform, followed by a point-wise nonlinearity and a local aggregation, typically implemented with a \(\ell_p\) norm. Their efficiency in recognition architectures is based on their ability to locally contract the input space, but also on their capacity to retain as much stable information as possible. We address this latter question by computing the upper and lower Lipschitz bounds of \(\ell_p\) pooling operators for \(p=1, 2, \infty\) as well as their half-rectified equivalents, which give sufficient conditions for the design of invertible pooling layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression."
1449,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Online Multi-Task Learning for Policy Gradient Methods,"Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, Matthew Taylor",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ammar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ammar14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ammar14.html,"Policy gradient algorithms have shown considerable recent success in solving high-dimensional sequential decision making tasks, particularly in robotics. However, these methods often require extensive experience in a domain to achieve high performance. To make agents more sample-efficient, we developed a multi-task policy gradient method to learn decision making tasks consecutively, transferring knowledge between tasks to accelerate learning. Our approach provides robust theoretical guarantees, and we show empirically that it dramatically accelerates learning on a variety of dynamical systems, including an application to quadrotor control."
1450,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient Estimation,"Tingting Zhao, Gang Niu, Ning Xie, Jucheng Yang, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhao15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhao15b,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhao15b.html,"Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces, which update the policy parameters along the steepest direction of the expected return. However, large variance of policy gradient estimation often causes instability of policy update. In this paper, we propose to suppress the variance of gradient estimation by directly employing the variance of policy gradients as a regularizer. Through experiments, we demonstrate that the proposed variance-regularization technique combined with parameter-based exploration and baseline subtraction provides more reliable policy updates than non-regularized counterparts."
1451,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Multitask Point Process Predictive Model,"Wenzhao Lian, Ricardo Henao, Vinayak Rao, Joseph Lucas, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lian15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/lian15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lian15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lian15.html,"Point process data are commonly observed in fields like healthcare and social science. Designing predictive models for such event streams is an under-explored problem, due to often scarce training data. In this work we propose a multitask point process model, leveraging information from all tasks via a hierarchical Gaussian process (GP). Nonparametric learning functions implemented by a GP, which map from past events to future rates, allow analysis of flexible arrival patterns. To facilitate efficient inference, we propose a sparse construction for this hierarchical model, and derive a variational Bayes method for learning and inference. Experimental results are shown on both synthetic data and an application on real electronic health records."
1452,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Fully-Automatic Bayesian Piecewise Sparse Linear Models,"Riki Eto, Ryohei Fujimaki, Satoshi Morinaga, Hiroshi Tamano","JMLR W&CP 33 :238-246, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/eto14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_eto14,http://jmlr.csail.mit.edu/proceedings/papers/v33/eto14.html,"Piecewise linear models (PLMs) have been widely used in many enterprise machine learning problems, which assign linear experts to individual partitions on feature spaces and express whole models as patches of local experts. This paper addresses simultaneous model selection issues of PLMs; partition structure determination and feature selection of individual experts. Our contributions are mainly three-fold. First, we extend factorized asymptotic Bayesian (FAB) inference for hierarchical mixtures of experts (probabilistic PLMs). FAB inference offers penalty terms w.r.t. partition and expert complexities, and enable us to resolve the model selection issue. Second, we propose posterior optimization which significantly improves predictive accuracy. Roughly speaking, our new posterior optimization mitigates accuracy degradation due to a gap between marginal log-likelihood maximization and predictive accuracy. Third, we present an application of energy demand forecasting as well as benchmark comparisons. The experiments show our capability of acquiring compact and highly-accurate models."
1453,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning with Maximum A-Posteriori Perturbation Models,"Andreea Gane, Tamir Hazan, Tommi Jaakkola","JMLR W&CP 33 :247-256, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/gane14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_gane14,http://jmlr.csail.mit.edu/proceedings/papers/v33/gane14.html,"Perturbation models are families of distributions induced from perturbations. They combine randomization of the parameters with maximization to draw unbiased samples. Unlike Gibbsê distributions, a perturbation model defined on the basis of low order statistics still gives rise to high order dependencies. In this paper, we analyze, extend and seek to estimate such dependencies from data. In particular, we shift the modelling focus from the parameters of the Gibbsê distribution used as a base model to the space of perturbations. We estimate dependent perturbations over the parameters using a hard-EM approach, cast in the form of inverse convex programs. Each inverse program confines the randomization to the parameter polytope responsible for generating the observed answer. We illustrate the method on several computer vision problems."
1454,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Subsampling Methods for Persistent Homology,"Frederic Chazal, Brittany Fasy, Fabrizio Lecci, Bertrand Michel, Alessandro Rinaldo, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chazal15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/chazal15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chazal15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chazal15.html,"Persistent homology is a multiscale method for analyzing the shape of sets and functions from point cloud data arising from an unknown distribution supported on those sets. When the size of the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We propose to compute the persistent homology of several subsamples of the data and then combine the resulting estimates. We study the risk of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity."
1455,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Exploiting Probabilistic Independence for Permutations,"Jonathan Huang, Carlos Guestrin, Xiaoye Jiang, Leonidas Guibas","5:248-255, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/huang09b/huang09b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_huang09b,http://jmlr.csail.mit.edu/proceedings/papers/v5/huang09b.html,Permutations are ubiquitous in many real world problems such as voting rankings and data association. Representing uncertainty over permutations is challenging since there are $n!$ possibilities. Recent Fourier-based approaches can be used to provide a compact representation over low-frequency components of the distribution. Though polynomial the complexity of these representations grows very rapidly especially if we want to maintain reasonable estimates for peaked distributions. In this paper we first characterize the notion of probabilistic independence for distribution over permutations. We then present a method for factoring distributions into independent components in the Fourier domain and use our algorithms to decompose large problems into much smaller ones. We demonstrate that our method provides very significant improvements in terms of running time on real tracking data.
1456,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,PeakSeg: constrained optimal segmentation and supervised penalty learning for peak detection in count data,"Toby Hocking, Guillem Rigaill, Guillaume Bourque",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hocking15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hocking15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hocking15.html,"Peak detection is a central problem in genomic data analysis, and current algorithms for this task are unsupervised and mostly effective for a single data type and pattern (e.g. H3K4me3 data with a sharp peak pattern). We propose PeakSeg, a new constrained maximum likelihood segmentation model for peak detection with an efficient inference algorithm: constrained dynamic programming. We investigate unsupervised and supervised learning of penalties for the critical model selection problem. We show that the supervised method has state-of-the-art peak detection across all data sets in a benchmark that includes both sharp H3K4me3 and broad H3K36me3 patterns."
1457,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Scalable Personalization of Long-Term Physiological Monitoring: Active Learning Methodologies for Epileptic Seizure Onset Detection,"Guha Balakrishnan, Zeeshan Syed",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/balakrishnan12b/balakrishnan12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_balakrishnan12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/balakrishnan12b.html,Patient-specific algorithms to detect adverse clinical events during long-term physiological monitoring substantially improve performance relative to patient-nonspecific ones. However these algorithms often rely on the availability of expert hand-labeled data for training which severely restricts the scalability of personalized monitoring within a real-world setting. While active learning offers a natural framework to address this issue the relative merits of different active learning methodologies have not been extensively studied in the setting of developing clinically useful detectors for infrequent time-series events. In this paper we identify a core set of principles that are relative to the specific goal of personalized long-term physiological monitoring. We describe and compare different approaches for initialization batch selection and termination within the active learning process. We position this work in the context of epileptic seizure onset detection. When evaluated on a database of scalp EEG recordings from 23 epileptic patients we show that a combined distance- and diversity-based measure to determine the data to be queried max-min clustering for identification of the initialization set and a comparison of consecutive support vector sets to guide termination results in an active learning-based detector that can achieve similar performance to a patient-specific detector while requiring two orders of magnitude fewer labeled examples for training.
1458,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Parsing epileptic events using a Markov switching process model for correlated time series,"Drausin Wulsin, Emily Fox, Brian Litt",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulsin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulsin13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wulsin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wulsin13.html,"Patients with epilepsy can manifest short, sub-clinical epileptic –bursts” in addition to full-blown clinical seizures. We believe the relationship between these two classes of eventsãsomething not previously studied quantitativelyãcould yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes. A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients. We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes. We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data. We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures."
1459,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Particle Gibbs with Ancestor Sampling for Probabilistic Programs,"Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/vandemeent15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_vandemeent15,http://jmlr.csail.mit.edu/proceedings/papers/v38/vandemeent15.html,"Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains."
1460,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,"Deep Learning, Dark Knowledge, and Dark Matter","Peter Sadowski, Julian Collado, Daniel Whiteson, Pierre Baldi",none,http://jmlr.csail.mit.edu/proceedings/papers/v42/sado14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_sado14,http://jmlr.csail.mit.edu/proceedings/papers/v42/sado14.html,"Particle colliders are the primary experimental instruments of high-energy physics. By creating conditions that have not occurred naturally since the Big Bang, collider experiments aim to probe the most fundamental properties of matter and the universe. These costly experiments generate very large amounts of noisy data, creating important challenges and opportunities for machine learning. In this work we use deep learning to greatly improve the statistical power on three benchmark problems involving: (1) Higgs bosons; (2) supersymmetric particles; and (3) Higgs boson decay modes. This approach increases the expected discovery significance over traditional shallow methods, by 50%, 2%, and 11% respectively. In addition, we explore the use of model compression to transfer information ( dark knowledge ) from deep networks to shallow networks."
1461,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,A near-optimal algorithm for finite partial-monitoring games against adversarial opponents,Gˆbor Bart„k,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bartok13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Bartok13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bartok13.html,"Partial monitoring is an online learning model where in every time step, after a learner and an opponent choose their actions, the loss and the feedback for the learner is calculated based on a loss and a feedback function, both of which are known to the learner ahead of time. As in other online learning scenarios, the goal of the learner is to minimize his cumulative loss. In this paper we present and analyze a new algorithm for locally observable partial monitoring games. We prove that the expected regret of our algorithm is of \(\tilde O(\sqrt{N'T})\) , where \(T\) is the time horizon and \(N'\) is the size of the largest point-local game. The most important improvement of this bound compared to previous results is that it does not depend directly on the number of actions, but rather on the structure of the game."
1462,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Maximum Margin Partial Label Learning,"Fei Yu, Min-Ling Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Yu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Yu15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Yu15.html,"Partial label learning deals with the problem that each training example is associated with a set of candidate labels, and only one among the set is the ground-truth label. The basic strategy to learn from partial label examples is disambiguation, i.e. by trying to recover the ground-truth labeling information from the candidate label set. As one of the major machine learning techniques, maximum margin criterion has been employed to solve the partial label learning problem. Therein, disambiguation is performed by optimizing the margin between the maximum modeling output from candidate labels and that from non-candidate labels. However, in this formulation the margin between the ground-truth label and other candidate labels is not differentiated. In this paper, a new maximum margin formulation for partial label learning is proposed which aims to directly maximize the margin between the ground-truth label and all other labels. Specifically, an alternating optimization procedure is utilized to coordinate ground-truth label identification and margin maximization . Extensive experiments show that the derived partial label learning approach achieves competitive performance against other state-of-the-art comparing approaches."
1463,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Probabilistic Partial Canonical Correlation Analysis,"Yusuke Mukuta, tatsuya Harada",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mukuta14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mukuta14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mukuta14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mukuta14.html,"Partial canonical correlation analysis (partial CCA) is a statistical method that estimates a pair of linear projections onto a low dimensional space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial CCA is known to be closely related to a causality measure between two time series. However, partial CCA requires the inverses of covariance matrices, so the calculation is not stable. This is particularly the case for high-dimensional data or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the probabilistic model. Our numerical experiments demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples."
1464,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,ODE parameter inference using adaptive gradient matching with Gaussian processes,"Frank Dondelinger, Dirk Husmeier, Simon Rogers, Maurizio Filippone",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/dondelinger13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_dondelinger13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/dondelinger13a.html,"Parameter inference in mechanistic models based on systems of coupled differential equations is a topical yet computationally challenging problem, due to the need to follow each parameter adaptation with a numerical integration of the differential equations. Techniques based on gradient matching, which aim to minimize the discrepancy between the slope of a data interpolant and the derivatives predicted from the differential equations, offer a computationally appealing shortcut to the inference problem. The present paper discusses a method based on nonparametric Bayesian statistics with Gaussian processes due to Calderhead et al. (2008), and shows how inference in this model can be substantially improved by consistently sampling from the joint distribution of the ODE parameters and GP hyperparameters. We demonstrate the efficiency of our adaptive gradient matching technique on three benchmark systems, and perform a detailed comparison with the method in Calderhead et al. (2008) and the explicit ODE integration approach, both in terms of parameter inference accuracy and in terms of computational efficiency."
1465,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Controversy in mechanistic modelling with Gaussian processes,"Benn Macdonald, Catherine Higham, Dirk Husmeier",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/macdonald15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_macdonald15,http://jmlr.csail.mit.edu/proceedings/papers/v37/macdonald15.html,"Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by."
1466,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Pan-sharpening with a Bayesian nonparametric dictionary learning model,"Xinghao Ding, Yiyong Jiang, Yue Huang, John Paisley","JMLR W&CP 33 :176-184, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ding14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ding14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/ding14b.html,"Pan-sharpening, a method for constructing high resolution images from low resolution observations, has recently been explored from the perspective of compressed sensing and sparse representation theory. We present a new pan-sharpening algorithm that uses a Bayesian nonparametric dictionary learning model to give an underlying sparse representation for image reconstruction. In contrast to existing dictionary learning methods, the proposed method infers parameters such as dictionary size, patch sparsity and noise variances. In addition, our regularization includes image constraints such as a total variation penalization term and a new gradient penalization on the reconstructed PAN image. Our method does not require high resolution multiband images for dictionary learning, which are unavailable in practice, but rather the dictionary is learned directly on the reconstructed image as part of the inversion process. We present experiments on several images to validate our method and compare with several other well-known approaches."
1467,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Chromatic PAC-Bayes Bounds for Non-IID Data,"Liva Ralaivola, Marie Szafranski, Guillaume Stempfel","5:416-423, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/ralaivola09a/ralaivola09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_ralaivola09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/ralaivola09a.html,Pac-Bayes bounds are among the most accurate generalization bounds for classifiers learned with IID data and it is particularly so for margin classifiers. However there are many practical cases where the training data show some dependencies and where the traditional IID assumption does not apply. Stating generalization bounds for such frameworks is therefore of the utmost interest both from theoretical and practical standpoints. In this work we propose the first --~to the best of our knowledge~-- \pac-Bayes generalization bounds for classifiers trained on data exhibiting dependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data in sets of independent data through the tool of graph fractional covers. Our bounds are very general since being able to find an upper bound on the (fractional) chromatic number of the dependency graph is sufficient to get new \pac-Bayes bounds for specific settings. We show how our results can be used to derive bounds for bipartite ranking and windowed prediction on sequential data.
1468,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Learning low-rank output kernels,F. Dinuzzo & K. Fukumizu,"20:181_196, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/dinuzzo11/dinuzzo11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_dinuzzo11,http://jmlr.csail.mit.edu/proceedings/papers/v20/dinuzzo11.html,Output kernel learning techniques allow to simultaneously learn a vector-valued function and a positive semide_nite matrix which describes the relationships between the outputs. In this paper we introduce a new formulation that imposes a low-rank constraint on the output kernel and operates directly on a factor of the kernel matrix. First we investigate the connection between output kernel learning and a regularization problem for an architecture with two layers. Then we show that a variety of methods such as nuclear norm regularized regression reduced-rank regression principal component analysis and low rank matrix approximation can be seen as special cases of the output kernel learning framework. Finally we introduce a block coordinate descent strategy for learning low-rank output kernels.   Page last modified on Sun Nov 6 15:43:17 2011.
1469,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Learning to Locate Relative Outliers,S. Li & I.W. Tsang,"20:47_62, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/li11/li11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_li11,http://jmlr.csail.mit.edu/proceedings/papers/v20/li11.html,Outliers usually spread across regions of low density. However due to the absence or scarcity of outliers designing a robust detector to sift outliers from a given dataset is still very challenging. In this paper we consider to identify relative outliers from the target dataset with respect to another reference dataset of normal data. Particularly we employ Maximum Mean Discrepancy (MMD) for matching the distribution between these two datasets and present a novel learning framework to learn a relative outlier detector. The learning task is formulated as a Mixed Integer Programming (MIP) problem which is computationally hard. To this end we propose an e_ective procedure to _nd a largely violated labeling vector for identifying relative outliers from abundant normal patterns and its convergence is also presented. Then a set of largely violated labeling vectors are combined by multiple kernel learning methods to robustly locate relative outliers. Comprehensive empirical studies on real-world datasets verify that our proposed relative outlier detection outperforms existing methods.   Page last modified on Sun Nov 6 15:42:16 2011.
1470,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Local Deep Kernel Learning for Efficient Non-linear SVM Prediction,"Cijo Jose, Prasoon Goyal, Parv Aggrwal, Manik Varma",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jose13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jose13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jose13.html,"Our objective is to speed up non-linear SVM prediction while maintaining classification accuracy above an acceptable limit. We generalize Localized Multiple Kernel Learning so as to learn a primal feature space embedding which is high dimensional, sparse and computationally deep. Primal based classification decouples prediction costs from the number of support vectors and our tree-structured features efficiently encode non-linearities while speeding up prediction exponentially over the state-of-the-art. We develop routines for optimizing over the space of tree-structured features and efficiently scale to problems with over half a million training points. Experiments on benchmark data sets reveal that our formulation can reduce prediction costs by more than three orders of magnitude in some cases with a moderate sacrifice in classification accuracy as compared to RBF-SVMs. Furthermore, our formulation leads to much better classification accuracies over leading methods."
1471,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On p-norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection,"Pratik Jawanpuria, Manik Varma, Saketha Nath",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/jawanpuria14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/jawanpuria14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_jawanpuria14,http://jmlr.csail.mit.edu/proceedings/papers/v32/jawanpuria14.html,"Our objective is to develop formulations and algorithms for efficiently computing the feature selection path _ i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to \(l_{p\geq1}\) regularization ( \(l_{p}\) -MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art \(l_p\) -MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path. We propose a novel conjecture which states that, for certain \(l_p\) -MKL formulations, the number of features selected in the optimal solution monotonically decreases as \(p\) is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of \(p\) thereby reducing optimization costs while simultaneously providing approximation guarantees. We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other \(l_p\) -MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art \(l_p\) -MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware."
1472,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stable and Efficient Representation Learning with Nonnegativity Constraints,"Tsung-Han Lin, H. T. Kung",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/line14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_line14,http://jmlr.csail.mit.edu/proceedings/papers/v32/line14.html,"Orthogonal matching pursuit (OMP) is an efficient approximation algorithm for computing sparse representations. However, prior research has shown that the representations computed by OMP may be of inferior quality, as they deliver suboptimal classification accuracy on several im- age datasets. We have found that this problem is caused by OMPês relatively weak stability under data variations, which leads to unreliability in supervised classifier training. We show that by imposing a simple nonnegativity constraint, this nonnegative variant of OMP (NOMP) can mitigate OMPês stability issue and is resistant to noise overfitting. In this work, we provide extensive analysis and experimental results to examine and validate the stability advantage of NOMP. In our experiments, we use a multi-layer deep architecture for representation learning, where we use K-means for feature learning and NOMP for representation encoding. The resulting learning framework is not only efficient and scalable to large feature dictionaries, but also is robust against input noise. This framework achieves the state-of-the-art accuracy on the STL-10 dataset."
1473,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Uniqueness of Ordinal Embedding,"Matth _ us Kleindessner, Ulrike von Luxburg","JMLR W&CP 35 :40-67, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kleindessner14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kleindessner14,http://jmlr.csail.mit.edu/proceedings/papers/v35/kleindessner14.html,"Ordinal embedding refers to the following problem: all we know about an unknown set of points \(x_1,\ldots, x_n \in \mathbb{R}^d\) are ordinal constraints of the form \(\|x_i - x_j\| _ \|x_k - x_l\|\) ; the task is to construct a realization \(y_1,\ldots, y_n \in \mathbb{R}^d\) that preserves these ordinal constraints. It has been conjectured since the 1960ies that upon knowledge of all ordinal constraints a large but finite set of points can be approximately reconstructed up to a similarity transformation. The main result of our paper is a formal proof of this conjecture."
1474,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning Exercise Policies for American Options,"Yuxi Li, Csaba Szepesvari, Dale Schuurmans","5:352-359, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/li09d/li09d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_li09d,http://jmlr.csail.mit.edu/proceedings/papers/v5/li09d.html,Options are important instruments in modern finance. In this paper we investigate reinforcement learning (RL) methods---in particular least-squares policy iteration (LSPI)---for the problem of learning exercise policies for American options. We develop finite-time bounds on the performance of the policy obtained with LSPI and compare LSPI and the fitted Q-iteration algorithm (FQI) with the Longstaff-Schwartz method (LSM) the standard least-squares Monte Carlo algorithm from the finance community. Our empirical results show that the exercise policies discovered by LSPI and FQI gain larger payoffs than those discovered by LSM on both real and synthetic data. Furthermore we find that for all methods the policies learned from real data generally gain similar payoffs to the policies learned from simulated data. Our work shows that solution methods developed in machine learning can advance the state-of-the-art in an important and challenging application area while demonstrating that computational finance remains a promising area for future applications of machine learning methods.
1475,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Coordinate-descent for learning orthogonal matrices through Givens rotations,"Uri Shalit, Gal Chechik",none,http://jmlr.org/proceedings/papers/v32/shalit14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/shalit14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_shalit14,http://jmlr.csail.mit.edu/proceedings/papers/v32/shalit14.html,"Optimizing over the set of orthogonal matrices is a central component in problems like sparse-PCA or tensor decomposition. Unfortunately, such optimization is hard since simple operations on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a large amount of computation. Here we propose a framework for optimizing orthogonal matrices, that is the parallel of coordinate-descent in Euclidean spaces. It is based on Givens-rotations , a fast-to-compute operation that affects a small number of entries in the learned matrix, and preserves orthogonality. We show two applications of this approach: an algorithm for tensor decompositions used in learning mixture models, and an algorithm for sparse-PCA. We study the parameter regime where a Givens rotation approach converges faster and achieves a superior model on a genome-wide brain-wide mRNA expression dataset."
1476,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Sequential Information Maximization: When is Greedy Near-optimal?,"Yuxin Chen, S. Hamed, Hassani, Amin Karbasi, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chen15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Chen15b,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chen15b.html,"Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantifies the usefulness of observations is Shannonês mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations. In this paper, we analyze the widely used greedy policy for this task, and identify problem instances where it provides provably near-maximal utility, even in the challenging setting of persistent noise. Our results depend on a natural separability condition associated with a channel injecting noise into the observations. We also identify examples where this separability parameter is necessary in the bound: if it is too small, then the greedy policy fails to select informative tests."
1477,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing,"Antoine Bordes, Xavier Glorot, Jason Weston, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bordes12/bordes12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bordes12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bordes12.html,Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR - a formal representation of its sense). Unfortunately large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70000 words mapped to more than 40000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words entities and MRs via a multi-task training process operating on these diverse sources of data. Hence the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.
1478,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Taking machine learning research online with OpenML,"Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/vanschoren15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_vanschoren15,http://jmlr.csail.mit.edu/proceedings/papers/v41/vanschoren15.html,"OpenML is an online platform where scientists can automatically log and share machine learning data sets, code, and experiments, organize them online, and build directly on the work of others. It helps to automate many tedious aspects of research, is readily integrated into several machine learning tools, and offers easy-to-use APIs. It also enables large-scale and real-time collaboration, allowing researchers to share their very latest results, while keeping track of their impact and reuse. The combined and linked results provide a wealth of information to speed up research, assist people while analyzing data, or automate the process altogether."
1479,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Doubly Aggressive Selective Sampling Algorithms for Classification,Koby Crammer,"JMLR W&CP 33 :140-148, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/crammer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/crammer14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_crammer14,http://jmlr.csail.mit.edu/proceedings/papers/v33/crammer14.html,"Online selective sampling algorithms learn to perform binary classification, and additionally they decided whether to ask, or query, for a label of any given example. We introduce two stochastic linear algorithms and analyze them in the worst-case mistake-bound framework. Even though stochastic, for some inputs, our algorithms query with probability 1 and make an update even if there is no mistake, yet the margin is small, hence they are doubly aggressive. We prove bounds in the worst-case settings, which may be lower than previous bounds in some settings. Experiments with 33 document classification datasets, some with 100Ks examples, show the superiority of doubly-aggressive algorithms both in performance and number of queries."
1480,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Online Bayesian Passive-Aggressive Learning,"Tianlin Shi, Jun Zhu",none,http://jmlr.org/proceedings/papers/v32/shi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/shi14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_shi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/shi14.html,"Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts."
1481,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Incremental Sparsification for Real-time Online Model Learning,"Duy Nguyen_Tuong, Jan Peters","9:557-564, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/nguyen_tuong10a/nguyen_tuong10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_nguyen_tuong10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/nguyen_tuong10a.html,Online model learning in real-time is required by many applications for example robot tracking control. It poses a difficult problem as fast and incremental online regression with large data sets is the essential component and cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper we propose a framework for online incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independency measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems.
1482,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Online Learning under Delayed Feedback,"Pooria Joulani, Andras Gyorgy, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/joulani13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_joulani13,http://jmlr.csail.mit.edu/proceedings/papers/v28/joulani13.html,"Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity."
1483,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Unified Algorithms for Online Learning and Competitive Analysis,"Niv Buchbinder, Shahar Chen, Joshep (Seffi) Naor and Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/buchbinder12/buchbinder12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_buchbinder12,http://jmlr.csail.mit.edu/proceedings/papers/v23/buchbinder12.html,"Online learning and competitive analysis are two widely studied frameworks for online decisionmaking settings. Despite the frequent similarity of the problems they study, there are significant differences in their assumptions, goals and techniques, hindering a unified analysis and richer interplay between the two. In this paper, we provide several contributions in this direction. We provide a single unified algorithm which by parameter tuning, interpolates between optimal regret for learning from experts (in online learning) and optimal competitive ratio for the metrical task systems problem (MTS) (in competitive analysis), improving on the results of Blum and Burch (1997). The algorithm also allows us to obtain new regret bounds against ""drifting"" experts, which might be of independent interest. Moreover, our approach allows us to go beyond experts/MTS, obtaining similar unifying results for structured action sets and ""combinatorial experts"", whenever the setting has a certain matroid structure."
1484,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Sparse Adaptive Dirichlet-Multinomial-like Processes,Marcus Hutter,none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Hutter13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Hutter13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Hutter13.html,"Online estimation and modelling of i.i.d. data for shortsequences over large or complex –alphabets” is a ubiquitous (sub)problem in machine learning, information theory, data compression, statistical language processing, and document analysis. The Dirichlet-Multinomial distribution (also called Polya urn scheme) and extensions thereof are widely applied for online i.i.d. estimation. Good a-priori choices for the parameters in this regime are difficult to obtain though. I derive an optimal adaptive choice for the main parameter via tight, data-dependent redundancy bounds for a related model. The 1-line recommendation is to set the êtotal massê = êprecisionê = êconcentrationê parameter to \(m/[2\ln\frac{n+1}{m}]\) , where \(n\) is the (past) sample size and \(m\) the number of different symbols observed (so far). The resulting estimator is simple, online, fast,and experimental performance is superb."
1485,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Modeling Skill Acquisition Over Time with Sequence and Topic Modeling,Jos_ Gonzˆlez-Brenes,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/gonzalez-brenes15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_gonzalez-brenes15,http://jmlr.csail.mit.edu/proceedings/papers/v38/gonzalez-brenes15.html,"Online education provides data from students solving problems at different levels of proficiency over time. Unfortunately, methods that use these data for inferring student knowledge rely on costly domain expertise. We propose three novel data-driven methods that bridge sequence modeling with topic models to infer studentsê time varying knowledge. These methods differ in complexity, interpretability, accuracy and human supervision. For example, our most interpretable method has similar classification accuracy to the models created by domain experts, but requires much less effort. On the other hand, the most accurate method is completely data-driven and improves predictions by up to 15% in AUC, an evaluation metric for classifiers."
1486,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Statistical Perspective on Algorithmic Leveraging,"Ping Ma, Michael Mahoney, Bin Yu",none,http://jmlr.org/proceedings/papers/v32/ma14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ma14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ma14.html,"One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method. Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with –shrinked” leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance."
1487,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On Learning Distributions from their Samples,"Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, Ananda Theertha Suresh",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kamath15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kamath15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kamath15.html,"One of the most natural and important questions in statistical learning is: how well can a distribution be approximated from its samples. Surprisingly, this question has so far been resolved for only one loss, the KL-divergence and even in this case, the estimator used is ad hoc and not well understood. We study distribution approximations for general loss measures. For \(\ell_2^2\) we determine the best approximation possible, for \(\ell_1\) and \(\chi^2\) we derive tight bounds on the best approximation, and when the probabilities are bounded away from zero, we resolve the question for all sufficiently smooth loss measures, thereby providing a coherent understanding of the rate at which distributions can be approximated from their samples."
1488,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Generalized Darting Monte Carlo,"Cristian Sminchisescu, Max Welling","2:516-523, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/sminchisescu07a/sminchisescu07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_sminchisescu07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/sminchisescu07a.html,One of the main shortcomings of Markov chain Monte Carlo samplers is their inability to mix between modes of the target distribution. In this paper we show that advance knowledge of the location of these modes can be incorporated into the MCMC sampler by introducing mode-hopping moves that satisfy detailed balance. The proposed sampling algorithm explores local mode structure through local MCMC moves (e.g. diffusion or Hybrid Monte Carlo) but in addition also represents the relative strengths of the different modes correctly using a set of global moves. This `mode-hopping' MCMC sampler can be viewed as a generalization of the darting method [1]. We illustrate the method on a `real world' vision application of inferring 3-D human body pose from single 2-D images.
1489,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Physics-Based Model Prior for Object-Oriented MDPs,"Jonathan Scholz, Martin Levihn, Charles Isbell, David Wingate",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/scholz14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_scholz14,http://jmlr.csail.mit.edu/proceedings/papers/v32/scholz14.html,"One of the key challenges in using reinforcement learning in robotics is the need for models that capture natural world structure. There are, methods that formalize multi-object dynamics using relational representations, but these methods are not sufficiently compact for real-world robotics. We present a physics-based approach that exploits modern simulation tools to efficiently parameterize physical dynamics. Our results show that this representation can result in much faster learning, by virtue of its strong but appropriate inductive bias in physical environments."
1490,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Randomized Optimum Models for Structured Prediction,"Daniel Tarlow, Ryan Adams, Richard Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/tarlow12b/tarlow12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_tarlow12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/tarlow12b.html,One approach to modeling structured discrete data is to describe the probability of states via an energy function and Gibbs distribution. A recurring difficulty in these models is the computation of the partition function which may require an intractable sum. However in many such models the mode can be found efficiently even when the partition function is unavailable. Recent work on Perturb-and-MAP (PM) models (Papandreou and Yuille 2011) has exploited this discrepancy to approximate the Gibbs distribution for Markov random fields (MRFs). Here we explore a broader class of models called Randomized Optimum Models (RandOMs) which include PM as a special case. This new class of models encompasses not only MRFs but also other models that have intractable partition functions yet permit efficient mode-finding such as those based on bipartite matchings shortest paths or connected components in a graph. We develop likelihood-based learning algorithms for RandOMs which empirical results indicate can produce better models than PM.
1491,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Structured Output Learning with High Order Loss Functions,"Daniel Tarlow, Richard Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/tarlow12a/tarlow12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_tarlow12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/tarlow12a.html,Often when modeling structured domains it is desirable to leverage information that is not naturally expressed as simply a label. Examples include knowledge about the evaluation measure that will be used at test time and partial (weak) label information. When the additional information has structure that factorizes according to small subsets of variables (i.e. is \emph{low order} or \emph{decomposable}) several approaches can be used to incorporate it into a learning procedure. Our focus in this work is the more challenging case where the additional information does not factorize according to low order graphical model structure; we call this the \emph{high order} case. We propose to formalize various forms of this additional information as high order loss functions which may have complex interactions over large subsets of variables. We then address the computational challenges inherent in learning according to such loss functions particularly focusing on the loss-augmented inference problem that arises in large margin learning; we show that learning with high order loss functions is often practical giving strong empirical results with one popular and several novel high-order loss functions in several settings.
1492,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Minimax rates for homology inference,"Sivaraman Balakrishnan, Alesandro Rinaldo, Don Sheehy, Aarti Singh, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/balakrishnan12a/balakrishnan12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_balakrishnan12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/balakrishnan12a.html,Often high dimensional data lie close to a low-dimensional submanifold and it is of interest to understand the geometry of these submanifolds. The homology groups of a (sub)manifold are important topological invariants that provide an algebraic summary of the manifold. These groups contain rich topological information for instance about the connected components holes tunnels and (sometimes) the dimension of the manifold. In this paper we consider the statistical problem of estimating the homology of a manifold from noisy samples under several different noise models. We derive upper and lower bounds on the minimax risk for this problem. Our upper bounds are based on estimators which are constructed from a union of balls of appropriate radius around carefully selected sample points. In each case we establish complementary lower bounds using Le Cam's lemma.
1493,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Off-policy Model-based Learning under Unknown Factored Dynamics,"Assaf Hallak, Francois Schnitzler, Timothy Mann, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hallak15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hallak15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hallak15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hallak15.html,"Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples."
1494,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,An Empirical Analysis of Off-policy Learning in Discrete MDPs,"Cosmin P_duraru, Doina Precup, Joelle Pineau, Gheorghe Com_nici","24:89-102, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/paduraru12a/paduraru12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_paduraru12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/paduraru12a.html,Off-policy evaluation is the problem of evaluating a decision-making policy using data collected under a different behaviour policy. While several methods are available for addressing off-policy evaluation little work has been done on identifying the best methods. In this paper we conduct an in-depth comparative study of several off-policy evaluation methods in non-bandit finite-horizon MDPs using randomly generated MDPs as well as a Mallard population dynamics model [Anderson 1975] . We find that un-normalized importance sampling can exhibit prohibitively large variance in problems involving look-ahead longer than a few time steps and that dynamic programming methods perform better than Monte-Carlo style methods.
1495,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Effective Wrapper-Filter hybridization through GRASP Schemata,Mohamed Amir Esseghir,"10:45-54, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/esseghir10a/esseghir10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_esseghir10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/esseghir10a.html,Of all of the challenges which face the selection of relevant features for predictive data mining or pattern recognition modeling the adaptation of computational intelligence techniques to feature selection problem requirements is one of the primary impediments. A new improved metaheuristic based on \textit{Greedy Randomized Adaptive Search Procedure} (GRASP) is proposed for the problem of Feature Selection. Our devised optimization approach provides an effective scheme for wrapper-filter hybridization through the adaptation of GRASP components. The paper investigates the GRASP component design as well as its adaptation to the feature selection problem. Carried out experiments showed Empirical effectiveness of the devised approach.
1496,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Subspace Embeddings and \(\ell_p\)-Regression Using Exponential Random Variables,"David Woodruff, Qin Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Woodruff13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Woodruff13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Woodruff13.html,"Oblivious low-distortion subspace embeddings are a crucial building block for numerical linear algebra problems. We show for any real \(p, 1 \leq p _ \infty\) , given a matrix \(M \in \mathbb{R}^{n \times d}\) with \(n \gg d\) , with constant probability we can choose a matrix \(\Pi\) with \(\max(1, n^{1-2/p}) \text{poly}(d)\) rows and \(n\) columns so that simultaneously for all \(x \in \mathbb{R}^d\) , \(\|Mx\|_p \leq \|\Pi Mx\|_{\infty} \leq \text{poly}(d) \|Mx\|_p.\) Importantly, \(\Pi M\) can be computed in the optimal \(O(\text{nnz}(M))\) time, where \(\text{nnz}(M)\) is the number of non-zero entries of \(M\) . This generalizes all previous oblivious subspace embeddings which required \(p \in [1,2]\) due to their use of \(p\) -stable random variables. Using our new matrices \(\Pi\) , we also improve the best known distortion of oblivious subspace embeddings of \(\ell_1\) into \(\ell_1\) with \(\tilde{O}(d)\) target dimension in \(O(\text{nnz}(M))\) time from \(\tilde{O}(d^3)\) to \(\tilde{O}(d^2)\) , which can further be improved to \(\tilde{O}(d^{3/2}) \log^{1/2} n\) if \(d = \Omega(\log n)\) , answering a question of Meng and Mahoney (STOC, 2013). We apply our results to \(\ell_p\) -regression, obtaining a \((1+\epsilon)\) -approximation in \(O(\text{nnz}(M)\log n) + \text{poly}(d/\epsilon)\) time, improving the best known \(\text{poly}(d/\epsilon)\) factors for every \(p \in [1, \infty) \setminus \{2\}\) . If one is just interested in a \(\text{poly}(d)\) rather than a \((1+\epsilon)\) -approximation to \(\ell_p\) -regression, a corollary of our results is that for all \(p \in [1, \infty)\) we can solve the \(\ell_p\) -regression problem without using general convex programming, that is, since our subspace embeds into \(\ell_{\infty}\) it suffices to solve a linear programming problem. Finally, we give the first protocols for the distributed \(\ell_p\) -regression problem for every \(p \geq 1\) which are nearly optimal in communication and computation."
1497,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Efficient Sample Mining for Object Detection,"Olivier Canevet, Francois Fleuret",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/canevet14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_canevet14a,http://jmlr.csail.mit.edu/proceedings/papers/v39/canevet14a.html,"Object detectors based on the sliding window technique are usually trained in two successive steps: first, an initial classifier is trained on a population of positive samples (i.e. images of the object to detect) and negative samples randomly extracted from scenes which do not contain the object to detect. Then, the scenes are scanned with that initial classifier to enrich the initial set with negative samples incorrectly classified as positive. This bootstrapping process provides the learning algorithm with –hard” samples, which help to improve the decision boundary. Little work has been done on how to efficiently enrich the training set. While the standard bootstrapping approach densely visits the scenes, we propose to evaluate which regions of scenes can be discarded without any further computation to concentrate the search on promising areas. We apply our method to two standard object detection settings, pedestrian and face detection, and show that it provides a multi-fold speed up."
1498,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Volumetric Spanners: an Efficient Exploration Basis for Learning,"Elad Hazan, Zohar Karnin, Raghu Meka","JMLR W&CP 35 :408-422, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/hazan14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_hazan14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/hazan14b.html,"Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set."
1499,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems,"Qiang Zhou, Qi Zhao",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoua15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhoua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhoua15.html,"Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening."
1500,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Relative Novelty Detection,"Alex Smola, Le Song, Choon Hui Teo","5:536-543, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/smola09a/smola09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_smola09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/smola09a.html,Novelty detection is an important tool for unsupervised data analysis. It relies on finding regions of low density within which events are then flagged as novel. By design this is dependent on the underlying measure of the space. In this paper we derive a formulation which is able to address this problem by allowing for a reference measure to be given in the form of a sample from an alternate distribution. We show that this optimization problem can be solved efficiently and that it works well in practice.
1501,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Incremental Tree-Based Inference with Dependent Normalized Random Measures,"Juho Lee, Seungjin Choi","JMLR W&CP 33 :558-566, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/lee14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/lee14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_lee14,http://jmlr.csail.mit.edu/proceedings/papers/v33/lee14.html,"Normalized random measures (NRMs) form a broad class of discrete random measures that are used as priors for Bayesian nonparametric models. Dependent normalized random measures (DNRMs) introduce dependencies in a set of NRMs, to facilitate the handling of data where the assumption of exchangeability is violated. Various methods have been developed to construct DNRMs; of particular interest is mixed normalized random measures (MNRMs), where DNRM is represented as a mixture of underlying shared normalized random measures. Emphasis in existing works is placed on the construction methods of DNRMs, but there is a little work on efficient inference for DNRMs. In this paper, we present a tree-based inference method for MNRM mixture models, extending Bayesian hierarchical clustering (BHC) which was originally developed as a deterministic approximate inference for Dirichlet process mixture (DPM) models. We also present an incremental inference for MNRM mixture models, building a tree incrementally in the sense that the tree structure is partially updated whenever a new data point comes in. The tree, when constructed in such a way, allows us to efficiently perform tree-consistent MAP inference in MRNM mixture models, determining a most probable tree-consistent partition, as well as to compute a marginal likelihood approximately. Numerical experiments on both synthetic and real-world datasets demonstrate the usefulness of our algorithm, compared to MCMC methods."
1502,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Opportunistic Strategies for Generalized No-Regret Problems,"Andrey Bernstein, Shie Mannor, Nahum Shimkin",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bernstein13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Bernstein13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Bernstein13.html,"No-regret algorithms has played a key role in on-line learning and prediction problems. In this paper, we focus on a generalized no-regret problem with vector-valued rewards, defined in terms of a desired reward set of the agent. For each mixed action q of the opponent, the agent has a set \(R(q)\) where the average reward should reside. In addition, the agent has a response mixed action p which brings the expected reward under these two actions, \(r(p, q)\) , to \(R(q)\) . If a strategy of the agent ensures that the average reward converges to \(R(\bar{q}_n)\) , where \(\bar{q}_n\) is the empirical distribution of the opponentês actions, for any strategy of the opponent, we say that it is a no-regret strategy with respect to \(R(q)\) . The standard no-regret problem is obtained as a special case for scalar rewards and \(R(q)\) = \({r \in R: r \geq r(q)}\) , where \(r(q) = \max_p r(p, q)\) . For this problem, the multifunction \(R(q)\) is convex, and no-regret strategies can be devised. Our main interest in this paper is in cases where this convexity property does not hold. The best that can be guaranteed in general then is the convergence of the average reward to \(R^c(\bar{q}_n)\) , the convex hull of \(R(\bar{q}_n)\) . However, as the game unfolds, it may turn out that the opponentês choices of actions are limited in some way. If these restrictions were known in advance, the agent could possibly ensure convergence of the average reward to some desired subset of \(R^c(\bar{q}_n)\) , or even approach \(R(\bar{q}_n)\) itself. We formulate appropriate goals for opportunistic no-regret strategies, in the sense that they may exploit such limitations on the opponentês action sequence in an on-line manner, without knowing them beforehand. As the main technical tool, we propose a class of approachability algorithms that rely on a calibrated forecast of the opponentês actions, which are opportunistic in the above mentioned sense. As an application, we consider the online no-regret problem with average cost constraints, introduced in Mannor, Tsitsiklis, and Yu (2009), where the best-response-in-hindsight is not generally attainable, but only its convex relaxation. Our proposed algorithm applied to this problem does attain the best-response-in-hindsight if the opponentês play happens to be stationary (either in terms of its mixed actions, or the empirical frequencies of its pure actions)."
1503,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Regularized Linear Regression: A Precise Analysis of the Estimation Error,"Christos Thrampoulidis, Samet Oymak, Babak Hassibi",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Thrampoulidis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Thrampoulidis15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Thrampoulidis15.html,"Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The machinery builds upon Gordonês Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known."
1504,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,On the High Dimensional Power of a Linear-Time Two Sample Test under Mean-shift Alternatives,"Sashank Reddi, Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/reddi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/reddi15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_reddi15,http://jmlr.csail.mit.edu/proceedings/papers/v38/reddi15.html,"Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ( general alternatives), and those which are designed to specifically test easier alternatives, like a difference in means ( mean-shift alternatives). The main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the testês power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and the first analysis of how tests designed for general alternatives perform against easier ones."
1505,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models,"Sinead Williamson, Avinava Dubey, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/williamson13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/williamson13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_williamson13,http://jmlr.csail.mit.edu/proceedings/papers/v28/williamson13.html,"Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods."
1506,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,A Fast Hierarchical Alternating Least Squares Algorithm for Orthogonal Nonnegative Matrix Factorization,"Keigo Kimura, Yuzuru Tanaka, Mineichi Kudo",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/kimura14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_kimura14,http://jmlr.csail.mit.edu/proceedings/papers/v39/kimura14.html,"Nonnegative Matrix Factorization (NMF) is a popular technique in a variety of fields due to its component-based representation with physical interpretablity. NMF finds a nonnegative hidden structures as oblique bases and coefficients. Recently, Orthogonal NMF (ONMF), imposing an orthogonal constraint into NMF, has been gathering a great deal of attention. ONMF is more appropriate for the clustering task because the resultant constrained matrix consisting of the coefficients can be considered as an indicator matrix. All traditional ONMF algorithms are based on multiplicative update rules or project gradient descent method. However, these algorithms are slow in convergence compared with the state-of-the-art algorithms used for regular NMF. This is because they update a matrix in each iteration step. In this paper, therefore, we propose to update the current matrix column-wisely using Hierarchical Alternating Least Squares algorithm (HALS) that is typically used for NMF. The orthogonality and nonnegativity constraints are both utilized efficiently in the column-wise update procedure. Through theoretical analysis and experiments on six real-life datasets, it was shown that the proposed algorithm converges faster than the other conventional ONMF algorithms due to a smaller number of iterations, although the theoretical complexity is the same. It was also shown that the orthogonality is also attained in an earlier stage."
1507,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Intersecting Faces: Non-negative Matrix Factorization With New Guarantees,"Rong Ge, James Zou",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/geb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/geb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_geb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/geb15.html,"Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems."
1508,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Statistical Optimization of Non-Negative Matrix Factorization,"Anoop Korattikara Balan, Levi Boyles, Max Welling, Jingu Kim, Haesun Park","15:128-136, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/balan11a/balan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_balan11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/balan11a.html,Non-Negative Matrix Factorization (NMF) is a dimensionality reduction method that has been shown to be very useful for a variety of tasks in machine learning and data mining. One of the fastest algorithms for NMF is the Block Principal Pivoting method (BPP) which follows a block coordinate descent approach. The optimization in each iteration involves solving a large number of expensive least squares problems. Taking the view that the design matrix was generated by a stochastic process and using the asymptotic normality of the least squares estimator we propose a method for improving the performance of BPP. Our method starts with a small subset of the columns and rows of the original matrix and uses frequentist hypothesis tests to adaptively increase the size of the problem. This achieves two objectives: 1) during the initial phase of the algorithm we solve far fewer much smaller sized least squares problems and 2) all hypothesis tests failing while using all the data represents a principled automatic stopping criterion. Our experiments on three real world datasets show that our algorithm significantly improves the performance of the original BPP algorithm.
1509,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Linear-time training of nonlinear low-dimensional embeddings,"Max Vladymyrov, Miguel Carreira-Perpinan","JMLR W&CP 33 :968-977, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/vladymyrov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/vladymyrov14-supp.tgz,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_vladymyrov14,http://jmlr.csail.mit.edu/proceedings/papers/v33/vladymyrov14.html,"Nonlinear embeddings such as stochastic neighbor embedding or the elastic embedding achieve better results than spectral methods but require an expensive, nonconvex optimization, where the objective function and gradient are quadratic on the sample size. We address this bottleneck by formulating the optimization as an \(N\) -body problem and using fast multipole methods (FMMs) to approximate the gradient in linear time. We study the effect, in theory and experiment, of approximating gradients in the optimization and show that the expected error is related to the mean curvature of the objective function, and that gradually increasing the accuracy level in the FMM over iterations leads to a faster training. When combined with standard optimizers, such as gradient descent or L-BFGS, the resulting algorithm beats the \(\mathcal{O}(N \log N)\) Barnes-Hut method and achieves reasonable embeddings for one million points in around three hoursê runtime."
1510,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Iterative Embedding with Robust Correction using Feedback of Error Observed,"Praneeth Vepakomma, Ahmed Elgammal",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/vepakomma15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_vepakomma15,http://jmlr.csail.mit.edu/proceedings/papers/v43/vepakomma15.html,"Nonlinear dimensionality reduction techniques of today are highly sensitive to outliers. Almost all of them are spectral methods and differ from each other over their treatment of the notion of neighborhood similarities computed amongst the high-dimensional input data points. These techniques aim to preserve the notion of this similarity structure in the low-dimensional output. The presence of unwanted outliers in the data directly influences the preservation of these neighborhood similarities amongst the majority of the non-outlier data, as these points ocuring in majority need to simultaneously satisfy their neighborhood similarities they form with the outliers while also satisfying the similarity structure they form with the non-outlier data. This issue disrupts the intrinsic structure of the manifold on which the majority of the non-outlier data lies when preserved via a homeomorphism on a low-dimensional manifold. In this paper we come up with an iterative algorithm that analytically solves for a non-linear embedding with mono- tonic improvements after each iteration. As an application of this iterative manifold learning algorithm, we come up with a framework that decomposes the pair-wise error observed between all pairs of points and update the neighborhood similarity matrix dynamically to downplay the effect of the outliers, over the majority of the non-outlier data being embedded into a lower dimension."
1511,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Nonlinear Dimensionality Reduction as Information Retrieval,"Jarkko Venna, Samuel Kaski","2:572-579, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/venna07a/venna07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_venna07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/venna07a.html,Nonlinear dimensionality reduction has so far been treated either as a data representation problem or as a search for a lowerdimensional manifold embedded in the data space. A main application for both is in information visualization to make visible the neighborhood or proximity relationships in the data but neither approach has been designed to optimize this task. We give such visualization a new conceptualization as an information retrieval problem; a projection is good if neighbors of data points can be retrieved well based on the visualized projected points. This makes it possible to rigorously quantify goodness in terms of precision and recall. A method is introduced to optimize retrieval quality; it turns out to be an extension of Stochastic Neighbor Embedding one of the earlier nonlinear projection methods for which we give a new interpretation: it optimizes recall. The new method is shown empirically to outperform existing dimensionality reduction methods.
1512,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Majorization-Minimization for Manifold Embedding,"Zhirong Yang, Jaakko Peltonen, Samuel Kaski",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15a-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_yang15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15a.html,"Nonlinear dimensionality reduction by manifold embedding has become a popular and powerful approach both for visualization and as preprocessing for predictive tasks, but more efficient optimization algorithms are still crucially needed. Majorization-Minimization (MM) is a promising approach that monotonically decreases the cost function, but it remains unknown how to tightly majorize the manifold embedding objective functions such that the resulting MM algorithms are efficient and robust. We propose a new MM procedure that yields fast MM algorithms for a wide variety of manifold embedding problems. In our majorization step, two parts of the cost function are respectively upper bounded by quadratic and Lipschitz surrogates, and the resulting upper bound can be minimized in closed form. For cost functions amenable to such QL-majorization, the MM yields monotonic improvement and is efficient: in experiments the newly developed MM algorithms outperform five state-of-the-art optimization approaches in manifold embedding tasks."
1513,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Quality assessment of nonlinear dimensionality reduction based on K-ary neighborhoods,"John Lee, Michel Verleysen","4:21-35, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/lee08a/lee08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_lee08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/lee08a.html,Nonlinear dimensionality reduction aims at providing low-dimensional representions of high-dimensional data sets. Many new methods have been recently proposed but the question of their assessment and comparison remains open. This paper reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. In this context the comparison of the ranks in the high- and low-dimensional spaces leads to the definition of the co-ranking matrix. Rank errors and concepts such as neighborhood intrusions and extrusions can be associated with different blocks of the co-ranking matrix. The considered quality criteria are then cast within this unifying framework and the blocks they involve are identified. The same framework allows us to propose simpler criteria which quantify two aspects of the embedding namely its overall quality and its tendency to favor either intrusions or extrusions. Eventually a simple experiment illustrates the soundness of the approach.
1514,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems,"Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gong13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13a.html,"Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets."
1515,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,High-dimensional Inference via Lipschitz Sparsity-Yielding Regularizers,"Zheng Pan, Changshui Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/pan13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/pan13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_pan13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/pan13a.html,"Non-convex regularizers are more and more applied to high-dimensional inference with sparsity prior knowledge. In general, the non-convex regularizer is superior to the convex ones in inference but it suffers the difficulties brought by local optimums and massive computation. A ""good"" regularizer should perform well in both inference and optimization. In this paper, we prove that some non-convex regularizers can be such ""good"" regularizers. They are a family of sparsity-yielding penalties with proper Lipschitz subgradients. These regularizers keep the superiority of non-convex regularizers in inference. Their estimation conditions based on sparse eigenvalues are weaker than the convex regularizers. Meanwhile, if properly tuned, they behave like convex regularizers since standard proximal methods guarantee to give stationary solutions. These stationary solutions, if sparse enough, are identical to the global solutions. If the solution sequence provided by proximal methods is along a sparse path, the convergence rate to the global optimum is on the order of \(1/k\) where \(k\) is the number of iterations."
1516,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Modeling Information Propagation with Survival Theory,"Manuel Gomez-Rodriguez, Jure Leskovec, Bernhard Schlkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gomez-rodriguez13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gomez-rodriguez13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gomez-rodriguez13.html,"Networks provide a •skeletonê for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data."
1517,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Discovering Latent Network Structure in Point Process Data,"Scott Linderman, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/linderman14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/linderman14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_linderman14,http://jmlr.csail.mit.edu/proceedings/papers/v32/linderman14.html,"Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts. Most often in network analysis, the edges are given. However, in many systems it is difficult or impossible to measure the network directly. Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence. In these cases, we are limited to noisy observations of events associated with each node. To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models. We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm. We evaluate this new model empirically on several datasets."
1518,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks,Nikolai Slavov,"9:757-764, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/slavov10a/slavov10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_slavov10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/slavov10a.html,Networks are becoming a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. Here I develop and apply a generative approach to network inference (RCweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. From all possible factor analysis (FA) decompositions explaining the variance in the data RCweb selects the FA decomposition that is consistent with a sparse underlying network. The sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy robustness to noise complexity scaling and computational efficiency) methods using l1 norm relaxation such as K-SVD and l1-based sparse principle component analysis (PCA). Results from simulated models demonstrate that RCweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. RCweb is robust to noise with gradual decrease in the parameter ranges as the noise level increases.
1519,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,A kernel method for unsupervised structured network inference,"Christoph Lippert, Oliver Stegle, Zoubin Ghahramani, Karsten Borgwardt","5:368-375, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/lippert09a/lippert09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_lippert09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/lippert09a.html,Network inference is the problem of inferring edges between a set of real-world objects for instance interactions between pairs of proteins in bioinformatics. Current kernel-based approaches to this problem share a set of common features: (i) they are supervised and hence require labeled training data; (ii) edges in the network are treated as mutually independent and hence topological properties are largely ignored; (iii) they lack a statistical interpretation. We argue that these common assumptions are often undesirable for network inference and propose (i) an unsupervised kernel method (ii) that takes the global structure of the network into account and (iii) is statistically motivated. We show that our approach can explain commonly used heuristics in statistical terms. In experiments on social networks different variants of our method demonstrate appealing predictive performance.
1520,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning,"Daniel Tarlow, Kevin Swersky, Laurent Charlin, Ilya Sutskever, Rich Zemel",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tarlow13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/tarlow13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tarlow13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tarlow13.html,"Neighborhood Components Analysis (NCA) is a popular method for learning a distance metric to be used within a k-nearest neighbors (kNN) classifier. A key assumption built into the model is that each point stochastically selects a single neighbor, which makes the model well-justified only for kNN with k=1. However, kNN classifiers with k_1 are more robust and usually preferred in practice. Here we present kNCA, which generalizes NCA by learning distance metrics that are appropriate for kNN with arbitrary k. The main technical contribution is showing how to efficiently compute and optimize the expected accuracy of a kNN classifier. We apply similar ideas in an unsupervised setting to yield kSNE and ktSNE, generalizations of Stochastic Neighbor Embedding (SNE, tSNE) that operate on neighborhoods of size k, which provide an axis of control over embeddings that allow for more homogeneous and interpretable regions. Empirically, we show that kNCA often improves classification accuracy over state of the art methods, produces qualitative differences in the embeddings as k is varied, and is more robust with respect to label noise."
1521,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Modeling Order in Neural Word Embeddings at Scale,"Andrew Trask, David Gilmore, Matthew Russell",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/trask15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_trask15,http://jmlr.csail.mit.edu/proceedings/papers/v37/trask15.html,"Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network."
1522,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results,Philip Thomas,none,http://jmlr.csail.mit.edu/proceedings/papers/v32/thomasb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_thomasb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/thomasb14.html,"Natural gradient ascent (NGA) is a popular optimization method that uses a positive definite metric tensor. In many applications the metric tensor is only guaranteed to be positive semidefinite (e.g., when using the Fisher information matrix as the metric tensor), in which case NGA is not applicable. In our first contribution, we derive generalized natural gradient ascent (GeNGA), a generalization of NGA which allows for positive semidefinite non-smooth metric tensors. In our second contribution we show that, in standard settings, GeNGA and NGA can both be divergent. We then establish sufficient conditions to ensure that both achieve various forms of convergence. In our third contribution we show how several reinforcement learning methods that use NGA without positive definite metric tensors can be adapted to properly use GeNGA."
1523,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Approximating Mutual Information by Maximum Likelihood Density Ratio Estimation,"Taiji Suzuki, Masashi Sugiyama, Jun Sese, Takafumi Kanamori","4:5-20, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/suzuki08a/suzuki08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_suzuki08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/suzuki08a.html,Mutual information is useful in various data processing tasks such as feature selection or independent component analysis. In this paper we propose a new method of approximating mutual information based on maximum likelihood estimation of a density ratio function. Our method called Maximum Likelihood Mutual Information (MLMI) has several attractive properties e.g. density estimation is not involved it is a single-shot procedure the global optimal solution can be efficiently computed and cross-validation is available for model selection. Numerical experiments show that MLMI compares favorably with existing methods.
1524,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance,"Simone Romano, James Bailey, Vinh Nguyen, Karin Verspoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/romano14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/romano14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_romano14,http://jmlr.csail.mit.edu/proceedings/papers/v32/romano14.html,"Mutual information is a very popular measure for comparing clusterings. Previous work has shown that it is beneficial to make an adjustment for chance to this measure, by subtracting an expected value and normalizing via an upper bound. This yields the constant baseline property that enhances intuitiveness. In this paper, we argue that a further type of statistical adjustment for the mutual information is also beneficial - an adjustment to correct selection bias. This type of adjustment is useful when carrying out many clustering comparisons, to select one or more preferred clusterings. It reduces the tendency for the mutual information to choose clustering solutions i) with more clusters, or ii) induced on fewer data points, when compared to a reference one. We term our new adjusted measure the *standardized mutual information*. It requires computation of the variance of mutual information under a hypergeometric model of randomness, which is technically challenging. We derive an analytical formula for this variance and analyze its complexity. We then experimentally assess how our new measure can address selection bias and also increase interpretability. We recommend using the standardized mutual information when making multiple clustering comparisons in situations where the number of records is small compared to the number of clusters considered."
1525,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Scalable Nonparametric Multiway Data Analysis,"Shandian Zhe, Zenglin Xu, Xinqi Chu, Yuan Qi, Youngja Park",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhe15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_zhe15,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhe15.html,"Multiway data analysis deals with multiway arrays, i.e., tensors, and the goal is twofold: predicting missing entries by modeling the interactions between array elements and discovering hidden patterns, such as clusters or communities in each mode. Despite the success of existing tensor factorization approaches, they are either unable to capture nonlinear interactions, or computationally expensive to handle massive data. In addition, most of the existing methods lack a principled way to discover latent clusters, which is important for better understanding of the data. To address these issues, we propose a scalable nonparametric tensor decomposition model. It employs Dirichlet process mixture (DPM) prior to model the latent clusters; it uses local Gaussian processes (GPs) to capture nonlinear relationships and to improve scalability. An efficient online variational Bayes Expectation-Maximization algorithm is proposed to learn the model. Experiments on both synthetic and real-world data show that the proposed model is able to discover latent clusters with higher prediction accuracy than competitive methods. Furthermore, the proposed model obtains significantly better predictive performance than the state-of-the-art large scale tensor decomposition algorithm, GigaTensor, on two large datasets with billions of entries."
1526,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,One-Pass Multi-View Learning,"Yue Zhu, Wei Gao, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhu15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhu15.html,"Multi-view learning has been an important learning paradigm where data come from multiple channels or appear in multiple modalities. Many approaches have been developed in this field, and have achieved better performance than single-view ones. Those approaches, however, always work on small-size datasets with low dimensionality, owing to their high computational cost. In recent years, it has been witnessed that many applications involve large-scale multi-view data, e.g., hundreds of hours of video (including visual, audio and text views) is uploaded to YouTube every minute, bringing a big challenge to previous multi-view algorithms. This work concentrates on the large-scale multi-view learning for classification and proposes the One-Pass Multi-View (OPMV) framework which goes through the training data only once without storing the entire training examples. This approach jointly optimizes the composite objective functions with consistency linear constraints for different views. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm."
1527,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Integration of Single-view Graphs with Diffusion of Tensor Product Graphs for Multi-view Spectral Clustering,"Le Shu, Longin Jan Latecki",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Shu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Shu15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Shu15.html,"Multi-view clustering takes diversity of multiple views (representations) into consideration. Multiple views may be obtained from various sources or different feature subsets and often provide complementary information to each other. In this paper, we propose a novel graph-based approach to integrate multiple representations to improve clustering performance. While original graphs have been widely used in many existing multi-view clustering approaches, the key idea of our approach is to integrate multiple views by exploring higher order information. In particular, given graphs constructed separately from single view data, we build cross-view tensor product graphs (TPGs), each of which is a Kronecker product of a pair of single-view graphs. Since each cross-view TPG captures higher order relationships of data under two different views, it is no surprise that we obtain more reliable similarities. We linearly combine multiple cross-view TPGs to integrate higher order information. Efficient graph diffusion process on the fusion TPG helps to reveal the underlying cluster structure and boosts the clustering performance. Empirical study shows that the proposed approach outperforms state-of-the-art methods on benchmark datasets."
1528,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data,"Yarin Gal, Yutian Chen, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gala15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/gala15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gala15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gala15.html,"Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data."
1529,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Factorial Multi-Task Learning : A Bayesian Nonparametric Approach,"Sunil Gupta, Dinh Phung, Svetha Venkatesh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gupta13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gupta13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/gupta13a.html,"Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods."
1530,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Probabilistic Model for Dirty Multi-task Feature Selection,"Daniel Hernandez-Lobato, Jose Miguel Hernandez-Lobato, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoa15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hernandez-lobatoa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoa15.html,"Multi-task feature selection methods often make the hypothesis that learning tasks share relevant and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for only some of the tasks (outlier features). To account for this, we propose a model for multi-task feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features. Expectation propagation can be used for efficient approximate inference under the proposed prior. Several experiments show that a model based on the new robust prior provides better predictive performance than other benchmark methods."
1531,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices,"Jie Wang, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangf15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wangf15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wangf15.html,"Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening ruleãthat is based on the dual projection onto convex sets (DPC)ãto quickly identify the inactive featuresãthat have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive featuresãespecially for high dimensional dataãwhich leads to a speedup up to several orders of magnitude."
1532,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Boosting multi-step autoregressive forecasts,"Souhaib Ben Taieb, Rob Hyndman",none,http://jmlr.org/proceedings/papers/v32/taieb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/taieb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_taieb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/taieb14.html,"Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies."
1533,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Two-Layer Multiple Kernel Learning,"Jinfeng Zhuang, Ivor W. Tsang, Steven C.H. Hoi","15:909-917, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhuang11a/zhuang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhuang11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhuang11a.html,Multiple Kernel Learning (MKL) aims to learn kernel machines for solving a real machine learning problem (e.g. classification) by exploring the combinations of multiple kernels. The traditional MKL approach is in general ñshallowî in the sense that the target kernel is simply a linear (or convex) combination of some base kernels. In this paper we investigate a framework of Multi-Layer Multiple Kernel Learning (MLMKL) that aims to learn ñdeepî kernel machines by exploring the combinations of multiple kernels in a multi-layer structure which goes beyond the conventional MKL approach. Through a multiple layer mapping the proposed MLMKL framework offers higher flexibility than the regular MKL for finding the optimal kernel for applications. As the first attempt to this new MKL framework we present a two-Layer Multiple Kernel Learning (2LMKL) method together with two efficient algorithms for classification tasks. We analyze their generalization performances and have conducted an extensive set of experiments over 16 benchmark datasets in which encouraging results showed that our method outperformed the conventional MKL methods.
1534,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,MILEAGE: Multiple Instance LEArning with Global Embedding,"Dan Zhang, Jingrui He, Luo Si, Richard Lawrence",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13a.html,"Multiple Instance Learning (MIL) methods generally represent each example as a collection of instances such that the features for local objects can be better captured, whereas traditional learning methods typically extract a global feature vector for each example as an integral part. However, there is limited research work on which of the two learning scenarios performs better. This paper proposes a novel framework _ Multiple Instance LEArning with Global Embedding (MILEAGE) , in which the global feature vectors for traditional learning methods are integrated into the MIL setting. MILEAGE can leverage the benefits derived from both learning settings. Within the proposed framework, a large margin method is formulated. In particular, the proposed method adaptively tunes the weights on the two different kinds of feature representations (i.e., global and multiple instance) for each example and trains the classifier simultaneously. An alternative algorithm is proposed to solve the resulting optimization problem, which extends the bundle method to the non-convex case. Some important properties of the proposed method, such as the convergence rate and the generalization error rate, are analyzed. A series of experiments have been conducted to demonstrate the advantages of the proposed method over several state-of-the-art multiple instance and traditional learning methods."
1535,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,On the Consistency of Multi-Label Learning,"Wei Gao, Zhi-Hua Zhou","19:341-358, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/gao11a/gao11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_gao11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/gao11a.html,Multi-label learning has attracted much attention during the past few years. Many multi-label learning approaches have been developed mostly working with surrogate loss functions since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity. Though these approaches are effective to the best of our knowledge there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper focusing on two well-known multi-label loss functions i.e. \textit{ranking loss} and \textit{hamming loss} we prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that surprisingly none convex surrogate loss is consistent with the ranking loss. Inspired by the finding we introduce the \textit{partial ranking loss} with which some surrogate functions are consistent. For hamming loss we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classification and give a surrogate loss function which is consistent for the deterministic case. Finally we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classification problems.
1536,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Efficient Multi-label Classification with Many Labels,"Wei Bi, James Kwok",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bi13.html,"Multi-label classification deals with the problem where each instance can be associated with a set of class labels. However, in many real-world applications, the number of class labels can be in the hundreds or even thousands, and existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are either based on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by randomized sampling where the sampling probability of each class label reflects its importance among all the labels. Theoretical analysis shows that this randomized sampling approach is highly efficient. Experiments on a number of real-world multi-label datasets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm."
1537,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Multi-instance multi-label learning in the presence of novel class instances,"Anh Pham, Raviv Raich, Xiaoli Fern, Jesìs P_rez Arriaga",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/pham15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/pham15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_pham15,http://jmlr.csail.mit.edu/proceedings/papers/v37/pham15.html,"Multi-instance multi-label learning (MIML) is a framework for learning in the presence of label ambiguity. In MIML, experts provide labels for groups of instances (bags), instead of directly providing a label for every instance. When labeling efforts are focused on a set of target classes, instances outside this set will not be appropriately modeled. For example, ornithologists label bird audio recordings with a list of species present. Other additional sound instances, e.g., a rain drop or a moving vehicle sound, are not labeled. The challenge is due to the fact that for a given bag, the presence or absence of novel instances is latent. In this paper, this problem is addressed using a discriminative probabilistic model that accounts for novel instances. We propose an exact and efficient implementation of the maximum likelihood approach to determine the model parameters and consequently learn an instance-level classifier for all classes including the novel class. Experiments on both synthetic and real datasets illustrate the effectiveness of the proposed approach."
1538,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Near-Optimal Bounds for Cross-Validation via Loss Stability,"Ravi Kumar, Daniel Lokshtanov, Sergei Vassilvitskii, Andrea Vattani",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kumar13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/kumar13a.html,Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm. Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds. In this work we introduce a new and weak measure of stability called loss stability and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal. Our work thus quantitatively improves the current best bounds on cross-validation.
1539,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,"Multicore Gibbs Sampling in Dense, Unstructured Graphs","Tianbing Xu, Alexander Ihler","15:798-806, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/xu11a/xu11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_xu11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/xu11a.html,Multicore computing is on the rise but algorithms such as Gibbs sampling are fundamentally sequential and may require close consideration to be made parallel. Existing techniques either exploit sparse problem structure or make approximations to the algorithm; in this work we explore an alternative to these ideas. We develop a parallel Gibbs sampling algorithm for shared-memory systems that does not require any independence structure among the variables yet does not approximate the sampling distributions. Our method uses a look-ahead sampler which uses bounds to attempt to sample variables before the results of other threads are made available. We demonstrate our algorithm on Gibbs sampling in Boltzmann machines and latent Dirichlet allocation (LDA). We show in experiments that our algorithm achieves near linear speed-up in the number of cores is faster than existing exact samplers and is nearly as fast as approximate samplers while maintaining the correct stationary distribution.
1540,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning an Internal Dynamics Model from Control Demonstration,"Matthew Golub, Steven Chase, Byron Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/golub13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_golub13,http://jmlr.csail.mit.edu/proceedings/papers/v28/golub13.html,"Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics. However, if the controller is a human or animal subject, the subjectês internal dynamics model may differ from the true plant dynamics. Here, we consider the problem of learning the subjectês internal model from demonstrations of control and knowledge of task goals. Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state. We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model, internal state trajectories, and feedback delay. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface (BMI) control. We discovered that the subjectês internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics."
1541,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Lower Bounds and Hardness Amplification for Learning Shallow Monotone Formulas,"Vitaly Feldman, Homin K. Lee, Rocco A. Servedio","19:273-292, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/feldman11a/feldman11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_feldman11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/feldman11a.html,"Much work has been done on learning various classes of ``simple""monotone functions under the uniform distribution.In this paper we give the first unconditional lower bounds for learningproblems of this sort by showing that polynomial-time algorithms cannotlearn shallow monotone Boolean formulas under the uniform distributionin the well-studied Statistical Query (SQ) model.We introduce a new approach to understanding the learnability of ``simple"" monotone functions that is based ona recent characterization of Strong SQ learnability by \citet{Simon-2007}Using the characterization we first show that depth-3 monotone formulas of size $n^{o(1)}$ cannot be learned byany polynomial-time SQ algorithm to accuracy $1 -1/(\log n)^{\Omega(1)}.$ We then build on this result to show thatdepth-4 monotone formulas of size $n^{o(1)}$ cannot be learned evento a certain ${\frac 1 2} + o(1)$ accuracy in polynomial time. Thisimproved hardness is achieved using a general technique that weintroduce for amplifying the hardness of ``mildly hard'' learningproblems in either the PAC or SQ framework. Thishardness amplification for learning builds on the ideas in the work of\citet{ODonnell-2002}on hardness amplification forapproximating functions using small circuits and is applicable to a number of other contexts.Finally we demonstrate that our approach can also be used to reduce the well-known open problem of learning juntas to learning of depth-3 monotone formulas.\ignore{Text version:Much work has been done on learning various classes of ""simple""monotone functions under the uniform distribution.In this paper we give the first unconditional lower bounds for learningproblems of this sort by showing that polynomial-time algorithms cannotlearn constant-depth monotone Boolean formulas under the uniform distributionin the well-studied Statistical Query model.Using a recent characterization of Strong Statistical Querylearnability due to Feldman (FOCS 2009) we first show thatdepth-3 monotone formulas of size $n^{o(1)}$ cannot be learned byany polynomial-time Statistical Query algorithm to accuracy $1 -1/(\log n)^{\Omega(1)}.$ We then build on this result to show thatdepth-4 monotone formulas of size $n^{o(1)}$ cannot be learned evento a certain $1/2 + o(1)$ accuracy in polynomial time. Thisimproved hardness is achieved using a general technique that weintroduce for amplifying the hardness of ``mildly hard'' learningproblems in either the PAC or Statistical Query framework. Thishardness amplification for learning builds on the ideas in the work ofO'Donnell (STOC 2002) on hardness amplification forapproximating functions using small circuits and is applicable to a number of other contexts.Finally we demonstrate that our technique can also be used to reduce the well-known open problem of learning juntas to learning of depth-3 monotone formulas.}"
1542,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust RegBayes: Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models,"Shike Mei, Jun Zhu, Jerry Zhu",none,http://jmlr.org/proceedings/papers/v32/mei14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mei14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mei14.html,"Much research in Bayesian modeling has been done to elicit a prior distribution that incorporates domain knowledge. We present a novel and more direct approach by imposing First-Order Logic (FOL) rules on the posterior distribution. Our approach unifies FOL and Bayesian modeling under the regularized Bayesian framework. In addition, our approach automatically estimates the uncertainty of FOL rules when they are produced by humans, so that reliable rules are incorporated while unreliable ones are ignored. We apply our approach to latent topic modeling tasks and demonstrate that by combining FOL knowledge and Bayesian modeling, we both improve the task performance and discover more structured latent representations in unsupervised and supervised learning."
1543,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Why Does Unsupervised Pre-training Help Deep Learning?,"Dumitru Erhan, Aaron Courville, Yoshua Bengio, Pascal Vincent","9:201-208, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/erhan10a/erhan10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_erhan10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/erhan10a.html,Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results being obtained in several areas mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al. 2009) and as an aid to optimization (Bengio et al. 2007). Our results build on the work of Erhan et al. 2009 showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting with a virtually unlimited data stream point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.
1544,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Nested Chinese Restaurant Franchise Process: Applications to User Tracking and Document Modeling,"Amr Ahmed, Liangjie Hong, Alexander Smola",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ahmed13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ahmed13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ahmed13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ahmed13.html,"Much natural data is hierarchical in nature. Moreover, this hierarchy is often shared between different instances. We introduce the nested Chinese Restaurant Franchise Process as a means to obtain both hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin to the Hierarchical Dirichlet Process. Moreover, by decoupling the structure generating part of the process from the components responsible for the observations, we are able to apply the same statistical approach to a variety of user generated data. In particular, we model the joint distribution of microblogs and locations for Twitter for users. This leads to a 40% reduction in location uncertainty relative to the best previously published results. Moreover, we model documents from the NIPS papers dataset, obtaining excellent perplexity relative to (hierarchical) Pachinko allocation and LDA."
1545,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Nonparametric Mixture of Gaussian Processes with Constraints,"James Ross, Jennifer Dy",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ross13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ross13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/ross13a.html,"Motivated by the need to identify new and clinically relevant categories of lung disease, we propose a novel clustering with constraints method using a Dirichlet process mixture of Gaussian processes in a variational Bayesian nonparametric framework. We claim that individuals should be grouped according to biological and/or genetic similarity regardless of their level of disease severity; therefore, we introduce a new way of looking at subtyping/clustering by recasting it in terms of discovering associations of individuals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). The nonparametric nature of our algorithm allows for learning the unknown number of meaningful trajectories. Additionally, we acknowledge the usefulness of expert guidance by providing for their input using must-link and cannot- link constraints. These constraints are encoded with Markov random fields. We also provide an efficient variational approach for performing inference on our model."
1546,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,"Inferring (k,l)-context-sensitive probabilistic context-free grammars using hierarchical Pitman-Yor processes",Chihiro Shibata,"JMLR W&CP 34 :153-166, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/shibata14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_shibata14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/shibata14a.html,"Motivated by the idea of applying nonparametric Bayesian models to dual approaches for distributional learning, we define \((k,l)\) -context-sensitive probabilistic context-free grammars (PCFGs) using hierarchical Pitman-Yor processes (PYPs). The data sparseness problem that occurs when inferring context-sensitive probabilities for rules is handled by the smoothing effect of hierarchical PYPs. Many possible definitions or constructions of PYP hierarchies can be used to represent the context sensitivity of derivations of CFGs in Chomsky normal form. In this study, we use a definition that is considered to be the most natural as an extension of infinite PCFGs defined in previous studies. A Markov Chain Monte Carlo method called blocked Metropolis-Hastings (MH) sampling is known to be effective for inferring PCFGs from unsupervised sentences. Blocked MH sampling is applicable to \((k,l)\) -context-sensitive PCFGs by modifying their so-called inside probabilities. We show that the computational cost of blocked MH sampling for \((k,l)\) -context-sensitive PCFGs is \(O(|V|^{l+3}|s|^3)\) for each sentence \(s\) , where \(V\) is a set of nonterminals. This cost is too high to iterate sufficient sampling times, especially when \(l \neq 0\) , thus we propose an alternative sampling method that separates the sampling procedure into pointwise sampling for nonterminals and blocked sampling for rules. The computational cost of this sampling method is \(O(\min\{|s|^{l},|V|^{l}\} (|V||s|^2+|s|^3) )\) ."
1547,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,A Correlation Clustering Approach to Link Classification in Signed Networks,"NicolÖ Cesa-Bianchi, Claudio Gentile, Fabio Vitale and Giovanni Zappella",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/cesa-bianchi12/cesa-bianchi12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_cesa-bianchi12,http://jmlr.csail.mit.edu/proceedings/papers/v23/cesa-bianchi12.html,"Motivated by social balance theory, we develop a theory of link classification in signed networks using the correlation clustering index as measure of label regularity. We derive learning bounds in terms of correlation clustering within three fundamental transductive learning settings: online, batch and active. Our main algorithmic contribution is in the active setting, where we introduce a new family of efficient link classifiers based on covering the input graph with small circuits. These are the first active algorithms for link classification with mistake bounds that hold for arbitrary signed networks."
1548,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Neyman-Pearson classification under a strict constraint,"Philippe Rigollet, Xin Tong","19:595-614, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/rigollet11a/rigollet11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_rigollet11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/rigollet11a.html,Motivated by problems of anomaly detection this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classification with a convex loss. Given a finite collection of classifiers we combine them and obtain a new classifier that satisfies simultaneously the two following properties with high probability: (i) its probability of type~I error is below a pre-specified level and (ii) it has probability of type ~II error close to the minimum possible. The proposed classifier is obtained by minimizing an empirical objective subject to an empirical constraint. The novelty of the method is that the classifier output by this problem is shown to satisfy the original constraint on type~I error. This strict enforcement of the constraint has interesting consequences on the control of the type~II error and we develop new techniques to handle this situation. Finally connections with chance constrained optimization are evident and are investigated.
1549,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Batched Bandit Problems,"Vianney Perchet, Philippe Rigollet, Sylvain Chassang, Erik Snowberg",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Perchet15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Perchet15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Perchet15.html,"Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic multi-armed bandits under the constraint that the employed policy must split trials into a small number of batches. Our results show that a very small number of batches gives already close to minimax optimal regret bounds and we also evaluate the number of trials in each batch. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits."
1550,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,"Bandits, Query Learning, and the Haystack Dimension","Kareem Amin, Michael Kearns, Umar Syed","19:87-106, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/amin11a/amin11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_amin11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/amin11a.html,Motivated by multi-armed bandits (MAB) problems with a very large or even infinite number of arms we consider the problem of finding a maximum of an unknown target function by querying the function at chosen inputs (or arms).We give an analysis of the query complexity of this problem under the assumption that the payoff of each arm is given by a function belonging to a known finite but otherwise arbitrary function class. Our analysis centers on a new notion of function class complexity that we callthe \emph{haystack dimension} which is used to prove the approximate optimality of a simple greedy algorithm. This algorithm is then usedas a subroutine in a \parametric MAB algorithm yielding provably near-optimal regret. We provide a generalization to the infinite cardinality setting andcomment on how our analysis is connected to and improves upon existing results for query learning and generalized binary search.
1551,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Local Algorithm for Finding Well-Connected Clusters,"Zeyuan Allen Zhu, Silvio Lattanzi, Vahab Mirrokni",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/allenzhu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/allenzhu13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_allenzhu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/allenzhu13.html,"Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data. More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering."
1552,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits,"Nicolas Galichet, Michèle Sebag, Olivier Teytaud","JMLR W&CP 29 :245-260, 2013",http://jmlr.org/proceedings/papers/v29/Galichet13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Galichet13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Galichet13.html,"Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MaRaB) algorithm. With the goal of limiting the exploration of risky arms, MaRaB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MaRaB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is supported by extensive experimental validation of MIN and MaRaB compared to UCB and state-of-art risk-aware MAB algorithms on artificial and real-world problems."
1553,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Efficient Sampling for Gaussian Graphical Models via Spectral Sparsification,"Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cheng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Cheng15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cheng15.html,"Motivated by a sampling problem basic to computational statistical inference, we develop a toolset based on spectral sparsification for a family of fundamental problems involving Gaussian sampling, matrix functionals, and reversible Markov chains. Drawing on the connection between Gaussian graphical models and the recent breakthroughs in spectral graph theory, we give the first nearly linear time algorithm for the following basic matrix problem: Given an \(n\times n\) Laplacian matrix \(\mathbf{M}\) and a constant \(-1 \leq p \leq 1\) , provide efficient access to a sparse \(n\times n\) linear operator \(\tilde{\mathbf{C}}\) such that \[\mathbf{M}^{p} \approx \tilde{\mathbf{C}} \tilde{\mathbf{C}}^\top,\] where \(\approx\) denotes spectral similarity. When \(p\) is set to \(-1\) , this gives the first parallel sampling algorithm that is essentially optimal both in total work and randomness for Gaussian random fields with symmetric diagonally dominant (SDD) precision matrices. It only requires nearly linear work and \(2n\) i.i.d. random univariate Gaussian samples to generate an \(n\) -dimensional i.i.d. Gaussian random sample in polylogarithmic depth. The key ingredient of our approach is an integration of spectral sparsification with multilevel method: Our algorithms are based on factoring \(\mathbf{M}^p\) into a product of well-conditioned matrices, then introducing powers and replacing dense matrices with sparse approximations. We give two sparsification methods for this approach that may be of independent interest. The first invokes Maclaurin series on the factors, while the second builds on our new nearly linear time spectral sparsification algorithm for random-walk matrix polynomials. We expect these algorithmic advances will also help to strengthen the connection between machine learning and spectral graph theory, two of the most active fields in understanding large data and networks."
1554,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Probabilistic acoustic tube: a probabilistic generative model of speech for speech analysis/synthesis,"Zhijian Ou, Yang Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/ou12/ou12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_ou12,http://jmlr.csail.mit.edu/proceedings/papers/v22/ou12.html,Most speech analysis/synthesis systems are based on the basic physical model of speech production - the acoustic tube model. There are two main drawbacks with current speech analysis methods. First a common design paradigm seems to build a special-purpose signal-processing front-end followed by (when appropriate) a back-end based on probabilistic models. A difficulty is that most features are nonlinear operators of the speech waveform whose statistical behavior is hard to be modeled. Second different tasks of speech analysis are carried out separately. These practices are admittedly useful but not optimal due to the incomplete use of available information. These examinations motivate us to directly model the spectrogram and to integrate together the three fundamental speech parameters - the pitch energy and spectral envelope. We successfully devise such a model called probabilistic acoustic tube (PAT) model. The integration is performed in a principled manner with explicit physical meaning. We demonstrate the capability of PAT for a number of speech analysis/synthesis tasks such as pitch tracking under both clean and additive noise conditions speech synthesis and phoneme clustering.
1555,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sparse projections onto the simplex,"Anastasios Kyrillidis, Stephen Becker, Volkan Cevher, Christoph Koch",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kyrillidis13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kyrillidis13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kyrillidis13.html,"Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the \(\ell_1\) -norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints."
1556,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Filtered Search for Submodular Maximization with Controllable Approximation Bounds,"Wenlin Chen, Yixin Chen, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chen15c,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15c.html,"Most existing submodular maximization algorithms provide theoretical guarantees with approximation bounds. However, in many cases, users may be interested in an anytime algorithm that can offer a flexible trade-off between computation time and optimality guarantees. In this paper, we propose a filtered search (FS) framework that allows the user to set an arbitrary approximation bound guarantee with a –tunable knob”, from 0 (arbitrarily bad) to 1 (globally optimal). FS naturally handles monotone and non-monotone functions as well as unconstrained problems and problems with cardinality, matroid, and knapsack constraints. Further, it can also be applied to (non-negative) non-submodular functions and still gives controllable approximation bounds based on their submodularity ratio. Finally, FS encompasses the greedy algorithm as a special case. Our framework is based on theory in A* search, but is substantially more efficient because it only requires heuristics that are critically admissible (CA) rather than admissibleãa condition that gives more effective pruning and is substantially easier to implement."
1557,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Bayesian Hierarchical Cross-Clustering,"Dazhuo Li, Patrick Shafto","15:443-451, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/li11c/li11c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_li11c,http://jmlr.csail.mit.edu/proceedings/papers/v15/li11c.html,Most clustering algorithms assume that all dimensions of the data can be described by a single structure. Cross-clustering (or multi- view clustering) allows multiple structures each applying to a subset of the dimen- sions. We present a novel approach to cross- clustering based on approximating the so- lution to a Cross Dirichlet Process mixture (CDPM) model [Shafto et al. 2006 Mans- inghka et al. 2009]. Our bottom-up de- terministic approach results in a hierarchi- cal clustering of dimensions and at each node a hierarchical clustering of data points. We also present a randomized approxima- tion based on a truncated hierarchy that scales linearly in the number of levels. Re- sults on synthetic and real-world data sets demonstrate that the cross-clustering based algorithms perform as well or better than the clustering based algorithms our determinis- tic approaches models perform as well as the MCMC-based CDPM and the randomized approximation provides a remarkable speed- up relative to the full deterministic approxi- mation with minimal cost in predictive error.
1558,47,http://jmlr.csail.mit.edu/proceedings/papers/v47/,U-statistics on network-structured data with kernels of degree larger than one,"Yuyi Wang, Christos Pelekis, Jan Ramon",none,http://jmlr.csail.mit.edu/proceedings/papers/v47/wang14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v47/,,27th November 2015,41897,ECML/PKDD Workshop on Statistically Sound Data Mining 2014 Proceedings,Statistically Sound Data Mining,"Joensuu, Finland","Wilhelmiina HÕ_mÕ_lÕ_inen, FranÕ_ois Petitjean, Geoffrey, I. Webb",v47_wang14a,http://jmlr.csail.mit.edu/proceedings/papers/v47/wang14a.html,"Most analysis of \(U\) -statistics assumes that data points are independent or stationary. However, when we analyze network data, these two assumptions do not hold any more. We first define the problem of weighted \(U\) -statistics on networked data by extending previous work. We analyze their variance using Hoeffdingês decomposition and also give exponential concentration inequalities. Two efficiently solvable linear programs are proposed to find estimators with minimum worst-case variance or with tighter concentration inequalities."
1559,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Guided Monte Carlo Tree Search for Planning in Learned Environments,"Jelle Van Eyck, Jan Ramon, Fabian Guiza, Geert MeyFroidt, Maurice Bruynooghe, Greet Van den Berghe","JMLR W&CP 29 :33-47, 2013",http://jmlr.org/proceedings/papers/v29/Eyck13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Eyck13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Eyck13.html,"Monte Carlo tree search (MCTS) is a sampling and simulation based technique for searching in large search spaces containing both decision nodes and probabilistic events. This technique has recently become popular due to its successful application to games, e.g. Poker and Go. Such games have known rules and the alternation between self-moves and non-deterministic events or opponent moves can be used to prune uninteresting branches. In this paper we study a real-world setting where the processes in the domain have a high degree of uncertainty and the need for longer-term planning implies a sequence of (planning) decisions without any intermediate feedback. Fortunately, unlike the combinatorial complexity in strategic games, many real-world environments can be approximated by efficient algorithms on a short term. This paper proposes an MCTS variant using a new type of prior information based on estimating the effects of part of the world and explores its application to the problem of hospital planning, where machine learning algorithms can be used to predict the length of stay of patients for each of the different stages of their recovery."
1560,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Fast Variational Mode-Seeking,"Bo Thiesson, Jingu Kim",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/thiesson12/thiesson12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_thiesson12,http://jmlr.csail.mit.edu/proceedings/papers/v22/thiesson12.html,Mode-seeking algorithms (e.g. mean-shift) constitute a class of powerful non-parametric clustering methods but they are slow. We present VMS a dual-tree based variational EM framework for mode-seeking that greatly accelerates performance. VMS has a number of pleasing properties: it generalizes across different mode-seeking algorithms it does not have typical homoscedasticity constraints on kernel bandwidths and it is the first truly sub-quadratic acceleration method that maintains provable convergence for a well-defined objective function. Experimental results demonstrate acceleration benefits over competing methods and show that VMS is particularly desirable for data sets of massive size where a coarser approximation is needed to improve the computational efficiency.
1561,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Mixed-Variate Restricted Boltzmann Machines,"T. Tran, D. Phung & S. Venkatesh","20:213_229, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/tran11/tran11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_tran11,http://jmlr.csail.mit.edu/proceedings/papers/v20/tran11.html,Modern datasets are becoming heterogeneous. To this end we present in this paper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling variables of multiple types and modalities including binary and continuous responses categorical options multicategorical choices ordinal assessment and category-ranked preferences. Dependency among variables is modeled using latent binary variables each of which can be interpreted as a particular hidden aspect of the data. The proposed model similar to the standard RBMs allows fast evaluation of the posterior for the latent variables. Hence it is naturally suitable for many common tasks including but not limited to (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors thereby o_ering a dimensionality reduction capacity (b) as a classi_er supporting binary multiclass multilabel and label-ranking outputs or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. We evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization data completion and prediction.   Page last modified on Sun Nov 6 15:43:31 2011.
1562,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Optimizing Non-decomposable Performance Measures: A Tale of Two Classes,"Harikrishna Narasimhan, Purushottam Kar, Prateek Jain",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhana15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhana15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_narasimhana15,http://jmlr.csail.mit.edu/proceedings/papers/v37/narasimhana15.html,"Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data."
1563,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Estimating Probabilities in Recommendation Systems,"Mingxuan Sun, Guy Lebanon, Paul Kidwell","15:734-742, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/sun11a/sun11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_sun11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/sun11a.html,Modeling ranked data is an essential component in a number of important applications including recommendation systems and web-search. In many cases judges omit preference among unobserved items and between unobserved and observed items. This case of analyzing incomplete rankings is very important from a practical perspective and yet has not been fully studied due to considerable computational difficulties. We show how to avoid such computational difficulties and efficiently construct a non-parametric model for rankings with missing items. We demonstrate our approach and show how it applies in the context of collaborative filtering.
1564,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Predicting Preference Reversals via Gaussian Process Uncertainty Aversion,"Rikiya Takahashi, Tetsuro Morimura",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/takahashi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/takahashi15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_takahashi15,http://jmlr.csail.mit.edu/proceedings/papers/v38/takahashi15.html,"Modeling of a product or serviceês attractiveness as a function of its own attributes (e.g., price and quality) is one of the foundations in econometric forecasts, which have been provided with an assumption that each human rationally has a consistent preference order among his choice decisions. Yet the preference orders by real humans become irrationally reversed, when the choice set of available options is manipulated. In order to accurately predict choice decisions involving preference reversals, which existing econometric methods have failed to incorporate, the authors introduce a new cognitive choice model whose parameters are efficiently fitted with a global convex optimization algorithm. The proposed model captures each human as a Bayesian decision maker facing a mental conflict between objective evaluation samples and a subjective prior, where the underlying objective evaluation function is rationally independent from contexts while the subjective prior is irrationally determined by each choice set. As the key idea to analytically handle the irrationality and to yield the convex optimization, the Bayesian decision mechanism is implemented as a closed-form Gaussian process regression using similarities among the available options in each context. By explaining the irrational decisions as a consequence of averting uncertainty, the proposed model outperformed the existing econometric models in predicting the irrational choice decisions recorded in real-world datasets."
1565,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,A Tensor Spectral Approach to Learning Mixed Membership Community Models,"Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham Kakade",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anandkumar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Anandkumar13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anandkumar13.html,"Modeling community formation and detecting hidden communities in networks is a well studied problem. However, theoretical analysis of community detection has been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and consider a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced in Aioroldi et. al (2008). This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of \(3\) -star counts. Our learning method is fast and is based on simple linear algebra operations, e.g. singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. Additionally, our results match the best known scaling requirements in the special case of the stochastic block model."
1566,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Bayesian Learning of Recursively Factored Environments,"Marc Bellemare, Joel Veness, Michael Bowling",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bellemare13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bellemare13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bellemare13.html,"Model-based reinforcement learning techniques have historically encountered a number of difficulties scaling up to large observation spaces. One promising approach has been to decompose the model learning task into a number of smaller, more manageable sub-problems by factoring the observation space. Typically, many different factorizations are possible, which can make it difficult to select an appropriate factorization without extensive testing. In this paper we introduce the class of recursively decomposable factorizations, and show how exact Bayesian inference can be used to efficiently guarantee predictive performance close to the best factorization in this class. We demonstrate the strength of this approach by presenting a collection of empirical results for 20 different Atari 2600 games."
1567,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A New Generalized Error Path Algorithm for Model Selection,"Bin Gu, Charles Ling",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gu15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gu15.html,"Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search."
1568,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function,"Yong Liu, Shali Jiang, Shizhong Liao",none,http://jmlr.org/proceedings/papers/v32/liua14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liua14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liua14.html,"Model selection is one of the key issues both in recent research and application of kernel methods. Cross-validation is a commonly employed and widely accepted model selection criterion. However, it requires multiple times of training the algorithm under consideration, which is computationally intensive. In this paper, we present a novel strategy for approximating the cross-validation based on the Bouligand influence function (BIF), which only requires the solution of the algorithm once. The BIF measures the impact of an infinitesimal small amount of contamination of the original distribution. We first establish the link between the concept of BIF and the concept of cross-validation. The BIF is related to the first order term of a Taylor expansion. Then, we calculate the BIF and higher order BIFs, and apply these theoretical results to approximate the cross-validation error in practice. Experimental results demonstrate that our approximate cross-validation criterion is sound and efficient."
1569,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Approximate Model Selection for Large Scale LSSVM,L. Ding & S. Liao,"20:165_180, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/ding11/ding11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_ding11,http://jmlr.csail.mit.edu/proceedings/papers/v20/ding11.html,Model selection is critical to least squares support vector machine (LSSVM). A major problem of existing model selection approaches of LSSVM is that the inverse of the kernel matrix need to be calculated with O ( n   3  ) complexity for each iteration where n is the number of training examples. It is prohibitive for the large scale application. In this paper we propose an approximate approach to model selection of LSSVM. We use multilevel circulant matrices to approximate the kernel matrix so that the fast Fourier transform (FFT) can be applied to reduce the computational cost of matrix inverse. With such approximation we _rst design an e_cient LSSVM algorithm with O ( n log( n )) complexity and theoretically analyze the e_ect of kernel matrix approximation on the decision function of LSSVM. We further show that the approximate optimal model produced with the multilevel circulant matrix is consistent with the accurate one produced with the original kernel matrix. Under the guarantee of consistency we present an approximate model selection scheme whose complexity is signi_cantly lower than the previous approaches. Experimental results on benchmark datasets demonstrate the e_ectiveness of approximate model selection.   Page last modified on Sun Nov 6 15:43:10 2011.
1570,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Detecting Sentiment Change in Twitter Streaming Data,"Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Ricard Gavalda","17:5-11, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/bifet11a/bifet11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_bifet11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/bifet11a.html,"MOA-TweetReader is a real-time system to read tweets in real time, to detect changes, and to find the terms whose frequency changed. Twitter is a micro-blogging service built to discover what is happening at any moment in time, anywhere in the world. Twitter messages are short, and generated constantly, and well suited for knowledge discovery using data stream mining. MOA-TweetReader is a software extension to the MOA framework. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA-TweetReader is released under the GNU GPL license."
1571,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Preface,"Sa_o D_eroski, Pierre Geurts, Juho Rousu","8:1-2, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/dzeroski10a/dzeroski10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_dzeroski10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/dzeroski10a.html,"MLSB09, the Third International Workshop on Machine Learning in Systems Biology was held in Ljubljana, Slovenia on September 5-6 2009 at the Jo_ef Stefan Institute. This volume contains revised selected papers presented at the workshop. The technical program of the workshop consisted of 6 invited lectures, 12 oral presentations and 22 poster presentations. All the lectures were recorded and are available for viewing via the videolectures.net portal. More information on the workshop can be found at mlsb09.ijs.si"
1572,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,"A Rate of Convergence for Mixture Proportion Estimation, with Application to Learning from Noisy Labels",Clayton Scott,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/scott15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_scott15,http://jmlr.csail.mit.edu/proceedings/papers/v38/scott15.html,"Mixture proportion estimation (MPE) is a fundamental tool for solving a number of weakly supervised learning problems _ supervised learning problems where label information is noisy or missing. Previous work on MPE has established a universally consistent estimator. In this work we establish a rate of convergence for mixture proportion estimation under an appropriate distributional assumption, and argue that this rate of convergence is useful for analyzing weakly supervised learning algorithms that build on MPE. To illustrate this idea, we examine an algorithm for classification in the presence of noisy labels based on surrogate risk minimization, and show that the rate of convergence for MPE enables proof of the algorithmês consistency. Finally, we provide a practical implementation of mixture proportion estimation and demonstrate its efficacy in classification with noisy labels."
1573,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,A Method of Moments for Mixture Models and Hidden Markov Models,"Animashree Anandkumar, Daniel Hsu and Sham M. Kakade",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/anandkumar12/anandkumar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_anandkumar12,http://jmlr.csail.mit.edu/proceedings/papers/v23/anandkumar12.html,"Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics ( e.g. , the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient method of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment."
1574,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Alternating Minimization for Mixed Linear Regression,"Xinyang Yi, Constantine Caramanis, Sujay Sanghavi",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yia14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yia14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yia14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yia14.html,"Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels). In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EMês performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem."
1575,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Mixability is Bayes Risk Curvature Relative to Log Loss,"Tim van Erven, Mark D. Reid, Robert C. Williamson","19:233-252, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/vanerven11a/vanerven11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_vanerven11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/vanerven11a.html,Mixability of a loss governs the best possible performance when aggregating expert predictions with respect to that loss. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the Hessian of the Bayes risk relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses we also show how to carry the results across to improper losses.
1576,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Generalized Mixability via Entropic Duality,"Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson, Nishant Mehta",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Reid15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Reid15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Reid15.html,"Mixability is a property of a loss which characterizes when constant regret is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the \(\exp\) and \(\log\) operations present in the usual theory are not as special as one might have thought. In doing so we introduce a more general notion of \(\Phi\) -mixability where \(\Phi\) is a general entropy ( i.e. , any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical Aggregating Algorithm, is guaranteed a constant regret when used with \(\Phi\) -mixable losses. We characterize which \(\Phi\) have non-trivial \(\Phi\) -mixable losses and relate \(\Phi\) -mixability and its associated Aggregating Algorithm to potential-based methods, a Blackwell-like condition, mirror descent, and risk measures from finance. We also define a notion of –dominance” between different entropies in terms of bounds they guarantee and conjecture that classical mixability gives optimal bounds, for which we provide some supporting empirical evidence."
1577,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Minimum Volume Embedding,"Blake Shaw, Tony Jebara","2:460-467, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/shaw07a/shaw07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_shaw07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/shaw07a.html,Minimum Volume Embedding (MVE) is an algorithm for non-linear dimensionality reduction that uses semidefinite programming (SDP) and matrix factorization to find a low-dimensional embedding that preserves local distances between points while representing the dataset in many fewer dimensions. MVE follows an approach similar to algorithms such as Semidefinite Embedding (SDE) in that it learns a kernel matrix using an SDP before applying Kernel Principal Component Analysis (KPCA). However the objective function for MVE directly optimizes the eigenspectrum of the data to preserve as much of its energy as possible within the few dimensions available to the embedding. Simultaneously remaining eigenspectrum energy is minimized in directions orthogonal to the embedding thereby keeping data in a so-called minimum volume manifold. We show how MVE improves upon SDE in terms of the volume of the preserved embedding and the resulting eigenspectrum producing better visualizations for a variety of synthetic and real-world datasets including simple toy examples face images handwritten digits phylogenetic trees and social networks.
1578,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Computational Complexity of Linear Large Margin Classification With Ramp Loss,"SËren Frejstrup Maibing, Christian Igel",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/frejstrupmaibing15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_frejstrupmaibing15,http://jmlr.csail.mit.edu/proceedings/papers/v38/frejstrupmaibing15.html,"Minimizing the binary classification error with a linear model leads to an NP-hard problem. In practice, surrogate loss functions are used, in particular loss functions leading to large margin classification such as the hinge loss and the ramp loss. The intuitive large margin concept is theoretically supported by generalization bounds linking the expected classification error to the empirical margin error and the complexity of the considered hypotheses class. This article addresses the fundamental question about the computational complexity of determining whether there is a hypotheses class with a hypothesis such that the upper bound on the generalization error is below a certain value. Results of this type are important for model comparison and selection. This paper takes a first step and proves that minimizing a basic margin-bound is NP-hard when considering linear hypotheses and the rho-margin loss function, which generalizes the ramp loss. This result directly implies the hardness of ramp loss minimization."
1579,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,In Defense of Minhash over Simhash,"Anshumali Shrivastava, Ping Li","JMLR W&CP 33 :886-894, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/shrivastava14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_shrivastava14,http://jmlr.csail.mit.edu/proceedings/papers/v33/shrivastava14.html,"MinHash and SimHash are the two widely adopted Locality Sensitive Hashing (LSH) algorithms for large-scale data processing applications. Deciding which LSH to use for a particular problem at hand is an important question, which has no clear answer in the existing literature. In this study, we provide a theoretical answer (validated by experiments) that MinHash virtually always outperforms SimHash when the data are binary, as common in practice such as search. The collision probability of MinHash is a function of resemblance similarity ( \(\mathcal{R}\) ), while the collision probability of SimHash is a function of cosine similarity ( \(\mathcal{S}\) ). To provide a common basis for comparison, we evaluate retrieval results in terms of \(\mathcal{S}\) for both MinHash and SimHash. This evaluation is valid as we can prove that MinHash is a valid LSH with respect to \(\mathcal{S}\) , by using a general inequality \(\mathcal{S}^2\leq \mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}\) . Our worst case analysis can show that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is also substantially better than SimHash even in datasets where most of the data points are not too similar to each other. This is partly because, in practical data, often \(\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}\) holds where \(z\) is only slightly larger than 2 (e.g., \(z\leq 2.1\) ). Our restricted worst case analysis by assuming \(\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}\) shows that MinHash indeed significantly outperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines for search in practice, especially when the data are sparse."
1580,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sequential Bayesian Search,"Zheng Wen, Branislav Kveton, Brian Eriksson, Sandilya Bhamidipati",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wen13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wen13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wen13.html,"Millions of people search daily for movies, music, and books on the Internet. Unfortunately, non-personalized exploration of items can result in an infeasible number of costly interaction steps. We study the problem of efficient, repeated interactive search. In this problem, the user is navigated to the items of interest through a series of options and our objective is to learn a better search policy from past interactions with the user. We propose an efficient learning algorithm for solving the problem, sequential Bayesian search (SBS), and prove that it is Bayesian optimal. We also analyze the algorithm from the frequentist point of view and show that its regret is sublinear in the number of searches. Finally, we evaluate our method on a real-world movie discovery problem and show that it performs nearly optimally as the number of searches increases."
1581,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Robust Structural Metric Learning,"Daryl Lim, Gert Lanckriet, Brian McFee",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lim13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lim13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lim13.html,"Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking. However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction. Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings."
1582,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning to Search Better than Your Teacher,"Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, John Langford",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/changb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/changb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_changb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/changb15.html,"Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications."
1583,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Learning Scale Free Networks by Reweighted L1 regularization,"Qiang Liu, Alexander Ihler","15:40-48, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/liu11a/liu11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_liu11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/liu11a.html,Methods for L1-type regularization have been widely used in Gaussian graphical model selection tasks to encourage sparse structures. However often we would like to include more structural information than mere sparsity. In this work we focus on learning so-called ``scale-free'' models a common feature that appears in many real-work networks. We replace the L1 regularization with a power law regularization and optimize the objective function by a sequence of iteratively reweighted L1 regularization problems where the regularization coefficients of nodes with high degree are reduced encouraging the appearance of hubs with high degree. Our method can be easily adapted to improve any existing L1-based methods such as graphical lasso neighborhood selection and JSRM when the underlying networks are believed to be scale free or have dominating hubs. We demonstrate in simulation that our method significantly outperforms the a baseline L1 method at learning scale-free networks and hub networks and also illustrate its behavior on gene expression data.
1584,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Memory (and Time) Efficient Sequential Monte Carlo,"Seong-Hwan Jun, Alexandre Bouchard-C»t_",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/jun14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/jun14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_jun14,http://jmlr.csail.mit.edu/proceedings/papers/v32/jun14.html,"Memory efficiency is an important issue in Sequential Monte Carlo (SMC) algorithms, arising for example in inference of high-dimensional latent variables via Rao-Blackwellized SMC algorithms, where the size of individual particles combined with the required number of particles can stress the main memory. Standard SMC methods have a memory requirement that scales linearly in the number of particles present at all stage of the algorithm. Our contribution is a simple scheme that makes the memory cost of SMC methods depends on the number of distinct particles that survive resampling. We show that this difference has a large empirical impact on the quality of the approximation in realistic scenarios, and alsoãsince memory access is generally slowãon the running time. The method is based on a two pass generation of the particles, which are represented implicitly in the first pass. We parameterize the accuracy of our algorithm with a memory budget rather than with a fixed number of particles. Our algorithm adaptively selects an optimal number of particle to exploit this fixed memory budget. We show that this adaptation does not interfere with the usual consistency guarantees that come with SMC algorithms."
1585,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fast Mean Shift with Accurate and Stable Convergence,"Ping Wang, Dongryeol Lee, Alexander Gray, James M. Rehg","2:604-611, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07d/wang07d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_wang07d,http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07d.html,Mean shift is a powerful but computationally expensive method for nonparametric clustering and optimization. It iteratively moves each data point to its local mean until convergence. We introduce a fast algorithm for computing mean shift based on the dual-tree. Unlike previous speed-up attempts our algorithm maintains a relative error bound at each iteration resulting in significantly more stable and accurate convergence. We demonstrate the benefit of our method in clustering experiments with real and synthetic data.
1586,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Efficient Sampling from Combinatorial Space via Bridging,"Dahua Lin, John Fisher",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/lin12/lin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_lin12,http://jmlr.csail.mit.edu/proceedings/papers/v22/lin12.html,MCMC sampling has been extensively studied and used in probabilistic inference. Many algorithms rely on local updates to explore the space often resulting in slow convergence or failure to mix when there is no path from one set of states to another via local changes. We propose an efficient method for sampling from combinatorial spaces that addresses these issues via ``bridging states'' that facilitate the communication between different parts of the space. Such states can be created dynamically providing more flexibility than methods relying on specific space structures to design jump proposals. Theoretical analysis of the approach yields bounds on mixing times. Empirical analysis demonstrates the practical utility on two problems: constrained map labeling and inferring partial order of object layers in a video.
1587,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing,"Huayan Wang, Koller Daphne",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13b.html,"Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing messages over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a unified message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition)."
1588,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Tighter and Convex Maximum Margin Clustering,"Yu-Feng Li, Ivor W. Tsang, Jame Kwok, Zhi-Hua Zhou","5:344-351, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/li09c/li09c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_li09c,http://jmlr.csail.mit.edu/proceedings/papers/v5/li09c.html,Maximum margin principle has been successfully applied to many supervised and semi-supervised problems in machine learning. Recently this principle was extended for clustering referred to as Maximum Margin Clustering (MMC) and achieved promising performance in recent studies. To avoid the problem of local minima MMC can be solved globally via convex semi-definite programming (SDP) relaxation. Although many efficient approaches have been proposed to alleviate the computational burden of SDP convex MMCs are still not scalable for medium data sets. In this paper we propose a novel convex optimization method LG-MMC which maximizes the margin of opposite clusters via label generation. It can be shown that LG-MMC is much more scalable than existing convex approaches. Moreover we show that our convex relaxation is tighter than state-of-art convex MMCs. Experiments on eighteen UCI datasets and MNIST dataset show significant improvement over existing MMC algorithms.
1589,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Polynomial Runtime Bounds for Fixed-Rank Unsupervised Least-Squares Classification,"Fabian Gieseke, Tapio Pahikkala, Christian Igel","JMLR W&CP 29 :62-71, 2013",http://jmlr.org/proceedings/papers/v29/Gieseke13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Gieseke13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Gieseke13.html,"Maximum margin clustering can be regarded as the direct extension of support vector machines to unsupervised learning scenarios. The goal is to partition unlabeled data into two classes such that a subsequent application of a support vector machine would yield the overall best result (with respect to the optimization problem associated with support vector machines). While being very appealing from a conceptual point of view, the combinatorial nature of the induced optimization problem renders a direct application of this concept difficult. In order to obtain efficient optimization schemes, various surrogates of the original problem definition have been proposed in the literature. In this work, we consider one of these variants, called unsupervised regularized least-squares classification, which is based on the square loss, and develop polynomial upper runtime bounds for the induced combinatorial optimization task. In particular, we show that for \(n\) patterns and kernel matrix of fixed rank \(r\) (with given eigendecomposition), one can obtain an optimal solution in \(\mathcal{O}(n^{r})\) time for \(r \leq 2\) and in \(\mathcal{O}(n^{r-1})\) time for \(r\geq 3\) . The algorithmic framework is based on an interesting connection to the field of quadratic zero-one programming and permits the computation of exact solutions for the more general case of non-linear kernel functions in polynomial time."
1590,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,On the Asymptotic Optimality of Maximum Margin Bayesian Networks,"Sebastian Tschiatschek, Franz Pernkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/tschiatschek13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/tschiatschek13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_tschiatschek13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/tschiatschek13a.html,"Maximum margin Bayesian networks (MMBNs) are Bayesian networks with discriminatively optimized parameters. They have shown good classification performance in various applications. However, there has not been any theoretic analysis of their asymptotic performance, e.g. their Bayes consistency. For specific classes of MMBNs, i.e. MMBNs with fully connected graphs and discrete-valued nodes, we show Bayes consistency for binary-class problems and a sufficient condition for Bayes consistency in the multi-class case. We provide simple examples showing that MMBNs in their current formulation are not Bayes consistent in general. These examples are especially interesting, as the model used for the MMBNs can represent the assumed true distributions. This indicates that the current formulations of MMBNs may be deficient. Furthermore, experimental results on the generalization performance are presented."
1591,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Statistical and Computational Tradeoffs in Stochastic Composite Likelihood,"Joshua Dillon, Guy Lebanon","5:129-136, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/dillon09a/dillon09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_dillon09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/dillon09a.html,Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. We prove the consistency of the estimators provide formulas for their asymptotic variance and computational complexity and discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators in achieving a predefined balance between computational complexity and statistical accuracy.
1592,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scalable Semidefinite Relaxation for Maximum A Posterior Estimation,"Qixing Huang, Yuxin Chen, Leonidas Guibas",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/huang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/huang14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_huang14,http://jmlr.csail.mit.edu/proceedings/papers/v32/huang14.html,"Maximum a posteriori (MAP) inference over discrete Markov random fields is a central task spanning a wide spectrum of real-world applications but known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of SDR. Encouragingly, the proposed procedure allows solving SDR for large-scale problems, e.g. problems comprising hundreds of thousands of variables with multiple states on a grid graph. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability. This contradicts the commonly held belief that semidefinite relaxation can only been applied on small-scale problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC. Experimental results demonstrate that for a broad class of problems, SDPAD-LR outperforms state-of-the-art algorithms in producing better MAP assignments."
1593,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Linear Approximation to ADMM for MAP inference,"Sholeh Forouzan, Alexander Ihler","JMLR W&CP 29 :48-61, 2013",http://jmlr.org/proceedings/papers/v29/Forouzan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Forouzan13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Forouzan13.html,"Maximum a posteriori (MAP) inference is one of the fundamental inference tasks in graphical models. MAP inference is in general NP-hard, making approximate methods of interest for many problems. One successful class of approximate inference algorithms is based on linear programming (LP) relaxations. The augmented Lagrangian method can be used to overcome a lack of strict convexity in LP relaxations, and the Alternating Direction Method of Multipliers (ADMM) provides an elegant algorithm for finding the saddle point of the augmented Lagrangian. Here we present an ADMM-based algorithm to solve the primal form of the MAP-LP whose closed form updates are based on a linear approximation technique. Our technique gives efficient, closed form updates that converge to the global optimum of the LP relaxation. We compare our algorithm to two existing ADMM-based MAP-LP methods, showing that our technique is faster on general, non-binary or non-pairwise models."
1594,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning,"Debarghya Ghoshdastidar, Ambedkar Dukkipati",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ghoshdastidar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ghoshdastidar15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ghoshdastidar15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ghoshdastidar15.html,"Matrix spectral methods play an important role in statistics and machine learning, and most often the word `matrixê is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of –higher-order” algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings."
1595,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,General Functional Matrix Factorization Using Gradient Boosting,"Tianqi Chen, Hang Li, Qiang Yang, Yong Yu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13e,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13e.html,"Matrix factorization is among the most successful techniques for collaborative filtering. One challenge of collaborative filtering is how to utilize available auxiliary information to improve prediction accuracy. In this paper, we study the problem of utilizing auxiliary information as features of factorization and propose formalizing the problem as general functional matrix factorization, whose model includes conventional matrix factorization models as its special cases. Moreover, we propose a gradient boosting based algorithm to efficiently solve the optimization problem. Finally, we give two specific algorithms for efficient feature function construction for two specific tasks. Our method can construct more suitable feature functions by searching in an infinite functional space based on training data and thus can yield better prediction accuracy. The experimental results demonstrate that the proposed method outperforms the baseline methods on three real-world datasets."
1596,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Collaborative Filtering on a Budget,"Alexandros Karatzoglou, Alex Smola, Markus Weimer","9:389-396, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/karatzoglou10a/karatzoglou10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_karatzoglou10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/karatzoglou10a.html,Matrix factorization is a successful technique for building collaborative filtering systems. While it works well on a large range of problems it is also known for requiring significant amounts of storage for each user or item to be added to the database. This is a problem whenever the collaborative filtering task is larger than the medium-sized Netflix Prize data. In this paper we propose a new model for representing and compressing matrix factors via hashing. This allows for essentially unbounded storage (at a graceful storage / performance trade-off) for users and items to be represented in a pre-defined memory footprint. It allows us to scale recommender systems to very large numbers of users or conversely obtain very good performance even for tiny models (e.g. 400kB of data suffice for a representation of the EachMovie problem). We provide both experimental results and approximation bounds for our compressed representation and we show how this approach can be extended to multipartite problems.
1597,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Computational Limits for Matrix Completion,"Moritz Hardt, Raghu Meka, Prasad Raghavendra, Benjamin Weitz","JMLR W&CP 35 :703-725, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/hardt14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_hardt14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/hardt14b.html,"Matrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries. Important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random. Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if we make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank \(4\) but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown matrix is incoherent and are shown \(90\%\) of the entries. This result relies on the conjectured hardness of the \(4\) -Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that \(\mathrm{P}\ne \mathrm{NP}.\) Our results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems."
1598,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Fast Exact Matrix Completion with Finite Samples,"Prateek Jain, Praneeth Netrapalli",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Jain15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Jain15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Jain15.html,"Matrix completion is the problem of recovering a low rank matrix by observing a small fraction of its entries. A series of recent works (Keshavan 2012),(Jain et al. 2013) and (Hardt, 2013) have proposed fast non-convex optimization based iterative algorithms to solve this problem. However, the sample complexity in all these results is sub-optimal in its dependence on the rank, condition number and the desired accuracy. In this paper, we present a fast iterative algorithm that solves the matrix completion problem by observing \(O\left(nr^5 \log^3 n\right)\) entries, which is independent of the condition number and the desired accuracy. The run time of our algorithm is \(O\left( nr^7\log^3 n\log 1/\epsilon \right)\) which is near linear in the dimension of the matrix. To the best of our knowledge, this is the first near linear time algorithm for exact matrix completion with finite sample complexity (i.e. independent of \(\epsilon\) ). Our algorithm is based on a well known projected gradient descent method, where the projection is onto the (non-convex) set of low rank matrices. There are two key ideas in our result: 1) our argument is based on a \(\ell_\infty\) norm potential function (as opposed to the spectral norm) and provides a novel way to obtain perturbation bounds for it. 2) we prove and use a natural extension of the Davis-Kahan theorem to obtain perturbation bounds on the best low rank approximation of matrices with good eigen gap. Both of these ideas may be of independent interest."
1599,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Coherent Matrix Completion,"Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward",none,http://jmlr.org/proceedings/papers/v32/chenc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenc14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chenc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chenc14.html,"Matrix completion concerns the recovery of a low-rank matrix from a subset of its revealed entries, and nuclear norm minimization has emerged as an effective surrogate for this combinatorial problem. Here, we show that nuclear norm minimization can recover an arbitrary \(n \times n\) matrix of rank r from O(nr log 2 (n)) revealed entries, provided that revealed entries are drawn proportionally to the local row and column coherences (closely related to leverage scores) of the underlying matrix. Our results are order-optimal up to logarithmic factors, and extend existing results for nuclear norm minimization which require strong incoherence conditions on the types of matrices that can be recovered, due to assumed uniformly distributed revealed entries. We further provide extensive numerical evidence that a proposed two-phase sampling algorithm can perform nearly as well as local-coherence sampling and without requiring a priori knowledge of the matrix coherence structure. Finally, we apply our results to quantify how weighted nuclear norm minimization can improve on unweighted minimization given an arbitrary set of sampled entries."
1600,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Can matrix coherence be efficiently and accurately estimated?,"Mehryar Mohri, Ameet Talwalkar","15:534-542, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/mohri11a/mohri11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mohri11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/mohri11a.html,Matrix coherence has recently been used to characterize the ability to extract global information from a subset of matrix entries in the context of low-rank approximations and other sampling-based algorithms. The significance of these results crucially hinges upon the possibility of efficiently and accurately testing this coherence assumption. This paper precisely addresses this issue. We introduce a novel sampling-based algorithm for estimating coherence present associated estimation guarantees and report the results of extensive experiments for coherence estimation. The quality of the estimation guarantees we present depends on the coherence value to estimate itself but this turns out to be an inherent property of sampling-based coherence estimation as shown by our lower bound. In practice however we find that these theoretically unfavorable scenarios rarely appear as our algorithm efficiently and accurately estimates coherence across a wide range of datasets and these estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis. These results are significant as they reveal the extent to which coherence assumptions made in a number of recent machine learning publications are testable.
1601,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Local Low-Rank Matrix Approximation,"Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lee13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lee13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lee13.html,"Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks."
1602,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Training Deep Convolutional Neural Networks to Play Go,"Christopher Clark, Amos Storkey",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/clark15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_clark15,http://jmlr.csail.mit.edu/proceedings/papers/v37/clark15.html,"Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more êhumanlikeê way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to êhard codeê symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time."
1603,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,"MOA: Massive Online Analysis, a Framework for Stream Classification and Clustering","Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Philipp Kranen, Hardy Kremer, Timm Jansen and Thomas Seidl","11:44-50, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/bifet10a/bifet10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_bifet10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/bifet10a.html,"Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA is designed to deal with the challenging problem of scaling up the implementation of state of the art algorithms to real world dataset sizes. It contains collection of offline and online for both classification and clustering as well as tools for evaluation. In particular, for classification it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. For clustering, it implements StreamKM++, CluStream, ClusTree, Den-Stream, D-Stream and CobWeb. Researchers benefit from MOA by getting insights into workings and problems of different approaches, practitioners can easily apply and compare several algorithms to real world data set and settings. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license."
1604,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,DART: Dropouts meet Multiple Additive Regression Trees,"Rashmi Korlakai Vinayak, Ran Gilad-Bachrach",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/korlakaivinayak15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_korlakaivinayak15,http://jmlr.csail.mit.edu/proceedings/papers/v38/korlakaivinayak15.html,"MART, an ensemble model of boosted regression trees, is known to deliver high prediction accuracy for diverse tasks, and is widely used in practice. However, it suffers an issue which we call over-specialization, wherein trees added at later iterations tend to impact the prediction of only a few instances, and make negligible contribution towards the remaining instances. This negatively affects the performance of the model on unseen data, and also makes the model over-sensitive to the contributions of the few, initially added tress. We show that the commonly used tool to address this issue, that of shrinkage, alleviates the problem only to a certain extent and the fundamental issue of over-specialization still remains. In this work, we explore a different approach to address the problem, that of employing dropouts, a tool that has been recently proposed in the context of learning deep neural networks. We propose a novel way of employing dropouts to tackle the issue of over-specialization in MART, resulting in the DART algorithm. We evaluate DART on ranking, regression and classification tasks, using large scale, publicly available datasets, and show that DART outperforms MART in each of the tasks, with a significant margin."
1605,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Bootstrapping and Learning PDFA in Data Streams,"Borja Balle, Jorge Castro and Ricard Gavald_","21:34-48, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/balle12a/balle12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_balle12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/balle12a.html,"Markovian models with hidden state are widely-used formalisms for modeling sequential phenomena. Learnability of these models has been well studied when the sample is given in batch mode, and algorithms with PAC-like learning guarantees exist for specific classes of models such as Probabilistic Deterministic Finite Automata (PDFA). Here we focus on PDFA and give an algorithm for infering models in this class under the stringent data stream scenario: unlike existing methods, our algorithm works incrementally and in one pass, uses memory sublinear in the stream length, and processes input items in amortized constant time. We provide rigorous PAC-like bounds for all of the above, as well as an evaluation on synthetic data showing that the algorithm performs well in practice. Our algorithm makes a key usage of several old and new sketching techniques. In particular, we develop a new sketch for implementing bootstrapping in a streaming setting which may be of independent interest. In experiments we have observed that this sketch yields important reductions in the examples required for performing some crucial statistical tests in our algorithm."
1606,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Mixed Graphical Models via Exponential Families,"Eunho Yang, Yulia Baker, Pradeep Ravikumar, Genevera Allen, Zhandong Liu","JMLR W&CP 33 :1042-1050, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14a-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_yang14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14a.html,"Markov Random Fields, or undirected graphical models are widely used to model high-dimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions to graphical models specified by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a novel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly different univariate exponential family. We study several instances of our model, and propose scalable \(M\) -estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods."
1607,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Solving Markov Random Fields with Spectral Relaxation,"Timothee Cour, Jianbo Shi","2:75-82, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/cour07a/cour07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_cour07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/cour07a.html,Markov Random Fields (MRFs) are used in a large array of computer vision and maching learning applications. Finding the Maximum Aposteriori (MAP) solution of an MRF is in general intractable and one has to resort to approximate solutions such as Belief Propagation Graph Cuts or more recently approaches based on quadratic programming. We propose a novel type of approximation Spectral relaxation to Quadratic Programming (SQP). We show our method offers tighter bounds than recently published work while at the same time being computationally efficient. We compare our method to other algorithms on random MRFs in various settings.
1608,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning Nearest-Neighbor Quantizers from Labeled Data by Information Loss Minimization,"Svetlana Lazebnik, Maxim Raginsky","2:251-258, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/lazebnik07a/lazebnik07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_lazebnik07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/lazebnik07a.html,Markov Random Fields (MRFs) are used in a large array of computer vision and maching learning applications. Finding the Maximum Aposteriori (MAP) solution of an MRF is in general intractable and one has to resort to approximate solutions such as Belief Propagation Graph Cuts or more recently approaches based on quadratic programming. We propose a novel type of approximation Spectral relaxation to Quadratic Programming (SQP). We show our method offers tighter bounds than recently published work while at the same time being computationally efficient. We compare our method to other algorithms on random MRFs in various settings.
1609,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Accurate and conservative estimates of MRF log-likelihood using reverse annealing,"Yuri Burda, Roger Grosse, Ruslan Salakhutdinov",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/burda15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_burda15,http://jmlr.csail.mit.edu/proceedings/papers/v38/burda15.html,"Markov random fields (MRFs) are difficult to evaluate as generative models because computing the test log-probabilities requires the intractable partition function. Annealed importance sampling (AIS) is widely used to estimate MRF partition functions, and often yields quite accurate results. However, AIS is prone to overestimate the log-likelihood with little indication that anything is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower bound on the log-likelihood of an approximation to the original MRF model. RAISE requires only the same MCMC transition operators as standard AIS. Experimental results indicate that RAISE agrees closely with AIS log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the side of underestimating, rather than overestimating, the log-likelihood."
1610,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Parallelizable Sampling of Markov Random Fields,"James Martens, Ilya Sutskever","9:517-524, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/martens10a/martens10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_martens10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/martens10a.html,Markov Random Fields (MRFs) are an important class of probabilistic models which are used for density estimation classification denoising and for constructing Deep Belief Networks. Every application of an MRF requires addressing its inference problem which can be done using deterministic inference methods or using stochastic Markov Chain Monte Carlo methods. In this paper we introduce a new Markov Chain transition operator that updates all the variables of a pairwise MRF in parallel by using auxiliary Gaussian variables. The proposed MCMC operator is extremely simple to implement and to parallelize. This is achieved by a formal equivalence result between arbitrary pairwise MRFs and a particular type of Restricted Boltzmann Machine. This result also implies that the later can be learned in place of the former without any loss of modeling power a possibility we explore in experiments.
1611,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Learning Markov Networks With Arithmetic Circuits,"Daniel Lowd, Amirmohammad Rooshenas",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/lowd13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_lowd13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/lowd13a.html,"Markov networks are an effective way to represent complex probability distributions. However, learning their structure and parameters or using them to answer queries is typically intractable. One approach to making learning and inference tractable is to use approximations, such as pseudo-likelihood or approximate inference. An alternate approach is to use a restricted class of models where exact inference is always efficient. Previous work has explored low treewidth models, models with tree-structured features, and latent variable models. In this paper, we introduce ACMN, the first ever method for learning efficient Markov networks with arbitrary conjunctive features. The secret to ACMNês greater flexibility is its use of arithmetic circuits, a linear-time inference representation that can handle many high treewidth models by exploiting local structure. ACMN uses the size of the corresponding arithmetic circuit as a learning bias, allowing it to trade off accuracy and inference complexity. In experiments on 12 standard datasets, the tractable models learned by ACMN are more accurate than both tractable models learned by other algorithms and approximate inference in intractable models."
1612,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes,"Jonathan Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash Mansinghka",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsa15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hugginsa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsa15.html,"Markov jump processes (MJPs) are used to model a wide range of phenomenon from disease progression to RNA path folding. However, existing methods suffer from a number of shortcomings: degenerate trajectories in the case of ML estimation of parametric models and poor inferential performance in the case of nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call JUMP-means . Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy."
1613,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach,"R_mi Bardenet, Arnaud Doucet, Chris Holmes",none,http://jmlr.org/proceedings/papers/v32/bardenet14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bardenet14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bardenet14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bardenet14.html,"Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICMLê14), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples."
1614,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Fast-Mixing Models for Structured Prediction,"Jacob Steinhardt, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardtb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardtb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_steinhardtb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardtb15.html,"Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the resulting approximate gradients can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks."
1615,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Optimal Distributed Market-Based Planning for Multi-Agent Systems with Shared Resources,"Sue Ann Hong, Geoffrey Gordon","15:351-360, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/hong11a/hong11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_hong11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/hong11a.html,Market-based algorithms have become popular in collaborative multi-agent planning due to their simplicity distributedness low communication requirements and proven success in domains such as task allocation and robotic exploration. Most existing market-based algorithms however suffer from two main drawbacks: resource prices must be carefully handcrafted for each problem domain and there is no guarantee on final solution quality. We present an optimal market-based algorithm derived from a mixed integer program formulation of planning problems. Our method is based on two well-known techniques for optimization: Dantzig-Wolfe decomposition and Gomory cuts. The former prices resources optimally for a relaxed version of the problem while the latter introduces new derivative resources to correct pricing imbalances that arise from the relaxation. Our algorithm is applicable to a wide variety of multi-agent planning domains. We provide optimality guarantees and demonstrate the effectiveness of our algorithm in both centralized and distributed settings on synthetic planning problems.
1616,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Coherence Functions for Multicategory Margin-based Classification Methods,"zhihua zhang, Michael Jordan, Wu-Jun Li, Dit-Yan Yeung","5:647-654, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/zhang09a/zhang09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_zhang09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/zhang09a.html,Margin-based classification methods are typically devised based on a majorization-minimization procedure which approximately solves an otherwise intractable minimization problem defined with the 0-l loss. However extension of such methods from the binary classification setting to the more general multicategory setting turns out to be non-trivial. In this paper our focus is to devise margin-based classification methods that can be seamlessly applied to both settings with the binary setting simply as a special case. In particular we propose a new majorization loss function that we call the coherence function and then devise a new multicategory margin-based boosting algorithm based on the coherence function. Analogous to deterministic annealing the coherence function is characterized by a temperature factor. It is closely related to the multinomial log-likelihood function and its limit at zero temperature corresponds to a multicategory hinge loss function.
1617,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Factorial Mixture of Gaussians and the Marginal Independence Model,"Ricardo Silva, Zoubin Ghahramani","5:520-527, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/silva09b/silva09b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_silva09b,http://jmlr.csail.mit.edu/proceedings/papers/v5/silva09b.html,Marginal independence constraints play an important role in learning with graphical models. One way of parameterizing a model of marginal independencies is by building a latent variable model where two independent observed variables have no common latent source. In sparse domains however it might be advantageous to model the marginal observed distribution directly without explicitly including latent variables in the model. There have been recent advances in Gaussian and binary models of marginal independence but no models with non-linear dependencies between continuous variables has been proposed so far. In this paper we describe how to generalize the Gaussian model of marginal independencies based on mixtures and how to learn parameters. This requires a non-standard parameterization and raises difficult non-linear optimization issues.
1618,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning from Corrupted Binary Labels via Class-Probability Estimation,"Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, Bob Williamson",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/menon15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/menon15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_menon15,http://jmlr.csail.mit.edu/proceedings/papers/v37/menon15.html,"Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels."
1619,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Semi-supervised Clustering by Input Pattern Assisted Pairwise Similarity Matrix Completion,"Jinfeng Yi, Lijun Zhang, Rong Jin, Qi Qian, Anil Jain",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/yi13.html,"Many semi-supervised clustering algorithms have been proposed to improve the clustering accuracy by effectively exploring the available side information that is usually in the form of pairwise constraints. Despite the progress, there are two main shortcomings of the existing semi-supervised clustering algorithms. First, they have to deal with non-convex optimization problems, leading to clustering results that are sensitive to the initialization. Second, none of these algorithms is equipped with theoretical guarantee regarding the clustering performance. We address these limitations by developing a framework for semi-supervised clustering based on input pattern assisted matrix completion . The key idea is to cast clustering into a matrix completion problem, and solve it efficiently by exploiting the correlation between input patterns and cluster assignments. Our analysis shows that under appropriate conditions, only \(O(\log n)\) pairwise constraints are needed to accurately recover the true cluster partition. We verify the effectiveness of the proposed algorithm by comparing it to the state-of-the-art semi-supervised clustering algorithms on several benchmark datasets."
1620,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Hierarchical Relative Entropy Policy Search,"Christian Daniel, Gerhard Neumann, Jan Peters",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/daniel12/daniel12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_daniel12,http://jmlr.csail.mit.edu/proceedings/papers/v22/daniel12.html,Many real-world problems are inherently hi- erarchically structured. The use of this struc- ture in an agent's policy may well be the key to improved scalability and higher per- formance. However such hierarchical struc- tures cannot be exploited by current policy search algorithms. We will concentrate on a basic but highly relevant hierarchy - the 'mixed option' policy. Here a gating network first decides which of the options to execute and subsequently the option-policy deter- mines the action. In this paper we reformulate learning a hi- erarchical policy as a latent variable estima- tion problem and subsequently extend the Relative Entropy Policy Search (REPS) to the latent variable case. We show that our Hierarchical REPS can learn versatile solu- tions while also showing an increased perfor- mance in terms of learning speed and quality of the found policy in comparison to the non- hierarchical approach.
1621,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Modeling Temporal Evolution and Multiscale Structure in Networks,"Tue Herlau, Morten Mrup, Mikkel Schmidt",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/herlau13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/herlau13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_herlau13,http://jmlr.csail.mit.edu/proceedings/papers/v28/herlau13.html,Many real-world networks exhibit both temporal evolution and multiscale structure. We propose a model for temporally correlated multifurcating hierarchies in complex networks which jointly capture both effects. We use the Gibbs fragmentation tree as prior over multifurcating trees and a change-point model to account for the temporal evolution of each vertex. We demonstrate that our model is able to infer time-varying multiscale structure in synthetic as well as three real world time-evolving complex networks. Our modeling of the temporal evolution of hierarchies brings new insights into the changing roles and position of entities and possibilities for better understanding these dynamic complex systems.
1622,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Reduction from Cost-Sensitive Multiclass Classification to One-versus-One Binary Classification,Hsuan-Tien Lin,none,http://jmlr.csail.mit.edu/proceedings/papers/v39/lin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_lin14,http://jmlr.csail.mit.edu/proceedings/papers/v39/lin14.html,"Many real-world applications require varying costs for different types of mis-classification errors. Such a cost-sensitive classification setup can be very different from the regular classification one, especially in the multiclass case. Thus, traditional meta-algorithms for regular multiclass classification, such as the popular one-versus-one approach, may not always work well under the cost-sensitive classification setup. In this paper, we extend the one-versus-one approach to the field of cost-sensitive classification. The extension is derived using a rigorous mathematical tool called the cost-transformation technique, and takes the original one-versus-one as a special case. Experimental results demonstrate that the proposed approach can achieve better performance in many cost-sensitive classification scenarios when compared with the original one-versus-one as well as existing cost-sensitive classification algorithms."
1623,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Markov Network Estimation From Multi-attribute Data,"Mladen Kolar, Han Liu, Eric Xing",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kolar13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kolar13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/kolar13a.html,"Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating multi-attribute graphs. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features. Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions."
1624,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multilinear Multitask Learning,"Bernardino Romera-Paredes, Hane Aung, Nadia Bianchi-Berthouze, Massimiliano Pontil",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/romera-paredes13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/romera-paredes13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_romera-paredes13,http://jmlr.csail.mit.edu/proceedings/papers/v28/romera-paredes13.html,"Many real world datasets occur or can be arranged into multi-modal structures. With such datasets, the tasks to be learnt can be referenced by multiple indices. Current multitask learning frameworks are not designed to account for the preservation of this information. We propose the use of multilinear algebra as a natural way to model such a set of related tasks. We present two learning methods; one is an adapted convex relaxation method used in the context of tensor completion. The second method is based on the Tucker decomposition and on alternating minimization. Experiments on synthetic and real data indicate that the multilinear approaches provide a significant improvement over other multitask learning methods. Overall our second approach yields the best performance in all datasets."
1625,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Bayesian Online Learning for Multi-label and Multi-variate Performance Measures,"Xinhua Zhang, Thore Graepel, Ralf Herbrich","9:956-963, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10b/zhang10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhang10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10b.html,Many real world applications employ multi-variate performance measures and each example can belong to multiple classes. The currently most popular approaches train an SVM for each class followed by ad hoc thresholding. Probabilistic models using Bayesian decision theory are also commonly adopted. In this paper we propose a Bayesian online multi-label classification framework (BOMC) which learns a probabilistic linear classifier. The likelihood is modeled by a graphical model similar to TrueSkill^{TM} and inference is based on Gaussian density filtering with expectation propagation. Using samples from the posterior we label the testing data by maximizing the expected $F_1$-score. Our experiments on Reuters1-v2 dataset show BOMC compares favorably to the state-of-the-art online learners in macro-averaged $F_1$-score and training time.
1626,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Continuous-Time Markov Chain Estimation,"Monir Hajiaghayi, Bonnie Kirkpatrick, Liangliang Wang, Alexandre Bouchard-C»t_",none,http://jmlr.org/proceedings/papers/v32/hajiaghayi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hajiaghayi14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hajiaghayi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hajiaghayi14.html,"Many problems of practical interest rely on Continuous-time Markov chains (CTMCs) defined over combinatorial state spaces, rendering the computation of transition probabilities, and hence probabilistic inference, difficult or impossible with existing methods. For problems with countably infinite states, where classical methods such as matrix exponentiation are not applicable, the main alternative has been particle Markov chain Monte Carlo methods imputing both the holding times and sequences of visited states. We propose a particle-based Monte Carlo approach where the holding times are marginalized analytically. We demonstrate that in a range of realistic inferential setups, our scheme dramatically reduces the variance of the Monte Carlo approximation and yields more accurate parameter posterior approximations given a fixed computational budget. These experiments are performed on both synthetic and real datasets, drawing from two important examples of CTMCs having combinatorial state spaces: string-valued mutation models in phylogenetics and nucleic acid folding pathways."
1627,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning Structured Models with the AUC Loss and Its Generalizations,"Nir Rosenfeld, Ofer Meshi, Danny Tarlow, Amir Globerson","JMLR W&CP 33 :841-849, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/rosenfeld14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/rosenfeld14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_rosenfeld14,http://jmlr.csail.mit.edu/proceedings/papers/v33/rosenfeld14.html,"Many problems involve the prediction of multiple, possibly dependent labels. The structured output prediction framework builds predictors that take these dependencies into account and use them to improve accuracy. In many such tasks, performance is evaluated by the Area Under the ROC Curve (AUC). While a framework for optimizing the AUC loss for unstructured models exists, it does not naturally extend to structured models. In this work, we propose a representation and learning formulation for optimizing structured models over the AUC loss, show how our approach generalizes the unstructured case, and provide algorithms for solving the resulting inference and learning problems. We also explore several new variants of the AUC measure which naturally arise from our formulation. Finally, we empirically show the utility of our approach in several domains."
1628,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Linearized Alternating Direction Method with Parallel Splitting and Adaptive Penalty for Separable Convex Programs in Machine Learning,"Risheng Liu, Zhouchen Lin, Zhixun Su","JMLR W&CP 29 :116-132, 2013",http://jmlr.org/proceedings/papers/v29/Liu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Liu13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Liu13.html,"Many problems in statistics and machine learning (e.g., probabilistic graphical model, feature extraction, clustering and classification, etc) can be (re)formulated as linearly constrained separable convex programs. The traditional alternating direction method (ADM) or its linearized version (LADM) is for the two-variable case and cannot be naively generalized to solve the multi-variable case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-variable separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, we devise a practical version of LADMPSAP for faster convergence. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the speed and accuracy advantages of LADMPSAP."
1629,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Deep Structured Models,"Liang-Chieh Chen, Alexander Schwing, Alan Yuille, Raquel Urtasun",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chenb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chenb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chenb15.html,"Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains."
1630,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Elliptical slice sampling,"Iain Murray, Ryan Adams, David MacKay","9:541-548, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/murray10a/murray10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_murray10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/murray10a.html,Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple generic code applicable to many models 2) it has no free parameters 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building removing the need to spend time deriving and tuning updates for more complex algorithms.
1631,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Policies for Contextual Submodular Prediction,"Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, Drew Bagnell",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ross13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ross13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ross13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/ross13b.html,"Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization."
1632,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,The Complexity of Learning Halfspaces using Generalized Linear Methods,"Amit Daniely, Nati Linial, Shai Shalev-Shwartz","JMLR W&CP 35 :244-286, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/daniely14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_daniely14a,http://jmlr.csail.mit.edu/proceedings/papers/v35/daniely14a.html,"Many popular learning algorithms (E.g. Regression, Fourier-Transform based algorithms, Kernel SVM and Kernel ridge regression) operate by reducing the problem to a convex optimization problem over a set of functions. These methods offer the currently best approach to several central problems such as learning half spaces and learning DNFês. In addition they are widely used in numerous application domains. Despite their importance, there are still very few proof techniques to show limits on the power of these algorithms. We study the performance of this approach in the problem of (agnostically and improperly) learning halfspaces with margin \(\gamma\) . Let \(D\) be a distribution over labeled examples. The \(\gamma\) -margin error of a hyperplane \(h\) is the probability of an example to fall on the wrong side of \(h\) or at a distance \(\le\gamma\) from it. The \(\gamma\) -margin error of the best \(h\) is denoted \(\mathrm{Err}_\gamma(D)\) . An \(\alpha(\gamma)\) -approximation algorithm receives \(\gamma,\epsilon\) as input and, using i.i.d. samples of \(D\) , outputs a classifier with error rate \(\le \alpha(\gamma)\mathrm{Err}_\gamma(D) + \epsilon\) . Such an algorithm is efficient if it uses \(\mathrm{poly}(\frac{1}{\gamma},\frac{1}{\epsilon})\) samples and runs in time polynomial in the sample size. The best approximation ratio achievable by an efficient algorithm is \(O\left(\frac{1/\gamma}{\sqrt{\log(1/\gamma)}}\right)\) and is achieved using an algorithm from the above class. Our main result shows that the approximation ratio of every efficient algorithm from this family must be \(\ge \Omega\left(\frac{1/\gamma}{\mathrm{poly}\left(\log\left(1/\gamma\right)\right)}\right)\) , essentially matching the best known upper bound."
1633,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery,"Yudong Chen, Constantine Caramanis",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13d-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13d,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13d.html,"Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known \(\ell^2\) -recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise."
1634,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains,"Katharina Blechschmidt, Joachim Giesen, Soeren Laue",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/blechschmidt15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_blechschmidt15,http://jmlr.csail.mit.edu/proceedings/papers/v37/blechschmidt15.html,"Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned carefully since the choice of their values can have a significant impact on the statistical performance of the learning methods. In most cases the parameter space does not carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one parameter received quite some attention over the years. First, algorithms for tracking an optimal solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since these algorithms can suffer from numerical problems more robust and efficient approximate path tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization-and kernel hyperparameter paths with optimal path complexities that depend only on the prescribed approximation error. Here we extend the work on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the complexity of approximately exploring a multi-dimensional parameter domain that is the product of the corresponding path complexities. We also show a matching upper bound that can be turned into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm the theoretical complexity analysis."
1635,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Distributed Representations of Sentences and Documents,"Quoc Le, Tomas Mikolov",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/le14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_le14,http://jmlr.csail.mit.edu/proceedings/papers/v32/le14.html,"Many machine learning algorithms require the input to be represented as a fixed length feature vector. When it comes to texts, one of the most common representations is bag-of-words. Despite their popularity, bag-of-words models have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, –powerful,” –strong” and –Paris” are equally distant. In this paper, we propose an unsupervised algorithm that learns vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
1636,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Robust sketching for multiple square-root LASSO problems,"Vu Pham, Laurent El Ghaoui",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/pham15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_pham15,http://jmlr.csail.mit.edu/proceedings/papers/v38/pham15.html,"Many learning tasks, such as cross-validation, parameter search, or leave-one-out analysis, involve multiple instances of similar problems, each instance sharing a large part of learning data with the others. We introduce a robust framework for solving multiple square-root LASSO problems, based on a sketch of the learning data that uses low-rank approximations. Our approach allows a dramatic reduction in computational effort, in effect reducing the number of observations from \(m\) (the number of observations to start with) to \(k\) (the number of singular values retained in the low-rank model), while not sacrificingãsometimes even improvingãthe statistical performance. Theoretical analysis, as well as numerical experiments on both synthetic and real data, illustrate the efficiency of the method in large scale applications."
1637,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification,"Junfeng Wen, Chun-Nam Yu, Russell Greiner",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wen14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wen14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wen14.html,"Many learning situations involve learning the conditional distribution \(p(y|x)\) when the training instances are drawn from the training distribution \(p_{tr}(x)\) , even though it will later be used to predict for instances drawn from a different test distribution \(p_{te}(x)\) . Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution. However, reweighing does not always help, because (we show that) the test error also depends on the correctness of the underlying model class. This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary. We characterize when such reweighing is needed, and also provide an algorithm, robust covariate shift adjustment (RCSA), that provides relevant weights. Our empirical studies, on UCI datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies, and that our RCSA works effectively."
1638,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning to Disentangle Factors of Variation with Manifold Interaction,"Scott Reed, Kihyuk Sohn, Yuting Zhang, Honglak Lee",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/reed14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_reed14,http://jmlr.csail.mit.edu/proceedings/papers/v32/reed14.html,"Many latent factors of variation interact to generate sensory data; for example pose, morphology and expression in face images. We propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Most existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a factor of variation. Furthermore, we propose a manifold-based training strategy that allows effective disentangling, meaning that units in each group encode a distinct type of variation. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database, and we also demonstrate disentangled features learned on the CMU Multi-PIE dataset."
1639,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A New Frontier of Kernel Design for Structured Data,Kilho Shin,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/shin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_shin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/shin13.html,"Many kernels for discretely structured data in the literature are designed within the framework of the convolution kernel and its generalization, the mapping kernel. The two most important advantages to use this framework is an easy-to-check criteria of positive definiteness and efficient computation based on the dynamic programming methodology of the resulting kernels. On the other hand, the recent theory of partitionable kernels reveals that the known kernels only take advantage of a very small portion of the potential of the framework. In fact, we have good opportunities to find novel and important kernels in the unexplored area. In this paper, we shed light on a novel important class of kernels within the framework: We give a mathematical characterization of the class, show a parametric method to optimize kernels of the class to specific problems, based on this characterization, and present some experimental results, which show the new kernels are promising in both accuracy and efficiency."
1640,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Algorithms and Error Analysis for the Modified Nystrom Method,"Shusen Wang, Zhihua Zhang","JMLR W&CP 33 :996-1004, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14c-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wang14c,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14c.html,"Many kernel methods suffer from high time and space complexities and are thus prohibitive in big-data applications. To tackle the computational challenge, the Nystr_m method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nystr_m method speedups computation by constructing an approximation of the kernel matrix using only a few columns of the matrix. Recently, a variant of the Nystr_m method called the modified Nystr_m method has demonstrated significant improvement over the standard Nystr_m method in approximation accuracy, both theoretically and empirically. In this paper, we propose two algorithms that make the modified Nystr_m method practical. First, we devise a simple column selection algorithm with a provable error bound. Our algorithm is more efficient and easier to implement than and nearly as accurate as the state-of-the-art algorithm. Second, with the selected columns at hand, we propose an algorithm that computes the approximation in lower time complexity than the approach in the previous work. Furthermore, we prove that the modified Nystr_m method is exact under certain conditions, and we establish a lower error bound for the modified Nystr_m method."
1641,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Convex Perturbations for Scalable Semidefinite Programming,"Brian Kulis, Suvrit Sra, Inderjit Dhillon","5:296-303, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/kulis09a/kulis09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_kulis09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/kulis09a.html,Many important machine learning problems are modeled and solved via semidefinite programs; examples include metric learning nonlinear embedding and certain clustering problems. Often off-the-shelf software is invoked for the associated optimization which can be inappropriate due to excessive computational and storage requirements. In this paper we introduce the use of convex perturbations for solving semidefinite programs (SDPs) and for a specific perturbation we derive an algorithm that has several advantages over existing techniques: a) it is simple requiring only a few lines of Matlab b) it is a first-order method and thereby scalable and c) it can easily exploit the structure of a given SDP (e.g. when the constraint matrices are low-rank a situation common to several machine learning SDPs). A pleasant byproduct of our method is a fast kernelized version of the large-margin nearest neighbor metric learning algorithm. We demonstrate that our algorithm is effective in finding fast approximations to large-scale SDPs arising in some machine learning applications.
1642,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,"James Bergstra, Daniel Yamins, David Cox",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bergstra13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bergstra13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bergstra13.html,"Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a methodês full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures."
1643,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Sequential Learning of Classifiers for Structured Prediction Problems,"Dan Roth, Kevin Small, Ivan Titov","5:440-447, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/roth09a/roth09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_roth09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/roth09a.html,Many classification problems with structured outputs can be regarded as a set of interrelated sub-problems where constraints dictate valid variable assignments. The standard approaches to these problems include either independent learning of individual classifiers for each of the sub-problems or joint learning of the entire set of classifiers with the constraints enforced during learning. We propose an intermediate approach where we learn these classifiers in a sequence using previously learned classifiers to guide learning of the next classifier by enforcing constraints between their outputs. We provide a theoretical motivation to explain why this learning protocol is expected to outperform both alternatives when individual problems have different `complexity'. This analysis motivates an algorithm for choosing a preferred order of classifier learning. We evaluate our technique on artificial experiments and on the entity and relation identification problem where the proposed method outperforms both joint and independent learning.
1644,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Scalable and Robust Bayesian Inference via the Median Posterior,"Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, David Dunson",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/minsker14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/minsker14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_minsker14,http://jmlr.csail.mit.edu/proceedings/papers/v32/minsker14.html,"Many Bayesian learning methods for massive data benefit from working with small subsets of observations. In particular, significant progress has been made in scalable Bayesian learning via stochastic approximation. However, Bayesian learning methods in distributed computing environments are often problem- or distribution-specific and use ad hoc techniques. We propose a novel general approach to Bayesian inference that is scalable and robust to corruption in the data. Our technique is based on the idea of splitting the data into several non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the results. The main novelty is the proposed aggregation step which is based on finding the geometric median of posterior distributions. We present both theoretical and numerical results illustrating the advantages of our approach."
1645,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Active Learning for Interactive Visualization,"Tomoharu Iwata, Neil Houlsby, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/iwata13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_iwata13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/iwata13a.html,"Many automatic visualization methods have been proposed. However, a visualization that is automatically generated might be different to how a user wants to arrange the objects in visualization space. By allowing users to re-locate objects in the embedding space of the visualization, they can adjust the visualization to their preference. We propose an active learning framework for interactive visualization which selects objects for the user to re-locate so that they can obtain their desired visualization by re-locating as few as possible. The framework is based on an information theoretic criterion, which favors objects that reduce the uncertainty of the visualization. We present a concrete application of the proposed framework to the Laplacian eigenmap visualization method. We demonstrate experimentally that the proposed framework yields the desired visualization with fewer user interactions than existing methods."
1646,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Grammar Induction: Beyond Local Search,Jason Eisner,"21:112-113, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/eisner12a/eisner12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_eisner12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/eisner12a.html,Many approaches to probabilistic grammar induction operate by iteratively improving a single grammar beginning with an initial guess. These local search paradigms include (variational) EM MCMC and greedy model merging or splitting procedures. Unfortunately local search methods tend to get caught in local optima even with random restarts. Two approaches are outlined that try to avoid this problem. One uses branch-and-bound methods from mathematical programming to eliminate regions of parameter space that cannot contain the global optimum. The other is inspired by recent work on deep learning and uses spectral methods to build up featural representations of all substrings without premature commitment to which substrings are constituents.
1647,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Pushing the Limits of Affine Rank Minimization by Adapting Probabilistic PCA,"Bo Xin, David Wipf",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/xin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_xin15,http://jmlr.csail.mit.edu/proceedings/papers/v37/xin15.html,"Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark."
1648,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations,"Tam Le, Marco Cuturi",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/le15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/le15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_le15,http://jmlr.csail.mit.edu/proceedings/papers/v37/le15.html,"Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches."
1649,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,An Information Geometry of Statistical Manifold Learning,"Ke Sun, St_phane Marchand-Maillet",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/suna14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/suna14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_suna14,http://jmlr.csail.mit.edu/proceedings/papers/v32/suna14.html,"Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold learning theory."
1650,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Learning a Metric Space for Neighbourhood Topology Estimation: Application to Manifold Learning,"Karim Abou- Moustafa, Dale Schuurmans, Frank Ferrie","JMLR W&CP 29 :341-356, 2013",http://jmlr.org/proceedings/papers/v29/Moustafa13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Moustafa13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Moustafa13.html,"Manifold learning algorithms rely on a neighbourhood graph to provide an estimate of the dataês local topology. Unfortunately, current methods for estimating local topology assume local Euclidean geometry and locally uniform data density, which often leads to poor data embeddings. We address these shortcomings by proposing a framework that combines local learning with parametric density estimation for local topology estimation. Given a data set \(\mathcal{D} \subset \mathcal{X}\) , we first estimate a new metric space \((\mathbb{X},d_{\mathbb{X}})\) that characterizes the varying sample density of \(\mathcal{X}\) in \(\mathbb{X}\) , then use \((\mathbb{X},d_{\mathbb{X}})\) as a new (pilot) input space for the graph construction step of the manifold learning process. The proposed framework results in significantly improved embeddings, which we demonstrated objectively by assessing clustering accuracy."
1651,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models,"En-Hsu Yen, Xin Lin, Kai Zhong, Pradeep Ravikumar, Inderjit Dhillon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yen15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yen15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yen15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yen15.html,"MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for Bayesian Nonparametric models. However, the combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes formulation for DP and Hierarchical DP (HDP) mixture model. We show that an exemplar-based MAD-Bayes formulation can be relaxed to a convex structural-regularized program that, under cluster-separation conditions, shares the same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based objective."
1652,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Hybrid Discriminative-Generative Approach with Gaussian Processes,"Ricardo Andrade Pacheco, James Hensman, Max Zwiessele, Neil Lawrence","JMLR W&CP 33 :47-56, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/andradepacheco14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/andradepacheco14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_andradepacheco14,http://jmlr.csail.mit.edu/proceedings/papers/v33/andradepacheco14.html,"Machine learning practitioners are often faced with a choice between a discriminative and a generative approach to modelling. Here, we present a model based on a hybrid approach that breaks down some of the barriers between the discriminative and generative points of view, allowing continuous dimensionality reduction of hybrid discrete-continuous data, discriminative classification with missing inputs and manifold learning informed by class labels."
1653,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,On utility of gene set signatures in gene expression-based cancer class prediction,"Minca Mramor, Marko Toplak, Gregor Leban, Toma_ Curk, Janez Dem_ar, Bla_ Zupan","8:55-64, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/mramor10a/mramor10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_mramor10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/mramor10a.html,"Machine learning methods that can use additional knowledge in their inference process are central to the development of integrative bioinformatics. Inclusion of background knowledge improves robustness, predictive accuracy and interpretability. Recently, a set of such techniques has been proposed that use information on gene sets for supervised data mining of class-labeled microarray data sets. We here present a new gene set-based supervised learning approach named setsig and systematically investigate the predictive accuracy of this and other gene set approaches compared to the standard inference model where only gene expression information is used. Our results indicate that setsig outperforms other gene set approaches, but contrary to earlier reports, transformation of gene expression data to the space of gene set signatures does not result in increased accuracy of predictive models when compared to those trained directly from original (not transformed) data."
1654,47,http://jmlr.csail.mit.edu/proceedings/papers/v47/,Look before you leap: Some insights into learner evaluation with cross-validation,"Gitte Vanwinckelen, Hendrik Blockeel",none,http://jmlr.csail.mit.edu/proceedings/papers/v47/vanwinckelen14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v47/,,27th November 2015,41897,ECML/PKDD Workshop on Statistically Sound Data Mining 2014 Proceedings,Statistically Sound Data Mining,"Joensuu, Finland","Wilhelmiina HÕ_mÕ_lÕ_inen, FranÕ_ois Petitjean, Geoffrey, I. Webb",v47_vanwinckelen14a,http://jmlr.csail.mit.edu/proceedings/papers/v47/vanwinckelen14a.html,"Machine learning is largely an experimental science, of which the evaluation of predictive models is an important aspect. These days, cross-validation is the most widely used method for this task. There are, however, a number of important points that should be taken into account when using this methodology. First, one should clearly state what they are trying to estimate. Namely, a distinction should be made between the evaluation of a model learned on a single dataset, and that of a learner trained on a random sample from a given data population. Each of these two questions requires a different statistical approach and should not be confused with each other. While this has been noted before, the literature on this topic is generally not very accessible. This paper tries to give an understandable overview of the statistical aspects of these two evaluation tasks. We also pose that because of the often limited availability of data, and the difficulty of selecting an appropriate statistical test, it is in some cases perhaps better to abstain from statistical testing, and instead focus on an interpretation of the immediate results."
1655,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Mixture of Watson Distributions: A Generative Model for Hyperspherical Embeddings,"Avleen S. Bijral, Markus Breitenbach, Greg Grudic","2:35-42, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/bijral07a/bijral07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_bijral07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/bijral07a.html,Machine learning applications often involve data that can be analyzed as unit vectors on a d-dimensional hypersphere or equivalently are directional in nature. Spectral clustering techniques generate embeddings that constitute an example of directional data and can result in different shapes on a hypersphere (depending on the original structure). Other examples of directional data include text and some sub-domains of bioinformatics. The Watson distribution for directional data presents a tractable form and has more modeling capability than the simple von Mises-Fisher distribution. In this paper we present a generative model of mixtures of Watson distributions on a hypersphere and derive numerical approximations of the parameters in an Expectation Maximization (EM) setting. This model also allows us to present an explanation for choosing the right embedding dimension for spectral clustering. We analyze the algorithm on a generated example and demonstrate its superiority over the existing algorithms through results on real datasets.
1656,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Data-Guided Approach for Learning and Improving User Experience in Computer Networks,"Yanan Bao, Xin Liu, Amit Pande",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Bao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Bao15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Bao15.html,"Machine learning algorithms have been traditionally used to understand user behavior or system performance. In computer networks, with a subset of input features as controllable network parameters, we envision developing a data-driven network resource allocation framework that can optimize user experience. In particular, we explore how to leverage a classifier learned from training instances to optimally guide network resource allocation to improve the overall performance on test instances. Based on logistic regression, we propose an optimal resource allocation algorithm, as well as heuristics with low-complexity. We evaluate the performance of the proposed algorithms using a synthetic Gaussian dataset, a real world dataset on video streaming over throttled networks, and a tier-one cellular operatorês customer complaint traces. The evaluation demonstrates the effectiveness of the proposed algorithms; e.g., the optimal algorithm can have a 400% improvement compared with the baseline."
1657,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams,"Rose Yu, Dehua Cheng, Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yua15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yua15.html,"Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time."
1658,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Low Rank Approximation using Error Correcting Coding Matrices,"Shashanka Ubaru, Arya Mazumdar, Yousef Saad",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ubaru15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ubaru15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ubaru15.html,"Low-rank matrix approximation is an integral component of tools such as principal component analysis (PCA), as well as is an important instrument used in applications like web search models, text mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms or Hadamard matrices, which require sampling \(O(k\log k)\) columns for a rank- \(k\) approximation, the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian random matrices and other structured random matrices."
1659,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Non-Linear Stationary Subspace Analysis with Application to Video Classification,"Mahsa Baktashmotlagh, Mehrtash Harandi, Abbas Bigdeli, Brian Lovell, Mathieu Salzmann",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/baktashmotlagh13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/baktashmotlagh13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_baktashmotlagh13,http://jmlr.csail.mit.edu/proceedings/papers/v28/baktashmotlagh13.html,"Low-dimensional representations are key to the success of many video classification algorithms. However, the commonly-used dimensionality reduction techniques fail to account for the fact that only part of the signal is shared across all the videos in one class. As a consequence, the resulting representations contain instance-specific information, which introduces noise in the classification process. In this paper, we introduce Non-Linear Stationary Subspace Analysis: A method that overcomes this issue by explicitly separating the stationary parts of the video signal (i.e., the parts shared across all videos in one class), from its non-stationary parts (i.e., specific to individual videos). We demonstrate the effectiveness of our approach on action recognition, dynamic texture classification and scene recognition."
1660,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Riemannian Pursuit for Big Matrix Recovery,"Mingkui Tan, Ivor W. Tsang, Li Wang, Bart Vandereycken, Sinno Jialin Pan",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/tan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/tan14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_tan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/tan14.html,"Low rank matrix recovery is a fundamental task in many real-world applications. The performance of existing methods, however, deteriorates significantly when applied to ill-conditioned or large-scale matrices. In this paper, we therefore propose an efficient method, called Riemannian Pursuit (RP), that aims to address these two problems simultaneously. Our method consists of a sequence of fixed-rank optimization problems. Each subproblem, solved by a nonlinear Riemannian conjugate gradient method, aims to correct the solution in the most important subspace of increasing size. Theoretically, RP converges linearly under mild conditions and experimental results show that it substantially outperforms existing methods when applied to large-scale and ill-conditioned matrices."
1661,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Rank-One Matrix Pursuit for Matrix Completion,"Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wanga14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wanga14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wanga14.html,"Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large scale datasets. Results show that our algorithm is much more efficient than state-of-the-art matrix completion algorithms while achieving similar or better prediction performance."
1662,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Linear Dynamical System Model for Text,"David Belanger, Sham Kakade",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/belanger15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/belanger15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_belanger15,http://jmlr.csail.mit.edu/proceedings/papers/v37/belanger15.html,"Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore wordsê local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where wordsê representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple co-occurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity."
1663,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Distance Preserving Embeddings for General n-Dimensional Manifolds,Nakul Verma,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/verma12/verma12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_verma12,http://jmlr.csail.mit.edu/proceedings/papers/v23/verma12.html,"Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that, given access to just the samples, embed the underlying n - dimensional manifold into R d (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) and guarantee to approximately preserve all interpoint geodesic distances."
1664,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,The Geometry of Losses,Robert C. Williamson,"JMLR W&CP 35 :1078-1108, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/williamson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_williamson14,http://jmlr.csail.mit.edu/proceedings/papers/v35/williamson14.html,"Loss functions are central to machine learning because they are the means by which the quality of a prediction is evaluated. Any loss that is not proper, or can not be transformed to be proper via a link function is inadmissible. All admissible losses for \(n\) -class problems can be obtained in terms of a convex body in \(\mathbb{R}^n\) . We show this explicitly and show how some existing results simplify when viewed from this perspective. This allows the development of a rich algebra of losses induced by binary operations on convex bodies (that return a convex body). Furthermore it allows us to define an –inverse loss” which provides a universal –substitution function” for the Aggregating Algorithm. In doing so we show a formal connection between proper losses and norms."
1665,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"A Convergence Rate Analysis for LogitBoost, MART and Their Variant","Peng Sun, Tong Zhang, Jie Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunc14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sunc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sunc14.html,"LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has \(O(\frac{1}{T})\) rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis."
1666,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,An EM Algorithm on BDDs with Order Encoding for Logic-based Probabilistic Models,"Masakazu Ishihata, Yoshitaka Kameya, Taisuke Sato, and Shin-ichi Minato","13:161-176, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/ishihata10a/ishihata10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_ishihata10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/ishihata10a.html,Logic-based probabilistic models (LBPMs) enable us to handle various problems in the real world thanks to the expressive power of logic. However most of LBPMs have restrictions to realize efficient probability computation and learning. We propose an EM algorithm working on BDDs with order encoding for LBPMs. A notable advantage of our algorithm over existing approaches is that it copes with multi-valued random variables without restrictions. The complexity of our algorithm is proportional to the size of the BDD. In the case of hidden Markov models (HMMs) the complexity is the same as that specialized for HMMs. As an example to eliminate restrictions of existing approaches we utilize our algorithm to give diagnoses for failure in a logic circuit involving stochastic error gates.
1667,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Large-scale log-determinant computation through stochastic Chebyshev expansions,"Insu Han, Dmitry Malioutov, Jinwoo Shin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hana15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hana15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hana15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hana15.html,"Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables."
1668,1,http://jmlr.csail.mit.edu/proceedings/papers/v1/,Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology,"Jarno Vanhatalo, Aki Vehtari","1:73-89, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v1/vanhatalo07a/vanhatalo07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v1/,,11th March 2007,"June 12-13, 2006",Gaussian Processes in Practice,Gaussian Processes in Practice,"Bletchley Park, Bletchley, U.K.","Neil Lawrence, Anton Schwaighofer and Joaquin QuiÕ±onero Candela",v1_vanhatalo07a,http://jmlr.csail.mit.edu/proceedings/papers/v1/vanhatalo07a.html,Log Gaussian processes are an attractive manner to construct intensity surfaces for the purposes of spatial epidemiology. The intensity surfaces are naturally smoothed by placing a Gaussian process (GP) prior over the relative log Poisson rate and the spatial correlations between areas can be included in an explicit and natural way into the model via a correlation function. The drawback with using a Gaussian process is the computational burden of the covariance matrix calculations. To overcome the computational limitations a number of approximations for Gaussian process have been suggested in the literature. In this work a fully independent training conditional sparse approximation is used to speed up the computations. The posterior inference is conducted using Markov chain Monte Carlo simulations and the sampling of the latent values is sped up by a transformation taking into account their posterior covariance. The sparse approximation is compared to a full GP with two sets of mortality data.
1669,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Locality Preserving Feature Learning,"Quanquan Gu, Marina Danilevsky, Zhenhui Li, Jiawei Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/gu12/gu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_gu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/gu12.html,Locality Preserving Indexing (LPI) has been quite successful in tackling document analysis problems such as clustering or classification. The approach relies on the Locality Preserving Criterion which preserves the locality of the data points. However LPI takes every word in a data corpus into account even though many words may not be useful for document clustering. To overcome this problem we propose an approach called Locality Preserving Feature Learning (LPFL) which incorporates feature selection into LPI. Specifically we aim to find a subset of features and learn a linear transformation to optimize the Locality Preserving Criterion based on these features. The resulting optimization problem is a mixed integer programming problem which we relax into a constrained Frobenius norm minimization problem and solve using a variation of Alternating Direction Method (ADM). ADM which iteratively updates the linear transformation matrix the residue matrix and the Lagrangian multiplier is theoretically guaranteed to converge at the rate O(1/t). Experiments on benchmark document datasets show that our proposed method outperforms LPI as well as other state-of-the-art document analysis approaches.
1670,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Community Detection Using Time-Dependent Personalized PageRank,"Haim Avron, Lior Horesh",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/avron15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/avron15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_avron15,http://jmlr.csail.mit.edu/proceedings/papers/v37/avron15.html,"Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion - the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions."
1671,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Understanding and Evaluating Sparse Linear Discriminant Analysis,"Yi Wu, David Wipf, Jeong-Min Yun",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/wu15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wu15,http://jmlr.csail.mit.edu/proceedings/papers/v38/wu15.html,"Linear discriminant analysis (LDA) represents a simple yet powerful technique for partitioning a p-dimensional feature vector into one of K classes based on a linear projection learned from N labeled observations. However, it is well-established that in the high-dimensional setting (p _ N) the underlying projection estimator degenerates. Moreover, any linear discriminate function involving a large number of features may be difficult to interpret. To ameliorate these issues, two general categories of sparse LDA modifications have been proposed, both to reduce the number of active features and to stabilize the resulting projections. The first, based on optimal scoring, is more straightforward to implement and analyze but has been heavily criticized for its ambiguous connection with the original LDA formulation. In contrast, a second strategy applies sparse penalty functions directly to the original LDA objective but requires additional heuristic trade-off parameters, has unknown global and local minima properties, and requires a greedy sequential optimization procedure. In all cases the choice of sparse regularizer can be important, but no rigorous guidelines have been provided regarding which penalty might be preferable. Against this backdrop, we winnow down the broad space of candidate sparse LDA algorithms and promote a specific selection based on optimal scoring coupled with a particular, complementary sparse regularizer. This overall process ultimately progresses our understanding of sparse LDA in general, while leading to targeted modifications of existing algorithms that produce superior results in practice on three high-dimensional gene data sets."
1672,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Efficient Distributed Linear Classification Algorithms via the Alternating Direction Method of Multipliers,"Caoxie Zhang, Honglak Lee, Kang Shin",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12a/zhang12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhang12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12a.html,Linear classification has demonstrated success in many areas of applications. Modern algorithms for linear classification can train reasonably good models while going through the data in only tens of rounds. However large data often does not fit in the memory of a single machine which makes the bottleneck in large-scale learning the disk I/O not the CPU. Following this observation Yu et al. (2010) made significant progress in reducing disk usage and their algorithms now outperform LIBLINEAR. In this paper rather than optimizing algorithms on a single machine we propose and implement distributed algorithms that achieve parallel disk loading and access the disk only once. Our large-scale learning algorithms are based on the framework of alternating direction methods of multipliers. The framework derives a subproblem that remains to be solved efficiently for which we propose using dual coordinate descent. Our experimental evaluations on large datasets demonstrate that the proposed algorithms achieve significant speedup over the classifier proposed by Yu et al. running on a single machine. Our algorithms are faster than existing distributed solvers such as Zinkevich et al. (2010)'s parallel stochastic gradient descent and Vowpal Wabbit.
1673,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification,"Ofir Pele, Ben Taskar, Amir Globerson, Michael Werman",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/pele13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_pele13,http://jmlr.csail.mit.edu/proceedings/papers/v28/pele13.html,"Linear classiffers are much faster to learn and test than non-linear ones. On the other hand, non-linear kernels offer improved performance, albeit at the increased cost of training kernel classiffers. To use non-linear mappings with efficient linear learning algorithms, explicit embeddings that approximate popular kernels have recently been proposed. However, the embedding process itself is often costly and the results are usually less accurate than kernel methods. In this work we propose a non-linear feature map that is both very efficient, but at the same time highly expressive. The method is based on discretization and interpolation of individual features values and feature pairs. The discretization allows us to model different regions of the feature space separately, while the interpolation preserves the original continuous values. Using this embedding is strictly more general than a linear model and as efficient as the second-order polynomial explicit feature map. An extensive empirical evaluation shows that our method consistently signiffcantly outperforms other methods, including a wide range of kernels. This is in contrast to other proposed embeddings that were faster than kernel methods, but with lower accuracy."
1674,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Completeness Results for Lifted Variable Elimination,"Nima Taghipour, Daan Fierens, Guy Van den Broeck, Jesse Davis, Hendrik Blockeel",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/taghipour13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/taghipour13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_taghipour13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/taghipour13a.html,"Lifting aims at improving the efficiency of probabilistic inference by exploiting symmetries in the model. Various methods for lifted probabilistic inference have been proposed, but our understanding of these methods and the relationships between them is still limited, compared to their propositional counterparts. The only existing theoretical characterization of lifting is a completeness result for weighted first-order model counting. This paper addresses the question whether the same completeness result holds for other lifted inference algorithms. We answer this question positively for lifted variable elimination (LVE). Our proof relies on introducing a novel inference operator for LVE."
1675,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Lifted Variable Elimination with Arbitrary Constraints,"Nima Taghipour, Daan Fierens, Jesse Davis, Hendrik Blockeel",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/taghipour12/taghipour12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_taghipour12,http://jmlr.csail.mit.edu/proceedings/papers/v22/taghipour12.html,Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically they identify groups of interchangeable variables and perform inference once for each group as opposed to once for each variable. The groups are defined by means of constraints so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference rely on (in)equality constraints. We show how inference methods can be generalized to work with arbitrary constraints. This allows them to capture a broader range of symmetries leading to more opportunities for lifting. We empirically demonstrate that this improves inference efficiency with orders of magnitude allowing exact inference in cases where until now only approximate inference was feasible.
1676,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Lifted Linear Programming,"Martin Mladenov, Babak Ahmadi, Kristian Kersting",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/mladenov12/mladenov12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_mladenov12,http://jmlr.csail.mit.edu/proceedings/papers/v22/mladenov12.html,Lifted inference approaches have rendered large previously intractable probabilistic inference problems quickly solvable by handling whole sets of indistinguishable objects together. Triggered by this success we show that another important AI technique is liftable too namely linear programming. Intuitively given a linear program (LP) we employ a lifted variant of Gaussian belief propagation (GaBP) to solve the systems of linear equations arising when running an interior-point method to solve the LP. However this naive solution cannot make use of standard solvers for linear equations and is doomed to construct lifted networks in each iteration of the interior-point method again an operation that can itself be quite costly. To address both issues we show how to read off an equivalent LP from the lifted GaBP computations that can be solved using any off-the-shelf LP solver. We prove the correctness of this compilation approach including a lifted duality theorem and experimentally demonstrate that it can greatly reduce the cost of solving LPs.
1677,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret,"Haitham Bou Ammar, Rasul Tutunov, Eric Eaton",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ammar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ammar15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ammar15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ammar15.html,"Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control."
1678,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Risk Bounds for Levy Processes in the PAC-Learning Framework,"Chao Zhang, Dacheng Tao","9:948-955, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10a/zhang10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_zhang10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/zhang10a.html,Levy processes play an important role in the stochastic process theory. However since samples are non-i.i.d. statistical learning results based on the i.i.d. scenarios cannot be utilized to study the risk bounds for Levy processes. In this paper we present risk bounds for non-i.i.d. samples drawn from Levy processes in the PAC-learning framework. In particular by using a concentration inequality for infinitely divisible distributions we first prove that the function of risk error is Lipschitz continuous with a high probability and then by using a specific concentration inequality for Levy processes we obtain the risk bounds for non-i.i.d. samples drawn from Levy processes without Gaussian components. Based on the resulted risk bounds we analyze the factors that affect the convergence of the risk bounds and then prove the convergence.
1679,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling,Michael Betancourt,none,http://jmlr.csail.mit.edu/proceedings/papers/v37/betancourt15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_betancourt15,http://jmlr.csail.mit.edu/proceedings/papers/v37/betancourt15.html,"Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo."
1680,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Domain Adaptation under Target and Conditional Shift,"Kun Zhang, Bernhard Schlkopf, Krikamol Muandet, Zhikun Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13d-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13d,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13d.html,"Let \(X\) denote the feature and \(Y\) the target. We consider domain adaptation under three possible scenarios: (1) the marginal \(P_Y\) changes, while the conditional \(P_{X|Y}\) stays the same ( target shift ), (2) the marginal \(P_Y\) is fixed, while the conditional \(P_{X|Y}\) changes with certain constraints ( conditional shift ), and (3) the marginal \(P_{Y}\) changes, and the conditional \(P_{X|Y}\) changes with constraints ( generalized target shift ). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework."
1681,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Random Projections for Support Vector Machines,"Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/paul13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_paul13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/paul13a.html,"Let \(X\) be a data matrix of rank \(\rho\) , representing \(n\) points in \(d\) -dimensional space. The linear support vector machine constructs a hyperplane separator that maximizes the 1-norm soft margin. We develop a new oblivious dimension reduction technique which is precomputed and can be applied to any input matrix \(X\) . We prove that, with high probability, the margin and minimum enclosing ball in the feature space are preserved to within \(\epsilon\) -relative error, ensuring comparable generalization as in the original space. We present extensive experiments with real and synthetic data to support our theory."
1682,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Submodular Losses with the Lovasz Hinge,"Jiaqian Yu, Matthew Blaschko",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yub15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yub15.html,"Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel convex surrogate loss function for submodular losses, the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through a real world image labeling task."
1683,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Yahoo! Learning to Rank Challenge Overview,O. Chapelle & Y. Chang,"14:1_24, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/chapelle11a/chapelle11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_chapelle11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/chapelle11a.html,Learning to rank for information retrieval has gained a lot of interest in the recent years but there is a lack for large real-world datasets to benchmark algorithms. That led us to publicly release two datasets used internally at Yahoo! for learning the web search ranking function. To promote these datasets and foster the development of state-of-the-art learning to rank algorithms we organized the Yahoo! Learning to Rank Challenge in spring 2010. This paper provides an overview and an analysis of this challenge along with a detailed description of the released datasets.   Page last modified on Wed Jan 26 10:36:40 2011.
1684,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,On learning to localize objects with minimal supervision,"Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/songb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_songb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/songb14.html,"Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection."
1685,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Structural Expectation Propagation (SEP): Bayesian structure learning for networks with latent variables,"Nevena Lazic, Christopher Bishop, John Winn",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/lazic13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/lazic13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_lazic13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/lazic13a.html,"Learning the structure of discrete Bayesian networks has been the subject of extensive research in machine learning, with most Bayesian approaches focusing on fully observed networks. One of few the methods that can handle networks with latent variables is the ""structural EM algorithm"" which interleaves greedy structure search with the estimation of latent variables and parameters, maintaining a single best network at each step. We introduce Structural Expectation Propagation (SEP), an extension of EP which can infer the structure of Bayesian networks having latent variables and missing data. SEP performs variational inference in a joint model of structure, latent variables, and parameters, offering two advantages: (i) it accounts for uncertainty in structure and parameter values when making local distribution updates (ii) it returns a variational distribution over network structures rather than a single network. We demonstrate the performance of SEP both on synthetic problems and on real-world clinical data."
1686,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Inferring Block Structure of Graphical Models in Exponential Families,"Siqi Sun, Hai Wang, Jinbo Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/sun15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/sun15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_sun15,http://jmlr.csail.mit.edu/proceedings/papers/v38/sun15.html,"Learning the structure of a graphical model is a fundamental problem and it is used extensively to infer the relationship between random variables. In many real world applications, we usually have some prior knowledge about the underlying graph structure, such as degree distribution and block structure. In this paper, we propose a novel generative model for describing the block structure in general exponential families, and optimize it by an Expectation-Maximization(EM) algorithm with variational Bayes. Experimental results show that our method performs well on both synthetic and real data. Further, our method can predict overlapped block structure of a graphical model in general exponential families."
1687,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Learning Sparse Markov Network Structure via Ensemble-of-Trees Models,"Yuanqing Lin, Shenghuo Zhu, Daniel Lee, Ben Taskar","5:360-367, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/lin09a/lin09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_lin09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/lin09a.html,Learning the sparse structure of a general Markov network is a hard problem. One of the main difficulties is the computation of its generally intractable partition function. To circumvent this difficulty this paper proposes to learn the network structure using an ensemble-of-trees (ET) model. The ET model was first introduced by Meila and Jaakkola [1] and it approximates a Markov network using a mixture of all possible (super-exponentially many) spanning trees. The advantage of the ET model is that although it needs to sum over super-exponentially many trees its partition function as well as data likelihood can be computed in a closed form. Furthermore since the ET model tends to represent a Markov network using as small number of trees as possible it provides a natural regularization for finding a sparse network structure. Our simulation results show that the proposed ET approach is able to accurately recover the true Markov network connectivity and significantly outperform the state-of-art approaches for both discrete and continuous random variable networks. Furthermore we also demonstrate the usage of the ET model for discovering the network of words from blog posts.
1688,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,A Machine Learning Framework for Programming by Example,"Aditya Menon, Omer Tamuz, Sumit Gulwani, Butler Lampson, Adam Kalai",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_menon13,http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13.html,"Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums."
1689,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning,"François Denis, Mattias Gybels, Amaury Habrard",none,http://jmlr.org/proceedings/papers/v32/denis14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_denis14,http://jmlr.csail.mit.edu/proceedings/papers/v32/denis14.html,"Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix \(H_S\) , called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in \(H_S\) and on the distance between \(H_S\) and its mean \(H_r\) . Existing concentration bounds seem to indicate that the concentration over \(H_r\) gets looser with its size, suggesting to make a trade-off between the quantity of used information and the size of \(H_r\) . We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size."
1690,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Scale-Free Networks by Dynamic Node Specific Degree Prior,"Qingming Tang, Siqi Sun, Jinbo Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/tangb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/tangb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_tangb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/tangb15.html,"Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the \(l_1\) norm."
1691,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Is Feature Selection Secure against Training Data Poisoning?,"Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, Fabio Roli",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/xiao15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_xiao15,http://jmlr.csail.mit.edu/proceedings/papers/v37/xiao15.html,"Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures."
1692,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,On Anomaly Ranking and Excess-Mass Curves,"Nicolas Goix, Anne Sabourin, St_phan Cl_mençon",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/goix15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/goix15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_goix15,http://jmlr.csail.mit.edu/proceedings/papers/v38/goix15.html,"Learning how to rank multivariate unlabeled observations depending on their degree of abnormality/novelty is a crucial problem in a wide range of applications. In practice, it generally consists in building a real valued –scoring” function on the feature space so as to quantify to which extent observations should be considered as abnormal. In the 1-d situation, measurements are generally considered as ”abnormal” when they are remote from central measures such as the mean or the median. Anomaly detection then relies on tail analysis of the variable of interest. Extensions to the multivariate setting are far from straightforward and it is precisely the main purpose of this paper to introduce a novel and convenient (functional) criterion for measuring the performance of a scoring function regarding the anomaly ranking task, referred to as the Excess-Mass curve (EM-curve). In addition, an adaptive algorithm for building a scoring function based on unlabeled data with a nearly optimal EM is proposed and is analyzed from a statistical perspective."
1693,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Gaussian Process Classification and Active Learning with Multiple Annotators,"Filipe Rodrigues, Francisco Pereira, Bernardete Ribeiro",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/rodrigues14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rodrigues14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rodrigues14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rodrigues14.html,"Learning from multiple annotators took a valuable step towards modelling data that does not fit the usual single annotator setting. However, multiple annotators sometimes offer varying degrees of expertise. When disagreements arise, the establishment of the correct label through trivial solutions such as majority voting may not be adequate, since without considering heterogeneity in the annotators, we risk generating a flawed model. In this paper, we extend GP classification in order to account for multiple annotators with different levels expertise. By explicitly handling uncertainty, Gaussian processes (GPs) provide a natural framework to build proper multiple-annotator models. We empirically show that our model significantly outperforms other commonly used approaches, such as majority voting, without a significant increase in the computational cost of approximate Bayesian inference. Furthermore, an active learning methodology is proposed, which is able to reduce annotation cost even further."
1694,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Boqing Gong, Kristen Grauman, Fei Sha",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gong13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13.html,"Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly."
1695,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Generalized Aitchison Embeddings for Histograms,"Tam Le, Marco Cuturi","JMLR W&CP 29 :293-308, 2013",http://jmlr.org/proceedings/papers/v29/Le13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Le13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Le13.html,"Learning distances that are specifically designed to compare histograms in the probability simplex has recently attracted the attention of the community. Learning such distances is important because most machine learning problems involve bags of features rather than simple vectors. Ample empirical evidence suggests that the Euclidean distance in general and Mahalanobis metric learning in particular may not be suitable to quantify distances between points in the simplex. We propose in this paper a new contribution to address this problem by generalizing a family of embeddings proposed by Aitchison (1982) to map the probability simplex onto a suitable Euclidean space. We provide algorithms to estimate the parameters of such maps, and show that these algorithms lead to representations that outperform alternative approaches to compare histograms."
1696,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,"Learning Sparse Metrics, One Feature at a Time","Yuval, Atzmon, Uri Shalit, Gal Chechik",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/atzmon2015.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_atzmon2015,http://jmlr.csail.mit.edu/proceedings/papers/v44/atzmon2015.html,"Learning distance metrics from data is a fundamental problem in machine learning and useful way to extract data-driven features by using the matrix root of a distance matrix. Finding a proper metric amounts to optimization over the cone of positive definite (PD) matrices. This optimization is difficult since restricting optimization to remain within the PD cone or repeatedly projecting to the cone is prohibitively costly. Here we describe COMET, a block-coordinate descent procedure, which efficiently keeps the search within the PD cone, avoiding both costly projections and unnecessary computation of full gradients. COMET also continuously maintains the Cholesky root of the matrix, providing feature extraction and embedding of samples in a metric space. We further develop a structurally sparse variant of COMET, where only a small number of features interacts with other features. Sparse-COMET significantly accelerates both training and inference while improving interpretability. As a block-coordinate descent procedure, COMET has fast convergence bounds showing linear convergence with high probability. When tested on benchmark datasets in a task of retrieving similar images and similar text documents, COMET has significantly better precision than competing projection-free methods. Furthermore, sparse-COMET achieves almost identical precision as dense-COMET in document classification, while running 4.5 times faster, maintaining a 0.5% sparsity level, and outperforming competing methods both in precision and in run time."
1697,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Rounding Methods for Discrete Linear Classification,"Yann Chevaleyre, Fr_d_erick Koriche, Jean-daniel Zucker",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chevaleyre13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chevaleyre13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chevaleyre13,http://jmlr.csail.mit.edu/proceedings/papers/v28/chevaleyre13.html,"Learning discrete linear functions is a notoriously difficult challenge. In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space, the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set. Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem. Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods. These methods are compared on both synthetic and real-world data."
1698,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Learning of Non-Parametric Control Policies with High-Dimensional State Features,"Herke Van Hoof, Jan Peters, Gerhard Neumann",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/vanhoof15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/vanhoof15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_vanhoof15,http://jmlr.csail.mit.edu/proceedings/papers/v38/vanhoof15.html,"Learning complex control policies from high-dimensional sensory input is a challenge for reinforcement learning algorithms. Kernel methods that approximate values functions or transition models can address this problem. Yet, many current approaches rely on instable greedy maximization. In this paper, we develop a policy search algorithm that integrates robust policy updates and kernel embeddings. Our method can learn non-parametric control policies for infinite horizon continuous MDPs with high-dimensional sensory representations. We show that our method outperforms related approaches, and that our algorithm can learn an underpowered swing-up task task directly from high-dimensional image data."
1699,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Scalable Collaborative Bayesian Preference Learning,"Mohammad Emtiyaz Khan, Young Jun Ko, Matthias Seeger","JMLR W&CP 33 :475-483, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/khan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/khan14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_khan14,http://jmlr.csail.mit.edu/proceedings/papers/v33/khan14.html,"Learning about usersê utilities from preference, discrete choice or implicit feedback data is of integral importance in e-commerce, targeted advertising and web search. Due to the sparsity and diffuse nature of data, Bayesian approaches hold much promise, yet most prior work does not scale up to realistic data sizes. We shed light on why inference for such settings is computationally difficult for standard machine learning methods, most of which focus on predicting explicit ratings only. To simplify the difficulty, we present a novel expectation maximization algorithm, driven by expectation propagation approximate inference, which scales to very large datasets without requiring strong factorization assumptions. Our utility model uses both latent bilinear collaborative filtering and non-parametric Gaussian process (GP) regression. In experiments on large real-world datasets, our method gives substantially better results than either matrix factorization or GPs in isolation, and converges significantly faster."
1700,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Single-Pass Algorithm for Efficiently Recovering Sparse Cluster Centers of High-dimensional Data,"Jinfeng Yi, Lijun Zhang, Jun Wang, Rong Jin, Anil Jain",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yib14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yib14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yib14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yib14.html,"Learning a statistical model for high-dimensional data is an important topic in machine learning. Although this problem has been well studied in the supervised setting, little is known about its unsupervised counterpart. In this work, we focus on the problem of clustering high-dimensional data with sparse centers. In particular, we address the following open question in unsupervised learning: ``is it possible to reliably cluster high-dimensional data when the number of samples is smaller than the data dimensionality?"" We develop an efficient clustering algorithm that is able to estimate sparse cluster centers with a single pass over the data. Our theoretical analysis shows that the proposed algorithm is able to accurately recover cluster centers with only \(O(s\log d)\) number of samples (data points), provided all the cluster centers are \(s\) -sparse vectors in a \(d\) dimensional space. Experimental results verify both the effectiveness and efficiency of the proposed clustering algorithm compared to the state-of-the-art algorithms on several benchmark datasets."
1701,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Two-Graph Guided Multi-task Lasso Approach for eQTL Mapping,"Xiaohui Chen, Xinghua Shi, Xing Xu, Zhiyong Wang, Ryan Mills, Charles Lee, Jinbo Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12b/chen12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_chen12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12b.html,Learning a small number of genetic variants associated with multiple complex genetic traits is of practical importance and remains challenging due to the high dimensional nature of data. In this paper we proposed a two-graph guided multi-task Lasso to address this issue with an emphasis on estimating subnetwork-to-subnetwork associations in expression quantitative trait loci (eQTL) mapping. The proposed model can learn such subnetwork-to-subnetwork associations and therefore can be seen as a generalization of several state-of-the-art multi-task feature selection methods. Additionally this model has a nice property of allowing flexible structured sparsity on both feature and label domains. Simulation study shows the improved performance of our model and a human eQTL data set is analyzed to further demonstrate the applications of the model.
1702,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Geodesic Distance Function Learning via Heat Flow on Vector Fields,"Binbin Lin, Ji Yang, Xiaofei He, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/linb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_linb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/linb14.html,"Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm."
1703,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,"LAMORE: A Stable, Scalable Approach to Latent Vector Autoregressive Modeling of Categorical Time Series","Yubin Park, Carlos Carvalho, Joydeep Ghosh","JMLR W&CP 33 :733-742, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/park14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_park14,http://jmlr.csail.mit.edu/proceedings/papers/v33/park14.html,"Latent vector autoregressive models for categorical time series have a wide range of potential applications from marketing research to healthcare analytics. However, a brute-force particle filter implementation of the Expectation-Maximization (EM) algorithm often fails to estimate the maximum likelihood parameters due to the Monte Carlo approximation of the E-step and multiple local optima of the log-likelihood function. This paper proposes two auxiliary techniques that help stabilize and calibrate the estimated parameters. These two techniques, namely asymptotic mean regularization and low-resolution augmentation , do not require any additional parameter tuning, and can be implemented by modifying the brute-force EM algorithm. Experiments with simulated data show that the proposed techniques effectively stabilize the parameter estimation process. Also, experimental results using Medicare and MIMIC-II datasets illustrate various potential applications of the proposed model and methods."
1704,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs,"Stephen Bach, Bert Huang, Jordan Boyd-Graber, Lise Getoor",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bach15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/bach15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bach15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bach15.html,"Latent variables allow probabilistic graphical models to capture nuance and structure in important domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensiveãbecause they require repeated inferences to update beliefs about latent variablesãso lifting this restriction for useful classes of models is an important problem. Hinge-loss Markov random fields (HL-MRFs) are graphical models that allow highly scalable inference and learning in structured domains, in part by representing structured problems with continuous variables. However, this representation leads to challenges when learning with latent variables. We introduce paired-dual learning, a framework that greatly speeds up training by using tractable entropy surrogates and avoiding repeated inferences. Paired-dual learning optimizes an objective with a pair of dual inference problems. This allows fast, joint optimization of parameters and dual variables. We evaluate on social-group detection, trust prediction in social networks, and image reconstruction, finding that paired-dual learning trains models as accurate as those trained by traditional methods in much less time, often before traditional methods make even a single parameter update."
1705,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Dependent Indian Buffet Processes,"Sinead Williamson, Peter Orbanz, Zoubin Ghahramani","9:924-931, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/williamson10a/williamson10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_williamson10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/williamson10a.html,Latent variable models represent hidden structure in observational data.To account for the distribution of the observational data changing over time space or some other covariate we need generalizations of latent variable models that explicitly capture this dependency on the covariate. A variety of such generalizations has been proposed for latent variable models based on the Dirichlet process. We address dependency on covariates in binary latent feature models by introducing a dependent Indian buffet process. The model generates for each value of the covariate a binary random matrix with an unbounded number of columns. Evolution of the binary matrices over the covariate set is controlled by a hierarchical Gaussian process model. The choice of covariance functions controls the dependence structure and exchangeability properties of the model. We derive a Markov Chain Monte Carlo sampling algorithm for Bayesian inference and provide experiments on both synthetic and real-world data. The experimental results show that explicit modeling of dependencies significantly improves accuracy of predictions.
1706,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models,"Mohammad Emtiyaz Khan, Aleksandr Aravkin, Michael Friedlander, Matthias Seeger",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/emtiyazkhan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_emtiyazkhan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/emtiyazkhan13.html,"Latent Gaussian models (LGMs) are widely used in statistics and machine learning. Bayesian inference in non-conjugate LGM is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods. Algorithms based on Variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use. However, the structure of optimization problems associated with them remains poorly understood, and standard solvers take too long to converge. In this paper, we derive a novel dual variational inference approach, which exploits the convexity property of the VG approximations. The implications of our approach is that we obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods. Using real world data, we demonstrate these advantages on a variety of LGMs including Gaussian process classification and latent Gaussian Markov random fields."
1707,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Kernel Topic Models,"Philipp Hennig, David Stern, Ralf Herbrich, Thore Graepel",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/hennig12/hennig12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_hennig12,http://jmlr.csail.mit.edu/proceedings/papers/v22/hennig12.html,Latent Dirichlet Allocation models discrete data as a mixture of discrete distributions using Dirichlet beliefs over the mixture weights. We study a variation of this concept in which the documents' mixture weight beliefs are replaced with squashed Gaussian distributions. This allows documents to be associated with elements of a Hilbert space admitting kernel topic models (KTM) modelling temporal spatial hierarchical social and other structure between documents. The main challenge is efficient approximate inference on the latent Gaussian. We present an approximate algorithm cast around a Laplace approximation in a transformed basis. The KTM can also be interpreted as a type of Gaussian process latent variable model or as a topic model conditional on document features uncovering links between earlier work in these areas.
1708,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Dual online inference for latent Dirichlet allocation,"Khoat Than, Tung Doan",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/than14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_than14,http://jmlr.csail.mit.edu/proceedings/papers/v39/than14.html,"Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worst case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is very general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and more interpretable topics, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents."
1709,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,The Security of Latent Dirichlet Allocation,"Shike Mei, Xiaojin Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/mei15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/mei15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_mei15,http://jmlr.csail.mit.edu/proceedings/papers/v38/mei15.html,"Latent Dirichlet allocation (LDA) is an increasingly popular tool for data analysis in many domains. If LDA output affects decision making (especially when money is involved), there is an incentive for attackers to compromise it. We ask the question: how can an attacker minimally poison the corpus so that LDA produces topics that the attacker wants the LDA user to see? Answering this question is important to characterize such attacks, and to develop defenses in the future. We give a novel bilevel optimization formulation to identify the optimal poisoning attack. We present an efficient solution (up to local optima) using descent method and implicit functions. We demonstrate poisoning attacks on LDA with extensive experiments, and discuss possible defenses."
1710,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks,"Jose Miguel Hernandez-Lobato, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hernandez-lobatoc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hernandez-lobatoc15.html,"Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights."
1711,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Robust Forward Algorithms via PAC-Bayes and Laplace Distributions,"Asaf Noy, Koby Crammer","JMLR W&CP 33 :678-686, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/noy14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/noy14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_noy14,http://jmlr.csail.mit.edu/proceedings/papers/v33/noy14.html,"Laplace random variables are commonly used to model extreme noise in many fields, while systems trained to deal with such noises are often characterized by robustness properties. We introduce new learning algorithms that minimize objectives derived directly from PAC-Bayes bounds, incorporating Laplace distributions. The resulting algorithms are regulated by the Huber loss function and are robust to noise, as the Laplace distribution integrated large deviation of parameters. We analyze the convexity properties of the objective, and propose a few bounds which are fully convex, two of which jointly convex in the mean and standard-deviation under certain conditions. We derive new forward algorithms analogous to recent boosting algorithms, providing novel relations between boosting and PAC-Bayes analysis. Experiments show that our algorithms outperforms AdaBoost, L1-LogBoost, and RobustBoost in a wide range of input noise."
1712,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Fast Variational Approach for Learning Markov Random Field Language Models,"Yacine Jernite, Alexander Rush, David Sontag",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/jernite15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/jernite15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_jernite15,http://jmlr.csail.mit.edu/proceedings/papers/v37/jernite15.html,"Language modelling is a fundamental building block of natural language processing. However, in practice the size of the vocabulary limits the distributions applicable for this task: specifically, one has to either resort to local optimization methods, such as those used in neural language models, or work with heavily constrained distributions. In this work, we take a step towards overcoming these difficulties. We present a method for global-likelihood optimization of a Markov random field language model exploiting long-range contexts in time independent of the corpus size. We take a variational approach to optimizing the likelihood and exploit underlying symmetries to greatly simplify learning. We demonstrate the efficiency of this method both for language modelling and for part-of-speech tagging."
1713,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Active Sensing,"Shipeng Yu, Balaji Krishnapuram, Romer Rosales, R. Bharat Rao","5:639-646, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/yu09a/yu09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_yu09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/yu09a.html,Labels are often expensive to get and this motivates \emph{active learning} which chooses the most informative samples for label acquisition. In this paper we study \emph{active sensing} in a multi-view setting motivated from many problems where grouped features are also expensive to obtain and need to be acquired (or \emph{sensed}) actively (e.g.~in cancer diagnosis each patient might go through many tests such as CT Ultrasound and MRI to get valuable features). The strength of this model is that one actively sensed (sample view) pair would improve the \emph{joint} multi-view classification on all the samples. For this purpose we extend the Bayesian co-training framework such that it can handle missing views in a principled way and introduce two criteria for view acquisition. Experiments on one toy data and two real-world medical problems show the effectiveness of this model.
1714,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Label Propagation,"Yasuhiro Fujiwara, Go Irie",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/fujiwara14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_fujiwara14,http://jmlr.csail.mit.edu/proceedings/papers/v32/fujiwara14.html,"Label propagation is a popular graph-based semi-supervised learning framework. So as to obtain the optimal labeling scores, the label propagation algorithm requires an inverse matrix which incurs the high computational cost of O(n 3 +cn 2 ), where n and c are the numbers of data points and labels, respectively. This paper proposes an efficient label propagation algorithm that guarantees exactly the same labeling results as those yielded by optimal labeling scores. The key to our approach is to iteratively compute lower and upper bounds of labeling scores to prune unnecessary score computations. This idea significantly reduces the computational cost to O(cnt) where t is the average number of iterations for each label and t __ n in practice. Experiments demonstrate the significant superiority of our algorithm over existing label propagation methods."
1715,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Low-Rank Spectral Learning with Weighted Loss Functions,"Alex Kulesza, Nan Jiang, Satinder Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kulesza15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kulesza15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kulesza15.html,"Kulesza et al. recently observed that low-rank spectral learning algorithms, which discard the smallest singular values of a moment matrix during training, can behave in unexpected ways, producing large errors even when the discarded singular values are arbitrarily small. In this paper we prove that when learning predictive state representations those problematic cases disappear if we introduce a particular weighted loss function and learn using sufficiently large sets of statistics; our main result is a bound on the loss of the learned low-rank model in terms of the singular values that are discarded. Practically speaking, this suggests that regardless of the model rank we should use the largest possible sets of statistics, and we show empirically that this is true on both synthetic and real-world domains."
1716,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Open Problem: Better Bounds for Online Logistic Regression,H. Brendan McMahan and Matthew Streeter,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/mcmahan12/mcmahan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_mcmahan12,http://jmlr.csail.mit.edu/proceedings/papers/v23/mcmahan12.html,"Known algorithms applied to online logistic regression on a feasible set of L 2 diameter D achieve regret bounds like O ( e D log T ) in one dimension, but we show a bound of O (í D + log T ) is possible in a binary 1-dimensional problem. Thus, we pose the following question: Is it possible to achieve a regret bound for online logistic regression that is O (poly( D ) log( T ))? Even if this is not possible in general, it would be interesting to have a bound that reduces to our bound in the one-dimensional case."
1717,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Multiclass Latent Locally Linear Support Vector Machines,"Marco Fornoni, Barbara Caputo, Francesco Orabona","JMLR W&CP 29 :229-244, 2013",http://jmlr.org/proceedings/papers/v29/Fornoni13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Fornoni13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Fornoni13.html,"Kernelized Support Vector Machines (SVM) have gained the status of off-the-shelf classifiers, able to deliver state of the art performance on almost any problem. Still, their practical use is constrained by their computational and memory complexity, which grows super-linearly with the number of training samples. In order to retain the low training and testing complexity of linear classifiers and the flexibility of non linear ones, a growing, promising alternative is represented by methods that learn non-linear classifiers through local combinations of linear ones. In this paper we propose a new multi class local classifier, based on a latent SVM formulation. The proposed classifier makes use of a set of linear models that are linearly combined using sample and class specific weights. Thanks to the latent formulation, the combination coefficients are modeled as latent variables. We allow soft combinations and we provide a closed-form solution for their estimation, resulting in an efficient prediction rule. This novel formulation allows to learn in a principled way the sample specific weights and the linear classifiers, in a unique optimization problem, using a CCCP optimization procedure. Extensive experiments on ten standard UCI machine learning datasets, one large binary dataset, three character and digit recognition databases, and a visual place categorization dataset show the power of the proposed approach."
1718,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Scaling up Kernel SVM on Limited Resources: A Low-rank Linearization Approach,"Kai Zhang, Liang Lan, Zhuang Wang, Fabian Moerchen",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12d/zhang12d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhang12d,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12d.html,Kernel Support Vector Machine delivers state-of-the-art results in non-linear classification but the need to maintain a large number of support vectors poses a challenge in large scale training and testing. In contrast linear SVM is much more scalable even on limited computing recourses (e.g. daily life PCs) but the learned model cannot capture non-linear concepts. To scale up kernel SVM on limited resources we propose a low-rank linearization approach that transforms a non-linear SVM to a linear one via a novel approximate empirical kernel map computed from efficient low-rank approximation of kernel matrices. We call it LLSVM (Low-rank Linearized SVM). We theoretically study the gap between the solutions of the optimal and approximate kernel map which is used in turn to provide important guidance on the sampling based kernel approximations. Our algorithm inherits high efficiency of linear SVMs and rich repesentability of kernel classifiers. Evaluation against large-scale linear and kernel SVMs on several truly large data sets shows that the proposed method achieves a better tradeoff between scalability and model representability.
1719,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,A Dimension-Independent Generalization Bound for Kernel Supervised Principal Component Analysis,"Hassan Ashtiani, Ali Ghodsi",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/Ashtiani2015.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_Ashtiani2015,http://jmlr.csail.mit.edu/proceedings/papers/v44/Ashtiani2015.html,"Kernel supervised principal component analysis (KSPCA) is a computationally efficient supervised feature extraction method that can learn non-linear transformations. We start the study of the statistical properties of KSPCA, providing the first bound on its sample complexity. This bound is dimension-independent, which justifies the good performance of KSPCA on high-dimensional data. Another observation is that in the kernelized version, the number of parameters of KSPCA grows linearly with the sample size. While this potentially increases the risk of over-fitting, KSPCA works well in practice. In this work, we justify this compelling characteristic of KSPCA by providing a guarantee indicating that KSPCA generalizes well even when the number of parameters is large, as long as they have small norms."
1720,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Metric Learning for Kernel Regression,"Kilian Q. Weinberger, Gerald Tesauro","2:612-619, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/weinberger07a/weinberger07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_weinberger07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/weinberger07a.html,Kernel regression is a well-established method for nonlinear regression in which the target value for a test point is estimated using a weighted average of the surrounding training samples. The weights are typically obtained by applying a distance-based kernel function to each of the samples which presumes the existence of a well-defined distance metric. In this paper we construct a novel algorithm for supervised metric learning which learns a distance function by directly minimizing the leave-one-out regression error. We show that our algorithm makes kernel regression comparable with the state of the art on several benchmark datasets and we provide efficient implementation details enabling application to datasets with ~O(10k) instances. Further we show that our algorithm can be viewed as a supervised variation of PCA and can be used for dimensionality reduction and high dimensional data visualization.
1721,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,The Kernel Path in Kernelized LASSO,"Gang Wang, Dit-Yan Yeung, Frederick H. Lochovsky","2:580-587, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07a/wang07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_wang07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07a.html,Kernel methods implicitly map data points from the input space to some feature space where even relatively simple algorithms such as linear methods can deliver very impressive performance. Of crucial importance though is the choice of the kernel function which determines the mapping between the input space and the feature space. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. In this paper we study the problem of learning the kernel hyperparameter in the context of the kernelized LASSO regression model. Specifically we propose a solution path algorithm with respect to the hyperparameter of the kernel function. As the kernel hyperparameter changes its value the solution path can be traced exactly without having to train the model multiple times. As a result the optimal solution can be identified efficiently. Some simulation results will be presented to demonstrate the effectiveness of our proposed kernel path algorithm.
1722,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A la Carte _ Learning Fast Kernels,"Zichao Yang, Andrew Wilson, Alex Smola, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15b-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_yang15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/yang15b.html,"Kernel methods have great promise for learning rich statistical representations of large modern datasets. However, compared to neural networks, kernel methods have been perceived as lacking in scalability and flexibility. We introduce a family of fast, flexible, general purpose, and lightly parametrized kernel learning methods, derived from Fastfood basis function expansions. We provide mechanisms to learn the properties of groups of spectral frequencies in these expansions, which require only O(m log d) time and O(m) memory, for m basis functions and d input dimensions. We show that the proposed methods can learn a wide class of kernels, outperforming the alternatives in accuracy, speed, and memory consumption."
1723,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Scalable Algorithm for Structured Kernel Feature Selection,"Shaogang Ren, Shuai Huang, John Onofrey, Xenios Papademetris, Xiaoning Qian",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/ren15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/ren15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_ren15,http://jmlr.csail.mit.edu/proceedings/papers/v38/ren15.html,"Kernel methods are powerful tools for nonlinear feature representation. Incorporated with structured LASSO, the kernelized structured LASSO is an effective feature selection approach that can preserve the nonlinear input-output relationships as well as the structured sparseness. But as the data dimension increases, the method can quickly become computationally prohibitive. In this paper we propose a stochastic optimization algorithm that can efficiently address this computational problem on account of the redundant kernel representations of the given data. Experiments on simulation data and PET 3D brain image data show that our method can achieve superior accuracy with less computational cost than existing methods."
1724,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Compact Random Feature Maps,"Raffay Hamid, Ying Xiao, Alex Gittens, Dennis Decoste",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hamid14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hamid14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hamid14.html,"Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results."
1725,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,On the Impact of Kernel Approximation on Learning Accuracy,"Corinna Cortes, Mehryar Mohri, Ameet Talwalkar","9:113-120, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/cortes10a/cortes10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_cortes10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/cortes10a.html,Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms including SVMs KRR and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However we also give a specific analysis of the Nystrom low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystrom low-rank kernel approximation when used with ridge regression.
1726,18,http://jmlr.csail.mit.edu/proceedings/papers/v18/,The Yahoo! Music Dataset and KDD-Cupê11,"G. Dror, N. Koenigstein, Y. Koren & M. Weimer","18:3_18, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v18/dror12a/dror12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v18/,,1st June 2012,40776,KDD Cup 2011,Recommending Music Items based on the Yahoo! Music Dataset,none,"Gideon Dror, Yehuda Koren, Markus Weimer",v18_dror12a,http://jmlr.csail.mit.edu/proceedings/papers/v18/dror12a.html,KDD-Cup 2011 challenged the community to identify user tastes in music by leveraging Yahoo! Music user ratings. The competition hosted two tracks which were based on two datasets sampled from the raw data including hundreds of millions of ratings. The underlying ratings were given to four types of musical items: tracks  albums  artists  and genres  forming a four level hierarchical taxonomy.
1727,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Near-Optimal Joint Object Matching via Convex Relaxation,"Yuxin Chen, Leonidas Guibas, Qixing Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chend14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chend14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chend14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chend14.html,"Joint object matching aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve the correspondences computed between pairs of objects, typically by exploiting global map compatibility. Despite some practical advances on this problem, from the theoretical point of view, the error-correction ability of existing algorithms are limited by a constant barrier ã none of them can provably recover the correct solution when more than a constant fraction of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding and realistic to match instances that are only partially similar to each other. In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, where the provided pairwise feature correspondences can be densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we attempt recovery via a two-step procedure, that is, a spectral technique followed by a parameter-free convex program called MatchLift. Under a natural randomized model, MatchLift exhibits near-optimal error-correction ability, i.e. it guarantees the recovery of the ground-truth maps even when a dominant fraction of the inputs are randomly corrupted. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of the proposed algorithm."
1728,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Symmetric Iterative Proportional Fitting,Sven Kurras,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kurras15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/kurras15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kurras15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kurras15.html,"Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=Wê and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning."
1729,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Loopy Belief Propagation in the Presence of Determinism,"David Smith, Vibhav Gogate","JMLR W&CP 33 :895-903, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/smith14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_smith14,http://jmlr.csail.mit.edu/proceedings/papers/v33/smith14.html,"It is well known that loopy Belief propagation (LBP) performs poorly on probabilistic graphical models (PGMs) with determinism. In this paper, we propose a new method for remedying this problem. The key idea in our method is finding a reparameterization of the graphical model such that LBP, when run on the reparameterization, is likely to have better convergence properties than LBP on the original graphical model. We propose several schemes for finding such reparameterizations, all of which leverage unique properties of zeros as well as research on LBP convergence done over the last decade. Our experimental evaluation on a variety of PGMs clearly demonstrates the promise of our method _ it often yields accuracy and convergence time improvements of an order of magnitude or more over LBP."
1730,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,A New Look at Nearest Neighbours: Identifying Benign Input Geometries via Random Projections,Ata Kaban,none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kaban15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Kaban15b,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kaban15b.html,"It is well known that in general, the nearest neighbour rule (NN) has sample complexity that is exponential in the input space dimension \(d\) when only smoothness is assumed on the label posterior function. Here we consider NN on randomly projected data, and we show that, if the input domain has a small –metric size”, then the sample complexity becomes exponential in the metric entropy integral of the set of normalised chords of the input domain. This metric entropy integral measures the complexity of the input domain, and can be much smaller than \(d\) _ for instance in cases when the data lies in a linear or a smooth nonlinear subspace of the ambient space, or when it has a sparse representation. We then show that the guarantees we obtain for the compressive NN also hold for the dataspace NN in bounded domains; thus the random projection takes the role of an analytic tool to identify benign structures under which NN learning is possible from a small sample size. Numerical simulations on data designed to have intrinsically low complexity confirm our theoretical findings, and display a striking agreement in the empirical performances of compressive NN and dataspace NN. This suggests that high dimensional data sets that have a low complexity underlying structure are well suited for computationally cheap compressive NN learning."
1731,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Minimax Algorithm for Learning Rotations,"Wojciech Kot_owski, Manfred K. Warmuth","19:823-826, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/kotlowski11b/kotlowski11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_kotlowski11b,http://jmlr.csail.mit.edu/proceedings/papers/v19/kotlowski11b.html,It is unknown what is the most suitable regularization for rotation matrices and how to maintain uncertainty overrotations in an online setting.We propose to address these questions by studying the minimax algorithm for rotations and begin by working out the 2-dimensional case.
1732,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Scoring anomalies: a M-estimation formulation,"St_phan Cl_mençon, J_r_mie Jakubowicz",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/clemencon13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_clemencon13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/clemencon13a.html,"It is the purpose of this paper to formulate the issue of scoring multivariate observations depending on their degree of abnormality/novelty as an unsupervised learning task. Whereas in the 1-d situation, this problem can be dealt with by means of tail estimation techniques, observations being viewed as all the more –abnormal” as they are located far in the tail(s) of the underlying probability distribution. In a wide variety of applications, it is desirable to dispose of a scalar valued –scoring” function allowing for comparing the degree of abnormality of multivariate observations. Here we formulate the issue of scoring anomalies as a M-estimation problem. A (functional) performance criterion is proposed, whose optimal elements are, as expected, nondecreasing transforms of the density. The question of empirical estimation of this criterion is tackled and preliminary statistical results related to the accuracy of partition-based techniques for optimizing empirical estimates of the empirical performance measure are established."
1733,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Unifying View of Representer Theorems,"Andreas Argyriou, Francesco Dinuzzo",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/argyriou14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/argyriou14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_argyriou14,http://jmlr.csail.mit.edu/proceedings/papers/v32/argyriou14.html,"It is known that the solution of regularization and interpolation problems with Hilbertian penalties can be expressed as a linear combination of the data. This very useful property, called the representer theorem, has been widely studied and applied to machine learning problems. Analogous optimality conditions have appeared in other contexts, notably in matrix regularization. In this paper we propose a unified view, which generalizes the concept of representer theorems and extends necessary and sufficient conditions for such theorems to hold. Our main result shows a close connection between representer theorems and certain classes of regularization penalties, which we call orthomonotone functions. This result not only subsumes previous representer theorems as special cases but also yields a new class of optimality conditions, which goes beyond the classical linear combination of the data. Moreover, orthomonotonicity provides a useful criterion for testing whether a representer theorem holds for a specific regularization problem."
1734,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multiple Testing under Dependence via Semiparametric Graphical Models,"Jie Liu, Chunming Zhang, Elizabeth Burnside, David Page",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/liue14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/liue14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liue14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liue14.html,"It has been shown that graphical models can be used to leverage the dependence in large-scale multiple testing problems with significantly improved performance (Sun & Cai, 2009; Liu et al., 2012). These graphical models are fully parametric and require that we know the parameterization of f1, the density function of the test statistic under the alternative hypothesis. However in practice, f1 is often heterogeneous, and cannot be estimated with a simple parametric distribution. We propose a novel semiparametric approach for multiple testing under dependence, which estimates f1 adaptively. This semiparametric approach exactly generalizes the local FDR procedure (Efron et al., 2001) and connects with the BH procedure (Benjamini & Hochberg, 1995). A variety of simulations show that our semiparametric approach outperforms classical procedures which assume independence and the parametric approaches which capture dependence."
1735,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Speeding Up Syntactic Learning Using Contextual Information,"Leonor Becerra Bonache, Elisa Fromont, Amaury Habrard, Michaïl Perrot and Marc Sebban","21:49-53, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/becerra12a/becerra12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_becerra12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/becerra12a.html,It has been shown in (Angluin and Becerra-Bonache 2010 2011) that interactions between a learner and a teacher can help language learning. In this paper we make use of additional contextual information in a pairwise-based generative approach aiming at learning (situationsentence)-pair-hidden markov models. We show that this allows a significant speed-up of the convergence of the syntactic learning. We apply our model on a toy natural language task in Spanish dealing with geometric objects.
1736,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Better Mixing via Deep Representations,"Yoshua Bengio, Gregoire Mesnil, Yann Dauphin, Salah Rifai",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bengio13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bengio13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bengio13.html,"It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples."
1737,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Efficient Representations for Lifelong Learning and Autoencoding,"Maria-Florina Balcan, Avrim Blum, Santosh Vempala",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Balcan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Balcan15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Balcan15.html,"It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, –learning to learn” as they do so. In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal. Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm. Our aim is to learn new internal representations as the algorithm learns new target functions, that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning nonlinear Boolean combinations of features. Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural –anchor-set” assumption."
1738,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Efficient Multioutput Gaussian Processes through Variational Inducing Kernels,"Mauricio çlvarez, David Luengo, Michalis Titsias, Neil Lawrence","9:25-32, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/alvarez10a/alvarez10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_alvarez10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/alvarez10a.html,Interest in multioutput kernel methods is increasing whether under the guise of multitask learning multisensor networks or structured output data. From the Gaussian process perspective a multioutput Mercer kernel is a covariance function over correlated output functions. One way to construct such kernels is based on convolution processes (CP). A key problem for this approach is efficient inference. Alvarez and Lawrence recently presented a sparse approximation for CPs that enabled efficient inference. In this paper we extend this work in two directions: we introduce the concept of variational inducing functions to handle potential non-smooth functions involved in the kernel CP construction and we consider an alternative approach to approximate inference based on variational methods extending the work by Titsias (2009) to the multiple output case. We demonstrate our approaches on prediction of school marks compiler performance and financial time series.
1739,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Composition Theorem for Differential Privacy,"Peter Kairouz, Sewoong Oh, Pramod Viswanath",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kairouz15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kairouz15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kairouz15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kairouz15.html,"Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature."
1740,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Distributed optimization of deeply nested systems,"Miguel Carreira-Perpinan, Weiran Wang","JMLR W&CP 33 :10-19, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/carreira-perpinan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/carreira-perpinan14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_carreira-perpinan14,http://jmlr.csail.mit.edu/proceedings/papers/v33/carreira-perpinan14.html,"Intelligent processing of complex signals such as images is often performed by a hierarchy of nonlinear processing layers, such as a deep net or an object recognition cascade. Joint estimation of the parameters of all the layers is a difficult nonconvex optimization. We describe a general strategy to learn the parameters and, to some extent, the architecture of nested systems, which we call the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, can perform some model selection on the fly, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations."
1741,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization,"Stefano Ermon, Carla Gomes, Ashish Sabharwal, Bart Selman",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ermon13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ermon13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ermon13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ermon13.html,"Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection."
1742,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Largest Source Subset Selection for Instance Transfer,"Shuang Zhou, Gijs Schoenmakers, Evgueni Smirnov, Ralf Peeters, Kurt Driessens, Siqi Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhou15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhou15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhou15.html,Instance-transfer learning has emerged as a promising learning framework to boost performance of prediction models on newly-arrived tasks. The success of the framework depends on the relevance of the source data to the target data. This paper proposes a new approach to source data selection for instance-transfer learning. The approach is capable of selecting the largest subset \(S^*\) of the source data which relevance to the target data is statistically guaranteed to be the highest among any superset of \(S^*\) . The approach is formally described and theoretically justified. Experimental results on real-world data sets demonstrate that the approach outperforms existing instance selection methods.
1743,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/xuc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/xuc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_xuc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/xuc15.html,"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO."
1744,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Robust learning of inhomogeneous PMMs,"Ralf Eggeling, Teemu Roos, Petri Myllym_ki, Ivo Grosse","JMLR W&CP 33 :229-237, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/eggeling14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_eggeling14,http://jmlr.csail.mit.edu/proceedings/papers/v33/eggeling14.html,"Inhomogeneous parsimonious Markov models have recently been introduced for modeling symbolic sequences, with a main application being DNA sequence analysis. Structure and parameter learning of these models has been proposed using a Bayesian approach, which entails the practically challenging choice of the prior distribution. Cross validation is a possible way of tuning the prior hyperparameters towards a specific task such as prediction or classification, but it is overly time-consuming. On this account, robust learning methods, which do not require explicit prior specification and _ in the absence of prior knowledge _ no hyperparameter tuning, are of interest. In this work, we empirically investigate the performance of robust alternatives for structure and parameter learning that extend the practical applicability of inhomogeneous parsimonious Markov models to more complex settings than before."
1745,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Generative Modeling for Maximizing Precision and Recall in Information Visualization,"Jaakko Peltonen, Samuel Kaski","15:579-587, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/peltonen11a/peltonen11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_peltonen11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/peltonen11a.html,Information visualization has recently been formulated as an information retrieval problem where the goal is to find similar data points based on the visualized nonlinear projection and the visualization is optimized to maximize a compromise between (smoothed) precision and recall. We turn the visualization into a generative modeling task where a simple user model parameterized by the data coordinates is optimized neighborhood relations are the observed data and straightforward maximum likelihood estimation corresponds to Stochastic Neighbor Embedding (SNE). While SNE maximizes pure recall adding a mixture component that ``explains away'' misses allows our generative model to focus on maximizing precision as well. The resulting model is a generative solution to maximizing tradeoffs between precision and recall. The model outperforms earlier models in terms of precision and recall and in external validation by unsupervised classification.
1746,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,"Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm","Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, Bernhard Schoelkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/daneshmand14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/daneshmand14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_daneshmand14,http://jmlr.csail.mit.edu/proceedings/papers/v32/daneshmand14.html,"Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d 3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice."
1747,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Finding Good Cascade Sampling Processes for the Network Inference Problem,"Manuel Gomez-Rodriguez, Le Song, Bernhard Schoelkopf","JMLR W&CP 35 :1276-1279, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/gomezrodriguez14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_gomezrodriguez14,http://jmlr.csail.mit.edu/proceedings/papers/v35/gomezrodriguez14.html,"Information spreads across social and technological networks, but often the network structures are hidden and we only observe the traces left by the diffusion processes, called cascades. It is known that, under a popular continuous-time diffusion model, as long as the model parameters satisfy a natural incoherence condition, it is possible to recover the correct network structure with high probability if we observe \(O(d^3 \log N)\) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. However, the incoherence condition depends, in a non-trivial way, on the source (node) distribution of the cascades, which is typically unknown. Our open problem is whether it is possible to design an active algorithm which samples the source locations in a sequential manner and achieves the same or even better sample complexity, e.g., \(o(d_i^3 \log N)\) , than previous work."
1748,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Max-Margin Infinite Hidden Markov Models,"Aonan Zhang, Jun Zhu, Bo Zhang",none,http://jmlr.org/proceedings/papers/v32/zhangb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhangb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhangb14.html,"Infinite hidden Markov models (iHMMs) are nonparametric Bayesian extensions of hidden Markov models (HMMs) with an infinite number of states. Though flexible in describing sequential data, the generative formulation of iHMMs could limit their discriminative ability in sequential prediction tasks. Our paper introduces max-margin infinite HMMs (M2iHMMs), new infinite HMMs that explore the max-margin principle for discriminative learning. By using the theory of Gibbs classifiers and data augmentation, we develop efficient beam sampling algorithms without making restricting mean-field assumptions or truncated approximation. For single variate classification, M2iHMMs reduce to a new formulation of DP mixtures of max-margin machines. Empirical results on synthetic and real data sets show that our methods obtain superior performance than other competitors in both single variate classification and sequential prediction tasks."
1749,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Identifying Cause and Effect on Discrete Data using Additive Noise Models,"Jonas Peters, Dominik Janzing, Bernhard Sch_lkopf","9:597-604, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/peters10a/peters10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_peters10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/peters10a.html,Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. Recently methods using additive noise models have been suggested to approach the case of continuous variables. In many situations however the variables of interest are discrete or even have only finitely many states. In this work we extend the notion of additive noise models to these cases. Whenever the joint distribution P^(XY) admits such a model in one direction e.g. Y=f(X)+N N independent of X it does not admit the reversed model X=g(Y)+N' N' independent of Y as long as the model is chosen in a generic way. Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables. We show that this algorithm works both on synthetic and real data sets.
1750,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Approximate Inference by Intersecting Semidefinite Bound and Local Polytope,"Jian Peng, Tamir Hazan, Nathan Srebro, Jinbo Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/peng12/peng12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_peng12,http://jmlr.csail.mit.edu/proceedings/papers/v22/peng12.html,Inference in probabilistic graphical models can be represented as a constrained optimization problem of a free-energy functional. Substantial research has been focused on the approximation of the constraint set also known as the marginal polytope. This paper presents a novel inference algorithm that tightens and solves the optimization problem by intersecting the popular local polytope and the semidefinite outer bound of the marginal polytope. Using dual decomposition our method separates the optimization problem into two subproblems: a semidefinite program (SDP) which is solved by a low-rank SDP algorithm and a free-energy based optimization problem which is solved by convex belief propagation. Our method has a very reasonable computational complexity and its actual running time is typically within a small factor (=10) of convex belief propagation. Tested on both synthetic data and a real-world protein side-chain packing benchmark our method significantly outperforms tree-reweighted belief propagation in both marginal probability inference and MAP inference. Our method is competitive with the state-of-the-art in MRF inference solving all protein tasks solved by the recently presented MPLP method and beating MPLP on lattices with strong edge potentials.
1751,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Efficient Lifting of MAP LP Relaxations Using k-Locality,"Martin Mladenov, Kristian Kersting, Amir Globerson","JMLR W&CP 33 :623-632, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/mladenov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_mladenov14,http://jmlr.csail.mit.edu/proceedings/papers/v33/mladenov14.html,"Inference in large scale graphical models is an important task in many domains, and in particular for probabilistic relational models (e.g,. Markov logic networks). Such models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Here we address this task in the context of the MAP inference problem and its linear programming relaxations. We show that symmetry in these problems can be discovered using an elegant algorithm known as the \(k\) -dimensional Weisfeiler-Lehman (k-WL) algorithm. We run k-WL on the original graphical model, and not on the far larger graph of the linear program (LP) as proposed in earlier work in the field. Furthermore, the algorithm is polynomial and thus far more practical than other previous approaches which rely on orbit partitions that are GI complete to find. The fact that k-WL can be used in this manner follows from the recently introduced notion of \(k\) -local LPs and their relation to Sherali Adams relaxations of graph automorphisms. Finally, for relational models such as Markov logic networks, the benefits of our approach are even more dramatic, as we can discover symmetries in the original domain graph, as opposed to running lifting on the much larger grounded model."
1752,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Bethe Bounds and Approximating the Global Optimum,"Adrian Weller, Tony Jebara",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/weller13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_weller13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/weller13a.html,"Inference in general Markov random fields (MRFs) is NP-hard, though identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with submodular cost functions is efficiently solvable using graph cuts. Marginal inference, however, even for this restricted class, is #P-hard. Restricting to binary pairwise models, we prove new formulations of derivatives of the Bethe free energy, provide bounds on the derivatives and bracket the locations of stationary points. Several results apply whether the model is associative or not. Applying these to discretized pseudo-marginals in the associative case, we present a polynomial time approximation scheme for global optimization of the Bethe free energy provided the maximum degree \(\Delta=O(\log n)\) , where \(n\) is the number of variables. Runtime is guaranteed \(O(\epsilon^{-3/2} n^6 \Sigma^{3/4} \Omega^{3/2})\) , where \(\Sigma=O(\Delta/n)\) is the fraction of possible edges present and \(\Omega\) is a function of MRF parameters. We examine use of the algorithm in practice, demonstrating runtime that is typically much faster, and discuss several extensions."
1753,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Exact Learning of Bounded Tree-width Bayesian Networks,"Janne Korhonen, Pekka Parviainen",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/korhonen13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_korhonen13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/korhonen13a.html,"Inference in Bayesian networks is known to be NP-hard, but if the network has bounded tree-width, then inference becomes tractable. Not surprisingly, learning networks that closely match the given data and have a bounded tree-width has recently attracted some attention. In this paper we aim to lay groundwork for future research on the topic by studying the exact complexity of this problem. We give the first non-trivial exact algorithm for the NP-hard problem of finding an optimal Bayesian network of tree-width at most \(w\) , with running time \(3^n n^{w + O(1)}\) , and provide an implementation of this algorithm. Additionally, we propose a variant of Bayesian network learning with –super-structures”, and show that finding a Bayesian network consistent with a given super-structure is fixed-parameter tractable in the tree-width of the super-structure."
1754,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Scaling the Indian Buffet Process via Submodular Maximization,"Colorado Reed, Ghahramani Zoubin",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/reed13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/reed13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_reed13,http://jmlr.csail.mit.edu/proceedings/papers/v28/reed13.html,"Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Wellings (2008)ês maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3-approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model."
1755,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Online Inference of Topics with Latent Dirichlet Allocation,"Kevin Canini, Lei Shi, Thomas Griffiths","5:65-72, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/canini09a/canini09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_canini09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/canini09a.html,Inference algorithms for topic models are typically designed to be run over an entire collection of documents after they have been observed. However in many applications of these models the collection grows over time making it infeasible to run batch algorithms repeatedly. This problem can be addressed by using online algorithms which update estimates of the topics as each document is observed. We introduce two related Rao-Blackwellized online inference algorithms for the latent Dirichlet allocation (LDA) model -- incremental Gibbs samplers and particle filters -- and compare their runtime and performance to that of existing algorithms.
1756,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Information Retrieval Perspective to Meta-visualization,"Jaakko Peltonen, Ziyuan Lin","JMLR W&CP 29 :165-180, 2013",http://jmlr.org/proceedings/papers/v29/Peltonen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Peltonen13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Peltonen13.html,"In visual data exploration with scatter plots, no single plot is sufficient to analyze complicated high-dimensional data sets. Given numerous visualizations created with different features or methods, meta-visualization is needed to analyze the visualizations together. We solve how to arrange numerous visualizations onto a meta-visualization display , so that their similarities and differences can be analyzed. We introduce a machine learning approach to optimize the meta-visualization, based on an information retrieval perspective: two visualizations are similar if the analyst would retrieve similar neighborhoods between data samples from either visualization. Based on the approach, we introduce a nonlinear embedding method for meta-visualization: it optimizes locations of visualizations on a display, so that visualizations giving similar information about data are close to each other."
1757,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Estimating the accuracies of multiple classifiers without labeled data,"Ariel Jaffe, Boaz Nadler, Yuval Kluger",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/jaffe15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/jaffe15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_jaffe15,http://jmlr.csail.mit.edu/proceedings/papers/v38/jaffe15.html,"In various situations one is given only the predictions of multiple classifiers over a large unlabeled test data. This scenario raises the following questions: Without any labeled data and without any a-priori knowledge about the reliability of these different classifiers, is it possible to consistently and computationally efficiently estimate their accuracies? Furthermore, also in a completely unsupervised manner, can one construct a more accurate unsupervised ensemble classifier? In this paper, focusing on the binary case, we present simple, computationally efficient algorithms to solve these questions. Furthermore, under standard classifier independence assumptions, we prove our methods are consistent and study their asymptotic error. Our approach is spectral, based on the fact that the off-diagonal entries of the classifiersê covariance matrix and 3-d tensor are rank-one. We illustrate the competitive performance of our algorithms via extensive experiments on both artificial and real datasets."
1758,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Parametric-Output HMMs with Two Aliased States,"Roi Weiss, Boaz Nadler",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/weiss15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/weiss15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_weiss15,http://jmlr.csail.mit.edu/proceedings/papers/v37/weiss15.html,"In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations."
1759,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Policy-Gradients for PSRs and POMDPs,"Douglas Aberdeen, Olivier Buffet, Owen Thomas","2:3-10, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/aberdeen07a/aberdeen07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_aberdeen07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/aberdeen07a.html,In uncertain and partially observable environments control policies must be a function of the complete history of actions and observations. Rather than present an ever growing history to a learner we instead track sufficient statistics of the history and map those to a control policy. The mapping has typically been done using dynamic programming requiring large amounts of memory. We present a general approach to mapping sufficient statistics directly to control policies by combining the tracking of sufficient statistics with the use of policy-gradient reinforcement learning. The best known sufficient statistic is the belief state computed from a known or estimated partially observable Markov decision process (POMDP) model. More recently predictive state representations (PSRs) have emerged as a potentially compact model of partially observable systems. Our experiments explore the usefulness of both of these sufficient statistics exact and estimated in direct policy-search.
1760,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Assisting Main Task Learning by Heterogeneous Auxiliary Tasks with Applications to Skin Cancer Screening,"Ning Situ, Xiaojing Yuan, George Zouridakis","15:688-697, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/situ11a/situ11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_situ11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/situ11a.html,In typical classification problems high level concept features provided by a domain expert are usually available during classifier training but not during its deployment. We address this problem from a multitask learning (MTL) perspective by treating these features as auxiliary learning tasks. Previous efforts in MTL have mostly assumed that all tasks have the same input space. However auxiliary tasks can have different input spaces since their learning targets are different. Thus to handle cases with heterogeneous input in this paper we present a newly developed model using heterogeneous auxiliary tasks to help main task learning. First we formulate a convex optimization problem for the proposed model and then we analyze its hypothesis class and derive true risk bounds. Finally we compare the proposed model with other relevant methods when applied to the problem of skin cancer screening and public datasets. Our results show that the performance of the proposed method is highly competitive compared to other relevant methods.
1761,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Graphical Gaussian modelling of multivariate time series with latent variables,Michael Eichler,"9:193-200, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/eichler10a/eichler10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_eichler10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/eichler10a.html,In time series analysis inference about cause-effect relationships among multiple times series is commonly based on the concept of Granger causality which exploits temporal structure to achieve causal ordering of dependent variables. One major problem in the application of Granger causality for the identification of causal relationships is the possible presence of latent variables that affect the measured components and thus lead to so-called spurious causalities. In this paper we describe a new graphical approach for modelling the dependence structure of multivariate stationary time series that are affected by latent variables. To this end we introduce dynamic maximal ancestral graphs (dMAGs) in which each time series is represented by a single vertex. For Gaussian processes this approach leads to vector autoregressive models with errors that are not independent but correlated according to the dashed edges in the graph. We discuss identifiability of the parameters and show that these models can be viewed as graphical ARMA models that satisfy the Granger causality restrictions encoded by the associated dynamic maximal ancestral graph.
1762,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Cross-associating unlabelled timbre distributions to create expressive musical mappings,Dan Stowell and Mark D. Plumbley,"11:28-35, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/stowell10a/stowell10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_stowell10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/stowell10a.html,"In timbre remapping applications such as concatenative synthesis, an audio signal is used as a template, and a mapping process derives control data for some audio synthesis algorithm such that it produces a new audio signal approximating the perceived trajectory of the original sound. Timbre is a multidimensional attribute with interactions between dimensions, and the control and synthesised signals typically represent sounds with different timbral ranges, so it is non-trivial to design a search process which makes best use of the timbral variety available in the synthesiser. We first discuss our preliminary work applying standard machine-learning techniques for this purpose (PCA, self-organising maps), and the reasons they were not satisfactory. We then describe a novel regression-tree technique which learns associations between unlabelled multidimensional timbre distributions."
1763,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Marginal Structured SVM with Hidden Variables,"Wei Ping, Qiang Liu, Alex Ihler",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/ping14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ping14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ping14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ping14.html,"In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice."
1764,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Simple Connectome Inference from Partial Correlation Statistics in Calcium Imaging,"Antonio Sutera, Arnaud Joly, Vincent Francois-Lavet, Aaron Qiu, Gilles Louppe, Damien Ernst, Pierre Geurts",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/sutera15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_sutera15,http://jmlr.csail.mit.edu/proceedings/papers/v46/sutera15.html,"In this work, we propose a simple yet effective solution to the problem of connectome inference in calcium imaging data. The proposed algorithm consists of two steps. First, processing the raw signals to detect neural peak activities. Second, inferring the degree of association between neurons from partial correlation statistics. This paper summarises the methodology that led us to win the Connectomics Challenge, proposes a simplified version of our method, and finally compares our results with respect to other inference methods."
1765,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Gated Feedback Recurrent Neural Networks,"Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chung15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/chung15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chung15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chung15.html,"In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions."
1766,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Online Kernel Learning with a Near Optimal Sparsity Bound,"Lijun Zhang, Jinfeng Yi, Rong Jin, Ming Lin, Xiaofei He",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13c-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13c,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13c.html,"In this work, we focus on Online Sparse Kernel Learning that aims to online learn a kernel classifier with a bounded number of support vectors. Although many online learning algorithms have been proposed to learn a sparse kernel classifier, most of them fail to bound the number of support vectors used by the final solution which is the average of the intermediate kernel classifiers generated by online algorithms. The key idea of the proposed algorithm is to measure the difficulty in correctly classifying a training example by the derivative of a smooth loss function, and give a more chance to a difficult example to be a support vector than an easy one via a sampling scheme. Our analysis shows that when the loss function is smooth, the proposed algorithm yields similar performance guarantee as the standard online learning algorithm but with a near optimal number of support vectors (up to a poly(lnT) factor). Our empirical study shows promising performance of the proposed algorithm compared to the state-of-the-art algorithms for online sparse kernel learning."
1767,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Partitioning Well-Clustered Graphs: Spectral Clustering Works!,"Richard Peng, He Sun, Luca Zanetti",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Peng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Peng15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Peng15.html,"In this work we study the widely used spectral clustering algorithms, i.e. partition a graph into \(k\) clusters via (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix, and (2) partitioning embedded points via \(k\) -means algorithms. We show that, for a wide class of well-clustered graphs , spectral clustering algorithms can give a good approximation of the optimal clustering. To the best of our knowledge, it is the first theoretical analysis of spectral clustering algorithms for a wide family of graphs, even though such approach was proposed in the early 1990s and has comprehensive applications. We also give a nearly-linear time algorithm for partitioning well-clustered graphs, which is based on heat kernel embeddings and approximate nearest neighbor data structures."
1768,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Spanning Tree Approximations for Conditional Random Fields,"Patrick Pletscher, Cheng Soon Ong, Joachim Buhmann","5:408-415, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/pletscher09a/pletscher09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_pletscher09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/pletscher09a.html,In this work we show that one can train Conditional Random Fields of intractable graphs effectively and efficiently by considering a mixture of random spanning trees of the underlying graphical model. Furthermore we show how a maximum-likelihood estimator of such a training objective can subsequently be used for prediction on the full graph. We present experimental results which improve on the state-of-the-art. Additionally the training objective is less sensitive to the regularization than pseudo-likelihood based training approaches. We perform the experimental validation on two classes of data sets where structure is important: image denoising and multilabel classification.
1769,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Hierarchical Cost-Sensitive Algorithms for Genome-Wide Gene Function Prediction,"NicolÖ Cesa-Bianchi, Giorgio Valentini","8:14-29, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/cesa-bianchi10a/cesa-bianchi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_cesa-bianchi10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/cesa-bianchi10a.html,"In this work we propose new ensemble methods for the hierarchical classification of gene functions. Our methods exploit the hierarchical relationships between the classes in different ways: each ensemble node is trained ""locally"", according to its position in the hierarchy; moreover, in the evaluation phase the set of predicted annotations is built so to minimize a global loss function defined over the hierarchy. We also address the problem of sparsity of annotations by introducing a cost-sensitive parameter that allows to control the precision-recall trade-off. Experiments with the model organism S. cerevisiae , using the FunCat taxonomy and seven biomolecular data sets, reveal a significant advantage of our techniques over ""flat"" and cost-insensitive hierarchical ensembles."
1770,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Hierarchical Latent Dictionaries for Models of Brain Activation,"Alona Fyshe, Emily Fox, David Dunson, Tom Mitchell",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/fyshe12/fyshe12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_fyshe12,http://jmlr.csail.mit.edu/proceedings/papers/v22/fyshe12.html,In this work we propose a hierarchical latent dictionary approach to estimate the time-varying mean and covariance of a process for which we have only limited noisy samples. We fully leverage the limited sample size and redundancy in sensor measurements by transferring knowledge through a hierarchy of lower dimensional latent processes. As a case study we utilize Magnetoencephalography (MEG) recordings of brain activity to identify the word being viewed by a human subject. Specifically we identify the word category for a single noisy MEG recording when given only limited noisy samples on which to train.
1771,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,Logistic Model Trees with AUC Split Criterion for the KDD Cup 2009 Small Challenge,none,none,http://jmlr.csail.mit.edu/proceedings/papers/v7/doetsch09/doetsch09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_doetsch09,http://jmlr.csail.mit.edu/proceedings/papers/v7/doetsch09.html,"In this work we describe our approach to the ""Small Challenge"" of the KDD cup 2009 a classification task with incomplete data. Preprocessing feature extraction and model selection are documented in detail. We suggest a criterion based on the number of missing values to select a suitable imputation method for each feature. Logistic Model Trees (LMT) are extended with a split criterion optimizing the Area under the ROC Curve (AUC) which was the requested evaluation criterion. By stacking boosted decision stumps and LMT we achieved the best result for the ""Small Challenge"" without making use of additional data from other feature sets resulting in an AUC score of 0.8081. We also present results of an AUC optimizing model combination that scored only slightly worse with an AUC score of 0.8074."
1772,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Sample Complexity Bounds on Differentially Private Learning via Communication Complexity,"Vitaly Feldman, David Xiao","JMLR W&CP 35 :1000-1019, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_feldman14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/feldman14b.html,"In this work we analyze the sample complexity of classification by differentially private algorithms. Differential privacy is a strong and well-studied notion of privacy introduced by Dwork et al. (2006) that ensures that the output of an algorithm leaks little information about the data point provided by any of the participating individuals. Sample complexity of private PAC and agnostic learning was studied in a number of prior works starting with (Kasiviswanathan et al., 2008) but a number of basic questions still remain open (Beimel et al. 2010; Chaudhuri and Hsu, 2011; Beimel et al., 2013a,b). Our main contribution is an equivalence between the sample complexity of differentially-private learning of a concept class \(C\) (or \(\mathrm{SCDP}(C)\) ) and the randomized one-way communication complexity of the evaluation problem for concepts from \(C\) . Using this equivalence we prove the following bounds: \(\mathrm{SCDP}(C) = \Omega(\mathrm{LDim}(C))\) , where \(\mathrm{LDim}(C)\) is the Littlestoneês (1987) dimension characterizing the number of mistakes in the online-mistake-bound learning model. This result implies that \(\mathrm{SCDP}(C)\) is different from the VC-dimension of \(C\) , resolving one of the main open questions from prior work. For any \(t\) , there exists a class \(C\) such that \(\mathrm{LDim}(C)=2\) but \(\mathrm{SCDP}(C) \geq t\) . For any \(t\) , there exists a class \(C\) such that the sample complexity of (pure) \(\alpha\) -differentially private PAC learning is \(\Omega(t/\alpha)\) but the sample complexity of the relaxed \((\alpha,\beta)\) -differentially private PAC learning is \(O(\log(1/\beta)/\alpha)\) . This resolves an open problem from (Beimel et al., 2013b). We also obtain simpler proofs for a number of known related results. Our equivalence builds on a characterization of sample complexity by Beimel et al., (2013a) and our bounds rely on a number of known results from communication complexity."
1773,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation,"Guangcan Liu, Huan Xu, Shuicheng Yan",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/liu12a/liu12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_liu12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/liu12a.html,In this work we address the following matrix recovery problem: suppose we are given a set of data points containing two parts one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces efficiently and exactly? We utilize a so-called Low-Rank Representation (LRR) method to solve this problem and prove that under mild technical conditions any solution to LRR exactly recover the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space this further implies that LRR can perform exact subspace segmentation and outlier detection in an efficient way.
1774,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Asymptotic Theory for Linear-Chain Conditional Random Fields,"Mathieu Sinn, Pascal Poupart","15:679-687, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/sinn11a/sinn11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_sinn11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/sinn11a.html,In this theoretical paper we develop an asymptotic theory for Linear-Chain Conditional Random Fields (L-CRFs) and apply it to derive conditions under which the Maximum Likelihood Estimates (MLEs) of the model weights are strongly consistent. We first define L-CRFs for infinite sequences and analyze some of their basic properties. Then we establish conditions under which ergodicity of the observations implies ergodicity of the joint sequence of observations and labels. This result is the key ingredient to derive conditions for strong consistency of the MLEs. Interesting findings are that the consistency crucially depends on the limit behavior of the Hessian of the likelihood function and that asymptotically the state feature functions do not matter.
1775,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning the beta-Divergence in Tweedie Compound Poisson Matrix Factorization Models,"Umut Simsekli, Ali Taylan Cemgil, Yusuf Kenan Yilmaz",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/simsekli13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_simsekli13,http://jmlr.csail.mit.edu/proceedings/papers/v28/simsekli13.html,"In this study, we derive algorithms for estimating mixed \(\beta\) -divergences. Such cost functions are useful for Nonnegative Matrix and Tensor Factorization models with a compound Poisson observation model. Compound Poisson is a particular Tweedie model, an important special case of exponential dispersion models characterized by the fact that the variance is proportional to a power function of the mean. There are several well known matrix and tensor factorization algorithms that minimize the \(\beta\) -divergence; these estimate the mean parameter. The probabilistic interpretation gives us more flexibility and robustness by providing us additional tunable parameters such as power and dispersion. Estimation of the power parameter is useful for choosing a suitable divergence and estimation of dispersion is useful for data driven regularization and weighting in collective/coupled factorization of heterogeneous datasets. We present three inference algorithms for both estimating the factors and the additional parameters of the compound Poisson distribution. The methods are evaluated on two applications: modeling symbolic representations for polyphonic music and lyric prediction from audio features. Our conclusion is that the compound poisson based factorization models can be useful for sparse positive data."
1776,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Evaluation of Signaling Cascades Based on the Weights from Microarray and ChIP-seq Data,"Zerrin Isik, Volkan Atalay, Rengul Cetin-Atalay","8:44-54, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/isik10a/isik10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_isik10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/isik10a.html,In this study we combined the ChIP-seq and the transcriptome data and integrated these data into signaling cascades. Integration was realized through a framework based on data- and model-driven hybrid approach. An enrichment model was constructed to evaluate signaling cascades which resulted in specific cellular processes. We used ChIP-seq and microarray data from public databases which were obtained from HeLa cells under oxidative stress having similar experimental setups. Both ChIP-seq and array data were analyzed by percentile ranking for the sake of simultaneous data integration on specific genes. Signaling cascades from KEGG pathway database were subsequently scored by taking sum of the individual scores of the genes involved within the cascade. This scoring information is then transferred to en route of the signaling cascade to form the final score. Signaling cascade model based framework that we describe in this study is a novel approach which calculates scores for the target process of the analyzed signaling cascade rather than assigning scores to gene product nodes.
1777,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Efficient Real-Time Pixelwise Object Class Labeling for Safe Human-Robot Collaboration in Industrial Domain,"Vivek Sharma, Frank Dittrich, Sule Yildirim-Yayilgan, Luc Van Gool",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/sharma15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_sharma15,http://jmlr.csail.mit.edu/proceedings/papers/v43/sharma15.html,"In this paper, we use a random decision forests (RDF) classifier with a conditional random field (CRF) for pixelwise object class labeling of real-world scenes. Our ultimate goal is to develop an application which will provide safe human-robot collaboration (SHRC) and interaction (SHRI) in industrial domain. Such an application has many aspects to consider and in this work, we particularly focus on minimizing the mislabeling of human and object parts using depth measurements. This aspect will be important in modelling human/robot and object interactions in future work. Our approach is driven by three key objectives namely computational efficiency, robustness, and time efficiency (i.e. real-time). Due to the ultimate goal of reducing the risk of human-robot interventions. Our data set is depth measurements stored in depth maps. The object classes are human body-parts (head, body, upper-arm, lower-arm, hand, and legs), table, chair, plant, and storage based on industrial domain. We train an RDF classifier on the depth measurements contained in the depth maps. In this context, the output of random decision forests is a label assigned to each depth measurement. The misclassification of labels assigned to depth measurements is minimized by modeling the labeling problem on a pairwise CRF. The RDF classifier with its CRF extension (optimal predictions obtained using graph cuts extended over RDF predictions) has been evaluated for its performance for pixelwise object class segmentation. The evaluation results show that the CRF extension improves the performance measure by approximately 10.8% in F1-measure over the RDF performance measures."
1778,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Consistent optimization of AMS by logistic loss minimization,Wojciech Kot _ owski,none,http://jmlr.csail.mit.edu/proceedings/papers/v42/kotl14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_kotl14,http://jmlr.csail.mit.edu/proceedings/papers/v42/kotl14.html,"In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function \(f\) is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, given \(f\) , a threshold \(\hat{\theta}\) is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting classifier (obtained from thresholding \(f\) on \(\hat{\theta}\) ) measured with respect to the squared AMS, is upperbounded by the regret of \(f\) measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS."
1779,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Inference of k-Testable Directed Acyclic Graph Languages,"Damiˆn L„pez, Jorge Calera-Rubio and Javier Gallego-Sˆnchez","21:149-163, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/lopez12a/lopez12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_lopez12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/lopez12a.html,"In this paper, we tackle the task of graph language learning. We first extend the well-known classes of k-testability and k-testability in the strict sense languages to directed graph languages. Second, we propose a graph automata model for directed acyclic graph languages. This graph automata model is used to propose a grammatical inference algorithm to learn the class of directed acyclic k -testable in the strict sense graph languages. The algorithm runs in polynomial time and identifies this class of languages from positive data."
1780,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,General Oracle Inequalities for Gibbs Posterior with Application to Ranking,"Cheng Li, Wenxin Jiang, Martin Tanner",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Li13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Li13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Li13.html,"In this paper, we summarize some recent results in Li et al. (2012), which can be used to extend an important PAC-Bayesian approach, namely the Gibbs posterior, to study the nonadditive ranking risk. The methodology is based on assumption-free risk bounds and nonasymptotic oracle inequalities, which leads to nearly optimal convergence rates and optimal model selection to balance the approximation errors and the stochastic errors."
1781,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,One-Bit Compressed Sensing: Provable Support and Vector Recovery,"Sivakant Gopi, Praneeth Netrapalli, Prateek Jain, Aditya Nori",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gopi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/gopi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gopi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gopi13.html,"In this paper, we study the problem of one-bit compressed sensing ( \(1\) -bit CS), where the goal is to design a measurement matrix \(A\) and a recovery algorithm s.t. a \(k\) -sparse vector \(\x^*\) can be efficiently recovered back from signed linear measurements, i.e., \(b=\sign(A\x^*)\) . This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification . We study this problem in two settings: a) support recovery: recover \(\supp(\x^*)\) , b) approximate vector recovery: recover a unit vector \(\hx\) s.t. \(|| \hat{x}-\x^*/||\x^*|| ||_2\leq \epsilon\) . For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to existing methods for support recovery, our methods are universal i.e. a single measurement matrix \(A\) can recover almost all the signals. For approximate recovery, we propose the first method to recover sparse vector using a near optimal number of measurements. We also empirically demonstrate effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods."
1782,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,(Near) Dimension Independent Risk Bounds for Differentially Private Learning,"Prateek Jain, Abhradeep Guha Thakurta",none,http://jmlr.org/proceedings/papers/v32/jain14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/jain14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_jain14,http://jmlr.csail.mit.edu/proceedings/papers/v32/jain14.html,"In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: Is it possible to design computationally efficient differentially private risk minimizers with excess risk bounds that do not explicitly depend on dimensionality ( \(p\) ) and do not require structural assumptions like restricted strong convexity? In this paper, we answer the question in the affirmative for a variant of the well-known output and objective perturbation algorithms [Chaudhuri et al., 2011]. In particular, we show that in generalized linear model, variants of both output and objective perturbation algorithms have no explicit dependence on \(p\) . Our results assume that the underlying loss function is a \(1\) -Lipschitz convex function and we show that the excess risk depends only on \(L_2\) norm of the true risk minimizer and that of training points. Next, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is a doubly differentiable convex function. Assuming that the training points have bounded \(L_\infty\) -norm, our algorithm provides risk bound that has only logarithmic dependence on \(p\) . We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on \(p\) . In contrast, the existing differentially private online learning methods incur \(O(\sqrt{p})\) dependence."
1783,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Selective sampling algorithms for cost-sensitive multiclass prediction,Alekh Agarwal,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/agarwal13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/agarwal13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_agarwal13,http://jmlr.csail.mit.edu/proceedings/papers/v28/agarwal13.html,"In this paper, we study the problem of active learning for cost-sensitive multiclass classification. We propose selective sampling algorithms, which process the data in a streaming fashion, querying only a subset of the labels. For these algorithms, we analyze the regret and label complexity when the labels are generated according to a generalized linear model. We establish that the gains of active learning over passive learning can range from none to exponentially large, based on a natural notion of margin. We also present a safety guarantee to guard against model mismatch. Numerical simulations show that our algorithms indeed obtain a low regret with a small number of queries."
1784,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multi-Task Learning with Gaussian Matrix Generalized Inverse Gaussian Model,"Ming Yang, Yingming Li, Zhongfei Zhang (Zhejiang University)",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13d,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13d.html,"In this paper, we study the multi-task learning problem with a new perspective of considering the structure of the residue error matrix and the low-rank approximation to the task covariance matrix simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) prior and define a Gaussian Matrix Generalized Inverse Gaussian (GMGIG) model for low-rank approximation to the task covariance matrix. Through combining the GMGIG model with the residual error structure assumption, we propose the GMGIG regression model for multi-task learning. To make the computation tractable, we simultaneously use variational inference and sampling techniques. In particular, we propose two sampling strategies for computing the statistics of the MGIG distribution. Experiments show that this model is superior to the peer methods in regression and prediction."
1785,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions,"Purushottam Kar, Bharath Sriperumbudur, Prateek Jain, Harish Karnick",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/kar13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kar13.html,"In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees."
1786,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Theory of Dual-sparse Regularized Randomized Reduction,"Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yangb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yangb15.html,"In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data."
1787,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Optimization with First-Order Surrogate Functions,Julien Mairal,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/mairal13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/mairal13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_mairal13,http://jmlr.csail.mit.edu/proceedings/papers/v28/mairal13.html,"In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning."
1788,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,An example distribution for probabilistic query learning of simple deterministic languages,"Yasuhiro Tajima, Genichiro Kikui","JMLR W&CP 34 :182-192, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/tajima14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_tajima14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/tajima14a.html,"In this paper, we show a special example distribution on which the learner can guess a correct simple deterministic grammar in polynomial time from membership queries and random examples. At first, we show a learning algorithm of simple deterministic languages from membership and equivalence queries. This algorithm is not a polynomial time algorithm but, assuming a special example distribution, we can modify it to the polynomial time probabilistic learning algorithm."
1789,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Scalable Heterogeneous Transfer Ranking,"Mohammad Taha Bahadori, Yi Chang, Bo Long, Yan Liu","JMLR W&CP 36 :214-228, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/bahadori14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_bahadori14,http://jmlr.csail.mit.edu/proceedings/papers/v36/bahadori14.html,"In this paper, we propose to study the problem of heterogeneous transfer ranking, a transfer learning problem with heterogeneous features in order to utilize the rich large-scale labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, to solve the problem by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on two application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods."
1790,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model,"Min Xiao, Yuhong Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/xiao13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_xiao13,http://jmlr.csail.mit.edu/proceedings/papers/v28/xiao13.html,"In this paper, we propose to address the problem of domain adaptation for sequence labeling tasks via distributed representation learning by using a log-bilinear language adaptation model. The proposed neural probabilistic language model simultaneously models two different but related data distributions in the source and target domains based on induced distributed representations, which encode both generalizable and domain-specific latent features. We then use the learned dense real-valued representation as augmenting features for natural language processing systems. We empirically evaluate the proposed learning technique on WSJ and MEDLINE domains with POS tagging systems, and on WSJ and Brown corpora with syntactic chunking and name entity recognition systems. Our primary results show that the proposed domain adaptation method outperforms a number comparison methods for cross domain sequence labeling tasks."
1791,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Learning with Augmented Multi-Instance View,"Yue Zhu, Jianxin Wu, Yuan Jiang, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/zhu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_zhu14,http://jmlr.csail.mit.edu/proceedings/papers/v39/zhu14.html,"In this paper, we propose the Augmented Multi-Instance View (AMIV) framework to construct a better model by exploiting augmented information. For example, abstract screening tasks may be difficult because only abstract information is available, whereas the performance can be improved when the abstracts of references listed in the document can be exploited as augmented information. If each abstract is represented as an instance (i.e., a feature vector) x, then with the augmented information, it can be represented as an instance-bag pair (x;B), where B is a bag of instances (i.e., the abstracts of references). Note that if x has a label y, then we assume that there must exist at least one instance in the bag B having the label y. We regard x and B as two views, i.e., a single-instance view augmented with a multi-instance view, and propose the AMIV-lss approach by establishing a latent semantic subspace between the two views. The AMIV framework can be applied when the augmented information is presented as multi-instance bags and to the best of our knowledge, such a learning with augmented multi-instance view problem has not been touched before. Experimental results on twelve TechPaper datasets, five PubMed data sets and a WebPage data set validate the effectiveness of our AMIV-lss approach."
1792,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Correlation Clustering with Noisy Partial Information,"Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Makarychev15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Makarychev15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Makarychev15.html,"In this paper, we propose and study a semi-random model for the Correlation Clustering problem on arbitrary graphs \(G\) . We give two approximation algorithms for Correlation Clustering instances from this model. The first algorithm finds a solution of value \((1+ \delta)\mathrm{opt-cost} + O_{\delta}(n\log^3 n)\) with high probability, where \(\mathrm{opt-cost}\) is the value of the optimal solution (for every \(\delta _ 0\) ). The second algorithm finds the ground truth clustering with an arbitrarily small classification error \(\eta\) (under some additional assumptions on the instance)."
1793,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fixed-Point Model For Structured Labeling,"Quannan Li, Jingdong Wang, David Wipf, Zhuowen Tu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/li13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_li13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/li13b.html,"In this paper, we propose a simple but effective solution to the structured labeling problem: a fixed-point model. Recently, layered models with sequential classifiers/regressors have gained an increasing amount of interests for structural prediction. Here, we design an algorithm with a new perspective on layered models; we aim to find a fixed-point function with the structured labels being both the output and the input. Our approach alleviates the burden in learning multiple/different classifiers in different layers. We devise a training strategy for our method and provide justifications for the fixed-point function to be a contraction mapping. The learned function captures rich contextual information and is easy to train and test. On several widely used benchmark datasets, the proposed method observes significant improvement in both performance and efficiency over many state-of-the-art algorithms."
1794,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Online Mean Field Approximation for Automated Experimentation,"Shaona Ghosh, Adam Prôgel-Bennett",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/ghosh15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_ghosh15,http://jmlr.csail.mit.edu/proceedings/papers/v43/ghosh15.html,"In this paper, we propose a semi-supervised online graph labelling method that affords early learning capability. We use mean field approximation for predicting the unknown labels of the vertices of the graph with high accuracy on the standard benchmark datasets. The minimum cut is the energy function of our probabilistic model that encodes the uncertainty about the labels of the vertices. Our method shows that it can learn early given any choice of experiments that may take place in the automated experimentation systems used for scientific discovery."
1795,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nearest Neighbors Using Compact Sparse Codes,Anoop Cherian,none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cherian14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cherian14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cherian14.html,"In this paper, we propose a novel scheme for approximate nearest neighbor (ANN) retrieval based on dictionary learning and sparse coding. Our key innovation is to build compact codes, dubbed SpANN codes, using the active set of sparse coded data. These codes are then used to index an inverted file table for fast retrieval. The active sets are often found to be sensitive to small differences among data points, resulting in only near duplicate retrieval. We show that this sensitivity is related to the coherence of the dictionary; small coherence resulting in better retrieval. To this end, we propose a novel dictionary learning formulation with incoherence constraints and an efficient method to solve it. Experiments are conducted on two state-of-the-art computer vision datasets with 1M data points and show an order of magnitude improvement in retrieval accuracy without sacrificing memory and query time compared to the state-of-the-art methods."
1796,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing,"Rongda Zhu, Quanquan Gu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhua15.html,"In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of \(O(s/\epsilon^2)\) for strong signals, and \(O(s\log d/\epsilon^2)\) for weak signals, where \(s\) is the number of nonzero entries in the signal vector, \(d\) is the signal dimension and \(\epsilon\) is the recovery error. For general signals, the sample complexity of our algorithm lies between \(O(s/\epsilon^2)\) and \(O(s\log d/\epsilon^2)\) . This is a remarkable improvement over the existing best sample complexity \(O(s\log d/\epsilon^2)\) . Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting."
1797,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Non-parametric Conditional Factor Regression Model for Multi-Dimensional Input and Response,"Ava Bargi, Richard Yi Xu, Zoubin Ghahramani, Massimo Piccardi","JMLR W&CP 33 :77-85, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/bargi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_bargi14,http://jmlr.csail.mit.edu/proceedings/papers/v33/bargi14.html,"In this paper, we propose a non-parametric conditional factor regression (NCFR) model for domains with multi-dimensional input and response. NCFR enhances linear regression in two ways: a) introducing low-dimensional latent factors leading to dimensionality reduction and b) integrating the Indian Buffet Process as prior for the latent layer to dynamically derive an optimal number of sparse factors. Thanks to IBPês enhancements to the latent factors, NCFR can significantly avoid over-fitting even in the case of a very small sample size compared to the dimensionality. Experimental results on three diverse datasets comparing NCRF to a few baseline alternatives give evidence of its robust learning, remarkable predictive performance, good mixing and computational efficiency."
1798,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Transition Matrix Estimation in High Dimensional Time Series,"Fang Han, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/han13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_han13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/han13a.html,"In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging."
1799,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Ordered Representations with Nested Dropout,"Oren Rippel, Michael Gelbart, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/rippel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/rippel14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_rippel14,http://jmlr.csail.mit.edu/proceedings/papers/v32/rippel14.html,"In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows the use of codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction."
1800,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Stochastic Block Model and Community Detection in Sparse Graphs: A spectral algorithm with optimal rate of recovery,"Peter Chin, Anup Rao, Van Vu",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chin15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Chin15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Chin15.html,"In this paper, we present and analyze a simple and robust spectral algorithm for the stochastic block model with \(k\) blocks, for any \(k\) fixed. Our algorithm works with graphs having constant edge density, under an optimal condition on the gap between the density inside a block and the density between the blocks. As a co-product, we settle an open question posed by Abbe et. al. concerning censor block models."
1801,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Heterogeneous Domain Adaptation for Multiple Classes,"Joey Tianyi Zhou, Ivor W.Tsang, Sinno Jialin Pan, Mingkui Tan","JMLR W&CP 33 :1095-1103, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/zhou14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_zhou14,http://jmlr.csail.mit.edu/proceedings/papers/v33/zhou14.html,"In this paper, we present an efficient Multi-class Heterogeneous Domain Adaptation (HDA) method, where data from the source and target domains are represented by heterogeneous features with different dimensions. Specifically, we propose to reconstruct a sparse feature transformation matrix to map the features of multiple classes from the source domain to the target domain. We cast this learning task as a compressed sensing problem, where each classifier can be deemed as a measurement sensor. Based on compressive sensing theory, the estimation error of the transformation matrix decreases with the increasing number of classifiers. Therefore, to guarantee the reconstruction performance, we construct sufficiently many binary classifiers based on the error correcting output correcting. Extensive experiments are conducted on both toy data and three real-world HDA applications to verify the superiority of our proposed method over existing state-of-the-art HDA methods in terms of prediction accuracy."
1802,42,http://jmlr.csail.mit.edu/proceedings/papers/v42/,Optimization of AMS using Weighted AUC optimized models,"Roberto DÍaz-Morales, çngel Navia-Vˆzquez",none,http://jmlr.csail.mit.edu/proceedings/papers/v42/diaz14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v42/,,27th August 2015,December 13 2014,HEPML 2014 Proceedings,HEPML workshop at NIPS14,"Paris, France","Glen Cowan, CÕ©cile Germain, Isabelle Guyon, BalÕçzs KÕ©gl, David Rousseau",v42_diaz14,http://jmlr.csail.mit.edu/proceedings/papers/v42/diaz14.html,"In this paper, we present an approach to deal with the maximization of the approximate median discovery significance (AMS) in high energy physics. This paper proposes the maximization of the Weighted AUC as a criterion to train different models and the subsequent creation of an ensemble that maximizes the AMS. The algorithm described in this paper was our solution for the Higgs Boson Machine Learning Challenge and we complement this paper describing the preprocessing of the dataset, the training procedure and the experimental results that our model obtained in the challenge. This approach has proven its good performance finishing in ninth place among the solutions of 1785 teams."
1803,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Two-Stage Metric Learning,"Jun Wang, Ke Sun, Fei Sha, St_phane Marchand-Maillet, Alexandros Kalousis",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_wangc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/wangc14.html,"In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric which presents unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM."
1804,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Convex Adversarial Collective Classification,"MohamadAli Torkamani, Daniel Lowd",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/torkamani13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/torkamani13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_torkamani13,http://jmlr.csail.mit.edu/proceedings/papers/v28/torkamani13.html,"In this paper, we present a novel method for robustly performing collective classification in the presence of a malicious adversary that can modify up to a fixed number of binary-valued attributes. Our method is formulated as a convex quadratic program that guarantees optimal weights against a worst-case adversary in polynomial time. In addition to increased robustness against active adversaries, this kind of adversarial regularization can also lead to improved generalization even when no adversary is present. In experiments on real and simulated data, our method consistently outperforms both non-adversarial and non-relational baselines."
1805,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Lifted MAP Inference for Markov Logic Networks,"Somdeb Sarkhel, Deepak Venugopal, Parag Singla, Vibhav Gogate","JMLR W&CP 33 :859-867, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/sarkhel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_sarkhel14,http://jmlr.csail.mit.edu/proceedings/papers/v33/sarkhel14.html,"In this paper, we present a new approach for lifted MAP inference in Markov Logic Networks (MLNs). Our approach is based on the following key result that we prove in the paper: if an MLN has no shared terms then MAP inference over it can be reduced to MAP inference over a Markov network having the following properties: (i) the number of random variables in the Markov network is equal to the number of first-order atoms in the MLN; and (ii) the domain size of each variable in the Markov network is equal to the number of groundings of the corresponding first-order atom. We show that inference over this Markov network is exponentially more efficient than ground inference, namely inference over the Markov network obtained by grounding all first-order atoms in the MLN. We improve this result further by showing that if non-shared MLNs contain no self joins, namely every atom appears at most once in each of its formulas, then all variables in the corresponding Markov network need only be bi-valued. Our approach is quite general and can be easily applied to an arbitrary MLN by simply grounding all of its shared terms. The key feature of our approach is that because we reduce lifted inference to propositional inference, we can use any propositional MAP inference algorithm for performing lifted MAP inference. Within our approach, we experimented with two propositional MAP inference algorithms: Gurobi and MaxWalkSAT. Our experiments on several benchmark MLNs clearly demonstrate our approach is superior to ground MAP inference in terms of scalability and solution quality."
1806,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Very efficient learning of structured classes of subsequential functions from positive data,"Adam Jardine, Jane Chandlee, R_mi Eyraud, Jeffrey Heinz","JMLR W&CP 34 :94-108, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/jardine14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_jardine14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/jardine14a.html,"In this paper, we present a new algorithm that can identify in polynomial time and data using positive examples any class of subsequential functions that share a particular finite-state structure. While this structure is given to the learner a priori , it allows for the exact learning of partial functions, and both the time and data complexity of the algorithm are linear. We demonstrate the algorithm on examples from natural language phonology and morphology in which the needed structure has been argued to be plausibly known in advance. A procedure for making any subsequential transducer onward without changing its structure is also presented."
1807,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Infinite Markov-Switching Maximum Entropy Discrimination Machines,Sotirios Chatzis,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chatzis13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chatzis13,http://jmlr.csail.mit.edu/proceedings/papers/v28/chatzis13.html,"In this paper, we present a method that combines the merits of Bayesian nonparametrics, specifically stick-breaking priors, and large-margin kernel machines in the context of sequential data classification. The proposed model postulates a set of (theoretically) infinite interdependent large-margin classifiers as model components, that robustly capture local nonlinearity of complex data. The postulated large-margin classifiers are connected in the context of a Markov-switching construction that allows for capturing complex temporal dynamics in the modeled datasets. Appropriate stick-breaking priors are imposed over the component switching mechanism of our model to allow for data-driven determination of the optimal number of component large-margin classifiers, under a standard nonparametric Bayesian inference scheme. Efficient model training is performed under the maximum entropy discrimination (MED) framework, which integrates the large-margin principle with Bayesian posterior inference. We evaluate our method using several real-world datasets, and compare it to state-of-the-art alternatives."
1808,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Concurrent Reinforcement Learning from Customer Interactions,"David Silver, Leonard Newnham, David Barker, Suzanne Weller, Jason McFall",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/silver13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_silver13,http://jmlr.csail.mit.edu/proceedings/papers/v28/silver13.html,"In this paper, we explore applications in which a company interacts concurrently with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for concurrent reinforcement learning, using a variant of temporal-difference learning to learn efficiently from partial interaction sequences. We evaluate our algorithms in two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records."
1809,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Spectral Clustering of Graphs with General Degrees in the Extended Planted Partition Model,"Kamalika Chaudhuri, Fan Chung and Alexander Tsiatas",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/chaudhuri12/chaudhuri12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_chaudhuri12,http://jmlr.csail.mit.edu/proceedings/papers/v23/chaudhuri12.html,"In this paper, we examine a spectral clustering algorithm for similarity graphs drawn from a simple random graph model, where nodes are allowed to have varying degrees, and we provide theoretical bounds on its performance. The random graph model we study is the Extended Planted Partition (EPP) model, a variant of the classical planted partition model. The standard approach to spectral clustering of graphs is to compute the bottom k singular vectors or eigenvectors of a suitable graph Laplacian, project the nodes of the graph onto these vectors, and then use an iterative clustering algorithm on the projected nodes. However a challenge with applying this approach to graphs generated from the EPP model is that unnormalized Laplacians do not work, and normalized Laplacians do not concentrate well when the graph has a number of low degree nodes. We resolve this issue by introducing the notion of a degree-corrected graph Laplacian. For graphs with many low degree nodes, degree correction has a regularizing effect on the Laplacian. Our spectral clustering algorithm projects the nodes in the graph onto the bottom k right singular vectors of the degree-corrected random-walk Laplacian, and clusters the nodes in this subspace. We show guarantees on the performance of this algorithm, demonstrating that it outputs the correct partition under a wide range of parameter values. Unlike some previous work, our algorithm does not require access to any generative parameters of the model."
1810,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,On the Relationship between Sum-Product Networks and Bayesian Networks,"Han Zhao, Mazen Melibari, Pascal Poupart",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaoc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaoc15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_zhaoc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/zhaoc15.html,"In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN."
1811,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Sparse Uncorrelated Linear Discriminant Analysis,"Xiaowei Zhang, Delin Chu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13.html,"In this paper, we develop a novel approach for sparse uncorrelated linear discriminant analysis (ULDA). Our proposal is based on characterization of all solutions of the generalized ULDA. We incorporate sparsity into the ULDA transformation by seeking the solution with minimum \(\ell_1\) -norm from all minimum dimension solutions of the generalized ULDA. The problem is then formulated as a \(\ell_{1}\) -minimization problem and is solved by accelerated linearized Bregman method. Experiments on high-dimensional gene expression data demonstrate that our approach not only computes extremely sparse solutions but also performs well in classification. Experimental results also show that our approach can help for data visualization in low-dimensional space."
1812,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Aligned Subtree Kernel for Weighted Graphs,"Lu Bai, Luca Rossi, Zhihong Zhang, Edwin Hancock",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bai15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/bai15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bai15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bai15.html,"In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depth-based representations. We demonstrate that this kernel can be seen as an aligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy."
1813,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Simple Homotopy Algorithm for Compressive Sensing,"Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhang15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_zhang15,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhang15.html,"In this paper, we consider the problem of recovering the \(s\) largest elements of an arbitrary vector from noisy measurements. Inspired by previous work, we develop an homotopy algorithm which solves the \(\ell_1\) -regularized least square problem for a sequence of decreasing values of the regularization parameter. Compared to the previous method, our algorithm is more efficient in the sense it only updates the solution once for each intermediate problem, and more practical in the sense it has a simple stopping criterion by checking the sparsity of the intermediate solution. Theoretical analysis reveals that our method enjoys a linear convergence rate in reducing the recovery error. Furthermore, our guarantee for recovering the top \(s\) elements of the target vector is tighter than previous results, and that for recovering the target vector itself matches the state of the art in compressive sensing."
1814,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Differentially Private Online Learning,"Prateek Jain, Pravesh Kothari and Abhradeep Thakurta",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/jain12/jain12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_jain12,http://jmlr.csail.mit.edu/proceedings/papers/v23/jain12.html,"In this paper, we consider the problem of preserving privacy in the context of online learning. Online learning involves learning from data in real-time, due to which the learned model as well as its predictions are continuously changing. This makes preserving privacy of each data point significantly more challenging as its effect on the learned model can be easily tracked by observing changes in the subsequent predictions. Furthermore, with more and more online systems (e.g. search engines like Bing, Google etc.) trying to learn their customers' behavior by leveraging their access to sensitive customer data (through cookies etc.), the problem of privacy preserving online learning has become critical. We study the problem in the framework of online convex programming (OCP) -- a popular online learning setting with several theoretical and practical implications -- while using differential privacy as the formal measure of privacy. For this problem, we provide a generic framework that can be used to convert any given OCP algorithm into a private OCP algorithm with provable privacy as well as regret guarantees (utility), provided that the given OCP algorithm satisfies the following two criteria: 1) linearly decreasing sensitivity, i.e., the effect of the new data points on the learned model decreases linearly, 2) sub-linear regret. We then illustrate our approach by converting two popular OCP algorithms into corresponding differentially private algorithms while guaranteeing ê(íT) regret for strongly convex functions. Next, we consider the practically important class of online linear regression problems, for which we generalize the approach by Dwork et al. (2010a) to provide a differentially private algorithm with just poly-log regret. Finally, we show that our online learning framework can be used to provide differentially private algorithms for the offline learning problem as well. For the offline learning problem, our approach guarantees better error bounds and is more practical than the existing state-of-the-art methods (Chaudhuri et al., 2011; Rubinstein et al., 2009)."
1815,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Saddle Points and Accelerated Perceptron Algorithms,"Adams Wei Yu, Fatma Kilinc-Karzan, Jaime Carbonell",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yuc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yuc14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yuc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yuc14.html,"In this paper, we consider the problem of finding a linear (binary) classifier or providing a near-infeasibility certificate if there is none. We bring a new perspective to addressing these two problems simultaneously in a single efficient process, by investigating a related Bilinear Saddle Point Problem (BSPP). More specifically, we show that a BSPP-based approach provides either a linear classifier or an \(\epsilon\) -infeasibility certificate. We show that the accelerated primal-dual algorithm, Mirror Prox, can be used for this purpose and achieves the best known convergence rate of \(O({\sqrt{\log n}\over\rho(A)})\) ( \(O({\sqrt{\log n}\over\epsilon})\) ), which is almost independent of the problem size , \(n\) . Our framework also solves kernelized and conic versions of the problem, with the same rate of convergence. We support our theoretical findings with an empirical study on synthetic and real data, highlighting the efficiency and numerical stability of our algorithms, especially on large-scale instances."
1816,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Differentially Private Learning with Kernels,"Prateek Jain, Abhradeep Thakurta",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jain13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/jain13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jain13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jain13.html,"In this paper, we consider the problem of differentially private learning where access to the training features is through a kernel function only. Existing work on this problem is restricted to translation invariant kernels only, where (approximate) training features are available explicitly. In fact, for general class of kernel functions and in general setting of releasing different private predictor ( \(\w^*\) ), the problem is impossible to solve . In this work, we relax the problem setting into three different easier but practical settings. In our first problem setting, we consider an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. This setting is prevalent in modern online systems like search engines, ad engines etc. In the second model, the learner sends back a differentially private version of the optimal parameter vector \(\w^*\) but requires access to a small subset of unlabeled test set beforehand. This also is a practical setting that involves two parties interacting through trusted third party. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of \(\w^*\) , but the kernels are restricted to efficiently computable functions over low-dimensional vector spaces. For each of the models, we derive differentially private learning algorithms with provable –utlity” or error bounds. Moreover, we show that our methods can also be applied to the traditional setting of . Here, our sample complexity bounds have only \(O(d^{1/3})\) dependence on the dimensionality \(d\) while existing methods require \(O(d^{1/2})\) samples to achieve same generalization error."
1817,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection,"Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yanga15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yanga15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yanga15.html,"In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is better than uniform sampling, and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms."
1818,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,PU Learning for Matrix Completion,"Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit Dhillon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsiehb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsiehb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hsiehb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hsiehb15.html,"In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only –likes” or –friendships” are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a –shifted matrix completion” method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a –biased matrix completion” method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds ã if \(M \in R^{n \times n}\) , the error is bounded as \(O(1-\rho)\) , where \(1-\rho\) denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks."
1819,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Community Detection via Random and Adaptive Sampling,"Se-Young Yun, Alexandre Proutiere","JMLR W&CP 35 :138-175, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/yun14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_yun14,http://jmlr.csail.mit.edu/proceedings/papers/v35/yun14.html,"In this paper, we consider networks consisting of a finite number of non-overlapping communities. To extract these communities, the interaction between pairs of nodes may be sampled from a large available data set, which allows a given node pair to be sampled several times. When a node pair is sampled, the observed outcome is a binary random variable, equal to 1 if nodes interact and to 0 otherwise. The outcome is more likely to be positive if nodes belong to the same communities. For a given budget of node pair samples or observations, we wish to jointly design a sampling strategy (the sequence of sampled node pairs) and a clustering algorithm that recover the hidden communities with the highest possible accuracy. We consider both non-adaptive and adaptive sampling strategies, and for both classes of strategies, we derive fundamental performance limits satisfied by any sampling and clustering algorithm. In particular, we provide necessary conditions for the existence of algorithms recovering the communities accurately as the network size grows large. We also devise simple algorithms that accurately reconstruct the communities when this is at all possible, hence proving that the proposed necessary conditions for accurate community detection are also sufficient. The classical problem of community detection in the stochastic block model can be seen as a particular instance of the problems consider here. But our framework covers more general scenarios where the sequence of sampled node pairs can be designed in an adaptive manner. The paper provides new results for the stochastic block model, and extends the analysis to the case of adaptive sampling."
1820,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Safe Screening of Non-Support Vectors in Pathwise SVM Computation,"Kohei Ogawa, Yoshiki Suzuki, Ichiro Takeuchi",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ogawa13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/ogawa13b.html,"In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost."
1821,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events,"Lisa Friedland, David Jensen, Michael Lavine",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/friedland13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/friedland13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_friedland13,http://jmlr.csail.mit.edu/proceedings/papers/v28/friedland13.html,"In this paper, we analyze the task of inferring rare links between pairs of entities that seem too similar to have occurred by chance. Variations of this task appear in such diverse areas as social network analysis, security, fraud detection, and entity resolution. To address the task in a general form, we propose a simple, flexible mixture model in which most entities are generated independently from a distribution but a small number of pairs are constrained to be similar. We predict the true pairs using a likelihood ratio that trades off the entitiesê similarity with their rarity. This method always outperforms using only similarity; however, with certain parameter settings, similarity turns out to be surprisingly competitive. Using real data, we apply the model to detect twins given their birth weights and to re-identify cell phone users based on distinctive usage patterns."
1822,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Principal Component Analysis on non-Gaussian Dependent Data,"Fang Han, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/han13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_han13,http://jmlr.csail.mit.edu/proceedings/papers/v28/han13.html,"In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han & Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes (m-dependency or a more general phi-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han & Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods."
1823,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Gaussian Process Optimization with Mutual Information,"Emile Contal, Vianney Perchet, Nicolas Vayatis",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/contal14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_contal14,http://jmlr.csail.mit.edu/proceedings/papers/v32/contal14.html,"In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic."
1824,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Approximate Slice Sampling for Bayesian Posterior Inference,"Christopher DuBois, Anoop Korattikara, Max Welling, Padhraic Smyth","JMLR W&CP 33 :185-193, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/dubois14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_dubois14,http://jmlr.csail.mit.edu/proceedings/papers/v33/dubois14.html,"In this paper, we advance the theory of large scale Bayesian posterior inference by introducing a new approximate slice sampler that uses only small mini-batches of data in every iteration. While this introduces a bias in the stationary distribution, the computational savings allow us to draw more samples in a given amount of time and reduce sampling variance. We empirically verify on three different models that the approximate slice sampling algorithm can significantly outperform a traditional slice sampler if we are allowed only a fixed amount of computing time for our simulations."
1825,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Online Learning for Time Series Prediction,"Oren Anava, Elad Hazan, Shie Mannor, Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anava13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Anava13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Anava13.html,"In this paper, we address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop effective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithmês performances asymptotically approaches the performance of the best ARMA model in hindsight."
1826,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Correlation Clustering in Data Streams,"KookJin Ahn, Graham Cormode, Sudipto Guha, Andrew McGregor, Anthony Wirth",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ahn15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ahn15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ahn15.html,"In this paper, we address the problem of correlation clustering in the dynamic data stream model. The stream consists of updates to the edge weights of a graph on \(n\) nodes and the goal is to find a node-partition such that the end-points of negative-weight edges are typically in different clusters whereas the end-points of positive-weight edges are typically in the same cluster. We present polynomial-time, \(O(n\cdot \text{polylog} n)\) -space approximation algorithms for natural problems that arise. We first develop data structures based on linear sketches that allow the –quality” of a given node-partition to be measured. We then combine these data structures with convex programming and sampling techniques to solve the relevant approximation problem. However the standard LP and SDP formulations are not obviously solvable in \(O(n\cdot \text{polylog} n)\) -space. Our work presents space-efficient algorithms for the convex programming required, as well as approaches to reduce the adaptivity of the sampling. Note that the improved space and running-time bounds achieved from streaming algorithms are also useful for offline settings such as MapReduce models."
1827,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression,"Divyanshu Vats, Richard Baraniuk","JMLR W&CP 33 :948-957, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/vats14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_vats14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/vats14a.html,"In this paper, we address the challenging problem of selecting tuning parameters for high-dimensional sparse regression. We propose a simple and computationally efficient method, called path thresholding PaTh, that transforms any tuning parameter-dependent sparse regression algorithm into an asymptotically tuning-free sparse regression algorithm. More specifically, we prove that, as the problem size becomes large (in the number of variables and in the number of observations), PaTh performs accurate sparse regression, under appropriate conditions, without specifying a tuning parameter. In finite-dimensional settings, we demonstrate that PaTh can alleviate the computational burden of model selection algorithms by significantly reducing the search space of tuning parameters."
1828,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Generalization Bound for Infinitely Divisible Empirical Process,"Chao Zhang, Dacheng Tao","15:864-872, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhang11b/zhang11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhang11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhang11b.html,In this paper we study the generalization bound for an empirical process of samples independently drawn from an infinitely divisible (ID) distribution which is termed as the ID empirical process. In particular based on a martingale method we develop deviation inequalities for the sequence of random variables of an ID distribution. By applying the obtained deviation inequalities we then show the generalization bound for ID empirical process based on the annealed Vapnik- Chervonenkis (VC) entropy. Afterward according to SauerÍs lemma we get the generalization bound for ID empirical process based on the VC dimension. Finally by using a resulted result bound we analyze the asymptotic convergence of ID empirical process and show that the convergence rate of ID empirical process can reach O((\frac{\Lambda_\mathcal{F}(2N)}{N})^{\frac{1}{1.3}}) and it is faster than the results of the generic i.i.d. empirical process (Vapnik 1999)
1829,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,L1 Covering Numbers for Uniformly Bounded Convex Functions,Adityanand Guntuboyina and Bodhisattva Sen,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/guntuboyina12/guntuboyina12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_guntuboyina12,http://jmlr.csail.mit.edu/proceedings/papers/v23/guntuboyina12.html,"In this paper we study the covering numbers of the space of convex and uniformly bounded functions in multi-dimension. We find optimal upper and lower bounds for the _-covering number M ( C ([ a, b ] d , B ), _, L 1 ) in terms of the relevant constants, where d _ 1, a _ b _ R , B _ 0, and C ([ a, b ] d , B ) denotes the set of all convex functions on [ a, b ] d that are uniformly bounded by B . We summarize previously known results on covering numbers for convex functions and also provide alternate proofs of some known results. Our results have direct implications in the study of rates of convergence of empirical minimization procedures as well as optimal convergence rates in the numerous convexity constrained function estimation problems."
1830,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Contextual Bandits with Linear Payoff Functions,"Wei Chu, Lihong Li, Lev Reyzin, Robert Schapire","15:208-214, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/chu11a/chu11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_chu11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/chu11a.html,In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions. For $T$ rounds $K$ actions and $d$ dimensional feature vectors we prove an $O(\sqrt{Td\ln^3(KT\ln(T)/\delta)})$ regret bound that holds with probability $1-\delta$ for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem. We also prove a lower bound of $\Omega(\sqrt{Td})$ for this setting matching the upper bound up to logarithmic factors.
1831,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Consistency of structured output learning with missing labels,"Kostiantyn Antoniuk, Vojtech Franc, Vaclav Hlavac",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Antoniuk15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Antoniuk15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Antoniuk15.html,"In this paper we study statistical consistency of partial losses suitable for learning structured output predictors from examples containing missing labels. We provide sufficient conditions on data generating distribution which admit to prove that the expected risk of the structured predictor learned by minimizing the partial loss converges to the optimal Bayes risk defined by an associated complete loss. We define a concept of surrogate classification calibrated partial losses which are easier to optimize yet their minimization preserves the statistical consistency. We give some concrete examples of surrogate partial losses which are classification calibrated. In particular, we show that the ramp-loss which is in the core of many existing algorithms is classification calibrated."
1832,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,"The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures","Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James Voss","JMLR W&CP 35 :1135-1164, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/anderson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_anderson14,http://jmlr.csail.mit.edu/proceedings/papers/v35/anderson14.html,"In this paper we show that very large mixtures of Gaussians are efficiently learnable in high dimension. More precisely, we prove that a mixture with known identical covariance matrices whose number of components is a polynomial of any fixed degree in the dimension \(n\) is polynomially learnable as long as a certain non-degeneracy condition on the means is satisfied. It turns out that this condition is generic in the sense of smoothed complexity, as soon as the dimensionality of the space is high enough. Moreover, we prove that no such condition can possibly exist in low dimension and the problem of learning the parameters is generically hard. In contrast, much of the existing work on Gaussian Mixtures relies on low-dimensional projections and thus hits an artificial barrier. Our main result on mixture recovery relies on a new ``Poissonization""-based technique, which transforms a mixture of Gaussians to a linear map of a product distribution. The problem of learning this map can be efficiently solved using some recent results on tensor decompositions and Independent Component Analysis (ICA), thus giving an algorithm for recovering the mixture. In addition, we combine our low-dimensional hardness results for Gaussian mixtures with Poissonization to show how to embed difficult instances of low-dimensional Gaussian mixtures into the ICA setting, thus establishing exponential information-theoretic lower bounds for underdetermined ICA in low dimension. To the best of our knowledge, this is the first such result in the literature. In addition to contributing to the problem of Gaussian mixture learning, we believe that this work is among the first steps toward better understanding the rare phenomenon of the ``blessing of dimensionality"" in the computational aspects of statistical inference."
1833,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Learning to rank with extremely randomized trees,P. Geurts & G. Louppe,"14:49_61, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/geurts11a/geurts11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_geurts11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/geurts11a.html,In this paper we report on our experiments on the Yahoo! Labs Learning to Rank challenge organized in the context of the 23rd International Conference of Machine Learning (ICML 2010). We competed in both the learning to rank and the transfer learning tracks of the challenge with several tree-based ensemble methods including Tree Bagging ( ? ) Random Forests ( ? ) and Extremely Randomized Trees ( ? ). Our methods ranked 10th in the _rst track and 4th in the second track. Although not at the very top of the ranking our results show that ensembles of randomized trees are quite competitive for the ñlearning to rankî problem. The paper also analyzes computing times of our algorithms and presents some post-challenge experiments with transfer learning methods.   Page last modified on Wed Jan 26 10:36:55 2011.
1834,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,A Finite-Sample Generalization Bound for Semiparametric Regression: Partially Linear Models,"Ruitong Huang, Csaba Szepesvari","JMLR W&CP 33 :402-410, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/huang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/huang14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_huang14,http://jmlr.csail.mit.edu/proceedings/papers/v33/huang14.html,"In this paper we provide generalization bounds for semiparametric regression with the so-called partially linear models where the regression function is written as the sum of a linear parametric and a nonlinear, nonparametric function, the latter taken from a some set \(\mathcal{H}\) with finite entropy-integral. The problem is technically challenging because the parametric part is unconstrained and the model is underdetermined, while the response is allowed to be unbounded with subgaussian tails. Under natural regularity conditions, we bound the generalization error as a function of the metric entropy of \(\mathcal{H}\) and the dimension of the linear model. Our main tool is a ratio-type concentration inequality for increments of empirical processes, based on which we are able to give an exponential tail bound on the size of the parametric component. We also provide a comparison to alternatives of this technique and discuss why and when the unconstrained parametric part in the model may cause a problem in terms of the expected risk. We also explain by means of a specific example why this problem cannot be detected using the results of classical asymptotic analysis often seen in the statistics literature."
1835,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,New Bounds on Compressive Linear Least Squares Regression,Ata Kaban,"JMLR W&CP 33 :448-456, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kaban14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kaban14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kaban14.html,"In this paper we provide a new analysis of compressive least squares regression that removes a spurious log N factor from previous bounds, where N is the number of training points. Our new bound has a clear interpretation and reveals meaningful structural properties of the linear regression problem that makes it solvable effectively in a small dimensional random subspace. In addition, the main part of our analysis does not require the compressive matrix to have the Johnson-Lindenstrauss property, or the RIP property. Instead, we only require its entries to be drawn i.i.d. from a 0-mean symmetric distribution with finite first four moments."
1836,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Dependent Normalized Random Measures,"Changyou Chen, Vinayak Rao, Wray Buntine, Yee Whye Teh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13i.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13i-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13i,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13i.html,"In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modelling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process."
1837,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Structured Sparse Canonical Correlation Analysis,"Xi Chen, Liu Han, Jaime Carbonell",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12a/chen12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_chen12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/chen12a.html,In this paper we propose to apply sparse canonical correlation analysis (sparse CCA) to an important genome-wide association study problem eQTL mapping. Existing sparse CCA models do not incorporate structural information among variables such as pathways of genes. This work extends the sparse CCA so that it could exploit either the pre-given or unknown group structure via the structured-sparsity-inducing penalty. Such structured penalty poses new challenge on optimization techniques. To address this challenge by specializing the excessive gap framework we develop a scalable primal-dual optimization algorithm with a fast rate of convergence. Empirical results show that the proposed optimization algorithm is more efficient than existing state-of-the-art methods. We also demonstrate the effectiveness of the structured sparse CCA on both simulated and genetic datasets.
1838,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Scalable Graph Building from Text Data,"Thibault Debatty, Pietro Michiardi, Olivier Thonnard, Wim Mees","JMLR W&CP 36 :120-132, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/debatty14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_debatty14,http://jmlr.csail.mit.edu/proceedings/papers/v36/debatty14.html,"In this paper we propose NNCTPH, a new MapReduce algorithm that is able to build an approximate k-NN graph from large text datasets. The algorithm uses a modified version of Context Triggered Piecewise Hashing to bin the input data into buckets, and uses an exhaustive search inside the buckets to build the graph. It also uses multiple stages to join the different unconnected subgraphs. We experimentally test the algorithm on different datasets consisting of the subject of spam emails. Although the algorithm is still at an early development stage, it already proves to be four times faster than a MapReduce implementation of NN-Descent, for the same quality of produced graph."
1839,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Nonparametric Estimation of Conditional Information and Divergences,"Barnabas Poczos, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/poczos12/poczos12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_poczos12,http://jmlr.csail.mit.edu/proceedings/papers/v22/poczos12.html,In this paper we propose new nonparametric estimators for a family of conditional mutual information and divergences. Our estimators are easy to compute; they only use simple k nearest neighbor based statistics. We prove that the proposed conditional information and divergence estimators are consistent under certain conditions and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well.
1840,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Robust Interactive Learning,Maria Florina Balcan and Steve Hanneke,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12c/balcan12c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_balcan12c,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12c.html,"In this paper we propose and study a generalization of the standard active-learning model where a more general type of queries including class conditional queries and mistake queries are allowed. Such queries have been quite useful in applications, but have been lacking theoretical understanding. In this work, we characterize the power of such queries under several well-known noise models. We give nearly tight upper and lower bounds on the number of queries needed to learn both for the general agnostic setting and for the bounded noise model. We further show that our methods can be made adaptive to the (unknown) noise rate, with only negligible loss in query complexity."
1841,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes,"Yves-Laurent Kom Samo, Stephen Roberts",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/samo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/samo15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_samo15,http://jmlr.csail.mit.edu/proceedings/papers/v37/samo15.html,"In this paper we propose an efficient, scalable non-parametric Gaussian process model for inference on Poisson point processes. Our model does not resort to gridding the domain or to introducing latent thinning points. Unlike competing models that scale as \(O(n^3)\) over n data points, our model has a complexity \(O(nk^2)\) where k __ n. We propose a MCMC sampler and show that the model obtained is faster, more accurate and generates less correlated samples than competing approaches on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches."
1842,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Choosing a Variable to Clamp,"Frederik Eaton, Zoubin Ghahramani","5:145-152, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/eaton09a/eaton09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_eaton09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/eaton09a.html,In this paper we propose an algorithm for approximate inference on graphical models based on belief propagation (BP). Our algorithm is an approximate version of Cutset Conditioning in which a set of variables is instantiated to make the rest of the graph singly connected. We relax the constraint of single-connectedness and select variables one at a time for conditioning running belief propagation after each selection. We consider the problem of determining the best variable to clamp at each level of recursion and propose a fast heuristic which applies backpropagation to the BP updates. We demonstrate that the heuristic performs better than selecting variables at random and give experimental results which show that it performs competitively with existing approximate inference algorithms.
1843,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Krylov Subspace Descent for Deep Learning,"Oriol Vinyals, Daniel Povey",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/vinyals12/vinyals12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_vinyals12,http://jmlr.csail.mit.edu/proceedings/papers/v22/vinyals12.html,In this paper we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix and then use a subset of the training data samples to optimize over this subspace. As with the Hessian Free (HF) method of Martens (2010) the Hessian matrix is never explicitly constructed and is computed using a subset of data. In practice as in HF we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix. We investigate the effectiveness of our proposed method on deep neural networks and compare its performance to widely used methods such as stochastic gradient descent conjugate gradient descent and L-BFGS and also to HF. Our method leads to faster convergence than either L-BFGS or HF and generally performs better than either of them in cross-validation accuracy. It is also simpler and more general than HF as it does not require a positive semidefinite approximation of the Hessian matrix to work well nor the setting of a damping parameter. The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.
1844,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Adaptive Step-size Policy Gradients with Average Reward Metric,"Takamitsu Matsubara, Tetsuro Morimura, and Jun Morimoto","13:285-298, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/matsubara10a/matsubara10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_matsubara10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/matsubara10a.html,In this paper we propose a novel adaptive step-size approach for policy gradient reinforcement learning. A new metric is defined for policy gradients that measures the effect of changes on average reward with respect to the policy parameters. Since the metric directly measures the effects on the average reward the resulting policy gradient learning employs an adaptive step-size strategy that can effectively avoid falling into a stagnant phase from the complex structure of the average reward function with respect to the policy parameters. Two algorithms are derived with the metric as variants of ordinary and natural policy gradients. Their properties are compared with previously proposed policy gradients through numerical experiments with simple but non-trivial 3-state Markov Decision Processes (MDPs). We also show performance improvements over previous methods in on-line learning with more challenging 20-state MDPs.
1845,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Boosted Optimization for Network Classification,"Timothy Hancock, Hiroshi Mamitsuka","9:305-312, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/hancock10a/hancock10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_hancock10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/hancock10a.html,In this paper we propose a new classification algorithm designed for application on complex networks motivated by algorithmic similarities between boosting learning and message passing. We consider a network classifier as a logistic regression where the variables define the nodes and the interaction effects define the edges. From this definition we represent the problem as a factor graph of local exponential loss functions. Using the factor graph representation it is possible to interpret the network classifier as an ensemble of individual node classifiers. We then combine ideas from boosted learning with network optimization algorithms to define two novel algorithms Boosted Expectation Propagation (BEP) and Boosted Message Passing (BMP). These algorithms optimize the global network classifier performance by locally weighting each node classifier by the error of the surrounding network structure. We compare the performance of BEP and BMP to logistic regression as well state of the art penalized logistic regression models on simulated grid structured networks. The results show that using local boosting to optimize the performance of a network classifier increases classification performance and is especially powerful in cases when the whole network structure must be considered for accurate classification.
1846,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Deterministic Annealing for Semi-Supervised Structured Output Learning,"Paramveer Dhillon, Sathiya Keerthi, Kedar Bellare, Olivier Chapelle, Sundararajan Sellamanickam",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/dhillon12/dhillon12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_dhillon12,http://jmlr.csail.mit.edu/proceedings/papers/v22/dhillon12.html,In this paper we propose a new approach for semi-supervised structured output learning. Our approach uses relaxed labeling on unlabeled data to deal with the combinatorial nature of the label space and further uses domain constraints to guide the learning. Since the overall objective is non-convex we alternate between the optimization of the model parameters and the label distribution of unlabeled data. The alternating optimization coupled with deterministic annealing helps us achieve better local optima and as a result our approach leads to better constraint satisfaction during inference. Experimental results on sequence labeling benchmarks show superior performance of our approach compared to Constraint Driven Learning (CoDL) and Posterior Regularization (PR).
1847,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Learning Polyhedral Classifiers Using Logistic Function,Naresh Manwani and P. S. Sastry,"13:17-30, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/manwani10a/manwani10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_manwani10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/manwani10a.html,In this paper we propose a new algorithm for learning polyhedral classifiers. In contrast to existing methods for learning polyhedral classifier which solve a constrained optimization problem our method solves an unconstrained optimization problem. Our method is based on a logistic function based model for the posterior probability function. We propose an alternating optimization algorithm namely SPLA1 (Single Polyhedral Learning Algorithm1) which maximizes the loglikelihood of the training data to learn the parameters. We also extend our method to make it independent of any user specified parameter (e.g. number of hyperplanes required to form a polyhedral set) in SPLA2. We show the effectiveness of our approach with experiments on various synthetic and real world datasets and compare our approach with a standard decision tree method (OC1) and a constrained optimization based method for learning polyhedral sets.
1848,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Generative Kernels for Exponential Families,"Arvind Agarwal, Hal Daum_ III","15:85-92, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/agarwal11b/agarwal11b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_agarwal11b,http://jmlr.csail.mit.edu/proceedings/papers/v15/agarwal11b.html,In this paper we propose a family of kernels for the data distributions belonging to the exponential family. We call these kernels generative kernels because they take into account the generative process of the data. Our proposed method considers the geometry of the data distribution to build a set of efficient closed-form kernels best suited for that distribution. We compare our generative kernels on multinomial data and observe improved empirical performance across the board. Moreover our generative kernels perform significantly better when training size is small an important property of the generative models.
1849,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Infinite Hierarchical Hidden Markov Models,"Katherine Heller, Yee Whye Teh, Dilan Gorur","5:224-231, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/heller09a/heller09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_heller09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/heller09a.html,In this paper we present the Infinite Hierarchical Hidden Markov Model (IHHMM) a nonparametric generalization of Hierarchical Hidden Markov Models (HHMMs). HHMMs have been used for modeling sequential data in applications such as speech recognition detecting topic transitions in video and extracting information from text. The IHHMM provides more flexible modeling of sequential data by allowing a potentially unbounded number of levels in the hierarchy instead of requiring the specification of a fixed hierarchy depth. Inference and learning are performed efficiently using Gibbs sampling and a modified forward-backtrack algorithm. We show encouraging demonstrations of the workings of the IHHMM.
1850,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Bayesian Probabilistic Models for Image Retrieval,"Vassilios Stathopoulos, Joemon M. Jose","17:41-47, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/stathopoulos11a/stathopoulos11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_stathopoulos11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/stathopoulos11a.html,In this paper we present new probabilistic ranking functions for content based image retrieval. Our methodology generalises previous approaches and is based on the predictive densities of generative probabilistic models modelling the density of image features. We evaluate the proposed methodology and compare it against two state of the art image retrieval systems using a well known image collection.
1851,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Cluster Canonical Correlation Analysis,"Nikhil Rasiwasia, Dhruv Mahajan, Vijay Mahadevan, Gaurav Aggarwal","JMLR W&CP 33 :823-831, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/rasiwasia14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_rasiwasia14,http://jmlr.csail.mit.edu/proceedings/papers/v33/rasiwasia14.html,"In this paper we present cluster canonical correlation analysis (cluster-CCA) for joint dimensionality reduction of two sets of data points. Unlike the standard pairwise correspondence between the data points, in our problem each set is partitioned into multiple clusters or classes, where the class labels define correspondences between the sets. Cluster-CCA is able to learn discriminant low dimensional representations that maximize the correlation between the two sets while segregating the different classes on the learned space. Furthermore, we present a kernel extension, kernel cluster canonical correlation analysis (cluster-KCCA) that extends cluster-CCA to account for non-linear relationships. Cluster-(K)CCA is shown to be computationally efficient, the complexity being similar to standard (K)CCA. By means of experimental evaluation on benchmark datasets, cluster-(K)CCA is shown to achieve state of the art performance for cross-modal retrieval tasks."
1852,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Using Descendants as Instrumental Variables for the Identification of Direct Causal Effects in Linear SEMs,"Hei Chan, Manabu Kuroki","9:73-80, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/chan10a/chan10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_chan10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/chan10a.html,In this paper we present an extended set of graphical criteria for the identification of direct causal effects in linear Structural Equation Models (SEMs). Previous methods of graphical identification of direct causal effects in linear SEMs include methods such as the single-door criterion the instrumental variable and the IV-pair and the accessory set. However there remain graphical models where a direct causal effect can be identified and these graphical criteria all fail. As a result we introduce a new set of graphical criteria which uses descendants of either the cause variable or the effect variable as ``path-specific instrumental variables'' for the identification of the direct causal effect as long as certain conditions are satisfied. These conditions are based on edge removal and the existing graphical criteria of instrumental variables and the identifiability of certain other total effects and thus can be easily checked.
1853,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Learning Interpretations Using Sequence Classification,Menno van Zaanen and Janneke van de Loo,"21:220-223, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/vanzaanen12a/vanzaanen12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_vanzaanen12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/vanzaanen12a.html,"In this paper we present a system that assigns interpretations, in the form of shallow semantic frame descriptions, to natural language sentences. The system searches for relevant patterns, consisting of words from the sentences, to identify the correct semantic frame and associated slot values. For each of these choices, a separate classifier is trained. Each classifier learns the boundaries between different languages, which each correspond to a particular class. The different classifiers each have their own viewpoint on the data depending on which aspect needs to be identified."
1854,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,CorrLog: Correlated Logistic Models for Joint Prediction of Multiple Labels,"Wei Bian, Bo Xie, Dacheng Tao",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/bian12/bian12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_bian12,http://jmlr.csail.mit.edu/proceedings/papers/v22/bian12.html,In this paper we present a simple but effective method for multi-label classification (MLC) termed Correlated Logistic Models (Corrlog) which extends multiple Independent Logistic Regressions (ILRs) by modeling the pairwise correlation between labels. Algorithmically we propose an efficient method for learning parameters of Corrlog which is based on regularized maximum pseudo-likelihood estimation and has a linear computational complexity with respect to the number of labels. Theoretically we show that Corrlog enjoys a satisfying generalization bound which is independent of the number of labels. The effectiveness of Corrlog on modeling label correlations is illustrated by a toy example and further experiments on real data show that Corrlog achieves competitive performance compared with popular MLC algorithms.
1855,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,CAKE: Convex Adaptive Kernel Density Estimation,"Ravi Sastry Ganti Mahapatruni, Alexander Gray","15:498-506, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/mahapatruni11a/mahapatruni11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mahapatruni11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/mahapatruni11a.html,In this paper we present a generalization of kernel density estimation called Convex Adaptive Kernel Density Estimation (CAKE) that replaces single bandwidth se- lection by a convex aggregation of kernels at all scales where the convex aggregation is allowed to vary from one training point to another treating the fundamental problem of heterogeneous smoothness in a novel way. Learning the CAKE estimator given a training set reduces to solving a single con- vex quadratic programming problem. We derive rates of convergence of CAKE like estimator to the true underlying density under smoothness assumptions on the class and show that given a sufficiently large sample the mean squared error of such estimators is optimal in a minimax sense. We also give a risk bound of the CAKE estimator in terms of its empirical risk. We empirically compare CAKE to other density estimators proposed in the statistics literature for handling heterogeneous smoothness on different synthetic and natural distributions.
1856,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation,"Frank Wood, Yee Whye Teh","5:607-614, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/wood09a/wood09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_wood09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/wood09a.html,In this paper we present a doubly hierarchical Pitman-Yor process language model. Its bottom layer of hierarchy consists of multiple hierarchical Pitman-Yor process language models one each for some number of domains. The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength. Intuitively this sharing results in the ?adaptation? of a latent shared language model to each domain. We introduce a general formalism capable of describing the overall model which we call the graphical Pitman-Yor process and explain how to perform Bayesian inference in it. We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry.
1857,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Online Learning of Search Heuristics,Michael Fink,"2:114-122, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/fink07a/fink07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_fink07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/fink07a.html,In this paper we learn heuristic functions that efficiently find the shortest path between two nodes in a graph. We rely on the fact that often several elementary admissible heuristics might be provided either by human designers or from formal domain abstractions. These simple heuristics are traditionally composed into a new admissible heuristic by selecting the highest scoring elementary heuristic in each distance evaluation. We suggest that learning a weighted sum over the elementary heuristics can often generate a heuristic with higher dominance than the heuristic defined by the highest score selection. The weights within our composite heuristic are trained in an online manner using nodes to which the true distance has already been revealed during previous search stages. Several experiments demonstrate that the proposed method typically finds the optimal path while significantly reducing the search complexity. Our theoretical analysis describes conditions under which finding the shortest path can be guaranteed.
1858,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Learning Dissimilarities for Categorical Symbols,"Jierui Xie, Boleslaw Szymanski and Mohammed Zaki","10:97-106, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/xie10a/xie10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_xie10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/xie10a.html,In this paper we learn a dissimilarity measure for categorical data for effective classification of the data points. Each categorical feature (with values taken from a finite set of symbols) is mapped onto a continuous feature whose values are real numbers. Guided by the classification error based on a nearest neighbor based technique we repeatedly update the assignment of categorical symbols to real numbers to minimize this error. Intuitively the algorithm pushes together points with the same class label while enlarging the distances to points labeled differently. Our experiments show that 1) the learned dissimilarities improve classification accuracy by using the affinities of categorical symbols; 2) they outperform dissimilarities produced by previous data-driven methods; 3) our enhanced nearest neighbor classifier (called LD) based on the new space is competitive compared with classifiers such as decision trees RBF neural networks Naive Bayes and support vector machines on a range of categorical datasets.
1859,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Evaluation Method for Feature Rankings and their Aggregations for Biomarker Discovery,"Ivica Slavkov, Bernard _enko, Sa_o D_eroski","8:122-135, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/slavkov10a/slavkov10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_slavkov10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/slavkov10a.html,"In this paper we investigate the problem of evaluating ranked lists of biomarkers which are typically an output of the analysis of high-throughput data. This can be a list of probes from microarray experiments which are ordered by the strength of their correlation to a disease. Usually the ordering of the biomarkers in the ranked lists varies a lot if they are a result of different studies or methods. Our work consists of two parts. First we propose a method for evaluating the ""correctness"" of the ranked lists. Second we conduct a preliminary study of different aggregation approaches of the feature rankings like aggregating rankings produced from different ranking algorithms and different datasets. We perform experiments on multiple public Neuroblastoma microarray studies. Our results show that there is a generally beneficial effect of aggregating feature rankings as compared to the ones produced by a single study or single method."
1860,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Fast Active-set-type Algorithms for L1-regularized Linear Regression,"Jingu Kim, Haesun Park","9:397-404, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/kim10a/kim10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_kim10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/kim10a.html,In this paper we investigate new active-set-type methods for l1-regularized linear regression that overcome some difficulties of existing active set methods. By showing a relationship between l1-regularized linear regression and the linear complementarity problem with bounds we present a fast active-set-type method called block principal pivoting. This method accelerates computation by allowing exchanges of several variables among working sets. We further provide an improvement of this method discuss its properties and also explain a connection to the structure learning of Gaussian graphical models. Experimental comparisons on synthetic and real data sets show that the proposed method is significantly faster than existing active set methods and competitive against recently developed iterative methods.
1861,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Actively Learning Probabilistic Subsequential Transducers,"Hasan Ibne Akram, Colin de La Higuera and Claudia Eckert","21:19-33, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/akram12a/akram12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_akram12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/akram12a.html,In this paper we investigate learning of probabilistic subsequential transducers in an active learning environment. In our learning algorithm the learner interacts with an oracle by asking probabilistic queries on the observed data. We prove our algorithm in an identification in the limit model. We also provide experimental evidence to show the correctness and to analyze the learnability of the proposed algorithm.
1862,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Maximum Variance Correction with Application to A* Search,"Wenlin Chen, Kilian Weinberger, Yixin Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13c,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13c.html,"In this paper we introduce Maximum Variance Correction (MVC), which finds large-scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as effective heuristics to speed-up A* search (Rayner et al. 2011). We demonstrate that MVC embeddings lead to un-matched reductions in search time across several non-trivial A* benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications."
1863,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Deep Gaussian Processes,"Andreas Damianou, Neil Lawrence",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/damianou13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_damianou13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/damianou13a.html,In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.
1864,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Minimum Conditional Entropy Clustering: A Discriminative Framework for Clustering,Bo Dai and Baogang Hu,"13:47-62, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/dai10a/dai10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_dai10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/dai10a.html,In this paper we introduce an assumption which makes it possible to extend the learning ability of discriminative model to unsupervised setting. We propose an information-theoretic framework as an implementation of the low-density separation assumption. The proposed framework provides a unified perspective of Maximum Margin Clustering (MMC) Discriminative k-means Spectral Clustering and Unsupervised Renyi_fs Entropy Analysis and also leads to a novel and efficient algorithm Accelerated Maximum Relative Margin Clustering (ARMC) which maximizes the margin while considering the spread of projections and affine invariance. Experimental results show that the proposed discriminative unsupervised learning method is more efficient in utilizing data and achieves the state-of-the-art or even better performance compared with mainstream clustering methods.
1865,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Controller Space,"Javier Almingol, Lui Montesano, Manuel Lopes",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/almingol13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/almingol13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_almingol13,http://jmlr.csail.mit.edu/proceedings/papers/v28/almingol13.html,"In this paper we introduce a method to learn multiple behaviors in the form of motor primitives from an unlabeled dataset. One of the difficulties of this problem is that in the measurement space, behaviors can be very mixed, despite existing a latent representation where they can be easily separated. We propose a mixture model based on Dirichlet Process (DP) to simultaneously cluster the observed time-series and recover a sparse representation of the behaviors using a Laplacian prior as the base measure of the DP. We show that for linear models, e.g potential functions generated by linear combinations of a large number of features, it is possible to compute analytically the marginal of the observations and derive an efficient sampler. The method is evaluated using robot behaviors and real data from human motion and compared to other techniques."
1866,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,The Coding Divergence for Measuring the Complexity of Separating Two Sets,Mahito Sugiyama and Akihiro Yamamoto,"13:127-143, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/sugiyama10b/sugiyama10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_sugiyama10b,http://jmlr.csail.mit.edu/proceedings/papers/v13/sugiyama10b.html,In this paper we integrate two essential processes discretization of continuous data and learning of a model that explains them towards fully computational machine learning from continuous data. Discretization is fundamental for machine learning and data mining since every continuous datum; e.g. a real-valued datum obtained by observation in the real world must be discretized and converted from analog (continuous) to digital (discrete) form to store in databases. However most machine learning methods do not pay attention to the situation; i.e. they use digital data in actual applications on a computer whereas assume analog data (usually vectors of real numbers) theoretically. To bridge the gap we propose a novel measure of the difference between two sets of data called the coding divergence and unify two processes discretization and learning computationally. Discretization of continuous data is realized by a topological mapping (in the sense of mathematics) from the d-dimensional Euclidean space into the Cantor space and the simplest model is learned in the Cantor space which corresponds to the minimum open set separating the given two sets of data. Furthermore we construct a classifier using the divergence and experimentally demonstrate robust performance of it. Our contribution is not only introducing a new measure from the computational point of view but also triggering more interaction between experimental science and machine learning.
1867,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Gibbs Collapsed Sampling for Latent Dirichlet Allocation on Spark,"Zhuolin Qiu, Bin Wu, Bai Wang, Le Yu","JMLR W&CP 36 :17-28, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/qiu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_qiu14,http://jmlr.csail.mit.edu/proceedings/papers/v36/qiu14.html,"In this paper we implement a collapsed Gibbs sampling method for the widely used latent Dirichlet allocation (LDA) model on Spark. Spark is a fast in-memory cluster computing framework for large-scale data processing, which has been the talk of the Big Data town for a while. It is suitable for iterative and interactive algorithm. Our approach splits the dataset into P*P partitions, shuffles and recombines these partitions into P sub-datasets using rules to avoid conflicts of sampling, where each of P sub-datasets only contains P partitions, and then parallel processes each sub-dataset one by one. Despite increasing the number of iterations, this method reduces data communication overhead, makes good use of Sparkês efficient iterative execution and results in significant speedup on large-scale datasets in our experiments."
1868,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Temporal Difference Methods for the Variance of the Reward To Go,"Aviv Tamar, Dotan Di Castro, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tamar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/tamar13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tamar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tamar13.html,"In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose variants of both TD(0) and LSTD( \(\lambda\) ) with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem."
1869,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Scale Invariant Conditional Dependence Measures,"Sashank J Reddi, Barnabas Poczos",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jreddi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/jreddi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jreddi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jreddi13.html,"In this paper we develop new dependence and conditional dependence measures and provide their estimators. An attractive property of these measures and estimators is that they are invariant to any monotone increasing transformations of the random variables, which is important in many applications including feature selection. Under certain conditions we show the consistency of these estimators, derive upper bounds on their convergence rates, and show that the estimators do not suffer from the curse of dimensionality. However, when the conditions are less restrictive, we derive a lower bound which proves that in the worst case the convergence can be arbitrarily slow similarly to some other estimators. Numerical illustrations demonstrate the applicability of our method."
1870,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Sequential Monte Carlo Samplers for Dirichlet Process Mixtures,"Yener Ulker, Bilge Gônsel, Taylan Cemgil","9:876-883, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ulker10a/ulker10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ulker10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ulker10a.html,In this paper we develop a novel online algorithm based on the Sequential Monte Carlo(SMC) samplers framework for posterior inference in Dirichlet Process Mixtures (DPM). Our method generalizes many sequential importance sampling approaches. It provides a computationally efficient improvement to particle filtering that is less prone to getting stuck in isolated modes. The proposed method is a particular SMC sampler that enables us to design sophisticated clustering update schemes such as updating past trajectories of the particles in light of recent observations and still ensures convergence to the true DPM target distribution asymptotically. Performance has been evaluated in a Bayesian Infinite Gaussian mixture density estimation problem and it is shown that the proposed algorithm outperforms conventional Monte Carlo approaches in terms of estimation variance and average log-marginal likelihood.
1871,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Supervised Sequential Classification Under Budget Constraints,"Kirill Trapeznikov, Venkatesh Saligrama",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/trapeznikov13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/trapeznikov13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_trapeznikov13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/trapeznikov13a.html,"In this paper we develop a framework for a sequential decision making under budget constraints for multi-class classification. In many classification systems, such as medical diagnosis and homeland security, sequential decisions are often warranted. For each instance, a sensor is first chosen for acquiring measurements and then based on the available information one decides (rejects) to seek more measurements from a new sensor/modality or to terminate by classifying the example based on the available information. Different sensors have varying costs for acquisition, and these costs account for delay, throughput or monetary value. Consequently, we seek methods for maximizing performance of the system subject to budget constraints. We formulate a multi-stage multi-class empirical risk objective and learn sequential decision functions from training data. We show that reject decision at each stage can be posed as supervised binary classification. We derive bounds for the VC dimension of the multi-stage system to quantify the generalization error. We compare our approach to alternative strategies on several multi-class real world datasets."
1872,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Stochastic Semi-supervised Learning,"J. Xie & T. Xiong ; 16:85_98, 2011. [ abs ] [ pdf ]","16:85_98, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/xie11a/xie11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_xie11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/xie11a.html,In this paper we describe the stochastic semi-supervised learning approach that we used in our submission to all six tasks in 2009-2010 Active Learning Challenge. The method is designed to tackle the binary classi_cation problem under the condition that the number of labeled data points is extremely small and the two classes are highly imbalanced. It starts with only one positive seed given by the contest organizer. We randomly pick additional unlabeled data points and treat them as ñnegativeî seeds based on the fact that the positive label is rare across all datasets. A classi_er is trained using the ñlabeledî data points and then is used to predict the unlabeled dataset. We take the _nal result to be the average of n stochastic iterations. Supervised learning was used as a large number of labels were purchased. Our approach is shown to work well in 5 out of 6 datasets. The overall results ranked 3rd in the contest.   Page last modified on Wed Mar 30 11:09:49 2011.
1873,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,O-IPCAC and its Application to EEG Classification,"Alessandro Rozza, Gabriele Lombardi, Marco Rosa and Elena Casiraghi","11:4-11, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/rozza10a/rozza10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_rozza10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/rozza10a.html,"In this paper we describe an online/incremental linear binary classifier based on an interesting approach to estimate the Fisher subspace. The proposed method allows to deal with datasets having high cardinality, being dynamically supplied, and it efficiently copes with high dimensional data without employing any dimensionality reduction technique. Moreover, this approach obtains promising classification performance even when the cardinality of the training set is comparable to the data dimensionality. We demonstrate the efficacy of our algorithm by testing it on EEG data. This classification problem is particularly hard since the data are high dimensional, the cardinality of the data is lower than the space dimensionality, and the classes are strongly unbalanced. The promising results obtained in the MLSP competition, without employing any feature extraction/selection step, have demonstrated that our method is effective; this is further proved both by our tests and by the comparison with other well-known classifiers."
1874,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Lower and Upper Bounds on the Generalization of Stochastic Exponentially Concave Optimization,"Mehrdad Mahdavi, Lijun Zhang, Rong Jin",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Mahdavi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Mahdavi15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Mahdavi15.html,"In this paper we derive high probability lower and upper bounds on the excess risk of stochastic optimization of exponentially concave loss functions. Exponentially concave loss functions encompass several fundamental problems in machine learning such as squared loss in linear regression, logistic loss in classification, and negative logarithm loss in portfolio management. We demonstrate an \(O(d \log T/T)\) upper bound on the excess risk of stochastic online Newton step algorithm, and an \(O(d/T)\) lower bound on the excess risk of any stochastic optimization method for squared loss , indicating that the obtained upper bound is optimal up to a logarithmic factor. The analysis of upper bound is based on recent advances in concentration inequalities for bounding self-normalized martingales, which is interesting by its own right, and the proof technique used to achieve the lower bound is a probabilistic method and relies on an information-theoretic minimax analysis."
1875,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Deterministic Annealing for Multiple-Instance Learning,"Peter V. Gehler, Olivier Chapelle","2:123-130, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/gehler07a/gehler07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_gehler07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/gehler07a.html,In this paper we demonstrate how deterministic annealing can be applied to different SVM formulations of the multiple-instance learning (MIL) problem. Our results show that we find better local minima compared to the heuristic methods those problems are usually solved with. However this does not always translate into a better test error suggesting an inadequacy of the objective function. Based on this finding we propose a new objective function which together with the deterministic annealing algorithm finds better local minima and achieves better performance on a set of benchmark datasets. Furthermore the results also show how the structure of MIL datasets influence the performance of MIL algorithms and we discuss how future benchmark datasets for the MIL problem should be designed.
1876,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,On Combining Graph-based Variance Reduction schemes,"Vibhav Gogate, Rina Dechter","9:257-264, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/gogate10a/gogate10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_gogate10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/gogate10a.html,In this paper we consider two variance reduction schemes that exploit the structure of the primal graph of the graphical model: Rao-Blackwellised w-cutset sampling and AND/OR sampling. We show that the two schemes are orthogonal and can be combined to further reduce the variance. Our combination yields a new family of estimators which trade time and space with variance. We demonstrate experimentally that the new estimators are superior often yielding an order of magnitude improvement over previous schemes on several benchmarks.
1877,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Multi-armed Bandit Problems with History,"Pannagadatta Shivaswamy, Thorsten Joachims",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/shivaswamy12/shivaswamy12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_shivaswamy12,http://jmlr.csail.mit.edu/proceedings/papers/v22/shivaswamy12.html,In this paper we consider the stochastic multi-armed bandit problem. However unlike in the conventional version of this problem we do not assume that the algorithm starts from scratch. Many applications offer observations of (some of) the arms even before the algorithm starts. We propose three novel multi-armed bandit algorithms that can exploit this data. An upper bound on the regret is derived in each case. The results show that a logarithmic amount of historic data can reduce regret from logarithmic to constant. The effectiveness of the proposed algorithms are demonstrated on a large-scale malicious URL detection problem.
1878,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Dynamic Policy Programming with Function Approximation,"Mohammad Gheshlaghi Azar, Vicenç G„mez, Bert Kappen","15:119-127, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/azar11a/azar11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_azar11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/azar11a.html,In this paper we consider the problem of planning in the infinite-horizon discounted-reward Markov decision problems. We propose a novel iterative method called dynamic policy programming (DPP) which updates the parametrized policy by a Bellman-like iteration. For discrete state-action case we establish sup-norm loss bounds for the performance of the policy induced by DPP and prove that it asymptotically converges to the optimal policy. Then we generalize our approach to large-scale (continuous) state-action problems using function approximation technique. We provide sup-norm performance-loss bounds for approximate DPP and compare these bounds with the standard results from approximate dynamic programming (ADP) showing that approximate DPP results in a tighter asymptotic bound than standard ADP methods. We also numerically compare the performance of DPP to other ADP and RL methods. We observe that approximate DPP asymptotically outperforms other methods on the mountain-car problem.
1879,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Online Stochastic Optimization under Correlated Bandit Feedback,"Mohammad Gheshlaghi azar, Alessandro Lazaric, Emma Brunskill",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/azar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/azar14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_azar14,http://jmlr.csail.mit.edu/proceedings/papers/v32/azar14.html,"In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel anytime \(\mathcal X\) -armed bandit algorithm, and derive regret bounds matching the performance of state-of-the-art algorithms in terms of the dependency on number of steps and the near-optimality dimension. The main advantage of HCT is that it handles the challenging case of correlated bandit feedback (reward), whereas existing methods require rewards to be conditionally independent. HCT also improves on the state-of-the-art in terms of the memory requirement, as well as requiring a weaker smoothness assumption on the mean-reward function in comparison with the existing anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results."
1880,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Open Problem: Learning Dynamic Network Models from a Static Snapshot,Jan Ramon and Constantin Comendant,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/ramon12/ramon12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_ramon12,http://jmlr.csail.mit.edu/proceedings/papers/v23/ramon12.html,"In this paper we consider the problem of learning a graph generating process given the evolving graph at a single point in time. Given a graph of sufficient size, can we learn the (repeatable) process that generated it? We formalize the generic problem and then consider two simple instances which are variations on the well-know graph generation models by Erd„s-R_nyi and Albert-Barabasi."
1881,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons,"Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, Inderjit Dhillon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/park15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/park15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_park15,http://jmlr.csail.mit.edu/proceedings/papers/v37/park15.html,"In this paper we consider the collaborative ranking setting: a pool of users each provides a set of pairwise preferences over a small subset of the set of d possible items; from these we need to predict each userês preferences for items s/he has not yet seen. We do so via fitting a rank r score matrix to the pairwise data, and provide two main contributions: (a) We show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as \(O(r \log^2 d)\) pairwise comparisons ã essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and also matching a lower bound we establish here. (b) We develop a large-scale non-convex implementation, which we call AltSVM, which trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and other measures of ranking performance."
1882,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Robust Cost Sensitive Support Vector Machine,"Shuichi Katsumata, Akiko Takeda",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/katsumata15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/katsumata15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_katsumata15,http://jmlr.csail.mit.edu/proceedings/papers/v38/katsumata15.html,"In this paper we consider robust classifications and show equivalence between the regularized classifications. In general, robust classifications are used to create a classifier robust to data by taking into account the uncertainty of the data. Our result shows that regularized classifications inherit robustness and provide reason on why some regularized classifications tend to be robust against data. Although most robust classification problems assume that every uncertain data lie within an identical bounded set, this paper considers a generalized model where the sizes of the bounded sets are different for each data. These models can be transformed into regularized classification models where the penalties for each data are assigned according to their losses. We see that considering such models opens up for new applications. For an example, we show that this robust classification technique can be used for Imbalanced Data Learning. We conducted experimentation with actual data and compared it with other IDL algorithms such as Cost Sensitive SVMs. This is a novel usage for the robust classification scheme and encourages it to be a suitable candidate for imbalanced data learning."
1883,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Online Learning in Markov Decision Processes with Changing Cost Sequences,"Travis Dick, Andras Gyorgy, Csaba Szepesvari",none,http://jmlr.org/proceedings/papers/v32/dick14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/dick14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_dick14,http://jmlr.csail.mit.edu/proceedings/papers/v32/dick14.html,"In this paper we consider online learning in finite Markov decision processes (MDPs) with changing cost sequences under full and bandit-information. We propose to view this problem as an instance of online linear optimization. We propose two methods for this problem: MD 2 (mirror descent with approximate projections) and the continuous exponential weights algorithm with Dikin walks. We provide a rigorous complexity analysis of these techniques, while providing near-optimal regret-bounds (in particular, we take into account the computational costs of performing approximate projections in MD 2 ). In the case of full-information feedback, our results complement existing ones. In the case of bandit-information feedback we consider the online stochastic shortest path problem, a special case of the above MDP problems, and manage to improve the existing results by removing the previous restrictive assumption that the state-visitation probabilities are uniformly bounded away from zero under all policies."
1884,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Passive Learning with Target Risk,"Mehrdad Mahdavi, Rong Jin",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Mahdavi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Mahdavi13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Mahdavi13.html,"In this paper we consider learning in passive setting but with a slight modification. We assume that the target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. Unlike most studies in the learning theory that only incorporate the prior knowledge into the generalization bounds, we are able to explicitly utilize the target risk in the learning process. Our analysis reveals a surprising result on the sample complexity of learning: by exploiting the target risk in the learning algorithm, we show that when the loss function is both strongly convex and smooth, the sample complexity reduces to \(\mathcal{O}(\log \left(\frac{1}{\epsilon}\right))\) , an exponential improvement compared to the sample complexity \(\mathcal{O}(\frac{1}{\epsilon})\) for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful."
1885,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Deterministic Policy Gradient Algorithms,"David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller",none,http://jmlr.org/proceedings/papers/v32/silver14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/silver14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_silver14,http://jmlr.csail.mit.edu/proceedings/papers/v32/silver14.html,"In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces."
1886,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Active Learning and Experimental Design with SVMs,"C.-H. Ho, M.-H. Tsai & C.-J. Lin ; 16:71_84, 2011. [ abs ] [ pdf ]","16:71_84, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/ho11a/ho11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_ho11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/ho11a.html,In this paper we consider active learning as a procedure of iteratively performing two steps: _rst we train a classi_er based on labeled and unlabeled data. Second we query labels of some data points. The _rst part is achieved mainly by standard classi_ers such as SVM and logistic regression. We develop additional techniques when there are very few labeled data. These techniques help to obtain good classi_ers in the early stage of the active learning procedure. In the second part based on SVM or logistic regression decision values we propose a framework to §exibly select points for query. We _nd that selecting points with various distances to the decision boundary is important but including more points close to the decision boundary further improves the performance. Our experiments are conducted on the data sets of Causality Active Learning Challenge. With measurements of Area Under Curve (AUC) and Area under the Learning Curve (ALC) we _nd suitable methods for di_erent data sets.   Page last modified on Wed Mar 30 11:09:38 2011.
1887,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Kernel Multi-task Learning using Task-specific Features,"Edwin V. Bonilla, Felix V. Agakov, Christopher K. I. Williams","2:43-50, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/bonilla07a/bonilla07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_bonilla07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/bonilla07a.html,In this paper we are concerned with multitask learning when task-specific features are available. We describe two ways of achieving this using Gaussian process predictors: in the first method the data from all tasks is combined into one dataset making use of the task-specific features. In the second method we train specific predictors for each reference task and then combine their predictions using a gating network. We demonstrate these methods on a compiler performance prediction problem where a task is defined as predicting the speed-up obtained when applying a sequence of code transformations to a given program.
1888,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Learning for Larger Datasets with the Gaussian Process Latent Variable Model,Neil D. Lawrence,"2:243-250, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/lawrence07a/lawrence07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_lawrence07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/lawrence07a.html,In this paper we apply the latest techniques in sparse Gaussian process regression (GPR) to the Gaussian process latent variable model (GPLVM). We review three techniques and discuss how they may be implemented in the context of the GP-LVM. Each approach is then implemented on a well known benchmark data set and compared with earlier attempts to sparsify the model.
1889,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Adaptive Hamiltonian and Riemann Manifold Monte Carlo,"Ziyu Wang, Shakir Mohamed, De Freitas Nando",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13e,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13e.html,"In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and demonstrate on several models and data sets that the use of our adaptive algorithms makes it is easy to obtain more efficient samplers, in some precluding the need for more complex models. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice."
1890,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Maximum Margin Learning with Incomplete Data: Learning Networks instead of Tables,"Sandor Szedmak, Yizhao Ni and Steve R. Gunn","11:96-102, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/szedmak10a/szedmak10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_szedmak10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/szedmak10a.html,"In this paper we address the problem of predicting when the available data is incomplete. We show that changing the generally accepted table-wise view of the sample items into a graph representable one allows us to solve these kind of problems in a very concise way by using the well known convex, one-class classification based, optimisation framework. The use of the one-class formulation in the learning phase and in the prediction as well makes the entire procedure highly consistent. The graph representation can express the complex interdependencies among the data sources. The underlying optimisation problem can be transformed into a on-line algorithm, e.g. a perceptron type one, and in this way it can deal with data sets of million items. This framework covers and encompasses supervised, semi-supervised and some unsupervised learning problems. Furthermore, the data sources can be chosen as not only simple binary variables or vectors but text documents, images or even graphs with complex internal structures."
1891,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Fuzzy Grammar-based Prediction of Amyloidogenic Regions,Olgierd Unold,"21:210-219, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/unold12a/unold12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_unold12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/unold12a.html,In this paper we address the problem of predicting the location of amyloidogenic regions in proteins. The language of protein sequence can be described by using a formal system such as fuzzy context-free grammar and the problem of amyloidogenic region recognition can be replaced by fuzzy grammar induction. The induced fuzzy grammar achieved 70.6% accuracy and 96.7% specificity on a recently published amyloidogenic dataset. Our results are comparable to other methods dedicated to recognize amyloid proteins.
1892,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,UPAL: Unbiased Pool Based Active Learning,"Ravi Ganti, Alexander Gray",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/ganti12/ganti12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_ganti12,http://jmlr.csail.mit.edu/proceedings/papers/v22/ganti12.html,In this paper we address the problem of pool based active learning and provide an algorithm called UPAL that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space. For the space of linear classifiers and the squared loss we show that UPAL is equivalent to an exponentially weighted average forecaster. Exploiting some recent results regarding the spectra of random matrices allows us to analyze UPAL with squared losses for the noiseless setting. Empirical comparison with an active learner implementation in Vowpal Wabbit and a previously proposed pool based active learner implementation show good empirical performance and better scalability.
1893,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,QWA: Spectral Algorithm,R. BAILLY,"20:147_163, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/bailly11/bailly11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_bailly11,http://jmlr.csail.mit.edu/proceedings/papers/v20/bailly11.html,In this paper we address the problem of non-parametric density estimation on a set of strings _   _  . We introduce a probabilistic model _ called quadratic weighted automaton or QWA _ and we present some methods which can be used in a density estimation task. A spectral analysis method leads to an e_ective regularization and a consistent estimate of the parameters. We provide a set of theoretical results on the convergence of this method. Experiments show that the combination of this method with likelihood maximization may be an interesting alternative to the well-known Baum-Welch algorithm.   Page last modified on Sun Nov 6 15:43:02 2011.
1894,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Induction of Directed Acyclic Word Graph in a Bioinformatics Task,"Wojciech Wieczorek, Olgierd Unold","JMLR W&CP 34 :207-217, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/wieczorek14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_wieczorek14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/wieczorek14a.html,"In this paper a new algorithm for the induction of a Directed Acyclic Word Graph (DAWG) is proposed. A DAWG can serve as a very efficient data structure for lexicon representation and fast string matching, and have a variety of applications. Similar structures are being investigated in the theory of formal languages and grammatical inference, namely deterministic and nondeterministic finite automata (DFA and NFA, respectively). Since a DAWG is acyclic the proposed method is suited for problems where the target language does not necessarily have to be infinite. The experiments have been performed for a dataset from the domain of bioinformatics, and our results are compared with those obtained using the current state-of-the-art methods in heuristic DFA induction."
1895,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Interactive Visual Big Data Analytics for Large Area Farm Biosecurity Monitoring: i-EKbase System,"Ritaban Dutta, Heiko Mueller, Daniel Smith, Aruneema Das, Jagannath Aryal",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/dutta15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_dutta15,http://jmlr.csail.mit.edu/proceedings/papers/v41/dutta15.html,In this industrial application paper a novel application of salad leaf disease detection has been developed using a combination of big data analytics and on field multi-dimensional sensing. We propose a cloud computing based intelligent big data analysis and interactive visual analytics platform to predict farm hot spots with high probability of potential biosecurity threats and early monitoring system aiming to save the farm from significant economic damage.
1896,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Trust Region Policy Optimization,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/schulman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/schulman15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_schulman15,http://jmlr.csail.mit.edu/proceedings/papers/v37/schulman15.html,"In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters."
1897,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Automating News Content Analysis: An Application to Gender Bias and Readability,"Omar Ali, Ilias Flaounas, Tijl De Bie, Nick Mosdell, Justin Lewis and Nello Cristianini","11:36-43, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/ali10a/ali10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_ali10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/ali10a.html,"In this article we present an application of text-analysis technologies to support social science research, in particular the analysis of patterns in news content. We describe a system that gathers and annotates large volumes of textual data in order to extract patterns and trends. We have examined 3.5 million news articles and show that their topic is related to the gender bias and readability of their content. This study is intended to illustrate how pattern analysis technology can be deployed to automate tasks commonly performed by humans in the social sciences, in order to enable large scale studies that would otherwise be impossible."
1898,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Streaming Variational Inference for Bayesian Nonparametric Mixture Models,"Alex Tank, Nicholas Foti, Emily Fox",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/tank15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/tank15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_tank15,http://jmlr.csail.mit.edu/proceedings/papers/v38/tank15.html,"In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity based on the amount of data observed. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms either are not applicable to streaming applications or are not extensible to nonparametric models. For the special case of Dirichlet processes, streaming inference has been considered. However, there is growing interest in more flexible BNP models, in particular building on the class of normalized random measures (NRMs). We work within this general framework and present a streaming variational inference algorithm for NRM mixture models based on assumed density filtering. Extensions to expectation propagation algorithms are possible in the batch data setting. We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora."
1899,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Estimating Unknown Sparsity in Compressed Sensing,Miles Lopes,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lopes13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/lopes13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lopes13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lopes13.html,"In the theory of compressed sensing (CS), the sparsity \(\|x\|_0\) of the unknown signal \(x\in\R^p\) is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of CS depend on knowing \(\|x\|_0\) , it is important to estimate this parameter in a data-driven way. A second practical concern is that \(\|x\|_0\) is a highly unstable function of \(x\) . In particular, for real signals with entries not exactly equal to 0, the value \(\|x\|_0=p\) is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity \(s(x):=\|x\|_1^2/\|x\|_2^2\) , which is a sharp lower bound on \(\|x\|_0\) . Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for \(s(x)\) is provided, and its width is shown to have no dependence on the signal dimension \(p\) . Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating \(s(x)\) . This is accomplished by proving that the minimax risk for estimating \(s(x)\) with deterministic measurements is large when \(n\ll p\) ."
1900,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learnability of the Superset Label Learning Problem,"Liping Liu, Thomas Dietterich",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/liug14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_liug14,http://jmlr.csail.mit.edu/proceedings/papers/v32/liug14.html,"In the Superset Label Learning (SLL) problem, weak supervision is provided in the form of a superset of labels that contains the true label. If the classifier predicts a label outside of the superset, it commits a superset error . Most existing SLL algorithms learn a multiclass classifier by minimizing the superset error. However, only limited theoretical analysis has been dedicated to this approach. In this paper, we analyze Empirical Risk Minimizing learners that use the superset error as the empirical risk measure. SLL data can arise either in the form of independent instances or as multiple-instance bags. For both scenarios, we give the conditions for ERM learnability and sample complexity for the realizable case."
1901,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Approachability in unknown games: Online learning meets multi-objective optimization,"Shie Mannor, Vianney Perchet, Gilles Stoltz","JMLR W&CP 35 :339-355, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/mannor14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_mannor14,http://jmlr.csail.mit.edu/proceedings/papers/v35/mannor14.html,"In the standard setting of approachability there are two players and a target set. The players play a repeated vector-valued game where one of them wants to have the average vector-valued payoff converge to the target set which the other player tries to exclude. We revisit the classical setting and consider the setting where the player has a preference relation between target sets: she wishes to approach the smallest (–best”) set possible given the observed average payoffs in hindsight. Moreover, as opposed to previous works on approachability, and in the spirit of online learning, we do not assume that there is a known game structure with actions for two players. Rather, the player receives an arbitrary vector-valued reward vector at every round. We show that it is impossible, in general, to approach the best target set in hindsight. We further propose a concrete strategy that approaches a non-trivial relaxation of the best-in-hindsight given the actual rewards. Our approach does not require projection onto a target set and amounts to switching between scalar regret minimization algorithms that are performed in episodes."
1902,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Putting MRFs on a Tensor Train,"Alexander Novikov, Anton Rodomanov, Anton Osokin, Dmitry Vetrov",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/novikov14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/novikov14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_novikov14,http://jmlr.csail.mit.edu/proceedings/papers/v32/novikov14.html,In the paper we present a new framework for dealing with probabilistic graphical models. Our approach relies on the recently proposed Tensor Train format (TT-format) of a tensor that while being compact allows for efficient application of linear algebra operations. We present a way to convert the energy of a Markov random field to the TT-format and show how one can exploit the properties of the TT-format to attack the tasks of the partition function estimation and the MAP-inference. We provide theoretical guarantees on the accuracy of the proposed algorithm for estimating the partition function and compare our methods against several state-of-the-art algorithms.
1903,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,"A Bayesian View of Challenges in Feature Selection: Feature Aggregation, Multiple Targets, Redundancy and Interaction","Peter Antal, Andras Millinghoffer, Gˆbor Hullˆm, Csaba Szalai, Andrˆs Falus","4:74-89, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/antal08a/antal08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_antal08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/antal08a.html,In the paper we discuss applications of the Bayesian approach to new challenges in relevance analysis. Earlier we formulated a Bayesian approach to Feature Subset Selection using Bayesian networks to jointly estimate the posteriors of Markov Blanket Memberships (MBMs) Markov Blanket Sets (MBSs) and Markov Blanket Graphs (MBGs) for a given target variable. These results of the Bayesian Multilevel Analysis of relevance (BMLA) correspond respectively to a model-based pairwise relevance relevance of sets and to the interaction models of relevant variables. Now we formulate refined levels in BMLA by introducing the concepts of k-MBSs and k-MBGs which are intermediate scalable model properties expressing relevance. We consider the extension of BMLA to multiple targets. We introduce and investigate a score for feature redundancy and interaction based on the decomposability of the structure posterior. Finally we overview the problems of conditional and contextual relevance. We demonstrate the use of concepts and methods in the field of genomics of asthma.
1904,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,The Rademacher Complexity of Co-Regularized Kernel Classes,"David S. Rosenberg, Peter L. Bartlett","2:396-403, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/rosenberg07a/rosenberg07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_rosenberg07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/rosenberg07a.html,"In the multi-view approach to semisupervised learning we choose one predictor from each of multiple hypothesis classes and we ""co-regularize"" our choices by penalizing disagreement among the predictors on the unlabeled data. We examine the co-regularization method used in the coregularized least squares (CoRLS) algorithm [12] in which the views are reproducing kernel Hilbert spaces (RKHS's) and the disagreement penalty is the average squared diffrence in predictions. The final predictor is the pointwise average of the predictors from each view. We call the set of predictors that can result from this procedure the co-regularized hypothesis class. Our main result is a tight bound on the Rademacher complexity of the co-regularized hypothesis class in terms of the kernel matrices of each RKHS. We find that the co-regularization reduces the Rademacher complexity by an amount that depends on the distance between the two views as measured by a data dependent metric. We then use standard techniques to bound the gap between training error and test error for the CoRLS algorithm. Experimentally we find that the amount of reduction in complexity introduced by co-regularization correlates with the amount of improvement that co-regularization gives in the CoRLS algorithm."
1905,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Continuous RAVE,"A. Cou ï toux, M. Milone, M. Brendel, H. Doghmen, M.S. & O. Teytaud","20:19_31, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/couetoux11/couetoux11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_couetoux11,http://jmlr.csail.mit.edu/proceedings/papers/v20/couetoux11.html,In the last decade Monte-Carlo Tree Search (MCTS) has revolutionized the domain of large-scale Markov Decision Process problems. MCTS most often uses the Upper Con_dence Tree algorithm to handle the exploration versus exploitation trade-o_ while a few heuristics are used to guide the exploration in large search spaces. Among these heuristics is Rapid Action Value  Estimate (RAVE). This paper is concerned with extending the RAVE heuristics to continuous action and state spaces. The approach is experimentally validated on two arti_cial benchmark problems: the treasure hunt game and a real-world energy management problem.   Page last modified on Sun Nov 6 15:42:00 2011.
1906,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,An investigation of imitation learning algorithms for structured prediction,Andreas Vlachos,"24:143-154, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/vlachos12a/vlachos12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_vlachos12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/vlachos12a.html,In the imitation learning paradigm algorithms learn from expert demonstrations in order to become able to accomplish a particular task. Daum_ III et al. [2009] framed structured prediction in this paradigm and developed the search-based structured prediction algorithm (Searn) which has been applied successfully to various natural language processing tasks with state-of-the-art performance. Recently Ross et al. [2011] proposed the dataset aggre- gation algorithm (DAgger) and compared it with Searn in sequential prediction tasks. In this paper we compare these two algorithms in the context of a more complex structured prediction task namely biomedical event extraction. We demonstrate that DAgger has more stable performance and faster learning than Searn and that these advantages are more pronounced in the parameter-free versions of the algorithms.
1907,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Inferring Graphs from Cascades: A Sparse Recovery Framework,"Jean Pouget-Abadie, Thibaut Horel",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/pouget-abadie15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/pouget-abadie15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_pouget-abadie15,http://jmlr.csail.mit.edu/proceedings/papers/v37/pouget-abadie15.html,"In the Graph Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graphês edges with high probability and O(s log m) measurements where s is the maximum degree of the graph and m is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of \(\Omega(s \log m/s)\) and validate our approach empirically on synthetic graphs."
1908,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Random Decision Hashing for Massive Data Learning,"Xiatian Zhang, Wei Fan, Nan Du",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/zhang15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_zhang15,http://jmlr.csail.mit.edu/proceedings/papers/v41/zhang15.html,"In the era of Big Data, the iterative nature of most traditional learning algorithms renders them increasingly inefficient to address large learning problems. Random decision trees algorithm is an efficient and decent learning algorithm, but the complexity of tree structure makes the algorithm inefficient for the big data problems. Inspired by the theoretical analyses of random decision trees, we propose a randomized algorithm for big data classification tasks based on unsupervised locality sensitive hashing. Our algorithm is essentially non-iterative, very flexible to deploy over clusters of machines, and thus able to handle large datasets efficiently. Experiments on real datasets demonstrate that the proposed algorithm can easily scale up to millions of data samples and features while still achieves at most 17% and 800% improvement in accuracy and efficiency respectively with moderate memory consumption over existing algorithms."
1909,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Complexity Theoretic Lower Bounds for Sparse Principal Component Detection,"Quentin Berthet, Philippe Rigollet",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Berthet13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Berthet13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Berthet13.html,"In the context of sparse principal component detection, we bring evidence towards the existence of a statistical price to pay for computational efficiency. We measure the performance of a test by the smallest signal strength that it can detect and we propose a computationally efficient method based on semidefinite programming. We also prove that the statistical performance of this test cannot be strictly improved by any computationally efficient method. Our results can be viewed as complexity theoretic lower bounds conditionally on the assumptions that some instances of the planted clique problem cannot be solved in randomized polynomial time."
1910,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Learning Where to Sample in Structured Prediction,"Tianlin Shi, Jacob Steinhardt, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/shi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_shi15,http://jmlr.csail.mit.edu/proceedings/papers/v38/shi15.html,"In structured prediction, most inference algorithms allocate a homogeneous amount of computation to all parts of the output, which can be wasteful when different parts vary widely in terms of difficulty. In this paper, we propose a heterogeneous approach that dynamically allocates computation to the different parts. Given a pre-trained model, we tune its inference algorithm (a sampler) to increase test-time throughput. The inference algorithm is parametrized by a meta-model and trained via reinforcement learning, where actions correspond to sampling candidate parts of the output, and rewards are log-likelihood improvements. The meta-model is based on a set of domain-general meta-features capturing the progress of the sampler. We test our approach on five datasets and show that it attains the same accuracy as Gibbs sampling but is 2 to 5 times faster."
1911,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors,"Junya Honda, Akimichi Takemura","JMLR W&CP 33 :375-383, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/honda14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/honda14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_honda14,http://jmlr.csail.mit.edu/proceedings/papers/v33/honda14.html,"In stochastic bandit problems, a Bayesian policy called Thompson sampling (TS) has recently attracted much attention for its excellent empirical performance. However, the theoretical analysis of this policy is difficult and its asymptotic optimality is only proved for one-parameter models. In this paper we discuss the optimality of TS for the model of normal distributions with unknown means and variances as one of the most fundamental examples of multiparameter models. First we prove that the expected regret of TS with the uniform prior achieves the theoretical bound, which is the first result to show that the asymptotic bound is achievable for the normal distribution model. Next we prove that TS with Jeffreys prior and reference prior cannot achieve the theoretical bound. Therefore choice of priors is important for TS and non-informative priors are sometimes risky in cases of multiparameter models."
1912,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,New Algorithms for Learning Incoherent and Overcomplete Dictionaries,"Sanjeev Arora, Rong Ge, Ankur Moitra","JMLR W&CP 35 :779-806, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/arora14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_arora14,http://jmlr.csail.mit.edu/proceedings/papers/v35/arora14.html,"In sparse recovery we are given a matrix \(A \in \mathbb{R}^{n\times m}\) (–the dictionary”) and a vector of the form \(A X\) where \(X\) is sparse , and the goal is to recover \(X\) . This is a central notion in signal processing, statistics and machine learning. But in applications such as sparse coding , edge detection, compression and super resolution, the dictionary \(A\) is unknown and has to be learned from random examples of the form \(Y = AX\) where \(X\) is drawn from an appropriate distribution ã this is the dictionary learning problem. In most settings, \(A\) is overcomplete : it has more columns than rows. This paper presents a polynomial-time algorithm for learning overcomplete dictionaries; the only previously known algorithm with provable guarantees is the recent work of Spielman et al. (2012) who who gave an algorithm for the undercomplete case, which is rarely the case in applications. Our algorithm applies to incoherent dictionaries which have been a central object of study since they were introduced in seminal work of Donoho and Huo (1999). In particular, a dictionary is \(\mu\) -incoherent if each pair of columns has inner product at most \(\mu / \sqrt{n}\) . The algorithm makes natural stochastic assumptions about the unknown sparse vector \(X\) , which can contain \(k \leq c \min(\sqrt{n}/\mu \log n, m^{1/2 - \eta})\) non-zero entries (for any \(\eta _ 0\) ). This is close to the best \(k\) allowable by the best sparse recovery algorithms even if one knows the dictionary \(A\) exactly . Moreover, both the running time and sample complexity depend on \(\log 1/\epsilon\) , where \(\epsilon\) is the target accuracy, and so our algorithms converge very quickly to the true dictionary. Our algorithm can also tolerate substantial amounts of noise provided it is incoherent with respect to the dictionary (e.g., Gaussian). In the noisy setting, our running time and sample complexity depend polynomially on \(1/\epsilon\) , and this is necessary."
1913,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Planning in Reward-Rich Domains via PAC Bandits,"Sergiu Goschin, Ari Weinstein, Michael L. Littman, Erick Chastain","24:25-42, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/goschin12a/goschin12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_goschin12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/goschin12a.html,"In some decision-making environments, successful solutions are common. If the evaluation of candidate solutions is noisy, however, the challenge is knowing when a ""good enough"" answer has been found. We formalize this problem as an infinite-armed bandit and provide upper and lower bounds on the number of evaluations or ""pulls"" needed to identify a solution whose evaluation exceeds a given threshold r0 . We present several algorithms and use them to identify reliable strategies for solving screens from the video games Infinite Mario and Pitfall ! We show order of magnitude improvements in sample complexity over a natural approach that pulls each arm until a good estimate of its success probability is known."
1914,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Near-Optimal Algorithms for Online Matrix Prediction,"Elad Hazan, Satyen Kale and Shai Shalev-Shwartz",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/hazan12b/hazan12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_hazan12b,http://jmlr.csail.mit.edu/proceedings/papers/v23/hazan12b.html,"In several online prediction problems of recent interest the comparison class is composed of matrices with bounded entries. For example, in the online max-cut problem, the comparison class is matrices which represent cuts of a given graph and in online gambling the comparison class is matrices which represent permutations over n teams. Another important example is online collaborative filtering in which a widely used comparison class is the set of matrices with a small trace norm. In this paper we isolate a property of matrices, which we call (_,_)-decomposability, and derive an efficient online learning algorithm, that enjoys a regret bound of ê(í __T ) for all problems in which the comparison class is composed of (_,_)-decomposable matrices. By analyzing the decomposability of cut matrices, low trace-norm matrices and triangular matrices, we derive near optimal regret bounds for online max-cut, online collaborative filtering and online gambling. In particular, this resolves (in the affirmative) an open problem posed by Abernethy (2010); Kleinberg et al. (2010). Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro (2011)."
1915,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost,"Ferdinando Cicalese, Eduardo Laber, Aline Medeiros Saettler",none,http://jmlr.org/proceedings/papers/v32/cicalese14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cicalese14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cicalese14.html,"In several applications of automatic diagnosis and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function. In general reading the value of a variable is done at the expense of some cost (computational or possibly a fee to pay the corresponding experiment). The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variablesê assignments). We provide an algorithm that builds a strategy (decision tree) with both expected cost and worst cost which are at most an \(O(\log n)\) factor away from, respectively, the minimum possible expected cost and the minimum possible worst cost. Our algorithm provides the best possible approximation simultaneously with respect to both criteria. In fact, there is no algorithm that can guarantee \(o(\log n)\) approximation, under the assumption that \({\cal P} \neq {\cal NP}\) ."
1916,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression,"Toby Hocking, Guillem Rigaill, Jean-Philippe Vert, Francis Bach",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/hocking13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_hocking13,http://jmlr.csail.mit.edu/proceedings/papers/v28/hocking13.html,"In segmentation models, the number of change-points is typically chosen using a penalized cost function. In this work, we propose to learn the penalty and its constants in databases of signals with weak change-point annotations. We propose a convex relaxation for the resulting interval regression problem, and solve it using accelerated proximal gradient methods. We show that this method achieves state-of-the-art change-point detection in a database of annotated DNA copy number profiles from neuroblastoma tumors."
1917,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Deeper Look at Planning as Learning from Replay,"Harm Vanseijen, Rich Sutton",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/vanseijen15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_vanseijen15,http://jmlr.csail.mit.edu/proceedings/papers/v37/vanseijen15.html,"In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation."
1918,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Improving Policy Gradient Estimates with In_uence Information,"J. Pinto, A. Fern, T. Bauer & M. Erwig","20:1_18, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/pinto11/pinto11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_pinto11,http://jmlr.csail.mit.edu/proceedings/papers/v20/pinto11.html,In reinforcement learning (RL) it is often possible to obtain sound but incomplete information about in§uences and independencies among problem variables and rewards even when an exact domain model is unknown. For example such information can be computed based on a partial qualitative domain model or via domain-speci_c analysis techniques. While intuitively such information appears useful for RL there are no algorithms that incorporate it in a sound way. In this work we describe how to leverage such information for improving the estimation of policy gradients which can be used to speedup gradient-based RL. We prove general conditions under which our estimator is unbiased and show that it will typically have reduced variance compared to standard unbiased gradient estimates. We evaluate the approach in the domain of Adaptation-Based Programming where RL is used to optimize the performance of programs and independence information can be computed via standard program analysis techniques. Incorporating independence information produces a large speedup in learning on a variety of adaptive programs.   Page last modified on Sun Nov 6 15:41:52 2011.
1919,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning by Stretching Deep Networks,"Gaurav Pandey, Ambedkar Dukkipati",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/pandey14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/pandey14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_pandey14,http://jmlr.csail.mit.edu/proceedings/papers/v32/pandey14.html,"In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called •stretchingê, that allows the same models to perform considerably better with very little training. We show that learning can be done tractably, even when the weight matrix is stretched to infinity, for some specific models. We also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence. Our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks. More importantly, for a fixed network architecture, one can achieve much better accuracy using stretching rather than learning the weights using backpropagation."
1920,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Low-density Parity Constraints for Hashing-Based Discrete Integration,"Stefano Ermon, Carla Gomes, Ashish Sabharwal, Bart Selman",none,http://jmlr.org/proceedings/papers/v32/ermon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/ermon14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_ermon14,http://jmlr.csail.mit.edu/proceedings/papers/v32/ermon14.html,"In recent years, a number of probabilistic inference and counting techniques have been proposed that exploit pairwise independent hash functions to infer properties of succinctly defined high-dimensional sets. While providing desirable statistical guarantees, typical constructions of such hash functions are themselves not amenable to efficient inference. Inspired by the success of LDPC codes, we propose the use of low-density parity constraints to make inference more tractable in practice. While not strongly universal, we show that such sparse constraints belong to a new class of hash functions that we call Average Universal. These weaker hash functions retain the desirable statistical guarantees needed by most such probabilistic inference methods. Thus, they continue to provide provable accuracy guarantees while at the same time making a number of algorithms significantly more scalable in practice. Using this technique, we provide new, tighter bounds for challenging discrete integration and model counting problems."
1921,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Maximum Mean Discrepancy for Class Ratio Estimation: Convergence Bounds and Kernel Selection,"Arun Iyer, Saketha Nath, Sunita Sarawagi",none,http://jmlr.org/proceedings/papers/v32/iyer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/iyer14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_iyer14,http://jmlr.csail.mit.edu/proceedings/papers/v32/iyer14.html,"In recent times, many real world applications have emerged that require estimates of class ratios in an unlabeled instance collection as opposed to labels of individual instances in the collection. In this paper we investigate the use of maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) for estimating such ratios. First, we theoretically analyze the MMD-based estimates. Our analysis establishes that, under some mild conditions, the estimate is statistically consistent. More importantly, it provides an upper bound on the error in the estimate in terms of intuitive geometric quantities like class separation and data spread. Next, we use the insights obtained from the theoretical analysis, to propose a novel convex formulation that automatically learns the kernel to be employed in the MMD-based estimation. We design an efficient cutting plane algorithm for solving this formulation. Finally, we empirically compare our estimator with several existing methods, and show significantly improved performance under varying datasets, class ratios, and training sizes."
1922,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Outlier Path: A Homotopy Algorithm for Robust SVM,"Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, Ichiro Takeuchi",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/suzumura14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/suzumura14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_suzumura14,http://jmlr.csail.mit.edu/proceedings/papers/v32/suzumura14.html,"In recent applications with massive but less reliable data (e.g., labels obtained by a semi-supervised learning method or crowdsourcing), non-robustness of the support vector machine (SVM) often causes considerable performance deterioration. Although improving the robustness of SVM has been investigated for long time, robust SVM (RSVM) learning still poses two major challenges: obtaining a good (local) solution from a non-convex optimization problem and optimally controlling the robustness-efficiency trade-off. In this paper, we address these two issues simultaneously in an integrated way by introducing a novel homotopy approach to RSVM learning. Based on theoretical investigation of the geometry of RSVM solutions, we show that a path of local RSVM solutions can be computed efficiently when the influence of outliers is gradually suppressed as simulated annealing. We experimentally demonstrate that our algorithm tends to produce better local solutions than the alternative approach based on the concave-convex procedure, with the ability of stable and efficient model selection for controlling the influence of outliers."
1923,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing,"Xi Chen, Qihang Lin, Dengyong Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13f.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13f-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13f,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13f.html,"In real crowdsourcing applications, each label from a crowd usually comes with a certain cost. Given a pre- fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However, DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed. We then extend the MDP framework to deal with inhomogeneous workers and tasks with contextual information available. The experiments on both simulated and real data demonstrate the superiority of our method."
1924,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Efficient Online Sparse Regression,Satyen Kale,"JMLR W&CP 35 :1299-1301, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kale14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kale14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/kale14b.html,"In practical scenarios, it is often necessary to be able to make predictions with very limited access to the features of any example. We provide one natural formulation as an online sparse regression problem with squared loss, and ask whether it is possible to achieve sublinear regret with efficient algorithms (i.e. polynomial running time in the natural parameters of the problem)."
1925,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Combinatorial Partial Monitoring Game with Linear Feedback and Its Applications,"Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, Wei Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lind14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lind14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lind14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lind14.html,"In online learning, a player chooses actions to play and receives reward and feedback from the environment with the goal of maximizing her reward over time. In this paper, we propose the model of combinatorial partial monitoring games with linear feedback, a model which simultaneously addresses limited feedback, infinite outcome space of the environment and exponentially large action space of the player. We present the Global Confidence Bound (GCB) algorithm, which integrates ideas from both combinatorial multi-armed bandits and finite partial monitoring games to handle all the above issues. GCB only requires feedback on a small set of actions and achieves \(O(T^{\frac{2}{3}}\log T)\) distribution-independent regret and \(O(\log T)\) distribution-dependent regret (the latter assuming unique optimal action), where \(T\) is the total time steps played. Moreover, the regret bounds only depend linearly on \(\log |X|\) rather than \(|X|\) , where \(X\) is the action space. GCB isolates offline optimization tasks from online learning and avoids explicit enumeration of all actions in the online learning part. We demonstrate that our model and algorithm can be applied to a crowdsourcing application leading to both an efficient learning algorithm and low regret, and argue that they can be applied to a wide range of combinatorial applications constrained with limited feedback."
1926,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,One-bit Compressed Sensing with the k-Support Norm,"Sheng Chen, Arindam Banerjee",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chen15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15a.html,"In one-bit compressed sensing (1-bit CS), one attempts to estimate a structured parameter (signal) only using the sign of suitable linear measurements. In this paper, we investigate 1-bit CS problems for sparse signals using the recently proposed k-support norm. We show that the new estimator has a closed-form solution, so no optimization is needed. We establish consistency and recovery guarantees of the estimator for both Gaussian and sub-Gaussian random measurements. For Gaussian measurements, our estimator is comparable to the best known in the literature, along with guarantees on support recovery. For sub-Gaussian measurements, our estimator has an irreducible error which, unlike existing results, can be controlled by scaling the measurement vectors. In both cases, our analysis covers the setting of model misspecification, i.e., when the true sparsity is unknown. Experimental results illustrate several strengths of the new estimator."
1927,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Probabilistic Models for Incomplete Multi-dimensional Arrays,"Wei Chu, Zoubin Ghahramani","5:89-96, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/chu09a/chu09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_chu09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/chu09a.html,In multiway data each sample is measured by multiple sets of correlated attributes. We develop a probabilistic framework for modeling structural dependency from partially observed multi-dimensional array data known as pTucker. Latent components associated with individual array dimensions are jointly retrieved while the core tensor is integrated out. The resulting algorithm is capable of handling large-scale data sets. We verify the usefulness of this approach by comparing against classical models on applications to modeling amino acid fluorescence collaborative filtering and a number of benchmark multiway array data.
1928,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Efficient Variational Inference for Gaussian Process Regression Networks,"Trung Nguyen, Edwin Bonilla",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/nguyen13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/nguyen13b-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_nguyen13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/nguyen13b.html,"In multi-output regression applications the correlations between the response variables may vary with the input space and can be highly non-linear. Gaussian process regression networks (GPRNs) are flexible and effective models to represent such complex adaptive output dependencies. However, inference in GPRNs is intractable. In this paper we propose two efficient variational inference methods for GPRNs. The first method, GPRN-MF, adopts a mean-field approach with full Gaussians over the GPRNês parameters as its factorizing distributions. The second method, GPRN-NPV, uses a nonparametric variational inference approach. We derive analytical forms for the evidence lower bound on both methods, which we use to learn the variational parameters and the hyper-parameters of the GPRN model. We obtain closed-form updates for the parameters of GPRN-MF and show that, while having relatively complex approximate posterior distributions, our approximate methods require the estimation of O(N) variational parameters rather than O(N2) for the parametersê covariances. Our experiments on real data sets show that GPRN-NPV may give a better approximation to the posterior distribution compared to GPRN-MF, in terms of both predictive performance and stability."
1929,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Analogy-preserving Semantic Embedding for Visual Object Categorization,"Sung Ju Hwang, Kristen Grauman, Fei Sha",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/juhwang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_juhwang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/juhwang13.html,"In multi-class categorization tasks, knowledge about the classesê semantic relationships can provide valuable information beyond the class labels themselves. However, existing techniques focus on preserving the semantic distances between classes (e.g., according to a given object taxonomy for visual recognition), limiting the influence to pairwise structures. We propose to model analogies that reflect the relationships between multiple pairs of classes simultaneously, in the form `` \(p\) is to \(q\) , as \(r\) is to \(s\) –”. We translate semantic analogies into higher-order geometric constraints called analogical parallelograms , and use them in a novel convex regularizer for a discriminatively learned label embedding. Furthermore, we show how to discover analogies from attribute-based class descriptions, and how to prioritize those likely to reduce inter-class confusion. Evaluating our Analogy-preserving Semantic Embedding (ASE) on two visual recognition datasets, we demonstrate clear improvements over existing approaches, both in terms of recognition accuracy and analogy completion."
1930,14,http://jmlr.csail.mit.edu/proceedings/papers/v14/,Web-Search Ranking with Initialized Gradient Boosted Regression Trees,"A. Mohan, Z. Chen & K. Weinberger","14:77_89, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v14/mohan11a/mohan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v14/,,26th January 2011,"June 25, 2010,",Proceedings of the Learning to Rank Challenge,Proceedings of the Yahoo! Learning to Rank Challenge,"Haifa, Israel","Olivier Chapelle, Yi Chang, Tie-Yan Liu",v14_mohan11a,http://jmlr.csail.mit.edu/proceedings/papers/v14/mohan11a.html,In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge . This paper summarizes the approach by the highly placed team Washington University in St. Louis . We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We demonstrate that it yields surprisingly accurate ranking results „ comparable to or better than GBRT. We combine the two algorithms by _rst learning a ranking function with RF and using it as initialization for GBRT. We refer to this setting as iGBRT. Following a recent discussion by ?  we show that the results of iGBRT can be improved upon even further when the web-search ranking task is cast as classi_cation instead of regression. We provide an upper bound of the Expected Reciprocal Rank ( ? ) in terms of classi_cation error and demonstrate that iGBRT outperforms GBRT and RF on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising consistency.   Page last modified on Wed Jan 26 10:37:05 2011.
1931,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Budget Allocation Problem with Multiple Advertisers: A Game Theoretic View,"Takanori Maehara, Akihiro Yabe, Ken-ichi Kawarabayashi",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/maehara15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/maehara15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_maehara15,http://jmlr.csail.mit.edu/proceedings/papers/v37/maehara15.html,"In marketing planning, advertisers seek to maximize the number of customers by allocating given budgets to each media channel effectively. The budget allocation problem with a bipartite influence model captures this scenario; however, the model is problematic because it assumes there is only one advertiser in the market. In reality, there are many advertisers which are in conflict of advertisement; thus we must extend the model for such a case. By extending the budget allocation problem with a bipartite influence model, we propose a game-theoretic model problem that considers many advertisers. By simulating our model, we can analyze the behavior of a media channel market, e.g., we can estimate which media channels are allocated by an advertiser, and which customers are influenced by an advertiser. Our model has many attractive features. First, our model is a potential game; therefore, it has a pure Nash equilibrium. Second, any Nash equilibrium of our game has 2-optimal social utility, i.e., the price of anarchy is 2. Finally, the proposed model can be simulated very efficiently; thus it can be used to analyze large markets."
1932,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Gaussian Processes for time-marked time-series data,"John Cunningham, Zoubin Ghahramani, Carl Rasmussen",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/cunningham12/cunningham12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_cunningham12,http://jmlr.csail.mit.edu/proceedings/papers/v22/cunningham12.html,In many settings data is collected as multiple time series where each recorded time series is an observation of some underlying dynamical process of interest. These observations are often time-marked with known event times and one desires to do a range of standard analyses. When there is only one time marker one simply aligns the observations temporally on that marker. When multiple time-markers are present and are at different times on different time series observations these analyses are more difficult. We describe a Gaussian Process model for analyzing multiple time series with multiple time markings and we test it on a variety of data.
1933,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques,"J_r_mie Mary, Philippe Preux, Olivier Nicol",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mary14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mary14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mary14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mary14.html,"In many recommendation applications such as news recommendation, the items that can be recommended come and go at a very fast pace. This is a challenge for recommender systems (RS) to face this setting. Online learning algorithms seem to be the most straight forward solution. The contextual bandit framework was introduced for that very purpose. In general the evaluation of a RS is a critical issue. Live evaluation is often avoided due to the potential loss of revenue, hence the need for offline evaluation methods. Two options are available. Model based methods are biased by nature and are thus difficult to trust when used alone. Data driven methods are therefore what we consider here. Evaluating online learning algorithms with past data is not simple but some methods exist in the literature. Nonetheless their accuracy is not satisfactory mainly due to their mechanism of data rejection that only allow the exploitation of a small fraction of the data. We precisely address this issue in this paper. After highlighting the limitations of the previous methods, we present a new method, based on bootstrapping techniques. This new method comes with two important improvements: it is much more accurate and it provides a measure of quality of its estimation. The latter is a highly desirable property in order to minimize the risks entailed by putting online a RS for the first time. We provide both theoretical and experimental proofs of its superiority compared to state-of-the-art methods, as well as an analysis of the convergence of the measure of quality."
1934,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Classification with Asymmetric Label Noise: Consistency and Maximal Denoising,"Clayton Scott, Gilles Blanchard, Gregory Handy",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Scott13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Scott13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Scott13.html,"In many real-world classification problems, the labels of training examples are randomly corrupted. Thus, the set of training examples for each class is contaminated by examples of the other class. Previous theoretical work on this problem assumes that the two classes are separable, that the label noise is independent of the true class label, or that the noise proportions for each class are known. We introduce a general framework for classification with label noise that eliminates these assumptions. Instead, we give assumptions ensuring identifiability and the existence of a consistent estimator of the optimal risk, with associated estimation strategies. For any arbitrary pair of contaminated distributions, there is a unique pair of non-contaminated distributions satisfying the proposed assumptions, and we argue that this solution corresponds in a certain sense to maximal denoising. In particular, we find that learning in the presence of label noise is possible even when the class-conditional distributions overlap and the label noise is not symmetric. A key to our approach is a universally consistent estimator of the maximal proportion of one distribution that is present in another, a problem we refer to as–mixture proportion estimation. This work is motivated by a problem in nuclear particle classification."
1935,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning and Evaluation in Presence of Non-i.i.d. Label Noise,"Nico G_rnitz, Anne Porbadnigk, Alexander Binder, Claudia Sannelli, Mikio Braun, Klaus-Robert Mueller, Marius Kloft","JMLR W&CP 33 :293-302, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/gornitz14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/gornitz14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_gornitz14,http://jmlr.csail.mit.edu/proceedings/papers/v33/gornitz14.html,"In many real-world applications, the simplified assumption of independent and identically distributed noise breaks down, and labels can have structured, systematic noise. For example, in brain-computer interface applications, training data is often the result of lengthy experimental sessions, where the attention levels of participants can change over the course of the experiment. In such application cases, structured label noise will cause problems because most machine learning methods assume independent and identically distributed label noise. In this paper, we present a novel methodology for learning and evaluation in presence of systematic label noise. The core of which is a novel extension of support vector data description / one-class SVM that can incorporate latent variables. Controlled simulations on synthetic data and a real-world EEG experiment with 20 subjects from the domain of brain-computer-interfacing show that our method achieves accuracies that go beyond the state of the art."
1936,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Clustered Support Vector Machines,"Quanquan Gu, Jiawei Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/gu13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_gu13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/gu13b.html,"In many problems of machine learning, the data are distributed nonlinearly. One way to address this kind of data is training a nonlinear classifier such as kernel support vector machine (kernel SVM). However, the computational burden of kernel SVM limits its application to large scale datasets. In this paper, we propose a Clustered Support Vector Machine (CSVM), which tackles the data in a divide and conquer manner. More specifically, CSVM groups the data into several clusters, followed which it trains a linear support vector machine in each cluster to separate the data locally. Meanwhile, CSVM has an additional global regularization, which requires the weight vector of each local linear SVM aligning with a global weight vector. The global regularization leverages the information from one cluster to another, and avoids over-fitting in each cluster. We derive a data-dependent generalization error bound for CSVM, which explains the advantage of CSVM over linear SVM. Experiments on several benchmark datasets show that the proposed method outperforms linear SVM and some other related locally linear classifiers. It is also comparable to a fine-tuned kernel SVM in terms of prediction performance, while it is more efficient than kernel SVM."
1937,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Baseline Methods for Active Learning,"G.C. Cawley ; 16:47_57, 2011. [ abs ] [ pdf ]","16:47_57, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/cawley11a/cawley11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_cawley11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/cawley11a.html,In many potential applications of machine learning unlabelled data are abundantly available at low cost but there is a paucity of labelled data and labeling unlabelled examples is expensive and/or time-consuming. This motivates the development of active learning methods that seek to direct the collection of labelled examples such that the greatest performance gains can be achieved using the smallest quantity of labelled data. In this paper we describe some simple pool-based active learning strategies based on optimally regularised linear [kernel] ridge regression providing a set of baseline submissions for the Active Learning Challenge. A simple random strategy where unlabelled patterns are submitted to the oracle purely at random is found to be surprisingly e_ective being competitive with more complex approaches.   Page last modified on Wed Mar 30 11:09:12 2011.
1938,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Dynamic Sensing: Better Classification under Acquisition Constraints,"Oran Richman, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/richman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_richman15,http://jmlr.csail.mit.edu/proceedings/papers/v37/richman15.html,"In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sampleês quality. In most cases this option remains unused and the dataês quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problemês model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems."
1939,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Online Local Learning,Paul Christiano,"JMLR W&CP 35 :1290-1294, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/christiano14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_christiano14,http://jmlr.csail.mit.edu/proceedings/papers/v35/christiano14.html,"In many learning problems, we attempt to infer global structure in the interest of making local predictions. For example, we might try to infer the skills of the competitors in a tournament in order to predict who will win a match, or we might try to predict characteristics of users and films in order to predict which users will like which films. In even relatively simple settings of this type, it is typically NP-hard to find the latent data which best explain some observations. But do these complexity-theoretic obstructions actually prevent us from making good predictions? Because each prediction depends on only a small number of variables, it might be possible to make good predictions without actually finding a good global assignment. This may seem to be a purely technical distinction, but recent work has shown that several local prediction problems actually are easy even though the corresponding global inference problem is hard. The question we pose is: how general is this phenomenon?"
1940,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Making the Most of Bag of Words: Sentence Regularization with Alternating Direction Method of Multipliers,"Dani Yogatama, Noah Smith",none,http://jmlr.org/proceedings/papers/v32/yogatama14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yogatama14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yogatama14.html,"In many high-dimensional learning problems, only some parts of an observation are important to the prediction task; for example, the cues to correctly categorizing a document may lie in a handful of its sentences. We introduce a learning algorithm that exploits this intuition by encoding it in a regularizer. Specifically, we apply the sparse overlapping group lasso with one group for every bundle of features occurring together in a training-data sentence, leading to thousands to millions of overlapping groups. We show how to efficiently solve the resulting optimization challenge using the alternating directions method of multipliers. We find that the resulting method significantly outperforms competitive baselines (standard ridge, lasso, and elastic net regularizers) on a suite of real-world text categorization problems."
1941,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Active Learning for Multi-Objective Optimization,"Marcela Zuluaga, Guillaume Sergent, Andreas Krause, Markus Pôschel",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zuluaga13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zuluaga13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zuluaga13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zuluaga13.html,"In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal tradeoffs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm, which intelligently samples the design space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on PALês sampling cost required to achieve a desired accuracy. Further, we show an experimental evaluation on three real-world data sets. The results show PALês effectiveness; in particular it improves significantly over a state-of-the-art evolutionary algorithm, saving in many cases about 33%."
1942,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Statistical test for consistent estimation of causal effects in linear non-Gaussian models,"Doris Entner, Patrik Hoyer, Peter Spirtes",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/entner12/entner12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_entner12,http://jmlr.csail.mit.edu/proceedings/papers/v22/entner12.html,In many fields of science researchers are faced with the problem of estimating causal effects from non-experimental data. A key issue is to avoid inconsistent estimators due to confounding by measured or unmeasured covariates a problem commonly solved by 'adjusting for' a subset of the observed variables. When the data generating process can be represented by a directed acyclic graph and this graph structure is known there exist simple graphical procedures for determining which subset of covariates should be adjusted for to obtain consistent estimators of the causal effects. However when the graph is not known no general and complete procedures for this task are available. In this paper we introduce such a method for linear non-Gaussian models requiring only partial knowledge about the temporal ordering of the variables: We provide a simple statistical test for inferring whether an estimator of a causal effect is consistent when controlling for a subset of measured covariates and we present heuristics to search for such a set. We show empirically that this statistical test identifies consistent vs inconsistent estimates and that the search heuristics outperform the naive approach of adjusting for all observed covariates.
1943,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Competing with the Empirical Risk Minimizer in a Single Pass,"Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Frostig15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Frostig15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Frostig15.html,"In many estimation problems, e.g. linear and logistic regression, we wish to minimize an unknown objective given only unbiased samples of the objective function. Furthermore, we aim to achieve this using as few samples as possible. In the absence of computational constraints, the minimizer of a sample average of observed data _ commonly referred to as either the empirical risk minimizer (ERM) or the \(M\) -estimator _ is widely regarded as the estimation strategy of choice due to its desirable statistical convergence properties. Our goal in this work is to perform as well as the ERM, on every problem, while minimizing the use of computational resources such as running time and space usage. We provide a simple streaming algorithm which, under standard regularity assumptions on the underlying problem, enjoys the following properties: The algorithm can be implemented in linear time with a single pass of the observed data, using space linear in the size of a single sample. The algorithm achieves the same statistical rate of convergence as the empirical risk minimizer on every problem, even considering constant factors. The algorithmês performance depends on the initial error at a rate that decreases super-polynomially. The algorithm is easily parallelizable. Moreover, we quantify the (finite-sample) rate at which the algorithm becomes competitive with the ERM."
1944,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Performance Guarantees for Information Theoretic Active Inference,"Jason L. Williams, John W. Fisher III, Alan S. Willsky","2:620-627, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/williams07a/williams07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_williams07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/williams07a.html,In many estimation problems the measurement process can be actively controlled to alter the information received. The control choices made in turn determine the performance that is possible in the underlying inference task. In this paper we discuss performance guarantees for heuristic algorithms for adaptive measurement selection in sequential estimation problems where the inference criterion is mutual information. We also demonstrate the performance of our tighter online computable performance guarantees through computational simulations.
1945,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Support Matrix Machines,"Luo Luo, Yubo Xie, Zhihua Zhang, Wu-Jun Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/luo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/luo15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_luo15,http://jmlr.csail.mit.edu/proceedings/papers/v37/luo15.html,"In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods."
1946,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Convex Collective Matrix Factorization,"Guillaume Bouchard, Dawei Yin, Shengbo Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/bouchard13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_bouchard13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/bouchard13a.html,"In many applications, multiple interlinked sources of data are available and they cannot be represented by a single adjacency matrix, to which large scale factorization method could be applied. Collective matrix factorization is a simple yet powerful approach to jointly factorize multiple matrices, each of which represents a relation between two entity types. Existing algorithms to estimate parameters of collective matrix factorization models are based on non-convex formulations of the problem; in this paper, a convex formulation of this approach is proposed. This enables the derivation of large scale algorithms to estimate the parameters, including an iterative eigenvalue thresholding algorithm. Numerical experiments illustrate the benefits of this new approach."
1947,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming,"Pekka Parviainen, Hossein Shahrabi Farahani, Jens Lagergren","JMLR W&CP 33 :751-759, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/parviainen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_parviainen14,http://jmlr.csail.mit.edu/proceedings/papers/v33/parviainen14.html,"In many applications one wants to compute conditional probabilities given a Bayesian network. This inference problem is NP-hard in general but becomes tractable when the network has low tree-width. Since the inference problem is common in many application areas, we provide a practical algorithm for learning bounded tree-width Bayesian networks. We cast this problem as an integer linear program (ILP). The program can be solved by an anytime algorithm which provides upper bounds to assess the quality of the found solutions. A key component of our program is a novel integer linear formulation for bounding tree-width of a graph. Our tests clearly indicate that our approach works in practice, as our implementation was able to find an optimal or nearly optimal network for most of the data sets."
1948,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Toward Understanding Complex Spaces: Graph Laplacians on Manifolds with Singularities and Boundaries,"Mikhail Belkin, Qichao Que, Yusu Wang and Xueyuan Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/belkin12/belkin12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_belkin12,http://jmlr.csail.mit.edu/proceedings/papers/v23/belkin12.html,"In manifold learning, algorithms based on graph Laplacian constructed from data have received considerable attention both in practical applications and theoretical analysis. Much of the existing work has been done under the assumption that the data is sampled from a manifold without boundaries and singularities or that the functions of interest are evaluated away from such points. At the same time, it can be argued that singularities and boundaries are an important aspect of the geometry of realistic data. Boundaries occur whenever the process generating data has a bounding constraint; while singularities appear when two different manifolds intersect or if a process undergoes a ""phase transition"", changing non-smoothly as a function of a parameter. In this paper we consider the behavior of graph Laplacians at points at or near boundaries and two main types of other singularities: intersections , where different manifolds come together and sharp ""edges"" , where a manifold sharply changes direction. We show that the behavior of graph Laplacian near these singularities is quite different from that in the interior of the manifolds. In fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of Fourier series, can be observed in the behavior of graph Laplacian near such points. Unlike in the interior of the domain, where graph Laplacian converges to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a first-order differential operator, which exhibits different scaling behavior as a function of the kernel width. One important implication is that while points near the singularities occupy only a small part of the total volume, the difference in scaling results in a disproportionately large contribution to the total behavior. Another significant finding is that while the scaling behavior of the operator is the same near different types of singularities, they are very distinct at a more refined level of analysis. We believe that a comprehensive understanding of these structures in addition to the standard case of a smooth manifold can take us a long way toward better methods for analysis of complex non-linear data and can lead to significant progress in algorithm design. Keywords: Graph Laplacian, singularities, limit analysis"
1949,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Novelty detection: Unlabeled data definitely help,"Clayton Scott, Gilles Blanchard","5:464-471, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/scott09a/scott09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_scott09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/scott09a.html,In machine learning one formulation of the novelty detection problem is to build a detector based on a training sample consisting of only nominal data. The standard (inductive) approach to this problem has been to declare novelties where the nominal density is low which reduces the problem to density level set estimation. In this paper we consider the setting where an unlabeled and possibly contaminated sample is also available at learning time. We argue that novelty detection is naturally solved by a general reduction to a binary classification problem. In particular a detector with a desired false positive rate can be achieved through a reduction to Neyman-Pearson classification. Unlike the inductive approach our approach yields detectors that are optimal (e.g. statistically consistent) regardless of the distribution on novelties. Therefore in novelty detection unlabeled data have a substantial impact on the theoretical properties of the decision rule.
1950,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Reducing Data Loading Bottleneck with Coarse Feature Vectors for Large Scale Learning,"Shingo Takamatsu, Carlos Guestrin","JMLR W&CP 36 :46-60, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/takamatsu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_takamatsu14,http://jmlr.csail.mit.edu/proceedings/papers/v36/takamatsu14.html,"In large scale learning, disk I/O for data loading is often the runtime bottleneck. We propose a lossy data compression scheme with a fast decompression to reduce disk I/O, allocating fewer than the standard 32 bits for each real value in the data set. We theoretically show that the estimation error induced by the loss in compression decreases exponentially with the number of the bits used per value. Our experiments show the proposed method achieves excellent performance with a small number of bits and substantial speedups during training."
1951,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Discrete Chebyshev Classifiers,"Elad Eban, Elad Mezuman, Amir Globerson",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/eban14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_eban14,http://jmlr.csail.mit.edu/proceedings/papers/v32/eban14.html,"In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals. Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input."
1952,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,On multi-class classification through the minimization of the confusion matrix norm,"Sokol Koço, C_cile Capponi","JMLR W&CP 29 :277-292, 2013",http://jmlr.org/proceedings/papers/v29/Koco13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Koco13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Koco13.html,"In imbalanced multi-class classification problems, the misclassification rate as an error measure may not be a relevant choice. Several methods have been developed where the performance measure retained richer information than the mere misclassification rate: misclassification costs, ROC-based information, etc. Following this idea of dealing with alternate measures of performance, we propose to address imbalanced classification problems by using a new measure to be optimized: the norm of the confusion matrix. Indeed, recent results show that using the norm of the confusion matrix as an error measure can be quite interesting due to the fine-grain informations contained in the matrix, especially in the case of imbalanced classes. Our first contribution then consists in showing that optimizing criterion based on the confusion matrix gives rise to a common background for cost-sensitive methods aimed at dealing with imbalanced classes learning problems. As our second contribution, we propose an extension of a recent multi-class boosting method ã namely AdaBoost.MM ã to the imbalanced class problem, by greedily minimizing the empirical norm of the confusion matrix. A theoretical analysis of the properties of the proposed method is presented, while experimental results illustrate the behavior of the algorithm and show the relevancy of the approach compared to other methods."
1953,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Movement Segmentation and Recognition for Imitation Learning,"Franziska Meier, Evangelos Theodorou, Stefan Schaal",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/meier12/meier12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_meier12,http://jmlr.csail.mit.edu/proceedings/papers/v22/meier12.html,In human movement learning it is most common to teach constituent elements of complex movements in isolation before chaining them into complex movements. Segmentation and recognition of observed movement could thus proceed out of this existing knowledge which is directly compatible with movement generation. In this paper we address exactly this scenario. We assume that a library of movement primitives has already been taught and we wish to identify elements of the library in a complex motor act where the individual elements have been smoothed together and occasionally there might be a movement segment that is not in our library yet. We employ a flexible machine learning representation of movement primitives based on learnable nonlinear attractor system. For the purpose of movement segmentation and recognition it is possible to reformulate this representation as a controlled linear dynamical system. An Expectation-Maximization algorithm can be developed to estimate the open parameters of a movement primitive from the library using as input an observed trajectory piece. If no matching primitive from the library can be found a new primitive is created. This process allows a straightforward sequential segmentation of observed movement into known and new primitives which are suitable for robot imitation learning. We illustrate our approach with synthetic examples and data collected from human movement.
1954,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Conceptual Imitation Learning: An Application to Human-robot Interaction,"Hossein Hajimirsadeghi, Majid Nili Ahmadabadi, Mostafa Ajallooeian, Babak Araabi, and Hadi Moradi","13:331-346, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/hajimirsadeghi10a/hajimirsadeghi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_hajimirsadeghi10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/hajimirsadeghi10a.html,In general imitation is imprecisely used to address different levels of social learning from high level knowledge transfer to low level regeneration of motor commands. However true imitation is based on abstraction and conceptualization. This paper presents a conceptual approach for imitation learning using feedback cues and interactive training to abstract spatio-temporal demonstrations based on their perceptual and functional characteristics. Abstraction concept acquisition and self-organization of proto-symbols are performed through an incremental and gradual learning algorithm. In this algorithm Hidden Markov Models (HMMs) are used to abstract perceptually similar demonstrations. However abstract (relational) concepts emerge as a collection of HMMs irregularly scattered in the perceptual space. Performance of the proposed algorithm is evaluated in a human-robot interaction task of imitating signs produced by hand movements. Experimental results show efficiency of our model for concept extraction symbol emergence motion pattern recognition and regeneration.
1955,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Characterizing EVOI-Sufficient k-Response Query Sets in Decision Problems,"Robert Cohn, Satinder Singh, Edmund Durfee","JMLR W&CP 33 :131-139, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/cohn14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_cohn14,http://jmlr.csail.mit.edu/proceedings/papers/v33/cohn14.html,"In finite decision problems where an agent can query its human user to obtain information about its environment before acting, a queryês usefulness is in terms of its Expected Value of Information (EVOI). The usefulness of a query set is similarly measured in terms of the EVOI of the queries it contains. When the only constraint on what queries can be asked is that they have exactly k possible responses (with \(k \ge 2\) ), we show that the set of \(k\) -response decision queries (which ask the user to select his/her preferred decision given a choice of \(k\) decisions) is EVOI-Sufficient, meaning that no single \(k\) -response query can have higher EVOI than the best single \(k\) -response decision query for any decision problem. When multiple queries can be asked before acting, we provide a negative result that shows the set of depth- \(n\) query trees constructed from \(k\) -response decision queries is not EVOI-Sufficient. However, we also provide a positive result that the set of depth- \(n\) query trees constructed from \(k\) -response decision-set queries, which ask the user to select from among \(k\) sets of decisions as to which set contains the best decision, is EVOI-Sufficient. We conclude with a discussion and analysis of algorithms that draws on a connection to other recent work on decision-theoretic knowledge elicitation."
1956,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising,"Ling Yan, Wu-Jun Li, Gui-Rong Xue, Dingyi Han",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yan14.html,"In display advertising, click through rate(CTR) prediction is the problem of estimating the probability that an advertisement (ad) is clicked when displayed to a user in a specific context. Due to its easy implementation and promising performance, logistic regression(LR) model has been widely used for CTR prediction, especially in industrial systems. However, it is not easy for LR to capture the nonlinear information, such as the conjunction information, from user features and ad features. In this paper, we propose a novel model, called coupled group lasso(CGL), for CTR prediction in display advertising. CGL can seamlessly integrate the conjunction information from user features and ad features for modeling. Furthermore, CGL can automatically eliminate useless features for both users and ads, which may facilitate fast online prediction. Scalability of CGL is ensured through feature hashing and distributed implementation. Experimental results on real-world data sets show that our CGL model can achieve state-of-the-art performance on web-scale CTR prediction tasks."
1957,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Latent Goal Analysis for Dimension Reduction in Reinforcement Learning,"Matthias Rolf, Minoru Asada",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/rolf15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_rolf15,http://jmlr.csail.mit.edu/proceedings/papers/v43/rolf15.html,"In contrast to reinforcement learning, adaptive control formulations [Nguyen-Tuong and Peters, 2011] already come with expressive and typically low-dimensional goal and task representations, which have been generally considered more expressive than the RL setting [Kaelbling et al., 1996]. Goal and actual values in motor control define a relation similar [Rolf and Steil, 2014] to actual and target outputs in classical supervised learning settings by providing –directional information” in contrast to a mere –magnitude of an error” in reinforcement learning [Barto, 1994]. Recent work [Rolf and Asada, 2014] however showed that these two problem formulations can be transformed into each other. Hence, highly descriptive task representations can be extracted out of reinforcement learning problems by transforming them into adaptive control problems. After introducing the method called Latent Goal Analysis, we discuss the possible application of this approach as dimension reduction technique in reinforcement learning. Experimental results in a web recommender scenario confirm the potential of this technique."
1958,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Efficient large margin semisupervised learning,Junhui Wang,"2:588-595, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07b/wang07b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_wang07b,http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07b.html,In classification semisupervised learning involves a large amount of unlabeled data with only a small number of labeled data. This imposes great challenge in that the class probability given input can not be well estimated through labeled data alone. To enhance predictability of classification this article introduces a large margin semisupervised learning method constructing an efficient loss to measure the contribution of unlabeled instances to classification. The loss is iteratively refined based on which an iterative scheme is derived for implementation. The proposed method is examined for two large margin classifiers: support vector machines and $\psi$-learning. Our theoretical and numerical analyses indicate that the method achieves the desired objective of delivering higher performances over any other method initializing the scheme.
1959,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,SuperSlicing Frame Restoration for Anisotropic ssTEM and Video Data,"Dmitry Laptev, Joachim M. Buhmann",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/laptev15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_laptev15,http://jmlr.csail.mit.edu/proceedings/papers/v46/laptev15.html,"In biological imaging the data is often represented by a sequence of anisotropic frames ã the resolution in one dimension is significantly lower than in the other dimensions. E.g. in electron microscopy it arises from the thickness of a scanned section. This leads to blurred images and raises problems in tasks like neuronal image segmentation. We present the details and additional evaluation of an approach originally introduced in Laptev et al. (2014) called SuperSlicing to decompose the observed frame into a sequence of plausible hidden sub-frames. Based on sub-frame decomposition by SuperSlicing we propose a novel automated method to perform neuronal structure segmentation. We test our approach on a popular connectomics benchmark, where SuperSlicing preserves topological structures significantly better than other algorithms. We also generalize the approach for video anisotropicity that comes from the long exposure time and show that our method outperforms baseline methods on a reconstruction of low frame rate videos of natural scenes."
1960,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems,"Takafumi Kanamori, Akiko Takeda and Taiji Suzuki",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/kanamori12/kanamori12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_kanamori12,http://jmlr.csail.mit.edu/proceedings/papers/v23/kanamori12.html,"In binary classification problems, mainly two approaches have been proposed; one is loss function approach and the other is minimum distance approach. The loss function approach is applied to major learning algorithms such as support vector machine (SVM) and boosting methods. The loss function represents the penalty of the decision function on the training samples. In the learning algorithm, the empirical mean of the loss function is minimized to obtain the classifier. Against a backdrop of the development of mathematical programming, nowadays learning algorithms based on loss functions are widely applied to real-world data analysis. In addition, statistical properties of such learning algorithms are well-understood based on a lots of theoretical works. On the other hand, some learning methods such as _-SVM, mini-max probability machine (MPM) can be formulated as minimum distance problems. In the minimum distance approach, firstly, the so-called uncertainty set is defined for each binary label based on the training samples. Then, the best separating hyperplane between the two uncertainty sets is employed as the decision function. This is regarded as an extension of the maximum-margin approach. The minimum distance approach is considered to be useful to construct the statistical models with an intuitive geometric interpretation, and the interpretation is helpful to develop the learning algorithms. However, the statistical properties of the minimum distance approach have not been intensively studied. In this paper, we consider the relation between the above two approaches. We point out that the uncertainty set in the minimum distance approach is described by using the level set of the conjugate of the loss function. Based on such relation, we study statistical properties of the minimum distance approach."
1961,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Semi-Supervised Apprenticeship Learning,"Michal Valko, Mohammad Ghavamzadeh, Alessandro Lazaric","24:131-142, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/valko12a/valko12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_valko12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/valko12a.html,In apprenticeship learning we aim to learn a good policy by observing the behavior of an expert or a set of experts. In particular we consider the case where the expert acts so as to maximize an unknown reward function defined as a linear combination of a set of state features. In this paper we consider the setting where we observe many sample trajectories (i.e. sequences of states) but only one or a few of them are labeled as expertsÍ trajectories. We investigate the conditions under which the remaining unlabeled trajectories can help in learning a policy with a good performance. In particular we define an extension to the max-margin inverse reinforcement learning proposed by Abbeel and Ng [2004] where at each iteration the max-margin optimization step is replaced by a semi-supervised optimiza- tion problem which favors classifiers separating clusters of trajectories. Finally we report empirical results on two grid-world domains showing that the semi-supervised algorithm is able to output a better policy in fewer iterations than the related algorithm that does not take the unlabeled trajectories into account.
1962,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Learning Nondeterministic Mealy Machines,"Ali Khalili, Armando Tacchella","JMLR W&CP 34 :109-123, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/khalili14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_khalili14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/khalili14a.html,"In applications where abstract models of reactive systems are to be inferred, one important challenge is that the behavior of such systems can be inherently nondeterministic. To cope with this challenge, we developed an algorithm to infer nondeterministic computation models in the form of Mealy machines. We introduce our approach and provide extensive experimental results to assess its potential in the identification of black-box reactive systems. The experiments involve both artificially-generated abstract Mealy machines, and the identification of a TFTP server model starting from a publicly-available implementation."
1963,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,An Efficient Posterior Regularized Latent Variable Model for Interactive Sound Source Separation,"Nicholas Bryan, Gautham Mysore",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bryan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bryan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bryan13.html,"In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single-channel recording into its respective sources. One of the current most effective class of methods to do so is based on non-negative matrix factorization and related latent variable models. Such techniques, however, typically perform poorly when no isolated training data is given and do not allow user feedback to correct for poor results. To overcome these issues, we allow a user to interactively constrain a latent variable model by painting on a time-frequency display of sound to guide the learning process. The annotations are used within the framework of posterior regularization to impose linear grouping constraints that would otherwise be difficult to achieve via standard priors. For the constraints considered, an efficient expectation-maximization algorithm is derived with closed-form multiplicative updates, drawing connections to non-negative matrix factorization methods, and allowing for high-quality interactive-rate separation without explicit training data."
1964,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Preserving Modes and Messages via Diverse Particle Selection,"Jason Pacheco, Silvia Zuffi, Michael Black, Erik Sudderth",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/pacheco14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/pacheco14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_pacheco14,http://jmlr.csail.mit.edu/proceedings/papers/v32/pacheco14.html,"In applications of graphical models arising in domains such as computer vision and signal processing, we often seek the most likely configurations of high-dimensional, continuous variables. We develop a particle-based max-product algorithm which maintains a diverse set of posterior mode hypotheses, and is robust to initialization. At each iteration, the set of hypotheses at each node is augmented via stochastic proposals, and then reduced via an efficient selection algorithm. The integer program underlying our optimization-based particle selection minimizes errors in subsequent max-product message updates. This objective automatically encourages diversity in the maintained hypotheses, without requiring tuning of application-specific distances among hypotheses. By avoiding the stochastic resampling steps underlying particle sum-product algorithms, we also avoid common degeneracies where particles collapse onto a single hypothesis. Our approach significantly outperforms previous particle-based algorithms in experiments focusing on the estimation of human pose from single images."
1965,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE),"Maurizio Filippone, Raphael Engler",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/filippone15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_filippone15,http://jmlr.csail.mit.edu/proceedings/papers/v37/filippone15.html,"In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors."
1966,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Bigger is Not Always Better: on the Quality of Hypotheses in Active Automata Learning,"Rick Smetsers, Michele Volpato, Frits Vaandrager, Sicco Verwer","JMLR W&CP 34 :167-181, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/smetsers14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_smetsers14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/smetsers14a.html,"In Angluinês \(L^{\ast}\) algorithm a learner constructs a sequence of hypotheses in order to learn a regular language. Each hypothesis is consistent with a larger set of observations and is described by a bigger model. From a behavioral perspective, however, a hypothesis is not always better than the previous one, in the sense that the minimal length of a counterexample that distinguishes a hypothesis from the target language may decrease. We present a simple modification of the \(L^{\ast}\) algorithm that ensures that for subsequent hypotheses the minimal length of a counterexample never decreases, which implies that the distance to the target language never increases in a corresponding ultrametric. Preliminary experimental evidence suggests that our algorithm speeds up learning in practical applications by reducing the number of equivalence queries."
1967,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Support Vector Machines Under Adversarial Label Noise,"B. Biggio, B. Nelson & P. Laskov","20:97_112, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/biggio11/biggio11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_biggio11,http://jmlr.csail.mit.edu/proceedings/papers/v20/biggio11.html,In adversarial classi_cation tasks like spam _ltering and intrusion detection malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus besides achieving good classi_cation performances machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classi_cation problems their e_ectiveness in adversarial classi_cation tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular we assume that the adversary has control over some training data and aims to subvert the SVM learning process. Within this assumption we show that this is indeed possible and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.   Page last modified on Sun Nov 6 15:42:38 2011.
1968,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,An Analysis of Active Learning with Uniform Feature Noise,"Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman","JMLR W&CP 33 :805-813, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ramdas14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/ramdas14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ramdas14,http://jmlr.csail.mit.edu/proceedings/papers/v33/ramdas14.html,"In active learning, the user sequentially chooses values for feature \(X\) and an oracle returns the corresponding label \(Y\) . In this paper, we consider the effect of feature noise in active learning, which could arise either because \(X\) itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as–errors in variables” and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width \(\sigma\) . Our simple but revealing setting is that of one-dimensional binary classification setting where the goal is to learn a threshold (point where the probability of a \(+\) label crosses half). We deal with regression functions that are antisymmetric in a region of size \(\sigma\) around the threshold and also satisfy Tsybakovês margin condition around the threshold. We prove minimax lower and upper bounds which demonstrate that when \(\sigma\) is smaller than the minimiax active/passive noiseless error derived in Castro & Nowak (2007), then noise has no effect on the rates and one achieves the same noiseless rates. For larger \(\sigma\) , the unflattening of the regression function on convolution with uniform noise, along with its local antisymmetry around the threshold, together yield a behaviour where noise appears to be beneficial. Our key result is that active learning can buy significant improvement over a passive strategy even in the presence of feature noise."
1969,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Evaluation of a Bayesian model-based approach in GA studies,"Gˆbor Hullˆm, P_ter Antal, Csaba Szalai, Andrˆs Falus","8:30-43, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/hullam10a/hullam10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_hullam10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/hullam10a.html,"In a typical Genetic Association Study (GAS) several hundreds to millions of genomic variables are measured and tested for association with a given set of a phenotypic variables (e.g., a given disease state or a complete expression profile), with the aim of identifying the genetic background of complex, multifactorial diseases. These highly varying requirements resulted in a number of different statistical tools applying different approaches either bayesian or non-bayesian, model-based or conditional. In this paper we evaluate dedicated GAS tools and general purpose feature subset selection (FSS) tools including a Bayesian model-based tool BMLA in a GAS context. In the evaluation we used an artificial data set generated from a reference model with 113 genotypic variables that was based on a real-world genotype data."
1970,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically,"Mrinal Das, Suparna Bhattacharya, Chiranjib Bhattacharyya, Gopinath Kanchi",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/das13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/das13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_das13,http://jmlr.csail.mit.edu/proceedings/papers/v28/das13.html,"In a recent pioneering approach LDA was used to discover cross cutting concerns(CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models(STM). STM uses a generalized stick breaking process(GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics."
1971,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Harmonic Exponential Families on Manifolds,"Taco Cohen, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohenb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohenb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_cohenb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/cohenb15.html,"In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train."
1972,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Minimax Regret of Finite Partial-Monitoring Games in Stochastic Environments,"Gˆbor Bart„k, Dˆvid Pˆl, Csaba Szepesvˆri","19:133-154, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/bartok11a/bartok11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_bartok11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/bartok11a.html,In a partial monitoring game the learner repeatedly chooses an action theenvironment responds with an outcome and then the learner suffers a loss andreceives a feedback signal both of which are fixed functions of the action andthe outcome. The goal of the learner is to minimize his regret which is thedifference between his total cumulative loss and the total loss of the bestfixed action in hindsight.Assuming that the outcomes are generated in an i.i.d. fashion from an arbitrary andunknown probability distribution we characterize the minimax regret of anypartial monitoring game with finitely many actions andoutcomes. It turns out that the minimax regret of any such game is either zero$\widetilde{\Theta}(\sqrt{T})$ $\Theta(T^{2/3})$ or $\Theta(T)$. We provide a computationally efficient learningalgorithm that achieves the minimax regret within logarithmic factor for any game.
1973,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Contextual Bandits with Similarity Information,Aleksandrs Slivkins,"19:679-702, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/slivkins11a/slivkins11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_slivkins11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/slivkins11a.html,"In a multi-armed bandit (MAB) problem an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets where one needs to assume extra structure in order to make the problem tractable. In particular recent literature considered information on similarity between arms.We consider similarity information in the setting of \emph{contextual bandits} a natural extension of the basic MAB problem where before each round an algorithm is given the \emph{context} -- a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on webpages one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a \emph{similarity distance} between the context-arm pairs which bounds from above the difference between the respective expected payoffs.Prior work on contextual bandits with similarity uses ``uniform"" partitions of the similarity space so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on ``uniform"" partitions disregard the structure of the payoffs and the context arrivals which is potentially wasteful. We present algorithms that are based on {\em adaptive} partitions and take advantage of ""benign"" payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings e.g. MAB with constrained temporal change~\citep{DynamicMAB-colt08} and sleeping bandits~\citep{sleeping-colt08}."
1974,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Active Batch Learning with Tree Ensembles,"A. Borisov, E. Tuv & G. Runger ; 16:59_69, 2011. [ abs ] [ pdf ]","16:59_69, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/borisov11a/borisov11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_borisov11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/borisov11a.html,In a conventional machine learning approach one uses labeled data to train the model. However often we have a data set with few labeled instances and a large number of unlabeled ones. This is called a semi-supervised learning problem. It is well known that often unlabeled data could be used to improve a model. In real world scenarios labeled data can usually be obtained dynamically. However obtaining new labels in most cases requires human e_ort and/or is costly. An active learning (AL) paradigm tries to direct the queries in such way that a good model can be trained with a relatively small number of queries. In this work we focus on so-called pool-based active learning i.e. learning when there is a _xed large pool of unlabeled data and we can query the label for any instance from this pool at some cost. Existing methods are often based on strong assumptions for the joint input/output distribution (i.e. a mixture of Gaussians linearly separable input space etc.) or use a distance-based approach (such as Euclidean or Mahalanobis distances). That makes such methods very susceptible to noise in input space and they often work poorly in high dimensions. Also such methods assume numeric inputs only. In addition for most methods relying on distance computations and/or linear models computational complexity scales at least quadratically with respect to the number of unlabeled samples rendering them useless on large datasets. In real world applications data is often large noisy contains irrelevant inputs missing values and mixed variable types. Often queries should be arranged in groups or batches (this is called batch AL). In batch querying one should consider both the ÍusefulnessÍ of individual queries within a batch and the batch diversity. Batch AL although being very practical by nature is rarely addressed by existing AL approaches. Here we propose a new non-parametric approach to the AL problem called Stochastic Query by Forest (SQRF) that e_ectively addresses the challenges described above. Our algorithm is based on a QBC algorithm applied to an RF ensemble and our main contribution is the batch diversi_cation strategy. We describe two di_erent strategies for batch selection the _rst of which achieved the highest average score on the AISTATS 2010 active learning challenge and ranked top on one of the challenge datasets. Our work focuses on binary classi_cation problems but our method can be directly applied to regression or multi-class problems with minor modi_cations.   Page last modified on Wed Mar 30 11:09:21 2011.
1975,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: A (missing) boosting-type convergence result for AdaBoost.MH with factorized multi-class classifiers,Balˆzs K_gl,"JMLR W&CP 35 :1268-1275, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kegl14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kegl14,http://jmlr.csail.mit.edu/proceedings/papers/v35/kegl14.html,"In (K_gl, 2014), we recently showed empirically that AdaBoost.MH is one of the best multi-class boosting algorithms when the classical one-against-all base classifiers, proposed in the seminal paper of Schapire and Singer (1999), are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector. In a slightly different setup, a similar factorization coupled with an iterative optimization of the two factors also proved to be an excellent approach (Gao and Koller, 2011). The main algorithmic advantage of our approach over the original setup of Schapire and Singer (1999) is that trees can be built in a straightforward way by using the binary classifier at inner nodes. In this open problem paper we take a step back to the basic setup of boosting generic multi-class factorized (Hamming) classifiers (so no trees), and state the classical problem of boosting-like convergence of the training error. Given a vote vector, training the classifier leads to a standard weighted binary classification problem. The main difficulty of proving the convergence is that, unlike in binary AdaBoost , the sum of the weights in this weighted binary classification problem is less than one, which means that the lower bound on the edge, coming from the weak learning condition, shrinks. To show the convergence, we need a (uniform) lower bound on the sum of the weights in this derived binary classification problem."
1976,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Efficient Reductions for Imitation Learning,"Stephane Ross, Drew Bagnell","9:661-668, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/ross10a/ross10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_ross10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/ross10a.html,Imitation Learning while applied successfully on many large real-world problems is typically addressed as a standard supervised learning problem where it is assumed the training and testing data are i.i.d.. This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner's policy is slowly modified from executing the expert's policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.
1977,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Inverse Optimal Heuristic Control for Imitation Learning,"Nathan Ratliff, Brian Ziebart, Kevin Peterson, J. Andrew Bagnell, Martial Hebert, Anind K. Dey, Siddhartha Srinivasa","5:424-431, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/ratliff09a/ratliff09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_ratliff09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/ratliff09a.html,Imitation learning is an increasingly important tool for both developing automatic decision making systems as well as for learning to predict decision-making and behavior by observation. Two basic approaches are common: the first which we here term \emph{behavioral cloning} (BC)\cite{BehavioralCloningALVINNDAVE} treats the imitation learning problem as a straightforward one of supervised learning (e.g. classification) where the goal is to map observations to controls. Secondly the notion of\emph{inverse optimal control} (IOC) \cite{BoydIOCng00irlAbbeel04cmmp06} for modeling such decision making behavior has gained prominence as it allows for learned decision-making that reasons sequentially and over a long horizon. Unfortunately such inverse optimal control methods rely upon the ability to efficiently solve a planning problem and suffer from the usual ``curse of dimensionality'' when the state space gets large. This paper presents a novel approach to imitation learning that we call \emph{Inverse Optimal Heuristic Control }(IOHC) which capitalizes on the strengths of both paradigms by allowing long-horizon planning style reasoning in a low dimensional space while enabling a high dimensional additional set of features to guide overall action selection. We frame this combined problem as one of optimization and although the resulting objective function is actually non-convex we are able to provide convex upper and lower bounds to optimize as surrogates. Further these bounds as well as our empirical results show that the objective function is nearly convex and leads to improved performance on a set of imitation learning problems including turn prediction of drivers as well as predicting the likely paths taken by pedestrians in an office environment.
1978,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Forecastable Component Analysis,Georg Goerg,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/goerg13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/goerg13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_goerg13,http://jmlr.csail.mit.edu/proceedings/papers/v28/goerg13.html,"I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to fi nancial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classi cation. The R package ForeCA accompanies this work and is publicly available on CRAN."
1979,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Collaborative hyperparameter tuning,"R_mi Bardenet, Mˆtyˆs Brendel, Balˆzs K_gl, Michèle Sebag",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/bardenet13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/bardenet13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_bardenet13,http://jmlr.csail.mit.edu/proceedings/papers/v28/bardenet13.html,"Hyperparameter learning has traditionally been a manual task because of the limited number of trials. Todayês computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to hyperparameter learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the hyperparameter space, and they could significantly improve on manual hyperparameter tuning. What may make experienced practitioners even better at hyperparameter optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization."
1980,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Human Boosting,"Harsh Pareek, Pradeep Ravikumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/pareek13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/pareek13-supp.zip,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_pareek13,http://jmlr.csail.mit.edu/proceedings/papers/v28/pareek13.html,"Humans may be exceptional learners but they have biological limitations and moreover, inductive biases similar to machine learning algorithms. This puts limits on human learning ability and on the kinds of learning tasks humans can easily handle. In this paper, we consider the problem of –boosting” human learners to extend the learning ability of human learners and achieve improved performance on tasks which individual humans find difficult. We consider classification (category learning) tasks, propose a boosting algorithm for human learners and give theoretical justifications. We conduct experiments using Amazonês Mechanical Turk on two synthetic datasets _ a crosshair task with a nonlinear decision boundary and a gabor patch task with a linear boundary but which is inaccessible to human learners _ and one real world dataset _ the Opinion Spam detection task introduced in (Ott et al). Our results show that boosting human learners produces gains in accuracy and can overcome some fundamental limitations of human learners."
1981,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning from Human-Generated Lists,"Kwang-Sung Jun, Jerry Zhu, Burr Settles, Timothy Rogers",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jun13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jun13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jun13.html,"Human-generated lists are a form of non-iid data with important applications in machine learning and cognitive psychology. We propose a generative model - sampling with reduced replacement (SWIRL) - for such lists. We discuss SWIRLês relation to standard sampling paradigms, provide the maximum likelihood estimate for learning, and demonstrate its value with two real-world applications: (i) In a –”feature volunteering–” task where non-experts spontaneously generate feature=_label pairs for text classification, SWIRL improves the accuracy of state-of-the-art feature-learning frameworks. (ii) In a –”verbal fluency–” task where brain-damaged patients generate word lists when prompted with a category, SWIRL parameters align well with existing psychological theories, and our model can classify healthy people vs. patients from the lists they generate."
1982,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,Handwritten Text Recognition for Ancient Documents,"Alfons Juan, Ver„nica Romero, Joan Andreu Sˆnchez, Nicolˆs Serrano, Alejandro H. Toselli and Enrique Vidal","11:58-65, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/juan10a/juan10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_juan10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/juan10a.html,"Huge amounts of legacy documents are being published by on-line digital libraries world wide. However, for these raw digital images to be really useful, they need to be transcribed into a textual electronic format that would allow unrestricted indexing, browsing and querying. In some cases, adequate transcriptions of the handwritten text images are already available. In this work three systems are presented to deal with this sort of documents. The first two address two different approaches for semi-automatic transcription of document images. The third system implements an alignment method to find mappings between word images of a handwritten document and their respective words in its given transcription."
1983,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Reduced-Rank Hidden Markov Models,"Sajid Siddiqi, Byron Boots, Geoffrey Gordon","9:741-748, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/siddiqi10a/siddiqi10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_siddiqi10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/siddiqi10a.html,Hsu et al.(2009) recently proposed an efficient accurate spectral learning algorithm for Hidden Markov Models (HMMs). In this paper we relax their assumptions and prove a tighter finite-sample error bound for the case of Reduced-Rank HMMs i.e. HMMs with low-rank transition matrices. Since rank-k RR-HMMs are a larger class of models than k-state HMMs while being equally efficient to work with this relaxation greatly increases the learning algorithm's scope. In addition we generalize the algorithm and bounds to models where multiple observations are needed to disambiguate state and to models that emit multivariate real-valued observations. Finally we prove consistency for learning Predictive State Representations an even larger class of models. Experiments on synthetic data and a toy video as well as on difficult robot vision data yield accurate models that compare favorably with alternatives in simulation quality and prediction accuracy.
1984,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Learning Social Infectivity in Sparse Low-rank Networks Using Multi-dimensional Hawkes Processes,"Ke Zhou, Hongyuan Zha, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/zhou13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_zhou13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/zhou13a.html,"How will the behaviors of individuals in a social network be influenced by their neighbors, the authorities and the communities? Such knowledge is often hidden from us and we only observe its manifestation in the form of recurrent and time-stamped events occurring at the individuals involved. It is an important yet challenging problem to infer the network of social inference based on the temporal patterns of these historical events. We propose a convex optimization approach to discover the hidden network of social influence by modeling the recurrent events at different individuals as multi-dimensional Hawkes processes. Furthermore, our estimation procedure, using nuclear and \(\ell_1\) norm regularization simultaneously on the parameters, is able to take into account the prior knowledge of the presence of neighbor interaction, authority influence, and community coordination. To efficiently solve the problem, we also design an algorithm ADM4 which combines techniques of alternating direction method of multipliers and majorization minimization. We experimented with both synthetic and real world data sets, and showed that the proposed method can discover the hidden network more accurately and produce a better predictive model."
1985,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Near-Optimally Teaching the Crowd to Classify,"Adish Singla, Ilija Bogunovic, Gabor Bartok, Amin Karbasi, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/singla14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/singla14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_singla14,http://jmlr.csail.mit.edu/proceedings/papers/v32/singla14.html,"How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm."
1986,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Near Optimal Bayesian Active Learning for Decision Making,"Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, Drew Bagnell, Siddhartha Srinivasa","JMLR W&CP 33 :430-438, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/javdani14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/javdani14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_javdani14,http://jmlr.csail.mit.edu/proceedings/papers/v33/javdani14.html,"How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator."
1987,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Triggering Kernels for Multi-dimensional Hawkes Processes,"Ke Zhou, Hongyuan Zha, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhou13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhou13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhou13.html,"How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi-dimensional Hawkes processes. In particular, we focus on the nonparametric learning of the triggering kernels, and propose an algorithm MMEL that combines the idea of decoupling the parameters through constructing a tight upper-bound of the objective function and application of Euler-Lagrange equations for optimization in infinite dimensional functional space. We show that the proposed method performs significantly better than alternatives in experiments on both synthetic and real world datasets."
1988,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Neural Variational Inference and Learning in Belief Networks,"Andriy Mnih, Karol Gregor",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mnih14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mnih14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mnih14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mnih14.html,"Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset."
1989,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Time-Regularized Interrupting Options (TRIO),"Timothy Mann, Daniel Mankowitz, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/mannb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/mannb14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_mannb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/mannb14.html,"High-level skills relieve planning algorithms from low-level details. But when the skills are poorly designed for the domain, the resulting plan may be severely suboptimal. Sutton et al. 1999 made an important step towards resolving this problem by introducing a rule that automatically improves a set of skills called options. This rule terminates an option early whenever switching to another option gives a higher value than continuing with the current option. However, they only analyzed the case where the improvement rule is applied once. We show conditions where this rule converges to the optimal set of options. A new Bellman-like operator that simultaneously improves the set of options is at the core of our analysis. One problem with the update rule is that it tends to favor lower-level skills. Therefore we introduce a regularization term that favors longer duration skills. Experimental results demonstrate that this approach can derive a good set of high-level skills even when the original set of skills cannot solve the problem."
1990,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Feature Selection in High-Dimensional Classification,"Mladen Kolar, Han Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kolar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kolar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kolar13.html,"High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems."
1991,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering,"Justin Eldridge, Mikhail Belkin, Yusu Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Eldridge15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Eldridge15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Eldridge15.html,"Hierarchical clustering is a popular method for analyzing data which associates a tree to a dataset. Hartigan consistency has been used extensively as a framework to analyze such clustering algorithms from a statistical point of view. Still, as we show in the paper, a tree which is Hartigan consistent with a given density can look very different than the correct limit tree. Specifically, Hartigan consistency permits two types of undesirable configurations which we term over-segmentation and improper nesting . Moreover, Hartigan consistency is a limit property and does not directly quantify difference between trees. In this paper we identify two limit properties, separation and minimality , which address both over-segmentation and improper nesting and together imply (but are not implied by) Hartigan consistency. We proceed to introduce a merge distortion metric between hierarchical clusterings and show that convergence in our distance implies both separation and minimality. We also prove that uniform separation and minimality imply convergence in the merge distortion metric. Furthermore, we show that our merge distortion metric is stable under perturbations of the density. Finally, we demonstrate applicability of these concepts by proving convergence results for two clustering algorithms. First, we show convergence (and hence separation and minimality) of the recent robust single linkage algorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence results on manifolds for topological split tree clustering."
1992,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Active Clustering: Robust and Efficient Hierarchical Clustering using Adaptively Selected Similarities,"Brian Eriksson, Gautam Dasarathy, Aarti Singh, Rob Nowak","15:260-268, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/eriksson11a/eriksson11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_eriksson11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/eriksson11a.html,Hierarchical clustering based on pairwise similarities is a common tool used in a broad range of scientific applications. However in many problems it may be expensive to obtain or compute similarities between the items to be clustered. This paper investigates the possibility of hierarchical clustering of N items based on a small subset of pairwise similarities significantly less than the complete set of N(N-1)/2 similarities. First we show that if the intracluster similarities exceed intercluster similarities then it is possible to correctly determine the hierarchical clustering from as few as 3N log N similarities. We demonstrate this order of magnitude saving in the number of pairwise similarities necessitates sequentially selecting which similarities to obtain in an adaptive fashion rather than picking them at random. Finally we propose an active clustering method that is robust to a limited fraction of anomalous similarities and show how even in the presence of these noisy similarity values we can resolve the hierarchical clustering using only O(N log^2 N) pairwise similarities.
1993,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Convex Calibrated Surrogates for Hierarchical Classification,"Harish Ramaswamy, Ambuj Tewari, Shivani Agarwal",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/ramaswamy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/ramaswamy15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_ramaswamy15,http://jmlr.csail.mit.edu/proceedings/papers/v37/ramaswamy15.html,"Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical classification algorithms with respect to a natural loss, namely the tree distance metric on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that the Bayes optimal classifier for this loss classifies an instance according to the deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is greater than \(\frac{1}{2}\) . We exploit this insight to develop new consistent algorithm for hierarchical classification, that makes use of an algorithm known to be consistent for the –multiclass classification with reject option (MCRO)” problem as a sub-routine. Our experiments on a number of benchmark datasets show that the resulting algorithm, which we term OvA-Cascade, gives improved performance over other state-of-the-art hierarchical classification algorithms."
1994,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets,"Diederik Kingma, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kingma14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kingma14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kingma14.html,"Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments."
1995,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Spectral Algorithm for Inference in Hidden semi-Markov Models,"Igor Melnyk, Arindam Banerjee",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/melnyk15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/melnyk15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_melnyk15,http://jmlr.csail.mit.edu/proceedings/papers/v38/melnyk15.html,"Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Our approach is based on estimating certain sample moments, whose order depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few spectral decompositions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the promise of the algorithm."
1996,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning Heterogeneous Hidden Markov Random Fields,"Jie Liu, Chunming Zhang, Elizabeth Burnside, David Page","JMLR W&CP 33 :576-584, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/liu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_liu14,http://jmlr.csail.mit.edu/proceedings/papers/v33/liu14.html,"Hidden Markov random fields (HMRFs) are conventionally assumed to be homogeneous in the sense that the potential functions are invariant across different sites. However in some biological applications, it is desirable to make HMRFs heterogeneous, especially when there exists some background knowledge about how the potential functions vary. We formally define heterogeneous HMRFs and propose an EM algorithm whose M-step combines a contrastive divergence learner with a kernel smoothing step to incorporate the background knowledge. Simulations show that our algorithm is effective for learning heterogeneous HMRFs and outperforms alternative binning methods. We learn a heterogeneous HMRF in a real-world study."
1997,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Variational Bridge Regression,Artin Armagan,"5:17-24, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/armagan09a/armagan09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_armagan09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/armagan09a.html,Here we obtain approximate Bayes inferences through variational methods when an exponential power family type prior is specified for the regression coefficients to mimic the characteristics of the Bridge regression. We accomplish this through hierarchical modeling of such priors. Although the mixing distribution is not explicitly stated for scale normal mixtures we obtain the required moments only to attain the variational distributions for the regression coefficients. By choosing specific values of hyper-parameters (tuning parameters) present in the model we can mimic the model selection performance of best subset selection in sparse underlying settings. The fundamental difference between MAP \emph{maximum a posteriori} estimation and the proposed method is that here we can obtain approximate inferences besides a point estimator. We also empirically analyze the frequentist properties of the estimator obtained. Results suggest that the proposed method yields an estimator that performs significantly better in sparse underlying setups than the existing state-of-the-art procedures in both n_p and p_n scenarios.
1998,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Near-Optimal Herding,"Nick Harvey, Samira Samadi","JMLR W&CP 35 :1165-1182, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/harvey14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_harvey14,http://jmlr.csail.mit.edu/proceedings/papers/v35/harvey14.html,"Herding is an algorithm of recent interest in the machine learning community, motivated by inference in Markov random fields. It solves the following sampling problem: given a set \(\mathcal{X} \subset \mathbb{R}^d\) with mean \(\mu\) , construct an infinite sequence of points from \(\mathcal{X}\) such that, for every \(t \geq 1\) , the mean of the first \(t\) points in that sequence lies within Euclidean distance \(O(1/t)\) of \(\mu\) . The classic Perceptron boundedness theorem implies that such a result actually holds for a wide class of algorithms, although the factors suppressed by the \(O(1/t)\) notation are exponential in \(d\) . Thus, to establish a non-trivial result for the sampling problem, one must carefully analyze the factors suppressed by the \(O(1/t)\) error bound. This paper studies the best error that can be achieved for the sampling problem. Known analysis of the Herding algorithm give an error bound that depends on geometric properties of \(\mathcal{X}\) but, even under favorable conditions, this bound depends linearly on \(d\) . We present a new polynomial-time algorithm that solves the sampling problem with error \(O\left(\sqrt{d} \log^{2.5}|\mathcal{X}| / t \right)\) assuming that \(\mathcal{X}\) is finite. Our algorithm is based on recent algorithmic results in discrepancy theory . We also show that any algorithm for the sampling problem must have error \(\Omega( \sqrt{d} / t )\) . This implies that our algorithm is optimal to within logarithmic factors."
1999,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Hartigan's Method: k-means Clustering without Voronoi,"Matus Telgarsky, Andrea Vattani","9:820-827, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/telgarsky10a/telgarsky10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_telgarsky10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/telgarsky10a.html,Hartigan's method for k-means clustering is the following greedy heuristic: select a point and optimally reassign it. This paper develops two other formulations of the heuristic one leading to a number of consistency properties the other showing that the data partition is always quite separated from the induced Voronoi partition. A characterization of the volume of this separation is provided. Empirical tests verify not only good optimization performance relative to Lloyd's method but also good running time.
2000,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization,"Xiaotong Yuan, Ping Li, Tong Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yuan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yuan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yuan14.html,"Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantees and impressive numerical performance. In this paper, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard truncation step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods when applied to learning tasks of sparse logistic regression and sparse support vector machines."
2001,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Gradient Hamiltonian Monte Carlo,"Tianqi Chen, Emily Fox, Carlos Guestrin",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cheni14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/cheni14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cheni14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cheni14.html,"Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization."
2002,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,On the Testability of Models with Missing Data,"Karthika Mohan, Judea Pearl","JMLR W&CP 33 :643-650, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/mohan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_mohan14,http://jmlr.csail.mit.edu/proceedings/papers/v33/mohan14.html,"Graphical models that depict the process by which data are lost are helpful in recovering information from missing data. We address the question of whether any such model can be submitted to a statistical test given that the data available are corrupted by missingness. We present sufficient conditions for testability in missing data applications and note the impediments for testability when data are contaminated by missing entries. Our results strengthen the available tests for MCAR and MAR and further provide tests in the category of MNAR. Furthermore, we provide sufficient conditions to detect the existence of dependence between a variable and its missingness mechanism. We use our results to show that model sensitivity persists in almost all models typically categorized as MNAR."
2003,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Lightning-speed Structure Learning of Nonlinear Continuous Networks,Gal Elidan,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/elidan12b/elidan12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_elidan12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/elidan12b.html,Graphical models are widely used to reason about high-dimensional domains. Yet learning the structure of the model from data remains a formidable challenge particularly in complex continuous domains. We present a highly accelerated structure learning approach for continuous densities based on the recently introduced copula Bayesian network representation. For two common copula families we prove that the expected likelihood of a building block edge in the model is monotonic in Spearman's rank correlation measure. We also show numerically that the same relationship holds for many other copula families. This allows us to perform structure learning while bypassing costly parameter estimation as well as explicit computation of the log-likelihood function. We demonstrate the merit of our approach for structure learning in three varied real-life domains. Importantly the computational benefits are such that they open the door for practical scaling-up of structure learning in complex nonlinear continuous domains.
2004,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Adaptive Belief Propagation,"Georgios Papachristoudis, John Fisher",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/papachristoudis15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/papachristoudis15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_papachristoudis15,http://jmlr.csail.mit.edu/proceedings/papers/v37/papachristoudis15.html,"Graphical models are widely used in inference problems. In practice, one may construct a single large-scale model to explain a phenomenon of interest, which may be utilized in a variety of settings. The latent variables of interest, which can differ in each setting, may only represent a small subset of all variables. The marginals of variables of interest may change after the addition of measurements at different time points. In such adaptive settings, naive algorithms, such as standard belief propagation (BP), may utilize many unnecessary computations by propagating messages over the entire graph. Here, we formulate an efficient inference procedure, termed adaptive BP (AdaBP), suitable for adaptive inference settings. We show that it gives exact results for trees in discrete and Gaussian Markov Random Fields (MRFs), and provide an extension to Gaussian loopy graphs. We also provide extensions on finding the most likely sequence of the entire latent graph. Lastly, we compare the proposed method to standard BP and to that of (Sumer et al., 2011), which tackles the same problem. We show in synthetic and real experiments that it outperforms standard BP by orders of magnitude and explore the settings that it is advantageous over (Sumer et al., 2011)."
2005,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Estimating the Partition Function of Graphical Models Using Langevin Importance Sampling,"Jianzhu Ma, Jian Peng, Sheng Wang, Jinbo Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/ma13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_ma13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/ma13a.html,"Graphical models are powerful in modeling a variety of applications. Computing the partition function of a graphical model is a typical inference problem and known as an NP-hard problem for general graphs. A few sampling algorithms like MCMC, Simulated Annealing Sampling (SAS), Annealed Importance Sampling (AIS) are developed to address this challenging problem. This paper describes a Langevin Importance Sampling (LIS) algorithm to compute the partition function of a graphical model. LIS first performs a random walk in the configuration-temperature space guided by the Langevin equation and then estimates the partition function using all the samples generated during the random walk, as opposed to the other configuration-temperature sampling methods, which uses only the samples at a specific temperature. Experimental results show that LIS can obtain much more accurate partition function than the others tested on several different types of graphical models. LIS performs especially well on relatively large graph models or those with a large number of local optima."
2006,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,"Empirical Risk Minimization of Graphical Model Parameters Given Approximate Inference, Decoding, and Model Structure","Veselin Stoyanov, Alexander Ropson, Jason Eisner","15:725-733, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/stoyanov11a/stoyanov11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_stoyanov11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/stoyanov11a.html,Graphical models are often used ``inappropriately'' with approximations in the topology inference and prediction. Yet it is still common to train their parameters to approximately maximize training likelihood. We argue that instead one should seek the parameters that minimize the empirical risk of the entire imperfect system. We show how to locally optimize this risk using back-propagation and stochastic meta-descent. Over a range of synthetic-data problems compared to the usual practice of choosing approximate MAP parameters our approach significantly reduces loss on test data sometimes by an order of magnitude.
2007,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch,"Partha Talukdar, William Cohen","JMLR W&CP 33 :940-947, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/talukdar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_talukdar14,http://jmlr.csail.mit.edu/proceedings/papers/v33/talukdar14.html,"Graph-based Semi-supervised learning (SSL) algorithms have been successfully used in a large number of applications. These methods classify initially unlabeled nodes by propagating label information over the structure of graph starting from seed nodes. Graph-based SSL algorithms usually scale linearly with the number of distinct labels ( \(m\) ), and require \(O(m)\) space on each node. Unfortunately, there exist many applications of practical significance with very large m over large graphs, demanding better space and time complexity. In this paper, we propose MAD-Sketch, a novel graph-based SSL algorithm which compactly stores label distribution on each node using Count-min Sketch, a randomized data structure. We present theoretical analysis showing that under mild conditions, MAD-Sketch can reduce space complexity at each node from \(O(m)\) to \(O(\log(m))\) , and achieve similar savings in time complexity as well. We support our analysis through experiments on multiple real world datasets. We observe that MAD-Sketch achieves similar performance as existing state-of-the-art graph-based SSL algorithms, while requiring smaller memory footprint and at the same time achieving up to 10x speedup. We find that MAD-Sketch is able to scale to datasets with one million labels, which is beyond the scope of existing graph-based SSL algorithms."
2008,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Entropic Graph-based Posterior Regularization,"Maxwell Libbrecht, Michael Hoffman, Jeff Bilmes, William Noble",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/libbrecht15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/libbrecht15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_libbrecht15,http://jmlr.csail.mit.edu/proceedings/papers/v37/libbrecht15.html,"Graph smoothness objectives have achieved great success in semi-supervised learning but have not yet been applied extensively to unsupervised generative models. We define a new class of entropic graph-based posterior regularizers that augment a probabilistic model by encouraging pairs of nearby variables in a regularization graph to have similar posterior distributions. We present a three-way alternating optimization algorithm with closed-form updates for performing inference on this joint model and learning its parameters. This method admits updates linear in the degree of the regularization graph, exhibits monotone convergence and is easily parallelizable. We are motivated by applications in computational biology in which temporal models such as hidden Markov models are used to learn a human-interpretable representation of genomic data. On a synthetic problem, we show that our method outperforms existing methods for graph-based regularization and a comparable strategy for incorporating long-range interactions using existing methods for approximate inference. Using genome-scale functional genomics data, we integrate genome 3D interaction data into existing models for genome annotation and demonstrate significant improvements in predicting genomic activity."
2009,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Globally Optimizing Graph Partitioning Problems Using Message Passing,"Elad Mezuman, Yair Weiss",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/mezuman12/mezuman12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_mezuman12,http://jmlr.csail.mit.edu/proceedings/papers/v22/mezuman12.html,"Graph partitioning algorithms play a central role in data analysis and machine learning. Most useful graph partitioning criteria correspond to optimizing a ratio between the cut and the size of the partitions this ratio leads to an NP-hard problem that is only solved approximately. This makes it difficult to know whether failures of the algorithm are due to failures of the optimization or to the criterion being optimized. In this paper we present a framework that seeks and finds the optimal solution of several NP-hard graph partitioning problems. We use a classical approach to ratio problems where we repeatedly ask whether the optimal solution is greater than or less than some constant - lambda. Our main insight is the equivalence between this ""lambda question"" and performing inference in a graphical model with many local potentials and one high-order potential. We show that this specific form of the high-order potential is amenable to message-passing algorithms and how to obtain a bound on the optimal solution from the messages. Our experiments show that in many cases our approach yields the global optimum and improves the popular spectral solution."
2010,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Divide and Conquer Framework for Distributed Graph Clustering,"Wenzhuo Yang, Huan Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yange15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yange15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yange15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yange15.html,"Graph clustering is about identifying clusters of closely connected nodes, and is a fundamental technique of data analysis with many applications including community detection, VLSI network partitioning, collaborative filtering, and many others. In order to improve the scalability of existing graph clustering algorithms, we propose a novel divide and conquer framework for graph clustering, and establish theoretical guarantees of exact recovery of the clusters. One additional advantage of the proposed framework is that it can identify small clusters _ the size of the smallest cluster can be of size \(o(\sqrt{n})\) , in contrast to \(\Omega(\sqrt{n})\) required by standard methods. Extensive experiments on synthetic and real-world datasets demonstrate the efficiency and effectiveness of our framework."
2011,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Discovering Temporal Causal Relations from Subsampled Data,"Mingming Gong, Kun Zhang, Bernhard Schoelkopf, Dacheng Tao, Philipp Geiger",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gongb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/gongb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gongb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gongb15.html,"Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the high frequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches."
2012,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Evaluation of selection in context-free grammar learning systems,"Menno van Zaanen, Nanne van Noord","JMLR W&CP 34 :193-206, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/vanzaanen14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_vanzaanen14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/vanzaanen14a.html,"Grammatical inference deals with learning of grammars describing languages. Formal grammatical inference aims at identifying families of languages that have a shared property, which can be used to prove efficient learnability of the families formally. In contrast, in empirical grammatical inference research, practical systems are developed that are applied to languages. The effectiveness of these systems is measured by comparing the learned grammar against a Gold standard which indicates the ground truth. From successful empirical learnability results, either shared properties may be identified, leading to further formal learnability results, or modifications to the systems may be made, improving practical results. Proper evaluation of empirical systems is, therefore, essential. Here, we evaluate and compare existing state-of-the-art context-free grammar learning systems (and novel systems based on combinations of existing phases) in a standardized evaluation environment (on a corpus of plain natural language sentences), illustrating future directions for empirical grammatical inference research."
2013,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Infinite Predictor Subspace Models for Multitask Learning,"Piyush Rai, Hal Daume III","9:613-620, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/rai10a/rai10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_rai10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/rai10a.html,Given several related learning tasks we propose a nonparametric Bayesian model that captures task relatedness by assuming that the task parameters (i.e. predictors) share a latent subspace. More specifically the intrinsic dimensionality of the task subspace is not assumed to be known a priori. We use an infinite latent feature model to automatically infer this number (depending on and limited by only the number of tasks). Furthermore our approach is applicable when the underlying task parameter subspace is inherently sparse drawing parallels with l1 regularization and LASSO-style models. We also propose an augmented model which can make use of (labeled and additionally unlabeled if available) inputs to assist learning this subspace leading to further improvements in the performance. Experimental results demonstrate the efficacy of both the proposed approaches especially when the number of examples per task is small. Finally we discuss an extension of the proposed framework where a nonparametric mixture of linear subspaces can be used to learn a manifold over the task parameters and also deal with the issue of negative transfer from unrelated tasks.
2014,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,A Composite Likelihood View for Multi-Label Classification,"Yi Zhang, Jeff Schneider",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12b/zhang12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhang12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhang12b.html,Given limited training samples learning to classify multiple labels is challenging. Problem decomposition is widely used in this case where the original problem is decomposed into a set of easier-to-learn subproblems and predictions from subproblems are combined to make the final decision. In this paper we show the connection between composite likelihoods and many multi-label decomposition methods e.g. one-vs-all one-vs-one calibrated label ranking probabilistic classifier chain. This connection holds promise for improving problem decomposition in both the choice of subproblems and the combination of subproblem decisions. As an attempt to exploit this connection we design a composite marginal method that improves pairwise decomposition. Pairwise label comparisons which seem to be a natural choice for subproblems are replaced by bivariate label densities which are more informative and natural components in a composite likelihood. For combining subproblem decisions we propose a new mean-field approximation that minimizes the notion of composite divergence and is potentially more robust to inaccurate estimations in subproblems. Empirical studies on five data sets show that given limited training samples the proposed method outperforms many alternatives.
2015,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Group Nonnegative Matrix Factorization for EEG Classification,"Hyekyoung Lee, Seungjin Choi","5:320-327, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/lee09a/lee09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_lee09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/lee09a.html,Given EEG data measured from several subjects under the same condition our goal is to estimate common task-related bases in a linear model that capture intra-subject variations as well as inter-subject variations. Such bases capture the common phenomenon in a group data which is known as group analysis. In this paper we present a method of nonnegative matrix factorization (NMF) that is well suited to analyze EEG data of multiple subjects. The method is referred to as group nonnegative matrix factorization (GNMF) where we seek task-related common bases reflecting both intra-subject and inter-subject variations as well as bases involving individual characteristics. We compare GNMF with NMF and some modified NMFs in a task of learning spectral features from EEG data. Experiments on BCI competition data indicate that GNMF improves the EEG classification performance. In addition we also show that GNMF is useful in a task of subject-to-subject transfer where the prediction for an unseen subject is performed based on a linear model learned from different subjects in the same group.
2016,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems,"Yash Deshpande, Andrea Montanari",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Deshpande15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Deshpande15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Deshpande15.html,"Given a large data matrix \(A\in\mathbb{R}^{n\times n}\) , we consider the problem of determining whether its entries are i.i.d. from some known marginal distribution \(A_{ij}\sim P_0\) , or instead \(A\) contains a principal submatrix \(A_{{\sf Q},{\sf Q}}\) whose entries have marginal distribution \(A_{ij}\sim P_1\neq P_0\) . As a special case, the hidden (or planted) clique problem is finding a planted clique in an otherwise uniformly random graph. Assuming unbounded computational resources, this hypothesis testing problem is statistically solvable provided \(|{\sf Q}|\ge C \log n\) for a suitable constant \(C\) . However, despite substantial effort, no polynomial time algorithm is known that succeeds with high probability when \(|{\sf Q}| = o(\sqrt{n})\) . Recently, proposed a method to establish lower bounds for the hidden clique problem within the Sum of Squares (SOS) semidefinite hierarchy. Here we consider the degree- \(4\) SOS relaxation, and study the construction of to prove that SOS fails unless \(k\ge C\, n^{1/3}/\log n\) . An argument presented by implies that this lower bound cannot be substantially improved unless the witness construction is changed in the proof. Our proof uses the moment method to bound the spectrum of a certain random association scheme, i.e. a symmetric random matrix whose rows and columns are indexed by the edges of an Erd_s-Renyi random graph."
2017,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Hierarchical Label Queries with Data-Dependent Partitions,"Samory Kpotufe, Ruth Urner, Shai Ben-David",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kpotufe15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Kpotufe15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Kpotufe15.html,"Given a joint distribution \(P_{X, Y}\) over a space \(\Xcal\) and a label set \(\Ycal=\braces{0, 1}\) , we consider the problem of recovering the labels of an unlabeled sample with as few label queries as possible. The recovered labels can be passed to a passive learner, thus turning the procedure into an active learning approach. We analyze a family of labeling procedures based on a hierarchical clustering of the data. While such labeling procedures have been studied in the past, we provide a new parametrization of \(P_{X, Y}\) that captures their behavior in general low-noise settings, and which accounts for data-dependent clustering, thus providing new theoretical underpinning to practically used tools."
2018,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Transductive Learning with Multi-class Volume Approximation,"Gang Niu, Bo Dai, Christoffel du Plessis, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/niu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/niu14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_niu14,http://jmlr.csail.mit.edu/proceedings/papers/v32/niu14.html,"Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we propose a novel generalization to multiple classes, allowing applications of the large volume principle on more learning problems such as multi-class, multi-label and serendipitous learning in a transductive manner. Although the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained using O(n 3 ) time. Novel theoretical analyses are presented for the proposed method, and experimental results show it compares favorably with the one-vs-rest extension."
2019,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,"Revisiting MAP Estimation, Message Passing and Perfect Graphs","James Foulds, Nicholas Navaroli, Padhraic Smyth, Alexander Ihler","15:278-286, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/foulds11a/foulds11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_foulds11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/foulds11a.html,Given a graphical model one of them ost useful queries is to find the most likely configuration of its variables. This task known as the maximum a posteriori (MAP) problem can be solved efficiently via message passing techniques when the graph is a tree but is NP-hard for general graphs. Jebara (2009) shows that the MAP problem can be converted into the stable set problem which can be solved in polynomial time for a broad class of graphs known as perfect graphs via a linear programming relaxation technique. This is a result of great theoretical interest. However the article additionally claims that max-product linear programming (MPLP) message passing techniques of Globerson and Jaakkola (2007) are also guaranteed to solve these problems exactly and efficiently. We investigate this claim show that it does not hold and repair it with alternative message passing algorithms.
2020,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Finding Dense Subgraphs via Low-Rank Bilinear Optimization,"Dimitris Papailiopoulos, Ioannis Mitliagkas, Alexandros Dimakis, Constantine Caramanis",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/papailiopoulos14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/papailiopoulos14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_papailiopoulos14,http://jmlr.csail.mit.edu/proceedings/papers/v32/papailiopoulos14.html,"Given a graph, the Densest \(k\) -Subgraph () problem asks for the subgraph on \(k\) vertices that contains the largest number of edges. In this work, we develop a novel algorithm for that searches a low-dimensional space for provably good solutions. We obtain provable performance bounds that depend on the graph spectrum. One of our results is that if there exists a \(k\) -subgraph that contains a constant fraction of all the edges, we can approximate within a factor arbitrarily close to two in polynomial time. Our algorithm runs in nearly linear time, under spectral assumptions satisfied by most graphs found in applications. Moreover, it is highly scalable and parallelizable. We demonstrate this by implementing it in MapReduce and executing numerous experiments on massive real-world graphs that have up to billions of edges. We empirically show that our algorithm can find subgraphs of significantly higher density compared to the previous state of the art."
2021,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Enhanced statistical rankings via targeted data collection,"Braxton Osting, Christoph Brune, Stanley Osher",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/osting13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_osting13,http://jmlr.csail.mit.edu/proceedings/papers/v28/osting13.html,"Given a graph where vertices represent alternatives and pairwise comparison data, \(y_{ij}\) , is given on the edges, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with pairwise comparisons. We study the dependence of the statistical ranking problem on the available pairwise data, i.e., pairs (i,j) for which the pairwise comparison data \(y_{ij}\) is known, and propose a framework to identify data which, when augmented with the current dataset, maximally increases the Fisher information of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding an edge set on the graph (with a fixed number of edges) such that the second eigenvalue of the graph Laplacian is maximal. This reduction of the data collection problem to a spectral graph-theoretic question is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking."
2022,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Geometric Conditions for Subspace-Sparse Recovery,"Chong You, Rene Vidal",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/you15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/you15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_you15,http://jmlr.csail.mit.edu/proceedings/papers/v37/you15.html,"Given a dictionary \(\Pi\) and a signal \(\xi = \Pi \mathbf x\) generated by a few linearly independent columns of \(\Pi\) , classical sparse recovery theory deals with the problem of uniquely recovering the sparse representation \(\mathbf x\) of \(\xi\) . In this work, we consider the more general case where \(\xi\) lies in a low-dimensional subspace spanned by a few columns of \(\Pi\) , which are possibly linearly dependent . In this case, \(\mathbf x\) may not unique, and the goal is to recover any subset of the columns of \(\Pi\) that spans the subspace containing \(\xi\) . We call such a representation \(\mathbf x\) subspace-sparse . We study conditions under which existing pursuit methods recover a subspace-sparse representation. Such conditions reveal important geometric insights and have implications for the theory of classical sparse recovery as well as subspace clustering."
2023,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Learning to Top-K Search using Pairwise Comparisons,Brian Eriksson,none,http://jmlr.csail.mit.edu/proceedings/papers/v31/eriksson13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_eriksson13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/eriksson13a.html,"Given a collection of \(N\) items with some unknown underlying ranking, we examine how to use pairwise comparisons to determine the top ranked items in the set. Resolving the top items from pairwise comparisons has application in diverse fields ranging from recommender systems to image-based search to protein structure analysis. In this paper we introduce techniques to resolve the top ranked items using significantly fewer than all the possible pairwise comparisons using both random and adaptive sampling methodologies. Using randomly-chosen comparisons, a graph-based technique is shown to efficiently resolve the top \(O(\log{N})\) items when there are no comparison errors. In terms of adaptively-chosen comparisons, we show how the top \(O(\log{N})\) items can be found, even in the presence of corrupted observations, using a voting methodology that only requires \(O(N\log^2{N})\) pairwise comparisons."
2024,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Calibration of conditional composite likelihood for Bayesian inference on Gibbs random fields,"Julien Stoehr, Nial Friel",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/stoehr15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_stoehr15,http://jmlr.csail.mit.edu/proceedings/papers/v38/stoehr15.html,"Gibbs random fields play an important role in statistics, however, the resulting likelihood is typically unavailable due to an intractable normalizing constant. Composite likelihoods offer a principled means to construct useful approximations. This paper provides a mean to calibrate the posterior distribution resulting from using a composite likelihood and illustrate its performance in several examples."
2025,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Locally Substitutable Languages for Enhanced Inductive Leaps,"François Coste, Gaïlle Garet and Jacques Nicolas","21:97-111, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/coste12a/coste12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_coste12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/coste12a.html,"Genomic banks are fed continuously by large sets of DNA or RNA sequences coming from high throughput machines. Protein annotation is a task of first importance with respect to these banks. It consists of retrieving the genes that code for proteins within the sequences and then predict the function of these new proteins in the cell by comparison with known families. Many methods have been designed to characterize protein families and discover new members, mainly based on subsets of regular expressions or simple Hidden Markov Models. We are interested in more expressive models that are able to capture the long-range characteristic interactions occurring in the spatial structure of the analyzed protein family. Starting from the work of Clark and Eyraud (2007) and Yoshinaka (2008) on inference of substitutable and k,l -substitutable languages respectively, we introduce new classes of substitutable languages using local rather than global substitutability, a reasonable assumption with respect to protein structures to enhance inductive leaps performed by least generalized generalization approaches. The concepts are illustrated on a first experiment using a real proteic sequence set."
2026,47,http://jmlr.csail.mit.edu/proceedings/papers/v47/,Statistically significant subgraphs for genome-wide association study,"Jun Sese, Aika Terada, Yuki Saito, Koji Tsuda",none,http://jmlr.csail.mit.edu/proceedings/papers/v47/sese14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v47/,,27th November 2015,41897,ECML/PKDD Workshop on Statistically Sound Data Mining 2014 Proceedings,Statistically Sound Data Mining,"Joensuu, Finland","Wilhelmiina HÕ_mÕ_lÕ_inen, FranÕ_ois Petitjean, Geoffrey, I. Webb",v47_sese14a,http://jmlr.csail.mit.edu/proceedings/papers/v47/sese14a.html,"Genome-wide association studies (GWAS) have been widely used for understanding the associations of single-nucleotide polymorphisms (SNPs) with a disease. GWAS data are often combined with known biological networks, and they have been analyzed using graph-mining techniques toward a systems understanding of the biological changes caused by the SNPs. To determine which subgraphs are associated with the disease, a statistical test on each subgraph needs to be conducted. However, no statistically significant results were found because multiple testing correction causes an extremely small corrected significance level. We introduce a method called gLAMP to enumerate subgraphs having statistically significant associations with a diagnosis. gLAMP integrates the Limitless Arity Multiple-testing Procedure (LAMP) with a graph-mining algorithm called COmmon Itemset Network mining (COIN). LAMP gives us the smallest possible Bonferroni factor, and COIN provides us with efficient enumeration of testable subgraphs. Theoretical results of their combination show the potential to enumerate subgraphs statistically significantly associated with a disease."
2027,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Consensus Message Passing for Layered Graphical Models,"Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli, John Winn",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/jampani15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/jampani15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_jampani15,http://jmlr.csail.mit.edu/proceedings/papers/v38/jampani15.html,"Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing êconsensusê messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models."
2028,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Phrase-based Image Captioning,"Remi Lebret, Pedro Pinheiro, Ronan Collobert",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/lebret15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_lebret15,http://jmlr.csail.mit.edu/proceedings/papers/v37/lebret15.html,"Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely linear model to embed an image representation (generated from a previously trained Convolutional Neural Network) into a multimodal space that is common to the images and the phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on the sentence description statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO."
2029,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes,"Huitong Qiu, Sheng Xu, Fang Han, Han Liu, Brian Caffo",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/qiu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/qiu15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_qiu15,http://jmlr.csail.mit.edu/proceedings/papers/v37/qiu15.html,"Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction."
2030,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Gaussian Processes for Bayesian hypothesis tests on regression functions,"Alessio Benavoli, Francesca Mangili",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/benavoli15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_benavoli15,http://jmlr.csail.mit.edu/proceedings/papers/v38/benavoli15.html,"Gaussian processes have been used in different application domains such as classification, regression etc. In this paper we show that they can also be employed as a universal tool for developing a large variety of Bayesian statistical hypothesis tests for regression functions. In particular, we will use GPs for testing whether (i) two functions are equal; (ii) a function is monotone (even accounting for seasonality effects); (iii) a function is periodic; (iv) two functions are proportional. By simulation studies, we will show that, beside being more flexible, GP tests are also competitive in terms of performance with state-of-art algorithms."
2031,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Gaussian Process Kernels for Pattern Discovery and Extrapolation,"Andrew Wilson, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wilson13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/wilson13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wilson13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wilson13.html,"Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density _ the Fourier transform of a kernel _ with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework."
2032,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Analytic Long-Term Forecasting with Periodic Gaussian Processes,"Nooshin HajiGhassemi, Marc Deisenroth","JMLR W&CP 33 :303-311, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hajighassemi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hajighassemi14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hajighassemi14.html,"Gaussian processes are a state-of-the-art method for learning models from data. Data with an underlying periodic structure appears in many areas, e.g., in climatology or robotics. It is often important to predict the long-term evolution of such a time series, and to take the inherent periodicity explicitly into account. In a Gaussian process, periodicity can be accounted for by an appropriate kernel choice. However, the standard periodic kernel does not allow for analytic long-term forecasting, which requires to map distributions through the Gaussian process. To address this shortcoming, we re-parametrize the periodic kernel, which, in combination with a double approximation, allows for analytic long-term forecasting of a periodic state evolution with Gaussian processes. Our model allows for probabilistic long-term forecasting of periodic processes, which can be valuable in Bayesian decision making, optimal control, reinforcement learning, and robotics."
2033,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods,"Seth Flaxman, Andrew Wilson, Daniel Neill, Hannes Nickisch, Alex Smola",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/flaxman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/flaxman15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_flaxman15,http://jmlr.csail.mit.edu/proceedings/papers/v37/flaxman15.html,"Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require \(O(n^3)\) computations and \(O(n^2)\) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring \(O(D n^{(D+1)/D})\) operations and \(O(D n^{2/D})\) storage, for n training data-points on a dense D _ 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts."
2034,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Sparse Variational Inference for Generalized GP Models,"Rishit Sheth, Yuyang Wang, Roni Khardon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sheth15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sheth15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sheth15.html,"Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach."
2035,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Fast Near-GRID Gaussian Process Regression,"Yuancheng Luo, Ramani Duraiswami",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/luo13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_luo13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/luo13b.html,"Gaussian process regression (GPR) is a powerful non-linear technique for Bayesian inference and prediction. One drawback is its O( \(N^3\) ) computational complexity for both prediction and hyperparameter estimation for \(N\) input points which has led to much work in sparse GPR methods. In case that the covariance function is expressible as a tensor product kernel (TPK) and the inputs form a multidimensional grid, it was shown that the costs for exact GPR can be reduced to a sub-quadratic function of \(N\) . We extend these exact fast algorithms to sparse GPR and remark on a connection to Gaussian process latent variable models (GPLVMs). In practice, the inputs may also violate the multidimensional grid constraints so we pose and efficiently solve missing and extra data problems for both exact and sparse grid GPR. We demonstrate our method on synthetic, text scan, and magnetic resonance imaging (MRI) data reconstructions."
2036,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Scalable Variational Gaussian Process Classification,"James Hensman, Alexander Matthews, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/hensman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/hensman15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_hensman15,http://jmlr.csail.mit.edu/proceedings/papers/v38/hensman15.html,"Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of the art on benchmark datasets. Importantly, the variational formulation an be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments."
2037,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Local and global sparse Gaussian process approximations,"Edward Snelson, Zoubin Ghahramani","2:524-531, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/snelson07a/snelson07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_snelson07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/snelson07a.html,Gaussian process (GP) models are flexible probabilistic nonparametric models for regression classification and other tasks. Unfortunately they suffer from computational intractability for large data sets. Over the past decade there have been many different approximations developed to reduce this cost. Most of these can be termed global approximations in that they try to summarize all the training data via a small set of support points. A different approach is that of local regression where many local experts account for their own part of space. In this paper we start by investigating the regimes in which these different approaches work well or fail. We then proceed to develop a new sparse GP approximation which is a combination of both the global and local approaches. Theoretically we show that it is derived as a natural extension of the framework developed by Qui~onero Candela and Rasmussen [2005] for n sparse GP approximations. We demonstrate the benefits of the combined approximation on some 1D examples for illustration and on some large real-world data sets.
2038,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Adaptive Variable Clustering in Gaussian Graphical Models,"Siqi Sun, Yuancheng Zhu, Jinbo Xu","JMLR W&CP 33 :931-939, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/sun14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_sun14,http://jmlr.csail.mit.edu/proceedings/papers/v33/sun14.html,"Gaussian graphical models (GGMs) are widely-used to describe the relationship between random variables. In many real-world applications, GGMs have a block structure in the sense that the variables can be clustered into groups so that inter-group correlation is much weaker than intra-group correlation. We present a novel nonparametric Bayesian generative model for such a block-structured GGM and an efficient inference algorithm to find the clustering of variables in this GGM by combining a Gibbs sampler and a split-merge Metropolis-Hastings algorithm. Experimental results show that our method performs well on both synthetic and real data. In particular, our method outperforms generic clustering algorithms and can automatically identify the true number of clusters."
2039,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Latent Variable Gaussian Graphical Models,"Zhaoshi Meng, Brian Eriksson, Al Hero",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/meng14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/meng14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_meng14,http://jmlr.csail.mit.edu/proceedings/papers/v32/meng14.html,"Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference."
2040,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Entropic Affinities: Properties and Efficient Numerical Computation,"Max Vladymyrov, Miguel Carreira-Perpinan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/vladymyrov13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/vladymyrov13-supp.zip,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_vladymyrov13,http://jmlr.csail.mit.edu/proceedings/papers/v28/vladymyrov13.html,"Gaussian affinities are commonly used in graph-based methods such as spectral clustering or nonlinear embedding. Hinton and Roweis (2003) introduced a way to set the scale individually for each point so that it has a distribution over neighbors with a desired perplexity, or effective number of neighbors. This gives very good affinities that adapt locally to the data but are harder to compute. We study the mathematical properties of these –entropic affinities” and show that they implicitly define a continuously differentiable function in the input space and give bounds for it. We then devise a fast algorithm to compute the widths and affinities, based on robustified, quickly convergent root-finding methods combined with a tree- or density-based initialization scheme that exploits the slowly-varying behavior of this function. This algorithm is nearly optimal and much more accurate and fast than the existing bisection-based approach, particularly with large datasets, as we show with image and text data."
2041,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Functional Subspace Clustering with Application to Time Series,"Mohammad Taha Bahadori, David Kale, Yingying Fan, Yan Liu",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/bahadori15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/bahadori15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_bahadori15,http://jmlr.csail.mit.edu/proceedings/papers/v37/bahadori15.html,"Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering."
2042,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Interpretable Sparse High-Order Boltzmann Machines,"Martin Renqiang Min, Xia Ning, Chao Cheng, Mark Gerstein","JMLR W&CP 33 :614-622, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/min14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/min14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_min14,http://jmlr.csail.mit.edu/proceedings/papers/v33/min14.html,"Fully-observable high-order Boltzmann Machines are capable of identifying explicit high-order feature interactions theoretically. However, they have never been used in practice due to their prohibitively high computational cost for inference and learning. In this paper, we propose an efficient approach for learning a fully observable high-order Boltzmann Machine based on sparse learning and contrastive divergence, resulting in an interpretable Sparse High-order Boltzmann Machine, denoted as SHBM. Experimental results on synthetic datasets and a real dataset demonstrate that SHBM can produce higher pseudo-log-likelihood and better reconstructions on test data than the state-of-the-art methods. In addition, we apply SHBM to a challenging bioinformatics problem of discovering complex Transcription Factor interactions. Compared to conventional Boltzmann Machine and directed Bayesian Network, SHBM can identify much more biologically meaningful interactions that are supported by recent biological studies. To the best of our knowledge, SHBM is the first working Boltzmann Machine with explicit high-order feature interactions applied to real-world problems."
2043,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Inference for Scalable Probabilistic Modeling of Binary Matrices,"Jose Miguel Hernandez-Lobato, Neil Houlsby, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatoa14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatoa14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hernandez-lobatoa14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hernandez-lobatoa14.html,"Fully observed large binary matrices appear in a wide variety of contexts. To model them, probabilistic matrix factorization (PMF) methods are an attractive solution. However, current batch algorithms for PMF can be inefficient because they need to analyze the entire data matrix before producing any parameter updates. We derive an efficient stochastic inference algorithm for PMF models of fully observed binary matrices. Our method exhibits faster convergence rates than more expensive batch approaches and has better predictive performance than scalable alternatives. The proposed method includes new data subsampling strategies which produce large gains over standard uniform subsampling. We also address the task of automatically selecting the size of the minibatches of data used by our method. For this, we derive an algorithm that adjusts this hyper-parameter online."
2044,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Proceedings of the 4th Workshop on Machine Learning for Interactive Systems (MLIS-2015),"Heriberto Cuayˆhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/frontmatter.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_frontmatter,http://jmlr.csail.mit.edu/proceedings/papers/v43/frontmatter.html,Frontmatter for Workshop Proceedings.
2045,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning,"Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes","JMLR W&CP 35 :440-460, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/maurer14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_maurer14,http://jmlr.csail.mit.edu/proceedings/papers/v35/maurer14.html,From concentration inequalities for the suprema of Gaussian or Rademacher processes an inequality is derived. It is applied to sharpen existing and to derive novel bounds on the empirical Rademacher complexities of unit balls in various norms appearing in the context of structured sparsity and multitask dictionary learning or matrix factorization. A key role is played by the largest eigenvalue of the data covariance matrix.
2046,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Parallel Graph Mining with GPUs,"Robert Kessl, Nilothpal Talukder, Pranay Anchuri, Mohammed Zaki","JMLR W&CP 36 :1-16, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/kessl14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_kessl14,http://jmlr.csail.mit.edu/proceedings/papers/v36/kessl14.html,"Frequent graph mining is an important though computationally hard problem because it requires enumerating possibly an exponential number of candidate subgraph patterns, and checking their presence in a database of graphs. In this paper, we propose a novel approach for parallel graph mining on GPUs, which have emerged as a relatively cheap but powerful architecture for general purpose computing. However, the thread-model for GPUs is different from that of CPUs, which makes the parallelization of graph mining algorithms on GPUs a challenging task. We investigate the major challenges for GPU-based graph mining. We perform extensive experiments on several real-world and synthetic datasets, achieving speedups up to 9 over the sequential algorithm."
2047,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Compilation Target for Probabilistic Programming Languages,"Brooks Paige, Frank Wood",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/paige14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_paige14,http://jmlr.csail.mit.edu/proceedings/papers/v32/paige14.html,"Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory. Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target. This opens up a new hardware and systems research path for optimizing probabilistic programming systems."
2048,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Theoretical Analyses on Ensemble and Multiple Kernel Regressors,"Akira Tanaka, Ichigaku Takigawa, Hideyuki Imai, Mineichi Kudo",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/tanaka14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_tanaka14,http://jmlr.csail.mit.edu/proceedings/papers/v39/tanaka14.html,"For the last few decades, a combination of different learning machines so-called ensemble learning, including learning with multiple kernels, has attracted much attention in the field of machine learning. Although its efficacy was revealed numerically in many works, its theoretical grounds are not investigated sufficiently. In this paper, we discuss regression problems with a class of kernels whose corresponding reproducing kernel Hilbert spaces have a common subspace with an invariant metric and prove that the ensemble kernel regressor (the mean of kernel regressors with those kernels) gives a better learning result than the multiple kernel regressor (the kernel regressor with the sum of those kernels) in terms of the generalization ability of a model space."
2049,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Learning Local Invariant Mahalanobis Distances,"Ethan Fetaya, Shimon Ullman",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/fetaya15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_fetaya15,http://jmlr.csail.mit.edu/proceedings/papers/v37/fetaya15.html,"For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance."
2050,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Collaborative Ranking for Local Preferences,"Berk Kapicioglu, David Rosenberg, Robert Schapire, Tony Jebara","JMLR W&CP 33 :466-474, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kapicioglu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/kapicioglu14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kapicioglu14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kapicioglu14.html,"For many collaborative ranking tasks, we have access to relative preferences among subsets of items, but not to global preferences among all items. To address this, we introduce a matrix factorization framework called Collaborative Local Ranking (CLR). We justify CLR by proving a bound on its generalization error, the first such bound for collaborative ranking that we know of. We then derive a simple alternating minimization algorithm and prove that it converges in sublinear time. Lastly, we apply CLR to a novel venue recommendation task and demonstrate that it outperforms state-of-the-art collaborative ranking methods on real-world data sets."
2051,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Maximum-likelihood learning of cumulative distribution functions on graphs,"Jim Huang, Nebojsa Jojic","9:342-349, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10b/huang10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_huang10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/huang10b.html,For many applications a probability model can be easily expressed as a cumulative distribution functions (CDF) as compared to the use of probability density or mass functions (PDF/PMFs). Cumulative distribution networks (CDNs) have recently been proposed as a class of graphical models for CDFs. One advantage of CDF models is the simplicity of representing multivariate heavy-tailed distributions. Examples of fields that can benefit from the use of graphical models for CDFs include climatology and epidemiology where data may follow extreme value statistics and exhibit spatial correlations so that dependencies between model variables must be accounted for. The problem of learning from data in such settings may nevertheless consist of optimizing the log-likelihood function with respect to model parameters where we are required to optimize a log-PDF/PMF and not a log-CDF. We present a message-passing algorithm called the gradient-derivative-product (GDP) algorithm that allows us to learn the model in terms of the log-likelihood function whereby messages correspond to local gradients of the likelihood with respect to model parameters. We will demonstrate the GDP algorithm on real-world rainfall and H1N1 mortality data and we will show that CDNs provide a natural choice of parameterizations for the heavy-tailed multivariate distributions that arise in these problems.
2052,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Multiview Triplet Embedding: Learning Attributes in Multiple Maps,"Ehsan Amid, Antti Ukkonen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/amid15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_amid15,http://jmlr.csail.mit.edu/proceedings/papers/v37/amid15.html,"For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers. We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects."
2053,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Nonparametric prior for adaptive sparsity,"Vikas Raykar, Linda Zhao","9:629-636, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/raykar10a/raykar10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_raykar10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/raykar10a.html,For high-dimensional problems various parametric priors have been proposed to promote sparse solutions. While parametric priors has shown considerable success they are not very robust in adapting to varying degrees of sparsity. In this work we propose a discrete mixture prior which is partially nonparametric. The right structure for the prior and the amount of sparsity is estimated directly from the data. Our experiments show that the proposed prior adapts to sparsity much better than its parametric counterparts. We apply the proposed method to classification of high dimensional microarray datasets.
2054,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner,"Peng Sun, Jie Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sun13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/sun13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sun13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sun13.html,"For a well trained Boosting classifier, we are interested in how to save the testing time, i.e., to make the decision without evaluating all the base learners. To address this problem, in previous work the base learners are sequentially calculated and early stopping is allowed if the decision function has been confident enough to output its value. In such a chain structure, the order of base learners is critical: better order can lead to less evaluation time. In this paper, we present a novel method for ordering. We base our discussion on the data structure representing Boostingês decision function. Viewing the decision function a boolean expression, we propose a Binary Valued Tree for its representation. As a secondary contribution, such a representation unifies the work by previous researchers and helps devise new representation. Also, its connection to Binary Decision Diagram(BDD) is discussed."
2055,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,An Instantiation-Based Theorem Prover for First-Order Programming,"Erik Zawadzki, Geoffrey Gordon, Andre Platzer","15:855-863, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zawadzki11a/zawadzki11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zawadzki11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/zawadzki11a.html,First-order programming (FOP) is a new representation language that combines the strengths of mixed-integer linear programming (MILP) and first-order logic (FOL). In this paper we describe a novel feasibility proving system for FOP formulas that combines MILP solving with instance-based methods from theorem proving. This prover allows us to perform lifted inference by repeatedly refining a propositional MILP. We prove that this procedure is sound and refutationally complete: if a formula is infeasible our solver will demonstrate this fact in finite time. We conclude by demonstrating an implementation of our decision procedure on a simple first-order planning problem.
2056,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: The Oracle Complexity of Smooth Convex Optimization in Nonstandard Settings,Crist„bal Guzmˆn,none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Guzman15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Guzman15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Guzman15.html,"First-order convex minimization algorithms are currently the methods of choice for large-scale sparse _ and more generally parsimonious _ regression models. We pose the question on the limits of performance of black-box oriented methods for convex minimization in non-standard settings , where the regularity of the objective is measured in a norm not necessarily induced by the feasible domain. This question is studied for \(\ell_p/\ell_q\) -settings, and their matrix analogues (Schatten norms), where we find surprising gaps on lower bounds compared to state of the art methods. We propose a conjecture on the optimal convergence rates for these settings, for which a positive answer would lead to significant improvements on minimization algorithms for parsimonious regression models."
2057,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Fictitious Self-Play in Extensive-Form Games,"Johannes Heinrich, Marc Lanctot, David Silver",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/heinrich15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/heinrich15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_heinrich15,http://jmlr.csail.mit.edu/proceedings/papers/v37/heinrich15.html,"Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria."
2058,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis,"Zheng Zhao, Huan Liu","4:36-47, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/zhao08a/zhao08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_zhao08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/zhao08a.html,Feature selection is an effective approach to reducing dimensionality by selecting relevant original features. In this work we studied a novel problem of multi-source feature selection for unlabeled data: given multiple heterogeneous data sources (or data sets) select features from one source of interest by integrating information from various data sources. In essence we investigate how we can employ the information contained in multiple data sources to effectively derive intrinsic relationships that can help select more meaningful (or domain relevant) features. We studied how to adjust the covariance matrix of a data set using the geometric structure obtained from multiple data sources and how to select features of the target source using geometry-dependent covariance. We designed and conducted experiments to systematically compare the proposed approach with representative methods in our attempt to solve the novel problem of multi-source feature selection. The empirical study demonstrated the efficacy and potential of multi-source feature selection.
2059,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Feature Selection using Multiple Streams,"Paramveer Dhillon, Dean Foster, Lyle Ungar","9:153-160, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/dhillon10a/dhillon10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_dhillon10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/dhillon10a.html,"Feature selection for supervised learning can be greatly improved by making use of the fact that features often come in classes. For example in gene expression data the genes which serve as features may be divided into classes based on their membership in gene families or pathways. When labeling words with senses for word sense disambiguation features fall into classes including adjacent words their parts of speech and the topic and venue of the document the word is in. We present a streamwise feature selection method that allows dynamic generation and selection of features while taking advantage of the different feature classes and the fact that they are of different sizes and have different (but unknown) fractions of good features. Experimental results show that our approach provides significant improvement in performance and is computationally less expensive than comparable ``batch"" methods that do not take advantage of the feature classes and expect all features to be known in advance."
2060,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,High-Dimensional Structured Feature Screening Using Binary Markov Random Fields,"Jie Liu, Chunming Zhang, Catherine Mccarty, Peggy Peissig, Elizabeth Burnside, David Page",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/liu12b/liu12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_liu12b,http://jmlr.csail.mit.edu/proceedings/papers/v22/liu12b.html,Feature screening is a useful feature selection approach for high-dimensional data when the goal is to identify all the features relevant to the response variable. However common feature screening methods do not take into account the correlation structure of the covariate space. We propose the concept of a feature relevance network a binary Markov random field to represent the relevance of each individual feature by potentials on the nodes and represent the correlation structure by potentials on the edges. By performing inference on the feature relevance network we can accordingly select relevant features. Our algorithm does not yield sparsity which is different from the particular popular family of feature selection approaches based on penalized least squares or penalized pseudo-likelihood. We give one concrete algorithm under this framework and show its superior performance over common feature selection methods in terms of prediction error and recovery of the truly relevant features on real-world data and synthetic data.
2061,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Ultrahigh Dimensional Feature Screening via RKHS Embeddings,"Krishnakumar Balasubramanian, Bharath Sriperumbudur, Guy Lebanon",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/balasubramanian13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/balasubramanian13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_balasubramanian13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/balasubramanian13a.html,"Feature screening is a key step in handling ultrahigh dimensional data sets that are ubiquitous in modern statistical problems. Over the last decade, convex relaxation based approaches (e.g., Lasso/sparse additive model) have been extensively developed and analyzed for feature selection in high dimensional regime. But in the ultrahigh dimensional regime, these approaches suffer from several problems, both computationally and statistically. To overcome these issues, in this paper, we propose a novel Hilbert space embedding based approach to independence screening for ultrahigh dimensional data sets. The proposed approach is model-free (i.e., no model assumption is made between response and predictors) and could handle non-standard (e.g., graphs) and multivariate outputs directly. We establish the sure screening property of the proposed approach in the ultrahigh dimensional regime, and experimentally demonstrate its advantages and superiority over other approaches on several synthetic and real data sets."
2062,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,FEAST at Play: Feature ExtrAction using Score function Tensors,"Majid Janzamin, Hanie Sedghi, U.N. Niranjan, Animashree Anandkumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/janzamin2015.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_janzamin2015,http://jmlr.csail.mit.edu/proceedings/papers/v44/janzamin2015.html,"Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we build upon a novel framework called FEAST(Feature ExtrAction using Score function Tensors) which incorporates generative models for discriminative learning. FEAST considers a novel class of matrix and tensor-valued feature transform, which can be pre-trained using unlabeled samples. It uses an efficient algorithm for extracting discriminative information, given these pre-trained features and labeled samples for any related task. The class of features it adopts are based on higher-order score functions, which capture local variations in the probability density function of the input. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of overcomplete representations (where number of discriminative features is greater than input dimension). In this paper, we provide preliminary experiment results on real datasets."
2063,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,A New Perspective for Information Theoretic Feature Selection,Gavin Brown,"5:49-56, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/brown09a/brown09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_brown09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/brown09a.html,"Feature Filters are among the simplest and fastest approaches to feature selection. A ""filter"" defines a statistical criterion used to rank features on how useful they are expected to be for classification. The highest ranking features are retained and the lowest ranking can be discarded. A common approach is to use the Mutual Information between the features and class label. This area has seen a recent flurry of activity resulting in a confusing variety of heuristic criteria all based on mutual information and a lack of a principled way to understand or relate them. The contribution of this paper is a unifying theoretical understanding of such filters. In contrast to current methods which manually construct fi lter criteria with particular properties we show how to naturally derive a space of possible ranking criteria. We will show that several recent contributions in the feature selection literature are points within this space and that there exist many points that have never been explored."
2064,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On Compact Codes for Spatially Pooled Features,"Yangqing Jia, Oriol Vinyals, Trevor Darrell",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/jia13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/jia13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_jia13,http://jmlr.csail.mit.edu/proceedings/papers/v28/jia13.html,"Feature encoding with an overcomplete dictionary has demonstrated good performance in many applications, especially computer vision. In this paper we analyze the classification accuracy with respect to dictionary size by linking the encoding stage to kernel methods and sampling, and obtain useful bounds on accuracy as a function of size. The method also inspires us to revisit dictionary learning from local patches, and we propose to learn the dictionary in an end-to-end fashion taking into account pooling, a common computational layer in vision. We validate our contribution by showing how the derived bounds are able to explain the observed behavior of multiple datasets, and show that the pooling aware method efficiently reduces the dictionary size by a factor of two for a given accuracy."
2065,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fastfood - Computing Hilbert Space Expansions in loglinear time,"Quoc Le, Tamas Sarlos, Alexander Smola",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/le13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/le13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_le13,http://jmlr.csail.mit.edu/proceedings/papers/v28/le13.html,"Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint."
2066,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Learning Hash Functions Using Column Generation,"Xi Li, Guosheng Lin, Chunhua Shen, Anton Van den Hengel, Anthony Dick",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_li13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/li13a.html,"Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets."
2067,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Quasi Newton Temporal Difference Learning,"Arash Givchi, Maziar Palhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/givchi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_givchi14,http://jmlr.csail.mit.edu/proceedings/papers/v39/givchi14.html,"Fast convergent and computationally inexpensive policy evaluation is an essential part of reinforcement learning algorithms based on policy iteration. Algorithms such as LSTD, LSPE, FPKF and NTD, have faster convergence rate but they are computationally slow. On the other hand, there are algorithms that are computationally fast but with slower convergence rate, among them are TD, RG, GTD2 and TDC. This paper presents a regularized Quasi Newton Temporal Difference learning algorithm which uses the second-order information while maintaining a fast convergence rate. In simple language, we combine the idea of TD learning with Quasi Newton algorithm SGD-QN. We explore the development of QNTD algorithm and discuss its convergence properties. We support our ideas with empirical results on 4 standard benchmarks in reinforcement learning literature with 2 small problems, Random Walk and Boyan chain and 2 bigger problems, cart-pole and linked-pole balancing. Empirical studies show that QNTD speeds up convergence and provides better accuracy in comparison to the conventional TD."
2068,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Falling Rule Lists,"Fulton Wang, Cynthia Rudin",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15a-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wang15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15a.html,"Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods."
2069,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood,"Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hayashi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hayashi15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hayashi15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hayashi15.html,"Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover previously-unknown their relationship. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results."
2070,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Tensor Analyzers,"Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/tang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tang13.html,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings."
2071,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,"Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning","Mario Lucic, Mesrob Ohannessian, Amin Karbasi, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lucic15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lucic15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lucic15,http://jmlr.csail.mit.edu/proceedings/papers/v38/lucic15.html,"Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry. We also develop an algorithm, TRAM, to navigate the space/time/data/risk tradeoff in practice. In particular, we show that for a fixed risk (or data size), as the data size increases (resp. risk increases) the running time of TRAM decreases. Our extensive experiments on real data sets demonstrate the existence and practical utility of such tradeoffs, not only for k-means but also for Gaussian Mixture Models."
2072,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Coinciding Walk Kernels: Parallel Absorbing Random Walks for Learning with Graphs and Few Labels,"Marion Neumann, Roman Garnett, Kristian Kersting","29 :357-372, 2013",http://jmlr.org/proceedings/papers/v29/Neumann13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Neumann13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Neumann13.html,"Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is difficult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity _ the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label _ for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several real- world networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base."
2073,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Directed Exploration in Reinforcement Learning with Transferred Knowledge,"Timothy A. Mann, Yoonsuck Choe","24:59-76, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/mann12a/mann12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_mann12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/mann12a.html,Experimental results suggest that transfer learning (TL) compared to learning from scratch can decrease exploration by reinforcement learning (RL) algorithms. Most existing TL algorithms for RL are heuristic and may result in worse performance than learning from scratch (i.e. negative transfer). We introduce a theoretically grounded and flexible approach that transfers action-values via an intertask mapping and based on those explores the target task systematically. We characterize positive transfer as (1) decreasing sample complexity in the target task compared to the sample complexity of the base RL algorithm (without transferred action-values) and (2) guaranteeing that the algorithm converges to a near-optimal policy (i.e. negligible optimality loss). The sample complexity of our approach is no worse than the base algorithmÍs and our analysis reveals that positive transfer can occur even with highly inaccurate and partial intertask mappings. Finally we empirically test directed exploration with transfer in a multijoint reaching task which highlights the value of our analysis and the robustness of our approach under imperfect conditions.
2074,24,http://jmlr.csail.mit.edu/proceedings/papers/v24/,Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments,"Yevgeny Seldin, Csaba Szepesvˆri, Peter Auer, Yasin Abbasi-Yadkori","24:103-116, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v24/seldin12a/seldin12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v24/,,12th January 2013,"June 30-July 1, 2012",EWRL 2012 Proceedings,Proceedings of the Tenth European Workshop on Reinforcement Learning,"Edinburgh, Scotland","Marc Peter Deisenroth, Csaba SzepesvÕçri, Jan Peters",v24_seldin12a,http://jmlr.csail.mit.edu/proceedings/papers/v24/seldin12a.html,"EXP3 is a popular algorithm for adversarial multiarmed bandits suggested and analyzed in this setting by Auer et al. [2002b]. Recently there was an increased interest in the performance of this algorithm in the stochastic setting due to its new applications to stochastic multiarmed bandits with side information [Seldin et al. 2011] and to multiarmed bandits in the mixed stochastic-adversarial setting [Bubeck and Slivkins 2012]. We present an empirical evaluation and improved analysis of the performance of the EXP3 algorithm in the stochastic setting as well as a modification of the EXP3 algorithm capable of achieving ""logarithmic"" regret in stochastic environments."
2075,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Gibbs Max-Margin Topic Models with Fast Sampling Algorithms,"Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhu13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhu13.html,"Existing max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents Gibbs max-margin supervised topic models by minimizing an expected margin loss, an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables, we develop simple and fast Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems for both classification and regression. Empirical results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors."
2076,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Max-Margin Matrix Factorization with Data Augmentation,"Minjie Xu, Jun Zhu, Bo Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_xu13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13a.html,Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.
2077,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On A Nonlinear Generalization of Sparse Coding and Dictionary Learning,"Jeffrey Ho, Yuchen Xie, Baba Vemuri",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ho13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ho13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/ho13a.html,"Existing dictionary learning algorithms are based on the assumption that the data are vectors in an Euclidean vector space, and the dictionary is learned from the training data using the vector space structure and its Euclidean metric. However, in many applications, features and data often originated from a Riemannian manifold that does not support a global linear (vector space) structure. Furthermore, the extrinsic viewpoint of existing dictionary learning algorithms becomes inappropriate for modeling and incorporating the intrinsic geometry of the manifold that is potentially important and critical to the application. This paper proposes a novel framework for sparse coding and dictionary learning for data on a Riemannian manifold, and it shows that the existing sparse coding and dictionary learning methods can be considered as special (Euclidean) cases of the more general framework proposed here. We show that both the dictionary and sparse coding can be effectively computed for several important classes of Riemannian manifolds, and we validate the proposed method using two well-known classification problems in computer vision and medical imaging analysis."
2078,17,http://jmlr.csail.mit.edu/proceedings/papers/v17/,Employing The Complete Face in AVSR to Recover from Facial Occlusions,"Benjamin X. Hall, John Shawe-Taylor and Alan Johnston","17:33-40, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v17/hall11a/hall11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v17/,,21st October 2011,"19-21 October, 2011",Workshop on Applications of Pattern Analysis WAPA 2011 Proceedings,Proceedings of the Second Workshop on Applications of Pattern Analysis,"Castro Urdiales, Spain","Tom Diethe, Jose Balcazar, John Shawe-Taylor, and Cristina Tirnauca",v17_hall11a,http://jmlr.csail.mit.edu/proceedings/papers/v17/hall11a.html,"Existing Audio-Visual Speech Recognition (AVSR) systems visually focus intensely on a small region of the face, centred on the immediate mouth area. This is poor design for a variety reasons in real world situations because any occlusion to this small area renders all visual advantage null and void. This is poorby design because it is well known that humans use the complete face to speechread. We demonstrate a new application of a novel visual algorithm, the Multi-Channel Gradient Model, the deploys information from the complete face to perform AVSR. Our MCGM model performs near to the performance of Discrete Cosine Transforms in the case where a small region of interest around the lips, but in the case of an occluded face we can achieve results that match nearly 70% of the performance that DCTs can achieve on the DCT best case, lips centeric approach."
2079,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Factorized Orthogonal Latent Spaces,"Mathieu Salzmann, Carl Henrik Ek, Raquel Urtasun, Trevor Darrell","9:701-708, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/salzmann10a/salzmann10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_salzmann10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/salzmann10a.html,Existing approaches to multi-view learning are particularly effective when the views are either independent (i.e multi-kernel approaches) or fully dependent (i.e. shared latent spaces). However in real scenarios these assumptions are almost never truly satisfied. Recently two methods have attempted to tackle this problem by factorizing the information and learn separate latent spaces for modeling the shared (i.e. correlated) and private (i.e. independent) parts of the data. However these approaches are very sensitive to parameters setting or initialization. In this paper we propose a robust approach to factorizing the latent space into shared and private spaces by introducing orthogonality constraints which penalize redundant latent representations. Furthermore unlike previous approaches we simultaneously learn the structure and dimensionality of the latent spaces by relying on a regularizer that encourages the latent space of each data stream to be low dimensional. To demonstrate the benefits of our approach we apply it to two existing shared latent space models that assume full dependence of the views the sGPLVM and the sKIE and show that our constraints improve the performance of these models on the task of pose estimation from monocular images.
2080,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Consistent Histogram Estimator for Exchangeable Graph Models,"Stanley Chan, Edoardo Airoldi",none,http://jmlr.org/proceedings/papers/v32/chan14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chan14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chan14.html,"Exchangeable graph models (ExGM) subsume a number of popular network models. The mathematical object that characterizes an ExGM is termed a graphon. Finding scalable estimators of graphons, provably consistent, remains an open issue. In this paper, we propose a histogram estimator of a graphon that is provably consistent and numerically efficient. The proposed estimator is based on a sorting-and-smoothing (SAS) algorithm, which first sorts the empirical degree of a graph, then smooths the sorted graph using total variation minimization. The consistency of the SAS algorithm is proved by leveraging sparsity concepts from compressed sensing."
2081,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Nonparametric estimation and testing of exchangeable graph models,"Justin Yang, Christina Han, Edoardo Airoldi","33 :1060-1067, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_yang14c,http://jmlr.csail.mit.edu/proceedings/papers/v33/yang14c.html,"Exchangeable graph models (ExGM) are a nonparametric approach to modeling network data that subsumes a number of popular models. The key object that defines an ExGM is often referred to as a graphon, or graph kernel. Here, we make three contributions to advance the theory of estimation of graphons. We determine conditions under which a unique canonical representation for a graphon exists and it is identifiable. We propose a 3-step procedure to estimate the canonical graphon of any ExGM that satisfies these conditions. We then focus on a specific estimator, built using the proposed 3-step procedure, which combines probability matrix estimation by Universal Singular Value Thresholding (USVT) and empirical degree sorting of the observed adjacency matrix. We prove that this estimator is consistent. We illustrate how the proposed theory and methods can be used to develop hypothesis testing procedures for models of network data."
2082,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Scaling Multidimensional Gaussian Processes using Projected Additive Approximations,"Elad Gilboa, Yunus Saatçi, John Cunningham, Elad Gilboa",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gilboa13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/gilboa13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gilboa13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gilboa13.html,"Exact Gaussian Process (GP) regression has \(O(N^3)\) runtime for data size N, making it intractable for large N. Advances in GP scaling have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests a novel method of projected additive approximation to multidimensional GPs. We thoroughly illustrate the power of this method on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost."
2083,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Anytime Representation Learning,"Zhixiang Xu, Matt Kusner, Gao Huang, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_xu13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/xu13b.html,"Evaluation cost during test-time is becoming increasingly important as many real-world applications need fast evaluation (e.g. web search engines, email spam filtering) or use expensive features (e.g. medical diagnosis). We introduce Anytime Feature Representations (AFR), a novel algorithm that explicitly addresses this trade-off in the data representation rather than in the classifier. This enables us to turn conventional classifiers, in particular Support Vector Machines, into test-time cost sensitive anytime classifiers combining the advantages of anytime learning and large-margin classification."
2084,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Direct Density-Derivative Estimation and Its Application in KL-Divergence Approximation,"Hiroaki Sasaki, Yung-Kyun Noh, Masashi Sugiyama",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/sasaki15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_sasaki15,http://jmlr.csail.mit.edu/proceedings/papers/v38/sasaki15.html,"Estimation of density derivatives is a versatile tool in statistical data analysis. A naive approach is to first estimate the density and then compute its derivative. However, such a two-step approach does not work well because a good density estimator does not necessarily mean a good density-derivative estimator. In this paper, we give a direct method to approximate the density derivative without estimating the density itself. Our proposed estimator allows analytic and computationally efficient approximation of multi-dimensional high-order density derivatives, with the ability that all hyper-parameters can be chosen objectively by cross-validation. We further show that the proposed density-derivative estimator is useful in improving the accuracy of non-parametric KL-divergence estimation via metric learning. The practical superiority of the proposed method is experimentally demonstrated in change detection and feature selection."
2085,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Conditional Density Estimation via Least-Squares Density Ratio Estimation,"Masashi Sugiyama, Ichiro Takeuchi, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Daisuke Okanohara","9:781-788, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sugiyama10a/sugiyama10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sugiyama10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sugiyama10a.html,Estimating the conditional mean of an input-output relation is the goal of regression. However regression analysis is not sufficiently informative if the conditional distribution has multi-modality is highly asymmetric or contains heteroscedastic noise. In such scenarios estimating the conditional distribution itself would be more useful. In this paper we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach.
2086,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Generalized Exponential Concentration Inequality for Renyi Divergence Estimation,"Shashank Singh, Barnabas Poczos",none,http://jmlr.org/proceedings/papers/v32/singh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_singh14,http://jmlr.csail.mit.edu/proceedings/papers/v32/singh14.html,"Estimating divergences between probability distributions in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of Renyi divergence for a smooth Holder class of densities on the d-dimensional unit cube. We also illustrate our theoretical results with a numerical experiment."
2087,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees,"Mathieu Serrurier, Henri Prade",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/serrurier15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_serrurier15,http://jmlr.csail.mit.edu/proceedings/papers/v37/serrurier15.html,"Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm."
2088,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Similarity-based Contrastive Divergence Methods for Energy-based Deep Learning Models,"Adepu Ravi Sankar, Vineeth N Balasubramanian",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Sankar15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Sankar15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Sankar15.html,"Energy-based deep learning models like Restricted Boltzmann Machines are increasingly used for real-world applications. However, all these models inherently depend on the Contrastive Divergence (CD) method for training and maximization of log likelihood of generating the given data distribution. CD, which internally uses Gibbs sampling, often does not perform well due to issues such as biased samples, poor mixing of Markov chains and high-mass probability modes. Variants of CD such as PCD, Fast PCD and Tempered MCMC have been proposed to address this issue. In this work, we propose a new approach to CD-based methods, called Diss-CD, which uses dissimilar data to allow the Markov chain to explore new modes in the probability space. This method can be used with all variants of CD (or PCD), and across all energy-based deep learning models. Our experiments on using this approach on standard datasets including MNIST, Caltech-101 Silhouette and Synthetic Transformations, demonstrate the promise of this approach, showing fast convergence of error in learning and also a better approximation of log likelihood of the data."
2089,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Generic Methods for Optimization-Based Modeling,Justin Domke,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/domke12/domke12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_domke12,http://jmlr.csail.mit.edu/proceedings/papers/v22/domke12.html,"Energy models for continuous domains can be applied to many problems but often suffer from high computational expense in training due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive ""backpropagating"" versions of gradient descent heavy-ball and LBFGS. These are simple to use as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to ""full"" fitting."
2090,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Automated inference of point of view from user interactions in collective intelligence venues,"Sanmay Das, Allen Lavoie",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/das14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_das14,http://jmlr.csail.mit.edu/proceedings/papers/v32/das14.html,"Empirical evaluation of trust and manipulation in large-scale collective intelligence processes is challenging. The datasets involved are too large for thorough manual study, and current automated options are limited. We introduce a statistical framework which classifies point of view based on user interactions. The framework works on Web-scale datasets and is applicable to a wide variety of collective intelligence processes. It enables principled study of such issues as manipulation, trustworthiness of information, and potential bias. We demonstrate the modelês effectiveness in determining point of view on both synthetic data and a dataset of Wikipedia user interactions. We build a combined model of topics and points-of-view on the entire history of English Wikipedia, and show how it can be used to find potentially biased articles and visualize user interactions at a high level."
2091,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Imitation Learning Applied to Embodied Conversational Agents,"Piot Bilal, Olivier Pietquin, Matthieu Geist",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/piot15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_piot15,http://jmlr.csail.mit.edu/proceedings/papers/v43/piot15.html,"Embodied Conversational Agents (ECAs) are emerging as a key component to allow human interact with machines. Applications are numerous and ECAs can reduce the aversion to interact with a machine by providing user-friendly interfaces. Yet, ECAs are still unable to produce social signals appropriately during their interaction with humans, which tends to make the interaction less instinctive. Especially, very little attention has been paid to the use of laughter in human-avatar interactions despite the crucial role played by laughter in human-human interaction. In this paper, methods for predicting when and how to laugh during an interaction for an ECA are proposed. Different Imitation Learning (also known as Apprenticeship Learning) algorithms are used in this purpose and a regularized classification algorithm is shown to produce good behavior on real data."
2092,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,How Powerful Can Any Regression Learning Procedure Be?,Yuhong Yang,"2:636-643, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/yang07a/yang07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_yang07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/yang07a.html,Efforts have been directed at obtaining flexible learning procedures that optimally adapt to various possible characteristics of the data generating mechanism. A question that addresses the issue of how far one can go in this direction is: Given a regression procedure however sophisticated it is how many regression functions are estimated accurately? In this work for a given sequence of prescribed estimation accuracy (in sample size) we give an upper bound (in terms of metric entropy) on the number of regression functions for which the accuracy is achieved. Interesting consequences on adaptive and sparse estimations are also given.
2093,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Modelling Sparse Dynamical Systems with Compressed Predictive State Representations,"William L. Hamilton, Mahdi Milani Fard, Joelle Pineau",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/hamilton13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_hamilton13,http://jmlr.csail.mit.edu/proceedings/papers/v28/hamilton13.html,"Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model."
2094,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Planning by Prioritized Sweeping with Small Backups,"Harm Van Seijen, Rich Sutton",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/vanseijen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_vanseijen13,http://jmlr.csail.mit.edu/proceedings/papers/v28/vanseijen13.html,"Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations."
2095,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Generalization Bounds for Online Learning Algorithms with Pairwise Loss Functions,"Yuyang Wang, Roni Khardon, Dmitry Pechyony and Rosie Jones",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/wang12/wang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_wang12,http://jmlr.csail.mit.edu/proceedings/papers/v23/wang12.html,"Efficient online learning with pairwise loss functions is a crucial component in building largescale learning system that maximizes the area under the Receiver Operator Characteristic (ROC) curve. In this paper we investigate the generalization performance of online learning algorithms with pairwise loss functions. We show that the existing proof techniques for generalization bounds of online algorithms with a pointwise loss can not be directly applied to pairwise losses. Using the Hoeffding-Azuma inequality and various proof techniques for the risk bounds in the batch learning, we derive data-dependent bounds for the average risk of the sequence of hypotheses generated by an arbitrary online learner in terms of an easily computable statistic, and show how to extract a low risk hypothesis from the sequence. In addition, we analyze a natural extension of the perceptron algorithm for the bipartite ranking problem providing a bound on the empirical pairwise loss. Combining these results we get a complete risk analysis of the proposed algorithm."
2096,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,A New Algorithm for Compressed Counting with Applications in Shannon Entropy Estimation in Dynamic Data,"Ping Li, Cun-Hui Zhang","19:477-496, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/li11a/li11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_li11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/li11a.html,Efficient estimation of the moments and Shannon entropy of data streams is an important task in modern machine learning and data mining. To estimate the Shannon entropy it suffices to accurately estimate the $\alpha$-th moment with $\Delta = |1-\alpha|\approx0$. To guarantee that the error of estimated Shannon entropy is within a $\nu$-additive factor the method of {\em symmetric stable random projections} requires $O\left(\frac{1}{\nu^2\Delta^2}\right)$ samples which is extremely expensive. The first paper~\citep{Proc:Li_SODA09} in {\em Compressed Counting (CC)} based on {\em skewed-stable random projections} supplies a substantial improvement by reducing the sample complexity to $O\left(\frac{1}{\nu^2\Delta}\right)$ which is still expensive. The followup work~\citep{Proc:Li_UAI09} provides a practical algorithm which is however difficult to analyze theoretically.In this paper we propose a new accurate algorithm for Compressed Counting whose sample complexity is only $O\left(\frac{1}{\nu^2}\right)$ for $\nu$-additive Shannon entropy estimation. The constant factor for this bound is merely about 6. In addition we prove that our algorithm achieves an upper bound of the Fisher information and in fact it is close to $100\%$ statistically optimal. An empirical study is conducted to verify the accuracy of our algorithm.
2097,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Active Detection via Adaptive Submodularity,"Yuxin Chen, Hiroaki Shioi, Cesar Fuentes Montesinos, Lian Pin Koh, Serge Wich, Andreas Krause",none,http://jmlr.org/proceedings/papers/v32/chena14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/chena14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chena14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chena14.html,"Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision. In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives."
2098,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Reconstructing ecological networks with hierarchical Bayesian regression and Mondrian processes,"Andrej Aderhold, Dirk Husmeier, V. Anne Smith",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/aderhold13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_aderhold13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/aderhold13a.html,"Ecological systems consist of complex sets of interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change. However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space. Machine learning can enable the recovery of such complex, spatially varying interactions from relatively easily obtained species abundance data. Here, we describe a novel Bayesian regression and Mondrian process model (BRAMP) for reconstructing species interaction networks from observed field data. BRAMP enables robust inference of species interactions considering autocorrelation in species abundances and allowing for variation in the interactions across space. We evaluate the model on spatially explicit simulated data, produced using a trophic niche model combined with stochastic population dynamics. We compare the modelês performance against L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. Finally, we apply BRAMP to real ecological data."
2099,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Statistical Unfolded Logic Learning,"Wang-Zhou Dai, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Dai15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Dai15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Dai15.html,"During the past decade, Statistical Relational Learning (SRL) and Probabilistic Inductive Logic Programming (PILP), owing to their strength in capturing structure information, have attracted much attention for learning relational models such as weighted logic rules. Typically, a generative model is assumed for the structured joint distribution, and the learning process is accomplished in an enormous relational space. In this paper, we propose a new framework, i.e., Statistical Unfolded Logic (SUL) learning. In contrast to learning rules in the relational space directly, SUL propositionalizes the structure information into an attribute-value data set, and thus, statistical discriminative learning which is much more efficient than generative relational learning can be executed. In addition to achieving better generalization performance, SUL is able to conduct predicate invention that is hard to be realized by traditional SRL and PILP approaches. Experiments on real tasks show that our proposed approach is superior to state-of-the-art weighted rules learning approaches."
2100,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,"The lasso, persistence, and cross-validation","Darren Homrighausen, Daniel McDonald",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/homrighausen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_homrighausen13,http://jmlr.csail.mit.edu/proceedings/papers/v28/homrighausen13.html,"During the last fifteen years, the lasso procedure has been the target of a substantial amount of theoretical and applied research. Correspondingly, many results are known about its behavior for a fixed or optimally chosen smoothing parameter (given up to unknown constants). Much less, however, is known about the lassoês behavior when the smoothing parameter is chosen in a data dependent way. To this end, we give the first result about the risk consistency of lasso when the smoothing parameter is chosen via cross-validation. We consider the high-dimensional setting wherein the number of predictors \(p=n^\alpha,\ \alpha_0\) grows with the number of observations."
2101,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Improving Sybil Detection via Graph Pruning and Regularization Techniques,"Huanhuan Zhang, Jie Zhang, Carol Fung, Chang Xu",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhang15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhang15b,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhang15b.html,"Due to their open and anonymous nature, online social networks are particularly vulnerable to Sybil attacks. In recent years, there has been a rising interest in leveraging social network topological structures to combat Sybil attacks. Unfortunately, due to their strong dependency on unrealistic assumptions, existing graph-based Sybil defense mechanisms suffer from high false detection rates. In this paper, we focus on enhancing those mechanisms by considering additional graph structural information underlying social networks. Our solutions are based on our novel understanding and interpretation of Sybil detection as the problem of partially labeled classification. Specifically, we first propose an effective graph pruning technique to enhance the robustness of existing Sybil defense mechanisms against target attacks, by utilizing the local structural similarity between neighboring nodes in a social network. Second, we design a domain-specific graph regularization method to further improve the performance of those mechanisms by exploiting the relational property of the social network. Experimental results on four popular online social network datasets demonstrate that our proposed techniques can significantly improve the detection accuracy over the original Sybil defense mechanisms."
2102,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Max-Margin Zero-Shot Learning for Multi-class Classification,"Xin Li, Yuhong Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_li15d,http://jmlr.csail.mit.edu/proceedings/papers/v38/li15d.html,"Due to the dramatic expanse of data categories and the lack of labeled instances, zero-shot learning, which transfers knowledge from observed classes to recognize unseen classes, has started drawing a lot of attention from the research community. In this paper, we propose a semi-supervised max-margin learning framework that integrates the semi-supervised classification problem over observed classes and the unsupervised clustering problem over unseen classes together to tackle zero-shot multi-class classification. By further integrating label embedding into this framework, we produce a dual formulation that permits convenient incorporation of auxiliary label semantic knowledge to improve zero-shot learning. We conduct extensive experiments on three standard image data sets to evaluate the proposed approach by comparing to two state-of-the-art methods. Our results demonstrate the efficacy of the proposed framework."
2103,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Tree-Independent Dual-Tree Algorithms,"Ryan Curtin, William March, Parikshit Ram, David Anderson, Alexander Gray, Charles Isbell",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/curtin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_curtin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/curtin13.html,"Dual-tree algorithms are a widely used class of branch-and-bound algorithms. Unfortunately, developing dual-tree algorithms for use with different trees and problems is often complex and burdensome. We introduce a four-part logical split: the tree, the traversal, the point-to-point base case, and the pruning rule. We provide a meta-algorithm which allows development of dual-tree algorithms in a tree-independent manner and easy extension to entirely new types of trees. Representations are provided for five common algorithms; for k-nearest neighbor search, this leads to a novel, tighter pruning bound. The meta-algorithm also allows straightforward extensions to massively parallel settings."
2104,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Domain Adaptation with Coupled Subspaces,"John Blitzer, Sham Kakade, Dean Foster","15:173-181, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/blitzer11a/blitzer11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_blitzer11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/blitzer11a.html,Domain adaptation algorithms address a key issue in applied machine learning: How can we train a system under a source distribution but achieve high performance under a different target distribution? We tackle this question for divergent distributions where crucial predictive target features may not even have support under the source distribution. In this setting the key intuition is that that if we can link target-specific features to source features we can learn effectively using only source labeled data. We formalize this intuition as well as the assumptions under which such coupled learning is possible. This allows us to give finite sample target error bounds (using only source training data) and an algorithm which performs at the state-of-the-art on two natural language processing adaptation tasks which are characterized by novel target features.
2105,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Directional Statistics on Permutations,"Sergey Plis, Stephen McCracken, Terran Lane, Vince Calhoun","15:600-608, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/plis11a/plis11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_plis11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/plis11a.html,Distributions over permutations arise in applications ranging from multi-object tracking to ranking. The difficulty of dealing with these distributions is caused by the size of their domain which is factorial in the number of entities (n!). The direct definition of a multinomial distribution over permutation space is impractical for all but a very small n. In this work we propose an embedding of all n! permutations for a given n in a surface of a hypersphere defined in R^((n-1)^2). As a result we acquire the ability to define continuous distributions over a hypersphere with all the benefits of directional statistics. We provide polynomial time projections between the continuous hypersphere representation and the n!-element permutation space. The framework provides a way to use continuous directional probability densities and the methods developed thereof for establishing densities over permutations. As a demonstration of the benefits of the framework we derive an inference procedure for a state-space model over permutations. We demonstrate the approach with applications and comparisons to existing models.
2106,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Distribution-Free Distribution Regression,"Barnabas Poczos, Aarti Singh, Alessandro Rinaldo, Larry Wasserman",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/poczos13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/poczos13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_poczos13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/poczos13a.html,"Distribution regression refers to the situation where a response \(Y\) depends on a covariate \(P\) where \(P\) is a probability distribution. The model is \(Y=f(P) + e\) where \(f\) is an unknown regression function and \(e\) is a random error. Typically, we do not observe \(P\) directly, but rather, we observe a sample from \(P\) . In this paper we develop theory and methods for distribution-free versions of distribution regression. This means that we do not make strong distributional assumptions about the error term \(e\) and covariate \(P\) . We prove that when the effective dimension is small enough (as measured by the doubling dimension), then the excess prediction risk converges to zero with a polynomial rate."
2107,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Character-level Representations for Part-of-Speech Tagging,"Cicero Dos Santos, Bianca Zadrozny",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/santos14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_santos14,http://jmlr.csail.mit.edu/proceedings/papers/v32/santos14.html,"Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result."
2108,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,A Unified Framework for Jointly Learning Distributed Representations of Word and Attributes,"Liqiang Niu, Xin-Yu Dai, Shujian Huang, Jiajun Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Niu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Niu15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Niu15.html,"Distributed word representations have achieved great success in natural language processing (NLP) area. However, most distributed models focus on local context properties and learn task-specific representations individually, therefore lack the ability to fuse multi-attributes and learn jointly. In this paper, we propose a unified framework which jointly learns distributed representations of word and attributes: characteristics of word. In our models, we consider three types of attributes: topic, lemma and document. Besides learning distributed attribute representations, we find that using additional attributes is beneficial to improve word representations. Several experiments are conducted to evaluate the performance of the learned topic representations, document representations, and improved word representations, respectively. The experimental results show that our models achieve significant and competitive results."
2109,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Adding vs. Averaging in Distributed Primal-Dual Optimization,"Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, Martin Takac",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/mab15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/mab15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_mab15,http://jmlr.csail.mit.edu/proceedings/papers/v37/mab15.html,"Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines."
2110,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Asynchronous Distributed ADMM for Consensus Optimization,"Ruiliang Zhang, James Kwok",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhange14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_zhange14,http://jmlr.csail.mit.edu/proceedings/papers/v32/zhange14.html,"Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time."
2111,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Spectral Experts for Estimating Mixtures of Linear Regressions,"Arun Tejasvi Chaganty, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/tejasvichaganty13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/tejasvichaganty13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_tejasvichaganty13,http://jmlr.csail.mit.edu/proceedings/papers/v28/tejasvichaganty13.html,"Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for the mixture of linear regressions, a simple instance of discriminative latent-variable models. Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using the tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM)."
2112,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation,"Sebastian Brechtel, Tobias Gindele, Rdiger Dillmann",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/brechtel13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_brechtel13,http://jmlr.csail.mit.edu/proceedings/papers/v28/brechtel13.html,"Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems."
2113,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Unfolding Latent Tree Structures using 4th Order Tensors,"Mariya Ishteva, Haesun Park, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ishteva13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/ishteva13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ishteva13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ishteva13.html,"Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better."
2114,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,"A Parallel, Block Greedy Method for Sparse Inverse Covariance Estimation for Ultra-high Dimensions","Prabhanjan Kambadur, Aurelie Lozano",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/kambadur13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/kambadur13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_kambadur13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/kambadur13a.html,"Discovering the graph structure of a Gaussian Markov Random Field is an important problem in application areas such as computational biology and atmospheric sciences. This task, which translates to estimating the sparsity pattern of the inverse covariance matrix, has been extensively studied in the literature. However, the existing approaches are unable to handle ultra-high dimensional datasets and there is a crucial need to develop methods that are both highly scalable and memory-efficient. In this paper, we present GINCO, a blocked greedy method for sparse inverse covariance matrix estimation. We also present detailed description of a highly-scalable and memory-efficient implementation of GINCO, which is able to operate on both shared- and distributed-memory architectures. Our implementation is able recover the sparsity pattern of 25,000 vertex random and chain graphs with 87% and 84% accuracy in \(\le\) 5 minutes using \(\le\) 10GB of memory on a single 8-core machine. Furthermore, our method is statistically consistent in recovering the sparsity pattern of the inverse covariance matrix, which we demonstrate through extensive empirical studies."
2115,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fast search for Dirichlet process mixture models,Hal Daume III,"2:83-90, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/daume07a/daume07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_daume07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/daume07a.html,Dirichlet process (DP) mixture models provide a flexible Bayesian framework for density estimation. Unfortunately their flexibility comes at a cost: inference in DP mixture models is computationally expensive even when conjugate distributions are used. In the common case when one seeks only a maximum a posteriori assignment of data points to clusters we show that search algorithms provide a practical alternative to expensive MCMC and variational techniques. When a true posterior sample is desired the solution found by search can serve as a good initializer for MCMC. Experimental results show that using these techniques is it possible to apply DP mixture models to very large data sets.
2116,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Dirichlet Process Mixture Model for Spherical Data,"Julian Straub, Jason Chang, Oren Freifeld, John Fisher III",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/straub15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/straub15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_straub15,http://jmlr.csail.mit.edu/proceedings/papers/v38/straub15.html,"Directional data, naturally represented as points on the unit sphere, appear in many applications. However, unlike the case of Euclidean data, flexible mixture models on the sphere that can capture correlations, handle an unknown number of components and extend readily to high-dimensional data have yet to be suggested. For this purpose we propose a Dirichlet process mixture model of Gaussian distributions in distinct tangent spaces (DP-TGMM) to the sphere. Importantly, the formulation of the proposed model allows the extension of recent advances in efficient inference for Bayesian nonparametric models to the spherical domain. Experiments on synthetic data as well as real-world 3D surface normal and 20-dimensional semantic word vector data confirm the expressiveness and applicability of the DP-TGMM."
2117,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Mixed Cumulative Distribution Networks,"Ricardo Silva, Charles Blundell, Yee Whye Teh","15:670-678, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/silva11a/silva11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_silva11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/silva11a.html,Directed acyclic graphs (DAGs) are a popular framework to express multivariate probability distributions. Acyclic directed mixed graphs (ADMGs) are generalizations of DAGs that can succinctly capture much richer sets of conditional independencies and are especially useful in modeling the effects of latent variables implicitly. Unfortunately there are currently no parameterizations of general ADMGs. In this paper we apply recent work on cumulative distribution networks and copulas to propose one general construction for ADMG models. We consider a simple parameter estimation approach and report some encouraging experimental results.
2118,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Complex Neural Network Policies with Trajectory Optimization,"Sergey Levine, Vladlen Koltun",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/levine14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/levine14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_levine14,http://jmlr.csail.mit.edu/proceedings/papers/v32/levine14.html,"Direct policy search methods offer the promise of automatically learning controllers for complex, high-dimensional tasks. However, prior applications of policy search often required specialized, low-dimensional policy classes, limiting their generality. In this work, we introduce a policy search algorithm that can directly learn high-dimensional, general-purpose policies, represented by neural networks. We formulate the policy search problem as an optimization over trajectory distributions, alternating between optimizing the policy to match the trajectories, and optimizing the trajectories to match the policy and minimize expected cost. Our method can learn policies for complex tasks such as bipedal push recovery and walking on uneven terrain, while outperforming prior methods."
2119,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Guided Policy Search,"Sergey Levine, Vladlen Koltun",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/levine13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/levine13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_levine13,http://jmlr.csail.mit.edu/proceedings/papers/v28/levine13.html,"Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running."
2120,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Optimal Mean Robust Principal Component Analysis,"Feiping Nie, Jianjun Yuan, Heng Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/nieb14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_nieb14,http://jmlr.csail.mit.edu/proceedings/papers/v32/nieb14.html,"Dimensionality reduction techniques extract low-dimensional structure from high-dimensional data and are widespread in machine learning research. In practice, due to lacking labeled data, the unsupervised dimensionality reduction algorithms are more desired. Among them, Principal Component Analysis (PCA) is the most widely used approach. In recent research, several robust PCA algorithms were presented to enhance the robustness of PCA model. However, all existing robust PCA methods incorrectly center the data using the L2-norm distance to calculate the mean, which actually is not the optimal mean due to the L1-norm used in the objective functions. It is non-trivial to remove the optimal mean in the robust PCA, because of the sparsity-inducing norms used in the robust formulations. In this paper, we propose novel robust PCA objective functions with removing optimal mean automatically. We naturally integrate the mean calculation into the dimensionality reduction optimization, such that the optimal mean can be obtained to enhance the dimensionality reduction. Both theoretical analysis and empirical studies demonstrate our new methods can more effectively reduce data dimensionality than previous robust PCA methods."
2121,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Nonlinear Dimensionality Reduction of Data by Deep Distributed Random Samplings,Xiao-Lei Zhang,none,http://jmlr.csail.mit.edu/proceedings/papers/v39/zhang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_zhang14,http://jmlr.csail.mit.edu/proceedings/papers/v39/zhang14.html,"Dimensionality reduction is a fundamental problem of machine learning, and has been intensively studied, where classification and clustering are two special cases of dimensionality reduction that reduce high-dimensional data to discrete points. Here we describe a simple multilayer network for dimensionality reduction that each layer of the network is a group of mutually independent k-centers clusterings. We find that the network can be trained successfully layer-by-layer by simply assigning the centers of each clustering by randomly sampled data points from the input. Our results show that the described simple method outperformed 7 well-known dimensionality reduction methods on both very small-scale biomedical data and large-scale image and document data, with less training time than multilayer neural networks on large-scale data."
2122,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,On the Relationship Between Feature Selection and Classification Accuracy,"Andreas Janecek, Wilfried Gansterer, Michael Demel, Gerhard Ecker","4:90-105, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/janecek08a/janecek08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_janecek08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/janecek08a.html,Dimensionality reduction and feature subset selection are two techniques for reducing the attribute space of a feature set which is an important component of both supervised and unsupervised classification or regression problems. While in feature subset selection a subset of the original attributes is extracted dimensionality reduction in general produces linear combinations of the original attribute set.
2123,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Mixture of Mutually Exciting Processes for Viral Diffusion,"Shuang-Hong Yang, Hongyuan Zha",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13a.html,"Diffusion network inference and meme tracking have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web."
2124,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Factorized Diffusion Map Approximation,"Saeed Amizadeh, Hamed Valizadegan, Milos Hauskrecht",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/amizadeh12/amizadeh12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_amizadeh12,http://jmlr.csail.mit.edu/proceedings/papers/v22/amizadeh12.html,Diffusion maps are among the most powerful Machine Learning tools to analyze and work with complex high-dimensional datasets. Unfortunately the estimation of these maps from a finite sample is known to suffer from the curse of dimensionality. Motivated by other machine learning models for which the existence of structure in the underlying distribution of data can reduce the complexity of estimation we study and show how the factorization of the underlying distribution into independent subspaces can help us to estimate diffusion maps more accurately. Building upon this result we propose and develop an algorithm that can automatically factorize a high dimensional data space in order to minimize the error of estimation of its diffusion map even in the case when the underlying distribution is not decomposable. Experiments on both the synthetic and real-world datasets demonstrate improved estimation performance of our method over the regular diffusion-map framework.
2125,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Condensed Filter Tree for Cost-Sensitive Multi-Label Classification,"Chun-Liang Li, Hsuan-Tien Lin",none,http://jmlr.org/proceedings/papers/v32/lia14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/lia14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lia14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lia14.html,"Different real-world applications of multi-label classification often demand different evaluation criteria. We formalize this demand with a general setup, cost-sensitive multi-label classification (CSMLC), which takes the evaluation criteria into account during learning. Nevertheless, most existing algorithms can only focus on optimizing a few specific evaluation criteria, and cannot systematically deal with different ones. In this paper, we propose a novel algorithm, called condensed filter tree (CFT), for optimizing any criteria in CSMLC. CFT is derived from reducing CSMLC to the famous filter tree algorithm for cost-sensitive multi-class classification via constructing the label powerset. We successfully cope with the difficulty of having exponentially many extended-classes within the powerset for representation, training and prediction by carefully designing the tree structure and focusing on the key nodes. Experimental results across many real-world datasets validate that CFT is competitive with special purpose algorithms on special criteria and reaches better performance on general criteria."
2126,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Max-Margin Multiple-Instance Dictionary Learning,"Xinggang Wang, Baoyuan Wang, Xiang Bai, Wenyu Liu, Zhuowen Tu",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13d,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13d.html,"Dictionary learning has became an increasingly important task in machine learning, as it is fundamental to the representation problem. A number of emerging techniques specifically include a codebook learning step, in which a critical knowledge abstraction process is carried out. Existing approaches in dictionary (codebook) learning are either generative (unsupervised e.g. k-means) or discriminative (supervised e.g. extremely randomized forests). In this paper, we propose a multiple instance learning (MIL) strategy (along the line of weakly supervised learning) for dictionary learning. Each code is represented by a classifier, such as a linear SVM, which naturally performs metric fusion for multi-channel features. We design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in MIL. State-of-the-art results are observed in image classification benchmarks based on the learned codebooks, which observe both compactness and effectiveness."
2127,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Dynamic Scaled Sampling for Deterministic Constraints,"Lei Li, Bharath Ramsundar, Stuart Russell",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/li13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/li13a-supp.pdf,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_li13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/li13a.html,"Deterministic and near-deterministic relationships among subsets of random variables in multivariate systems are known to cause serious problems for Monte Carlo algorithms. We examine the case in which the relationship \(Z = f(X_1,...,X_k)\) holds, where each \(X_i\) has a continuous prior pdf and we wish to obtain samples from the conditional distribution \(P(X_1,...,X_k | Z= s)\) . When \(f\) is addition, the problem is NP-hard even when the \(X_i\) are independent. In more restricted cases ã for example, i.i.d. Boolean or categorical \(X_i\) ã efficient exact samplers have been obtained previously. For the general continuous case, we propose a dynamic scaling algorithm (DYSC), and prove that it has \(O(k)\) expected running time and finite variance. We discuss generalizations of DYSC to functions \(f\) described by binary operation trees. We evaluate the algorithm on several examples."
2128,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Fixed-point algorithms for learning determinantal point processes,"Zelda Mariet, Suvrit Sra",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/mariet15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_mariet15,http://jmlr.csail.mit.edu/proceedings/papers/v37/mariet15.html,"Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique."
2129,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning the Parameters of Determinantal Point Process Kernels,"Raja Hafiz Affandi, Emily Fox, Ryan Adams, Ben Taskar",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/affandi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/affandi14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_affandi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/affandi14.html,"Determinantal point processes (DPPs) are well-suited for modeling repulsion and have proven useful in applications where diversity is desired. While DPPs have many appealing properties, learning the parameters of a DPP is diffcult, as the likelihood is non-convex and is infeasible to compute in many scenarios. Here we propose Bayesian methods for learning the DPP kernel parameters. These methods are applicable in large-scale discrete and continuous DPP settings, even when the likelihood can only be bounded. We demonstrate the utility of our DPP learning methods in studying the progression of diabetic neuropathy based on the spatial distribution of nerve fibers, and in studying human perception of diversity in images."
2130,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Nystrom Approximation for Large-Scale Determinantal Processes,"Raja Hafiz Affandi, Alex Kulesza, Emily Fox, Ben Taskar",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/affandi13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_affandi13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/affandi13a.html,"Determinantal point processes (DPPs) are appealing models for subset selection problems where diversity is desired. They offer surprisingly efficient inference, including sampling in \(O(N^3)\) time and \(O(N^2)\) space, where \(N\) is the number of base items. However, in some applications, \(N\) may grow so large that sampling from a DPP becomes computationally infeasible. This is especially true in settings where the DPP kernel matrix cannot be represented by a linear decomposition of low-dimensional feature vectors. In these cases, we propose applying the Nystrom approximation to project the kernel matrix into a low-dimensional space. While theoretical guarantees for the Nystrom approximation in terms of standard matrix norms have been previously established, we are concerned with probabilistic measures, like total variation distance between the DPP generated by a kernel matrix and the one generated by its Nystrom approximation, that behave quite differently. In this paper we derive new error bounds for the Nystrom-approximated DPP and present empirical results to corroborate them. We then demonstrate the Nystrom-approximated DPP by applying it to a motion capture summarization task."
2131,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Concept Drift Detection Through Resampling,"Maayan Harel, Shie Mannor, Ran El-Yaniv, Koby Crammer",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/harel14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/harel14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_harel14,http://jmlr.csail.mit.edu/proceedings/papers/v32/harel14.html,"Detecting changes in data-streams is an important part of enhancing learning quality in dynamic environments. We devise a procedure for detecting concept drifts in data-streams that relies on analyzing the empirical loss of learning algorithms. Our method is based on obtaining statistics from the loss distribution by reusing the data multiple times via resampling. We present theoretical guarantees for the proposed procedure based on the stability of the underlying learning algorithms. Experimental results show that the detection method has high recall and precision, and performs well in the presence of noise."
2132,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Narrowing the Gap: Random Forests In Theory and In Practice,"Misha Denil, David Matheson, Nando De Freitas",none,http://jmlr.org/proceedings/papers/v32/denil14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/denil14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_denil14,http://jmlr.csail.mit.edu/proceedings/papers/v32/denil14.html,"Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoreti- cally tractable variant of random regression forests and prove that our algorithm is con- sistent. We also provide an empirical eval- uation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in prac- tice. Our experiments provide insight into the relative importance of different simplifi- cations that theoreticians have made to ob- tain tractable models for analysis."
2133,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Structure Discovery in Nonparametric Regression through Compositional Kernel Search,"David Duvenaud, James Lloyd, Roger Grosse, Joshua Tenenbaum, Ghahramani Zoubin",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/duvenaud13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_duvenaud13,http://jmlr.csail.mit.edu/proceedings/papers/v28/duvenaud13.html,"Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks."
2134,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Parameter Learning and Convergent Inference for Dense Random Fields,"Philipp Kraehenbuehl, Vladlen Koltun",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kraehenbuehl13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kraehenbuehl13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kraehenbuehl13.html,"Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model. As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge."
2135,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Marginalized Denoising Auto-encoders for Nonlinear Representations,"Minmin Chen, Kilian Weinberger, Fei Sha, Yoshua Bengio",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/cheng14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_cheng14,http://jmlr.csail.mit.edu/proceedings/papers/v32/cheng14.html,"Denoising auto-encoders (DAEs) have been successfully used to learn new representations for a wide range of machine learning tasks. During training, DAEs make many passes over the training dataset and reconstruct it from partial corruption generated from a pre-specified corrupting distribution. This process learns robust representation, though at the expense of requiring many training epochs, in which the data is explicitly corrupted. In this paper we present the marginalized Denoising Auto-encoder (mDAE), which (approximately) marginalizes out the corruption during training. Effectively, the mDAE takes into account infinitely many corrupted copies of the training data in every epoch, and therefore is able to match or outperform the DAE with much fewer training epochs. We analyze our proposed algorithm and show that it can be understood as a classic auto-encoder with a special form of regularization. In empirical evaluations we show that it attains 1-2 order-of-magnitude speedup in training time over other competing approaches."
2136,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Deep Clustered Convolutional Kernels,"Minyoung Kim, Luca Rigazio",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/kim2015a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_kim2015a,http://jmlr.csail.mit.edu/proceedings/papers/v44/kim2015a.html,"Deep neural networks have recently achieved state of the art performance thanks to new training algorithms for rapid parameter estimation and new regularizations to reduce over- fitting. However, in practice the network architecture has to be manually set by domain experts, generally by a costly trial and error procedure, which often accounts for a large portion of the final system performance. We view this as a limitation and propose a novel training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then eliminating model redundancy by selectively removing parameters at training time. For convolutional neural networks, our method relies on iterative split/merge clustering of convolutional kernels interleaved by stochastic gradient descent. We present a training algorithm and experimental results on three different vision tasks, showing improved performance compared to similarly sized hand-crafted architec- tures."
2137,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Stage-wise Training: An Improved Feature Learning Strategy for Deep Models,"Elnaz Barshan, Paul Fieguth",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/Barshan2015.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_Barshan2015,http://jmlr.csail.mit.edu/proceedings/papers/v44/Barshan2015.html,"Deep neural networks currently stand at the state of the art for many machine learning applications, yet there still remain limitations in the training of such networks because of their very high parameter dimensionality. In this paper we show that network training performance can be improved using a stage-wise learning strategy, in which the learning process is broken down into a number of related sub-tasks that are completed stage-by-stage. The idea is to inject the information to the network gradually so that in the early stages of training the –coarse-scale” properties of the data are captured while the –finer-scale” characteristics are learned in later stages. Moreover, the solution found in each stage serves as a prior to the next stage, which produces a regularization effect and enhances the generalization of the learned representations. We show that decoupling the classifier layer from the feature extraction layers of the network is necessary, as it alleviates the diffusion of gradient and over-fitting problems. Experimental results in the context of image classification support these claims."
2138,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Hierarchical Feature Extraction for Efficient Design of Microfluidic Flow Patterns,"Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar Ganapathysubramanian, Soumik Sarkar",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/lore15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_lore15,http://jmlr.csail.mit.edu/proceedings/papers/v44/lore15.html,"Deep neural networks are being widely used for feature representation learning in diverse problem areas ranging from object recognition and speech recognition to robotic perception and human disease prediction. We demonstrate a novel, perhaps the first application of deep learning in mechanical design, specifically to learn complex microfluidic flow patterns in order to solve inverse problems in fluid mechanics. A recent discovery showed the ability to control the fluid deformations in a microfluidic channel by placing a sequence of pillars. This provides a fundamental tool for numerous material science, manufacturing and biological applications. However, designing pillar sequences for user-defined deformations is practically infeasible as the current process requires laborious and time-consuming design iterations in a very large, highly nonlinear design space that can have as large as \(10^{15}\) possibilities. We demonstrate that hierarchical feature extraction can potentially lead to a scalable design tool via learning semantic representations from a relatively small number of flow pattern examples. The paper compares the performances of pre-trained deep neural networks and deep convolutional neural networks as well as their learnt features. We show that a balanced training data generation process with respect to a metric on the output space improves the feature extraction performance. Overall, the deep learning based design process is shown to expedite the current state-of-the-art design approaches by more than \(600\) times."
2139,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Learning Multi-channel Deep Feature Representations for Face Recognition,"Xue-wen Chen, Melih Aslan, Kunlei Zhang, Thomas Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/chen15learning.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_chen15learning,http://jmlr.csail.mit.edu/proceedings/papers/v44/chen15learning.html,"Deep learning provides a natural way to obtain feature representations from data without relying on hand-crafted descriptors. In this paper, we propose to learn deep feature representations using unsupervised and supervised learning in a cascaded fashion to produce generically descriptive yet class speci c features. The proposed method can take full advantage of the availability of large-scale unlabeled data and learn discriminative features (supervised) from generic features (unsupervised). It is then applied to multiple essential facial regions to obtain multi-channel deep facial representations for face recognition. The effecacy of the proposed feature representations is validated on both controlled (i.e., extended Yale- B, Yale, and AR) and uncontrolled (PubFig) benchmark face databases. Experimental results show its effectiveness."
2140,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Open Problem: The landscape of the loss surfaces of multilayer networks,"Anna, Choromanska, Yann, LeCun, G_rard Ben Arous",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Choromanska15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Choromanska15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Choromanska15.html,"Deep learning has enjoyed a resurgence of interest in the last few years for such applications as image and speech recognition, or natural language processing. The vast majority of practical applications of deep learning focus on supervised learning, where the supervised loss function is minimized using stochastic gradient descent. The properties of this highly non-convex loss function, such as its landscape and the behavior of critical points (maxima, minima, and saddle points), as well as the reason why large- and small-size networks achieve radically different practical performance, are however very poorly understood. It was only recently shown that new results in spin-glass theory potentially may provide an explanation for these problems by establishing a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models. The connection between both models relies on a number of possibly unrealistic assumptions, yet the empirical evidence suggests that the connection may exist in real. The question we pose is whether it is possible to drop some of these assumptions to establish a stronger connection between both models."
2141,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Learning Deep Sigmoid Belief Networks with Data Augmentation,"Zhe Gan, Ricardo Henao, David Carlson, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/gan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/gan15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_gan15,http://jmlr.csail.mit.edu/proceedings/papers/v38/gan15.html,"Deep directed generative models are developed. The multi-layered model is designed by stacking sigmoid belief networks, with sparsity-encouraging priors placed on the model parameters. Learning and inference of layer-wise model parameters are implemented in a Bayesian setting. By exploring the idea of data augmentation and introducing auxiliary Polya-Gamma variables, simple and efficient Gibbs sampling and mean-field variational Bayes (VB) inference are implemented. To address large-scale datasets, an online version of VB is also developed. Experimental results are presented for three publicly available datasets: MNIST, Caltech 101 Silhouettes and OCR letters."
2142,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning the Structure of Deep Sparse Graphical Models,"Ryan Adams, Hanna Wallach, Zoubin Ghahramani","9:1-8, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/adams10a/adams10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_adams10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/adams10a.html,Deep belief networks are a powerful way to model complex probability distributions. However it is difficult to learn the structure of a belief network particularly one with hidden units. The Indian buffet process has been used as a nonparametric Bayesian prior on the structure of a directed belief network with a single infinitely wide hidden layer. Here we introduce the cascading Indian buffet process (CIBP) which provides a prior on the structure of a layered directed belief network that is unbounded in both depth and width yet allows tractable inference. We use the CIBP prior with the nonlinear Gaussian belief network framework to allow each unit to vary its behavior between discrete and continuous representations. We use Markov chain Monte Carlo for inference in this model and explore the structures learned on image data.
2143,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Approximation properties of DBNs with binary hidden units and real-valued visible units,"Oswin Krause, Asja Fischer, Tobias Glasmachers, Christian Igel",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/krause13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_krause13,http://jmlr.csail.mit.edu/proceedings/papers/v28/krause13.html,"Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case. We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain."
2144,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On the importance of initialization and momentum in deep learning,"Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sutskever13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/sutskever13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sutskever13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sutskever13.html,"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods."
2145,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Top-down particle filtering for Bayesian decision trees,"Balaji Lakshminarayanan, Daniel Roy, Yee Whye Teh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lakshminarayanan13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/lakshminarayanan13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lakshminarayanan13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lakshminarayanan13.html,"Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulationswhich introduce a prior distribution over decision trees, and formulate learning as posterior inference given datahave been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff."
2146,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Generalized Linear Models for Aggregated Data,"Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/bhowmik15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_bhowmik15,http://jmlr.csail.mit.edu/proceedings/papers/v38/bhowmik15.html,"Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms."
2147,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,High density-focused uncertainty sampling for active learning over evolving stream data,"Dino Ienco, Indr_ _liobait_, Bernhard Pfahringer","36 :133-148, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/ienco14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_ienco14,http://jmlr.csail.mit.edu/proceedings/papers/v36/ienco14.html,"Data labeling is an expensive and time-consuming task, hence carefully choosing which labels to use for training a model is becoming increasingly important. In the active learning setting, a classifier is trained by querying labels from a small representative fraction of data. While many approaches exist for non-streaming scenarios, few works consider the challenges of the data stream setting. We propose a new active learning method for evolving data streams based on a combination of density and prediction uncertainty (DBALStream). Our approach decides to label an instance or not, considering whether it lies in an high density partition of the data space. This allows focusing labelling efforts in the instance space where more data is concentrated; hence, the benefits of learning a more accurate classifier are expected to be higher. Instance density is approximated in an online manner by a sliding window mechanism, a standard technique for data streams. We compare our method with state-of-the-art active learning strategies over benchmark datasets. The experimental analysis demonstrates good predictive performance of the new approach."
2148,8,http://jmlr.csail.mit.edu/proceedings/papers/v8/,Accuracy-Rejection Curves (ARCs) for Comparing Classification Methods with a Reject Option,"Malik Sajjad Ahmed Nadeem, Jean-Daniel Zucker, Blaise Hanczar","8:65-81, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v8/nadeem10a/nadeem10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v8/,,2nd March 2010,"September 5-6, 2009",Proceedings of MLSB 2009,Proceedings of the third International Workshop on Machine Learning in Systems Biology,"Ljubljana, Slovenia","SaÅço DÅ_eroski, Pierre Geurts, Juho Rousu",v8_nadeem10a,http://jmlr.csail.mit.edu/proceedings/papers/v8/nadeem10a.html,Data extracted from microarrays are now considered an important source of knowledge about various diseases. Several studies based on microarray data and the use of receiver operating characteristics (ROC) graphs have compared supervised machine learning approaches. These comparisons are based on classification schemes in which all samples are classified regardless of the degree of confidence associated with the classification of a particular sample on the basis of a given classifier. In the domain of healthcare it is safer to refrain from classifying a sample if the confidence assigned to the classification is not high enough rather than classifying all samples even if confidence is low. We describe an approach in which the performance of different classifiers is compared with the possibility of rejection based on several reject areas. Using a tradeoff between accuracy and rejection we propose the use of accuracy-rejection curves (ARCs) and three types of relationship between ARCs for comparisons of the ARCs of two classifiers. Empirical results based on purely synthetic data semi-synthetic data (generated from real data obtained from patients) and public microarray data for binary classification problems demonstrate the efficacy of this method.
2149,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,Towards Optimal Execution of Density-based Clustering on Heterogeneous Hardware,"Dirk Habich, Stefanie Gahrig, Wolfgang Lehner","36 :104-119, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/habich14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_habich14,http://jmlr.csail.mit.edu/proceedings/papers/v36/habich14.html,"Data Clustering is an important and highly utilized data mining technique in various application domains. With ever increasing data volumes in the era of big data, the efficient execution of clustering algorithms is a fundamental prerequisite to gain understanding and acquire novel, previously unknown knowledge from data. To establish an efficient execution, the clustering algorithms have to be re-engineered to fully exploit the provided hardware capabilities. Shared-memory multiprocessor systems like graphics processing units (GPUs) provide extremely high parallelism combined with a high bandwidth transfer at low cost. The availability of such computing units increases with upcoming processors, where a common CPU and various computing units, like GPU, are tightly coupled using a unified shared memory hierarchy. In this paper, we consider density-based clustering for such heterogeneous systems. In particular, we optimize the configuration of CUDA-DClust _ a density-based clustering algorithm for GPUs _ and show that our configuration approach enables an efficient and deterministic execution. Our configuration approach is based on data as well as hardware properties, so that we are able to adjust the algorithm execution in both directions. In our evaluation, we show the applicability of our approach and present open challenges which have to be solved next."
2150,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Efficient Hypergraph Clustering,"Marius Leordeanu, Cristian Sminchisescu",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/leordeanu12/leordeanu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_leordeanu12,http://jmlr.csail.mit.edu/proceedings/papers/v22/leordeanu12.html,Data clustering is an essential problem in data mining machine learning and computer vision. In this paper we present a novel method for the hypergraph clustering problem in which second or higher order affinities between sets of data points are considered. Our algorithm has important theoretical properties such as convergence and satisfaction of first order necessary optimality conditions. It is based on an efficient iterative procedure which by updating the cluster membership of all points in parallel is able to achieve state of the art results in very few steps. We outperform current hypergraph clustering methods especially in terms of computational speed but also in terms of accuracy. Moreover we show that our method could be successfully applied both to higher-order assignment problems and to image segmentation.
2151,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Distributed and Adaptive Darting Monte Carlo through Regenerations,"Sungjin Ahn, Yutian Chen, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/ahn13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_ahn13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/ahn13a.html,"Darting Monte Carlo (DMC) is a MCMC procedure designed to effectively mix between multiple modes of a probability distribution. We propose an adaptive and distributed version of this method by using regenerations. This allows us to run multiple chains in parallel and adapt the shape of the jump regions as well as all other aspects of the Markov chain on the fly. We show that this significantly improves the performance of DMC because 1) a population of chains has a higher chance of finding the modes in the distribution, 2) jumping between modes becomes easier due to the adaptation of their shapes, 3) computation is much more efficient due to parallelization across multiple processors. While the curse of dimensionality is a challenge for both DMC and regeneration, we find that their combination ameliorates this issue slightly."
2152,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Statistical Tests for Contagion in Observational Social Network Studies,"Greg Ver Steeg, Aram Galstyan",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/steeg13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,http://jmlr.csail.mit.edu/proceedings/papers/v31/steeg13a-supp.zip,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_steeg13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/steeg13a.html,"Current tests for contagion in social network studies are vulnerable to the confounding effects of latent homophily (i.e., ties form preferentially between individuals with similar hidden traits). We demonstrate a general method to lower bound the strength of causal effects in observational social network studies, even in the presence of arbitrary, unobserved individual traits. Our tests require no parametric assumptions and each test is associated with an algebraic proof. We demonstrate the effectiveness of our approach by correctly deducing the causal effects for examples previously shown to expose defects in existing methodology. Finally, we discuss preliminary results on data taken from the Framingham Heart Study."
2153,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Bayesian structure discovery in Bayesian networks with less space,"Pekka Parviainen, Mikko Koivisto","9:589-596, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/parviainen10a/parviainen10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_parviainen10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/parviainen10a.html,Current exact algorithms for score-based structure discovery in Bayesian networks on n nodes run in time and space within a polynomial factor of 2^n. For practical use the space requirement is the bottleneck which motivates trading space against time. Here previous results on finding an optimal network structure in less space are extended in two directions. First we consider the problem of computing the posterior probability of a given arc set. Second we operate with the general partial order framework and its specialization to bucket orders introduced recently for related permutation problems. The main technical contribution is the development of a fast algorithm for a novel zeta transform variant which may be of independent interest.
2154,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks,"Creighton Heaukulani, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/heaukulani13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_heaukulani13,http://jmlr.csail.mit.edu/proceedings/papers/v28/heaukulani13.html,"Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected. In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks. We demonstrate our modelês capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks."
2155,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,CUR Algorithm for Partially Observed Matrices,"Miao Xu, Rong Jin, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/xua15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/xua15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_xua15,http://jmlr.csail.mit.edu/proceedings/papers/v37/xua15.html,"CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they cannot deal with entries in a partially observed matrix, while incomplete matrices are found in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only \(O(n r\ln r)\) observed entries are needed by the proposed algorithm to perfectly recover a rank \(r\) matrix of size \(n\times n\) , which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm."
2156,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Divergences and Risks for Multiclass Experiments,Dario GarcÍa GarcÍa and Robert C. Williamson,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/garcia12/garcia12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_garcia12,http://jmlr.csail.mit.edu/proceedings/papers/v23/garcia12.html,Csiszˆr's f -divergence is a way to measure the similarity of two probability distributions. We study the extension of f -divergence to more than two distributions to measure their joint similarity. By exploiting classical results from the comparison of experiments literature we prove the resulting divergence satisfies all the same properties as the traditional binary one. Considering the multidistribution case actually makes the proofs simpler. The key to these results is a formal bridge between these multidistribution f -divergences and Bayes risks for multiclass classification problems.
2157,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Adaptive Task Assignment for Crowdsourced Classification,"Chien-Ju Ho, Shahin Jabbari, Jennifer Wortman Vaughan",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/ho13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_ho13,http://jmlr.csail.mit.edu/proceedings/papers/v28/ho13.html,"Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as –offensive” or –not offensive”) for instances (such as websites), are among the most common tasks posted, but due to a mix of human error and the overwhelming prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way. However, the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse."
2158,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Sequential crowdsourced labeling as an epsilon-greedy exploration in a Markov Decision Process,"Vikas Raykar, Priyanka Agrawal","33 :832-840, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/raykar14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/raykar14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_raykar14,http://jmlr.csail.mit.edu/proceedings/papers/v33/raykar14.html,"Crowdsourcing marketplaces are widely used for curating large annotated datasets by collecting labels from multiple annotators. In such scenarios one has to balance the tradeoff between the accuracy of the collected labels, the cost of acquiring these labels, and the time taken to finish the labeling task. With the goal of reducing the labeling cost, we introduce the notion of sequential crowdsourced labeling, where instead of asking for all the labels in one shot we acquire labels from annotators sequentially one at a time. We model it as an epsilon-greedy exploration in a Markov Decision Process with a Bayesian decision theoretic utility function that incorporates accuracy, cost and time. Experimental results confirm that the proposed sequential labeling procedure can achieve similar accuracy at roughly half the labeling cost and at any stage in the labeling process the algorithm achieves a higher accuracy compared to randomly asking for the next label."
2159,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multiple-source cross-validation,"Krzysztof Geras, Charles Sutton",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/geras13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_geras13,http://jmlr.csail.mit.edu/proceedings/papers/v28/geras13.html,"Cross-validation is an essential tool in machine learning and statistics. The typical procedure, in which data points are randomly assigned to one of the test sets, makes an implicit assumption that the data are exchangeable. A common case in which this does not hold is when the data come from multiple sources, in the sense used in transfer learning. In this case it is common to arrange the cross-validation procedure in a way that takes the source structure into account. Although common in practice, this procedure does not appear to have been theoretically analysed. We present new estimators of the variance of the cross-validation, both in the multiple-source setting and in the standard iid setting. These new estimators allow for much more accurate confidence intervals and hypothesis tests to compare algorithms."
2160,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Cross-Entropy Method Optimizes for Quantiles,"Sergiu Goschin, Ari Weinstein, Michael Littman",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/goschin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_goschin13,http://jmlr.csail.mit.edu/proceedings/papers/v28/goschin13.html,"Cross-entropy optimization (CE) has proven to be a powerful tool for search in control environments. In the basic scheme, a distribution over proposed solutions is repeatedly adapted by evaluating a sample of solutions and refocusing the distribution on a percentage of those with the highest scores. We show that, in the kind of noisy evaluation environments that are common in decision-making domains, this percentage-based refocusing does not optimize the expected utility of solutions, but instead a quantile metric. We provide a variant of CE (Proportional CE) that effectively optimizes the expected value. We show using variants of established noisy environments that Proportional CE can be used in place of CE and can improve solution quality."
2161,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels,"Kai Zhang, Vincent Zheng, Qiaojun Wang, James Kwok, Qiang Yang, Ivan Marsic",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zhang13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13b.html,"Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercers theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications."
2162,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,An Abstract Framework for Counterexample Analysis in Active Automata Learning,"Malte Isberner, Bernhard Steffen"," 34 :79-93, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/isberner14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_isberner14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/isberner14a.html,"Counterexample analysis has emerged as one of the key challenges in Angluin-style active automata learning. Rivest and Schapire (1993) showed for the \(\mathrm{L}^*\) algorithm that a single suffix of the counterexample was sufficient to ensure progress. This suffix can be obtained in a binary search fashion, requiring \(\Theta(\log m)\) membership queries for a counterexample of length \(m\) . Correctly implementing this algorithm can be quite tricky, and its correctness sometimes even has been disputed. In this paper, we establish an abstract framework for counterexample analysis, which basically reduces the problem of finding a suffix to finding distinct neighboring elements in a \(0/1\) sequence, where the first element is \(0\) and the last element is \(1\) . We demonstrate the conciseness and simplicity of our framework by using it to present new counterexample analysis algorithms, which, while maintaining the worst-case complexity of \(O(\log m)\) , perform significantly better in practice. Furthermore, we contributeãin a second instantiation of our framework, highlighting its generalityãthe first sublinear counterexample analysis procedures for the algorithm due to Kearns and Vazirani (1994)."
2163,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Co-Training with Insufficient Views,"Wei Wang, Zhi-Hua Zhou","29 :467-482, 2013",http://jmlr.org/proceedings/papers/v29/Wang13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Wang13b,http://jmlr.csail.mit.edu/proceedings/papers/v29/Wang13b.html,"Co-training is a famous semi-supervised learning paradigm exploiting unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sufficient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sufficient. We define the diversity between the two views with respect to the confidence of prediction and prove that if the two views have large diversity, co-training is able to improve the learning performance by exploiting unlabeled data even with insufficient views. We also discuss the relationship between view insufficiency and diversity, and give some implications for understanding of the difference between co-training and co-regularization."
2164,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting,"Oscar Beijbom, Mohammad Saberian, David Kriegman, Nuno Vasconcelos",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/beijbom14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/beijbom14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_beijbom14,http://jmlr.csail.mit.edu/proceedings/papers/v32/beijbom14.html,"Cost-sensitive multiclass classification has recently acquired significance in several applications, through the introduction of multiclass datasets with well-defined misclassification costs. The design of classification algorithms for this setting is considered. It is argued that the unreliable performance of current algorithms is due to the inability of the underlying loss functions to enforce a certain fundamental underlying property. This property, denoted guess-aversion, is that the loss should encourage correct classifications over the arbitrary guessing that ensues when all classes are equally scored by the classifier. While guess-aversion holds trivially for binary classification, this is not true in the multiclass setting. A new family of cost-sensitive guess-averse loss functions is derived, and used to design new cost-sensitive multiclass boosting algorithms, denoted GEL- and GLL-MCBoost. Extensive experiments demonstrate (1) the general importance of guess-aversion and (2) that the GLL loss function outperforms other loss functions for multiclass boosting."
2165,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Multivariate Maximal Correlation Analysis,"Hoang Vu Nguyen, Emmanuel Môller, Jilles Vreeken, Pavel Efros, Klemens B_hm",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/nguyenc14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_nguyenc14,http://jmlr.csail.mit.edu/proceedings/papers/v32/nguyenc14.html,"Correlation analysis is one of the key elements of statistics, and has various applications in data analysis. Whereas most existing measures can only detect pairwise correlations between two dimensions, modern analysis aims at detecting correlations in multi-dimensional spaces. We propose MAC, a novel multivariate correlation measure designed for discovering multi-dimensional patterns. It belongs to the powerful class of maximal correlation analysis, for which we propose a generalization to multivariate domains. We highlight the limitations of current methods in this class, and address these with MAC. Our experiments show that MAC outperforms existing solutions, is robust to noise, and discovers interesting and useful patterns."
2166,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Gaussian Process Vine Copulas for Multivariate Dependence,"David Lopez-Paz, Jose Miguel Hernˆndez-Lobato, Ghahramani Zoubin",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/lopez-paz13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_lopez-paz13,http://jmlr.csail.mit.edu/proceedings/papers/v28/lopez-paz13.html,"Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables. We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data."
2167,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Accelerated Coordinate Descent with Adaptive Coordinate Frequencies,"Tobias Glasmachers, Urun Dogan","29 :72-86, 2013",http://jmlr.org/proceedings/papers/v29/Glasmachers13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Glasmachers13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Glasmachers13.html,"Coordinate descent (CD) algorithms have become the method of choice for solving a number of machine learning tasks. They are particularly popular for training linear models, including linear support vector machine classification, LASSO regression, and logistic regression. We propose an extension of the CD algorithm, called the adaptive coordinate frequencies (ACF) method. This modified CD scheme does not treat all coordinates equally, in that it does not pick all coordinates equally often for optimization. Instead the relative frequencies of coordinates are subject to online adaptation. The resulting optimization scheme can result in significant speed-ups. We demonstrate the usefulness of our approach on a number of large scale machine learning problems."
2168,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Fast Bundle-based Anytime Algorithm for Poker and other Convex Games,"H. Brendan McMahan, Geoffrey J. Gordony","2:323-330, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/mcmahan07a/mcmahan07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_mcmahan07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/mcmahan07a.html,Convex games are a natural generalization of matrix (normal-form) games that can compactly model many strategic interactions with interesting structure. We present a new anytime algorithm for such games that leverages fast best-response oracles for both players to build a model of the overall game. This model is used to identify search directions; the algorithm then does an exact minimization in this direction via a specialized line search. We test the algorithm on a simplified version of Texas Hold'em poker represented as an extensive-form game. Our algorithm approximated the exact value of this game within $0.20 (the maximum pot size is $310.00) in a little over 2 hours using less than 1.5GB of memory; finding a solution with comparable bounds using a state-of-theart interior-point linear programming algorithm took over 4 days and 25GB of memory.
2169,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Convex envelopes of complexity controlling penalties: the case against premature envelopment,"Vladimir Jojic, Suchi Saria, Daphne Koller","15:399-406, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/jojic11a/jojic11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_jojic11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/jojic11a.html,Convex envelopes of the cardinality and rank function $l_1$ and nuclear norm have gained immense popularity due to their sparsity inducing properties. This gave rise to a natural approach to building objectives with sparse optima whereby such convex penalties are added to another objective. Such a heuristic approach to objective building does not always work. For example addition of an $L_1$ penalty to the KL-divergence fails to induce any sparsity as the $L_1$ norm of any vector in a simplex is a constant. However a convex envelope of KL and a cardinality penalty can be obtained that indeed trades off sparsity and KL-divergence. We consider cases of two composite penalties elastic net and fused lasso which combine multiple desiderata. In both of these cases we show that a hard objective relaxed to obtain penalties can be more tightly approximated. Further by construction it is impossible to get a better convex approximation than the ones we derive. Thus constructing a joint envelope across different parts of the objective provides means to trade off tightness and computational cost.
2170,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,A simple criterion for controlling selection bias,"Eunice Yuh-Jie Chen, Judea Pearl",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_chen13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13b.html,"Controlling selection bias, a statistical error caused by preferential sampling of data, is a fundamental problem in machine learning and statistical inference. This paper presents a simple criterion for controlling selection bias in the odds ratio, a widely used measure for association between variables, that connects the nature of selection bias with the graph modeling the selection mechanism. If the graph contains certain paths, we show that the odds ratio cannot be expressed using data with selection bias. Otherwise, we show that a d-separability test can determine whether the odds ratio can be recovered, and when the answer is affirmative, output an unbiased estimand of the odds ratio. The criterion can be test in linear time and enhances the power of the estimand."
2171,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,On the Convergence Properties of Contrastive Divergence,"Ilya Sutskever, Tijmen Tieleman","9:789-795, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/sutskever10a/sutskever10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_sutskever10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/sutskever10a.html,Contrastive Divergence (CD) is a popular method for estimating the parameters of Markov Random Fields (MRFs) by rapidly approximating an intractable term in the gradient of the log probability. Despite CD's empirical success little is known about its theoretical convergence properties. In this paper we analyze the CD-1 update rule for Restricted Boltzmann Machines (RBMs) with binary variables. We show that this update is not the gradient of any function and construct a counterintuitive ``regularization function'' that causes CD learning to cycle indefinitely. Nonetheless we show that the regularized CD update has a fixed point for a large class of regularization functions using Brower's fixed point theorem.
2172,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Contextual Bandit Learning with Predictable Rewards,"Alekh Agarwal, Miroslav Dudik, Satyen Kale, John Langford, Robert Schapire",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/agarwal12/agarwal12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_agarwal12,http://jmlr.csail.mit.edu/proceedings/papers/v22/agarwal12.html,Contextual bandit learning is a reinforcement learning problem where the learner repeatedly receives a set of features (context) takes an action and receives a reward based on the action and context. We consider this problem under a realizability assumption: there exists a function in a (known) function class always capable of predicting the expected reward given the action and context. Under this assumption we show three things. We present a new algorithm--Regressor Elimination -- with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However we do show that for \emph{any} set of policies (mapping contexts to actions) there is a distribution over rewards (given context) such that our new algorithm has {\em constant} regret unlike the previous approaches.
2173,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,An Unbiased Offline Evaluation of Contextual Bandit Algorithms with Generalized Linear Models,"Lihong Li, Wei Chu, John Langford, Taesup Moon, Xuanhui Wang","26:19-36, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/li12a/li12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,40726,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_li12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/li12a.html,"Contextual bandit algorithms have become popular tools in online recommendation and advertising systems. Offline evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their ``partial-label'' nature. A common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating the simulator itself is often difficult and modeling bias is usually unavoidably introduced. The purpose of this paper is two-fold. First, we review a recently proposed offline evaluation technique. Different from simulator-based approaches, the method is completely data-driven, is easy to adapt to different applications, and more importantly, provides provably unbiased evaluations. We argue for the wide use of this technique as standard practice when comparing bandit algorithms in real-life problems. Second, as an application of this technique, we compare and validate a number of new algorithms based on generalized linear models. Experiments using real Yahoo! data suggest substantial improvement over algorithms with linear models when the rewards are binary."
2174,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Dealing with small data: On the generalization of context trees,"Ralf Eggeling, Mikko Koivisto, Ivo Grosse",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/eggeling15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/eggeling15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_eggeling15,http://jmlr.csail.mit.edu/proceedings/papers/v37/eggeling15.html,"Context trees (CT) are a widely used tool in machine learning for representing context-specific independences in conditional probability distributions. Parsimonious context trees (PCTs) are a recently proposed generalization of CTs that can enable statistically more efficient learning due to a higher structural flexibility, which is particularly useful for small-data settings. However, this comes at the cost of a computationally expensive structure learning algorithm, which is feasible only for domains with small alphabets and tree depths. In this work, we investigate to which degree CTs can be generalized to increase statistical efficiency while still keeping the learning computationally feasible. Approaching this goal from two different angles, we (i) propose algorithmic improvements to the PCT learning algorithm, and (ii) study further generalizations of CTs, which are inspired by PCTs, but trade structural flexibility for computational efficiency. By empirical studies both on simulated and real-world data, we demonstrate that the synergy of combining of both orthogonal approaches yields a substantial improvement in obtaining statistically efficient and computationally feasible generalizations of CTs."
2175,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Skip Context Tree Switching,"Marc Bellemare, Joel Veness, Erik Talvitie",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/bellemare14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/bellemare14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_bellemare14,http://jmlr.csail.mit.edu/proceedings/papers/v32/bellemare14.html,"Context Tree Weighting (CTW) is a powerful probabilistic sequence prediction technique that efficiently performs Bayesian model averaging over the class of all prediction suffix trees of bounded depth. In this paper we show how to generalize this technique to the class of K-skip prediction suffix trees. Contrary to regular prediction suffix trees, K-skip prediction suffix trees are permitted to ignore up to K contiguous portions of the context. This allows for significant improvements in predictive accuracy when irrelevant variables are present, a case which often occurs within record-aligned data and images. We provide a regret-based analysis of our approach, and empirically evaluate it on the Calgary corpus and a set of Atari 2600 screen prediction tasks."
2176,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Low rank continuous-space graphical models,"Carl Smith, Frank Wood, Liam Paninski",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/smith12/smith12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_smith12,http://jmlr.csail.mit.edu/proceedings/papers/v22/smith12.html,"Constructing tractable dependent probability distributions over structured continuous random vectors is a central problem in statistics and machine learning. It has proven difficult to find general constructions for models in which efficient exact inference is possible outside of the classical cases of models with restricted graph structure (chain tree etc.) and linear-Gaussian or discrete potentials. In this work we identify a tree graphical model class in which exact inference can be performed efficiently owing to a certain ""low-rank"" structure in the potentials. We explore this new class of models by applying the resulting inference methods to neural spike rate estimation and motion-capture joint-angle smoothing tasks."
2177,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Kernels Based Tests with Non-asymptotic Bootstrap Approaches for Two-sample Problems,"Magalie Fromont, B_atrice Laurent, Matthieu Lerasle and Patricia Reynaud-Bouret",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/fromont12/fromont12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_fromont12,http://jmlr.csail.mit.edu/proceedings/papers/v23/fromont12.html,"Considering either two independent i.i.d. samples, or two independent samples generated from a heteroscedastic regression model, or two independent Poisson processes, we address the question of testing equality of their respective distributions. We first propose single testing procedures based on a general symmetric kernel. The corresponding critical values are chosen from a wild or permutation bootstrap approach, and the obtained tests are exactly (and not just asymptotically) of level. We then introduce an aggregation method, which enables to overcome the difficulty of choosing a kernel and/or the parameters of the kernel. We derive non-asymptotic properties for the aggregated tests, proving that they may be optimal in a classical statistical sense."
2178,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence,"Nihar Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin Wainwright",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/shah15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/shah15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_shah15,http://jmlr.csail.mit.edu/proceedings/papers/v38/shah15.html,"Consider the problem of identifying the underlying qualities of a set of items based on measuring noisy comparisons between pairs of items. The Bradley-Terry-Luce (BTL) and Thurstone models are the most widely used parametric models for such pairwise comparison data. Working within a standard minimax framework, this paper provides sharp upper and lower bounds on the optimal error in estimating the underlying qualities under the BTL and the Thurstone models. These bounds are are topology-aware, meaning that they change qualitatively depending on the comparison graph induced by the subset of pairs being compared. Thus, in settings where the subset of pairs may be chosen, our results provide some principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors. We use this result to investigate the relative merits of cardinal and ordinal measurement schemes."
2179,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Beating Bandits in Gradually Evolving Worlds,"Chao-Kai Chiang, Chia-Jung Lee, Chi-Jen Lu",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Chiang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Chiang13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Chiang13.html,"Consider the online convex optimization problem, in which a player has to choose actions iteratively and suffers corresponding losses according to some convex loss functions, and the goal is to minimize the regret. In the full-information setting, the player after choosing her action can observe the whole loss function in that round, while in the bandit setting, the only information the player can observe is the loss value of that action. Designing such bandit algorithms appears challenging, as the best regret currently achieved for general convex loss functions is much higher than that in the full-information setting, while for strongly convex loss functions, there is even a regret lower bound which is exponentially higher than that achieved in the full-information setting. To aim for smaller regrets, we adopt a relaxed two-point bandit setting in which the player can play two actions in each round and observe the loss values of those two actions. Moreover, we consider loss functions parameterized by their deviation D, which measures how fast they evolve, and we study how regrets depend on D. We show that two-point bandit algorithms can in fact achieve regrets matching those in the full-information setting in terms of D. More precisely, for convex loss functions, we achieve a regret of \(O(\sqrt{D})\) , while for strongly convex loss functions, we achieve a regret of \(O(\ln D)\) , which is much smaller than the \(\Omega(\sqrt{D})\) lower bound in the traditional bandit setting."
2180,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Improved Bounds for Online Learning Over the Permutahedron and Other Ranking Polytopes,Nir Ailon,"33 :29-37, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/ailon14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_ailon14,http://jmlr.csail.mit.edu/proceedings/papers/v33/ailon14.html,"Consider the following game: There is a fixed set \(V\) of \(n\) items. At each step an adversary chooses a score function \(s_t:V\mapsto[0,1]\) , a learner outputs a ranking of \(V\) , and then \(s_t\) is revealed. The learnerês loss is the sum over \(v\in V\) , of \(s_t(v)\) times \(v\) ês position ( \(0\) th, \(1\) st, \(2\) nd, ...) in the ranking. This problem captures, for example, online systems that iteratively present ranked lists of items to users, who then respond by choosing one (or more) sought items. The loss measures the usersê burden, which increases the further the sought items are from the top. It also captures a version of online rank aggregation. We present an algorithm of expected regret \(O(n\sqrt{OPT} + n^2)\) , where OPT is the loss of the best (single) ranking in hindsight. This improves the previously best known algorithm of Suehiro et. al (2012) by saving a factor of \(\Omega(\sqrt{\log n})\) . We also reduce the per-step running time from \(O(n^2)\) to \(O(n\log n)\) . We provide matching lower bounds."
2181,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation,"Sven Kurras, Ulrike von Luxburg, Gilles Blanchard",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/kurras14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/kurras14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_kurras14,http://jmlr.csail.mit.edu/proceedings/papers/v32/kurras14.html,"Consider a neighborhood graph, for example a k-nearest neighbor graph, that is constructed on sample points drawn according to some density p. Our goal is to re-weight the graphês edges such that all cuts and volumes behave as if the graph was built on a different sample drawn from an alternative density q. We introduce the f-adjusted graph and prove that it provides the correct cuts and volumes as the sample size tends to infinity. From an algebraic perspective, we show that its normalized Laplacian, denoted as the f-adjusted Laplacian, represents a natural family of diagonal perturbations of the original normalized Laplacian. Our technique allows to apply any cut and volume based algorithm to the f-adjusted graph, for example spectral clustering, in order to study the given graph as if it were built on an unaccessible sample from a different density. We point out applications in sample bias correction, data uniformization, and multi-scale analysis of graphs."
2182,46,http://jmlr.csail.mit.edu/proceedings/papers/v46/,Neural Connectivity Reconstruction from Calcium Imaging Signal using Random Forest with Topological Features,"Wojciech M. Czarnecki, Rafal Jozefowicz",none,http://jmlr.csail.mit.edu/proceedings/papers/v46/czarnecki15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v46/,,21st October 2015,"Sep 15, 2014 - Sep 15, 2014",NCIC 2014,ECML W - NCIC 2014 : ECML Workshop - _Neural Connectomics: From Imaging to ConnectivityÓ,"Nancy , France","Demian Battaglia, Isabelle Guyon, Vincent Lemaire, Jordi Soriano; Production Editor: Bisakha Ray",v46_czarnecki15,http://jmlr.csail.mit.edu/proceedings/papers/v46/czarnecki15.html,Connectomics is becoming an increasingly popular area of research. With the recent advances in optical imaging of the neural activity tens of thousands of neurons can be monitored simultaneously. In this paper we present a method of incorporating topological knowledge inside data representation for Random Forest classifier in order to reconstruct the neural connections from patterns of their activities. Proposed technique leads to the model competitive with state-of-the art methods like Deep Convolutional Neural Networks and Graph Decomposition techniques. This claim is supported by the results (5th place with 0.003 in terms of AUC ROC loss to the top contestant) obtained in the connectomics competition organized on the Kaggle platform.
2183,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,A Sufficient Statistics Construction of Exponential Family Le _vy Measure Densities for Nonparametric Conjugate Models,"Robert Finn, Brian Kulis",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/finn15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/finn15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_finn15,http://jmlr.csail.mit.edu/proceedings/papers/v38/finn15.html,"Conjugate pairs of distributions over infinite dimensional spaces are prominent in machine learning, particularly due to the widespread adoption of Bayesian nonparametric method- ologies for a host of models and applications. Much of the existing literature in the learning community focuses on processes possessing some form of computationally tractable con- jugacy as is the case for the beta process and the gamma process (and, via normalization, the Dirichlet process). For these processes, conjugacy is proved via statistical machinery tailored to the particular model. We seek to address the problem of obtaining a general construction of prior distributions over infi- nite dimensional spaces possessing distribu- tional properties amenable to conjugacy. Our result is achieved by generalizing Hjortês con- struction of the beta process via appropriate utilization of sufficient statistics for exponen- tial families."
2184,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Efficiency of conformalized ridge regression,"Evgeny Burnaev, Vladimir Vovk","35 :605-622, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/burnaev14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_burnaev14,http://jmlr.csail.mit.edu/proceedings/papers/v35/burnaev14.html,"Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied."
2185,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Efficient Second-Order Gradient Boosting for Conditional Random Fields,"Tianqi Chen, Sameer Singh, Ben Taskar, Carlos Guestrin",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15b-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chen15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15b.html,"Conditional random fields (CRFs) are an important class of models for accurate structured prediction, but effective design of the feature functions is a major challenge when applying CRF models to real world data. Gradient boosting, which is used to automatically induce and select feature functions, is a natural candidate solution to the problem. However, it is non-trivial to derive gradient boosting algorithms for CRFs due to the dense Hessian matrices introduced by variable dependencies. Existing approaches thus use only first-order information when optimizing likelihood, and hence face convergence issues. We incorporate second-order information by deriving a Markov Chain mixing rate bound to quantify the dependencies, and introduce a gradient boosting algorithm that iteratively optimizes an adaptive upper bound of the objective function. The resulting algorithm induces and selects features for CRFs via functional space optimization, with provable convergence guarantees. Experimental results on three real world datasets demonstrate that the mixing rate based upper bound is effective for learning CRFs with non-linear potentials."
2186,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,A Close Look to Margin Complexity and Related Parameters,"Michael Kallweit, Hans Ulrich Simon","19:437-456, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/kallweit11a/kallweit11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_kallweit11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/kallweit11a.html,Concept classes can canonically be represented by sign-matricesi.e. by matrices with entries $1$ and $-1$. The question whethera sign-matrix (concept class) $A$ can be learned by a machine that performs large margin classification is closely related to the``margin complexity'' associated with $A$. We consider severalvariants of margin complexity reveal how they are relatedto each other and we reveal how they are related to other notions of learning-theoretic relevance like SQ-dimension CSQ-dimensionand the Forster bound.
2187,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Empirical Bernstein Boosting,"Pannagadatta Shivaswamy, Tony Jebara","9:733-740, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/shivaswamy10a/shivaswamy10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_shivaswamy10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/shivaswamy10a.html,Concentration inequalities that incorporate variance information (such as Bernstein's or Bennett's inequality) are often significantly tighter than counterparts (such as Hoeffding's inequality) that disregard variance. Nevertheless many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding's inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle--sample variance penalization--which is motivated from an empirical version of Bernstein's inequality. This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization.
2188,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Online Learning of Eigenvectors,"Dan Garber, Elad Hazan, Tengyu Ma",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/garberb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/garberb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_garberb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/garberb15.html,"Computing the leading eigenvector of a symmetric real matrix is a fundamental primitive of numerical linear algebra with numerous applications. We consider a natural online extension of the leading eigenvector problem: a sequence of matrices is presented and the goal is to predict for each matrix a unit vector, with the overall goal of competing with the leading eigenvector of the cumulative matrix. Existing regret-minimization algorithms for this problem either require to compute an eigen decompostion every iteration, or suffer from a large dependency of the regret bound on the dimension. In both cases the algorithms are not practical for large scale applications. In this paper we present new algorithms that avoid both issues. On one hand they do not require any expensive matrix decompositions and on the other, they guarantee regret rates with a mild dependence on the dimension at most. In contrast to previous algorithms, our algorithms also admit implementations that enable to leverage sparsity in the data to further reduce computation. We extend our results to also handle non-symmetric matrices."
2189,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Convergence rates for persistence diagram estimation in Topological Data Analysis,"Fr_d_ric Chazal, Marc Glisse, Catherine Labruère, Bertrand Michel",none,http://jmlr.org/proceedings/papers/v32/chazal14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chazal14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chazal14.html,"Computational topology has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that the use of persistent homology can be naturally considered in general statistical frameworks. We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results."
2190,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Computational Education using Latent Structured Prediction,"Tanja K_ser, Alexander Schwing, Tamir Hazan, Markus Gross","33 :540-548, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kaser14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kaser14,http://jmlr.csail.mit.edu/proceedings/papers/v33/kaser14.html,"Computational education offers an important add-on to conventional teaching. To provide optimal learning conditions, accurate representation of studentsê current skills and adaptation to newly acquired knowledge are essential. To obtain sufficient representational power we investigate suitability of general graphical models and discuss adaptation by learning parameters of a log-linear distribution. For interpretability we propose to constrain the parameter space a-priori by leveraging domain knowledge. We show the benefits of general graphical models and of regularizing the parameter space by evaluation of our models on data collected from a computational education software for children having difficulties in learning mathematics."
2191,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Compressed Counting Meets Compressed Sensing,"Ping Li, Cun-Hui Zhang, Tong Zhang","35 :1058-1077, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/li14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_li14,http://jmlr.csail.mit.edu/proceedings/papers/v35/li14.html,"Compressed sensing (sparse signal recovery) has been a popular and important research topic in recent years. By observing that natural signals (e.g., images or network data) are often nonnegative, we propose a framework for nonnegative signal recovery using Compressed Counting (CC) . CC is a technique built on maximally-skewed \(\alpha\) -stable random projections originally developed for data stream computations (e.g., entropy estimations). Our recovery procedure is computationally efficient in that it requires only one linear scan of the coordinates. In our settings, the signal \(\mathbf{x}\in\mathbb{R}^N\) is assumed to be nonnegative, i.e., \(x_i\geq 0, \forall \ i\) . We prove that, when \(\alpha\in(0,\ 0.5]\) , it suffices to use \(M=(C_\alpha+o(1)) \epsilon^{-\alpha} \left(\sum_{i=1}^N x_i^\alpha\right)\log N/\delta\) measurements so that, with probability \(1-\delta\) , all coordinates will be recovered within \(\epsilon\) additive precision, in one scan of the coordinates. The constant \(C_\alpha=1\) when \(\alpha\rightarrow0\) and \(C_\alpha=\pi/2\) when \(\alpha=0.5\) . In particular, when \(\alpha\rightarrow0\) , the required number of measurements is essentially \(M=K\log N/\delta\) , where \(K = \sum_{i=1}^N 1\{x_i\neq 0\}\) is the number of nonzero coordinates of the signal."
2192,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Visualization Databases for the Analysis of Large Complex Datasets,"Saptarshi Guha, Paul Kidwell, Ryan P. Hafen, William S. Cleveland","5:193-200, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/guha09a/guha09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_guha09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/guha09a.html,Comprehensive visualization that preserves the information in a large complex dataset requires a visualization database (VDB): many displays some with many pages and with one or more panels per page. A single display using a specific display method results from partitioning the data into subsets sampling the subsets and applying the method to each sample typically one per panel. The time of the analyst to generate a display is not increased by choosing a large sample over a small one. Displays and display viewers can be designed to allow rapid scanning. Often it is not necessary to view every page of a display. VDBs already successful just with off-the-shelf tools can be greatly improved by research that rethinks all of the areas of data visualization in the context of VDBs.
2193,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Learning with Blocks: Composite Likelihood and Contrastive Divergence,"Arthur Asuncion, Qiang Liu, Alexander Ihler, Padhraic Smyth","9:33-40, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/asuncion10a/asuncion10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_asuncion10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/asuncion10a.html,Composite likelihood methods provide a wide spectrum of computationally efficient techniques for statistical tasks such as parameter estimation and model selection. In this paper we present a formal connection between the optimization of composite likelihoods and the well-known contrastive divergence algorithm. In particular we show that composite likelihoods can be stochastically optimized by performing a variant of contrastive divergence with random-scan blocked Gibbs sampling. By using higher-order composite likelihoods our proposed learning framework makes it possible to trade off computation time for increased accuracy. Furthermore one can choose composite likelihood blocks that match the model's dependence structure making the optimization of higher-order composite likelihoods computationally efficient. We empirically analyze the performance of blocked contrastive divergence on various models including visible Boltzmann machines conditional random fields and exponential random graph models and we demonstrate that using higher-order blocks improves both the accuracy of parameter estimates and the rate of convergence.
2194,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,Attribute Selection Based on FRiS-Compactness,"Nikolay Zagoruiko, Irina Borisova, Vladimir Dyubanov and Olga Kutnenko","10:35-44, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/zagoruiko10a/zagoruiko10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_zagoruiko10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/zagoruiko10a.html,Commonly to classify new object in Data Mining one should estimate its similarity with given classes. Function of Rival Similarity (FRiS) is assigned to calculate quantitative measure of similarity considering a competitive situation. FRiS-function allows constructing new effective algorithms for various Data Mining tasks solving. In particular it enables to obtain quantitative estimation of compactness of patterns which can be used as indirect criterion for informative attributes selection. FRiS-compactness predicts reliability of recognition of control sample more precisely than such widespread methods as One-Leave-Out and Cross-Validation. Presented in the paper results of real genetic task solving confirm efficiency of FRiS-function using in attributes selection and decision rules construction.
2195,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Risk and Regret of Hierarchical Bayesian Learners,"Jonathan Huggins, Josh Tenenbaum",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsb15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsb15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_hugginsb15,http://jmlr.csail.mit.edu/proceedings/papers/v37/hugginsb15.html,"Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater –robustness” and the ability to –share statistical strength.” Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Studentês \(t\) and hierarchical Gaussian priors allow us to formalize the concepts of –robustness” and –sharing statistical strength.” Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems."
2196,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Multi-View Clustering and Feature Learning via Structured Sparsity,"Hua Wang, Feiping Nie, Heng Huang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wang13c,http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13c.html,"Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods."
2197,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Column Subset Selection with Missing Data via Active Sampling,"Yining Wang, Aarti Singh",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15c-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_wang15c,http://jmlr.csail.mit.edu/proceedings/papers/v38/wang15c.html,"Column subset selection of massive data matrices has found numerous applications in real-world data systems. In this paper, we propose and analyze two sampling based algorithms for column subset selection without access to the complete input matrix. To our knowledge, these are the first algorithms for column subset selection with missing data that are provably correct. The proposed methods work for row/column coherent matrices by employing the idea of adaptive sampling. Furthermore, when the input matrix has a noisy low-rank structure, one algorithm enjoys a relative error bound."
2198,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Message Passing for Collective Graphical Models,"Tao Sun, Dan Sheldon, Akshat Kumar",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sunc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sunc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sunc15.html,"Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BPãit can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility."
2199,13,http://jmlr.csail.mit.edu/proceedings/papers/v13/,Efficient Collapsed Gibbs Sampling for Latent Dirichlet Allocation,Han Xiao and Thomas Stibor,"13:63-78, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v13/xiao10a/xiao10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v13/,,31st October 2010,"November 8-10, 2010",ACML 2010 Proceedings,Proceedings of 2nd Asian Conference on Machine Learning (ACML2010),"Tokyo, Japan",Masashi Sugiyama and Qiang Yang,v13_xiao10a,http://jmlr.csail.mit.edu/proceedings/papers/v13/xiao10a.html,Collapsed Gibbs sampling is a frequently applied method to approximate intractable integrals in probabilistic generative models such as latent Dirichlet allocation. This sampling method has however the crucial drawback of high computational complexity which makes it limited applicable on large data sets. We propose a novel dynamic sampling strategy to significantly improve the efficiency of collapsed Gibbs sampling. The strategy is explored in terms of efficiency convergence and perplexity. Besides we present a straight-forward parallelization to further improve the efficiency. Finally we underpin our proposed improvements with a comparative study on different scale data sets.
2200,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Second Order Online Collaborative Filtering,"Jing Lu, Steven Hoi, Jialei Wang","29 :325-340, 2013",http://jmlr.org/proceedings/papers/v29/Lu13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Lu13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Lu13.html,"Collaborative Filtering (CF) is one of the most successful learning techniques in building real-world recommender systems. Traditional CF algorithms are often based on batch machine learning methods which suffer from several critical drawbacks, e.g., extremely expensive model retraining cost whenever new samples arrive, unable to capture the latest change of user preferences over time, and high cost and slow reaction to new users or products extension. Such limitations make batch learning based CF methods unsuitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. To address these limitations, we investigate online collaborative filtering techniques for building live recommender systems where the CF model can evolve on-the-fly over time. Unlike the regular first order CF algorithms (e.g., online gradient descent for CF) that converge slowly, in this paper, we present a new framework of second order online collaborative filtering, i.e., Confidence Weighted Online Collaborative Filtering (CWOCF), which applies the second order online optimization methodology to tackle the online collaborative filtering task. We conduct extensive experiments on several large-scale datasets, in which the encouraging results demonstrate that the proposed algorithms obtain significantly lower errors (both RMSE and MAE) than the state-of-the-art first order CF algorithms when receiving the same amount of training data in the online learning process."
2201,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Coco-Q: Learning in Stochastic Games with Side Payments,"Eric Sodomka, Elizabeth Hilliard, Michael Littman, Amy Greenwald",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/sodomka13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_sodomka13,http://jmlr.csail.mit.edu/proceedings/papers/v28/sodomka13.html,"Coco (–”cooperative/competitive–”) values are a solution concept for two-player normal-form games with transferable utility, when binding agreements and side payments between players are possible. In this paper, we show that coco values can also be defined for stochastic games and can be learned using a simple variant of Q-learning that is provably convergent. We provide a set of examples showing how the strategies learned by the Coco-Q algorithm relate to those learned by existing multiagent Q-learning algorithms."
2202,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Stable Coactive Learning via Perturbation,"Karthik Raman, Thorsten Joachims, Pannaga Shivaswamy, Tobias Schnabel",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/raman13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/raman13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_raman13,http://jmlr.csail.mit.edu/proceedings/papers/v28/raman13.html,"Coactive Learning is a model of interaction between a learning system (e.g. search engine) and its human users, wherein the system learns from (typically implicit) user feedback during operational use. User feedback takes the form of preferences, and recent work has introduced online algorithms that learn from this weak feedback. However, we show that these algorithms can be unstable and ineffective in real-world settings where biases and noise in the feedback are significant. In this paper, we propose the first coactive learning algorithm that can learn robustly despite bias and noise. In particular, we explore how presenting users with slightly perturbed objects (e.g., rankings) can stabilize the learning process. We theoretically validate the algorithm by proving bounds on the average regret. We also provide extensive empirical evidence on benchmarks and from a live search engine user study, showing that the new algorithm substantially outperforms existing methods."
2203,43,http://jmlr.csail.mit.edu/proceedings/papers/v43/,Coactive Learning for Interactive Machine Translation,"Artem Sokolov, Stefan Riezler, Shay B. Cohen",none,http://jmlr.csail.mit.edu/proceedings/papers/v43/sokolov15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v43/,,18th June 2015,"Jul 11, 2015 - Jul 11, 2015",MLIS 2015 Proceedings, ICML Workshop on Machine Learning for Interactive Systems ,"Lille, France","Heriberto CuayÕçhuitl, Nina Dethlefs, Lutz Frommberger, Martijn Van Otterlo, Olivier Pietquin",v43_sokolov15,http://jmlr.csail.mit.edu/proceedings/papers/v43/sokolov15.html,"Coactive learning describes the interaction between an online structured learner and a human user who corrects the learner by responding with weak feedback, that is, with an improved, but not necessarily optimal, structure. We apply this framework to discriminative learning in interactive machine translation. We present a generalization to latent variable models and give regret and generalization bounds for online learning with a feedback-based latent perceptron. We show experimentally that learning from weak feedback in machine translation leads to convergence in regret and translation error."
2204,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,Adaptive Density Level Set Clustering,Ingo Steinwart,"19:703-738, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/steinwart11a/steinwart11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_steinwart11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/steinwart11a.html,Clusters are often defined to be the connected components of a density level set.Unfortunately this definition depends on a level that needs to be user specifiedby some means. In this paper we present a simple algorithm that is able to asymptotically determinethe optimal level that is the level at which there is the first split in the cluster treeof the data generating distribution. We further show that this algorithm asymptotically recoversthe corresponding connected components. Unlike previous work our analysis does not require strong assumptions on the density such as continuity or even smoothness.
2205,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Shared Execution of Clustering Tasks,"Padmashree Ravindra, Rajeev Gupta, Kemafor Anyanwu",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/ravindra15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_ravindra15,http://jmlr.csail.mit.edu/proceedings/papers/v41/ravindra15.html,"Clustering is a central problem in non-relational data analysis, with \(k\) -means being the most popular clustering technique. In various scenarios, it may be necessary to perform clustering over the same input data multiple times _ with different values of \(k\) , different clustering attributes, or different initial centroids _ before arriving at the final solution. In this paper, we propose algorithms for parallel execution of multiple runs of \(k\) -means clustering in a way that achieves substantial savings of IO and processing resources. Proposed algorithms can easily be implemented over Hadoop/MapReduce, Spark, etc., with savings in map and reduce phases. Extensive performance evaluation using real-world datasets show that the proposed algorithms result in up to 40% savings in response times when compared to other optimization techniques proposed in literature as well as open-source implementations. The algorithms scale well with increasing data sizes, values of \(k\) , and number of clustering tasks."
2206,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Near-Optimal Evasion of Convex-Inducing Classifiers,"Blaine Nelson, Benjamin Rubinstein, Ling Huang, Anthony Joseph, Shing_hon Lau, Steven Lee, Satish Rao, Anthony Tran, Doug Tygar","9:549-556, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/nelson10a/nelson10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_nelson10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/nelson10a.html,Classifiers are often used to detect miscreant activities. We study how an adversary can efficiently query a classifier to elicit information that allows the adversary to evade detection at near-minimal cost. We generalize results of Lowd and Meek (2005) to convex-inducing classifiers. We present algorithms that construct undetected instances of near-minimal cost using only polynomially many queries in the dimension of the space and without reverse engineering the decision boundary.
2207,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Unsupervised Aggregation for Classification Problems with Large Numbers of Categories,"Ivan Titov, Alexandre Klementiev, Kevin Small, Dan Roth","9:836-843, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/titov10a/titov10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_titov10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/titov10a.html,Classification problems with a very large or unbounded set of output categories are common in many areas such as natural language and image processing. In order to improve accuracy on these tasks it is natural for a decision-maker to combine predictions from various sources. However supervised data needed to fit an aggregation model is often difficult to obtain especially if needed for multiple domains. Therefore we propose a generative model for unsupervised aggregation which exploits the agreement signal to estimate the expertise of individual judges. Due to the large output space size this aggregation model cannot encode expertise of constituent judges with respect to every category for all problems. Consequently we extend it by incorporating the notion of category types to account for variability of the judge expertise depending on the type. The viability of our approach is demonstrated both on synthetic experiments and on a practical task of syntactic parser aggregation.
2208,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Randomized Nonlinear Component Analysis,"David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Ghahramani, Bernhard Schoelkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lopez-paz14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lopez-paz14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lopez-paz14.html,"Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale. In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements. In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. A simple R implementation of the presented algorithms is provided."
2209,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance,"Aditya Menon, Harikrishna Narasimhan, Shivani Agarwal, Sanjay Chawla",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13a-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_menon13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13a.html,"Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems."
2210,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Avoiding pathologies in very deep networks,"David Duvenaud, Oren Rippel, Ryan Adams, Zoubin Ghahramani","33 :202-210, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/duvenaud14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_duvenaud14,http://jmlr.csail.mit.edu/proceedings/papers/v33/duvenaud14.html,"Choosing appropriate architectures and regularization strategies of deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes."
2211,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Autonomous Experimentation,"C. Lovell, G. Jones, S.R. Gunn & K.-P. Zauner ; 16:141_155, 2011. [ abs ] [ pdf ]",2011,http://jmlr.csail.mit.edu/proceedings/papers/v16/lovel11a/lovel11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_lovel11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/lovel11a.html,Characterising response behaviours of biological systems is impaired by limited resources that restrict the exploration of high dimensional parameter spaces. Additionally experimental errors that provide observations not representative of the true underlying behaviour mean that observations obtained from these experiments cannot be regarded as always valid. To combat the problem of erroneous observations in situations where there are limited observations available to learn from we consider the use of multiple hypotheses where potentially erroneous observations are considered as being erroneous and valid in parallel by competing hypotheses. Here we describe work towards an autonomous experimentation machine that combines active learning techniques with computer controlled experimentation platforms to perform physical experiments. Whilst the target for our approach is the characterisation of the behaviours of networks of enzymes for novel computing mechanisms the algorithms we are working towards remain independent of the application domain.   Page last modified on Wed Mar 30 11:10:23 2011.
2212,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Adaptive Stream Clustering Using Incremental Graph Maintenance,"Marwan Hassani, Pascal Spaus, Alfredo Cuzzocrea, Thomas Seidl",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/hassani15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_hassani15,http://jmlr.csail.mit.edu/proceedings/papers/v41/hassani15.html,"Challenges for clustering streaming data are getting continuously more sophisticated. This trend is driven by the the emerging requirements of the application where those algorithms are used and the properties of the stream itself. Some of these properties are the continuous data arrival, the time-critical processing of objects, the evolution of the data streams, the presence of outliers and the varying densities of the data. Due to the fact that the stream evolves continuously in the process of its existence, it is crucial that the algorithm autonomously detects clusters of arbitrary shape, with different densities, and varying number of clusters. Recently, the first hierarchical density-based stream clustering algorithm based on cluster stability, called HASTREAM, was proposed. Although the algorithm was able to meet the above mentioned requirements, it inherited the main drawback of density-based hierarchical clustering algorithms, namely the efficiency issues. In this paper we propose I-HASTREAM , a first density-based hierarchical clustering algorithm that has considerably less computational time than HASTREAM. Our proposed method utilizes and introduces techniques from the graph theory domain to devise an incremental update of the underlying model instead of repeatedly performing the expensive calculations of the huge graph. Specifically the Primês algorithm for constructing the minimal spanning tree is adopted by introducing novel, incremental maintenance of the tree by vertex and edge insertion and deletion. The extensive experimental evaluation study on real world datasets shows that I-HASTREAM is considerably faster than a state-of-the-art hierarchical density-based stream clustering approach while delivering almost the same clustering quality."
2213,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,A Clustering Algorithm Merging MCMC and EM Methods Using SQL Queries,"David Matusevich, Carlos Ordonez","36 :61-76, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/matusevich14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_matusevich14,http://jmlr.csail.mit.edu/proceedings/papers/v36/matusevich14.html,"CClustering is an important problem in Statistics and Machine Learning that is usually solved using Likelihood Maximization Methods, of which the Expectation-Maximization Algorithm (EM) is the most common. In this work we present an SQL implementation of an algorithm merging Markov Chain Monte Carlo methods with the EM algorithm to find qualitatively better solutions for the clustering problem. Even though SQL is not optimized for complex calculations, as it is constrained to work on tables and columns, it is unparalleled in handling all aspects of storage management, security of the information, fault management, etc. Our algorithm makes use of these characteristics to produce portable solutions that are comparable to the results obtained by other algorithms and are more efficient since the calculations are all performed inside the DBMS. To simplify the calculation we use very simple scalar UDFs, of a type that is available in most DBMS. The solution has linear time complexity on the size of the data set and it has a linear speedup with the number of servers in the cluster. This was achieved using sufficient statistics and a simplified model that assigns the data-points to different clusters during the E-step in an incremental manner and the introduction of a Sampling step in order to explore the solution space in a more efficient manner. Preliminary experiments show very good agreement with standard solutions."
2214,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,SADA: A General Framework to Support Robust Causation Discovery,"Ruichu Cai, Zhenjie Zhang, Zhifeng Hao",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/cai13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_cai13,http://jmlr.csail.mit.edu/proceedings/papers/v28/cai13.html,"Causality discovery without manipulation is considered a crucial problem to a variety of applications, such as genetic therapy. The state-of-the-art solutions, e.g. LiNGAM, return accurate results when the number of labeled samples is larger than the number of variables. These approaches are thus applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge strategy, named SADA, to enhance the scalability of a wide class of causality discovery algorithms. SADA is able to accurately identify the causal variables, even when the sample size is significantly smaller than the number of variables. In SADA, the variables are partitioned into subsets, by finding cuts on the sparse probabilistic graphical model over the variables. By running mainstream causation discovery algorithms, e.g. LiNGAM, on the subproblems, complete causality can be reconstructed by combining all the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scale of problems without significant sacrifice on result accuracy, depending only on the local sparsity condition over the variables. Experiments on real-world datasets verify the improvements on scalability and accuracy by applying SADA on top of existing causation algorithms."
2215,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,DYNACARE: Dynamic Cardiac Arrest Risk Estimation,"Joyce Ho, Yubin Park, Carlos Carvalho, Joydeep Ghosh",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/ho13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_ho13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/ho13b.html,"Cardiac arrest is a deadly condition caused by a sudden failure of the heart with an in-hospital mortality rate of \(\sim 80\%\) . Therefore, the ability to accurately estimate patients at high risk of cardiac arrest is crucial for improving the survival rate. Existing research generally fails to utilize a patientês temporal dynamics. In this paper, we present two dynamic cardiac risk estimation models, focusing on different temporal signatures in a patientês risk trajectory. These models can track a patientês risk trajectory in real time, allow interpretability and predictability of a cardiac arrest event, provide an intuitive visualization to medical professionals, offer a personalized dynamic hazard function, and estimate the risk for a new patient."
2216,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Parameter Estimation of Generalized Linear Models without Assuming their Link Function,"Sreangsu Acharyya, Joydeep Ghosh",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/acharyya15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_acharyya15,http://jmlr.csail.mit.edu/proceedings/papers/v38/acharyya15.html,"Canonical generalized linear models (GLM) are completely specified by a finite dimensional vector and a monotonically increasing function called the link function. Standard parameter estimation techniques hold the link function fixed and optimizes over the parameter vector. We propose a parameter-recovery facilitating, jointly-convex, regularized loss functional that is optimized globally over the vector as well as the link function, with best rates possible under a first order oracle model. This widens the scope of GLMs to cases where the link function is unknown."
2217,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis,"Zhuang Ma, Yichao Lu, Dean Foster",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/maa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/maa15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_maa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/maa15.html,"Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient Augmented Approximate Gradient (AppGrad) scheme for finding top \(k\) dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width \(k\) and small matrix decomposition of dimension \(k\times k\) . Further, AppGrad achieves optimal storage complexity \(O(k(p_1+p_2))\) , compared with classical algorithms which usually require \(O(p_1^2+p_2^2)\) space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic AppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods."
2218,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment,"Billy Chang, Uwe Kruger, Rafal Kustra, Junping Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chang13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chang13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chang13,http://jmlr.csail.mit.edu/proceedings/papers/v28/chang13.html,"Canonical correlation analysis (CCA) is a well established technique for identifying linear relationships among two variable sets. Kernel CCA (KCCA) is the most notable nonlinear extension but it lacks interpretability and robustness against irrelevant features. The aim of this article is to introduce two nonlinear CCA extensions that rely on the recently proposed Hilbert-Schmidt independence criterion and the centered kernel target alignment. These extensions determine linear projections that provide maximally dependent projected data pairs. The paper demonstrates that the use of linear projections allows removing irrelevant features, whilst extracting combinations of strongly associated features. This is exemplified through a simulation and the analysis of recorded data that are available in the literature."
2219,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget,"Anoop Korattikara, Yutian Chen, Max Welling",none,http://jmlr.org/proceedings/papers/v32/korattikara14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/korattikara14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_korattikara14,http://jmlr.csail.mit.edu/proceedings/papers/v32/korattikara14.html,"Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time."
2220,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Influence Function Learning in Information Diffusion Networks,"Nan Du, Yingyu Liang, Maria Balcan, Le Song",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/du14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/du14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_du14,http://jmlr.csail.mit.edu/proceedings/papers/v32/du14.html,"Can we learn the influence of a set of people in a social network from cascades of information diffusion? This question is often addressed by a two-stage approach: first learn a diffusion model, and then calculate the influence based on the learned model. Thus, the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data. In this paper, we exploit the insight that the influence functions in many diffusion models are coverage functions, and propose a novel parameterization of such functions using a convex combination of random basis functions. Moreover, we propose an efficient maximum likelihood based algorithm to learn such functions directly from cascade data, and hence bypass the need to specify a particular diffusion model in advance. We provide both theoretical and empirical analysis for our approach, showing that the proposed approach can provably learn the influence function with low sample complexity, be robust to the unknown diffusion models, and significantly outperform existing approaches in both synthetic and real world data."
2221,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Learning Fourier Sparse Set Functions,"Peter Stobbe, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/stobbe12/stobbe12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_stobbe12,http://jmlr.csail.mit.edu/proceedings/papers/v22/stobbe12.html,Can we learn a sparse graph from observing the value of a few random cuts? This and more general problems can be reduced to the challenge of learning set functions known to have sparse Fourier support contained in some collection. We prove that if we choose O(k log^4 n) sets uniformly at random then with high probability observing any k-sparse function on those sets is sufficient to recover that function exactly. We further show that other properties such as symmetry or submodularity imply structure in the Fourier spectrum which can be exploited to further reduce sample complexity. One interesting special case is that it suffices to observe O(|E| log^4 |V|) values of a cut function to recover a graph. We demonstrate the effectiveness of our results on two real-world reconstruction problems: graph sketching and obtaining fast approximate surrogates to expensive submodular objective functions.
2222,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization,"Tyler Johnson, Carlos Guestrin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/johnson15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/johnson15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_johnson15,http://jmlr.csail.mit.edu/proceedings/papers/v37/johnson15.html,"By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints."
2223,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Latent Variable Copula Inference for Bundle Pricing from Retail Transaction Data,"Benjamin Letham, Wei Sun, Anshul Sheopuri",none,http://jmlr.org/proceedings/papers/v32/letham14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_letham14,http://jmlr.csail.mit.edu/proceedings/papers/v32/letham14.html,"Bundle discounts are used by retailers in many industries. Optimal bundle pricing requires learning the joint distribution of consumer valuations for the items in the bundle, that is, how much they are willing to pay for each of the items. We suppose that a retailer has sales transaction data, and the corresponding consumer valuations are latent variables. We develop a statistically consistent and computationally tractable inference procedure for fitting a copula model over correlated valuations, using only sales transaction data for the individual items. Simulations and data experiments demonstrate consistency, scalability, and the importance of incorporating correlations in the joint distribution."
2224,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Multitask Learning for Brain-Computer Interfaces,"Morteza Alamgir, Moritz Grosse_Wentrup, Yasemin Altun","9:17-24, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/alamgir10a/alamgir10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_alamgir10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/alamgir10a.html,Brain-computer interfaces (BCIs) are limited in their applicability in everyday settings by the current necessity to record subject-specific calibration data prior to actual use of the BCI for communication. In this paper we utilize the framework of multitask learning to construct a BCI that can be used without any subject-specific calibration process. We discuss how this out-of-the-box BCI can be further improved in a computationally efficient manner as subject-specific data becomes available. The feasibility of the approach is demonstrated on two sets of experimental EEG data recorded during a standard two-class motor imagery paradigm from a total of 19 healthy subjects. Specifically we show that satisfactory classification results can be achieved with zero training data and combining prior recordings with subject-specific calibration data substantially outperforms using subject-specific data only. Our results further show that transfer between recordings under slightly different experimental setups is feasible.
2225,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Bayesian Max-margin Multi-Task Learning with Data Augmentation,"Chengtao Li, Jun Zhu, Jianfei Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/lic14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_lic14,http://jmlr.csail.mit.edu/proceedings/papers/v32/lic14.html,"Both max-margin and Bayesian methods have been extensively studied in multi-task learning, but have rarely been considered together. We present Bayesian max-margin multi-task learning, which conjoins the two schools of methods, thus allowing the discriminative max-margin methods to enjoy the great flexibility of Bayesian methods on incorporating rich prior information as well as performing nonparametric Bayesian feature learning with the latent dimensionality resolved from data. We develop Gibbs sampling algorithms by exploring data augmentation to deal with the non-smooth hinge loss. For nonparametric models, our algorithms do not need to make mean-field assumptions or truncated approximation. Empirical results demonstrate superior performance than competitors in both multi-task classification and regression."
2226,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Open Problem: Lower bounds for Boosting with Hadamard Matrices,"Jiazhong Nie, Manfred K. Warmuth, S.V.N. Vishwanathan, Xinhua Zhang",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Nie13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Nie13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Nie13.html,"Boosting algorithms can be viewed as a zero-sum game. At each iteration a new column / hypothesis is chosen from a game matrix representing the entire hypotheses class. There are algorithms for which the gap between the value of the sub-matrix (the \(t\) columns chosen so far) and the value of the entire game matrix is \(O(\sqrt{\frac{\log n}{t}})\) . A matching lower bound has been shown for random game matrices for \(t\) up to \(n^\alpha\) where \(\alpha \in (0,\frac{1}{2})\) . We conjecture that with Hadamard matrices we can build a certain game matrix for which the game value grows at the slowest possible rate for \(t\) up to a fraction of \(n\) ."
2227,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Quickly Boosting Decision Trees _ Pruning Underachieving Features Early,"Ron Appel, Thomas Fuchs, Piotr Dollar, Pietro Perona",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/appel13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/appel13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_appel13,http://jmlr.csail.mit.edu/proceedings/papers/v28/appel13.html,"Boosted decision trees are one of the most popular and successful learning techniques used today. While exhibiting fast speeds at test time, relatively slow training makes them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early on in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it may be used in conjunction with existing Boosting algorithms and other sampling heuristics to achieve even greater speedups."
2228,29,http://jmlr.csail.mit.edu/proceedings/papers/v29/,Active Sampling of Pairs and Points for Large-scale Linear Bipartite Ranking,"Wei-Yuan Shen, Hsuan-Tien Lin","29 :388-403, 2013",http://jmlr.org/proceedings/papers/v29/Shen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v29/,,21st October 2013,"November 13-15, 2013",ACML 2013,Asian Conference on Machine Learning 2013,"Canberra,  Australia","Ong, Cheng Soon and Ho, Tu Bao",v29_Shen13,http://jmlr.csail.mit.edu/proceedings/papers/v29/Shen13.html,"Bipartite ranking is a fundamental ranking problem that learns to order relevant instances ahead of irrelevant ones. One major approach for bipartite ranking, called the pair-wise approach, tackles an equivalent binary classification problem of whether one instance out of a pair of instances should be ranked higher than the other. Nevertheless, the number of instance pairs constructed from the input data could be quadratic to the size of the input data, which makes pair-wise ranking generally infeasible on large-scale data sets. Another major approach for bipartite ranking, called the point-wise approach, directly solves a binary classification problem between relevant and irrelevant instance points. This approach is feasible for large-scale data sets, but the resulting ranking performance can be inferior. That is, it is difficult to conduct bipartite ranking accurately and efficiently at the same time. In this paper, we develop a novel scheme within the pair-wise approach to conduct bipartite ranking efficiently. The scheme, called Active Sampling, is inspired from the rich field of active learning and can reach a competitive ranking performance while focusing only on a small subset of the many pairs during training. Moreover, we propose a general Combined Ranking and Classification (CRC) framework to accurately conduct bipartite ranking. The framework unifies point-wise and pair-wise approaches and is simply based on the idea of treating each instance point as a pseudo-pair. Experiments on 14 real- word large-scale data sets demonstrate that the proposed algorithm of Active Sampling within CRC, when coupled with a linear Support Vector Machine, usually outperforms state-of-the-art point-wise and pair-wise ranking approaches in terms of both accuracy and efficiency."
2229,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Circulant Binary Embedding,"Felix Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/yub14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yub14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yub14.html,"Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from \(\mathcal{O}(d^2)\) to \(\mathcal{O}(d\log{d})\) , and the space complexity from \(\mathcal{O}(d^2)\) to \(\mathcal{O}(d)\) where \(d\) is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits."
2230,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Binary Embedding: Fundamental Limits and Fast Algorithm,"Xinyang Yi, Constantine Caramanis, Eric Price",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/yi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/yi15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_yi15,http://jmlr.csail.mit.edu/proceedings/papers/v37/yi15.html,"Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary \(N\) distinct points in \(\mathbb{S}^{p-1}\) , our goal is to encode each point using \(m\) -dimensional binary strings such that we can reconstruct their geodesic distance up to \(\delta\) uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time \(O(mp)\) . We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires \(m =\Omega(\frac{1}{\delta^2}\log{N})\) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity \(m = O(\frac{1} {\delta^2}\log{N})\) and near linear running time \(O(p \log p)\) whenever \(\log N \ll \delta \sqrt{p}\) , with a slightly worse running time for larger \(\log N\) ; (3) we also provide an analytic result about embedding a general set of points \(K \subseteq \mathbb{S}^{p-1}\) with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets."
2231,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Variational Gaussian Inference for Bilinear Models of Count Data,"Young-Jun Ko, Mohammad Khan",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/ko14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_ko14,http://jmlr.csail.mit.edu/proceedings/papers/v39/ko14.html,"Bilinear models of count data with Poisson distribution are popular in applications such as matrix factorization for recommendation systems, modeling of receptive fields of sensory neurons, and modeling of neural-spike trains. Bayesian inference in such models remains challenging due to the product term of two Gaussian random vectors. In this paper, we propose new algorithms for such models based on variational Gaussian (VG) inference. We make two contributions. First, we show that the VG lower bound for these models, previously known to be intractable, is available in closed form under certain non-trivial constraints on the form of the posterior. Second, we show that the lower bound is bi- concave and can be efficiently optimized for mean-field approximations. We also show that bi-concavity generalizes to the larger family of log-concave likelihoods that subsume the Poisson distribution. We present new inference algorithms based on these results and demonstrate better performance on real-world problems at the cost of a modest increase in computation. Our contributions in this paper, therefore, provide more choices for Bayesian inference in terms of a speed-vs-accuracy tradeoff."
2232,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Context-based Unsupervised Data Fusion for Decision Making,"Erfan Soltanmohammadi, Mort Naraghi-Pour, Mihaela van der Schaar",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/soltanmohammadi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_soltanmohammadi15,http://jmlr.csail.mit.edu/proceedings/papers/v37/soltanmohammadi15.html,"Big Data received from sources such as social media, in-stream monitoring systems, networks, and markets is often mined for discovering patterns, detecting anomalies, and making decisions or predictions. In distributed learning and real-time processing of Big Data, ensemble-based systems in which a fusion center (FC) is used to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC requires knowledge of the accuracy of the individual classifiers which, in many cases, is not available. Moreover, in many applications supervised training of the FC is not feasible since the true labels of the data set are not available. In this paper, we propose an unsupervised joint estimation-detection scheme to estimate the accuracies of the local classifiers as functions of data context and to fuse the local decisions of the classifiers. Numerical results show the dramatic improvement of the proposed method as compared with the state of the art approaches."
2233,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Precision-recall space to correct external indices for biclustering,"Blaise Hanczar, Mohamed Nadif",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/hanczar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_hanczar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/hanczar13.html,"Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach."
2234,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Bibliographic Analysis with the Citation Network Topic Model,"Kar Wai Lim, Wray Buntine",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/lim14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_lim14,http://jmlr.csail.mit.edu/proceedings/papers/v39/lim14.html,"Bibliographic analysis considers authorês research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and efficient inference algorithm for the model to explore subsets of research publications from CiteSeerX. Our model demonstrates improved performance in both model fitting and clustering task comparing to several baselines."
2235,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,A Computationally Efficient Method for Estimating Semi Parametric Regression Functions,"Xia Cui, Ying Lu, Heng Peng",none,http://jmlr.csail.mit.edu/proceedings/papers/v44/CuiLuPeng15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_CuiLuPeng15,http://jmlr.csail.mit.edu/proceedings/papers/v44/CuiLuPeng15.html,"Bias reduction is an important condition for effective feature extraction. Utilizing recent theoretical results in high dimensional statistical modeling, we propose a model-free yet computationally simple approach to estimate the partially linear model \(Y=X\beta+g(Z)+\varepsilon\) . Based on partitioning the support of \(Z\) , a simple local average is used to approximate the response surface \(g(Z)\) . The model can be estimated via least squares and no tuning parameter is needed. The proposed method seeks to strike a balance between computation burden and efficiency of the estimators while minimizing model bias. The desired theoretical properties of the proposed estimators are established. Moreover, since the proposed method bypasses data-driven bandwith selection of traditional nonparametric methods, it avoids the further efficiency loss due to computation burden."
2236,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Simple Exponential Family PCA,"Jun Li, Dacheng Tao","9:453-460, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_li10b,http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b.html,Bayesian principal component analysis (BPCA) a probabilistic reformulation of PCA with Bayesian model selection is a systematic approach to determining the number of essential principal components (PCs) for data representation. However it assumes that data are Gaussian distributed and thus it cannot handle all types of practical observations e.g. integers and binary values. In this paper we propose simple exponential family PCA (SePCA) a generalised family of probabilistic principal component analysers. SePCA employs exponential family distributions to handle general types of observations. By using Bayesian inference SePCA also automatically discovers the number of essential PCs. We discuss techniques for fitting the model develop the corresponding mixture model and show the effectiveness of the model based on experiments.
2237,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations,"David Barber, Yali Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/barber14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_barber14,http://jmlr.csail.mit.edu/proceedings/papers/v32/barber14.html,"Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient can be calculated easily. Parameter estimation is achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations, simplifying previous approaches and providing a natural generative model."
2238,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scalable Bayesian Optimization Using Deep Neural Networks,"Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/snoek15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/snoek15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_snoek15,http://jmlr.csail.mit.edu/proceedings/papers/v37/snoek15.html,"Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models."
2239,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Differentially Private Bayesian Optimization,"Matt Kusner, Jacob Gardner, Roman Garnett, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kusnera15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kusnera15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kusnera15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kusnera15.html,"Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are often near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy."
2240,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bayesian Multi-Scale Optimistic Optimization,"Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas","33 :1005-1014, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14d.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14d-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wang14d,http://jmlr.csail.mit.edu/proceedings/papers/v33/wang14d.html,"Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks, as well as a novel application to automate information extraction, demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates."
2241,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Bayesian Optimization with Inequality Constraints,"Jacob Gardner, Matt Kusner, Zhixiang, Kilian Weinberger, John Cunningham",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/gardner14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/gardner14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_gardner14,http://jmlr.csail.mit.edu/proceedings/papers/v32/gardner14.html,"Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations. It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design. However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective. Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions. We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail."
2242,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Input Warping for Bayesian Optimization of Non-Stationary Functions,"Jasper Snoek, Kevin Swersky, Rich Zemel, Ryan Adams",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/snoek14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_snoek14,http://jmlr.csail.mit.edu/proceedings/papers/v32/snoek14.html,"Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in –log-space”, to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably."
2243,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,High Dimensional Bayesian Optimisation and Bandits via Additive Models,"Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kandasamy15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kandasamy15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kandasamy15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kandasamy15.html,"Bayesian Optimisation (BO) is a technique used in optimising a \(D\) -dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on \(D\) even though the function depends on all \(D\) dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive."
2244,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Streaming Variational Inference for Dirichlet Process Mixtures,"Viet Huynh, Dinh Phung, Svetha Venkatesh",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Huynh15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Huynh15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Huynh15.html,"Bayesian nonparametric models are theoretically suitable to learn streaming data due to their complexity relaxation to the volume of observed data. However, most of the existing variational inference algorithms are not applicable to streaming applications since they require truncation on variational distributions. In this paper, we present two truncation-free variational algorithms, one for mix-membership inference called TFVB (truncation-free variational Bayes), and the other for hard clustering inference called TFME (truncation-free maximization expectation). With these algorithms, we further developed a streaming learning framework for the popular Dirichlet process mixture (DPM) models. Our experiments demonstrate the usefulness of our framework in both synthetic and real-world data."
2245,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Distributed Inference for Dirichlet Process Mixture Models,"Hong Ge, Yutian Chen, Moquan Wan, Zoubin Ghahramani",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gea15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gea15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gea15.html,"Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores."
2246,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Curriculum Learning of Bayesian Network Structures,"Yanpeng Zhao, Yetian Chen, Kewei Tu, Jin Tian",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhao15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Zhao15a,http://jmlr.csail.mit.edu/proceedings/papers/v45/Zhao15a.html,"Bayesian networks (BNs) are directed graphical models that have been widely used in various tasks for probabilistic reasoning and causal modeling. One major challenge in these tasks is to learn the BN structures from data. In this paper, we propose a novel heuristic algorithm for BN structure learning that takes advantage of the idea of curriculum learning . Our algorithm learns the BN structure by stages. At each stage a subnet is learned over a selected subset of the random variables conditioned on fixed values of the rest of the variables. The selected subset grows with stages and eventually includes all the variables. We prove theoretical advantages of our algorithm and also empirically show that it outperformed the state-of-the-art heuristic approach in learning BN structures."
2247,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Random Bayesian networks with bounded indegree,"Eunice Yuh-Jie Chen, Judea Pearl","33 :114-121, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/chen14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_chen14a,http://jmlr.csail.mit.edu/proceedings/papers/v33/chen14a.html,"Bayesian networks (BN) are an extensively used graphical model for representing a probability distribution in artificial intelligence, data mining, and machine learning. In this paper, we propose a simple model for large random BNs with bounded indegree, that is, large directed acyclic graphs (DAG) where the edges appear at random and each node has at most a given number of parents. Using this model, we can study useful asymptotic properties of large BNs and BN algorithms with basic combinatorics tools. We estimate the expected size of a BN, the expected size increase of moralization, the expected size of the Markov blanket, and the maximum size of a minimal d-separator. We also provide an upper bound on the average time complexity of an algorithm for finding a minimal d-separator. In addition, the estimates are evaluated against BNs learned from real world data."
2248,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Learning Optimal Bounded Treewidth Bayesian Networks via Maximum Satisfiability,"Jeremias Berg, Matti J_rvisalo, Brandon Malone","33 :86-95, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/berg14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_berg14,http://jmlr.csail.mit.edu/proceedings/papers/v33/berg14.html,"Bayesian network structure learning is the well-known computationally hard problem of finding a directed acyclic graph structure that optimally describes given data. A learned structure can then be used for probabilistic inference. While exact inference in Bayesian networks is in general NP-hard, it is tractable in networks with low treewidth. This provides good motivations for developing algorithms for the NP-hard problem of learning optimal bounded treewidth Bayesian networks (BTW-BNSL). In this work, we develop a novel score-based approach to BTW-BNSL, based on casting BTW-BNSL as weighted partial Maximum satisfiability. We demonstrate empirically that the approach scales notably better than a recent exact dynamic programming algorithm for BTW-BNSL."
2249,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Stochastic Variational Inference for Bayesian Time Series Models,"Matthew Johnson, Alan Willsky",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/johnson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_johnson14,http://jmlr.csail.mit.edu/proceedings/papers/v32/johnson14.html,"Bayesian models provide powerful tools for analyzing complex time series data, but performing inference with large datasets is a challenge. Stochastic variational inference (SVI) provides a new framework for approximating model posteriors with only a small number of passes through the data, enabling such models to be fit at scale. However, its application to time series models has not been studied. In this paper we develop SVI algorithms for several common Bayesian time series models, namely the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and the nonparametric HDP-HMM and HDP-HSMM. In addition, because HSMM inference can be expensive even in the minibatch setting of SVI, we develop fast approximate updates for HSMMs with durations distributions that are negative binomials or mixtures of negative binomials."
2250,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Evaluation of marginal likelihoods via the density of states,Michael Habeck,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/habeck12/habeck12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_habeck12,http://jmlr.csail.mit.edu/proceedings/papers/v22/habeck12.html,Bayesian model comparison involves the evaluation of the marginal likelihood the expectation of the likelihood under the prior distribution. Typically this high-dimensional integral over all model parameters is approximated using Markov chain Monte Carlo methods. Thermodynamic integration is a popular method to estimate the marginal likelihood by using samples from annealed posteriors. Here we show that there exists a robust and flexible alternative. The new method estimates the density of states which counts the number of states associated with a particular value of the likelihood. If the density of states is known computation of the marginal likelihood reduces to a one- dimensional integral. We outline a maximum likelihood procedure to estimate the density of states from annealed posterior samples. We apply our method to various likelihoods and show that it is superior to thermodynamic integration in that it is more flexible with regard to the annealing schedule and the family of bridging distributions. Finally we discuss the relation of our method with Skilling's nested sampling.
2251,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Bayesian Wilcoxon signed-rank test based on the Dirichlet process,"Alessio Benavoli, Giorgio Corani, Francesca Mangili, Marco Zaffalon, Fabrizio Ruggeri",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/benavoli14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/benavoli14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_benavoli14,http://jmlr.csail.mit.edu/proceedings/papers/v32/benavoli14.html,"Bayesian methods are ubiquitous in machine learning. Nevertheless, the analysis of empirical results is typically performed by frequentist tests. This implies dealing with null hypothesis significance tests and p-values, even though the shortcomings of such methods are well known. We propose a nonparametric Bayesian version of the Wilcoxon signed-rank test using a Dirichlet process (DP) based prior. We address in two different ways the problem of how to choose the infinite dimensional parameter that characterizes the DP. The proposed test has all the traditional strengths of the Bayesian approach; for instance, unlike the frequentist tests, it allows verifying the null hypothesis, not only rejecting it, and taking decision which minimize the expected loss. Moreover, one of the solutions proposed to model the infinitedimensional parameter of the DP, allows isolating instances in which the traditional frequentist test is guessing at random. We show results dealing with the comparison of two classifiers using real and simulated data."
2252,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Scalable Variational Bayesian Matrix Factorization with Side Information,"Yong-Deok Kim, Seungjin Choi","JMLR W&CP 33 :493-502, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/kim14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_kim14b,http://jmlr.csail.mit.edu/proceedings/papers/v33/kim14b.html,"Bayesian matrix factorization (BMF) is a popular method for collaborative prediction, because of its robustness to overfitting as well as of being free from cross-validation for fine tuning of regularization parameters. In practice, however, due to its cubic time complexity with respect to the rank of factor matrices, existing variational inference algorithms for BMF are not well suited to web-scale datasets where billions of ratings provided by millions of users are available. The time complexity even increases when the side information, such as user binary implicit feedback or item content information, is incorporated into variational Bayesian matrix factorization (VBMF). For instance, a state of the arts in VBMF with side information, is to place Gaussian priors on user and item factor matrices, where mean of each prior is regressed on the corresponding side information. Since this approach introduces additional cubic time complexity with respect to the size of feature vectors, the use of rich side information in a form of high-dimensional feature vector is prohibited. In this paper, we present a scalable inference for VBMF with side information, the complexity of which is linear in the rank K of factor matrices. Moreover, the algorithm can be easily parallelized on multi-core systems. Experiments on large-scale datasets demonstrate the useful behavior of our algorithm such as scalability, fast learning, and prediction accuracy."
2253,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Message passing with l1 penalized KL minimization,"Yuan Qi, Yandong Guo",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/qi13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/qi13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_qi13,http://jmlr.csail.mit.edu/proceedings/papers/v28/qi13.html,"Bayesian inference is often hampered by large computational expense. As a generalization of belief propagation (BP), expectation propagation (EP) approximates exact Bayesian computation with efficient message passing updates. However, when an approximation family used by EP is far from exact posterior distributions, message passing may lead to poor approximation quality and suffer from divergence. To address this issue, we propose an approximate inference method, relaxed expectation propagation(REP), based on a new divergence with a l1 penalty. Minimizing this penalized divergence adaptively relaxes EPês moment matching requirement for message passing. We apply REP to Gaussian process classification and experimental results demonstrate significant improvement of REP over EP and alpha-divergence based power EP _ in terms of algorithmic stability, estimation accuracy, and predictive performance. Furthermore, we develop relaxed belief propagation(RBP), a special case of REP, to conduct inference on discrete Markov random fields (MRFs). Our results show improved estimation accuracy of RBP over BP and fractional BP when interactions between MRF nodes are strong."
2254,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Polya-gamma augmentations for factor models,Arto Klami,none,http://jmlr.csail.mit.edu/proceedings/papers/v39/klami14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_klami14,http://jmlr.csail.mit.edu/proceedings/papers/v39/klami14.html,"Bayesian inference for latent factor models, such as principal component and canonical correlation analysis, is easy for Gaussian likelihoods. In particular, full conjugacy makes both Gibbs samplers and mean-field variational approximations straightforward. For other likelihood potentials one needs to either resort to more complex sampling schemes or to specifying dedicated forms of variational lower bounds. Recently, however, it was shown that for specific likelihoods related to the logistic function it is possible to augment the joint density with auxiliary variables following a Polya-Gamma distribution, leading to closed-form updates for binary and over-dispersed count models. In this paper we describe how Gibbs sampling and mean-field variational approximation for various latent factor models can be implemented for these cases, presenting easy-to-implement and efficient inference schemas."
2255,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Bayesian Hierarchical Clustering with Exponential Family: Small-Variance Asymptotics and Reducibility,"Juho Lee, Seungjin Choi",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15c-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lee15c,http://jmlr.csail.mit.edu/proceedings/papers/v38/lee15c.html,"Bayesian hierarchical clustering (BHC) is an agglomerative clustering method, where a probabilistic model is defined and its marginal likelihoods are evaluated to decide which clusters to merge. While BHC provides a few advantages over traditional distance-based agglomerative clustering algorithms, successive evaluation of marginal likelihoods and careful hyperparameter tuning are cumbersome and limit the scalability. In this paper we relax BHC into a non-probabilistic formulation, exploring smallvariance asymptotics in conjugate-exponential models. We develop a novel clustering algorithm, referred to as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that exhibits the scalability of distance-based agglomerative clustering algorithms as well as the flexibility of Bayesian nonparametric models. We also investigate the reducibility of the dissimilarity measure emerged from the asymptotic limit of the BHC model, allowing us to use scalable algorithms such as the nearest neighbor chain algorithm. Numerical experiments on both synthetic and real-world datasets demonstrate the validity and high performance of our method."
2256,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Evidence Estimation for Bayesian Partially Observed MRFs,"Yutian Chen, Max Welling",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_chen13c,http://jmlr.csail.mit.edu/proceedings/papers/v31/chen13c.html,"Bayesian estimation in Markov random fields is very hard due to the intractability of the partition function. The introduction of hidden units makes the situation even worse due to the presence of potentially very many modes in the posterior distribution. For the first time we propose a comprehensive procedure to address one of the Bayesian estimation problems, approximating the evidence of partially observed MRFs based on the Laplace approximation. We also introduce a number of approximate MCMC-based methods for comparison but find that the Laplace approximation significantly outperforms these."
2257,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Diagonal Orthant Multinomial Probit Models,"James Johndrow, David Dunson, Kristian Lum",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/johndrow13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_johndrow13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/johndrow13a.html,"Bayesian classification commonly relies on probit models, with data augmentation algorithms used for posterior computation. By imputing latent Gaussian variables, one can often trivially adapt computational approaches used in Gaussian models. However, MCMC for multinomial probit (MNP) models can be inefficient in practice due to high posterior dependence between latent variables and parameters, and to difficulties in efficiently sampling latent variables when there are more than two categories. To address these problems, we propose a new class of diagonal orthant (DO) multinomial models. The key characteristics of these models include conditional independence of the latent variables given model parameters, avoidance of arbitrary identifiability restrictions, and simple expressions for category probabilities. We show substantially improved computational efficiency and comparable predictive performance to MNP."
2258,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,A bottom-up efficient algorithm learning substitutable languages from positive examples,"François Coste, Ga ï lle Garet, Jacques Nicolas","JMLR W&CP 34 :49-63, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/coste14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_coste14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/coste14a.html,"Based on Harrisês substitutability criterion, the recent definitions of classes of substitutable languages have led to interesting polynomial learnability results for expressive formal languages. These classes are also promising for practical applications: in natural language analysis, because definitions have strong linguisitic support, but also in biology for modeling protein families, as suggested in our previous study introducing the class of local substitutable languages. But turning recent theoretical advances into practice badly needs truly practical algorithms. We present here an efficient learning algorithm, motivated by intelligibility and parsing efficiency of the result, which directly reduces the positive sample into a small non-redundant canonical grammar of the target substitutable language. Thanks to this new algorithm, we have been able to extend our experimentation to a complete protein dataset confirming that it is possible to learn grammars on proteins with high specificity and good sensitivity by a generalization based on local substitutability."
2259,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Fast Image Tagging,"Minmin Chen, Alice Zheng, Kilian Weinberger",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13j.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13j,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13j.html,"Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity."
2260,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,On autoencoder scoring,"Hanna Kamyshanska, Roland Memisevic",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/kamyshanska13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_kamyshanska13,http://jmlr.csail.mit.edu/proceedings/papers/v28/kamyshanska13.html,"Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized –score” to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier."
2261,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,One-Pass AUC Optimization,"Wei Gao, Rong Jin, Shenghuo Zhu, Zhi-Hua Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/gao13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_gao13,http://jmlr.csail.mit.edu/proceedings/papers/v28/gao13.html,"AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm."
2262,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Bias Reduction and Metric Learning for Nearest-Neighbor Estimation of Kullback-Leibler Divergence,"Yung-Kyun Noh, Masashi Sugiyama, Song Liu, Marthinus C. du Plessis, Frank Chongwoo Park, Daniel D. Lee","JMLR W&CP 33 :669-677, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/noh14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/noh14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_noh14,http://jmlr.csail.mit.edu/proceedings/papers/v33/noh14.html,"Asymptotically unbiased nearest-neighbor estimators for K-L divergence have recently been proposed and demonstrated in a number of applications. With small sample sizes, however, these nonparametric methods typically suffer from high estimation bias due to the non-local statistics of empirical nearest-neighbor information. In this paper, we show that this non-local bias can be mitigated by changing the distance metric, and we propose a method for learning an optimal Mahalanobis-type metric based on global information provided by approximate parametric models of the underlying densities. In both simulations and experiments, we demonstrate that this interplay between parametric models and nonparametric estimation methods significantly improves the accuracy of the nearest-neighbor K-L divergence estimator."
2263,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Learning a set of directions,"Wouter M. Koolen, Jiazhong Nie, Manfred Warmuth",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Koolen13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Koolen13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Koolen13.html,Assume our data consists of unit vectors (directions) and we are to find a small orthogonal set of the –the most important directions” summarizing the data. We develop online algorithms for this type of problem. The techniques used are similar to Principal Component Analysis which finds the most important small rank subspace of the data.The new problem is significantly more complex since the online algorithm maintains uncertainty over the most relevant subspace as well as directional information.
2264,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Guarantees for Approximate Incremental SVMs,"Nicolas Usunier, Antoine Bordes, L_on Bottou","9:884-891, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/usunier10a/usunier10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_usunier10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/usunier10a.html,Assume a teacher provides examples one by one. An approximate incremental SVM computes a sequence of classifiers that are close to the true SVM solutions computed on the successive incremental training sets. We show that simple algorithms can satisfy an averaged accuracy criterion with a computational cost that scales as well as the best SVM algorithms with the number of examples. Finally we exhibit some experiments highlighting the benefits of joining fast incremental optimization and curriculum and active learning (Schon and Cohn 2000; Bordes et al. 2005; Bengio et al. 2009).
2265,36,http://jmlr.csail.mit.edu/proceedings/papers/v36/,iPARAS: Incremental Construction of Parameter Space for Online Association Mining,"Xiao Qin, Ramoza Ahsan, Xika Lin, Elke Rundensteiner, Matthew Ward","JMLR W&CP 36 :149-165, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v36/qin14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v36/,,13th August 2014,"Aug 24, 2014 - Aug 24, 2014",BIGMINE 2014 Proceedings,"3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","New York, USA","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v36_qin14,http://jmlr.csail.mit.edu/proceedings/papers/v36/qin14.html,"Association rule mining is known to be computationally intensive, yet real-time decision-making applications are increasingly intolerant to delays. The state-of-the-art PARAS solution, a parameter space framework for online association mining, enables efficient rule mining by compactly indexing the final ruleset and providing efficient query-time redundancy resolution. Unfortunately, as many association mining models, PARAS was designed for static data. Modern transaction databases undergo regular data updates that quickly invalidating existing rules or introducing new rules for the PARAS index. While reloading the PARAS index from scratch is impractical, as even upon minor data changes, a complete rule inference and redundancy resolution steps would have to be performed. We now propose to tackle this open problem by designing an incremental parameter space construction approach, called iPARAS, that utilizes the previous mining result to minimally adjust the ruleset and associated redundancy relationships. iPARAS features two innovative techniques. First, iPARAS provides an end-to-end solution, composed of three algorithms, to efficiently update the final ruleset in the parameter space. Second, iPARAS designs a compact data structure to maintain the complex redundancy relationships. Overall, iPARAS achieves several times speed-up on parameter space construction for transaction databases comparing to the state-of-the-art online association rule mining system PARAS."
2266,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Learning the dependence structure of rare events: a non-asymptotic study,"Nicolas Goix, Anne Sabourin, St_phan Cl_mençon",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Goix15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Goix15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Goix15.html,"Assessing the probability of occurrence of extreme events is a crucial issue in various fields like finance, insurance, telecommunication or environmental sciences. In a multivariate framework, the tail dependence is characterized by the so-called stable tail dependence function ( stdf ). Learning this structure is the keystone of multivariate extremes. Although extensive studies have proved consistency and asymptotic normality for the empirical version of the stdf , non-asymptotic bounds are still missing. The main purpose of this paper is to fill this gap. Taking advantage of adapted VC-type concentration inequalities, upper bounds are derived with expected rate of convergence in \(O(k^{-1/2})\) . The concentration tools involved in this analysis rely on a more general study of maximal deviations in low probability regions, and thus directly apply to the classification of extreme data."
2267,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Posterior distributions are computable from predictive distributions,"Cameron Freer, Daniel Roy","9:233-240, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/freer10a/freer-roy10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_freer10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/freer10a.html,As we devise more complicated prior distributions will inference algorithms keep up? We highlight a negative result in computable probability theory by Ackerman Freer and Roy (2010) that shows that there exist computable priors with noncomputable posteriors. In addition to providing a brief survey of computable probability theory geared towards the A.I. and statistics community we give a new result characterizing when conditioning is computable in the setting of exchangeable sequences and provide a computational perspective on work by Orbanz (2010) on conjugate nonparametric models. In particular using a computable extension of de Finetti's theorem (Freer and Roy 2009) we describe how to transform a posterior predictive rule for generating an exchangeable sequence into an algorithm for computing the posterior distribution of the directing random measure.
2268,10,http://jmlr.csail.mit.edu/proceedings/papers/v10/,"Feature Selection, Association Rules Network and Theory Building",Sanjay Chawla,"10:14-21, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v10/chawla10a/chawla10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v10/,,26th May 2010,2010 Jun 21,FSDM 2010 Proceedings,Proceedings of the Fourth International Workshop on Feature Selection in Data Mining,"Hyderabad, India","Huan Liu, Hiroshi Motoda, Rudy Setiono, Zheng Zhao",v10_chawla10a,http://jmlr.csail.mit.edu/proceedings/papers/v10/chawla10a.html,As the size and dimensionality of data sets increase the task of feature selection has become increasingly important. In this paper we demonstrate how association rules can be used to build a network of features which we refer to as an association rules network to extract features from large data sets. Association rules network can play a fundamental role in {\it theory building} - which is a task common to all data sciences- statistics machine learning and data mining.
2269,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Hierarchical Regularization Cascade for Joint Learning,"Alon Zweig, Daphna Weinshall",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/zweig13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/zweig13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_zweig13,http://jmlr.csail.mit.edu/proceedings/papers/v28/zweig13.html,"As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased,until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods."
2270,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Graph-based Semi-supervised Learning: Realizing Pointwise Smoothness Probabilistically,"Yuan Fang, Kevin Chang, Hady Lauw",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/fang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/fang14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_fang14,http://jmlr.csail.mit.edu/proceedings/papers/v32/fang14.html,"As the central notion in semi-supervised learning, smoothness is often realized on a graph representation of the data. In this paper, we study two complementary dimensions of smoothness: its pointwise nature and probabilistic modeling. While no existing graph-based work exploits them in conjunction, we encompass both in a novel framework of Probabilistic Graph-based Pointwise Smoothness (PGP), building upon two foundational models of data closeness and label coupling. This new form of smoothness axiomatizes a set of probability constraints, which ultimately enables class prediction. Theoretically, we provide an error and robustness analysis of PGP. Empirically, we conduct extensive experiments to show the advantages of PGP."
2271,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Compressing Neural Networks with the Hashing Trick,"Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, Yixin Chen",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/chenc15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_chenc15,http://jmlr.csail.mit.edu/proceedings/papers/v37/chenc15.html,"As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance."
2272,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Residual Splash for Optimally Parallelizing Belief Propagation,"Joseph Gonzalez, Yucheng Low, Carlos Guestrin","5:177-184, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/gonzalez09a/gonzalez09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_gonzalez09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/gonzalez09a.html,As computer architectures move towards parallelism we must build a new theoretical understanding of parallelism in machine learning. In this paper we focus on parallelizing message passing inference algorithms in graphical models. We develop a theoretical understanding of the limitations of parallelism in belief propagation and bound the optimal achievable running parallel performance on a certain class of graphical models. We demonstrate that the fully synchronous parallelization of belief propagation is highly inefficient. We provide a new parallel belief propagation which achieves optimal performance on a certain class of graphical models. Using two challenging real-world problems we empirically evaluate the performance of our algorithm. On the real-world problems we find that our new algorithm achieves near linear performance improvements and out performs alternative parallel belief propagation algorithms.
2273,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Consistency of Online Random Forests,"Misha Denil, David Matheson, De Freitas Nando",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/denil13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/denil13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_denil13,http://jmlr.csail.mit.edu/proceedings/papers/v28/denil13.html,"As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests."
2274,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Online Clustering with Experts,"Anna Choromanska, Claire Monteleoni",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/choromanska12/choromanska12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_choromanska12,http://jmlr.csail.mit.edu/proceedings/papers/v22/choromanska12.html,Approximating the k-means clustering objective with an online learning algorithm is an open problem. We introduce a family of online clustering algorithms by extending algorithms for online supervised learning with access to expert predictors to the unsupervised learning setting. Instead of computing prediction errors in order to re-weight the experts the algorithms compute an approximation to the current value of the k-means objective obtained by each expert. When the experts are batch clustering algorithms with b-approximation guarantees with respect to the k-means objective (for example the k-means++ or k-means# algorithms) applied to a sliding window of the data stream our algorithms obtain approximation guarantees with respect to the k-means objective. The form of these online clustering approximation guarantees is novel and extends an evaluation framework proposed by Dasgupta as an analog to regret. Notably our approximation bounds are with respect to the optimal k-means cost on the entire data stream seen so far even though the algorithm is online. Our algorithm's empirical performance tracks that of the best clustering algorithm in its expert set.
2275,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Random Feature Maps for Dot Product Kernels,"Purushottam Kar, Harish Karnick",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/kar12/kar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_kar12,http://jmlr.csail.mit.edu/proceedings/papers/v22/kar12.html,Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.
2276,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Results of the PAutomaC Probabilistic Automaton Learning Competition,"Sicco Verwer, R_mi Eyraud, and Colin de la Higuera","21:243-248, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/verwer12a/verwer12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_verwer12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/verwer12a.html,Approximating distributions over strings is a hard learning problem. Typical GI techniques involve using finite state machines as models and attempting to learn both the structure and the weights simultaneously. The PAutomaC competition is the first challenge to allow comparison between methods and algorithms and builds a first state of the art for these techniques. Both artificial data and real data were proposed and contestants were to try to estimate the probabilities of test strings. The purpose of this paper is to provide an overview of the implementation details of PAutomaC and to report the final results of the competition.
2277,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Swept Approximate Message Passing for Sparse Estimation,"Andre Manoel, Florent Krzakala, Eric Tramel, Lenka Zdeborov_",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/manoel15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_manoel15,http://jmlr.csail.mit.edu/proceedings/papers/v37/manoel15.html,"Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality."
2278,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Collapsed Variational Bayesian Inference for Hidden Markov Models,"Pengyu Wang, Phil Blunsom",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_wang13b,http://jmlr.csail.mit.edu/proceedings/papers/v31/wang13b.html,"Approximate inference for Bayesian models is dominated by two approaches, variational Bayesian inference and Markov Chain Monte Carlo. Both approaches have their own advantages and disadvantages, and they can complement each other. Recently researchers have proposed collapsed variational Bayesian inference to combine the advantages of both. Such inference methods have been successful in several models whose hidden variables are conditionally independent given the parameters. In this paper we propose two collapsed variational Bayesian inference algorithms for hidden Markov models, a popular framework for representing time series data. We validate our algorithms on the natural language processing task of unsupervised part-of-speech induction, showing that they are both more computationally efficient than sampling, and more accurate than standard variational Bayesian inference for HMMs."
2279,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Accelerating ABC methods using Gaussian processes,Richard Wilkinson,"JMLR W&CP 33 :1015-1023, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/wilkinson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_wilkinson14,http://jmlr.csail.mit.edu/proceedings/papers/v33/wilkinson14.html,"Approximate Bayesian computation (ABC) methods are used to approximate posterior distributions using simulation rather than likelihood calculations. We introduce Gaussian process (GP) accelerated ABC, which we show can significantly reduce the number of simulations required. As computational resource is usually the main determinant of accuracy in ABC, GP-accelerated methods can thus enable more accurate inference in some models. GP models of the unknown log-likelihood function are used to exploit continuity and smoothness, reducing the required computation. We use a sequence of models that increase in accuracy, using intermediate models to rule out regions of the parameter space as implausible. The methods will not be suitable for all problems, but when they can be used, can result in significant computational savings. For the Ricker model, we are able to achieve accurate approximations to the posterior distribution using a factor of 100 fewer simulator evaluations than comparable Monte Carlo approaches, and for a population genetics model we are able to approximate the exact posterior for the first time."
2280,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,"Approachability, fast and slow","Vianney Perchet, Shie Mannor",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Perchet13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Perchet13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Perchet13.html,"Approachability has become a central tool in the analysis of repeated games and online learning. A player plays a repeated vector-valued game against Nature and her objective is to have her long-term average reward inside some target set. The celebrated results of Blackwell provide a \(1/\sqrt{n}\) convergence rate of the expected point-to-set distance if this is achievable, i.e., if the set is approachable. In this paper we provide a characterization for the convergence rates of approachability and show that in some cases a set can be approached with a \(1/n\) rate. Our characterization is solely based on a combination of geometric properties of the set with properties of the repeated game, and not on additional restrictive assumptions on Natureês behavior."
2281,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Global graph kernels using geometric embeddings,"Fredrik Johansson, Vinay Jethava, Devdatt Dubhashi, Chiranjib Bhattacharyya",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/johansson14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/johansson14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_johansson14,http://jmlr.csail.mit.edu/proceedings/papers/v32/johansson14.html,"Applications of machine learning methods increasingly deal with graph structured data through kernels. Most existing graph kernels compare graphs in terms of features defined on small subgraphs such as walks, paths or graphlets, adopting an inherently local perspective. However, several interesting properties such as girth or chromatic number are global properties of the graph, and are not captured in local substructures. This paper presents two graph kernels defined on unlabeled graphs which capture global properties of graphs using the celebrated Lovˆsz number and its associated orthonormal representation. We make progress towards theoretical results aiding kernel choice, proving a result about the separation margin of our kernel for classes of graphs. We give empirical results on classification of synthesized graphs with important global properties as well as established benchmark graph datasets, showing that the accuracy of our kernels is better than or competitive to existing graph kernels."
2282,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,MCMC Methods for Bayesian Mixtures of Copulas,"Ricardo Silva, Robert Gramacy","5:512-519, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/silva09a/silva09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_silva09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/silva09a.html,Applications of copula models have been increasing in number in recent years. This class of models provides a modular parameterization of joint distributions: the specification of the marginal distributions is parameterized separately from the dependence structure of the joint a convenient way of encoding a model for domains such as finance. Some recent advances on how to specify copulas for arbitrary dimensions have been proposed by means of mixtures of decomposable graphical models. This paper introduces a Bayesian approach for dealing with mixtures of copulas which due to the lack of prior conjugacy raise computational challenges. We motivate and present families of Markov chain Monte Carlo (MCMC) proposals that exploit the particular structure of mixtures of copulas. Different algorithms are evaluated according to their mixing properties and an application in financial forecasting with missing data illustrates the usefulness of the methodology.
2283,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Exact Bayesian Learning of Ancestor Relations in Bayesian Networks,"Yetian Chen, Lingjian Meng, Jin Tian",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15e.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15e-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_chen15e,http://jmlr.csail.mit.edu/proceedings/papers/v38/chen15e.html,"Ancestor relations in Bayesian networks (BNs) encode long-range causal relations among random variables. In this paper, we develop dynamic programming (DP) algorithms to compute the exact posterior probabilities of ancestor relations in Bayesian networks. Previous algorithm by Parviainen and Koivisto (2011) evaluates all possible ancestor relations in time O(n3 n ) and space O(3 n ). However, their algorithm assumes an order-modular prior over DAGs that does not respect Markov equivalence. The resulting posteriors would bias towards DAGs consistent with more linear orders. To adhere to the uniform prior, we develop a new DP algorithm that computes the exact posteriors of all possible ancestor relations in time O(n 2 5 n-1 ) and space O(3 n ). We also discuss the extension of our algorithm to computing the posteriors of s _p _t relations, i.e., a directed path from s to t via p. We apply our algorithm to a biological data set for discovering protein signaling pathways."
2284,31,http://jmlr.csail.mit.edu/proceedings/papers/v31/,Uncover Topic-Sensitive Information Diffusion Networks,"Nan Du, Le Song, Hyenkyun Woo, Hongyuan Zha",none,http://jmlr.csail.mit.edu/proceedings/papers/v31/du13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v31/,,29th April 2013,"April 29 - May 1, 2013",AISTATS 2013 Proceedings,"Sixteenth International Conference on Artificial Intelligence and Statistics","Scottsdale, AZ, USA","Carvalho, Carlos M. and Ravikumar, Pradeep",v31_du13a,http://jmlr.csail.mit.edu/proceedings/papers/v31/du13a.html,"Analyzing the spreading patterns of memes with respect to their topic distributions and the underlying diffusion network structures is an important task in social network analysis. This task in many cases becomes very challenging since the underlying diffusion networks are often hidden, and the topic specific transmission rates are unknown either. In this paper, we propose a continuous time model, TopicCascade, for topic-sensitive information diffusion networks, and infer the hidden diffusion networks and the topic dependent transmission rates from the observed time stamps and contents of cascades. One attractive property of the model is that its parameters can be estimated via a convex optimization which we solve with an efficient proximal gradient based block coordinate descent (BCD) algorithm. In both synthetic and real-world data, we show that our method significantly improves over the previous state-of-the-art models in terms of both recovering the hidden diffusion networks and predicting the transmission times of memes."
2285,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Scalable Simple Random Sampling and Stratified Sampling,Xiangrui Meng,none,http://jmlr.csail.mit.edu/proceedings/papers/v28/meng13a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_meng13a,http://jmlr.csail.mit.edu/proceedings/papers/v28/meng13a.html,"Analyzing data sets of billions of records has now become a regular task in many companies and institutions. In the statistical analysis of those massive data sets, sampling generally plays a very important role. In this work, we describe a scalable simple random sampling algorithm, named ScaSRS, which uses probabilistic thresholds to decide on the fly whether to accept, reject, or wait-list an item independently of others. We prove, with high probability, it succeeds and needs only \(O(\sqrt{k})\) storage, where \(k\) is the sample size. ScaSRS extends naturally to a scalable stratified sampling algorithm, which is favorable for heterogeneous data sets. The proposed algorithms, when implemented in MapReduce, can effectively reduce the size of intermediate output and greatly improve load balancing. Empirical evaluation on large-scale data sets clearly demonstrates their superiority."
2286,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Analogical Reasoning with Relational Bayesian Sets,"Ricardo Silva, Katherine A. Heller, Zoubin Ghahramani","2:500-507, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/silva07a/silva07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_silva07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/silva07a.html,Analogical reasoning depends fundamentally on the ability to learn and generalize about relations between objects. There are many ways in which objects can be related making automated analogical reasoning very challenging. Here we develop an approach which given a set of pairs of related objects $S = \{A^1:B^1 A^2:B^2 ... A^N:B^N \}$ measures how well other pairs A:B fit in with the set S. This addresses the question: is the relation between objects A and B analogous to those relations found in S? We recast this classical problem as a problem of Bayesian analysis of relational data. This problem is nontrivial because direct similarity between objects is not a good way of measuring analogies. For instance the analogy between an electron around the nucleus of an atom and a planet around the Sun is hardly justified by isolated non-relational comparisons of an electron to a planet and a nucleus to the Sun. We develop a generative model for predicting the existence of relationships and extend the framework of Ghahramani and Heller (2005) to provide a Bayesian measure for how analogous a relation is to other relations. This sheds new light on an old problem which we motivate and illustrate through practical applications in exploratory data analysis.
2287,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm,"Mark Schmidt, Ewout van den Berg, Michael Friedlander, Kevin Murphy","5:456-463, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/schmidt09a/schmidt09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_schmidt09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/schmidt09a.html,An optimization algorithm for minimizing a smooth function over a convex set is described. Each iteration of the method computes a descent direction by minimizing over the original constraints a diagonal plus low-rank quadratic approximation to the function. The quadratic approximation is constructed using a limited-memory quasi-Newton update. The method is suitable for large-scale problems where evaluation of the function is substantially more expensive than projection onto the constraint set. Numerical experiments on one-norm regularized test problems indicate that the proposed method is competitive with state-of-the-art methods such as bound-constrained L-BFGS and orthant-wise descent. We further show that the method generalizes to a wide class of problems and substantially improves on state-of-the-art methods for problems such as learning the structure of Gaussian graphical models and Markov random fields.
2288,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Causality with Gates,John Winn,none,http://jmlr.csail.mit.edu/proceedings/papers/v22/winn12/winn12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_winn12,http://jmlr.csail.mit.edu/proceedings/papers/v22/winn12.html,An intervention on a variable removes the influences that usually have a causal effect on that variable. Gates are a general-purpose graphical modelling notation for representing such context-specific independencies in the structure of a graphical model. We extend d-separation to cover gated graphical models and show that it subsumes do calculus when gates are used to represent interventions. We also show how standard message passing inference algorithms such as belief propagation can be applied to the gated graph. This demonstrates that causal reasoning can be performed by probabilistic inference alone.
2289,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Constrained 1-Spectral Clustering,"Syama Sundar Rangapuram, Matthias Hein",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/sundar12/sundar12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_sundar12,http://jmlr.csail.mit.edu/proceedings/papers/v22/sundar12.html,An important form of prior information in clustering comes in the form of cannot-link and must-link constraints of instances. We present a generalization of the popular spectral clustering technique which integrates such constraints. Motivated by the recently proposed 1-spectral clustering for the unconstrained normalized cut problem our method is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem. Opposite to all other methods which have been suggested for constrained spectral clustering we can always guarantee to satisfy all constraints. Moreover our soft formulation allows to optimize a trade-off between normalized cut and the number of violated constraints. An efficient implementation is provided which scales to large datasets. We outperform consistently all other proposed methods in the experiments.
2290,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Adaptive Sparsity in Gaussian Graphical Models,"Eleanor Wong, Suyash Awate, P. Thomas Fletcher",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/wong13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_wong13,http://jmlr.csail.mit.edu/proceedings/papers/v28/wong13.html,"An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffreysê hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation."
2291,44,http://jmlr.csail.mit.edu/proceedings/papers/v44/,Minimum description length (MDL) regularization for online learning,Gil I. Shamir,none,http://jmlr.csail.mit.edu/proceedings/papers/v44/shamir15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v44/,,8th December 2015,7 - 12 December 2015,NIPS Workshop on Feature Extraction 2015 Proceedings,Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS) ,"Montreal, Canada","Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar, Neil Lawrence",v44_shamir15,http://jmlr.csail.mit.edu/proceedings/papers/v44/shamir15.html,"An approach inspired by the Minimum Description Length (MDL) principle is proposed for adaptively selecting features during online learning based on their usefulness in improving the objective. The approach eliminates noisy or useless features from the optimization process, leading to improved loss. Several algorithmic variations on the approach are presented. They are based on using a Bayesian mixture in each of the dimensions of the feature space. By utilizing the MDL principle, the mixture reduces the dimensionality of the feature space to its subspace with the lowest loss. Bounds on the loss, derived, show that the loss for that subspace is essentially achieved. The approach can be tuned for trading off between model size and the loss incurred. Empirical results on large scale real-world systems demonstrate how it improves such tradeoffs. Huge model size reductions can be achieved with no loss in performance relative to standard techniques, while moderate loss improvements (translating to large regret improvements) are achieved with moderate size reductions. The results also demonstrate that overfitting is eliminated by this approach."
2292,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Robust Regression on MapReduce,"Xiangrui Meng, Michael Mahoney",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/meng13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_meng13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/meng13b.html,"Although the MapReduce framework is now the de facto standard for analyzing massive data sets, many algorithms (in particular, many iterative algorithms popular in machine learning, optimization, and linear algebra) are hard to fit into MapReduce. Consider, e.g. , the \(\ell_p\) regression problem: given a matrix \(A \in \mathbb{R}^{m \times n}\) and a vector \(b \in \mathbb{R}^m\) , find a vector \(x^* \in \mathbb{R}^n\) that minimizes \(f(x) = \|A x - b\|_p\) . The widely-used \(\ell_2\) regression, i.e. , linear least-squares, is known to be highly sensitive to outliers; and choosing \(p \in [1, 2)\) can help improve robustness. In this work, we propose an efficient algorithm for solving strongly over-determined \((m \gg n)\) robust \(\ell_p\) regression problems to moderate precision on MapReduce. Our empirical results on data up to the terabyte scale demonstrate that our algorithm is a significant improvement over traditional iterative algorithms on MapReduce for \(\ell_1\) regression, even for a fairly small number of iterations. In addition, our proposed interior-point cutting-plane method can also be extended to solving more general convex problems on MapReduce."
2293,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Alpha-Beta Divergences Discover Micro and Macro Structures in Data,"Karthik Narayan, Ali Punjani, Pieter Abbeel",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/narayan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/narayan15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_narayan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/narayan15.html,"Although recent work in non-linear dimensionality reduction investigates multiple choices of divergence measure during optimization , little work discusses the direct effects that divergence measures have on visualization. We study this relationship, theoretically and through an empirical analysis over 10 datasets. Our works shows how the \(\alpha\) and \(\beta\) parameters of the generalized alpha-beta divergence can be chosen to discover hidden macro-structures (categories, e.g. birds) or micro-structures (fine-grained classes, e.g. toucans). Our method, which generalizes t-SNE , allows us to discover such structure without extensive grid searches over \((\alpha, \beta)\) due to our theoretical analysis: such structure is apparent with particular choices of \((\alpha, \beta)\) that generalize across datasets. We also discuss efficient parallel CPU and GPU schemes which are non-trivial due to the tree-structures employed in optimization and the large datasets that do not fully fit into GPU memory. Our method runs 20x faster than the fastest published code . We conclude with detailed case studies on the following very large datasets: ILSVRC 2012, a standard computer vision dataset with 1.2M images; SUSY, a particle physics dataset with 5M instances; and HIGGS, another particle physics dataset with 11M instances. This represents the largest published visualization attained by SNE methods. We have open-sourced our visualization code: http://rll.berkeley.edu/absne/ ."
2294,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,The Most Generative Maximum Margin Bayesian Networks,"Robert Peharz, Sebastian Tschiatschek, Franz Pernkopf",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/peharz13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/peharz13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_peharz13,http://jmlr.csail.mit.edu/proceedings/papers/v28/peharz13.html,"Although discriminative learning in graphical models generally improves classification results, the generative semantics of the model are compromised. In this paper, we introduce a novel approach of hybrid generative-discriminative learning for Bayesian networks. We use an SVM-type large margin formulation for discriminative training, introducing a likelihood-weighted \(\ell^1\) -norm for the SVM-norm-penalization. This simultaneously optimizes the data likelihood and therefore partly maintains the generative character of the model. For many network structures, our method can be formulated as a convex problem, guaranteeing a globally optimal solution. In terms of classification, the resulting models outperform state-of-the art generative and discriminative learning methods for Bayesian networks, and are comparable with linear and kernelized SVMs. Furthermore, the models achieve likelihoods close to the maximum likelihood solution and show robust behavior in classification experiments with missing features."
2295,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Nonparametric Bayesian Approach to Modeling Overlapping Clusters,"Katherine A. Heller, Zoubin Ghahramani","2:187-194, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/heller07a/heller07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_heller07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/heller07a.html,Although clustering data into mutually exclusive partitions has been an extremely successful approach to unsupervised learning there are many situations in which a richer model is needed to fully represent the data. This is the case in problems where data points actually simultaneously belong to multiple overlapping clusters. For example a particular gene may have several functions therefore belonging to several distinct clusters of genes and a biologist may want to discover these through unsupervised modeling of gene expression data. We present a new nonparametric Bayesian method the Infinite Overlapping Mixture Model (IOMM) for modeling overlapping clusters. The IOMM uses exponential family distributions to model each cluster and forms an overlapping mixture by taking products of such distributions much like products of experts (Hinton 2002). The IOMM allows an unbounded number of clusters and assignments of points to (multiple) clusters is modeled using an Indian Buffet Process (IBP) (Griffiths and Ghahramani 2006). The IOMM has the desirable properties of being able to focus in on overlapping regions while maintaining the ability to model a potentially infinite number of clusters which may overlap. We derive MCMC inference algorithms for the IOMM and show that these can be used to cluster movies into multiple genres.
2296,39,http://jmlr.csail.mit.edu/proceedings/papers/v39/,Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo,"Hanchen Xiong, Sandor Szedmak, Justus Piater",none,http://jmlr.csail.mit.edu/proceedings/papers/v39/xiong14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v39/,,16th February 2015,26-28 November 2014,ACML 2014 Proceedings,6th Asian Conference on Machine Learning ,"Nha Trang city, Vietnam ","Dinh Phung, Hang Li",v39_xiong14,http://jmlr.csail.mit.edu/proceedings/papers/v39/xiong14.html,"Along with the emergence of algorithms such as persistent contrastive divergence (PCD), tempered transition and parallel tempering, the past decade has witnessed a revival of learning undirected graphical models (UGMs) with sampling-based approximations. In this paper, based upon the analogy between Robbins-Monroês stochastic approximation procedure and sequential Monte Carlo (SMC), we analyze the strengths and limitations of state-of-the-art learning algorithms from an SMC point of view. Moreover, we apply the rationale further in sampling at each iteration, and propose to learn UGMs using persistent sequential Monte Carlo (PSMC). The whole learning procedure is based on the samples from a long, persistent sequence of distributions which are actively constructed. Compared to the above-mentioned algorithms, one critical strength of PSMC- based learning is that it can explore the sampling space more effectively. In particular, it is robust when learning rates are large or model distributions are high-dimensional and thus multi-modal, which often causes other algorithms to deteriorate. We tested PSMC learning, also with other related methods, on carefully-designed experiments with both synthetic and real-word data, and our empirical results demonstrate that PSMC compares favorably with the state of the art."
2297,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Memory and Computation Efficient PCA via Very Sparse Random Projections,"Farhad Pourkamali Anaraki, Shannon Hughes",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/anaraki14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/anaraki14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_anaraki14,http://jmlr.csail.mit.edu/proceedings/papers/v32/anaraki14.html,"Algorithms that can efficiently recover principal components in very high-dimensional, streaming, and/or distributed data settings have become an important topic in the literature. In this paper, we propose an approach to principal component estimation that utilizes projections onto very sparse random vectors with Bernoulli-generated nonzero entries. Indeed, our approach is simultaneously efficient in memory/storage space, efficient in computation, and produces accurate PC estimates, while also allowing for rigorous theoretical performance analysis. Moreover, one can tune the sparsity of the random vectors deliberately to achieve a desired point on the tradeoffs between memory, computation, and accuracy. We rigorously characterize these tradeoffs and provide statistical performance guarantees. In addition to these very sparse random vectors, our analysis also applies to more general random projections. We present experimental results demonstrating that this approach allows for simultaneously achieving a substantial reduction of the computational complexity and memory/storage space, with little loss in accuracy, particularly for very high-dimensional data."
2298,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Hidden Topic Markov Models,"Amit Gruber, Yair Weiss, Michal Rosen-Zvi","2:163-170, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/gruber07a/gruber07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_gruber07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/gruber07a.html,Algorithms such as Latent Dirichlet Allocation (LDA) have achieved significant progress in modeling word document relationships. These algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document. Given these parameters the topics of all words in the same document are assumed to be independent. In this paper we propose modeling the topics of words in the document as a Markov chain. Specifically we assume that all words in the same sentence have the same topic and successive sentences are more likely to have the same topics. Since the topics are hidden this leads to using the well-known tools of Hidden Markov Models for learning and inference. We show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics. Quantitatively we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity.
2299,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Power-Law Graph Cuts,"Xiangyang Zhou, Jiaxin Zhang, Brian Kulis",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhou15b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_zhou15b,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhou15b.html,"Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human- segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the result- ing objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms."
2300,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Robust and Efficient Kernel Hyperparameter Paths with Guarantees,"Joachim Giesen, Soeren Laue, Patrick Wieschollek",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/giesen14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_giesen14,http://jmlr.csail.mit.edu/proceedings/papers/v32/giesen14.html,"Algorithmically, many machine learning tasks boil down to solving parameterized optimization problems. Finding good values for the parameters has significant influence on the statistical performance of these methods. Thus supporting the choice of parameter values algorithmically has received quite some attention recently, especially algorithms for computing the whole solution path of parameterized optimization problem. These algorithms can be used, for instance, to track the solution of a regularized learning problem along the regularization parameter path, or for tracking the solution of kernelized problems along a kernel hyperparameter path. Since exact path following algorithms can be numerically unstable, robust and efficient approximate path tracking algorithms became popular for regularized learning problems. By now algorithms with optimal path complexity are known for many regularized learning problems. That is not the case for kernel hyperparameter path tracking algorithms, where the exact path tracking algorithms can also suffer from numerical instabilities. The robust approximation algorithms for regularization path tracking can not be used directly for kernel hyperparameter path tracking problems since the latter fall into a different problem class. Here we address this problem by devising a robust and efficient path tracking algorithm that can also handle kernel hyperparameter paths and has asymptotically optimal complexity. We use this algorithm to compute approximate kernel hyperparamter solution paths for support vector machines and robust kernel regression. Experimental results for this problem applied to various data sets confirms the theoretical complexity analysis."
2301,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Open Problem: Adversarial Multiarmed Bandits with Limited Advice,"Yevgeny Seldin, Koby Crammer, Peter Bartlett",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Seldin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Seldin13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Seldin13.html,"Adversarial multiarmed bandits with expert advice is one of the fundamental problems in studying the exploration-exploitation trade-off. It is known that if we observe the advice of all experts on every round we can achieve \(O\left(\sqrt{KT \ln N}\right)\) regret, where \(K\) is the number of arms, \(T\) is the number of game rounds, and \(N\) is the number of experts. It is also known that if we observe the advice of just one expert on every round, we can achieve regret of order \(O\left(\sqrt{NT}\right)\) . Our open problem is what can be achieved by asking \(M\) experts on every round, where \(1_M_N\) ."
2302,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Particle Gibbs for Bayesian Additive Regression Trees,"Balaji Lakshminarayanan, Daniel Roy, Yee Whye Teh",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/lakshminarayanan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/lakshminarayanan15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_lakshminarayanan15,http://jmlr.csail.mit.edu/proceedings/papers/v38/lakshminarayanan15.html,"Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is also demand for probabilistic predictions with measures of uncertainty, the Bayesian additive regression trees (BART) model, introduced by Chipman et al. (2010), is increasingly popular. As data sets have grown in size, however, the standard Metropolis_Hastings algorithms used to per- form inference in BART are proving inadequate. In particular, these Markov chains make local changes to the trees and suffer from slow mixing when the data are high- dimensional or the best-fitting trees are more than a few layers deep. We present a novel sampler for BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a top-down particle filtering algorithm for Bayesian decision trees (Lakshminarayanan et al., 2013). Rather than making local changes to individual trees, the PG sampler proposes a complete tree to fit the residual. Experiments show that the PG sampler outperforms existing samplers in many settings."
2303,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Bayesian Divergence Prior for Classiffier Adaptation,"Xiao Li, Jeff Bilmes","2:275-282, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/li07a/li07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_li07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/li07a.html,"Adaptation of statistical classifiers is critical when a target (or testing) distribution is different from the distribution that governs training data. In such cases a classifier optimized for the training distribution needs to be adapted for optimal use in the target distribution. This paper presents a Bayesian ""divergence prior"" for generic classifier adaptation. Instantiations of this prior lead to simple yet principled adaptation strategies for a variety of classifiers which yield superior performance in practice. In addition this paper derives several adaptation error bounds by applying the divergence prior in the PAC-Bayesian setting."
2304,41,http://jmlr.csail.mit.edu/proceedings/papers/v41/,Big Data with ADAMS,"Peter Reutemann, Geoff Holmes",none,http://jmlr.csail.mit.edu/proceedings/papers/v41/reutemann15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v41/,,31st August 2015,42226,BigMine 2015 Proceedings,"4th International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Sydeny, Australia","Wei Fan, Albert Bifet, Qiang Yang, Philip S. Yu",v41_reutemann15,http://jmlr.csail.mit.edu/proceedings/papers/v41/reutemann15.html,"ADAMS is a modular open-source Java framework for developing workflows available for academic research as well as commercial applications. It integrates data mining applications, like MOA, WEKA, MEKA and R, image and video processing and feature generation capabilities, spreadsheet and database access, visualizations, GIS, webservices and fast protoyping of new functionality using scripting languages (Groovy/Jython)."
2305,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Efficient active learning with generalized linear models,"Jeremy Lewi, Robert Butera, Liam Paninski","2:267-274, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/lewi07a/lewi07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_lewi07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/lewi07a.html,Active learning can significantly reduce the amount of training data required to fit parametric statistical models for supervised learning tasks. Here we present an efficient algorithm for choosing the optimal (most informative) query when the output labels are related to the inputs by a generalized linear model (GLM). The algorithm is based on a Laplace approximation of the posterior distribution of the GLMÕs parameters. The algorithm requires only low-rank matrix manipulations and a single two-dimensional search to choose the optimal query and has complexity $O(n^2)$ (with n the dimension of the feature space) making active learning with GLMs feasible even for high-dimensional feature spaces. In certain cases the twodimensional search may be reduced to a onedimensional search further improving the algorithmÕs efficiency. Simulation results show that the model parameters can be estimated much more efficiently using the active learning technique than by using randomly chosen queries. We compute the asymptotic posterior covariance semi-analytically and demonstrate that the algorithm empirically achieves this asymptotic convergence rate which is generally better than the convergence rate in the random-query setting. Finally we generalize the approach to efficiently handle both output history effects (for applications to time-series models of autoregressive type) and slow non-systematic drifts in the model parameters
2306,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization,"Yuxin Chen, Andreas Krause",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chen13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13b.html,"Active learning can lead to a dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing, surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance. We consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some cases, surprisingly, the use of batches incurs competitively low cost, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on batch-mode active learning tasks, where it outperforms the state of the art, as well as the novel problem of multi-stage influence maximization in social networks."
2307,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Joint Transfer and Batch-mode Active Learning,"Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, Jieping Ye",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/chattopadhyay13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/chattopadhyay13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_chattopadhyay13,http://jmlr.csail.mit.edu/proceedings/papers/v28/chattopadhyay13.html,"Active learning and transfer learning are two different methodologies that address the common problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5% to 10% over the existing two-stage approach"
2308,16,http://jmlr.csail.mit.edu/proceedings/papers/v16/,Inspecting Sample Reusability for Active Learning,"K. Tomanek & K. Morik ; 16:169_181, 2011. [ abs ] [ pdf ]","16:169_181, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v16/tomanek11a/tomanek11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v16/,,21st April 2011,40314,Active Learning and Experimental Design Workshop,Active Learning and Experimental Design workshop,"Sardinia, Italy","Isabelle Guyon, Gavin Cawley, Gideon Dror, Vincent Lemaire, and Alexander Statnikov",v16_tomanek11a,http://jmlr.csail.mit.edu/proceedings/papers/v16/tomanek11a.html,Active Learning (AL) exploits a learning algorithm to selectively sample examples which are expected to be highly useful for model learning. The resulting sample is governed by a sampling selection bias. While a bias towards useful examples is desirable there is also a bias towards the learner applied during AL selection. This paper addresses sample reusability i.e. the question whether and under which conditions samples selected by AL using one learning algorithm are well-suited as training data for another learning algorithm.  Our empirical investigation on general classi_cation problems as well as the natural language processing subtask of Named Entity Recognition shows that many intuitive assumptions on reusability characteristics do not hold. For example using the same algorithm during AL selection (called selector) and for inducing the _nal model (called consumer) is not always the optimal choice. We investigate several putatively explanatory factors for sample reusability. One _nding is that the suitability of certain selector-consumer pairings cannot be estimated independently of the actual learning problem.    Page last modified on Wed Mar 30 11:10:43 2011.
2309,20,http://jmlr.csail.mit.edu/proceedings/papers/v20/,Bayesian inference for statistical abduction using Markov chain Monte Carlo,M. Ishihata & T. Sato,"20:81_96, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v20/ishihata11/ishihata11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v20/,,17th November 2011,"19-21 October, 2011",ACML 2011 Proceedings,3rd Asian Conference on Machine Learning,"Taoyuan, Taiwan",Chun-Nan Hsu and Wee Sun Lee,v20_ishihata11,http://jmlr.csail.mit.edu/proceedings/papers/v20/ishihata11.html,Abduction is one of the basic logical inferences (deduction induction and abduction) and derives the best explanations for our observation. Statistical abduction attempts to de_ne a probability distribution over explanations and to evaluate them by their probabilities. The framework of statistical abduction is general since many well-known probabilistic models i.e. BNs HMMs and PCFGs are formulated as statistical abduction. Logic-based probabilistic models (LBPMs) have been developed as a way to combine probabilities and logic and it enables us to perform statistical abduction. However most of existing LBPMs impose restrictions on explanations (logical formulas) to realize e_cient probability computation and learning. To relax those restrictions we propose two MCMC (Markov chain Monte Carlo) methods for Bayesian inference on LBPMs using binary decision diagrams. The main advantage of our methods over existing methods is that it has no restriction on formulas. In the context of statistical abduction with Bayesian inference whereas our deterministic knowledge can be described by logical formulas as rules and facts our non-deterministic knowledge like frequency and preference can be re§ected in a prior distribution in Bayesian inference. To illustrate our methods we _rst formulate LDA (latent Dirichlet allocation) which is a well-known generative probabilistic model for bag-of-words as a form of statistical abduction and compare the learning result of our methods with that of an MCMC method called collapsed Gibbs sampling specialized for LDA. We also apply our methods to diagnosis for failure in a logic circuit and evaluate explanations using a posterior distribution approximated by our method. The experiment shows Bayesian inference achieves better predicting accuracy than that of Maximum likelihood estimation.   Page last modified on Sun Nov 6 15:42:32 2011.
2310,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,On the Complexity of A/B Testing,"Emilie Kaufmann, Olivier Capp_, Aur_lien Garivier","JMLR W&CP 35 :461-481, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/kaufmann14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_kaufmann14,http://jmlr.csail.mit.edu/proceedings/papers/v35/kaufmann14.html,"A/B testing refers to the task of determining the best option among two alternatives that yield random outcomes. We provide distribution-dependent lower bounds for the performance of A/B testing that improve over the results currently available both in the fixed-confidence (or \(\delta\) -PAC) and fixed-budget settings. When the distribution of the outcomes are Gaussian, we prove that the complexity of the fixed-confidence and fixed-budget settings are equivalent, and that uniform sampling of both alternatives is optimal only in the case of equal variances. In the common variance case, we also provide a stopping rule that terminates faster than existing fixed-confidence algorithms. In the case of Bernoulli distributions, we show that the complexity of fixed-budget setting is smaller than that of fixed-confidence setting and that uniform sampling of both alternativesãthough not optimalãis advisable in practice when combined with an appropriate stopping criterion."
2311,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components,"Philipp Geiger, Kun Zhang, Bernhard Schoelkopf, Mingming Gong, Dominik Janzing",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/geiger15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_geiger15,http://jmlr.csail.mit.edu/proceedings/papers/v37/geiger15.html,"A widely applied approach to causal inference from a time series \(X\) , often referred to as –(linear) Granger causal analysis”, is to simply regress present on past and interpret the regression matrix \(\hat{B}\) causally. However, if there is an unmeasured time series \(Z\) that influences \(X\) , then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as \(Z\) . In this paper we take a different approach: We assume that \(X\) together with some hidden \(Z\) forms a first order vector autoregressive (VAR) process with transition matrix \(A\) , and argue why it is more valid to interpret \(A\) causally instead of \(\hat{B}\) . Then we examine under which conditions the most important parts of \(A\) are identifiable or almost identifiable from only \(X\) . Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from \(X\) to \(Z\) . We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using \(X\) ."
2312,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,The Power of Randomization: Distributed Submodular Maximization on Massive Datasets,"Rafael Barbosa, Alina Ene, Huy Nguyen, Justin Ward",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/barbosa15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/barbosa15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_barbosa15,http://jmlr.csail.mit.edu/proceedings/papers/v37/barbosa15.html,"A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting."
2313,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,On the Complexity of Learning with Kernels,"NicolÖ Cesa-Bianchi, Yishay Mansour, Ohad Shamir",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cesa-Bianchi15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Cesa-Bianchi15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Cesa-Bianchi15.html,"A well-recognized limitation of kernel learning is the requirement to handle a kernel matrix, whose size is quadratic in the number of training examples. Many methods have been proposed to reduce this computational cost, mostly by using a subset of the kernel matrix entries, or some form of low-rank matrix approximation, or a random projection method. In this paper, we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix. We show that there are kernel learning problems where no such method will lead to non-trivial computational savings. Our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function, the regularization parameter, the norm of the desired predictor, and the kernel matrix rank. Our results also suggest cases where more efficient kernel learning might be possible."
2314,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Decontamination of Mutually Contaminated Models,"Gilles Blanchard, Clayton Scott","JMLR W&CP 33 :1-9, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/blanchard14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,http://jmlr.csail.mit.edu/proceedings/papers/v33/blanchard14-supp.pdf,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_blanchard14,http://jmlr.csail.mit.edu/proceedings/papers/v33/blanchard14.html,"A variety of machine learning problems are characterized by data sets that are drawn from multiple different convex combinations of a fixed set of base distributions. We call this a mutual contamination model. In such problems, it is often of interest to recover these base distributions, or otherwise discern their properties. This work focuses on the problem of classification with multiclass label noise, in a general setting where the noise proportions are unknown and the true class distributions are nonseparable and potentially quite complex. We develop a procedure for decontamination of the contaminated models from data, which then facilitates the design of a consistent discrimination rule. Our approach relies on a novel method for estimating the error when projecting one distribution onto a convex combination of others, where the projection is with respect to an information divergence known as the separation distance. Under sufficient conditions on the amount of noise and purity of the base distributions, this projection procedure successfully recovers the underlying class distributions. Connections to novelty detection, topic modeling, and other learning problems are also discussed."
2315,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits,"Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/kveton15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/kveton15-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_kveton15,http://jmlr.csail.mit.edu/proceedings/papers/v38/kveton15.html,"A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits. In particular, we analyze a UCB-like algorithm for solving the problem, which is known to be computationally efficient; and prove O(K L (1 / ) n) and O() upper bounds on its n-step regret, where L is the number of ground items, K is the maximum number of chosen items, and is the gap between the expected returns of the optimal and best suboptimal solutions. The gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor."
2316,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Efficient Learning in Large-Scale Combinatorial Semi-Bandits,"Zheng Wen, Branislav Kveton, Azin Ashkan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/wen15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/wen15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_wen15,http://jmlr.csail.mit.edu/proceedings/papers/v37/wen15.html,"A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines."
2317,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Learning Modular Structures from Network Data and Node Variables,"Elham Azizi, Edoardo Airoldi, James Galagan",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/azizi14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_azizi14,http://jmlr.csail.mit.edu/proceedings/papers/v32/azizi14.html,"A standard technique for understanding underlying dependency structures among a set of variables posits a shared conditional probability distribution for the variables measured on individuals within a group. This approach is often referred to as module networks, where individuals are represented by nodes in a network, groups are termed modules, and the focus is on estimating the network structure among modules. However, estimation solely from node-specific variables can lead to spurious dependencies, and unverifiable structural assumptions are often used for regularization. Here, we propose an extended model that leverages direct observations about the network in addition to node-specific variables. By integrating complementary data types, we avoid the need for structural assumptions. We illustrate theoretical and practical significance of the model and develop a reversible-jump MCMC learning procedure for learning modules and model parameters. We demonstrate the method accuracy in predicting modular structures from synthetic data and capability to learn regulatory modules in the Mycobacterium tuberculosis gene regulatory network."
2318,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Exchangeable Variable Models,"Mathias Niepert, Pedro Domingos",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/niepert14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/niepert14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_niepert14,http://jmlr.csail.mit.edu/proceedings/papers/v32/niepert14.html,"A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and probabilistic models which are solely based on independence assumptions."
2319,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Distilled sensing: selective sampling for sparse signal recovery,"Jarvis Haupt, Rui Castro, Robert Nowak","5:216-223, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/haupt09a/haupt09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_haupt09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/haupt09a.html,A selective sampling methodology called Distilled Sensing (DS) is proposed for recovering sparse signals in noise. DS exploits the fact that it is often easier to rule out locations that do not contain signal than it is to detect the locations of non-zero signal components. We formalize this observation and use it to devise a sequential selective sensing strategy that focuses sensing/measurement resources towards the signal subspace. This adaptivity in sensing results in rather surprising gains in sparse signal recovery compared to non-adaptive sensing. We show that exponentially weaker sparse signals can be recovered via DS compared with conventional non-adaptive sensing.
2320,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Cascading Bandits: Learning to Rank in the Cascade Model,"Branislav Kveton, Csaba Szepesvari, Zheng Wen, Azin Ashkan",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/kveton15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/kveton15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_kveton15,http://jmlr.csail.mit.edu/proceedings/papers/v37/kveton15.html,"A search engine usually outputs a list of K web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify K most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated."
2321,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Revisiting the Limits of MAP Inference by MWSS on Perfect Graphs,Adrian Weller,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/weller15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_weller15,http://jmlr.csail.mit.edu/proceedings/papers/v38/weller15.html,"A recent, promising approach to identifying a configuration of a discrete graphical model with highest probability (termed MAP inference) is to reduce the problem to finding a maximum weight stable set (MWSS) in a derived weighted graph, which, if perfect, allows a solution to be found in polynomial time. Weller and Jebara (2013) investigated the class of binary pairwise models where this method may be applied. However, their analysis made a seemingly innocuous assumption which simplifies analysis but led to only a subset of possible reparameterizations being considered. Here we introduce novel techniques and consider all cases, demonstrating that this greatly expands the set of tractable models. We provide a simple, exact characterization of the new, enlarged set and show how such models may be efficiently identified, thus settling the power of the approach on this class."
2322,30,http://jmlr.csail.mit.edu/proceedings/papers/v30/,Blind Signal Separation in the Presence of Gaussian Noise,"Mikhail Belkin, Luis Rademacher, James Voss",none,http://jmlr.csail.mit.edu/proceedings/papers/v30/Belkin13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v30/,,13th June 2013,"June 12-14, 2013",COLT 2013,Conference on Learning Theory 2013,"Princeton, NJ, USA",Shai Shalev-Shwartz and Ingo Steinwart,v30_Belkin13,http://jmlr.csail.mit.edu/proceedings/papers/v30/Belkin13.html,"A prototypical blind signal separation problem is the so-called cocktail party problem, with n people talking simultaneously and n different microphones within a room. The goal is to recover each speech signal from the microphone inputs. Mathematically this can be modeled by assuming that we are given samples from a \(n\) -dimensional random variable \(X=AS\) , where \(S\) is a vector whose coordinates are independent random variables corresponding to each speaker. The objective is to recover the matrix \(A^{-1}\) given random samples from \(X\) . A range of techniques collectively known as Independent Component Analysis (ICA) have been proposed to address this problem in the signal processing and machine learning literature. Many of these techniques are based on using the kurtosis or other cumulants to recover the components. In this paper we propose a new algorithm for solving the blind signal separation problem in the presence of additive Gaussian noise, when we are given samples from \(X=AS+\eta\) , where \(\eta\) is drawn from an unknown, not necessarily spherical \(n\) -dimensional Gaussian distribution. Our approach is based on a method for decorrelating a sample with additive Gaussian noise under the assumption that the underlying distribution is a linear transformation of a distribution with independent components. Our decorrelation routine is based on the properties of cumulant tensors and can be combined with any standard cumulant-based method for ICA to get an algorithm that is provably robust in the presence of Gaussian noise. We derive polynomial bounds for sample complexity and error propagation of our method."
2323,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Generalized Do-Calculus with Testable Causal Assumptions,Jiji Zhang,"2:667-674, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/zhang07a/zhang07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_zhang07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/zhang07a.html,A primary object of causal reasoning concerns what would happen to a system under certain interventions. Specifically we are often interested in estimating the probability distribution of some random variables that would result from forcing some other variables to take certain values. The renowned do-calculus (Pearl 1995) gives a set of rules that govern the identification of such post-intervention probabilities in terms of (estimable) pre-intervention probabilities assuming available a directed acyclic graph (DAG) that represents the underlying causal structure. However a DAG causal structure is seldom fully testable given preintervention observational data since many competing DAG structures are equally compatible with the data. In this paper we extend the do-calculus to cover cases where the available causal information is summarized in a so-called partial ancestral graph (PAG) that represents an equivalence class of DAG structures. The causal assumptions encoded by a PAG are significantly weaker than those encoded by a full-blown DAG causal structure and are in principle fully testable by observed conditional independence relations.
2324,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Preference Relation-based Markov Random Fields for Recommender Systems,"Ahaowu Liu, Gang Li, Truyen Tran, Yuan Jiang",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Liu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Liu15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Liu15.html,"A preference relation -based Top-N recommendation approach, PrefMRF , is proposed to capture both the second-order and the higher-order interactions among users and items. Traditionally Top-N recommendation was achieved by predicting the item ratings first, and then inferring the item rankings, based on the assumption of availability of explicit feedbacks such as ratings, and the assumption that optimizing the ratings is equivalent to optimizing the item rankings. Nevertheless, both assumptions are not always true in real world applications. The proposed PrefMRF approach drops these assumptions by explicitly exploiting the preference relations, a more practical user feedback. Comparing to related work, the proposed PrefMRF approach has the unique property of modeling both the second-order and the higher-order interactions among users and items. To the best of our knowledge, this is the first time both types of interactions have been captured in preference relation -based method. Experiment results on public datasets demonstrate that both types of interactions have been properly captured, and significantly improved Top-N recommendation performance has been achieved."
2325,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,The Coherent Loss Function for Classification,"Wenzhuo Yang, Melvyn Sim, Huan Xu",none,http://jmlr.org/proceedings/papers/v32/yanga14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/yanga14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_yanga14,http://jmlr.csail.mit.edu/proceedings/papers/v32/yanga14.html,"A prediction rule in binary classification that aims to achieve the lowest probability of misclassification involves minimizing over a non-convex, 0-1 loss function, which is typically a computationally intractable optimization problem. To address the intractability, previous methods consider minimizing the cumulative loss _ the sum of convex surrogates of the 0-1 loss of each sample. In this paper, we revisit this paradigm and develop instead an axiomatic framework by proposing a set of salient properties on functions for binary classification and then propose the coherent loss approach, which is a tractable upper-bound of the empirical classification error over the entire sample set. We show that the proposed approach yields a strictly tighter approximation to the empirical classification error than any convex cumulative loss approach while preserving the convexity of the underlying optimization problem, and this approach for binary classification also has a robustness interpretation which builds a connection to robust SVMs. The experimental results show that our approach outperforms the standard SVM when additional constraints are imposed."
2326,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,On the Consistency of Output Code Based Learning Algorithms for Multiclass Learning Problems,"Harish G. Ramaswamy, Balaji Srinivasan Babu, Shivani Agarwal, Robert C. Williamson","JMLR W&CP 35 :885-902, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/ramaswamy14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_ramaswamy14,http://jmlr.csail.mit.edu/proceedings/papers/v35/ramaswamy14.html,"A popular approach to solving multiclass learning problems is to reduce them to a set of binary classification problems through some output code matrix: the widely used one-vs-all and all-pairs methods, and the error-correcting output code methods of Dietterich and Bakiri (1995), can all be viewed as special cases of this approach. In this paper, we consider the question of statistical consistency of such methods. We focus on settings where the binary problems are solved by minimizing a binary surrogate loss, and derive general conditions on the binary surrogate loss under which the one-vs-all and all-pairs code matrices yield consistent algorithms with respect to the multiclass 0-1 loss. We then consider general multiclass learning problems defined by a general multiclass loss, and derive conditions on the output code matrix and binary surrogates under which the resulting algorithm is consistent with respect to the target multiclass loss. We also consider probabilistic code matrices, where one reduces a multiclass problem to a set of class probability labeled binary problems, and show that these can yield benefits in the sense of requiring a smaller number of binary problems to achieve overall consistency. Our analysis makes interesting connections with the theory of proper composite losses (Buja et al., 2005; Reid and Williamson, 2010); these play a role in constructing the right •decodingê for converting the predictions on the binary problems to the final multiclass prediction. To our knowledge, this is the first work that comprehensively studies consistency properties of output code based methods for multiclass learning."
2327,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Parametric Herding,"Yutian Chen, Max Welling","9:97-104, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/chen10a/chen10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_chen10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/chen10a.html,A parametric version of herding is formulated. The nonlinear mapping between consecutive time slices is learned by a form of self-supervised training. The resulting dynamical system generates pseudo-samples that resemble the original data. We show how this parametric herding can be successfully used to compress a dataset consisting of binary digits. It is also verified that high compression rates translate into good prediction performance on unseen test data.
2328,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Shifting Experts on Easy Data,"Manfred K. Warmuth, Wouter M. Koolen","JMLR W&CP 35 :1295-1298, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/warmuth14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_warmuth14,http://jmlr.csail.mit.edu/proceedings/papers/v35/warmuth14.html,"A number of online algorithms have been developed that have small additional loss (regret) compared to the best –shifting expert”. In this model, there is a set of experts and the comparator is the best partition of the trial sequence into a small number of segments, where the expert of smallest loss is chosen in each segment. The regret is typically defined for worst-case data / loss sequences. There has been a recent surge of interest in online algorithms that combine good worst-case guarantees with much improved performance on easy data. A practically relevant class of easy data is the case when the loss of each expert is iid and the best and second best experts have a gap between their mean loss. In the full information setting, the FlipFlop algorithm by De Rooij et al. (2014) combines the best of the iid optimal Follow-The-Leader (FL) and the worst-case-safe Hedge algorithms, whereas in the bandit information case SAO by Bubeck and Slivkins (2012) competes with the iid optimal UCB and the worst-case-safe EXP3. We ask the same question for the shifting expert problem. First, we ask what are the simple and efficient algorithms for the shifting experts problem when the loss sequence in each segment is iid with respect to a fixed but unknown distribution. Second, we ask how to efficiently unite the performance of such algorithms on easy data with worst-case robustness. A particular intriguing open problem is the case when the comparator shifts within a small subset of experts from a large set under the assumption that the losses in each segment are iid."
2329,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Tree Block Coordinate Descent for MAP in Graphical Models,"David Sontag, Tommi Jaakkola","5:544-551, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/sontag09a/sontag09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_sontag09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/sontag09a.html,A number of linear programming relaxations have been proposed for finding most likely settings of the variables (MAP) in large probabilistic models. The relaxations are often succinctly expressed in the dual and reduce to different types of reparameterizations of the original model. The dual objectives are typically solved by performing local block coordinate descent steps. In this work we show how to perform block coordinate descent on spanning trees of the graphical model. We also show how all of the earlier dual algorithms are related to each other giving transformations from one type of reparameterization to another while maintaining monotonicity relative to a common objective function. Finally we quantify when the MAP solution can and cannot be decoded directly from the dual LP relaxation.
2330,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Semi-Supervised Mean Fields,"Fei Wang, Shijun Wang, Changshui Zhang, Ole Winther","2:596-603, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07c/wang07c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_wang07c,http://jmlr.csail.mit.edu/proceedings/papers/v2/wang07c.html,A novel semi-supervised learning approach based on statistical physics is proposed in this paper. We treat each data point as an Ising spin and the interaction between pairwise spins is captured by the similarity between the pairwise points. The labels of the data points are treated as the directions of the corresponding spins. In semi-supervised setting some of the spins have fixed directions (which corresponds to the labeled data) and our task is to determine the directions of other spins. An approach based on the Mean Field theory is proposed to achieve this goal. Finally the experimental results on both toy and real world data sets are provided to show the effectiveness of our method.
2331,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,A Kernel Independence Test for Random Processes,"Kacper Chwialkowski, Arthur Gretton",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/chwialkowski14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_chwialkowski14,http://jmlr.csail.mit.edu/proceedings/papers/v32/chwialkowski14.html,"A non-parametric approach to the problem of testing the independence of two random processes is developed. The test statistic is the Hilbert-Schmidt Independence Criterion (HSIC), which was used previously in testing independence for i.i.d. pairs of variables. The asymptotic behaviour of HSIC is established when computed from samples drawn from random processes. It is shown that earlier bootstrap procedures which worked in the i.i.d. case will fail for random processes, and an alternative consistent estimate of the p-values is proposed. Tests on artificial data and real-world forex data indicate that the new test procedure discovers dependence which is missed by linear approaches, while the earlier bootstrap procedure returns an elevated number of false positives."
2332,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Exploring the Mind: Integrating Questionnaires and fMRI,"Esther Salazar, Ryan Bogdan, Adam Gorka, Ahmad Hariri, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/salazar13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_salazar13,http://jmlr.csail.mit.edu/proceedings/papers/v28/salazar13.html,"A new model is developed for joint analysis of ordered, categorical, real and count data. The ordered and categorical data are answers to questionnaires, the (word) count data correspond to the text questions from the questionnaires, and the real data correspond to fMRI responses for each subject. The Bayesian model employs the von Mises distribution in a novel manner to infer sparse graphical models jointly across people, questions, fMRI stimuli and brain region, with this integrated within a new matrix factorization based on latent binary features. The model is compared with simpler alternatives on two real datasets. We also demonstrate the ability to predict the response of the brain to visual stimuli (as measured by fMRI), based on knowledge of how the associated person answered classical questionnaires."
2333,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Scalable Deep Poisson Factor Analysis for Topic Modeling,"Zhe Gan, Changyou Chen, Ricardo Henao, David Carlson, Lawrence Carin",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/gan15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/gan15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_gan15,http://jmlr.csail.mit.edu/proceedings/papers/v37/gan15.html,"A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models."
2334,33,http://jmlr.csail.mit.edu/proceedings/papers/v33/,Latent Gaussian Models for Topic Modeling,"Changwei Hu, Eunsu Ryu, David Carlson, Yingjian Wang, Lawrence Carin","JMLR W&CP 33 :393-401, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v33/hu14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v33/,,2nd of April 2014,"April 22 - April 25, 2014",AISTATS 2014 Proceedings,Seventeenth International Conference on  Artificial Intelligence and Statistics,"Reykjavik, Iceland","Samuel Kaski, Jukka Corander",v33_hu14,http://jmlr.csail.mit.edu/proceedings/papers/v33/hu14.html,"A new approach is proposed for topic modeling, in which the latent matrix factorization employs Gaussian priors, rather than the Dirichlet-class priors widely used in such models. The use of a latent-Gaussian model permits simple and efficient approximate Bayesian posterior inference, via the Laplace approximation. On multiple datasets, the proposed approach is demonstrated to yield results as accurate as state-of-the-art approaches based on Dirichlet constructions, at a small fraction of the computation. The framework is general enough to jointly model text and binary data, here demonstrated to produce accurate and fast results for joint analysis of voting rolls and the associated legislative text. Further, it is demonstrated how the technique may be scaled up to massive data, with encouraging performance relative to alternative methods."
2335,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Kernel Mean Estimation and Stein Effect,"Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, Bernhard Schoelkopf",none,http://jmlr.org/proceedings/papers/v32/muandet14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/muandet14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_muandet14,http://jmlr.csail.mit.edu/proceedings/papers/v32/muandet14.html,"A mean function in reproducing kernel Hilbert space (RKHS), or a kernel mean, is an important part of many algorithms ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given a finite sample, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved due to a well-known phenomenon in statistics called Stein phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard one. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator."
2336,19,http://jmlr.csail.mit.edu/proceedings/papers/v19/,The Sample Complexity of Dictionary Learning,"Daniel Vainsencher, Shie Mannor, Alfred M. Bruckstein","19:773-790, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v19/vainsencher11a/vainsencher11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v19/,,21st December 2011,"June 9-11, 2011",COLT 2011 Proceedings,Proceedings of the 24th Annual Conference on Learning Theory,"Budapest, Hungary",Sham M. Kakade and Ulrike von Luxburg,v19_vainsencher11a,http://jmlr.csail.mit.edu/proceedings/papers/v19/vainsencher11a.html,A large set of signals can sometimes be described sparsely using a dictionary that is every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications including classification denoising and signal separation learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples?We assume signals are generated from a fixed distribution and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection as measured by the expected $L_2$ error in representation when the dictionary is used.For the case of $l_1$ regularized coefficient selection we provide a generalization bound of the order of $O\left(\sqrt{np\ln(m \lambda)/m}\right)$ where $n$ is the dimension $p$ is the number of elements in the dictionary $\lambda$ is a bound on the $l_1$ norm of the coefficient vector and $m$ is the number of samples which complements existing results.For the case of representing a new signal as a combination of at most $k$ dictionary elements we provide a bound ofthe order $O(\sqrt{np\ln(m k)/m})$ under an assumption on the closeness to orthogonality of the dictionary (low Babel function).We further show that this assumption holds for {\em most} dictionaries in high dimensions in a strong probabilistic sense.Our results also include bounds that converge as $1/m$ not previously known for this problem.We provide similar results in a general setting using kernels with weak smoothness requirements.
2337,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,PAC-inspired Option Discovery in Lifelong Reinforcement Learning,"Emma Brunskill, Lihong Li",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/brunskill14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/brunskill14-supp.pdf,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_brunskill14,http://jmlr.csail.mit.edu/proceedings/papers/v32/brunskill14.html,"A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options. This analysis helps shed light on some interesting prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning."
2338,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Kernel Adaptive Metropolis-Hastings,"Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, Arthur Gretton",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/sejdinovic14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/sejdinovic14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_sejdinovic14,http://jmlr.csail.mit.edu/proceedings/papers/v32/sejdinovic14.html,"A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples."
2339,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction,Mingyuan Zhou,none,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhou15a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhou15a-supp.pdf,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_zhou15a,http://jmlr.csail.mit.edu/proceedings/papers/v38/zhou15a.html,"A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a Bernoulli-Poisson link. The model describes both homophily and stochastic equivalence, and is scalable to big sparse networks by focusing its computation on pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the modelsê scalability and state-of-the-art performance."
2340,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,An Analysis of Single-Layer Networks in Unsupervised Feature Learning,"Adam Coates, Andrew Ng, Honglak Lee","15:215-223, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/coates11a/coates11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_coates11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/coates11a.html,A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed much progress has been made on benchmark datasets like NORB and CIFAR-10 by employing increasingly complex unsupervised learning algorithms and deep models. In this paper however we show that several simple factors such as the number of hidden nodes in the model may be more important to achieving high performance than the learning algorithm or the depth of the model. Speci?cally we will apply several o?-the-shelf feature learning algorithms (sparse auto-encoders sparse RBMs K-means clustering and Gaussian mixtures) to CIFAR-10 NORB and STL datasets using only single-layer networks. We then present a detailed analysis of the e?ect of changes in the model setup: the receptive ?eld size num- ber of hidden nodes (features) the step-size (ñstrideî) between extracted features and the e?ect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performanceÑso critical in fact that when these parameters are pushed to their limits we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly our best performance is based on K-means clustering which is extremely fast has no hyper-parameters to tune beyond the model structure itself and is very easy to implement. Despite the simplicity of our system we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).
2341,34,http://jmlr.csail.mit.edu/proceedings/papers/v34/,Grammar Compression: Grammatical Inference by Compression and Its Application to Real Data,Hiroshi Sakamoto,"JMLR W&CP 34 :3-20, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v34/sakamoto14a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v34/,,30th August 2014,"September 17‹19, 2014",ICGI 2014 Proceedings,The 12th International Conference on Grammatical Inference,"Kyoto, Japan","Alexander Clark, Makoto Kanazawa, Ryo Yoshinaka",v34_sakamoto14a,http://jmlr.csail.mit.edu/proceedings/papers/v34/sakamoto14a.html,"A grammatical inference algorithm tries to find as a small grammar as possible representing a potentially infinite sequence of strings. Here, let us consider a simple restriction: the input is a finite sequence or it might be a singleton set. Then the restricted problem is called the grammar compression to find the smallest CFG generating just the input. In the last decade many researchers have tackled this problem because of its scalable applications, e.g., expansion of data storage capacity, speeding-up information retrieval, DNA sequencing, frequent pattern mining, and similarity search. We would review the history of grammar compression and its wide applications together with an important future work. The study of grammar compression has begun with the bad news: the smallest CFG problem is NP-hard. Hence, the first question is: Can we get a near-optimal solution in a polynomial time? (Is there a reasonable approximation algorithm?) And the next question is: Can we minimize the costs of time and space? (Does a linear time algorithm exist within an optimal working space?) The recent results produced by the research community answer affirmatively the questions. We introduce several important results and typical applications to a huge text collection. On the other hand, the shrinkage of the advantage of grammar compression is caused by the data explosion, since there is no working space for storing the whole data supplied from data stream. The last question is: How can we handle the stream data? For this question, we propose the framework of stream grammar compression for the next generation and its attractive application to fast data transmission."
2342,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Similarity Learning for High-Dimensional Sparse Data,"Kuan Liu, Aur_lien Bellet, Fei Sha",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/liu15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_liu15,http://jmlr.csail.mit.edu/proceedings/papers/v38/liu15.html,"A good measure of similarity between data points is crucial to many tasks in machine learning. Similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to the dimensionality of the data. In this paper, we propose a method that can learn efficiently similarity measure from high-dimensional sparse data. The core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures. The parameters are then optimized with an approximate Frank-Wolfe procedure to maximally satisfy relative similarity constraints on the training data. Our algorithm greedily incorporates one pair of features at a time into the similarity measure, providing an efficient way to control the number of active features and thus reduce overfitting. It enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space. Our experiments on real-world high-dimensional datasets demonstrate its potential for classification, dimensionality reduction and data exploration."
2343,38,http://jmlr.csail.mit.edu/proceedings/papers/v38/,Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices,"Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou",none,http://jmlr.csail.mit.edu/proceedings/papers/v38/acharya15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v38/,,21st February 2015,"May 9 - 12, 2015",AISTATS 2015 Proceedings,The 18th International Conference on  Artificial Intelligence and Statistics,"San Diego, California, USA","Guy Lebanon, S.V.N. Vishwanathan",v38_acharya15,http://jmlr.csail.mit.edu/proceedings/papers/v38/acharya15.html,"A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel Markov chain that sends the latent gamma random variables at time (t-1) as the shape parameters of those at time t, which are linked to observed or latent counts under the Poisson likelihood. The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results."
2344,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,A Bayesian nonparametric procedure for comparing algorithms,"Alessio Benavoli, Giorgio Corani, Francesca Mangili, Marco Zaffalon",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/benavoli15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/benavoli15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_benavoli15,http://jmlr.csail.mit.edu/proceedings/papers/v37/benavoli15.html,"A fundamental task in machine learning is to compare the performance of multiple algorithms. This is typically performed by frequentist tests (usually the Friedman test followed by a series of multiple pairwise comparisons). This implies dealing with null hypothesis significance tests and p-values, although the shortcomings of such methods are well known. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. Our derivations show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the analysis of the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, we apply our test to perform algorithms racing, i.e., the problem of identifying the best algorithm among a large set of candidates. We show by simulation that our approach is competitive both in terms of accuracy and speed in identifying the best algorithm."
2345,32,http://jmlr.csail.mit.edu/proceedings/papers/v32/,Nonmyopic \(\epsilon\)-Bayes-Optimal Active Learning of Gaussian Processes,"Trong Nghia Hoang, Bryan Kian Hsiang Low, Patrick Jaillet, Mohan Kankanhalli",none,http://jmlr.csail.mit.edu/proceedings/papers/v32/hoang14.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v32/,http://jmlr.csail.mit.edu/proceedings/papers/v32/hoang14-supp.zip,18th June 2014,"Jun 21, 2014 - Jun 26, 2014 ",ICML 2014 Proceedings,Proceedings of The 31st International Conference on Machine Learning,"Beijing, China","Eric P. Xing, Tony Jebara",v32_hoang14,http://jmlr.csail.mit.edu/proceedings/papers/v32/hoang14.html,"A fundamental issue in active learning of Gaussian processes is that of the exploration-exploitation trade-off. This paper presents a novel nonmyopic \(\epsilon\) -Bayes-optimal active learning ( \(\epsilon\) -BAL) approach that jointly and naturally optimizes the trade-off. In contrast, existing works have primarily developed myopic/greedy algorithms or performed exploration and exploitation separately. To perform active learning in real time, we then propose an anytime algorithm based on \(\epsilon\) -BAL with performance guarantee and empirically demonstrate using synthetic and real-world datasets that, with limited budget, it outperforms the state-of-the-art algorithms."
2346,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Dependent Hierarchical Beta Process for Image Interpolation and Denoising,"Mingyuan Zhou, Hongxia Yang, Guillermo Sapiro, David Dunson, Lawrence Carin","15:883-891, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11a/zhou11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhou11a,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhou11a.html,A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features with covariate-dependent feature usage. The dHBP is applicable to general covariates and data models imposing that signals with similar covariates are likely to be manifested in terms of similar features. Coupling the dHBP with the Bernoulli process and upon marginalizing out the dHBP the model may be interpreted as a covariate-dependent hierarchical Indian buffet process. As applications we consider interpolation and denoising of an image with covariates defined by the location of image patches within an image. Two types of noise models are considered: (i) typical white Gaussian noise; and (ii) spiky noise of arbitrary amplitude distributed uniformly at random. In these examples the features correspond to the atoms of a dictionary learned based upon the data under test (without a priori training data). State-of-the-art performance is demonstrated with efficient inference using hybrid Gibbs Metropolis-Hastings and slice sampling.
2347,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Learning Valuation Functions,"Maria Florina Balcan, Florin Constantin, Satoru Iwata and Lei Wang",none,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12b/balcan12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_balcan12b,http://jmlr.csail.mit.edu/proceedings/papers/v23/balcan12b.html,"A core element of microeconomics and game theory is that consumers have valuation functions over bundles of goods and that these valuations functions drive their purchases. A common assumption is that these functions are subadditive meaning that the value given to a bundle is at most the sum of values on the individual items. In this paper, we provide nearly tight guarantees on the efficient learnability of subadditive valuations. We also provide nearly tight bounds for the subclass of XOS (fractionally subadditive) valuations, also widely used in the literature. We additionally leverage the structure of valuations in a number of interesting subclasses and obtain algorithms with stronger learning guarantees."
2348,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Consistency versus Realizable H-Consistency for Multiclass Classification,"Phil Long, Rocco Servedio",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/long13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/long13-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_long13,http://jmlr.csail.mit.edu/proceedings/papers/v28/long13.html,"A consistent loss function for multiclass classification is one such that for any source of labeled examples, any tuple of scoring functions that minimizes the expected loss will have classification accuracy close to that of the Bayes optimal classifier. While consistency has been proposed as a desirable property for multiclass loss functions, we give experimental and theoretical results exhibiting a sequence of linearly separable data sources with the following property: a multiclass classification algorithm which optimizes a loss function due to Crammer and Singer (which is known not to be consistent) produces classifiers whose expected error goes to 0, while the expected error of an algorithm which optimizes a generalization of the loss function used by LogitBoost (a loss function which is known to be consistent) is bounded below by a positive constant. We identify a property of a loss function, realizable consistency with respect to a restricted class of scoring functions, that accounts for this difference. As our main technical results we show that the Crammer_Singer loss function is realizable consistent for the class of linear scoring functions, while the generalization of LogitBoost is not. Our result for LogitBoost is a special case of a more general theorem that applies to several other loss functions that have been proposed for multiclass classification."
2349,9,http://jmlr.csail.mit.edu/proceedings/papers/v9/,Convexity of Proper Composite Binary Losses,"Mark Reid, Robert Williamson","9:637-644, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v9/reid10a/reid10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v9,,31st March 2010,"May 13-15, 2010",AISTATS 2010 Proceedings,Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,"Chia Laguna Resort, Sardinia, Italy",Yee Whye Teh and Mike Titterington,v9_reid10a,http://jmlr.csail.mit.edu/proceedings/papers/v9/reid10a.html,A composite loss assigns a penalty to a real-valued prediction by associating the prediction with a probability via a link function then applying a class probability estimation (CPE) loss. If the risk for a composite loss is always minimised by predicting the value associated with the true class probability the composite loss is proper. We provide a novel explicit and complete characterisation of the convexity of any proper composite loss in terms of its link and its ``weight function'' associated with its proper CPE loss.
2350,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Cost-sensitive Multiclass Classification Risk Bounds,"Bernardo çvila Pires, Csaba Szepesvari, Mohammad Ghavamzadeh",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/avilapires13.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_avilapires13,http://jmlr.csail.mit.edu/proceedings/papers/v28/avilapires13.html,"A commonly used approach to multiclass classification is to replace the \(0-1\) loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the \(0-1\) excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced –Simplex Coding” scheme."
2351,45,http://jmlr.csail.mit.edu/proceedings/papers/v45/,Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias,"Yohei Kondo, Shin-ichi Maeda, Kohei Hayashi",none,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kondo15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v45/,,25th February 2016,"November 20-22, 2015",ACML 2015 Proceedings,7th Asian Conference on Machine Learning,"Hong Kong, China","Geoffrey Holmes, Tie-Yan Liu",v45_Kondo15,http://jmlr.csail.mit.edu/proceedings/papers/v45/Kondo15.html,"A common strategy for sparse linear regression is to introduce regularization, which eliminates irrelevant features by letting the corresponding weights be zeros. However, regularization often shrinks the estimator for relevant features, which leads to incorrect feature selection. Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a sparse estimation method which imposes no regularization on the weights. The key concept of BM is to introduce binary latent variables that randomly mask features. Estimating the masking rates determines the relevance of the features automatically. We derive a variational Bayesian inference algorithm that maximizes the lower bound of the factorized information criterion (FIC), which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood. In addition, we propose reparametrization to accelerate the convergence of the derived algorithm. Finally, we show that BM outperforms Lasso and automatic relevance determination (ARD) in terms of the sparsity-shrinkage trade-off."
2352,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Reified Context Models,"Jacob Steinhardt, Percy Liang",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardta15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardta15-supp.zip,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_steinhardta15,http://jmlr.csail.mit.edu/proceedings/papers/v37/steinhardta15.html,"A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the choice of factors in a graphical model (the contexts) be random variables inside the model itself. In this sense, the contexts are reified and can be chosen in a data-dependent way. Empirically, we show that our approach obtains expressivity and coverage on three sequence modeling tasks."
2353,22,http://jmlr.csail.mit.edu/proceedings/papers/v22/,Multi-label Subspace Ensemble,"Tianyi Zhou, Dacheng Tao",none,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12a/zhou12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v22/,,21st March 2012,"April 21-23, 2012",AISTATS 2012 Proceedings,Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,"La Palma, Canary Islands",Neil Lawrence and Mark Girolami,v22_zhou12a,http://jmlr.csail.mit.edu/proceedings/papers/v22/zhou12a.html,A challenging problem of multi-label learning is that both the label space and the model complexity will grow rapidly with the increase in the number of labels and thus makes the available training samples insufficient for training a proper model. In this paper we eliminate this problem by learning a mapping of each label in the feature space as a robust subspace and formulating the prediction as finding the group sparse representation of a given instance on the subspace ensemble. We term this approach as ``multi-label subspace ensemble (MSE)''. In the training stage the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization where each low-rank part defines a subspace mapped by a label. In the prediction stage the group sparse representation on the subspace ensemble is estimated by group lasso. Experiments on several benchmark datasets demonstrate the appealing performance of MSE.
2354,37,http://jmlr.csail.mit.edu/proceedings/papers/v37/,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli",none,http://jmlr.csail.mit.edu/proceedings/papers/v37/sohl-dickstein15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v37/,http://jmlr.csail.mit.edu/proceedings/papers/v37/sohl-dickstein15-supp.pdf,1st June 2015,"Jul 6, 2015 - Jul 11, 2015 ",ICML 2015 Proceedings,International Conference on Machine Learning 2015,"Lille, France","Francis Bach, David Blei",v37_sohl-dickstein15,http://jmlr.csail.mit.edu/proceedings/papers/v37/sohl-dickstein15.html,"A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm."
2355,40,http://jmlr.csail.mit.edu/proceedings/papers/v40/,Bad Universal Priors and Notions of Optimality,"Jan Leike, Marcus Hutter",none,http://jmlr.csail.mit.edu/proceedings/papers/v40/Leike15.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v40/,,26th June 2015,"July 3-6, 2015",COLT 2015 Proceedings,Conference on Learning Theory,"Paris, France","Peter GrÕ_nwald, Elad Hazan, Satyen Kale",v40_Leike15,http://jmlr.csail.mit.edu/proceedings/papers/v40/Leike15.html,"A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM."
2356,21,http://jmlr.csail.mit.edu/proceedings/papers/v21/,Marginalizing Out Transition Probabilities for Several Subclasses of PFAs,Chihiro Shibata and Ryo Yoshinaka,"21:259-263, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v21/shibata12a/shibata12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v21/,,16th August 2012,"September 5-8, 2012",ICGI 2012 Proceedings,Proceedings of the Eleventh International Conference on Grammatical Inference,"Maryland, United States","Jeffrey Heinz, Colin de la Higuera, and Tim Oates",v21_shibata12a,http://jmlr.csail.mit.edu/proceedings/papers/v21/shibata12a.html,A Bayesian manner which marginalizes transition probabilities can be generally applied to various kinds of probabilistic finite state machine models. Based on such a Bayesian manner we implemented and compared three algorithms: variable-length gram state merging method for PDFAs and collapsed Gibbs sampling for PFAs. Among those collapsed Gibbs sampling for PFAs performed the best on the data from the pre-competition stage of PAutomaC although it consumes large computation resources.
2357,11,http://jmlr.csail.mit.edu/proceedings/papers/v11/,_TOSS - Multiple hypothesis testing in an open software system,"Gilles Blanchard, Thorsten Dickhaus, Niklas Hack, Frank Konietschke, Kornelius Rohmeyer, Jonathan Rosenblatt, Marsel Scheer and Wiebke Werft","11:12-19, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v11/blanchard10a/blanchard10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v11/,,30th September 2010,"September 1-3, 2010",WAPA 2010 Proceedings,Proceedings of the First Workshop on Applications of Pattern Analysis,"Cumberland Lodge, Windsor, UK","Tom Diethe, Nello Cristianini, and John Shawe-Taylor",v11_blanchard10a,http://jmlr.csail.mit.edu/proceedings/papers/v11/blanchard10a.html,"_TOSS is an R package providing an open source, easy-to-extend platform for multiple hypothesis testing (MHT), one of the most active research fields in statistics over the last 10-15 years. Its first motivation is to establish a common platform and standardization for MHT procedures at large. The _TOSS software has been designed and written in the framework of a ""Harvest Programme"" call of the PASCAL2 European research network. Basically, it consists of the two R packages mutoss and mutossGUI. For researchers, it features a convenient unification of interfaces for MHT procedures (including standardized functions to access existing specific MHT R packages such as multtest and multcomp, as well as recent MHT procedures that are not available elsewhere) and helper functions facilitating the setup of benchmark simulations for comparison of competing methods. For end users, a graphical user interface and an online userês guide for finding appropriate methods for a given specification of the multiple testing problem is included. Ongoing maintenance and subsequent extensions will aim at establishing _TOSS as a state of the art in statistical computing for MHT."
2358,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Dissimilarity in Graph-Based Semi-Supervised Classification,"Andrew B. Goldberg, Xiaojin Zhu, Stephen Wright","2:155-162, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/goldberg07a/goldberg07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_goldberg07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/goldberg07a.html,"Label dissimilarity specifies that a pair of examples probably have different class labels. We present a semi-supervised classification algorithm that learns from dissimilarity and similarity information on labeled and unlabeled data. Our approach uses a novel graphbased encoding of dissimilarity that results in a convex problem, and can handle both binary and multiclass classification. Experiments on several tasks are promising."
2359,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Information Retrieval by Inferring Implicit Queries from Eye Movements,"David R. Hardoon, John Shawe-Taylor, Antti Ajanki, Kai Puolam_ki, Samuel Kaski","2:179-186, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/hardoon07a/hardoon07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_hardoon07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/hardoon07a.html,"We introduce a new search strategy, in which the information retrieval (IR) query is inferred from eye movements measured when the user is reading text during an IR task. In training phase, we know the users' interest, that is, the relevance of training documents. We learn a predictor that produces a ""query"" given the eye movements; the target of learning is an ""optimal"" query that is computed based on the known relevance of the training documents. Assuming the predictor is universal with respect to the users' interests, it can also be applied to infer the implicit query when we have no prior knowledge of the users' interests. The result of an empirical study is that it is possible to learn the implicit query from a small set of read documents, such that relevance predictions for a large set of unseen documents are ranked significantly better than by random guessing."
2360,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Incorporating Prior Knowledge on Features into Learning,"Eyal Krupka, Naftali Tishby","2:227-234, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/krupka07a/krupka07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_krupka07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/krupka07a.html,"In the standard formulation of supervised learning the input is represented as a vector of features. However, in most real-life problems, we also have additional information about each of the features. This information can be represented as a set of properties, referred to as meta-features. For instance, in an image recognition task, where the features are pixels, the meta-features can be the (x, y) position of each pixel. We propose a new learning framework that incorporates meta- features. In this framework we assume that a weight is assigned to each feature, as in linear discrimination, and we use the meta-features to define a prior on the weights. This prior is based on a Gaussian process and the weights are assumed to be a smooth function of the meta-features. Using this framework we derive a practical algorithm that improves gen- eralization by using meta-features and discuss the theoretical advantages of incorporating them into the learning. We apply our framework to design a new kernel for hand-written digit recognition. We obtain higher accuracy with lower computational complexity in the primal representation. Finally, we discuss the applicability of this framework to biological neural networks."
2361,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fast Low-Rank Semidefinite Programming for Embedding and Clustering,"Brian Kulis, Arun C. Surendran, John C. Platt","2:235-242, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/kulis07a/kulis07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_kulis07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/kulis07a.html,"Many non-convex problems in machine learning such as embedding and clustering have been solved using convex semidefinite relaxations. These semidefinite programs (SDPs) are expensive to solve and are hence limited to run on very small data sets. In this paper we show how we can improve the quality and speed of solving a number of these problems by casting them as low-rank SDPs and then directly solving them using a nonconvex optimization algorithm. In particular, we show that problems such as the k-means clustering and maximum variance unfolding (MVU) may be expressed exactly as low-rank SDPs and solved using our approach. We demonstrate that in the above problems our approach is significantly faster, far more scalable and often produces better results compared to traditional SDP relaxation techniques."
2362,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Treelets | A Tool for Dimensionality Reduction and Multi-Scale Analysis of Unstructured Data,"Ann B. Lee, Boaz Nadler","2:259-266, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/lee07a/lee07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_lee07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/lee07a.html,"In many modern data mining applications, such as analysis of gene expression or worddocument data sets, the data is highdimensional with hundreds or even thousands of variables, unstructured with no specific order of the original variables, and noisy. Despite the high dimensionality, the data is typically redundant with underlying structures that can be represented by only a few features. In such settings and specifically when the number of variables is much larger than the sample size, standard global methods may not perform well for common learning tasks such as classification, regression and clustering. In this paper, we present treelets -- a new tool for multi-resolution analysis that extends wavelets on smooth signals to general unstructured data sets. By construction, treelets provide an orthogonal basis that reflects the internal structure of the data. In addition, treelets can be useful for feature selection and dimensionality reduction prior to learning. We give a theoretical analysis of our algorithm for a linear mixture model, and present a variety of situations where treelets outperform classical principal component analysis, as well as variable selection schemes such as supervised (sparse) PCA."
2363,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Recall Systems: Effcient Learning and Use of Category Indices,"Omid Madani, Wiley Greiner, David Kempe, Mohammad R. Salavatipour","2:307-314, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/madani07a/madani07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_madani07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/madani07a.html,"We introduce the framework of recall systems for efficient learning and retrieval of categories when the number of categories is large. A recallsystem here is a simple feature-based intermediate filtering step which reduces the potential categories for an instance to a small manageable set. The correct categories from this set can then be determined using traditional classifiers. We present a formalization of the index learning problem and establish NP-hardness and approximation hardness. We proceed to give an efficient heuristic for learning indices, and evaluate it on several large data sets. In our experiments, the index is learned within minutes, and reduces the number of categories by several orders of magnitude, without affecting the quality of classification overall."
2364,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Inductive Transfer for Bayesian Network Structure Learning,"Alexandru Niculescu-Mizil, Rich Caruana","2:339-346, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/niculescu-mizil07a/niculescu-mizil07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_niculescu-mizil07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/niculescu-mizil07a.html,"We consider the problem of learning Bayes Net structures for related tasks. We present an algorithm for learning Bayes Net structures that takes advantage of the similarity between tasks by biasing learning toward similar structures for each task. Heuristic search is used to find a high scoring set of structures (one for each task), where the score for a set of structures is computed in a principled way. Experiments on problems generated from the ALARM and INSURANCE networks show that learning the structures for related tasks using the proposed method yields better results than learning the structures independently."
2365,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Approximate Counting of Graphical Models Via MCMC,Jose M. Pe_a,"2:355-362, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/pena07a/pena07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_pena07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/pena07a.html,"We apply MCMC to approximately calculate (i) the ratio of directed acyclic graph (DAG) models to DAGs for up to 20 nodes, and (ii) the fraction of chain graph (CG) models that are neither undirected graph (UG) models nor DAG models for up to 13 nodes. Our results suggest that, for the numbers of nodes considered, (i) the ratio of DAG models to DAGs is not very low, (ii) the ratio of DAG models to UG models is very high, (iii) the fraction of CG models that are neither UG models nor DAG models is rather high, and (iv) the ratio of CG models to CGs is rather low. Therefore, our results suggest that (i) when learning DAG/CG models, searching the space of DAG/CG models instead of the space of DAGs/CGs can result in a moderate/considerable gain in efficiency, and (ii) learning a CG model instead of an UG model or DAG model can result in a substantially better fit of the learning data."
2366,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Continuous Neural Networks,"Nicolas Le Roux, Yoshua Bengio","2:404-411, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/leroux07a/leroux07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_leroux07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/leroux07a.html,"This article extends neural networks to the case of an uncountable number of hidden units, in several ways. In the first approach proposed, a finite parametrization is possible, allowing gradient-based learning. While having the same number of parameters as an ordinary neural network, its internal structure suggests that it can represent some smooth functions much more compactly. Under mild assumptions, we also find better error bounds than with ordinary neural networks. Furthermore, this parametrization may help reducing the problem of saturation of the neurons. In a second approach, the input-to-hidden weights are fully nonparametric, yielding a kernel machine for which we demonstrate a simple kernel formula. Interestingly, the resulting kernel machine can be made hyperparameter-free and still generalizes in spite of an absence of explicit regularization."
2367,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Memory-Effcient Orthogonal Least Squares Kernel Density Estimation using Enhanced Empirical Cumulative Distribution Functions,"Martin Schaffoner, Edin Andelic, Marcel Katz, Sven E. Krôger, Andreas Wendemuth","2:428-435, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/schaffoner07a/schaffoner07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_schaffoner07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/schaffoner07a.html,"A novel training algorithm for sparse kernel density estimates by regression of the empirical cumulative density function (ECDF) is presented. It is shown how an overdetermined linear least-squares problem may be solved by a greedy forward selection procedure using updates of the orthogonal decomposition in an order-recursive manner. We also present a method for improving the accuracy of the estimated models which uses output-sensitive computation of the ECDF. Experiments show the superior performance of our proposed method compared to stateof-the-art density estimation methods such as Parzen windows, Gaussian Mixture Models, and $\epsilon$-Support Vector Density models [1]."
2368,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Bayesian Inference and Optimal Design in the Sparse Linear Model,"Matthias Seeger, Florian Steinke, Koji Tsuda","2:444-451, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/seeger07a/seeger07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_seeger07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/seeger07a.html,"The sparse linear model has seen many successful applications in Statistics, Machine Learning, and Computational Biology, such as identification of gene regulatory networks from micro-array expression data. Prior work has either approximated Bayesian inference by expensive Markov chain Monte Carlo, or replaced it by point estimation. We show how to obtain a good approximation to Bayesian analysis efficiently, using the Expectation Propagation method. We also address the problems of optimal design and hyperparameter estimation. We demonstrate our framework on a gene network identification task."
2369,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,A Unified Algorithmic Approach for Efficient Online Label Ranking,"Shai Shalev-Shwartz, Yoram Singer","2:452-459, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/shalev-shwartz07a/shalev-shwartz07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_shalev-shwartz07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/shalev-shwartz07a.html,"Label ranking is the task of ordering labels with respect to their relevance to an input instance. We describe a unified approach for the online label ranking task. We do so by casting the online learning problem as a game against a competitor who receives all the examples in advance and sets its label ranker to be the optimal solution of a constrained optimization problem. This optimization problem consists of two terms: the empirical label-ranking loss of the competitor and a complexity measure of the competitor's ranking function. We then describe and analyze a framework for online label ranking that incrementally ascends the dual problem corresponding to the competitor's optimization problem. The generality of our framework enables us to derive new online update schemes. In particular, we use the relative entropy as a complexity measure to derive efficient multiplicative algorithms for the label ranking task. Depending on the specific form of the instances, the multiplicative updates either have a closed form or can be calculated very efficiently by tailoring an interior point procedure to the label ranking task. We demonstrate the potential of our approach in a few experiments with email categorization tasks."
2370,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fast Kernel ICA using an Approximate Newton Method,"Hao Shen, Stefanie Jegelka, Arthur Gretton","2:476-483, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/shen07a/shen07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_shen07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/shen07a.html,"Recent approaches to independent component analysis (ICA) have used kernel independence measures to obtain very good performance, particularly where classical methods experience difficulty (for instance, sources with near-zero kurtosis). We present fast kernel ICA (FastKICA), a novel optimisation technique for one such kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). Our search procedure uses an approximate Newton method on the special orthogonal group, where we estimate the Hessian locally about independence. We employ incomplete Cholesky decomposition to efficiently compute the gradient and approximate Hessian. FastKICA results in more accurate solutions at a given cost compared with gradient descent, and is relatively insensitive to local minima when initialised far from independence. These properties allow kernel approaches to be extended to problems with larger numbers of sources and observations. Our method is competitive with other modern and classical ICA approaches in both speed and accuracy."
2371,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Ellipsoidal Machines,"Pannagadatta K. Shivaswamy, Tony Jebara","2:484-491, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/shivaswamy07a/shivaswamy07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_shivaswamy07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/shivaswamy07a.html,"A novel technique is proposed for improving the standard Vapnik-Chervonenkis (VC) dimension estimate for the Support Vector Machine (SVM) framework. The improved VC estimates are based on geometric arguments. By considering bounding ellipsoids instead of the usual bounding hyperspheres and assuming gap-tolerant classifiers, a linear classifier with a given margin is shown to shatter fewer points than previously estimated. This improved VC estimation method directly motivates a different estimator for the parameters of a linear classifier. Surprisingly, only VC-based arguments are needed to justify this modification to the SVM. The resulting technique is implemented using Semidefinite Programming (SDP) and is solvable in polynomial time. The new linear classifier also ensures certain invariances to affine transformations on the data which a standard SVM does not provide. We demonstrate that the technique can be kernelized via extensions to Hilbert spaces. Promising experimental results are shown on several standardized datasets."
2372,2,http://jmlr.csail.mit.edu/proceedings/papers/v2/,Fast State Discovery for HMM Model Selection and Learning,"Sajid M. Siddiqi, Geogrey J. Gordon, Andrew W. Moore","2:492-499, 2007.",http://jmlr.csail.mit.edu/proceedings/papers/v2/siddiqi07a/siddiqi07a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v2/,,27th October 2007,"March 21-24, 2007",AISTATS 2007 Proceedings,Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,"San Juan, Puerto Rico",Marina Meila and Xiaotong Shen,v2_siddiqi07a,http://jmlr.csail.mit.edu/proceedings/papers/v2/siddiqi07a.html,"Choosing the number of hidden states and their topology (model selection) and estimating model parameters (learning) are important problems for Hidden Markov Models. This paper presents a new state-splitting algorithm that addresses both these problems. The algorithm models more information about the dynamic context of a state during a split, enabling it to discover underlying states more effectively. Compared to previous top-down methods, the algorithm also touches a smaller fraction of the data per split, leading to faster model search and selection. Because of its efficiency and ability to avoid local minima, the state-splitting approach is a good way to learn HMMs even if the desired number of states is known beforehand. We compare our approach to previous work on synthetic data as well as several real-world data sets from the literature, revealing significant improvements in efficiency and test-set likelihoods. We also compare to previous algorithms on a sign-language recognition task, with positive results."
2373,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Design and Analysis of the Causation and Prediction Challenge,"Isabelle Guyon, Constantin Aliferis, Greg Cooper, Andr_ Elisseeff, Jean-Philippe Pellet,æ Peter Spirtes, and Alexander Statnikov","3:1-33, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v3/guyon08a/guyon08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_guyon08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/guyon08a.html,"We organized for WCCI 2008 a challenge to evaluate causal modeling techniques, focusing on predicting the effect of ""interventions"" performed by an external agent. Examples of that problem are found in the medical domain to predict the effect of a drug prior to administering it, or in econometrics to predict the effect of a new policy prior to issuing it. We concentrate on a given target variable to be predicted (e.g., health status of a patient) from a number of candidate predictive variables or ""features"" (e.g., risk factors in the medical domain). Under interventions, variable predictive power and causality are tied together. For instance, both smoking and coughing may be predictive of lung cancer (the target) in the absence of external intervention; however, prohibiting smoking (a possible cause) may prevent lung cancer, but administering a cough medicine to stop coughing (a possible consequence) would not. We propose four tasks from various application domains, each dataset including a training set drawn from a ""natural"" distribution and three test sets: one from the same distribution as the training set and two corresponding to data drawn when an external agent is manipulating certain variables. The goal is to predict a binary target variable, whose values on test data are withheld. The participants were asked to provide predictions of the target variable on test data and the list of variables (features) used to make predictions. The challenge platform remains open for post-challenge submissions and the organization of other events is under way (see http://clopinet.com/causality)."
2374,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,A Strategy for Making Predictions Under Manipulation,Laura E. Brown and Ioannis Tsamardinos,"3:35-52, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v3/brown08a/brown08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_brown08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/brown08a.html,"The first Causality Challenge competition posted several causal discovery problems that require researchers to employ the full arsenal of state-of-the-art causal discovery methods, while prompting the development of new ones. Our approach used the formalism of Causal Bayesian Networks to model and induce causal relations and to make predictions about the effects of the manipulation of the variables. Using state-of-the-art, under development, or newly invented methods specifically for the purposes of the competition, we addressed the following problems in turn in order to build and evaluate a model: (a) finding the Markov Blanket of the target even under some non-faithfulness conditions (e.g., parity functions), (b) reducing the problems to a size manageable by subsequent algorithms, (c) identifying and orienting the network edges, (d) identifying causal edges (i.e., not confounded), and (e) selecting the causal Markov Blanket of the target in the manipulated distribution. The results of the competition illustrate some of the strengths and weaknesses of the state-of-the-art of causal discovery methods and point to new directions in the field. An implementation of our approach is available at http://www.dsl-lab.org for use by other researchers."
2375,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Feature Ranking Using Linear SVM,Yin-Wen Chang and Chih-Jen Lin,"3:53-64, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v3/chang08a.html,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_chang08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/chang08a.html,"Feature ranking is useful to gain knowledge of data and identify relevant features. This article explores the performance of combining linear support vector machines with various feature ranking methods, and reports the experiments conducted when participating the Causality Challenge. Experiments show that a feature ranking using weights from linear SVM models yields good performances, even when the training and testing data are not identically distributed. Checking the difference of Area Under Curve (AUC) with and without removing each feature also gives similar rankings. Our study indicates that linear SVMs with simple feature rankings are effective on data sets in the Causality Challenge."
2376,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Random Sets Approach and its Applications,Vladimir Nikulin,"3:65-76, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v3/nikulin08a.html,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_nikulin08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/nikulin08a.html,"The random sets approach is heuristic in nature and has been inspired by the growing speed of computations. For example, we can consider a large number of classifiers where any single classifier is based on a relatively small subset of randomly selected features or random sets of features. Using cross-validation we can rank all random sets according to the selected criterion, and use this ranking for further feature selection. Another application of random sets was motivated by the huge imbalanced data, which represent significant problem because the corresponding classifier has a tendency to ignore patterns with smaller representation in the training set. Again, we propose to consider a large number of balanced training subsets where representatives from both patterns are selected randomly. The above models demonstrated competitive results in two data mining competitions."
2377,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Bernoulli Mixture Models for Markov Blanket Filtering and Classification,Mehreen Saeed,"æ3:77:91 , 2008",http://jmlr.csail.mit.edu/proceedings/papers/v3/saeed08a/saeed08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_saeed08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/saeed08a.html,"This paper presents the use of Bernoulli mixture models for Markov blanket filtering and classification of binary data. Bernoulli mixture models can be seen as a tool for partitioning an n-dimensional hypercube, identifying regions of high data density on the corners of the hypercube. Once Bernoulli mixture models are computed from a training dataset we use them for determining the Markov blanket of the target variable. An algorithm for Markov blanket filtering was proposed by Koller and Sahami (1996), which is a greedy search method for feature subset selection and it outputs an approximation to the optimal feature selection criterion. However, they use the entire training instances for computing the conditioning sets and have to limit the size of these sets for computational efficiency and avoiding data fragmentation. We have adapted their algorithm to use Bernoulli mixture models instead, hence, overcoming the short comings of their algorithm and increasing the efficiency of this algorithm considerably. Once a feature subset is identified we perform classification using these mixture models. We have applied this algorithm to the causality challenge datasets. Our prediction scores were ranked fourth on SIDO and our feature scores were ranked the best for test sets 1 and 2 of the same dataset."
2378,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Partial orientation and local structural learning of causal networks for prediction,"Jianxin Yin, You Zhou, Changzhang Wang, Ping He, Cheng Zheng, and Zhi Geng","3:93-105, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v3/yin08a/yin08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_yin08a,http://jmlr.csail.mit.edu/proceedings/papers/v3/yin08a.html,"For a prediction problem of a given target feature in a large causal network under external interventions, we propose in this paper two partial orientation and local structural learning (POLSL) approaches, Local-Graph and PCD-by-PCD (where PCD denotes Parents, Children and some Descendants). The POLSL approaches are used to discover the local structure of the target and to orient edges connected to the target without discovering a global causal network. Thus they can greatly reduce computational complexity of structural learning and improve power of statistical tests. This approach is stimulated by the challenge problems proposed in IEEE World Congress on Computational Intelligence (WCCI2008) competition workshop. For the cases with and without external interventions, we select different feature sets to build prediction models. We apply the L1 penalized logistic regression model to the prediction. For the case with noise and calibrant features in microarray data, we propose a two-stage filter to correct global and local patterns of noise."
2379,3,http://jmlr.csail.mit.edu/proceedings/papers/v3/,Causal and Non-Causal Feature Selection for Ridge Regression,Gavin C. Cawley,"3:107-128, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v3/cawley09a/cawley09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v3/,,31st December 2008,"June 1-6, 2008",Causation and Prediction Challenge WCCI 2008,Causation and Prediction Challenge at WCCI 2008,"Hong Kong, China","Isabelle Guyon, Constantin Aliferis, Greg Cooper, AndrÕ© Elisseeff, J.-P. Pellet, P. Spirtes, and A. Statnikov.",v3_cawley09a,http://jmlr.csail.mit.edu/proceedings/papers/v3/cawley09a.html,"In this paper we investigate the use of causal and non-causal feature selection methods for linear classifiers in situations where the causal relationships between the input and response variables may differ between the training and operational data. The causal feature selection methods investigated include inference of the Markov Blanket and inference of direct causes and of direct effects. The non-causal feature selection method is based on logistic regression with Bayesian regularisation using a Laplace prior. A simple ridge regression model is used as the base classifier, where the ridge parameter is efficiently tuned so as to minimise the leave-one-out error, via eigen-decomposition of the data covariance matrix. For tasks with more features than patterns, linear kernel ridge regression is used for computational efficiency. Results are presented for all of the WCCI-2008 Causation and Prediction Challenge datasets, demonstrating that, somewhat surprisingly, causal feature selection procedures do not provide significant benefits in terms of predictive accuracy over non-causal feature selection and/or classification using the entire feature set."
2380,4,http://jmlr.csail.mit.edu/proceedings/papers/v4/,Unsupervised feature selection applied to SPOT5 satellite images indexing,"Marine Campedel, Ivan Kyrgyzov, Henri Maitre","4:48-59, 2008.",http://jmlr.csail.mit.edu/proceedings/papers/v4/campedel08a/campedel08a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v4/,,11th September 2008,"September 15, 2008,",New Challenges for Feature Selection in Data Mining and Knowledge Discovery,New challenges for feature selection in data mining and knowledge discovery,"Antwerp, Belgium","Yvan Saeys, Huan Liu, IÕ±aki Inza, Louis Wehenkel and Yves Van de Peer",v4_campedel08a,http://jmlr.csail.mit.edu/proceedings/papers/v4/campedel08a.html,"Satellite images are numerous and weakly exploited: it is urgent to develop efficient and fast indexing algorithms to facilitate their access. In order to determinate the best features to be extracted, we propose a methodology based on automatic feature selection algorithms, applied unsupervisingly on a strongly redundant features set. In this article we also demonstrate the usefulness of consensus clustering as a feature selection algorithm, allowing selected number of features estimation and exploration facilities. The efficiency of our approach is demonstrated on SPOT5 images."
2381,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Sparse Probabilistic Principal Component Analysis,"Yue Guan, Jennifer Dy","5:185-192, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a/guan09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_guan09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a.html,"Principal component analysis (PCA) is a popular dimensionality reduction algorithm. However, it is not easy to interpret which of the original features are important based on the principal components. Recent methods improve interpretability by sparsifying PCA through adding an L1 regularizer. In this paper, we introduce a probabilistic formulation for sparse PCA. By presenting sparse PCA as a probabilistic Bayesian formulation, we gain the benet of automatic model selection. We examine three dierent priors for achieving sparsication: (1) a two-level hierarchical prior equivalent to a Laplacian distribution and consequently to an L1 regularization, (2) an inverse-Gaussian prior, and (3) a Jerey's prior. We learn these models by applying variational inference. Our experiments verify that indeed our sparse probabilistic model results in a sparse PCA solution."
2382,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Network Completion and Survey Sampling,"Steve Hanneke, Eric P. Xing","5:209-215, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/hanneke09a/hanneke09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_hanneke09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/hanneke09a.html,"We study the problem of learning the topology of an undirected network by observing a random subsample. Specifically, the sample is chosen by randomly selecting a fixed number of vertices, and for each we are allowed to observe all edges it is incident with. We analyze a general formalization of learning from such samples, and derive confidence bounds on the number of differences between the true and learned topologies, as a function of the number of observed mistakes and the algorithm's bias. In addition to this general analysis, we also analyze a variant of the problem under a stochastic block model assumption."
2383,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Particle Belief Propagation,"Alexander Ihler, David McAllester","5:256-263, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/ihler09a/ihler09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_ihler09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/ihler09a.html,"The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sample--based versions of belief propagation (BP) for more general (tree--structured or loopy) graphs. Already, several such algorithms have been proposed in the literature. However, many questions remain open about the behavior of particle--based BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods. We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finite-sample behavior and give a convergence rate for the algorithm on tree--structured graphs. Our convergence rate is $O(1/\sqrt{n})$ where $n$ is the number of samples, independent of the domain size of the variables."
2384,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Data Biased Robust Counter Strategies,"Michael Johanson, Michael Bowling","5:264-271, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/johanson09a/johanson09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_johanson09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/johanson09a.html,"The problem of exploiting information about the environment while still being robust to inaccurate or incomplete information arises in many domains. Competitive imperfect information games where the goal is to maximally exploit an unknown opponent's weaknesses are an example of this problem. Agents for these games must balance two objectives. First, they should aim to exploit data from past interactions with the opponent, seeking a best-response counter strategy. Second, they should aim to minimize losses since the limited data may be misleading or the opponent's strategy may have changed, suggesting an opponent-agnostic Nash equilibrium strategy. In this paper, we show how to partially satisfy both of these objectives at the same time, producing strategies with favorable tradeoffs between the ability to exploit an opponent and the capacity to be exploited. Like a recently published technique, our approach involves solving a modified game; however the result is more generally applicable and even performs well in situations with very limited data. We evaluate our technique in the game of two-player, Limit Texas Hold'em."
2385,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Sampling Techniques for the Nystrom Method,"Sanjiv Kumar, Mehryar Mohri, Ameet Talwalkar","5:304-311, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/kumar09a/kumar09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_kumar09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/kumar09a.html,"The Nystrom method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the distribution according to which columns are sampled from the original matrix. In this work, we present an analysis of different sampling techniques for the Nystrom method. Our analysis includes both empirical and theoretical components. We first present novel experiments with several real world datasets, comparing the performance of the Nystrom method when used with uniform versus non-uniform sampling distributions. Our results suggest that uniform sampling without replacement, in addition to being more efficient both in time and space, produces more effective approximations. This motivates the theoretical part of our analysis which gives the first performance bounds for the Nystrom method precisely when used with uniform sampling without replacement."
2386,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Latent Wishart Processes for Relational Kernel Learning,"Wu-Jun Li, zhihua zhang, Dit-Yan Yeung","5:336-343, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/li09b/li09b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_li09b,http://jmlr.csail.mit.edu/proceedings/papers/v5/li09b.html,"In this paper, we propose a novel relational kernel learning model based on latent Wishart processes (LWP) to learn the kernel function for relational data. This is done by seamlessly integrating the relational information and the input attributes into the kernel learning process. Through extensive experiments on diverse real-world applications, we demonstrate that our LWP model can give very promising performance in practice."
2387,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,PAC-Bayes Analysis Of Maximum Entropy Classification,"John Shawe-Taylor, David Hardoon","5:480-487, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/shawe-taylor09a/shawe-taylor09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_shawe-taylor09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/shawe-taylor09a.html,We extend and apply the PAC-Bayes theorem to the analysis of maximum entropy learning by considering maximum entropy classification. The theory introduces a multiple sampling technique that controls an effective margin of the bound. We further develop a dual implementation of the convex optimisation that optimises the bound. This algorithm is tested on some simple datasets and the value of the bound compared with the test error.
2388,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Hash Kernels,"Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, Alex Strehl, Vishy Vishwanathan","5:496-503, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/shi09a/shi09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_shi09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/shi09a.html,"We propose hashing to facilitate efficient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs."
2389,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Markov Topic Models,"Chong Wang, Bo Thiesson, Chris Meek, David Blei","5:583-590, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09b/wang09b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_wang09b,http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09b.html,"We develop Markov topic models (MTMs), a novel family of generative graphical models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora. MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora. We derive an efficient estimation procedure with variational expectation-maximization. We study the performance of our models on a corpus of abstracts from six different computer science conferences. Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art."
2390,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,An Information Geometry Approach for Distance Metric Learning,"Shijun Wang, Rong Jin","5:591-598, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09c/wang09c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_wang09c,http://jmlr.csail.mit.edu/proceedings/papers/v5/wang09c.html,"Metric learning is an important problem in machine learning and pattern recognition. In this paper, we propose a framework for metric learning based on information geometry. The key idea is to construct two kernel matrices for the given training data: one is based on the distance metric and the other is based on the assigned class labels. Inspired by the idea of information geometry, we relate these two kernel matrices to two Gaussian distributions, and the difference between the two kernel matrices is then computed by the Kullback-Leibler (KL) divergence between the two Gaussian distributions. The optimal distance metric is then found by minimizing the divergence between the two distributions. Based on this idea, we present two metric learning algorithms, one for linear distance metric and the other for nonlinear distance with the introduction of a kernel function. Unlike many existing algorithms for metric learning that require solving a non-trivial optimization problem and therefore are computationally expensive when the data dimension is high, the proposed algorithms have a closed-form solution and are computationally more efficient. Extensive experiments with data classification and face recognition show that the proposed algorithms are comparable to or better than the state-of-the-art algorithms for metric learning."
2391,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Speed and Sparsity of Regularized Boosting,"Yongxin Xi, Zhen Xiang, Peter Ramadge, Robert Schapire","5:615-622, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/xi09a/xi09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_xi09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/xi09a.html,"Boosting algorithms with l1 regularization are of interest because l1 regularization leads to sparser composite classifiers. Moreover, Rosset et al. have shown that for separable data, standard lp regularized loss minimization results in a margin maximizing classifier in the limit as regularization is relaxed. For the case p=1, we extend these results by obtaining explicit convergence bounds on the regularization required to yield a margin within prescribed accuracy of the maximum achievable margin. We derive similar rates of convergence for the epsilon AdaBoost algorithm, in the process providing a new proof that epsilon AdaBoost is margin maximizing as epsilon converges to 0. Because both of these known algorithms are computationally expensive, we introduce a new hybrid algorithm, AdaBoost+L1, that combines the virtues of AdaBoost with the sparsity of l1 regularization in a computationally efficient fashion. We prove that the algorithm is margin maximizing and empirically examine its performance on five datasets."
2392,5,http://jmlr.csail.mit.edu/proceedings/papers/v5/,Tree-Based Inference for Dirichlet Process Mixtures,"Yang Xu, Katherine Heller, Zoubin Ghahramani","5:623-630, 2009.",http://jmlr.csail.mit.edu/proceedings/papers/v5/xu09a/xu09a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v5/,,15th April 2009,"April 16-18, 2009",AISTATS 2009 Proceedings,Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,"Clearwater Beach, Florida ,USA",David van Dyk and Max Welling,v5_xu09a,http://jmlr.csail.mit.edu/proceedings/papers/v5/xu09a.html,"The Dirichlet process mixture (DPM) is a widely used model for clustering and for general nonparametric Bayesian density estimation. Unfortunately, like in many statistical models, exact inference in a DPM is intractable, and approximate methods are needed to perform efficient inference. While most attention in the literature has been placed on Markov chain Monte Carlo (MCMC), variational Bayesian (VB) and collapsed variational methods, \cite recently introduced a novel class of approximation for DPMs based on Bayesian hierarchical clustering (BHC). These tree-based combinatorial approximations efficiently sum over exponentially many ways of partitioning the data and offer a novel lower bound on the marginal likelihood of the DPM. In this paper we make the following contributions: (1) We show empirically that the BHC lower bounds are substantially tighter than the bounds given by VB and by collapsed variational methods on synthetic and real datasets. (2) We also show that BHC offers a more accurate predictive performance on these datasets. (3) We further improve the tree-based lower bounds with an algorithm that efficiently sums contributions from alternative trees. (4) We present a fast approximate method for BHC. Our results suggest that our combinatorial approximate inference methods and lower bounds may be useful not only in DPMs but in other models as well."
2393,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Causal Inference,Judea Pearl,"6:39-58, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/pearl10a/pearl10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_pearl10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/pearl10a.html,"This paper reviews a theory of causal inference based on the Structural Causal Model (SCM) described in Pearl (2000a). The theory unifies the graphical, potential-outcome (Neyman-Rubin), decision analytical, and structural equation approaches to causation, and provides both a mathematical foundation and a friendly calculus for the analysis of causes and counterfactuals. In particular, the paper establishes a methodology for inferring (from a combination of data and assumptions) the answers to three types of causal queries: (1) queries about the effect of potential interventions, (2) queries about counterfactuals, and (3) queries about the direct (or indirect) effect of one event on another."
2394,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Beware of the DAG!,A. Philip Dawid,"6:59-86, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/dawid10a/dawid10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_dawid10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/dawid10a.html,"Directed acyclic graph (DAG) models are popular tools for describing causal relationships and for guiding attempts to learn them from data. They appear to supply a means of extracting causal conclusions from probabilistic conditional independence properties inferred from purely observational data. I take a critical look at this enterprise, and suggest that it is in need of more, and more explicit, methodological and philosophical justification than it typically receives. In particular, I argue for the value of a clean separation between formal causal language and intuitive causal assumptions."
2395,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Causal Discovery as a Game,Frederick Eberhardt,"6:87-96, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/eberhardt10a/eberhardt10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_eberhardt10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/eberhardt10a.html,"This paper presents a game theoretic approach to causal discovery. The problem of causal discovery is framed as a game of the Scientist against Nature, in which Nature attempts to hide its secrets for as long as possible, and the Scientist makes her best effort at discovery while minimizing cost. This approach provides a very general framework for the assessment of different search procedures and a principled way of modeling the effect of choices between different experiments."
2396,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Sparse Causal Discovery in Multivariate Time Series,"Stefan Haufe, Klaus-Robert MÙller, Guido Nolte, Nicole Kr_mer","6:97-106, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/haufe10a/haufe10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_haufe10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/haufe10a.html,"Our goal is to estimate causal interactions in multivariate time series. Using vector autoregressive (VAR) models, these can be defined based on non-vanishing coefficients belonging to respective time-lagged instances. As in most cases a parsimonious causality structure is assumed, a promising approach to causal discovery consists in fitting VAR models with an additional sparsity-promoting regularization. Along this line we here propose that sparsity should be enforced for the subgroups of coefficients that belong to each pair of time series, as the absence of a causal relation requires the coefficients for all time-lags to become jointly zero. Such behavior can be achieved by means of l1,2-norm regularized regression, for which an efficient active set solver has been proposed recently. Our method is shown to outperform standard methods in recovering simulated causality graphs. The results are on par with a second novel approach which uses multiple statistical testing."
2397,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Inference of Graphical Causal Models: Representing the Meaningful Information of Probability Distributions,"Jan Lemeire, Kris Steenhaut","6:107-120, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/lemeire10a/lemeire10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_lemeire10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/lemeire10a.html,"This paper studies the feasibility and interpretation of learning the causal structure from observational data with the principles behind the Kolmogorov Minimal Sufficient Statistic (KMSS). The KMSS provides a generic solution to inductive inference. It states that we should seek for the minimal model that captures all regularities of the data. The conditional independencies following from the system's causal structure are the regularities incorporated in a graphical causal model. The meaningful information provided by a Bayesian network corresponds to the decomposition of the description of the system into Conditional Probability Distributions (CPDs). The decomposition is described by the Directed Acyclic Graph (DAG). For a causal interpretation of the DAG, the decomposition should imply modularity of the CPDs. The CPDs should match up with independent parts of reality that can be changed independently. We argue that if the shortest description of the joint distribution is given by separate descriptions of the conditional distributions for each variable given its effects, the decomposition given by the DAG should be considered as the top-ranked causal hypothesis. Even when the causal interpretation is faulty, it serves as a reference model. Modularity becomes, however, implausible if the concatenation of the description of some CPDs is compressible. Then there might be a kind of meta-mechanism governing some of the mechanisms or either a single mechanism responsible for setting the state of multiple variables."
2398,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Bayesian Algorithms for Causal Data Mining,"Subramani Mani, Constantin F. Aliferis, Alexander Statnikov","6:121-136, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/mani10a/mani10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_mani10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/mani10a.html,"We present two Bayesian algorithms CD-B and CD-H for discovering unconfounded cause and effect relationships from observational data without assuming causal sufficiency which precludes hidden common causes for the observed variables. The CD-B algorithm first estimates the Markov blanket of a nodeæXæusing a Bayesian greedy search method and then applies Bayesian scoring methods to discriminate the parents and children ofæX. Using the set of parents and set of children CD-B constructs a global Bayesian network and outputs the causal effects of a nodeæXæbased on the identification of Y arcs. Recall that if a nodeæXæhas two parent nodesæA, Bæand a child nodeæCæsuch that there is no arc betweenæA, BæandæA, Bæare not parents ofæC, then the arc fromæXætoæCæis called a Y arc. The CD-H algorithm uses the MMPC algorithm to estimate the union of parents and children of a target nodeæX. The subsequent steps are similar to those of CD-B. We evaluated the CD-B and CD-H algorithms empirically based on simulated data from four different Bayesian networks. We also present comparative results based on the identification of Y structures and Y arcs from the output of the PC, MMHC and FCI algorithms. The results appear promising for mining causal relationships that are unconfounded by hidden variables from observational data."
2399,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,When causality matters for prediction,"Robert E. Tillman, Peter Spirtes","6:137-146, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/tillman10a/tillman10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_tillman10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/tillman10a.html,"Recent evaluations have indicated that in practice, general methods for prediction which do not account for changes in the conditional distribution of a target variable given feature values in some cases outperform causal discovery based methods for prediction whichæcanæaccount for such changes. We investigate some possibilities which may explain these findings. We give theoretical conditions, which are confirmed experimentally, for when particular manipulations of variables should not affect predictions for a target. We then consider the tradeoff between errors related to causality, i.e. not accounting for changes in a distribution after variables are manipulated, and errors resulting from sample bias, overfitting, and assuming specific parametric forms that do not fit the data, which most existing causal discovery based methods are particularly prone to making."
2400,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Distinguishing between cause and effect,"Joris Mooij, Dominik Janzing","6:147-156, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/mooij10a/mooij10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_mooij10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/mooij10a.html,"We describe eight data sets that together formed theæCauseEffectPairsætask in theæCausality Challenge #2: Pot-Luckæcompetition. Each set consists of a sample of a pair of statistically dependent random variables. One variable is known to cause the other one, but this information was hidden from the participants; the task was to identify which of the two variables was the cause and which one the effect, based upon the observed sample. The data sets were chosen such that we expect common agreement on the ground truth. Even though part of the statistical dependences may also be due to hidden common causes, common sense tells us that there is a significant cause-effect relation between the two variables in each pair. We also present baseline results using three different causal inference methods."
2401,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Nonlinear acyclic causal models,"Kun Zhang, Aaapo Hyv_rinen","6:157-164, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/zhang10a/zhang10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_zhang10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/zhang10a.html,"Distinguishing causes from effects is an important problem in many areas. In this paper, we propose a very general but well defined nonlinear acyclic causal model, namely, post-nonlinear acyclic causal model with inner additive noise, to tackle this problem. In this model, each observed variable is generated by a nonlinear function of its parents, with additive noise, followed by a nonlinear distortion. The nonlinearity in the second stage takes into account the effect of sensor distortions, which are usually encountered in practice. In the two-variable case, if all the nonlinearities involved in the model are invertible, by relating the proposed model to the post-nonlinear independent component analysis (ICA) problem, we give the conditions under which the causal relation can be uniquely found. We present a two-step method, which is constrained nonlinear ICA followed by statistical independence tests, to distinguish the cause from the effect in the two-variable case. We apply this method to solve the problem ""CauseEffectPairs"" in the Pot-luck challenge, and successfully identify causes from effects."
2402,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Recovering Cyclic Causal Structure,"Sleiman Itani, Mesrob Ohannessian, Karen Sachs, Garry P. Nolan, Munther A. Dahleh","6:165-176, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/itani10a/itani10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_itani10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/itani10a.html,"Cyclic graphical models are unnecessary for accurate representation of joint probability distributions, but are often indispensable when a causal representation of variable relationships is desired. For variables with a cyclic causal dependence structure, DAGs are guaranteed not to recover the correct causal structure, and therefore may yield false predictions about the outcomes of perturbations (and even inference.) In this paper, we introduce an approach to generalize Bayesian Network structure learning to structures with cyclic dependence. We introduce a structure learning algorithm, prove its performance given reasonable assumptions, and use simulated data to compare its results to the results of standard Bayesian network structure learning. We then propose a modified, heuristic algorithm with more modest data requirements, and test its performance on a real-life dataset from molecular biology, containing causal, cyclic dependencies."
2403,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Causal learning without DAGs,"David Duvenaud, Daniel Eaton, Kevin Murphy, Mark Schmidt","6:177-190, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/duvenaud10a/duvenaud10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_duvenaud10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/duvenaud10a.html,"Causal learning methods are often evaluated in terms of their ability to discover a true underlying directed acyclic graph (DAG) structure. However, in general the true structure is unknown and may not be a DAG structure. We therefore consider evaluating causal learning methods in terms of predicting the effects of interventions on unseen test data. Given this task, we show that there exist a variety of approaches to modeling causality, generalizing DAG-based methods. Our experiments on synthetic and biological data indicate that some non-DAG models perform as well or better than DAG-based methods at causal prediction tasks."
2404,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Discover Local Causal Network around a Target to a Given Depth,"You Zhou, Changzhang Wang, Jianxin Yin, Zhi Geng","6:191-202, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/zhou10a/zhou10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_zhou10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/zhou10a.html,"For a given target nodeæTæand a given depthæk _ 1, we propose an algorithm for discovering a local causal network around the targetæTæto depthæk. In our algorithm, we find parents, children and some descendants (PCD) of nodes stepwise away from the targetæTæuntil all edges within the depthækælocal network cannot be oriented further. Our algorithm extends the PCD-by-PCD algorithm for prediction with intervention presented in Yin et al. (2008). Our algorithm can construct a local network to depthæk, has a more efficient stop rule and finds PCDs along some but not all paths starting from the target."
2405,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Fast Committee-Based Structure Learning,"Ernest Mwebaze, John A. Quinn","6:203-214, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/mwebaze10a/mwebaze10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_mwebaze10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/mwebaze10a.html,"Current methods for causal structure learning tend to be computationally intensive or intractable for large datasets. Some recent approaches have speeded up the process by first making hard decisions about the set of parents and children for each variable, in order to break large-scale problems into sets of tractable local neighbourhoods. We use this principle in order to apply a structure learning committee for orientating edges between variables. We find that a combination of weak structure learners can be effective in recovering causal dependencies. Though such a formulation would be intractable for large problems at the global level, we show that it can run quickly when processing local neighbourhoods in turn. Experimental results show that this localized, committee-based approach has advantages over standard causal discovery algorithms both in terms of speed and accuracy."
2406,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,SIGNET: Boolean Rile Deetermination for Abscisic Acid Signaling,Jerry Jenkins,"6:215-224, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/jenkins10a/jenkins10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_jenkins10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/jenkins10a.html,"This paper describes the SIGNET dataset generated for the Causality Challenge. Cellular signaling pathways are most elusive types of networks to access experimentally due to the lack of methods for determining the state of a signaling network in an intact living cell. Boolean network models are currently being used for the modeling of signaling networks due to their compact formulation and ability to adequately represent network dynamics without the need for chemical kinetics. The problem posed in the SIGNET challenge is to determine the set of Boolean rules that describe the interactions of nodes within a plant signaling network, given a set of 300 Boolean pseudodynamic simulations of the true rules. The two solution methods that were presented revealed that the problem can be solved to greater than 99% accuracy."
2407,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,The Use of Bernoulli Mixture Models for Identifying Corners of a Hypercube and Extracting Boolean Rules From Data,Mehreen Saeed,"6:225-236, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/saeed10a/saeed10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_saeed10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/saeed10a.html,"This paper describes the use of Bernoulli mixture models for extracting boolean rules from data. Bernoulli mixtures identify high data density areas on the corners of a hypercube. One corner represents a conjunction of literals in a boolean clause and the set of all identified corners, of the hypercube, indicates disjuncts of clauses to form a rule. Further class labels can be used to select features or variables, in the individual conjuncts, that are relevant to the target variable. This method was applied to the SIGNET dataset of the causality workbench challenge. The dataset is derived from a biological signaling network with 21 time steps and 43 random boolean variables. Results indicate that Bernoulli mixtures are quite effective at extracting boolean rules from data."
2408,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Reverse Engineering of Asynchronous Boolean Networks,"Cheng Zheng, Zhi Geng","6:237-248, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/zheng10a/zheng10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_zheng10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/zheng10a.html,"In this paper, we propose an approach for reconstructing asynchronous Boolean networks from observed data. We find the causal relationships in Boolean networks using an asynchronous evolution approach. In our approach, we first find a minimum explanatory set for a node to reduce complexity of candidate Boolean functions, and then we choose a Boolean function for the node based on the maximum likelihood. This approach is stimulated by the task SIGNET of the causal challenge #2 pot-luck (Jenkins, 2009). Besides the data set SIGNET, we also applied our approach to two other datasets to evaluate our approach: one is generated by Professor Isabelle Guyon and the other generated ourselves from the signal transduction network of Abscisic acid in guard cell."
2409,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,TIED: An Artificially Simulated Dataset with Multiple Markov Boundaries,"Alexander Statnikov, Constantin F. Aliferis","6:249-256, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/statnikov10a.html,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_statnikov10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/statnikov10a.html,"We present an artificially simulated dataset (TIED) constructed so that there are many minimal sets of variables with maximal predictivity (i.e., Markov boundaries) and likewise many sets of variables that are statistically indistinguishable from the set of direct causes and direct effects of the response variable. This dataset was used in the Potluck Causality Challenge to determine all statistically indistinguishable sets of direct causes and direct effects and all Markov boundaries of the response variable and also to predict the response variable in the independent test data. We also present baseline results of application of several algorithms to this dataset."
2410,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Learning Causal Models That Make Correct Manipulation Predictions,"Mark Voortman, Denver Dash, Marek J. Druzdzel","6:257-266, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/voortman10a/voortman10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_voortman10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/voortman10a.html,"One of the fundamental purposes of causal models is using them to predict the effects of manipulating various components of a system. It has been argued by Dash (2005, 2003) that theæDoæoperator will fail when applied to an equilibrium model, unless the underlying dynamic system obeys what he callsEquilibration-Manipulation Commutability. Unfortunately, this fact renders most existing causal discovery algorithms unreliable for reasoning about manipulations. Motivated by this caveat, in this paper we present a novel approach to causal discovery of dynamic models from time series. The approach uses a representation of dynamic causal models motivated by Iwasaki and Simon (1994), which asserts that all ""causation across time"" occurs because a variable's derivative has been affected instantaneously. We present an algorithm that exploits this representation within a constraint-based learning framework by numerically calculating derivatives and learning instantaneous relationships. We argue that due to numerical errors in higher order derivatives, care must be taken when learning causal structure, but we show that the Iwasaki-Simon representation reduces the search space considerably, allowing us to forego calculating many high-order derivatives. In order for our algorithm to discover the dynamic model, it is necessary that the time-scale of the data is much finer than any temporal process of the system. Finally, we show that our approach can correctly recover the structure of a fairly complex dynamic system, and can predict the effect of manipulations accurately when a manipulation does not cause an instability. To our knowledge, this is the first causal discovery algorithm that has demonstrated that it can correctly predict the effects of manipulations for a system that does not obey the EMC condition."
2411,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Comparison of Granger Causality and Phase Slope Index,"Guido Nolte, Andreas Ziehe, Nicole Kr_mer, Florin Popescu, Klaus-Robert MÙller","6:267-276, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/nolte10a/nolte10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_nolte10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/nolte10a.html,"We recently proposed a new measure, termed Phase Slope Index (PSI), It estimates the causal direction of interactions robustly with respect to instantaneous mixtures of independent sources with arbitrary spectral content. We compared this method to Granger Causality for linear systems containing spatially and temporarily mixed noise and found that, in contrast to PSI, the latter was not able to properly distinguish truly interacting systems from mixed noise. Here, we extent this analysis with respect to two aspects: a) we analyze Granger causality and PSI also for non-mixed noise, and b) we analyze PSI for nonlinear interactions. We found a) that Granger causality, in contrast to PSI, fails also for non-mixed noise if the memory-time of the sender of information is long compared to the transmission time of the information, and b) that PSI, being a linear method, eventually misses nonlinear interactions but is unlikely to give false positive results."
2412,6,http://jmlr.csail.mit.edu/proceedings/papers/v6/,Causality Challenge: Benchmarking relevant signal components for effective monitoring and process control,"Michael McCann, Yuhua Li, Liam Maguire, Adrian Johnston","6:277-288, 2010.",http://jmlr.csail.mit.edu/proceedings/papers/v6/mccann10a/mccann10a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v6/,,18th February 2010,39794,NIPS 2008 ,Causality: Objectives and Assessment (NIPS 2008 Workshop),"Whistler, Canada","Isabelle Guyon, Dominik Janzing, and Bernhard SchÕ_lkopf.",v6_mccann10a,http://jmlr.csail.mit.edu/proceedings/papers/v6/mccann10a.html,"A complex modern manufacturing process is normally under consistent surveillance via the monitoring of signals/variables collected from sensors. However, not all of these signals are equally valuable in a specific monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. It is often the case that useful information is buried in the latter two. Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then feature selection may be used to identify the most predictive signals. Once these signals have been identified causal relevance may then be investigated to try and identify the causal features. The Process Engineers may then use these signals to ensure a small scrap rate further downstream in the process, increase the throughput and reduce the per unit production costs. Working in partnership with industry we aim to address this complex problem as part of their process control engineering in the context of wafer fabrication production and enhance current business improvement techniques with the application of causal feature selection as an intelligent systems technique."
2413,7,http://jmlr.csail.mit.edu/proceedings/papers/v7/,KDD Cup 2009 @ Budapest: feature partitioning and boosting,"MiklÑs Kurucz, Dàvid SiklÑsi, Istvàn BÕrÑ, P_ter Csizsek, Zsolt Fekete, RÑbert Iwatt, Tamàs Kiss and Adrienn SzabÑ","7:65-75, 2009",http://jmlr.csail.mit.edu/proceedings/papers/v7/kurucz09/kurucz09.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v7/,,4th December 2009,39992,KDD 2009,Proceedings of KDD-Cup 2009 competition,"Paris, France","Gideon Dror, Marc BoullÕ©, Isabelle Guyon, Vincent Lemaire, David Vogel.",v7_kurucz09,http://jmlr.csail.mit.edu/proceedings/papers/v7/kurucz09.html,"We describe the method used in our final submission to KDD Cup 2009 as well as a selection of promising directions that are generally believed to work well but did not justify our expectations. Our final method consists of a combination of a LogitBoost and an ADTree classifier with a feature selection method that, as shaped by the experiments we have conducted, have turned out to be very different from those described in some well-cited surveys. Some methods that failed include distance, information and dependence measures for feature selection as well as combination of classifiers over a partitioned feature set. As another main lesson learned, alternating decision trees and LogitBoost outperformed most classifiers for most feature subsets of the KDD Cup 2009 data."
2414,12,http://jmlr.csail.mit.edu/proceedings/papers/v12/,Linking Granger Causality and the Pearl Causal Model with Settable Systems.,"Halbert White, Karim Chalak and Xun Lu","12:1-29, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v12/white11/white11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v12/,,3rd March 2011,40157,NIPS Mini-Symposium on Causality in Time Series,Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series," Vancouver, Canada",Florin Popescu and Isabelle Guyon,v12_white11.htm,http://jmlr.csail.mit.edu/proceedings/papers/v12/white11.htm,"The causal notions embodied in the concept of Granger causality have been argued to belong to a different category than those of Judea Pearl's Causal Model, and so far their relation has remained obscure. Here, we demonstrate that these concepts are in fact closely linked by showing how each relates to straightforward notions of direct causality embodied in settable systems, an extension and refinement of the Pearl Causal Model designed to accommodate optimization, equilibrium, and learning. We then provide straightforward practical methods to test for direct causality using tests for Granger causality."
2415,12,http://jmlr.csail.mit.edu/proceedings/papers/v12/,Robust Statistics for Describing Causality in Multivariate Time Series.,Florin Popescu,"12:30-64, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v12/popescu11/popescu11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v12/,,3rd March 2011,40157,NIPS Mini-Symposium on Causality in Time Series,Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series," Vancouver, Canada",Florin Popescu and Isabelle Guyon,v12_popescu11.htm,http://jmlr.csail.mit.edu/proceedings/papers/v12/popescu11.htm,"A widely agreed upon definition of time series causality inference, established in the seminal 1969 article of Clive \citet{Granger1969}, is based on the relative ability of the history of one time series to predict the current state of another, conditional on all other past information. While the Granger Causality (GC) principle remains uncontested, its literal application is challenged by practical and physical limitations of the process of discretely sampling continuous dynamic systems. Advances in methodology for time-series causality subsequently evolved mainly in econometrics and brain imaging: while each domain has specific data and noise characteristics the basic aims and challenges are similar. Dynamic interactions may occur at higher temporal or spatial resolution than our ability to measure them, which leads to the potentially false inference of causation where only correlation is present.ææ Causality assignment can be seen as the principled partition of spectral coherence among interacting signals using both auto-regressive (AR) modeling and spectral decomposition. While both approaches are theoretically equivalent, interchangeably describing linear dynamic processes, the purely spectral approach currently differs in its somewhat higher ability to accurately deal with mixed additive noise. Two new methods are introduced 1) a purely auto-regressive method named Causal Structural Information is introduced which unlike current AR-basedæ methods is robust to mixed additive noise and 2) a novel means of calculating multivariate spectra for unevenly sampled data based on cardinal trigonometric functions isæ incorporated into the recently introduced phase slope index (PSI) spectral causal inference method (Nolte et al. 2008). In addition to these, PSI,æ partial coherence-based PSI and existing AR-based causality measures were tested on a specially constructed data-set simulating possible confounding effects of mixed noise and another additionally testing the influence of common, background driving signals. Tabulated statistics are provided in which true causality influence is subjected to an acceptable level of false inference probability."
2416,12,http://jmlr.csail.mit.edu/proceedings/papers/v12/,Causal Time Series Analysis of Functional Magnetic Resonance Imaging Data.,"Alard Roebroeck, Anil K. Seth, Pedro Valdes-Sosa","12:65-94, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v12/roebroeck11/roebroeck11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v12/,,3rd March 2011,40157,NIPS Mini-Symposium on Causality in Time Series,Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series," Vancouver, Canada",Florin Popescu and Isabelle Guyon,v12_roebroeck11.htm,http://jmlr.csail.mit.edu/proceedings/papers/v12/roebroeck11.htm,"This review focuses on dynamic causal analysis of functional magnetic resonance (fMRI) data to infer brain connectivity from a time series analysis and dynamical systems perspective. Causal influence is expressed in the Wiener-Akaike-Granger-Schweder (WAGS) tradition and dynamical systems are treated in a state space modeling framework. The nature of the fMRI signal is reviewed with emphasis on the involved neuronal, physiological and physical processes and their modeling as dynamical systems. In this context, two streams of development in modeling causal brain connectivity using fMRI are discussed: time series approaches to causality in a discrete time tradition and dynamic systems and control theory approaches in a continuous time tradition. This review closes with discussion of ongoing work and future perspectives on the integration of the two approaches."
2417,12,http://jmlr.csail.mit.edu/proceedings/papers/v12/,Causal Search in Structural Vector Autoregressive Models.,"Alessio Moneta, Nadine Chlass, Doris Entner, Patrik Hoyer","12:95-114, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v12/moneta11/moneta11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v12/,,3rd March 2011,40157,NIPS Mini-Symposium on Causality in Time Series,Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series," Vancouver, Canada",Florin Popescu and Isabelle Guyon,v12_moneta11.htm,http://jmlr.csail.mit.edu/proceedings/papers/v12/moneta11.htm,"This paper reviews a class of methods to perform causal inference in the framework of a structural vector autoregressive model. We consider three different settings. In the first setting the underlying system is linear with normal disturbances and the structural model is identified by exploiting the information incorporated in the partial correlations of the estimated residuals. Zero partial correlations are used as input of a search algorithm formalized via graphical causal models. In the second, semi-parametric, setting the underlying system is linear with non-Gaussian disturbances. In this case the structural vector autoregressive model is identified through a search procedure based on independent component analysis. Finally, we explore the possibility of causal search in a nonparametric setting by studying the performance of conditional independence tests based on kernel density estimations."
2418,12,http://jmlr.csail.mit.edu/proceedings/papers/v12/,Time Series Analysis with the Causality Workbench.,"Isabelle Guyon, Alexander Satnikov, Constantin Aliferis","12:115-139, 2011.",http://jmlr.csail.mit.edu/proceedings/papers/v12/guyon11/guyon11.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v12/,,3rd March 2011,40157,NIPS Mini-Symposium on Causality in Time Series,Neural Information Processing Systems (NIPS) Mini-Symposium on Causality in Time Series," Vancouver, Canada",Florin Popescu and Isabelle Guyon,v12_guyon11.htm,http://jmlr.csail.mit.edu/proceedings/papers/v12/guyon11.htm,"The Causality Workbench project is an environment to test causal discovery algorithms. Via a web portal http://clopinet.com/causality, it provides a number of resources, including a repository of datasets, models, and software packages, and a virtual laboratory allowing users to benchmark causal discovery algorithms by performing virtual experiments to study artificial causal systems. We regularly organize competitions. In this paper, we describe what the platform offers for the analysis of causality in time series analysis."
2419,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –Learning Equivalence Classes of Acyclic Models with Latent and Selection Variables from Multiple Datasets with Overlapping Variables”,"Jiji Zhang, Ricardo Silva","15:16-18, 2011.,15:16-18, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_zhang11a_zhang11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/zhang11a/zhang11a.pdf,"In automated causal discovery, the constraint-based approach seeks to learn an (equivalence) class of causal structures (with possibly latent variables and/or selection variables) that are compatible (according to some assumptions, usually the causal Markov and faithfulness assumptions) with the conditional dependence and independence relations found in data. In the paper under discussion, Tillman and Spirtes (T&S) develop a constraint-based algorithm for learning causal structures from multiple, overlapping datasets. The basic setup of the problem is this: the variables of interest are not all measured at once in a single study. Instead there are several studies, each measuring a subset, which produce multiple datasets with overlapping variables. Assuming there is a common structure over the variables of interest (with possibly latent confounding variables and selection variables) that generated all the datasets, T&SÍs algorithm is designed to discover features of that structure by learning the features shared by all the causal structures that are compatible with all the datasets . Unlike standard constraint-based methods, which assume an oracle of conditional independence that can respond to every query of whether two observed variables are conditionally independent given a set of observed variables, the algorithm described in T&SÍs paper allows the oracle to be incomplete: some variables may be jointly measured in none of the available datasets; as a result, the query of conditional independence concerning these variables cannot be answered. Thus the algorithm provides a solution to the problem of learning from (a broad class of) incomplete oracles. To be sure, the algorithm does not allow the oracle to be arbitrarily incomplete: with respect to the variable set of each individual dataset, the oracle is still complete. But it seems to be general enough to handle the problem of incomplete oracle in some other contexts."
2420,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –Contextual Bandit Algorithms with Supervised Learning Guarantees”,Brendan McMahan,"15:27-28, 2011.,15:27-28, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_mcmahan11a_mcmahan11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/mcmahan11a/mcmahan11a.pdf,"It is my pleasure to provide some commentary on the paper ñContextual Bandit Algorithms with Supervised Learning Guaranteesî by Beygelzimer et. al. This discussion synthesizes my own impressions of the paper, as well as the comments of the anonymous reviewers. For applied machine learning, finding an appropriate formulation of the problem is essential. Supervised learning is perhaps the most successful and extensively studied formulation in the field. It is broad enough to encompass many real-world problems, but is narrow enough that significant theoretical results are possible. Nevertheless, it is becoming increasingly clear that supervised learning is too restrictive for some important applications. Supervised learning is about making good predictions, but often one cares more about the outcome of taking actions, in particular in environments where feedback is only received for the action taken. Applications in web search and advertising are important motivating examples: at the end of the day, what matters is what search results (or ads, or news results) we choose to show to users, and whether those users like (click) on those results. These web applications are particularly compelling because the scale of the problem and latencies required mandate an automated solution. The contextual bandit formulation captures both the measurement of success in terms of the real-world quantity of interest (clicks) as well as addressing the inherent explore/exploit trade-offs."
2421,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –The Neural Autoregressive Distribution Estimator”,Yoshua Bengio,"15:38-39, 2011.,15:38-39, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_bengio11a_bengio11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/bengio11a/bengio11a.pdf,"This is a discussion of Larochelle and Murray (2011). The Restricted Boltzmann Machine (Smolensky, 1986; Hinton et al., 2006) has inspired much research in recent years, in particular as a building block for deep architectures (see Bengio (2009) for a review). The Restricted Boltzmann Machine (RBM) is an undirected graphical model with latent variables, exact inference, rather simple sampling procedures (block Gibbs), and several successful learning algorithms based on approximations of the log-likelihood gradient. However, when it comes to actually computing the distribution or density function, it is intractable, except when either the number of inputs or latent variables is very small (about 25 binary hidden units with current computers and about an hour of computing, on MNIST). With applications in mind where the exact likelihood would be useful (e.g. when combining the model with other graphical models, or in order to perform exact likelihood comparisons between different models), Larochelle and Murray have introduced a new probabilistic model that is inspired by the RBM but whose likelihood can be computed very cheaply."
2422,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –Learning Scale Free Networks by Reweighted L1 regularization”,Deepak Agarwal,"15:49-50, 2011.,15:49-50, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_agarwal11a_agarwal11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/agarwal11a/agarwal11a.pdf,
2423,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –Spectral Dimensionality Reduction via Maximum Entropy”,Laurens van der Maaten,"15:60-62, 2011.,15:60-62, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_maaten11a_maaten11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/maaten11a/maaten11a.pdf,"Since the introduction of LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum et al., 2000), a large number of non-linear dimensionality reduction techniques (manifold learners) have been proposed. Many of these non-linear techniques can be viewed as instantiations of Kernel PCA; they employ a cleverly designed kernel matrix1 that preserves local data structure in the ñfeature spaceî (Bengio et al., 2004). The kernel matrices of the first manifold learners were handcrafted: for instance, LLE uses an inverse squared graph Laplacian of the reconstruction weight matrix as kernel matrix, Isomap uses a centered geodesic distance matrix, and Laplacian Eigenmaps uses an inverse neighborhood graph Laplacian (Belkin and Niyogi, 2002). More recently, several techniques have been proposed that, instead of designing the kernel matrix by hand, try to learn a good kernel matrix from the data (Weinberger and Saul, 2009; Shaw and Jebara, 2009). In particular, these techniques impose linear constraints on the kernel matrix K that are designed to preserve local data structure in the feature space; the techniques optimize an objective function that approximately minimizes the rank of the kernel matrix subject to these constraints. The (approximate) rank minimization is required because we wish to obtain a compact data representation. The main differences between ñkernel-learningî dimensionality reduction techniques are in the way the approximate rank constraint is implemented: for instance, Maximum Variance Unfolding (MVU) maximizes the variance of the embedding (Weinberger and Saul, 2009), whereas Structure Preserving Embedding maximizes the similarity between the eigenvectors of the kernel matrix and those of the data adjacency matrix (Shaw and Jebara, 2009)."
2424,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –A conditional game for comparing approximations”,Vincent Conitzer,"15:72-73, 2011.,15:72-73, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_conitzer11a_conitzer11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/conitzer11a/conitzer11a.pdf,"How should we assess the quality of an approximate inference algorithm? One obvious approach is to see how it performs on instances that are small enough to solve exactly. However, this seems to be a poor way of evaluating approximate inference algorithms, because that is precisely where we do not need them. Indeed, an easy way to perform well on such an evaluation could be to design an algorithm as follows: run an exact algorithm for a pre-specified amount of time, and if it times out, run a very fast (possibly bad) approximate algorithm. While it would be natural to prohibit such a strategy in (say) a competition, some algorithms may have roughly the same behavior without explicitly following such a two-phase strategy, resulting in a difficult and perhaps subjective decision of whether the algorithm violates the rule. It would be much better to evaluate approximate inference algorithms on large instances that are out of the reach of exact methods; for this, after all, they are designed. The problem is that it is hard to evaluate on such instances, because, by their fundamental characteristic, we do not know the ground truth for them. Eaton proposes a method for addressing this. Rather than attempting to compare the output of approximate inference algorithms to ground truth, Eaton proposes to have a pair of approximate inference algorithms compete head-to-head in a game designed for this purpose."
2425,15,http://jmlr.csail.mit.edu/proceedings/papers/v15/,Discussion of –The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling”,Frank Wood,"15:83-84, 2011.,15:83-84, 2011.",,http://jmlr.csail.mit.edu/proceedings/papers/v15/,,14th June 2011,"April 11-13, 2011",AISTATS 2011 Proceedings,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,"Fort Lauderdale, FL, USA","Geoffrey Gordon, David Dunson, and Miroslav DudÕ_k",v15_wood11a_wood11a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v15/wood11a/wood11a.pdf,"Mixed-membership models (e.g. ñtopic modelsî) are inarguably popular; especially latent Dirichlet allocation (LDA) [Blei et al., 2003] and its variants. Such models have become a fundamental tool in the analysis and exploration of many types of data. Originally designed to model text documents as per-word draws from a document-specific weighting of a finite collection of ñtopicsî (distributions over words), mixedmembership models now are applied very broadly. Example usage includes applications in information retrieval, image processing, audio classification, and more. Because of the wide applicability of mixedmembership modeling, improving models of this type has the potential to have significant impact. The discrete infinite logistic normal distribution (DILN) for mixed-membership modeling is a significant advance in mixed-membership modeling. A key to the success of mixed-membership models is simplicity. They are easy to describe mathematically and easy to explain informally. The latent variables defined by such models are often readily interpretable by lay practitioners and often are visibly fascinating. This kind of simplicity can also be an Achilles heel of sorts. To keep such models relatively simple, unreasonable assumptions about the nature of the true generative process must be made. This kind of trade-off is very common when designing or choosing a statistical model. The trick to designing good new models is to do so in such a way as to address the true shortcomings of the models being built upon while retaining desirable traits like interpretability, simplicity, and elegance. DILN is arguably such a contribution to mixedmembership modeling. DILN directly addresses one of the more pressing problems in mixed-membership modeling, namely, how to model correlations between Appearing in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR: W&CP 15. Copyright 2011 by the authors. latent features. Mixed membership models are characterized by grouped observations (think of a bag-of-words representation of a document or a bag-of-features representation of an image) generated by a mixture of latent distributions over the observation space (ñfeaturesî). In the canonical document modeling example a feature (ñtopicî) is a distribution over words and a document is a collection of draws from a per-document mixture of topics. The fact that certain words tend to co-occur in the same document can be used to infer both what topics occur in a corpus and what proportions of each topic are found in each document. Original work on mixed-membership modeling (notably LDA for document modeling) assumed both a fixed and finite number of statistically independent topics. In other words, the presence of one topic in a document was nearly independent of the presence of other topics, and the number of topics was fixed a priori to a pre-determined finite value. A great deal of subsequent work went into defining topic models of unbounded topic cardinality including, notably, the hierarchical Dirchlet process (HDP) as applied to mixedmembership modeling (HDP-LDA) [Teh et al., 2006]. Realistically though, specifying a mixed-membership model with a large number of latent features results in a model that is very nearly like that of a model with an unbounded number of features„Bayesian priors encourage sharing and discourage overfitting whether the number of topics is unbounded or large and fixed. Less work has gone into learning mixed-membership models in which the latent factors are correlated (e.g. work by [Blei and Lafferty, 2006, Li et al., 2007, DoshiVelez and Ghahramani, 2009, Rai and DaumÇe III, 2009]) though arguably this is the more important and less easy to remedy shortcoming of first generation mixed-membership models. To illustrate what is meant by correlations between latent factors we turn to an illuminating visual scene modeling example from the introduction to [Doshi-Velez and Ghahramani, 2009]. When modeling a visual scene using a mixed-membership model we can think of an ñimageî 84 Discussion of ñThe Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modelingî as being a collection of observations of a world containing some number of latent features. Latent features here can be thought of as being in correspondence with physical objects in the world like lamps, desks, elephants, and so forth. Clearly a model that accounts for correlations in the presence and/or absence of features (desks and lamps often occur together, chairs and elephants less so) should outperform one that does not. A similar story can be told when modeling document collections using a topic representation, namely that some topics tend to occur together. DILN is able to account for correlations between feature presence rates for mixed-membership models of ñbag-of-wordsî (order-invariant, discrete) grouped observations."
2426,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Commentary on ""Online Optimization with Gradual Variations""",Satyen Kale,"æ23: 6.21 - 6.24, 2012",http://jmlr.csail.mit.edu/proceedings/papers/v23/kale12/kale12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_kale12_kale12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/kale12/kale12.pdf,"This commentary is about (Chiang et al., 2012b). This paper is the result of a merge between two papers, (Yang et al., 2012) and (Chiang et al., 2012a). Both papers address the same question: is it possible to obtain regret bounds in various online learning settings that depend on some notion of variation in the costs, rather than the number of periods? Both papers give remarkably similar algorithms for this problem, although the analysis techniques are quite different, and obtain very similar results. While (Yang et al., 2012) gives two algorithms obtaining such regret bounds for general online convex optimization (OCO), (Chiang et al., 2012a) gives a unified framework to obtain such regret bounds for three specific cases of OCO: online linear optimization, online learning with experts, and online expconcave optimization."
2427,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Commentary on ""The Optimality of Jeffreys Prior for Online Density Estimation and the Asymptotic Normality of Maximum Likelihood Estimators""",Peter Grônwald,none,,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_grunwald12_grunwald12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/grunwald12/grunwald12.pdf,"In the field of prediction with expert advice, a standard goal is to sequentially predict data as well as the best expert in some reference set of ïexpert predictorsÍ. Universal data compression, a subfield of information theory, can be thought of as a special case. Here, the set of expert predictors is a statistical model, i.e. a family of probability distributions, and the predictions are scored using the logarithmic loss function, which, via the Kraft inequality, gives the procedure an interpretation in terms of data compression. A prediction strategy is a function that, for each n, given data x n _ x1, . . . , xn, outputs a ñpredictiveî probability distribution p(‡ | x n ) for Xi+1. For a given model M, the Shtarkov or Normalized Maximum Likelihood (NML) strategy relative to M, is the prediction strategy that achieves the minimax optimal individual-sequence regret relative to M. NML has a number of drawbacks, detailed below, and is therefore often approximated by more convenient strategies such as Sequential Normalized Maximum Likelihood (SNML) or the Bayesian strategy. The latter predicts using the Bayesian predictive distribution for the model M, defined relative to some prior _, which is often taken to be JeffreysÍ prior „ in that case we abbreviate it to J.B. The text below has been written so as to be (hopefully) understandable for readers who do not know too many details of these concepts; for such details, see e.g. GrÂunwald (2007) and/or Kotlowski and GrÂunwald (2011) (KG from now on)."
2428,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Commentary on ""Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures, Case-studies, and Consequences""",John Duchi,none,,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_duchi12_duchi12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/duchi12/duchi12.pdf,"In their paper, Recht and RÇe have presented conjectures and consequences of noncommutative variants of the arithmetic mean-geometric mean (AM-GM) inequality for positive definite matrices. Let A1, . . . , An be a collection of positive semidefinite matrices and i1, . . . , ik be random indices in {1, . . . , n}. To avoid symmetrization issues that arise since matrix products are non-commutative, Recht and RÇe define the expectation operators"
2429,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,"Commentary on ""Near-Optimal Algorithms for Online Matrix Prediction""",Rina Foygel,none,,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_foygel12_foygel12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/foygel12/foygel12.pdf,"This piece is a commentary on the paper by Hazan et al. (2012b). In their paper, they introduce the class of (_, _ )-decomposable matrices, and show that well-known matrix regularizers and matrix classes (e.g. matrices with bounded trace norm) can be viewed as special cases of their construction. The _ and _ terms can be related to the max norm and to the trace norm, respectively, as explored in the paper, which we discuss in detail below. The paperÍs main contribution is a powerful online learning guarantee when learning inside the (_, _ )-decomposable class, which scales with Ì _ ‡ _ , and an efficient algorithm for solving this learning problem. Crucially, the paper reframes the well-known problems of online max cut, learning a team ranking (ñgamblingî), and trace-norm regularized matrix completion (a.k.a. collaborative filtering) as special cases of learning inside (_, _ )-decomposable classes of matrices. This yields new algorithms for the three existing problems, with each algorithm giving a strong improvement over existing results in terms of either efficiency or error rate guarantees. In addition, the paper derives lower bounds on the error rates for each of the three problems that match (up to log factors) the upper bounds proved with the new algorithm„and in particular, for collaborative filtering with the trace norm, their lower bound solves an open problem posed by Shamir and Srebro in COLT 2011. In this commentary, we explore the connections between the class of (_, _ )-decomposable matrices, introduced by Hazan et al. (2012b), and the matrix trace norm (a.k.a. nuclear norm) and max norm. Specifically, we are interested in the idea of a ñtrade-offî between the _ and _ values for the class, and will consider how the resulting non-convex optimization question can be formulated as a series of convex optimization problems."
2430,23,http://jmlr.csail.mit.edu/proceedings/papers/v23/,Open Problem: Regret Bounds for Thompson Sampling,Lihong Li and Olivier Chapelle,none,http://jmlr.csail.mit.edu/proceedings/papers/v23/li12/li12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/,,16th June 2012,"June 25-27, 2012",COLT 2012,Proceedings of the 25th Annual Conference on Learning Theory,"Edinburgh, Scotland","Shie Mannor, Nathan Srebro, Robert C. Williamson",v23_li12_li12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v23/li12/li12.pdf,"Contextual multi-armed bandits (Langford and Zhang, 2008) have received substantial interests in recent years due to their wide applications on the Internet, such as new recommendation and advertising. The fundamental challenge here is to balance exploration and exploitation so that the total payoff collected by an algorithm approaches that of an optimal strategy. Exploration techniques like -greedy, UCB (upper confidence bound), and their many variants have been extensively studied. Interestingly, one of the oldest exploration heuristics, dated back to Thompson (1933), has not been popular in the literature until recently when researchers started to realize its effectiveness in critical real-world applications (Scott, 2010; Graepel et al., 2010; May and Leslie, 2011; Chapelle and Li, 2012). This heuristic, known as Thompson sampling, fulfills the principle of ñprobability matching,î which states that an arm is chosen with the probability that it is the optimal one. A generic description is given in Algorithm 1, where the algorithm maintains a posterior distribution P(_|D) over a parameter space _ that defines a set of greedy policies. At every step, a random model _ t is drawn from the posterior, and the greedy action according to the payoff predictions of _ t is chosen. Algorithm 1 Thompson sampling (adapted from Chapelle and Li (2012)) Initialize observed data set: D _ _ for t = 1, . . . , T do Observe context xt Draw _ t _ _ according to P(_|D), and select at = arg maxa Er r|xt , a, _t Observe payoff rt(at), and augment observed data set D _ D _ (xt , at , rt(at)) end for Thompson sampling has a number of significant advantages in practice. First, it can be easily combined with Bayesian approaches and complicated parametric models (Graepel et al., 2010; Chapelle and Li, 2012); in contrast, popular exploration strategies like UCB are often hard to derive except in special cases like (generalized) linear models. Second, a number of recent empirical studies, including those conducted on large-scale, real-world problems, have shown the algorithm is highly effective for balancing exploration and exploitation (Scott, 2010; Graepel et al., 2010; _ Work done while author was at Yahoo Research. c 2012 L. Li & O. Chapelle. LI CHAPELLE May and Leslie, 2011; Chapelle and Li, 2012). Furthermore, Thompson sampling appears to be more robust to observation delays of payoffs, compared to deterministic exploration strategies like UCB (Chapelle and Li, 2012)."
2431,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,A Note on Metric Properties for Some Divergence Measures: The Gaussian Case,K.T. Abou-Moustafa & F.P. Ferrie,"25:1-15, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/preface/preface.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_aboumoustafa12,http://jmlr.csail.mit.edu/proceedings/papers/v25/aboumoustafa12.html,"Multivariate Gaussian densities are pervasive in pattern recognition and machine learning. A central operation that appears in most of these areas is to measure the difference between two multivariate Gaussians. Unfortunately, traditional measures based on the Kullback{Leibler (KL) divergence and the Bhattacharyya distance do not satisfy all metric axioms necessary for many algorithms. In this paper we propose a modification for the KL divergence and the Bhattacharyya distance, for multivariate Gaussian densities, that transforms the two measures into distance metrics. Next, we show how these metric axioms impact the unfolding process of manifold learning algorithms. Finally, we illustrate the efficacy of the proposed metrics on two different manifold learning algorithms when used for motion clustering in video data. Our results show that, in this particular application, the new proposed metrics lead to boosts in performance (at least 7%) when compared to other divergence measures."
2432,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Multiresolution Mixture Modeling using Merging of Mixture Components,P.R. Adhikari & J. Hollm _ n,"25:17-32, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/adhikari12/adhikari12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_adhikari12,http://jmlr.csail.mit.edu/proceedings/papers/v25/adhikari12.html,"Observing natural phenomena at several levels of detail results in multiresolution data. Extending models and algorithms to cope with multiresolution data is a prerequisite for wide-spread exploitation of the data represented in the multiple resolutions. Mixture models are widely used probabilistic models, however, the mixture models in their standard form can be used to analyze the data represented in a single resolution. In this paper, we propose a multiresolution mixture model based on merging of the mixture components across models represented in different resolutions. Result of such an analysis scenario is to have multiple mixture models, one mixture model for each resolution of data. Our proposed solution is based on the idea on the interaction between mixture models. More specifically, we repeatedly merge component distributions of mixture models across different resolutions. We experiment our proposed algorithm on the two real-world chromosomal aberration datasets represented in two different resolutions. Results show an improvement on the compared multiresolution settings."
2433,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Learning Latent Variable Models by Pairwise Cluster Comparison,N. Asbeh & B. Lerner,"25:33-48, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/asbeh12/asbeh12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_asbeh12,http://jmlr.csail.mit.edu/proceedings/papers/v25/asbeh12.html,"Identification of latent variables that govern a problem and the relationships among them given measurements in the observed world are important for causal discovery. This identification can be made by analyzing constraints imposed by the latents in the measurements. We introduce the concept ofæpairwise cluster comparison PCCæto identify causal relationships from clusters and a two-stage algorithm, called LPCC, that learns a latent variable model (LVM) using PCC. First, LPCC learns the exogenous and the collider latents, as well as their observed descendants, by utilizing pairwise comparisons between clusters in the measurement space that may explain latent causes. Second, LPCC learns the non-collider endogenous latents and their children by splitting these latents from their previously learned latent ancestors. LPCC is not limited to linear or latent-tree models and does not make assumptions about the distribution. Using simulated and real-world datasets, we show that LPCC improves accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of-the-art algorithms."
2434,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Local Kernel Density Ratio-Based Feature Selection for Outlier Detection,"F. Azmandian, J.G. Dy, J.A. Aslam & D.R. Kaeli","25:49-64, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/azmandian12/azmandian12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_azmandian12,http://jmlr.csail.mit.edu/proceedings/papers/v25/azmandian12.html,"Selecting features is an important step of any machine learning task, though most of the focus has been to choose features relevant for classification and regression. In this work, we present a novel non-parametric evaluation criterion for filter-based feature selection which enhances outlier detection. Our proposed method seeks the subset of features that represents the inherent characteristics of the normal dataset while forcing outliers to stand out, making them more easily distinguished by outlier detection algorithms. Experimental results on real datasets show the advantage of this feature selection algorithm compared to popular and state-of-the-art methods. We also show that the proposed algorithm is able to overcome the small sample space problem and perform well on highly imbalanced datasets."
2435,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,A Coupled Indian Buffet Process Model for Collaborative Filtering,S.P. Chatzis,"25:65-79, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/chatzis12/chatzis12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_chatzis12,http://jmlr.csail.mit.edu/proceedings/papers/v25/chatzis12.html,"The dramatic rates new digital content becomes available has brought collaborative filtering systems in the epicenter of computer science research in the last decade. In this paper, we propose a novel methodology for rating prediction utilizing concepts from the field of Bayesian nonparametrics. The basic concept that underlies our approach is that each user rates a presented item based on the latent genres of the item and the latent interests of the user. Each item may belong to more than one genre, and each user may belong to more than one latent interest class. The number of existing latent genres and interests are not known beforehand, but should be inferred in a data-driven fashion. We devise a novel hierarchical factor analysis model to formulate our approach under these assumptions. We impose suitable priors over the allocation of items into genres, and users into interests; specifically, we utilize a novel scheme which comprises two coupled Indian buffet process priors that allow the number of latent classes (genres/interests) to be automatically inferred. We experiment on a large set of real ratings data, and show that our approach outperforms four common baselines, including two very competitive state-of-the-art approaches."
2436,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,A Ranking-based KNN Approach for Multi-Label Classification,"T.-H. Chiang, H.-Y. Lo & S.-D. Lin","25:81-96, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/chiang12/chiang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_chiang12,http://jmlr.csail.mit.edu/proceedings/papers/v25/chiang12.html,"Multi-label classification has attracted a great deal of attention in recent years. This paper presents an interesting finding, namely, being able to identify neighbors with trustable labels can significantly improve the classification accuracy. Based on this finding, we propose aæk-nearest-neighbor-based ranking approach to solve the multi-label classification problem. The approach exploits a ranking model to learn which neighbor's labels are more trustable candidates for a weighted KNN-based strategy, and then assigns higher weights to those candidates when making weighted-voting decisions. The weights can then be determined by using a generalized pattern search technique. We collect several real-word data sets from various domains for the experiment. Our experiment results demonstrate that the proposed method outperforms state-of-the-art instance-based learning approaches. We believe that appropriately exploiting k-nearest neighbors is useful to solve the multi-label problem."
2437,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,AIC and BIC based approaches for SVM parameter value estimation with RBF kernels,"S. Demyanov, J. Bailey, K. Ramamohanarao & C. Leckie","25:97-112, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/demyanov12/demyanov12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_demyanov12,http://jmlr.csail.mit.edu/proceedings/papers/v25/demyanov12.html,"We study the problem of selecting the best parameter values to use for a support vector machine (SVM) with RBF kernel. Our methods extend the well-known formulas for AIC and BIC, and we present two alternative approaches for calculating the necessary likelihood functions for these formulas. Our first approach is based on using the distances of support vectors from the separating hyperplane. Our second approach estimates the probability that the SVM hyperplane coincides with the Bayes classifier, by analysing the disposition of points in the kernel feature space. We experimentally compare our two approaches with several existing methods and show they are able to achieve good accuracy, whilst also having low running time."
2438,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Online Learning of a Dirichlet Process Mixture of Generalized Dirichlet Distributions for Simultaneous Clustering and Localized Feature Selection,W. Fan & N. Bouguila,"25:113-128, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/fan12/fan12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_fan12,http://jmlr.csail.mit.edu/proceedings/papers/v25/fan12.html,"Online algorithms allow data instances to be processed in a sequential way, which is important for large-scale and real-time applications. In this paper, we propose a novel online clustering approach based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions, which can be considered as an extension of the finite GD mixture model to the infinite case. Our approach is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. Moreover, an unsupervised localized feature selection scheme is integrated with the proposed nonparametric framework to improve the clustering performance. By learning the proposed model in an online manner using a variational approach, all the involved parameters and features saliencies are estimated simultaneously and effectively in closed forms. The proposed online infinite mixture model is validated through both synthetic data sets and two challenging real-world applications namely text document clustering and online human face detection."
2439,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,A stochastic bandit algorithm for scratch games,R. F _ raud & T. Urvoy,"25:129-143, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/feraud12/feraud12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_feraud12,http://jmlr.csail.mit.edu/proceedings/papers/v25/feraud12.html,"Stochastic multi-armed bandit algorithms are used to solve the exploration and exploitation dilemma in sequential optimization problems. The algorithms based on upper confidence bounds offer strong theoretical guarantees, they are easy to implement and efficient in practice. We considers a new bandit setting, called \scratch-games"", where arm budgets are limited and reward are drawn without replacement. Using Serfling inequality, we propose an upper confidence bound algorithm adapted to this setting. We show that the bound of expectation to play a suboptimal arm is lower than the one of UCB1 policy. We illustrate this result on both synthetic problems and realistic problems (ad-serving and emailing campaigns optimization)."
2440,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Max-Margin Ratio Machine,S. Gu & Y. Guo,"25:145-157, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/gu12/gu12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_gu12,http://jmlr.csail.mit.edu/proceedings/papers/v25/gu12.html,"In this paper, we investigate the problem of exploiting global information to improve the performance of SVMs on large scale classification problems. We first present a unified general framework for the existing min-max machine methods in terms of within-class dispersions and between-class dispersions. By defining a new within-class dispersion measure, we then propose a novel max-margin ratio machine (MMRM) method that can be formulated as a linear programming problem with scalability for large data sets. Kernels can be easily incorporated into our method to address non-linear classification problems. Our empirical results show that the proposed MMRM approach achieves promising results on large data sets."
2441,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Learning Temporal Association Rules on Symbolic Time Sequences,M. Guillame-Bert & J.L. Crowley,"25:159-174, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/guillame-bert12/guillame-bert12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_guillame-bert12,http://jmlr.csail.mit.edu/proceedings/papers/v25/guillame-bert12.html,"We introduce a temporal pattern model called Temporal Interval Tree Association Rules (Tita rules or Titar). This pattern model can express both uncertainty and temporal inaccuracy of temporal events. Among other things, Tita rules can express the usual time point operators, synchronicity, order, and chaining, as well as temporal negation and disjunctive temporal constraints. Using this representation, we present the Titar learner algorithm that can be used to extract Tita rules from large datasets expressed as Symbolic Time Sequences. The selection of temporal constraints (or time-frames) is at the core of the temporal learning. Our learning algorithm is based on two novel approaches for this problem. This first one is designed to select temporal constraints for the head of temporal association rules. The second selects temporal constraints for the body of such rules. We discuss the evaluation of probabilistic temporal association rules, evaluate our technique with two experiments, introduce a metric to evaluate sets of temporal rules, compare the results with two other approaches and discuss the results."
2442,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,More Is Better: Large Scale Partially-supervised Sentiment Classication,"Y. Haimovitch, K. Crammer & S. Mannor","25:175-190, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/haimovitch12/haimovitch12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_haimovitch12,http://jmlr.csail.mit.edu/proceedings/papers/v25/haimovitch12.html,"We describe a bootstrapping algorithm to learn from partially labeled data, and the results of an empirical study for using it to improve performance of sentiment classification using up to 15 million unlabeled Amazon product reviews. Our experiments cover semi-supervised learning, domain adaptation and weakly supervised learning. In some cases our methods were able to reduce test error by more than half using such large amount of data."
2443,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Frustratingly Simplified Deployment in WLAN Localization by Learning from Route Annotation,"R. Kawajiri, M. Shimosaka, R. Fukui & T. Sato","25:191-204, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/kawajiri12/kawajiri12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_kawajiri12,http://jmlr.csail.mit.edu/proceedings/papers/v25/kawajiri12.html,"Recently wireless LAN (WLAN) localization systems are gaining popularity in pervasive computing, machine learning and sensor networks communities, especially indoor scenarios where GPS coverage is limited. To accurately predict location, a large amount of fingerprints composed of received signal strength values is necessary. Moreover, standard supervised or semi-supervised approaches also require location information to each fingerprint, where annotation work is rather tedious and time consuming. To reduce the efforts and time required to build calibration data, we present a novel calibration methodology \route-annotation"" and a self-training algorithm for learning from route information effectively. On the proposed calibration methodology, an annotator walks around while measuring fingerprints, then occasionally stops to annotate fingerprints with route from previous location to current location. This calibration reduces work time even compared to partially annotation, while routes have richer information for learning. The proposed learning algorithm comprises following two iterative steps: 1) inferring locations of each fingerprint under route constraints and 2) updating parameters. Experimental results on real-world datasets demonstrate learning from route-annotated data is comparable to state-of-the-art supervised and semi-supervised approaches trained with large amount of calibration data."
2444,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Variational Bayesian Matching,A. Klami,"25:205-220, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/klami12/klami12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_klami12,http://jmlr.csail.mit.edu/proceedings/papers/v25/klami12.html,"Matching of samples refers to the problem of inferring unknown co-occurrence or alignment between observations in two data sets. Given two sets of equally many samples, the task is to find for each sample a representative sample in the other set, without prior knowledge on a distance measure between the sets. Recently a few alternative solutions have been suggested, based on maximization of joint likelihood or various measures of between-data statistical dependency. In this work we present an variational Bayesian solution for the problem, learning a Bayesian canonical correlation analysis model with a permutation parameter for re-ordering the samples in one of the sets. We approximate the posterior over the permutations, and demonstrate that the resulting matching algorithm clearly outperforms all of the earlier solutions."
2445,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Active Learning with Hinted Support Vector Machine,"C.-L. Li, C.-S. Ferng & H.-T. Lin","25:221-235, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/li12/li12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_li12,http://jmlr.csail.mit.edu/proceedings/papers/v25/li12.html,"The abundance of real-world data and limited labeling budget calls for active learning, which is an important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and representativeness when making querying decisions. However, exploiting representativeness with uncertainty concurrently usually requires tackling sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, calledæhinted sampling, which takes both uncertainty and representativeness into account in a simpler way. We design a novel active learning algorithm within the hinted sampling framework with an extended support vector machine. Experimental results validate that the novel active learning algorithm can result in a better and more stable performance than that achieved by state-of-the-art algorithms."
2446,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,A Convex-Concave Relaxation Procedure Based Subgraph Matching Algorithm,Z.-Y. Liu & H. Qiao,"25:237-252, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/liu12a/liu12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_liu12a,http://jmlr.csail.mit.edu/proceedings/papers/v25/liu12a.html,"Based on the convex-concave relaxation procedure (CCRP), the (extended) path following algorithms were recently proposed to approximately solve the equal-sized graph matching problem, and exhibited a state-of-the-art performance (Zaslavskiy etæal.,æ2009;æLiu etæal.,æ2012). However, they cannot be used for subgraph matching since either their convex or concave relaxation becomes no longer applicable. In this paper we extend the CCRP to tackle subgraph matching, by proposing a convex as well as a concave relaxation of the problem. Since in the context of CCRP, the convex relaxation can be viewed as an initialization of a concave programming, we introduce two other initializations for comparison. Meanwhile, the graduated assignment algorithm is also introduced in the experimental comparisons, which witness the validity of the proposed algorithm."
2447,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Key Instance Detection in Multi-Instance Learning,"G. Liu, J. Wu & Z.-H. Zhou","25:253-268, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/liu12b/liu12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_liu12b,http://jmlr.csail.mit.edu/proceedings/papers/v25/liu12b.html,"The goal of traditional multi-instance learning (MIL) is to predict the labels of the bags, whereas in many real applications, it is desirable to get the instance labels, especially the labels of key instances that trigger the bag labels, in addition to getting bag labels. Such a problem has been largely unexplored before. In this paper, we formulate the Key Instance Detection (KID) problem, and propose a voting framework (VF) solution to KID. The key of VF is to exploit the relationship among instances, represented by a citer kNN graph. This graph is different from commonly used nearest neighbor graphs, but is suitable for KID. Experiments validate the effectiveness of VF for KID. Additionally, VF also outperforms state-of-the-art MIL approaches on the performance of bag label prediction."
2448,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,On Using Nearly-Independent Feature Families for High Precision and Confidence,"O. Madani, M. Georg & D.A. Ross","25:269-284, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/madani12/madani12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_madani12,http://jmlr.csail.mit.edu/proceedings/papers/v25/madani12.html,"Often we require classification at a very high precision level, such as 99%. We report that when very different sources of evidence such as text, audio, and video features are available, combining the outputs of base classifiers trained on each feature type separately, aka late fusion, can substantially increase the recall of the combination at high precisions, compared to the performance of a single classifier trained on all the feature typesæi.e.,æearly fusion, or compared to the individual base classifiers. We show how the probability of a joint false-positive mistake can be upper bounded by the product of individual probabilities of conditional false-positive mistakes, by identifying a simple key criterion that needs to hold. This provides an explanation for the high precision phenomenon, and motivates referring to such feature families as (nearly) independent. We assess the relevant factors for achieving high precision empirically, and explore combination techniques informed by the analysis. We compare a number of early and late fusion methods, and observe that classifier combination via late fusion can more than double the recall at high precision."
2449,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Learning and Model-Checking Networks of I/O Automata,H. Mao & M. Jaeger,"25:285-300, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/mao12/mao12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_mao12,http://jmlr.csail.mit.edu/proceedings/papers/v25/mao12.html,"We introduce a new statistical relational learning (SRL) approach in which models for structured data, especially network data, are constructed as networks of communicating finite probabilistic automata. Leveraging existing automata learning methods from the area of grammatical inference, we can learn generic models for network entities in the form of automata templates. As is characteristic for SRL techniques, the abstraction level afforded by learning generic templates enables one to apply the learned model to new domains. A main benefit of learning models based on finite automata lies in the fact that one can analyse the resulting models using formal model-checking techniques, which adds a dimension of model analysis not usually available for traditional SRL modeling frameworks."
2450,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Sparse Additive Matrix Factorization for Robust PCA and Its Generalization,"S. Nakajima, M. Sugiyama & S.D. Babacan","25:301-316, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/mao12/mao12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_nakajima12,http://jmlr.csail.mit.edu/proceedings/papers/v25/nakajima12.html,"Principal component analysis (PCA) can be regarded as approximating a data matrix with a low-rank one by imposing sparsity on its singular values, and its robust variant further captures sparse noise. In this paper, we extend such sparse matrix learning methods, and propose a novel unified framework calledæsparse additive matrix factorizationæ(SAMF). SAMF systematically induces various types of sparsity by the so-calledæmodel-inducedregularizationæin the Bayesian framework. We propose an iterative algorithm called theæmeanæupdateæ(MU) for the variational Bayesian approximation to SAMF, which gives the global optimal solution for a large subset of parameters in each step. We demonstrate the usefulness of our method on artificial data and the foreground/background video separation."
2451,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Statistical Models for Exploring Individual Email Communication Behavior,"N. Navaroli, C. DuBois & P. Smyth","25:317-332, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/navaroli12/navaroli12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_navaroli12,http://jmlr.csail.mit.edu/proceedings/papers/v25/navaroli12.html,"As digital communication devices play an increasingly prominent role in our daily lives, the ability to analyze and understand our communication patterns becomes more important. In this paper, we investigate a latent variable modeling approach for extracting information from individual email histories, focusing in particular on understanding how an individual communicates over time with recipients in their social network. The proposed model consists of latent groups of recipients, each of which is associated with a piecewise-constant Poisson rate over time. Inference of group memberships, temporal changepoints, and rate parameters is carried out via Markov Chain Monte Carlo (MCMC) methods. We illustrate the utility of the model by applying it to both simulated and real-world email data sets."
2452,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,QBoost: Large Scale Classifier Training with Adiabatic Quantum Optimization,"H. Neven, V.S. Denchev, G. Rose & W.G. Macready","25:333-348, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/neven12/neven12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_neven12,http://jmlr.csail.mit.edu/proceedings/papers/v25/neven12.html,"We introduce a novel discrete optimization method for training in the context of a boosting framework for large scale binary classifiers. The motivation is to cast the training problem into the format required by existing adiabatic quantum hardware. First we provide theoretical arguments concerning the transformation of an originally continuous optimization problem into one with discrete variables of low bit depth. Next we proposeæQBoostæas an iterative training algorithm in which a subset of weak classifiers is selected by solving a hard optimization problem in each iteration. A strong classifier is incrementally constructed by concatenating the subsets of weak classifiers. We supplement the findings with experiments on one synthetic and two natural data sets and compare against the performance of existing boosting algorithms. Finally, by conducting a quantum Monte Carlo simulation we gather evidence that adiabatic quantum optimization is able to handle the discrete optimization problems generated by QBoost."
2453,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Recovering Networks from Distance Data,"S. Prabhakaran, K.J. Metzner, A. B _ hm & V. Roth","25:349-364, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/prabhakaran12/prabhakaran12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_prabhakaran12,http://jmlr.csail.mit.edu/proceedings/papers/v25/prabhakaran12.html,"A fully probabilistic approach to reconstructing Gaussian graphical models from distance data is presented. The main idea is to extend the usual central Wishart model in traditional methods to using a likelihood depending only on pairwise distances, thus being independent of geometric assumptions about the underlying Euclidean space. This extension has two advantages: the model becomes invariant against potential bias terms in the measurements, and can be used in situations which on input use a kernel- or distance matrix, without requiring direct access to the underlying vectors. The latter aspect opens up a huge new application field for Gaussian graphical models, as network reconstruction is now possible from any Mercer kernel, be it on graphs, strings, probabilities or more complex objects. We combine this likelihood with a suitable prior to enable Bayesian network inference. We present an efficient MCMC sampler for this model and discuss the estimation of module networks. Experiments depict the high quality and usefulness of the inferred networks."
2454,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Topographic Analysis of Correlated Components,"H. Sasaki, M.U. Gutmann, H. Shouno & A. Hyv _ rinen","25:365-378, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/sasaki12/sasaki12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_sasaki12,http://jmlr.csail.mit.edu/proceedings/papers/v25/sasaki12.html,"Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants are assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the source where the components can have linear and higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data."
2455,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Improved sequence classification using adaptive segmental sequence alignment,S. Shariat & V. Pavlovic,"25:379-394, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/shariat12/shariat12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_shariat12,http://jmlr.csail.mit.edu/proceedings/papers/v25/shariat12.html,"Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in some instances matching two segments of points may be preferred and can result in increased noise robustness. This paper presents an approach to segmental sequence alignment based on adaptive pairwise segmentation. We introduce a distance metric between segments based on average pairwise distances, which addresses deficiencies of prior approaches. We then present a modified pair-HMM that incorporates the proposed distance metric and use it to devise an e_cient algorithm to jointly segment and align the two sequences. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of problems, from EEG to motion sequence classification."
2456,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Supervised dimension reduction with topic models,"K. Than, T.B. Ho, D.K. Nguyen & N.K. Pham","25:395-410, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/than12/than12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_than12,http://jmlr.csail.mit.edu/proceedings/papers/v25/than12.html,"We consider supervised dimension reduction (SDR) for problems with discrete variables. Existing methods are computationally expensive, and often do not take the local structure of data into consideration when searching for a low-dimensional space. In this paper, we propose a novel framework for SDR which is (1) general and fiexible so that it can be easily adapted to various unsupervised topic models, (2) able to inherit scalability of unsupervised topic models, and (3) can exploit well label information and local structure of data when searching for a new space. Extensive experiments with adaptations to three models demonstrate that our framework can yield scalable and qualitative methods for SDR. One of those adaptations can perform better than the state-of-the-art method for SDR while enjoying significantly faster speed."
2457,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data Analysis,"T. Tran, D. Phung & S. Venkatesh","25:411-426, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12a/tran12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_tran12a,http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12a.html,"Ordinal data is omnipresent in almost all multiuser-generated feedback - questionnaires, preferences etc. This paper investigates modelling of ordinal data with Gaussian restricted Boltzmann machines (RBMs). In particular, we present the model architecture, learning and inference procedures for both vector-variate and matrix-variate ordinal data. We show that our model is able to capture latent opinion profile of citizens around the world, and is competitive against state-of-art collaborative filtering techniques on large-scale public datasets. The model thus has the potential to extend application of RBMs to diverse domains such as recommendation systems, product reviews and expert assessments."
2458,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Learning From Ordered Sets and Applications in Collaborative Ranking,"T. Tran, D. Phung & S. Venkatesh","25:427-442, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12b/tran12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_tran12b,http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12b.html,"Ranking over sets arise when users choose between groups of items. For example, a group may be of those movies deemed 5 stars to them, or a customized tour package. It turns out, to model this data type properly, we need to investigate the general combinatorics problem of partitioning a set and ordering the subsets. Here we construct a probabilistic log-linear model over a set of ordered subsets. Inference in this combinatorial space is highly challenging: The space size approaches (N!=2)6:93145N+1æasæNæapproaches infinity. We propose aæsplit-and-mergeæMetropolis-Hastings procedure that can explore the state-space efficiently. For discovering hidden aspects in the data, we enrich the model with latent binary variables so that the posteriors can be efficiently evaluated. Finally, we evaluate the proposed model on large-scale collaborative filtering tasks and demonstrate that it is competitive against state-of-the-art methods."
2459,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Two-way Parallel Class Expression Learning,"A.C. Tran, J. Dietrich, H.W. Guesgen & S. Marsland","25:443-458, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12c/tran12c.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_tran12c,http://jmlr.csail.mit.edu/proceedings/papers/v25/tran12c.html,"In machine learning, we often encounter datasets that can be described using simple rules and regular exception patterns describing situations where those rules do not apply. In this paper, we propose a two-way parallel class expression learning algorithm that is suitable for this kind of problem. This is a top-down refinement-based class expression learning algorithm for Description Logic (DL). It is distinguished from similar DL learning algorithms in the way it uses the concepts generated by the refinement operator. In our approach, we unify the computation of concepts describing positive and negative examples, but we maintain them separately, and combine them at the end. By doing so, we can avoid the use of negation in the refinement without any loss of generality. Evaluation shows that our approach can reduce the search space significantly, and therefore the learning time is reduced. Our implementation is based on the DL-Learner framework and we inherit the Parallel Class Expression Learning (ParCEL) algorithm design for parallelisation."
2460,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Multi-Stage Classifier Design,"K. Trapeznikov, V. Saligrama & D. Casta ~ n o n","25:459-474, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/trapeznikov12/trapeznikov12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_trapeznikov12,http://jmlr.csail.mit.edu/proceedings/papers/v25/trapeznikov12.html,"In many classification systems, sensing modalities have different acquisition costs. It is oftenæunnecessaryæto use every modality to classify a majority of examples. We study a multi-stage system in a prediction time cost reduction setting, where the full data is available for training, but for a test example, measurements in a new modality can be acquired at each stage for an additional cost. We seek decision rules to reduce the average measurement acquisition cost. We formulate an empirical risk minimization problem (ERM) for a multi-stage reject classifier, wherein the stageækæclassifier either classifies a sample using only the measurements acquired so far or rejects it to the next stage where more attributes can be acquired for a cost. To solve the ERM problem, we factorize the cost function into classification and rejection decisions. We then transform reject decisions into a binary classification problem. We construct stage-by-stage global surrogate risk, develop an iterative algorithm in the boosting framework and present convergence results. We test our work on synthetic, medical and explosives detection datasets. Our results demonstrate that substantial cost reduction without a significant sacrifice in accuracy is achievable."
2461,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Conditional validity of inductive conformal predictors,V. Vovk,"25:475-490, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/vovk12/vovk12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_vovk12,http://jmlr.csail.mit.edu/proceedings/papers/v25/vovk12.html,"Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications."
2462,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Spatial Locality-Aware Sparse Coding and Dictionary Learning,"J. Wang, J. Yuan, Z. Chen & Y. Wu","25:491-505, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/wang12a/wang12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_wang12a,http://jmlr.csail.mit.edu/proceedings/papers/v25/wang12a.html,"Nonlinear encoding of SIFT features has recently shown good promise in image classification. This scheme is able to reduce the training complexity of the traditional bag-of-feature approaches while achieving better performance. As a result, it is suitable for large-scale image classification applications. However, existing nonlinear encoding methods do not explicitly consider the spatial relationship when encoding the local features, but merely leaving the spatial information used at a later stage, e.g. through the spatial pyramid matching, is largely inadequate. In this paper, we propose a joint sparse coding and dictionary learning scheme that take the spatial information into consideration in encoding. Our experiments on synthetic data and benchmark data demonstrate that the proposed scheme can learn a better dictionary and achieve higher classification accuracy."
2463,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Multi-objective Monte-Carlo Tree Search,W. Wang & M. Sebag,"25:507-522, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/wang12b/wang12b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_wang12b,http://jmlr.csail.mit.edu/proceedings/papers/v25/wang12b.html,"Concerned with multi-objective reinforcement learning (MORL), this paper presents MO-MCTS, an extension of Monte-Carlo Tree Search to multi-objective sequential decision making. The known multi-objective indicator referred to as hyper-volume indicatoræis used to define an action selection criterion, replacing the UCB criterion in order to deal with multi-dimensional rewards. MO-MCTSæis firstly compared with an existing MORL algorithm on the artificial Deep Sea Treasure problem. Then a scalability study of MO-MCTSæis made on the NP-hard problem of grid scheduling, showing that the performance of MO-MCTSæmatches the non RL-based state of the art albeit with a higher computational cost."
2464,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Practical Large Scale Classification with Additive Kernels,H. Yang & J. Wu,"25:523-538, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/yang12/yang12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_yang12,http://jmlr.csail.mit.edu/proceedings/papers/v25/yang12.html,"For classification problems with millions of training examples or dimensions, accuracy, training and testing speed and memory usage are the main concerns. Recent advances have allowed linear SVM to tackle problems with moderate time and space cost, but for many tasks in computer vision, additive kernels would have higher accuracies. In this paper, we propose the PmSVM-LUT algorithm that employs Look-Up Tables to boost the training and testing speed and save memory usage of additive kernel SVM classification, in order to meet the needs of large scale problems. The PmSVM-LUT algorithm is based on PmSVM (Wu,æ2012), which employed polynomial approximation for the gradient function to speedup the dual coordinate descent method. We also analyze the polynomial approximation numerically to demonstrate its validity. Empirically, our algorithm is faster than PmSVM and feature mapping in many datasets with higher classification accuracies and can save up to 60% memory usage as well."
2465,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Online Rank Aggregation,"S. Yasutake, K. Hatano, E. Takimoto & M. Takeda","25:539-553, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/yasutake12/yasutake12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_yasutake12,http://jmlr.csail.mit.edu/proceedings/papers/v25/yasutake12.html,"We consider an online learning framework where the task is to predict a permutation which represents a ranking ofænæfixed objects. At each trial, the learner incurs a loss defined as Kendall tau distance between the predicted permutation and the true permutation given by the adversary. This setting is quite natural in many situations such as information retrieval and recommendation tasks. We prove a lower bound of the cumulative loss and hardness results. Then, we propose an algorithm for this problem and prove its relative loss bound which shows our algorithm is close to optimal."
2466,25,http://jmlr.csail.mit.edu/proceedings/papers/v25/,Multi-view Positive and Unlabeled Learning,"J.T. Zhou, S.J. Pan, Q. Mao & I.W. Tsang","25:555-570, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v25/zhou12/zhou12.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v25/,,17th November 2012,"November 4-6, 2012 ",ACML 2012 Proceedings,Proceedings of the Asian Conference on Machine Learning,"Singapore Management University, Singapore",Steven C.H. Hoi and Wray Buntine,v25_zhou12,http://jmlr.csail.mit.edu/proceedings/papers/v25/zhou12.html,"Learning with Positive and Unlabeled instances (PU learning) arises widely in information retrieval applications. To address the unavailability issue of negative instances, most existing PU learning approaches require to either identify a reliable set of negative instances from the unlabeled data or estimate probability densities as an intermediate step. However, inaccurate negative-instance identification or poor density estimation may severely degrade overall performance of the final predictive model. To this end, we propose a novel PU learning method based on density ratio estimation without constructing any sets of negative instances or estimating any intermediate densities. To further boost PU learning performance, we extend our proposed learning method in a multi-view manner by utilizing multiple heterogeneous sources. Extensive experimental studies demonstrate the effectiveness of our proposed methods, especially when positive labeled data are limited."
2467,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,Exploration and Exploitation with Insufficient Resources,"Chris Lovell, Gareth Jones, Klaus-Peter Zauner, Steve R. Gunn","26:37-61, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/lovell12a/lovell12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,2-Jul-11,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_lovell12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/lovell12a.html,"In physical experimentation, the resources available to discover new knowledge are typically extremely small in comparison to the size and dimensionality of the parameter spaces that can be searched. Additionally, due to the nature of physical experimentation, experimental errors will occur, particularly in biochemical experimentation where the reactants may undetectably denature, or reactant contamination could occur or equipment failure. These errors mean that not all experimental measurements and observations will be accurate or representative of the system being investigated. As the validity of observations is not guaranteed, resources must be split between exploration to discover new knowledge and exploitation to test the validity of the new knowledge. Currently we are investigating the automation of discovery in physical experimentation, with the aim of producing a fully autonomous closed-loop robotic machine capable of autonomous experimentation. This machine will build and evaluate hypotheses, determine experiments to perform and then perform them on an automated lab-on-chip experimentation platform for biochemical response characterisation. In the present work we examine how the trade-off between exploration and exploitation can occur in a situation where the number of experiments that can be performed is extremely small and where the observations returned are sometimes erroneous or unrepresentative of the behaviour being examined. To manage this trade-off we consider the use of a Bayesian notion of surprise, which is used to perform exploration experiments whilst observations are unsurprising from the predictions that can be made and exploits when observations are surprising as they do not match the predicted response."
2468,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,ICML Exploration & Exploitation Challenge: Keep it simple!,"Olivier Nicol, J_r_mie Mary, Philippe Preux","26:62-85, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/nicol12a/nicol12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,2-Jul-11,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_nicol12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/nicol12a.html,"Recommendation has become a key feature in the economy of a lot of companies (online shopping, search engines...). There is a lot of work going on regarding recommender systems and there is still a lot to do to improve them. Indeed nowadays in many companies most of the job is done by hand. Moreover even when a supposedly smart recommender system is designed, it is hard to evaluate it without using real audience which obviously involves economic issues. The ICML Exploration & Exploitation challenge is an attempt to make people propose efficient recommendation techniques and particularly focuses on limited computational resources. The challenge also proposes a framework to address the problem of evaluating a recommendation algorithm with real data. We took part in this challenge and achieved the best performances; this paper aims at reporting on this achievement; we also discuss the evaluation process and propose a better one for future challenges of the same kind."
2469,26,http://jmlr.csail.mit.edu/proceedings/papers/v26/,PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits,"Yevgeny Seldin, Nicol„ Cesa-Bianchi, Peter Auer, François Laviolette, John Shawe-Taylor","26:98-111, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v26/seldin12a/seldin12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v26/,,2nd May 2012,2-Jul-11,On-line Trading of Exploration and Exploitation 2011 Proceedings,Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2,"Washington, USA","Dorota Glowacka, Louis Dorard and John Shawe-Taylor",v26_seldin12a,http://jmlr.csail.mit.edu/proceedings/papers/v26/seldin12a.html,"We develop a new tool for data-dependent analysis of the exploration-exploitation trade-off in learning under limited feedback. Our tool is based on two main ingredients. The first ingredient is a new concentration inequality that makes it possible to control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. The second ingredient is an application of this inequality to the exploration-exploitation trade-off via importance weighted sampling. We apply the new tool to the stochastic multiarmed bandit problem, however, the main importance of this paper is the development and understanding of the new tool rather than improvement of existing algorithms for stochastic multiarmed bandits. In the follow-up work we demonstrate that the new tool can improve over state-of-the-art in structurally richer problems, such as stochastic multiarmed bandits with side information."
2470,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,ICML2011 Unsupervised and Transfer Learning Workshop,"D.L. Silver, I. Guyon, G. Taylor, G. Dror & V. Lemaire ; 27:1-16, 2012. [ abs ] [ pdf ]","27:1-16, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/silver12a/silver12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_silver12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/silver12a.html,"We organized a data mining challenge in ñunsupervised and transfer learningî (the UTL challenge) followed by a workshop of the same name at the ICML 2011 conference in Bellevue, Washington. This introduction presents the highlights of the outstanding contributions that were made, which are regrouped in this issue of JMLR W&CP. Novel methodologies emerged to capitalize on large volumes of unlabeled data from tasks related (but di_erent) from a target task, including a method to learn data kernels (similarity measures) and new deep architectures for feature learning."
2471,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Deep Learning of Representations for Unsupervised and Transfer Learning,"Y. Bengio ; 27:17-36, 2012. [ abs ] [ pdf ]","27:17-36, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/bengio12a/bengio12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_bengio12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/bengio12a.html,"Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features de_ned in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distributionæP(x) is structurally related to some task of interest, say predictingæP(y|x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution."
2472,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,"Autoencoders, Unsupervised Learning, and Deep Architectures","P. Baldi ; 27:37-50, 2012. [ abs ] [ pdf ]","27:37-50, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/baldi12a/baldi12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_baldi12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/baldi12a.html,"Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the di_erent kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory."
2473,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Information Theoretic Model Selection for Pattern Analysis,"J.M. Buhmann, M.H. Chehreghani, M. Frank & A.P. Streich ; 27:51-64, 2012. [ abs ] [ pdf ]","27:51-64, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/buhmann12a/buhmann12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_buhmann12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/buhmann12a.html,"Exploratory data analysis requires (i) to de_ne a set of patterns hypothesized to exist in the data, (ii) to specify a suitable quanti_cation principle or cost function to rank these patterns and (iii) to validate the inferred patterns. For data clustering, the patterns are object partitionings intoækægroups; for PCA or truncated SVD, the patterns are orthogonal transformations with projections to a low-dimensional space. We propose an information theoretic principle for model selection and model-order selection. Our principle ranks competing pattern cost functions according to their ability to extract context sensitive information from noisy data with respect to the chosen hypothesis class. Sets of approximative solutions serve as a basis for a communication protocol. Analogous toæ?, inferred models maximize the so-called approximation capacity that is the mutual information between coarsened training data patterns and coarsened test data patterns. We demonstrate how to apply our validation framework by the well-known Gaussian mixture model and by a multi-label clustering approach for role mining in binary user privilege assignments."
2474,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Clustering: Science or Art?,"U. von Luxburg, R.C. Williamson & I. Guyon ; 27:65-80, 2012. [ abs ] [ pdf ]","27:65-80, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/luxburg12a/luxburg12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_luxburg12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/luxburg12a.html,"We examine whether the quality of di_erent clustering algorithms can be compared by a general, scienti_cally sound procedure which is independent of particular clustering algorithms. We argue that the major obstacle is the di_culty in evaluating a clustering algorithm without taking into account the context: why does the user cluster his data in the _rst place, and what does he want to do with the clustering afterwards? We argue that clustering should not be treated as an application-independent mathematical problem, but should always be studied in the context of its end-use. Di_erent techniques to evaluate clustering algorithms have to be developed for di_erent uses of clustering. To simplify this procedure we argue that it will be useful to build a ñtaxonomy of clustering problemsî to identify clustering applications which can be treated in a uni_ed way and that such an e_ort will be more fruitful than attempting the impossible „ developing ñoptimalî domain-independent clustering algorithms or even classifying clustering algorithms in terms of how they work."
2475,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Transfer Learning by Kernel Meta-Learning,"F. Aiolli ; 27:81-95, 2012. [ abs ] [ pdf ]","27:81-95, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/aiolli12a/aiolli12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_aiolli12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/aiolli12a.html,"A crucial issue in machine learning is how to learn appropriate representations for data. Recently, much work has been devoted to kernel learning, that is, the problem of _nding a good kernel matrix for a given task. This can be done in a semi-supervised learning setting by using a large set of unlabeled data and a (typically small) set of i.i.d. labeled data. Another, even more challenging problem, is how one can exploit partially labeled data of a source task to learn good representations for a di_erent, but related, target task. This is the main subject of transfer learning.In this paper, we present a novel approach to transfer learning based on kernel learning. Speci_cally, we propose a kernel meta-learning algorithm which, starting from a basic kernel, tries to learn chains of kernel transforms that are able to produce good kernel matrices for the source tasks. The same sequence of transformations can be then applied to compute the kernel matrix for new related target tasks. We report on the application of this method to the _ve datasets of the Unsupervised and Transfer Learning (UTL) challenge benchmark1 , where we won the _rst phase of the competition."
2476,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Unsupervised and Transfer Learning Challenge: a Deep Learning Approach,"G. Mesnil et al ; 27:97-110, 2012. [ abs ] [ pdf ]","27:97-110, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/mesnil12a/mesnil12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_mesnil12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/mesnil12a.html,"Learning good representations from a large set of unlabeled data is a particularly challenging task. Recent work (see ? for a review) shows that training deep architectures is a good way to extract such representations, by extracting and disentangling gradually higher-level factors of variation characterizing the input distribution. In this paper, we describe di_erent kinds of layers we trained for learning representations in the setting of the Unsupervised and Transfer Learning Challenge. The strategy of our team won the _nal phase of the challenge. It combined and stacked di_erent one-layer unsupervised learning algorithms, adapted to each of the _ve datasets of the competition. This paper describes that strategy and the particular one-layer learning algorithms feeding a simple linear classi_er with a tiny number of labeled training samples (1 to 64 per class)."
2477,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Stochastic Unsupervised Learning on Unlabeled Data,"C. Liu, J. Xie, Y. Ge & H. Xiong ; 27:111-122, 2012. [ abs ] [ pdf ]","27:111-122, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/liu12a/liu12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_liu12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/liu12a.html,"In this paper, we introduce a stochastic unsupervised learning method that was used in the 2011 Unsupervised and Transfer Learning (UTL) challenge. This method is developed to preprocess the data that will be used in the subsequent classi_cation problems. Speci_cally, it performs K-means clustering on principal components instead of raw data to remove the impact of noisy/irrelevant/less-relevant features and improve the robustness of the results. To alleviate the over_tting problem, we also utilize a stochastic process to combine multiple clustering assignments on each data point. Finally, promising results were observed on all the test data sets. Indeed, this proposed method won us the second place in the overall performance of the challenge."
2478,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Transfer Learning with Cluster Ensembles,"A. Acharya, E.R. Hruschka, J. Ghosh & S. Acharyya ; 27:123-132, 2012. [ abs ] [ pdf ]","27:123-132, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/acharya12a/acharya12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_acharya12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/acharya12a.html,"Traditional supervised learning algorithms typically assume that the training data and test data come from a common underlying distribution. Therefore, they are challenged by the mismatch between training and test distributions encountered in transfer learning situations. The problem is further exacerbated when the test data actually comes from a di_erent domain and contains no labeled example. This paper describes an optimization framework that takes as input one or more classi_ers learned on the source domain as well as the results of a cluster ensemble operating solely on the target domain, and yields a consensus labeling of the data in the target domain. This framework is fairly general in that it admits a wide range of loss functions and classi_cation/clustering methods. Empirical results on both text and hyperspectral data indicate that the proposed method can yield superior classi_cation results compared to applying certain other transductive and transfer learning techniques or na¥vely applying the classi_er (ensemble) learnt on the source domain to the target domain."
2479,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Divide and Transfer: an Exploration of Segmented Transfer to Detect Wikipedia Vandalism,"S.-C. Chin & W.N. Street ; 27:133-144, 2012. [ abs ] [ pdf ]","27:133-144, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/chin12a/chin12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_chin12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/chin12a.html,"The paper applies knowledge transfer methods to the problem of detecting Wikipedia vandalism detection, de_ned as malicious editing intended to compromise the integrity of the content of articles. A major challenge of detecting Wikipedia vandalism is the lack of a large amount of labeled training data. Knowledge transfer addresses this challenge by leveraging previously acquired knowledge from a source task. However, the characteristics of Wikipedia vandalism are heterogeneous, ranging from a small replacement of a letter to a massive deletion of text. Selecting an informative subset from the source task to avoid potential negative transfer becomes a primary concern given this heterogeneous nature. The paper explores knowledge transfer methods to generalize learned models from a heterogeneous dataset to a more uniform dataset while avoiding negative transfer. The two novelæsegmented transferæ(ST)approaches map unlabeled data from the target task to the most related cluster from the source task, classifying the unlabeled data using the most relevant learned models."
2480,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Self-measuring Similarity for Multi-task Gaussian Process,"K. Hayashi, T. Takenouchi, R. Tomioka H. Kashima ; 27:145-154, 2012. [ abs ] [ pdf ]","27:145-154, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/hayashi12a/hayashi12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_hayashi12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/hayashi12a.html,"Multi-task learning aims at transferring knowledge between similar tasks. The multi-task Gaussian process framework of Bonillaæet al.æmodels (incomplete) responses ofæCædata points foræRætasks (e.g., the responses are given by anæRæ_æCæmatrix) by using a Gaussian process; the covariance function takes its form as the product of a covariance function de_ned on input-speci_c features and an inter-task covariance matrix (which is empirically estimated as a model parameter). We extend this framework by incorporating a novel similarity measurement, which allows for the representation of much more complex data structures. The proposed framework also enables us to exploit additional information (e.g., the input-speci_c features) when constructing the covariance matrices by combining additional information with the covariance function. We also derive an e_cient learning algorithm which uses an iterative method to make predictions. Finally, we apply our model to a real data set of recommender systems and show that the proposed method achieves the best prediction accuracy on the data set."
2481,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Transfer Learning for Auto-gating of Flow Cytometry Data,"G. Lee, L. Stoolman & C. Scott ; 27:155-166, 2012. [ abs ] [ pdf ]","27:155-166, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/lee12a/lee12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_lee12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/lee12a.html,"Flow cytometry is a technique for rapidly quantifying physical and chemical properties of large numbers of cells. In clinical applications, §ow cytometry data must be manually ñgatedî to identify cell populations of interest. While several researchers have investigated statistical methods for automating this process, most of them falls under the framework of unsupervised learning and mixture model _tting. We view the problem as one of transfer learning, which can leverage existing datasets previously gated by experts to automatically gate a new §ow cytometry dataset while accounting for biological variation. We illustrate our proposed method by automatically gating lymphocytes from peripheral blood samples."
2482,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Inductive Transfer for Bayesian Network Structure Learning,"A. Niculescu-Mizil & R. Caruana ; 27:167-180, 2012. [ abs ] [ pdf ]","27:167-180, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/niculescu12a/niculescu12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_niculescu12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/niculescu12a.html,"We study the multi-task Bayesian Network structure learning problem: given data for multiple related problems, learn a Bayesian Network structure for each of them, sharing information among the problems to boost performance. We learn the structures for all the problemsæsimultaneouslyæusing aæscore and searchæapproach that encourages the learned Bayes Net structures to be similar. Encouraging similarity promotes information sharing and prioritizes learning structural features that explain the data from all problems over features that only seem relevant to a single one. This leads to a signi_cant increase in the accuracy of the learned structures, especially when training data is scarce."
2483,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Unsupervised dimensionality reduction via gradient-based matrix factorization with two adaptive learning rates,"V. Nikulin & T.-H. Huang ; 27:181-194, 2012. [ abs ] [ pdf ]","27:181-194, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/nikulin12a/nikulin12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_nikulin12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/nikulin12a.html,"The high dimensionality of the data, the expressions of thousands of features in a much smaller number of samples, presents challenges that a_ect applicability of the analytical results. In principle, it would be better to describe the data in terms of a small number of meta-features, derived as a result of matrix factorization, which could reduce noise while still capturing the essential features of the data. Three novel and mutually relevant methods are presented in this paper: 1) gradient-based matrix factorization with two adaptive learning rates (in accordance with the number of factor matrices) and their automatic updates; 2) nonparametric criterion for the selection of the number of factors; and 3) nonnegative version of the gradient-based matrix factorization which doesnÍt require any extra computational costs in di_erence to the existing methods. We demonstrate e_ectiveness of the proposed methods to the supervised classi_cation of gene expression data."
2484,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,One-Shot Learning with a Hierarchical Nonparametric Bayesian Model,"R. Salakhutdinov, J. Tenenbaum & A. Torralba ; 27:195-206, 2012. [ abs ] [ pdf ]","27:195-206, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/salakhutdinov12a/salakhutdinov12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_salakhutdinov12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/salakhutdinov12a.html,"We develop a hierarchical Bayesian model that learns categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express di_erent priors for new classes. Given a single example of a novel category, we can e_ciently infer which super-category the novel category belongs to, and thereby estimate not only the new categoryÍs mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs signi_cantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples."
2485,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Multitask Learning in Computational Biology,"C. Widmer & G. R_tsch ; 27:207-216, 2012. [ abs ] [ pdf ]","27:207-216, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/widmer12a/widmer12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_widmer12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/widmer12a.html,"Computational Biology provides a wide range of applications for Multitask Learning (MTL) methods. As the generation of labels often is very costly in the biomedical domain, combining data from di_erent related problems or tasks is a promising strategy to reduce label cost. In this paper, we present two problems from sequence biology, where MTL was successfully applied. For this, we use regularization-based MTL methods, with a special focus on the case of a hierarchical relationship between tasks. Furthermore, we propose strategies to re_ne the measure of task relatedness, which is of central importance in MTL and _nally give some practical guidelines, when MTL strategies are likely to pay o_."
2486,27,http://jmlr.csail.mit.edu/proceedings/papers/v27/,Transfer Learning in Sequential Decision Problems: A Hierarchical Bayesian Approach,"A. Wilson, A. Fern & P. Tadepalli ; 27:217-227, 2012. [ abs ] [ pdf ]","27:217-227, 2012.",http://jmlr.csail.mit.edu/proceedings/papers/v27/wilson12a/wilson12a.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v27/,,27th June 2012,"June 28 through July 2, 2011",Unsupervised and Transfer Learning Workshop at ICML 2011, Unsupervised and Transfer Learning workshop ICML 2011,"Bellevue, Washington, USA","Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver",v27_wilson12a,http://jmlr.csail.mit.edu/proceedings/papers/v27/wilson12a.html,"Transfer learning is one way to close the gap between the apparent speed of human learning and the relatively slow pace of learning by machines. Transfer is doubly bene_cial in reinforcement learning where the agent not only needs to generalize from sparse experience, but also needs to e_ciently explore. In this paper, we show that the hierarchical Bayesian framework can be readily adapted to sequential decision problems and provides a natural formalization of transfer learning. Using our framework, we produce empirical results in a simple colored maze domain and a complex real-time strategy game. The results show that our Hierarchical Bayesian Transfer framework signi_cantly improves learning speed when tasks are hierarchically related."
2487,28,http://jmlr.csail.mit.edu/proceedings/papers/v28/,Scalable Optimization of Neighbor Embedding for Visualization,"Zhirong Yang, Jaakko Peltonen, Samuel Kaski",none,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v28/,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13b-supp.pdf,15th February 2013,"June 17 _ June 19, 2013.",ICML 2013 Proceedings,30th International Conference on Machine Learning ,"Atlanta, USA",Sanjoy Dasgupta and David McAllester,v28_yang13b,http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13b.html,"Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into ñforcesî exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their ñcenter of massî, rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings ñbig dataî within reach of visualization."
2488,35,http://jmlr.csail.mit.edu/proceedings/papers/v35/,Open Problem: Tensor Decompositions: Algorithms up to the Uniqueness Threshold?,"Aditya Bhaskara, Moses Charikar, Ankur Moitra, Aravindan Vijayaraghavan","JMLR W&CP 35 :1280-1282, 2014",http://jmlr.csail.mit.edu/proceedings/papers/v35/bhaskara14b.pdf,http://jmlr.csail.mit.edu/proceedings/papers/v35/,,29th of May 2014 ,"June 13-15, 2014",COLT 2014 Proceedings,27th Annual Conference on Learning Theory,"Barcelona, Spain","Maria Florina Balcan, Vitaly Feldman, Csaba SzepesvÕçri",v35_bhaskara14b,http://jmlr.csail.mit.edu/proceedings/papers/v35/bhaskara14b.html,"Factor analysis is a basic tool in statistics and machine learning, where the goal is to take many variables and explain them away using fewer unobserved variables, called factors. It was introduced in a pioneering study by psychologist Charles Spearman, who used it to test his theory that there are fundamentally two types of intelligence _ verbal and mathematical. This study has had a deep influence on modern psychology, to this day. However there is a serious mathematical limitation to this approach, which we describe next. In its most basic form, we are given a matrix M = PR i=1 ai _ bi . Our goal is to recover the factors {ai}i and {bi}i . However this decomposition is only unique if we add further assumptions, such as requiring the factors {ai}i and {bi}i to be orthonormal. Otherwise we could apply an R_R rotation to the factors {ai}i and its transpose to {bi}i and recover another valid decomposition. This is often called the rotation problem and has been a central stumbling block for factor analysis since SpearmanÍs work. To summarize: Even if there is a factorization M = PR i=1 ai _ bi that has a meaningful interpretation, there is no guarantee that factor analysis finds it! Tensor methods do not suffer from the same problems: Consider a tensor T = X R i=1 ai _ bi _ ci There are many natural conditions on the factors {ai}i , {bi}i and {ci}i (as we will describe below) that ensure this decomposition is unique among all decompositions with at most R factors. And so tensor decompositions are unique in a way that standard matrix decompositions are not, and have a broad range of applications in fields as diverse as psychometrics, chemometrics, medicine, signal processing and data mining. Next we describe the uniqueness conditions that are known. In a result that has been rediscovered many times, Jennrich gave a natural condition under which the above decomposition is unique. Moreover it comes with an algorithm for finding it!"